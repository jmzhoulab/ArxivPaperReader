paper_240326.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月26日 14:09
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri 22 Mar 24 18:00:00 GMT  to  Mon 25 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.15437
Date: Fri, 15 Mar 2024 21:38:26 GMT   (19kb)

Title: Apriori Knowledge in an Era of Computational Opacity: The Role of AI in
  Mathematical Discovery
Authors: Eamon Duede and Kevin Davey
Categories: cs.AI
\\
  Computation is central to contemporary mathematics. Many accept that we can
acquire genuine mathematical knowledge of the Four Color Theorem from Appel and
Haken's program insofar as it is simply a repetitive application of human forms
of mathematical reasoning. Modern LLMs / DNNs are, by contrast, opaque to us in
significant ways, and this creates obstacles in obtaining mathematical
knowledge from them. We argue, however, that if a proof-checker automating
human forms of proof-checking is attached to such machines, then we can obtain
apriori mathematical knowledge from them, even though the original machines are
entirely opaque to us and the proofs they output are not human-surveyable.
\\ ( https://arxiv.org/abs/2403.15437 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15456
Date: Tue, 19 Mar 2024 06:39:23 GMT   (1310kb,D)

Title: WoLF: Large Language Model Framework for CXR Understanding
Authors: Seil Kang, Donghyun Kim, Junhyeok Kim, Hyo Kyung Lee, Seong Jae Hwang
Categories: cs.AI cs.CL
\\
  Significant methodological strides have been made toward Chest X-ray (CXR)
understanding via modern vision-language models (VLMs), demonstrating
impressive Visual Question Answering (VQA) and CXR report generation abilities.
However, existing CXR understanding frameworks still possess several procedural
caveats. (1) Previous methods solely use CXR reports, which are insufficient
for comprehensive Visual Question Answering (VQA), especially when additional
health-related data like medication history and prior diagnoses are needed. (2)
Previous methods use raw CXR reports, which are often arbitrarily structured.
While modern language models can understand various text formats, restructuring
reports for clearer, organized anatomy-based information could enhance their
usefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize
linguistic correctness, lacking the capability to offer nuanced assessments of
the generated answers. In this work, to address the aforementioned caveats, we
introduce WoLF, a Wide-scope Large Language Model Framework for CXR
understanding. To resolve (1), we capture multi-faceted records of patients,
which are utilized for accurate diagnoses in real-world clinical scenarios.
Specifically, we adopt the Electronic Health Records (EHR) to generate
instruction-following data suited for CXR understanding. Regarding (2), we
enhance report generation performance by decoupling knowledge in CXR reports
based on anatomical structure even within the attention step via masked
attention. To address (3), we introduce an AI-evaluation protocol optimized for
assessing the capabilities of LLM. Through extensive experimental validations,
WoLF demonstrates superior performance over other models on MIMIC-CXR in the
AI-evaluation arena about VQA (up to +9.47%p mean score) and by metrics about
report generation (+7.3%p BLEU-1).
\\ ( https://arxiv.org/abs/2403.15456 ,  1310kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15504
Date: Fri, 22 Mar 2024 00:48:52 GMT   (21509kb,D)

Title: SymboSLAM: Semantic Map Generation in a Multi-Agent System
Authors: Brandon Curtis Colelough
Categories: cs.AI
Comments: 14 pages, 11 figures
\\
  Sub-symbolic artificial intelligence methods dominate the fields of
environment-type classification and Simultaneous Localisation and Mapping.
However, a significant area overlooked within these fields is solution
transparency for the human-machine interaction space, as the sub-symbolic
methods employed for map generation do not account for the explainability of
the solutions generated. This paper proposes a novel approach to
environment-type classification through Symbolic Simultaneous Localisation and
Mapping, SymboSLAM, to bridge the explainability gap. Our method for
environment-type classification observes ontological reasoning used to
synthesise the context of an environment through the features found within. We
achieve explainability within the model by presenting operators with
environment-type classifications overlayed by a semantically labelled occupancy
map of landmarks and features. We evaluate SymboSLAM with ground-truth maps of
the Canberra region, demonstrating method effectiveness. We assessed the system
through both simulations and real-world trials.
\\ ( https://arxiv.org/abs/2403.15504 ,  21509kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15574
Date: Fri, 22 Mar 2024 19:03:25 GMT   (1461kb,D)

Title: SensoryT5: Infusing Sensorimotor Norms into T5 for Enhanced Fine-grained
  Emotion Classification
Authors: Yuhan Xia, Qingqing Zhao, Yunfei Long, Ge Xu and Jia Wang
Categories: cs.AI
Comments: Accepted by CogALex 2024 conference
\\
  In traditional research approaches, sensory perception and emotion
classification have traditionally been considered separate domains. Yet, the
significant influence of sensory experiences on emotional responses is
undeniable. The natural language processing (NLP) community has often missed
the opportunity to merge sensory knowledge with emotion classification. To
address this gap, we propose SensoryT5, a neuro-cognitive approach that
integrates sensory information into the T5 (Text-to-Text Transfer Transformer)
model, designed specifically for fine-grained emotion classification. This
methodology incorporates sensory cues into the T5's attention mechanism,
enabling a harmonious balance between contextual understanding and sensory
awareness. The resulting model amplifies the richness of emotional
representations. In rigorous tests across various detailed emotion
classification datasets, SensoryT5 showcases improved performance, surpassing
both the foundational T5 model and current state-of-the-art works. Notably,
SensoryT5's success signifies a pivotal change in the NLP domain, highlighting
the potential influence of neuro-cognitive data in refining machine learning
models' emotional sensitivity.
\\ ( https://arxiv.org/abs/2403.15574 ,  1461kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15577
Date: Fri, 22 Mar 2024 19:04:58 GMT   (1160kb,D)

Title: Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based
  Adaptive Cruise Control
Authors: Xiao Li, H. Eric Tseng, Anouck Girard, Ilya Kolmanovsky
Categories: cs.AI cs.RO cs.SY eess.SY
\\
  Autonomous driving depends on perception systems to understand the
environment and to inform downstream decision-making. While advanced perception
systems utilizing black-box Deep Neural Networks (DNNs) demonstrate human-like
comprehension, their unpredictable behavior and lack of interpretability may
hinder their deployment in safety critical scenarios. In this paper, we develop
an Ensemble of DNN regressors (Deep Ensemble) that generates predictions with
quantification of prediction uncertainties. In the scenario of Adaptive Cruise
Control (ACC), we employ the Deep Ensemble to estimate distance headway to the
lead vehicle from RGB images and enable the downstream controller to account
for the estimation uncertainty. We develop an adaptive cruise controller that
utilizes Stochastic Model Predictive Control (MPC) with chance constraints to
provide a probabilistic safety guarantee. We evaluate our ACC algorithm using a
high-fidelity traffic simulator and a real-world traffic dataset and
demonstrate the ability of the proposed approach to effect speed tracking and
car following while maintaining a safe distance headway. The
out-of-distribution scenarios are also examined.
\\ ( https://arxiv.org/abs/2403.15577 ,  1160kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15586
Date: Fri, 22 Mar 2024 19:21:29 GMT   (641kb,D)

Title: Generative AI in Education: A Study of Educators' Awareness, Sentiments,
  and Influencing Factors
Authors: Aashish Ghimire, James Prather and John Edwards
Categories: cs.AI
\\
  The rapid advancement of artificial intelligence (AI) and the expanding
integration of large language models (LLMs) have ignited a debate about their
application in education. This study delves into university instructors'
experiences and attitudes toward AI language models, filling a gap in the
literature by analyzing educators' perspectives on AI's role in the classroom
and its potential impacts on teaching and learning. The objective of this
research is to investigate the level of awareness, overall sentiment
towardsadoption, and the factors influencing these attitudes for LLMs and
generative AI-based tools in higher education. Data was collected through a
survey using a Likert scale, which was complemented by follow-up interviews to
gain a more nuanced understanding of the instructors' viewpoints. The collected
data was processed using statistical and thematic analysis techniques. Our
findings reveal that educators are increasingly aware of and generally positive
towards these tools. We find no correlation between teaching style and attitude
toward generative AI. Finally, while CS educators show far more confidence in
their technical understanding of generative AI tools and more positivity
towards them than educators in other fields, they show no more confidence in
their ability to detect AI-generated work.
\\ ( https://arxiv.org/abs/2403.15586 ,  641kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15587
Date: Fri, 22 Mar 2024 19:21:44 GMT   (1896kb,D)

Title: Large language models for crowd decision making based on prompt design
  strategies using ChatGPT: models, analysis and challenges
Authors: Cristina Zuheros and David Herrera-Poyatos and Rosana Montes and
  Francisco Herrera
Categories: cs.AI
\\
  Social Media and Internet have the potential to be exploited as a source of
opinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a
methodology able to infer opinions and decisions from plain texts, such as
reviews published in social media platforms, by means of Sentiment Analysis.
Currently, the emergence and potential of Large Language Models (LLMs) lead us
to explore new scenarios of automatically understand written texts, also known
as natural language processing. This paper analyzes the use of ChatGPT based on
prompt design strategies to assist in CDM processes to extract opinions and
make decisions. We integrate ChatGPT in CDM processes as a flexible tool that
infer the opinions expressed in texts, providing numerical or linguistic
evaluations where the decision making models are based on the prompt design
strategies. We include a multi-criteria decision making scenario with a
category ontology for criteria. We also consider ChatGPT as an end-to-end CDM
model able to provide a general opinion and score on the alternatives. We
conduct empirical experiments on real data extracted from TripAdvisor, the
TripR-2020Large dataset. The analysis of results show a promising branch for
developing quality decision making models using ChatGPT. Finally, we discuss
the challenges of consistency, sensitivity and explainability associated to the
use of LLMs in CDM processes, raising open questions for future studies.
\\ ( https://arxiv.org/abs/2403.15587 ,  1896kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15640
Date: Fri, 22 Mar 2024 22:35:07 GMT   (384kb,D)

Title: Contextual Restless Multi-Armed Bandits with Application to Demand
  Response Decision-Making
Authors: Xin Chen, I-Hong Hou
Categories: cs.AI
\\
  This paper introduces a novel multi-armed bandits framework, termed
Contextual Restless Bandits (CRB), for complex online decision-making. This CRB
framework incorporates the core features of contextual bandits and restless
bandits, so that it can model both the internal state transitions of each arm
and the influence of external global environmental contexts. Using the dual
decomposition method, we develop a scalable index policy algorithm for solving
the CRB problem, and theoretically analyze the asymptotical optimality of this
algorithm. In the case when the arm models are unknown, we further propose a
model-based online learning algorithm based on the index policy to learn the
arm models and make decisions simultaneously. Furthermore, we apply the
proposed CRB framework and the index policy algorithm specifically to the
demand response decision-making problem in smart grids. The numerical
simulations demonstrate the performance and efficiency of our proposed CRB
approaches.
\\ ( https://arxiv.org/abs/2403.15640 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15696
Date: Sat, 23 Mar 2024 03:18:14 GMT   (1217kb,D)

Title: MixRED: A Mix-lingual Relation Extraction Dataset
Authors: Lingxing Kong, Yougang Chu, Zheng Ma, Jianbing Zhang, Liang He, and
  Jiajun Chen
Categories: cs.AI cs.CL
\\
  Relation extraction is a critical task in the field of natural language
processing with numerous real-world applications. Existing research primarily
focuses on monolingual relation extraction or cross-lingual enhancement for
relation extraction. Yet, there remains a significant gap in understanding
relation extraction in the mix-lingual (or code-switching) scenario, where
individuals intermix contents from different languages within sentences,
generating mix-lingual content. Due to the lack of a dedicated dataset, the
effectiveness of existing relation extraction models in such a scenario is
largely unexplored. To address this issue, we introduce a novel task of
considering relation extraction in the mix-lingual scenario called MixRE and
constructing the human-annotated dataset MixRED to support this task. In
addition to constructing the MixRED dataset, we evaluate both state-of-the-art
supervised models and large language models (LLMs) on MixRED, revealing their
respective advantages and limitations in the mix-lingual scenario. Furthermore,
we delve into factors influencing model performance within the MixRE task and
uncover promising directions for enhancing the performance of both supervised
models and LLMs in this novel task.
\\ ( https://arxiv.org/abs/2403.15696 ,  1217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15728
Date: Sat, 23 Mar 2024 05:29:09 GMT   (3727kb,D)

Title: Learnable WSN Deployment of Evidential Collaborative Sensing Model
Authors: Ruijie Liu, Tianxiang Zhan, Zhen Li, Yong Deng
Categories: cs.AI
\\
  In wireless sensor networks (WSNs), coverage and deployment are two most
crucial issues when conducting detection tasks. However, the detection
information collected from sensors is oftentimes not fully utilized and
efficiently integrated. Such sensing model and deployment strategy, thereby,
cannot reach the maximum quality of coverage, particularly when the amount of
sensors within WSNs expands significantly. In this article, we aim at achieving
the optimal coverage quality of WSN deployment. We develop a collaborative
sensing model of sensors to enhance detection capabilities of WSNs, by
leveraging the collaborative information derived from the combination rule
under the framework of evidence theory. In this model, the performance
evaluation of evidential fusion systems is adopted as the criterion of the
sensor selection. A learnable sensor deployment network (LSDNet) considering
both sensor contribution and detection capability, is proposed for achieving
the optimal deployment of WSNs. Moreover, we deeply investigate the algorithm
for finding the requisite minimum number of sensors that realizes the full
coverage of WSNs. A series of numerical examples, along with an application of
forest area monitoring, are employed to demonstrate the effectiveness and the
robustness of the proposed algorithms.
\\ ( https://arxiv.org/abs/2403.15728 ,  3727kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15760
Date: Sat, 23 Mar 2024 08:24:09 GMT   (11876kb,D)

Title: An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side
  Pre-trained Generator to Clients in Heterogeneous Federated Learning
Authors: Jianqing Zhang, Yang Liu, Yang Hua, Jian Cao
Categories: cs.AI cs.DC
Comments: Accepted by CVPR2024
\\
  Heterogeneous Federated Learning (HtFL) enables collaborative learning on
multiple clients with different model architectures while preserving privacy.
Despite recent research progress, knowledge sharing in HtFL is still difficult
due to data and model heterogeneity. To tackle this issue, we leverage the
knowledge stored in pre-trained generators and propose a new upload-efficient
knowledge transfer scheme called Federated Knowledge-Transfer Loop (FedKTL).
Our FedKTL can produce client-task-related prototypical image-vector pairs via
the generator's inference on the server. With these pairs, each client can
transfer pre-existing knowledge from the generator to its local model through
an additional supervised local task. We conduct extensive experiments on four
datasets under two types of data heterogeneity with 14 kinds of models
including CNNs and ViTs. Results show that our upload-efficient FedKTL
surpasses seven state-of-the-art methods by up to 7.31% in accuracy. Moreover,
our knowledge transfer scheme is applicable in scenarios with only one edge
client. Code: https://github.com/TsingZ0/FedKTL
\\ ( https://arxiv.org/abs/2403.15760 ,  11876kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15779
Date: Sat, 23 Mar 2024 09:26:15 GMT   (940kb,D)

Title: The Frontier of Data Erasure: Machine Unlearning for Large Language
  Models
Authors: Youyang Qu, Ming Ding, Nan Sun, Kanchana Thilakarathna, Tianqing Zhu,
  Dusit Niyato
Categories: cs.AI
\\
  Large Language Models (LLMs) are foundational to AI advancements,
facilitating applications like predictive text generation. Nonetheless, they
pose risks by potentially memorizing and disseminating sensitive, biased, or
copyrighted information from their vast datasets. Machine unlearning emerges as
a cutting-edge solution to mitigate these concerns, offering techniques for
LLMs to selectively discard certain data. This paper reviews the latest in
machine unlearning for LLMs, introducing methods for the targeted forgetting of
information to address privacy, ethical, and legal challenges without
necessitating full model retraining. It divides existing research into
unlearning from unstructured/textual data and structured/classification data,
showcasing the effectiveness of these approaches in removing specific data
while maintaining model efficacy. Highlighting the practicality of machine
unlearning, this analysis also points out the hurdles in preserving model
integrity, avoiding excessive or insufficient data removal, and ensuring
consistent outputs, underlining the role of machine unlearning in advancing
responsible, ethical AI.
\\ ( https://arxiv.org/abs/2403.15779 ,  940kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15864
Date: Sat, 23 Mar 2024 15:09:50 GMT   (6286kb,D)

Title: Using Large Language Models for OntoClean-based Ontology Refinement
Authors: Yihang Zhao, Neil Vetter, Kaveh Aryan
Categories: cs.AI
\\
  This paper explores the integration of Large Language Models (LLMs) such as
GPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing
on the OntoClean methodology. OntoClean, critical for assessing the
metaphysical quality of ontologies, involves a two-step process of assigning
meta-properties to classes and verifying a set of constraints. Manually
conducting the first step proves difficult in practice, due to the need for
philosophical expertise and lack of consensus among ontologists. By employing
LLMs with two prompting strategies, the study demonstrates that high accuracy
in the labelling process can be achieved. The findings suggest the potential
for LLMs to enhance ontology refinement, proposing the development of plugin
software for ontology tools to facilitate this integration.
\\ ( https://arxiv.org/abs/2403.15864 ,  6286kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15875
Date: Sat, 23 Mar 2024 15:52:37 GMT   (2106kb,D)

Title: LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series
  classification
Authors: Zhicheng Du, Zhaotian Xie, Yan Tong, Peiwu Qin
Categories: cs.AI cs.CL
Comments: Accepted as tiny paper in ICLR 2024
\\
  This study constructs the LanguAge Model with Prompt EngineeRing (LAMPER)
framework, designed to systematically evaluate the adaptability of pre-trained
language models (PLMs) in accommodating diverse prompts and their integration
in zero-shot time series (TS) classification. We deploy LAMPER in experimental
assessments using 128 univariate TS datasets sourced from the UCR archive. Our
findings indicate that the feature representation capacity of LAMPER is
influenced by the maximum input token threshold imposed by PLMs.
\\ ( https://arxiv.org/abs/2403.15875 ,  2106kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15879
Date: Sat, 23 Mar 2024 16:12:52 GMT   (82kb,D)

Title: TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse
  Unanswerable Questions
Authors: Gyubok Lee, Woosog Chay, Seonhee Cho, Edward Choi
Categories: cs.AI
Comments: Work in Progress
\\
  Recent advances in large language models (LLMs) have led to significant
improvements in translating natural language questions into SQL queries. While
achieving high accuracy in SQL generation is crucial, little is known about the
extent to which these text-to-SQL models can reliably handle diverse types of
questions encountered during real-world deployment, including unanswerable
ones. To explore this aspect, we present TrustSQL, a new benchmark designed to
assess the reliability of text-to-SQL models in both single-database and
cross-database settings. The benchmark tasks models with providing one of two
outcomes: 1) SQL prediction; or 2) abstention from making a prediction, either
when there is a potential error in the generated SQL or when faced with
unanswerable questions. For model evaluation, we explore various modeling
approaches specifically designed for this task. These include: 1) optimizing
separate models for answerability detection, SQL generation, and error
detection, which are then integrated into a single pipeline; and 2) developing
a unified approach that optimizes a single model to address the proposed task.
Experimental results using our new reliability score show that addressing this
challenge involves many different areas of research and opens new avenues for
model development. Nonetheless, none of the methods surpass the reliability
performance of the naive baseline, which abstains from answering all questions.
\\ ( https://arxiv.org/abs/2403.15879 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15901
Date: Sat, 23 Mar 2024 18:04:58 GMT   (8305kb,D)

Title: MatchSeg: Towards Better Segmentation via Reference Image Matching
Authors: Ruiqiang Xiao, Jiayu Huo, Haotian Zheng, Yang Liu, Sebastien Ourselin,
  Rachel Sparks
Categories: cs.AI cs.CV
\\
  Recently, automated medical image segmentation methods based on deep learning
have achieved great success. However, they heavily rely on large annotated
datasets, which are costly and time-consuming to acquire. Few-shot learning
aims to overcome the need for annotated data by using a small labeled dataset,
known as a support set, to guide predicting labels for new, unlabeled images,
known as the query set. Inspired by this paradigm, we introduce MatchSeg, a
novel framework that enhances medical image segmentation through strategic
reference image matching. We leverage contrastive language-image pre-training
(CLIP) to select highly relevant samples when defining the support set.
Additionally, we design a joint attention module to strengthen the interaction
between support and query features, facilitating a more effective knowledge
transfer between support and query sets. We validated our method across four
public datasets. Experimental results demonstrate superior segmentation
performance and powerful domain generalization ability of MatchSeg against
existing methods for domain-specific and cross-domain segmentation tasks. Our
code is made available at https://github.com/keeplearning-again/MatchSeg
\\ ( https://arxiv.org/abs/2403.15901 ,  8305kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15916
Date: Sat, 23 Mar 2024 19:13:01 GMT   (2236kb,D)

Title: Multi-agent transformer-accelerated RL for satisfaction of STL
  specifications
Authors: Albin Larsson Forsberg and Alexandros Nikou and Aneta Vulgarakis
  Feljan and Jana Tumova
Categories: cs.AI
Comments: Submitted to L4DC 2024 conference
\\
  One of the main challenges in multi-agent reinforcement learning is
scalability as the number of agents increases. This issue is further
exacerbated if the problem considered is temporally dependent. State-of-the-art
solutions today mainly follow centralized training with decentralized execution
paradigm in order to handle the scalability concerns. In this paper, we propose
time-dependent multi-agent transformers which can solve the temporally
dependent multi-agent problem efficiently with a centralized approach via the
use of transformers that proficiently handle the large input. We highlight the
efficacy of this method on two problems and use tools from statistics to verify
the probability that the trajectories generated under the policy satisfy the
task. The experiments show that our approach has superior performance against
the literature baseline algorithms in both cases.
\\ ( https://arxiv.org/abs/2403.15916 ,  2236kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15933
Date: Sat, 23 Mar 2024 21:16:56 GMT   (153kb)

Title: Understanding Domain-Size Generalization in Markov Logic Networks
Authors: Florian Chen, Felix Weitk\"amper and Sagar Malhotra
Categories: cs.AI cs.LG
Comments: Under Review. Contact Email: sagar.malhotra@tuwien.ac.at
\\
  We study the generalization behavior of Markov Logic Networks (MLNs) across
relational structures of different sizes. Multiple works have noticed that MLNs
learned on a given domain generalize poorly across domains of different sizes.
This behavior emerges from a lack of internal consistency within an MLN when
used across different domain sizes. In this paper, we quantify this
inconsistency and bound it in terms of the variance of the MLN parameters. The
parameter variance also bounds the KL divergence between an MLN's marginal
distributions taken from different domain sizes. We use these bounds to show
that maximizing the data log-likelihood while simultaneously minimizing the
parameter variance corresponds to two natural notions of generalization across
domain sizes. Our theoretical results apply to Exponential Random Graphs and
other Markov network based relational models. Finally, we observe that
solutions known to decrease the variance of the MLN parameters, like
regularization and Domain-Size Aware MLNs, increase the internal consistency of
the MLNs. We empirically verify our results on four different datasets, with
different methods to control parameter variance, showing that controlling
parameter variance leads to better generalization.
\\ ( https://arxiv.org/abs/2403.15933 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15961
Date: Sat, 23 Mar 2024 23:48:41 GMT   (480kb,D)

Title: SAT Encoding of Partial Ordering Models for Graph Coloring Problems
Authors: Daniel Faber and Adalat Jabrayilov and Petra Mutzel
Categories: cs.AI cs.DM cs.DS cs.LO
\\
  In this paper, we suggest new SAT encodings of the partial-ordering based ILP
model for the graph coloring problem (GCP) and the bandwidth coloring problem
(BCP). The GCP asks for the minimum number of colors that can be assigned to
the vertices of a given graph such that each two adjacent vertices get
different colors. The BCP is a generalization, where each edge has a weight
that enforces a minimal "distance" between the assigned colors, and the goal is
to minimize the "largest" color used. For the widely studied GCP, we
experimentally compare our new SAT encoding to the state-of-the-art approaches
on the DIMACS benchmark set. Our evaluation confirms that this SAT encoding is
effective for sparse graphs and even outperforms the state-of-the-art on some
DIMACS instances. For the BCP, our theoretical analysis shows that the
partial-ordering based SAT and ILP formulations have an asymptotically smaller
size than that of the classical assignment-based model. Our practical
evaluation confirms not only a dominance compared to the assignment-based
encodings but also to the state-of-the-art approaches on a set of benchmark
instances. Up to our knowledge, we have solved several open instances of the
BCP from the literature for the first time.
\\ ( https://arxiv.org/abs/2403.15961 ,  480kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16066
Date: Sun, 24 Mar 2024 08:33:13 GMT   (3123kb,D)

Title: A Temporal Graph Network Framework for Dynamic Recommendation
Authors: Yejin Kim, Youngbin Lee, Vincent Yuan, Annika Lee, Yongjae Lee
Categories: cs.AI
Comments: Presented at the AAAI 2024 Workshop on Recommendation Ecosystems:
  Modeling, Optimization and Incentive Design
\\
  Recommender systems, crucial for user engagement on platforms like e-commerce
and streaming services, often lag behind users' evolving preferences due to
static data reliance. After Temporal Graph Networks (TGNs) were proposed,
various studies have shown that TGN can significantly improve situations where
the features of nodes and edges dynamically change over time. However, despite
its promising capabilities, it has not been directly applied in recommender
systems to date. Our study bridges this gap by directly implementing Temporal
Graph Networks (TGN) in recommender systems, a first in this field. Using
real-world datasets and a range of graph and history embedding methods, we show
TGN's adaptability, confirming its effectiveness in dynamic recommendation
scenarios.
\\ ( https://arxiv.org/abs/2403.16066 ,  3123kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16071
Date: Sun, 24 Mar 2024 09:18:21 GMT   (1153kb,D)

Title: Landmark-Guided Cross-Speaker Lip Reading with Mutual Information
  Regularization
Authors: Linzhi Wu, Xingyu Zhang, Yakun Zhang, Changyan Zheng, Tiejun Liu,
  Liang Xie, Ye Yan and Erwei Yin
Categories: cs.AI cs.CV cs.MM
Comments: To appear in LREC-COLING 2024
Journal-ref: The 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)
\\
  Lip reading, the process of interpreting silent speech from visual lip
movements, has gained rising attention for its wide range of realistic
applications. Deep learning approaches greatly improve current lip reading
systems. However, lip reading in cross-speaker scenarios where the speaker
identity changes, poses a challenging problem due to inter-speaker variability.
A well-trained lip reading system may perform poorly when handling a brand new
speaker. To learn a speaker-robust lip reading model, a key insight is to
reduce visual variations across speakers, avoiding the model overfitting to
specific speakers. In this work, in view of both input visual clues and latent
representations based on a hybrid CTC/attention architecture, we propose to
exploit the lip landmark-guided fine-grained visual clues instead of
frequently-used mouth-cropped images as input features, diminishing
speaker-specific appearance characteristics. Furthermore, a max-min mutual
information regularization approach is proposed to capture speaker-insensitive
latent representations. Experimental evaluations on public lip reading datasets
demonstrate the effectiveness of the proposed approach under the intra-speaker
and inter-speaker conditions.
\\ ( https://arxiv.org/abs/2403.16071 ,  1153kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16097
Date: Sun, 24 Mar 2024 11:27:16 GMT   (435kb,D)

Title: Can Language Models Pretend Solvers? Logic Code Simulation with LLMs
Authors: Minyu Chen, Guoqiang Li, Ling-I Wu, Ruibang Liu, Yuxin Su, Xi Chang,
  Jianxin Xue
Categories: cs.AI cs.LO cs.SE
Comments: 12 pages, 8 figures
\\
  Transformer-based large language models (LLMs) have demonstrated significant
potential in addressing logic problems. capitalizing on the great capabilities
of LLMs for code-related activities, several frameworks leveraging logical
solvers for logic reasoning have been proposed recently. While existing
research predominantly focuses on viewing LLMs as natural language logic
solvers or translators, their roles as logic code interpreters and executors
have received limited attention. This study delves into a novel aspect, namely
logic code simulation, which forces LLMs to emulate logical solvers in
predicting the results of logical programs. To further investigate this novel
task, we formulate our three research questions: Can LLMs efficiently simulate
the outputs of logic codes? What strength arises along with logic code
simulation? And what pitfalls? To address these inquiries, we curate three
novel datasets tailored for the logic code simulation task and undertake
thorough experiments to establish the baseline performance of LLMs in code
simulation. Subsequently, we introduce a pioneering LLM-based code simulation
technique, Dual Chains of Logic (DCoL). This technique advocates a dual-path
thinking approach for LLMs, which has demonstrated state-of-the-art performance
compared to other LLM prompt strategies, achieving a notable improvement in
accuracy by 7.06% with GPT-4-Turbo.
\\ ( https://arxiv.org/abs/2403.16097 ,  435kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16100
Date: Sun, 24 Mar 2024 11:32:43 GMT   (31kb)

Title: Specifying Agent Ethics (Blue Sky Ideas)
Authors: Louise A. Dennis and Michael Fisher
Categories: cs.AI
Comments: To appear in Coordination, Organizations, Institutions, Norms and
  Ethics for Governance of Multi-Agent Systems 2024
\\
  We consider the question of what properties a Machine Ethics system should
have. This question is complicated by the existence of ethical dilemmas with no
agreed upon solution. We provide an example to motivate why we do not believe
falling back on the elicitation of values from stakeholders is sufficient to
guarantee correctness of such systems. We go on to define two broad categories
of ethical property that have arisen in our own work and present a challenge to
the community to approach this question in a more systematic way.
\\ ( https://arxiv.org/abs/2403.16100 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16101
Date: Sun, 24 Mar 2024 11:33:18 GMT   (2056kb,D)

Title: Evaluating Fairness Metrics Across Borders from Human Perceptions
Authors: Yuya Sasaki, Sohei Tokuno, Haruka Maeda, Osamu Sakura
Categories: cs.AI
\\
  Which fairness metrics are appropriately applicable in your contexts? There
may be instances of discordance regarding the perception of fairness, even when
the outcomes comply with established fairness metrics. Several surveys have
been conducted to evaluate fairness metrics with human perceptions of fairness.
However, these surveys were limited in scope, including only a few hundred
participants within a single country. In this study, we conduct an
international survey to evaluate the appropriateness of various fairness
metrics in decision-making scenarios. We collected responses from 1,000
participants in each of China, France, Japan, and the United States, amassing a
total of 4,000 responses, to analyze the preferences of fairness metrics. Our
survey consists of three distinct scenarios paired with four fairness metrics,
and each participant answers their preference for the fairness metric in each
case. This investigation explores the relationship between personal attributes
and the choice of fairness metrics, uncovering a significant influence of
national context on these preferences.
\\ ( https://arxiv.org/abs/2403.16101 ,  2056kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16133
Date: Sun, 24 Mar 2024 13:03:35 GMT   (570kb,D)

Title: SSHPool: The Separated Subgraph-based Hierarchical Pooling
Authors: Zhuo Xu, Lixin Cui, Yue Wang, Hangyuan Du, Lu Bai, Edwin R. Hancock
Categories: cs.AI cs.LG
\\
  In this paper, we develop a novel local graph pooling method, namely the
Separated Subgraph-based Hierarchical Pooling (SSHPool), for graph
classification. To this end, we commence by assigning the nodes of a sample
graph into different clusters, resulting in a family of separated subgraphs. We
individually employ a local graph convolution units as the local structure to
further compress each subgraph into a coarsened node, transforming the original
graph into a coarsened graph. Since these subgraphs are separated by different
clusters and the structural information cannot be propagated between them, the
local convolution operation can significantly avoid the over-smoothing problem
arising in most existing Graph Neural Networks (GNNs). By hierarchically
performing the proposed procedures on the resulting coarsened graph, the
proposed SSHPool can effectively extract the hierarchical global feature of the
original graph structure, encapsulating rich intrinsic structural
characteristics. Furthermore, we develop an end-to-end GNN framework associated
with the proposed SSHPool module for graph classification. Experimental results
demonstrate the superior performance of the proposed model on real-world
datasets, significantly outperforming state-of-the-art GNN methods in terms of
the classification accuracies.
\\ ( https://arxiv.org/abs/2403.16133 ,  570kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16162
Date: Sun, 24 Mar 2024 14:04:40 GMT   (1178kb,D)

Title: Multi-Task Learning with Multi-Task Optimization
Authors: Lu Bai, Abhishek Gupta, and Yew-Soon Ong
Categories: cs.AI
\\
  Multi-task learning solves multiple correlated tasks. However, conflicts may
exist between them. In such circumstances, a single solution can rarely
optimize all the tasks, leading to performance trade-offs. To arrive at a set
of optimized yet well-distributed models that collectively embody different
trade-offs in one algorithmic pass, this paper proposes to view Pareto
multi-task learning through the lens of multi-task optimization. Multi-task
learning is first cast as a multi-objective optimization problem, which is then
decomposed into a diverse set of unconstrained scalar-valued subproblems. These
subproblems are solved jointly using a novel multi-task gradient descent
method, whose uniqueness lies in the iterative transfer of model parameters
among the subproblems during the course of optimization. A theorem proving
faster convergence through the inclusion of such transfers is presented. We
investigate the proposed multi-task learning with multi-task optimization for
solving various problem settings including image classification, scene
understanding, and multi-target regression. Comprehensive experiments confirm
that the proposed method significantly advances the state-of-the-art in
discovering sets of Pareto-optimized models. Notably, on the large image
dataset we tested on, namely NYUv2, the hypervolume convergence achieved by our
method was found to be nearly two times faster than the next-best among the
state-of-the-art.
\\ ( https://arxiv.org/abs/2403.16162 ,  1178kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16190
Date: Sun, 24 Mar 2024 15:14:44 GMT   (239kb)

Title: Logic-based Explanations for Linear Support Vector Classifiers with
  Reject Option
Authors: Francisco Mateus Rocha Filho, Thiago Alves Rocha, Reginaldo Pereira
  Fernandes Ribeiro, Ajalmar R\^ego da Rocha Neto
Categories: cs.AI cs.LG cs.LO
Comments: 16 pages, submitted to BRACIS 2023 (Brazilian Conference on
  Intelligent Systems), accepted version published in Intelligent Systems,
  LNCS, vol 14195
ACM-class: I.2.4; I.2.6
DOI: 10.1007/978-3-031-45368-7_10
\\
  Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model
for linear classification problems. It can be used in conjunction with a reject
option strategy to reject instances that are hard to correctly classify and
delegate them to a specialist. This further increases the confidence of the
model. Given this, obtaining an explanation of the cause of rejection is
important to not blindly trust the obtained results. While most of the related
work has developed means to give such explanations for machine learning models,
to the best of our knowledge none have done so for when reject option is
present. We propose a logic-based approach with formal guarantees on the
correctness and minimality of explanations for linear SVCs with reject option.
We evaluate our approach by comparing it to Anchors, which is a heuristic
algorithm for generating explanations. Obtained results show that our proposed
method gives shorter explanations with reduced time cost.
\\ ( https://arxiv.org/abs/2403.16190 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16206
Date: Sun, 24 Mar 2024 15:59:47 GMT   (835kb)

Title: Rumor Detection with a novel graph neural network approach
Authors: Tianrui Liu, Qi Cai, Changxin Xu, Zhanxin Zhou, Fanghao Ni, Yuxin
  Qiao, and Tsungwei Yang
Categories: cs.AI
Comments: 10 pages, 5 figures
\\
  The wide spread of rumors on social media has caused a negative impact on
people's daily life, leading to potential panic, fear, and mental health
problems for the public. How to debunk rumors as early as possible remains a
challenging problem. Existing studies mainly leverage information propagation
structure to detect rumors, while very few works focus on correlation among
users that they may coordinate to spread rumors in order to gain large
popularity. In this paper, we propose a new detection model, that jointly
learns both the representations of user correlation and information propagation
to detect rumors on social media. Specifically, we leverage graph neural
networks to learn the representations of user correlation from a bipartite
graph that describes the correlations between users and source tweets, and the
representations of information propagation with a tree structure. Then we
combine the learned representations from these two modules to classify the
rumors. Since malicious users intend to subvert our model after deployment, we
further develop a greedy attack scheme to analyze the cost of three adversarial
attacks: graph attack, comment attack, and joint attack. Evaluation results on
two public datasets illustrate that the proposed MODEL outperforms the
state-of-the-art rumor detection models. We also demonstrate our method
performs well for early rumor detection. Moreover, the proposed detection
method is more robust to adversarial attacks compared to the best existing
method. Importantly, we show that it requires a high cost for attackers to
subvert user correlation pattern, demonstrating the importance of considering
user correlation for rumor detection.
\\ ( https://arxiv.org/abs/2403.16206 ,  835kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16222
Date: Sun, 24 Mar 2024 16:30:05 GMT   (31223kb,D)

Title: Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative
  Matrix Factorization
Authors: Ryan Barron, Maksim E. Eren, Manish Bhattarai, Nicholas Solovyev, Kim
  Rasmussen, Boian S. Alexandrov, Charles Nicholas, Cynthia Matuszek
Categories: cs.AI
Comments: Accepted at IEEE ISDFS
\\
  Much of human knowledge in cybersecurity is encapsulated within the
ever-growing volume of scientific papers. As this textual data continues to
expand, the importance of document organization methods becomes increasingly
crucial for extracting actionable insights hidden within large text datasets.
Knowledge Graphs (KGs) serve as a means to store factual information in a
structured manner, providing explicit, interpretable knowledge that includes
domain-specific information from the cybersecurity scientific literature. One
of the challenges in constructing a KG from scientific literature is the
extraction of ontology from unstructured text. In this paper, we address this
topic and introduce a method for building a multi-modal KG by extracting
structured ontology from scientific papers. We demonstrate this concept in the
cybersecurity domain. One modality of the KG represents observable information
from the papers, such as the categories in which they were published or the
authors. The second modality uncovers latent (hidden) patterns of text
extracted through hierarchical and semantic non-negative matrix factorization
(NMF), such as named entities, topics or clusters, and keywords. We illustrate
this concept by consolidating more than two million scientific papers uploaded
to arXiv into the cyber-domain, using hierarchical and semantic NMF, and by
building a cyber-domain-specific KG.
\\ ( https://arxiv.org/abs/2403.16222 ,  31223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16289
Date: Sun, 24 Mar 2024 20:40:51 GMT   (892kb,D)

Title: Engineering Safety Requirements for Autonomous Driving with Large
  Language Models
Authors: Ali Nouri, Beatriz Cabrero-Daniel, Fredrik T\"orner, H\.akan
  Sivencrona, Christian Berger
Categories: cs.AI
Comments: Accepted in 32nd IEEE International Requirements Engineering 2024
  conference, Iceland
\\
  Changes and updates in the requirement artifacts, which can be frequent in
the automotive domain, are a challenge for SafetyOps. Large Language Models
(LLMs), with their impressive natural language understanding and generating
capabilities, can play a key role in automatically refining and decomposing
requirements after each update. In this study, we propose a prototype of a
pipeline of prompts and LLMs that receives an item definition and outputs
solutions in the form of safety requirements. This pipeline also performs a
review of the requirement dataset and identifies redundant or contradictory
requirements. We first identified the necessary characteristics for performing
HARA and then defined tests to assess an LLM's capability in meeting these
criteria. We used design science with multiple iterations and let experts from
different companies evaluate each cycle quantitatively and qualitatively.
Finally, the prototype was implemented at a case company and the responsible
team evaluated its efficiency.
\\ ( https://arxiv.org/abs/2403.16289 ,  892kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16393
Date: Mon, 25 Mar 2024 03:17:27 GMT   (567kb,D)

Title: Concurrent Linguistic Error Detection (CLED) for Large Language Models
Authors: Jinhua Zhu, Javier Conde, Zhen Gao, Pedro Reviriego, Shanshan Liu and
  Fabrizio Lombardi
Categories: cs.AI cs.LG
Comments: 11 pages, 6 figures, 30 references
\\
  The wide adoption of Large language models (LLMs) makes their dependability a
pressing concern. Detection of errors is the first step to mitigating their
impact on a system and thus, efficient error detection for LLMs is an important
issue. In many settings, the LLM is considered as a black box with no access to
the internal nodes; this prevents the use of many error detection schemes that
need access to the model's internal nodes. An interesting observation is that
the output of LLMs in error-free operation should be valid and normal text.
Therefore, when the text is not valid or differs significantly from normal
text, it is likely that there is an error. Based on this observation we propose
to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts
some linguistic features of the text generated by the LLM and feeds them to a
concurrent classifier that detects errors. Since the proposed error detection
mechanism only relies on the outputs of the model, then it can be used on LLMs
in which there is no access to the internal nodes. The proposed CLED scheme has
been evaluated on the T5 model when used for news summarization and on the
OPUS-MT model when used for translation. In both cases, the same set of
linguistic features has been used for error detection to illustrate the
applicability of the proposed scheme beyond a specific case. The results show
that CLED can detect most of the errors at a low overhead penalty. The use of
the concurrent classifier also enables a trade-off between error detection
effectiveness and its associated overhead, so providing flexibility to a
designer.
\\ ( https://arxiv.org/abs/2403.16393 ,  567kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16416
Date: Mon, 25 Mar 2024 04:21:06 GMT   (1642kb,D)

Title: How Reliable is Your Simulator? Analysis on the Limitations of Current
  LLM-based User Simulators for Conversational Recommendation
Authors: Lixi Zhu, Xiaowen Huang, Jitao Sang
Categories: cs.AI
\\
  Conversational Recommender System (CRS) interacts with users through natural
language to understand their preferences and provide personalized
recommendations in real-time. CRS has demonstrated significant potential,
prompting researchers to address the development of more realistic and reliable
user simulators as a key focus. Recently, the capabilities of Large Language
Models (LLMs) have attracted a lot of attention in various fields.
Simultaneously, efforts are underway to construct user simulators based on
LLMs. While these works showcase innovation, they also come with certain
limitations that require attention. In this work, we aim to analyze the
limitations of using LLMs in constructing user simulators for CRS, to guide
future research. To achieve this goal, we conduct analytical validation on the
notable work, iEvaLM. Through multiple experiments on two widely-used datasets
in the field of conversational recommendation, we highlight several issues with
the current evaluation methods for user simulators based on LLMs: (1) Data
leakage, which occurs in conversational history and the user simulator's
replies, results in inflated evaluation results. (2) The success of CRS
recommendations depends more on the availability and quality of conversational
history than on the responses from user simulators. (3) Controlling the output
of the user simulator through a single prompt template proves challenging. To
overcome these limitations, we propose SimpleUserSim, employing a
straightforward strategy to guide the topic toward the target items. Our study
validates the ability of CRS models to utilize the interaction information,
significantly improving the recommendation results.
\\ ( https://arxiv.org/abs/2403.16416 ,  1642kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16424
Date: Mon, 25 Mar 2024 05:04:52 GMT   (193kb)

Title: An Experiment with the Use of ChatGPT for LCSH Subject Assignment on
  Electronic Theses and Dissertations
Authors: Eric H. C. Chow, TJ Kao and Xiaoli Li
Categories: cs.AI cs.DL cs.IR
Comments: 20 pages
\\
  This study delves into the potential use of Large Language Models (LLMs) for
generating Library of Congress Subject Headings (LCSH). The authors employed
ChatGPT to generate subject headings for electronic theses and dissertations
(ETDs) based on their titles and summaries. The results revealed that although
some generated subject headings were valid, there were issues regarding
specificity and exhaustiveness. The study showcases that LLMs can serve as a
strategic response to the backlog of items awaiting cataloging in academic
libraries, while also offering a cost-effective approach for promptly
generating LCSH. Nonetheless, human catalogers remain essential for verifying
and enhancing the validity, exhaustiveness, and specificity of LCSH generated
by LLMs.
\\ ( https://arxiv.org/abs/2403.16424 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16427
Date: Mon, 25 Mar 2024 05:12:18 GMT   (1129kb,D)

Title: Re2LLM: Reflective Reinforcement Large Language Model for Session-based
  Recommendation
Authors: Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya
  Wang, Jie Zhang
Categories: cs.AI
Comments: 11 pages, 4 figures
\\
  Large Language Models (LLMs) are emerging as promising approaches to enhance
session-based recommendation (SBR), where both prompt-based and
fine-tuning-based methods have been widely investigated to align LLMs with SBR.
  However, the former methods struggle with optimal prompts to elicit the
correct reasoning of LLMs due to the lack of task-specific feedback, leading to
unsatisfactory recommendations.
  Although the latter methods attempt to fine-tune LLMs with domain-specific
knowledge, they face limitations such as high computational costs and reliance
on open-source backbones.
  To address such issues, we propose a \underline{Re}flective
\underline{Re}inforcement \underline{L}arge \underline{L}anguage
\underline{M}odel (Re2LLM) for SBR, guiding LLMs to focus on specialized
knowledge essential for more accurate recommendations effectively and
efficiently.
  In particular, we first design the Reflective Exploration Module to
effectively extract knowledge that is readily understandable and digestible by
LLMs.
  To be specific, we direct LLMs to examine recommendation errors through
self-reflection and construct a knowledge base (KB) comprising hints capable of
rectifying these errors.
  To efficiently elicit the correct reasoning of LLMs, we further devise the
Reinforcement Utilization Module to train a lightweight retrieval agent.
  It learns to select hints from the constructed KB based on the task-specific
feedback, where the hints can serve as guidance to help correct LLMs reasoning
for better recommendations. Extensive experiments on multiple real-world
datasets demonstrate that our method consistently outperforms state-of-the-art
methods.
\\ ( https://arxiv.org/abs/2403.16427 ,  1129kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16501
Date: Mon, 25 Mar 2024 07:34:42 GMT   (10321kb,D)

Title: Learning To Guide Human Decision Makers With Vision-Language Models
Authors: Debodeep Banerjee, Stefano Teso, Burcu Sayin Grunel, Andrea Passerini
Categories: cs.AI
\\
  There is increasing interest in developing AIs for assisting human decision
making in \textit{high-stakes} tasks, such as medical diagnosis, for the
purpose of improving decision quality and reducing cognitive strain.
  %
  Mainstream approaches team up an expert with a machine learning model to
which safer decisions are offloaded, thus letting the former focus on cases
that demand their attention.
  %
  This \textit{separation of responsibilities} setup, however, is inadequate
for high-stakes scenarios. On the one hand, the expert may end up over-relying
on the machine's decisions due to \textit{anchoring bias}, thus losing the
human oversight that is increasingly being required by regulatory agencies to
ensure trustworthy AI. On the other hand, the expert is left entirely
unassisted on the (typically hardest) decisions on which the model abstained.
  %
  As a remedy, we introduce \textit{learning to guide} (LTG), an alternative
framework in which -- rather than taking control from the human expert -- the
machine provides \textit{guidance} useful for decision making, and the human is
entirely responsible for coming up with a decision.
  %
  In order to ensure guidance is \textit{interpretable} and
\textit{task-specific}, we develop \method, an approach for turning
\textit{any} vision-language model into a capable generator of textual guidance
by leveraging a modicum of human feedback.
  %
  Our empirical evaluation highlights the promise of \method on a challenging,
real-world medical diagnosis task.
\\ ( https://arxiv.org/abs/2403.16501 ,  10321kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16508
Date: Mon, 25 Mar 2024 07:47:52 GMT   (198kb,D)

Title: Return to Tradition: Learning Reliable Heuristics with Classical Machine
  Learning
Authors: Dillon Z. Chen, Felipe Trevizan, Sylvie Thi\'ebaux
Categories: cs.AI
Comments: Extended version of ICAPS 2024 paper
\\
  Current approaches for learning for planning have yet to achieve competitive
performance against classical planners in several domains, and have poor
overall performance. In this work, we construct novel graph representations of
lifted planning tasks and use the WL algorithm to generate features from them.
These features are used with classical machine learning methods which have up
to 2 orders of magnitude fewer parameters and train up to 3 orders of magnitude
faster than the state-of-the-art deep learning for planning models. Our novel
approach, WL-GOOSE, reliably learns heuristics from scratch and outperforms the
$h^{\text{FF}}$ heuristic in a fair competition setting. It also outperforms or
ties with LAMA on 4 out of 10 domains on coverage and 7 out of 10 domains on
plan quality. WL-GOOSE is the first learning for planning model which achieves
these feats. Furthermore, we study the connections between our novel WL feature
generation method, previous theoretically flavoured learning architectures, and
Description Logic Features for planning.
\\ ( https://arxiv.org/abs/2403.16508 ,  198kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16524
Date: Mon, 25 Mar 2024 08:09:01 GMT   (376kb,D)

Title: Harnessing the power of LLMs for normative reasoning in MASs
Authors: Bastin Tony Roy Savarimuthu, Surangika Ranathunga, Stephen Cranefield
Categories: cs.AI
Comments: 12 pages, 1 figure, accepted to COINE 2024 workshop at AAMAS 2024
  (https://coin-workshop.github.io/coine-2024-auckland/accepted_papers.html)
\\
  Software agents, both human and computational, do not exist in isolation and
often need to collaborate or coordinate with others to achieve their goals. In
human society, social mechanisms such as norms ensure efficient functioning,
and these techniques have been adopted by researchers in multi-agent systems
(MAS) to create socially aware agents. However, traditional techniques have
limitations, such as operating in limited environments often using brittle
symbolic reasoning. The advent of Large Language Models (LLMs) offers a
promising solution, providing a rich and expressive vocabulary for norms and
enabling norm-capable agents that can perform a range of tasks such as norm
discovery, normative reasoning and decision-making. This paper examines the
potential of LLM-based agents to acquire normative capabilities, drawing on
recent Natural Language Processing (NLP) and LLM research. We present our
vision for creating normative LLM agents. In particular, we discuss how the
recently proposed "LLM agent" approaches can be extended to implement such
normative LLM agents. We also highlight challenges in this emerging field. This
paper thus aims to foster collaboration between MAS, NLP and LLM researchers in
order to advance the field of normative agents.
\\ ( https://arxiv.org/abs/2403.16524 ,  376kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16527
Date: Mon, 25 Mar 2024 08:11:02 GMT   (104kb,D)

Title: Hallucination Detection in Foundation Models for Decision-Making: A
  Flexible Definition and Review of the State of the Art
Authors: Neeloy Chakraborty and Melkior Ornik and Katherine Driggs-Campbell
Categories: cs.AI cs.CL cs.RO
Comments: 31 pages, 2 tables
\\
  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to
agricultural field robots, and from health care assistants to the entertainment
industry. The majority of these systems are developed with modular
sub-components for decision-making, planning, and control that may be
hand-engineered or learning-based. While these existing approaches have been
shown to perform well under the situations they were specifically designed for,
they can perform especially poorly in rare, out-of-distribution scenarios that
will undoubtedly arise at test-time. The rise of foundation models trained on
multiple tasks with impressively large datasets from a variety of fields has
led researchers to believe that these models may provide common sense reasoning
that existing planners are missing. Researchers posit that this common sense
reasoning will bridge the gap between algorithm development and deployment to
out-of-distribution tasks, like how humans adapt to unexpected scenarios. Large
language models have already penetrated the robotics and autonomous systems
domains as researchers are scrambling to showcase their potential use cases in
deployment. While this application direction is very promising empirically,
foundation models are known to hallucinate and generate decisions that may
sound reasonable, but are in fact poor. We argue there is a need to step back
and simultaneously design systems that can quantify the certainty of a model's
decision, and detect when it may be hallucinating. In this work, we discuss the
current use cases of foundation models for decision-making tasks, provide a
general definition for hallucinations with examples, discuss existing
approaches to hallucination detection and mitigation with a focus on decision
problems, and explore areas for further research in this exciting field.
\\ ( https://arxiv.org/abs/2403.16527 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16649
Date: Mon, 25 Mar 2024 11:37:15 GMT   (2389kb,D)

Title: CLHA: A Simple yet Effective Contrastive Learning Framework for Human
  Alignment
Authors: Feiteng Fang, Liang Zhu, Min Yang, Xi Feng, Jinchang Hou, Qixuan Zhao,
  Chengming Li, Xiping Hu and Ruifeng Xu
Categories: cs.AI
\\
  Reinforcement learning from human feedback (RLHF) is a crucial technique in
aligning large language models (LLMs) with human preferences, ensuring these
LLMs behave in beneficial and comprehensible ways to users. However, a
longstanding challenge in human alignment techniques based on reinforcement
learning lies in their inherent complexity and difficulty in training. To
address this challenge, we present a simple yet effective Contrastive Learning
Framework for Human Alignment (CLHA) to align LLMs with human preferences
directly. CLHA employs a novel rescoring strategy to evaluate the noise within
the data by considering its inherent quality and dynamically adjusting the
training process. Simultaneously, CLHA utilizes pairwise contrastive loss and
adaptive supervised fine-tuning loss to adaptively modify the likelihood of
generating responses, ensuring enhanced alignment with human preferences. Using
advanced methods, CLHA surpasses other algorithms, showcasing superior
performance in terms of reward model scores, automatic evaluations, and human
assessments on the widely used ``\textit{Helpful and Harmless}'' dataset.
\\ ( https://arxiv.org/abs/2403.16649 ,  2389kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16667
Date: Mon, 25 Mar 2024 12:04:03 GMT   (530kb,D)

Title: Deep Reinforcement Learning and Mean-Variance Strategies for Responsible
  Portfolio Optimization
Authors: Fernando Acero, Parisa Zehtabi, Nicolas Marchesotti, Michael Cashmore,
  Daniele Magazzeni, Manuela Veloso
Categories: cs.AI
Comments: Presented at the AAAI 2024 Workshop on AI in Finance for Social
  Impact
\\
  Portfolio optimization involves determining the optimal allocation of
portfolio assets in order to maximize a given investment objective.
Traditionally, some form of mean-variance optimization is used with the aim of
maximizing returns while minimizing risk, however, more recently, deep
reinforcement learning formulations have been explored. Increasingly, investors
have demonstrated an interest in incorporating ESG objectives when making
investment decisions, and modifications to the classical mean-variance
optimization framework have been developed. In this work, we study the use of
deep reinforcement learning for responsible portfolio optimization, by
incorporating ESG states and objectives, and provide comparisons against
modified mean-variance approaches. Our results show that deep reinforcement
learning policies can provide competitive performance against mean-variance
approaches for responsible portfolio allocation across additive and
multiplicative utility functions of financial and ESG responsibility
objectives.
\\ ( https://arxiv.org/abs/2403.16667 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16728
Date: Mon, 25 Mar 2024 13:02:43 GMT   (6436kb,D)

Title: Improving Diffusion Models's Data-Corruption Resistance using Scheduled
  Pseudo-Huber Loss
Authors: Artem Khrapov, Vadim Popov, Tasnima Sadekova, Assel Yermekova, Mikhail
  Kudinov
Categories: cs.AI
Comments: 13 pages, 16 figures
\\
  Diffusion models are known to be vulnerable to outliers in training data. In
this paper we study an alternative diffusion loss function, which can preserve
the high quality of generated data like the original squared $L_{2}$ loss while
at the same time being robust to outliers. We propose to use pseudo-Huber loss
function with a time-dependent parameter to allow for the trade-off between
robustness on the most vulnerable early reverse-diffusion steps and fine
details restoration on the final steps. We show that pseudo-Huber loss with the
time-dependent parameter exhibits better performance on corrupted datasets in
both image and audio domains. In addition, the loss function we propose can
potentially help diffusion models to resist dataset corruption while not
requiring data filtering or purification compared to conventional training
algorithms.
\\ ( https://arxiv.org/abs/2403.16728 ,  6436kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16732
Date: Mon, 25 Mar 2024 13:06:31 GMT   (25480kb,D)

Title: Enabling Uncertainty Estimation in Iterative Neural Networks
Authors: Nikita Durasov, Doruk Oner, Jonathan Donier, Hieu Le, Pascal Fua
Categories: cs.AI
\\
  Turning pass-through network architectures into iterative ones, which use
their own output as input, is a well-known approach for boosting performance.
In this paper, we argue that such architectures offer an additional benefit:
The convergence rate of their successive outputs is highly correlated with the
accuracy of the value to which they converge. Thus, we can use the convergence
rate as a useful proxy for uncertainty. This results in an approach to
uncertainty estimation that provides state-of-the-art estimates at a much lower
computational cost than techniques like Ensembles, and without requiring any
modifications to the original iterative model. We demonstrate its practical
value by embedding it in two application domains: road detection in aerial
images and the estimation of aerodynamic properties of 2D and 3D shapes.
\\ ( https://arxiv.org/abs/2403.16732 ,  25480kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16750
Date: Mon, 25 Mar 2024 13:23:24 GMT   (268kb,D)

Title: All Artificial, Less Intelligence: GenAI through the Lens of Formal
  Verification
Authors: Deepak Narayan Gadde, Aman Kumar, Thomas Nalapat, Evgenii Rezunov and
  Fabio Cappellini
Categories: cs.AI
Comments: Published in DVCon U.S. 2024
\\
  Modern hardware designs have grown increasingly efficient and complex.
However, they are often susceptible to Common Weakness Enumerations (CWEs).
This paper is focused on the formal verification of CWEs in a dataset of
hardware designs written in SystemVerilog from Regenerative Artificial
Intelligence (AI) powered by Large Language Models (LLMs). We applied formal
verification to categorize each hardware design as vulnerable or CWE-free. This
dataset was generated by 4 different LLMs and features a unique set of designs
for each of the 10 CWEs we target in our paper. We have associated the
identified vulnerabilities with CWE numbers for a dataset of 60,000 generated
SystemVerilog Register Transfer Level (RTL) code. It was also found that most
LLMs are not aware of any hardware CWEs; hence they are usually not considered
when generating the hardware code. Our study reveals that approximately 60% of
the hardware designs generated by LLMs are prone to CWEs, posing potential
safety and security risks. The dataset could be ideal for training LLMs and
Machine Learning (ML) algorithms to abstain from generating CWE-prone hardware
designs.
\\ ( https://arxiv.org/abs/2403.16750 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16808
Date: Mon, 25 Mar 2024 14:32:18 GMT   (216kb,D)

Title: Navigating the EU AI Act: A Methodological Approach to Compliance for
  Safety-critical Products
Authors: J. Kelly, S. Ali Zafar, L. Heidemann, J. Zacchi, D. Espinoza, N. Mata
Categories: cs.AI
Comments: To be published in: 2024 IEEE Conference on Artificial Intelligence
  (CAI 2024)
\\
  In December 2023, the European Parliament provisionally agreed on the EU AI
Act. This unprecedented regulatory framework for AI systems lays out guidelines
to ensure the safety, legality, and trustworthiness of AI products. This paper
presents a methodology for interpreting the EU AI Act requirements for
high-risk AI systems by leveraging product quality models. We first propose an
extended product quality model for AI systems, incorporating attributes
relevant to the Act not covered by current quality models. We map the Act
requirements to relevant quality attributes with the goal of refining them into
measurable characteristics. We then propose a contract-based approach to derive
technical requirements at the stakeholder level. This facilitates the
development and assessment of AI systems that not only adhere to established
quality standards, but also comply with the regulatory requirements outlined in
the Act for high-risk (including safety-critical) AI systems. We demonstrate
the applicability of this methodology on an exemplary automotive supply chain
use case, where several stakeholders interact to achieve EU AI Act compliance.
\\ ( https://arxiv.org/abs/2403.16808 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16824
Date: Mon, 25 Mar 2024 14:48:54 GMT   (264kb)

Title: On Policy Reuse: An Expressive Language for Representing and Executing
  General Policies that Call Other Policies
Authors: Blai Bonet, Dominik Drexler, Hector Geffner
Categories: cs.AI
Comments: ICAPS 2024
\\
  Recently, a simple but powerful language for expressing and learning general
policies and problem decompositions (sketches) has been introduced in terms of
rules defined over a set of Boolean and numerical features. In this work, we
consider three extensions of this language aimed at making policies and
sketches more flexible and reusable: internal memory states, as in finite state
controllers; indexical features, whose values are a function of the state and a
number of internal registers that can be loaded with objects; and modules that
wrap up policies and sketches and allow them to call each other by passing
parameters. In addition, unlike general policies that select state transitions
rather than ground actions, the new language allows for the selection of such
actions. The expressive power of the resulting language for policies and
sketches is illustrated through a number of examples.
\\ ( https://arxiv.org/abs/2403.16824 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16858
Date: Mon, 25 Mar 2024 15:22:06 GMT   (12732kb,D)

Title: XAIport: A Service Framework for the Early Adoption of XAI in AI Model
  Development
Authors: Zerui Wang, Yan Liu, Abishek Arumugam Thiruselvi, Abdelwahab
  Hamou-Lhadj
Categories: cs.AI
Comments: Accepted at the ICSE'24 conference, NIER track
DOI: 10.1145/3639476.3639759
\\
  In this study, we propose the early adoption of Explainable AI (XAI) with a
focus on three properties: Quality of explanation, the explanation summaries
should be consistent across multiple XAI methods; Architectural Compatibility,
for effective integration in XAI, the architecture styles of both the XAI
methods and the models to be explained must be compatible with the framework;
Configurable operations, XAI explanations are operable, akin to machine
learning operations. Thus, an explanation for AI models should be reproducible
and tractable to be trustworthy. We present XAIport, a framework of XAI
microservices encapsulated into Open APIs to deliver early explanations as
observation for learning model quality assurance. XAIport enables configurable
XAI operations along with machine learning development. We quantify the
operational costs of incorporating XAI with three cloud computer vision
services on Microsoft Azure Cognitive Services, Google Cloud Vertex AI, and
Amazon Rekognition. Our findings show comparable operational costs between XAI
and traditional machine learning, with XAIport significantly improving both
cloud AI model performance and explanation stability.
\\ ( https://arxiv.org/abs/2403.16858 ,  12732kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16904
Date: Mon, 25 Mar 2024 16:14:45 GMT   (504kb,D)

Title: Multi-Agent Optimization for Safety Analysis of Cyber-Physical Systems:
  Position Paper
Authors: \"Onder G\"urcan, Nataliya Yakymets, Sara Tucci-Piergiovanni, Ansgar
  Radermacher
Categories: cs.AI cs.CR
Comments: 13 pages, 2 figures, 1 table, "2nd International Workshop on Emerging
  Ideas and Trends in Engineering of Cyber-Physical Systems, part of
  Cyber-Physical Systems Week, April 2015, Seattle, USA"
\\
  Failure Mode, Effects and Criticality Analysis (FMECA) is one of the safety
analysis methods recommended by most of the international standards. The
classical FMECA is made in a form of a table filled in either manually or by
using safety analysis tools. In both cases, the design engineers have to choose
the trade-offs between safety and other development constraints. In the case of
complex cyber-physical systems (CPS) with thousands of specified constraints,
this may lead to severe problems and significantly impact the overall
criticality of CPS. In this paper, we propose to adopt optimization techniques
to automate the decision making process conducted after FMECA of CPS. We
describe a multi-agent based optimization method which extends classical FMECA
for offering optimal solutions in terms of criticality and development
constraints of CPS.
\\ ( https://arxiv.org/abs/2403.16904 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16908
Date: Mon, 25 Mar 2024 16:19:33 GMT   (4270kb,D)

Title: Towards Trustworthy Automated Driving through Qualitative Scene
  Understanding and Explanations
Authors: Nassim Belmecheri, Arnaud Gotlieb, Nadjib Lazaar, Helge Spieker
Categories: cs.AI
Comments: SAE International Journal of Connected and Automated Vehicles
\\
  Understanding driving scenes and communicating automated vehicle decisions
are key requirements for trustworthy automated driving. In this article, we
introduce the Qualitative Explainable Graph (QXG), which is a unified symbolic
and qualitative representation for scene understanding in urban mobility. The
QXG enables interpreting an automated vehicle's environment using sensor data
and machine learning models. It utilizes spatio-temporal graphs and qualitative
constraints to extract scene semantics from raw sensor inputs, such as LiDAR
and camera data, offering an interpretable scene model. A QXG can be
incrementally constructed in real-time, making it a versatile tool for
in-vehicle explanations across various sensor types. Our research showcases the
potential of QXG, particularly in the context of automated driving, where it
can rationalize decisions by linking the graph with observed actions. These
explanations can serve diverse purposes, from informing passengers and alerting
vulnerable road users to enabling post-hoc analysis of prior behaviors.
\\ ( https://arxiv.org/abs/2403.16908 ,  4270kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16909
Date: Mon, 25 Mar 2024 16:21:25 GMT   (1033kb,D)

Title: Towards Algorithmic Fidelity: Mental Health Representation across
  Demographics in Synthetic vs. Human-generated Data
Authors: Shinka Mori, Oana Ignat, Andrew Lee, Rada Mihalcea
Categories: cs.AI cs.CL cs.CY
Comments: 14 pages, 16 figures
\\
  Synthetic data generation has the potential to impact applications and
domains with scarce data. However, before such data is used for sensitive tasks
such as mental health, we need an understanding of how different demographics
are represented in it. In our paper, we analyze the potential of producing
synthetic data using GPT-3 by exploring the various stressors it attributes to
different race and gender combinations, to provide insight for future
researchers looking into using LLMs for data generation. Using GPT-3, we
develop HEADROOM, a synthetic dataset of 3,120 posts about
depression-triggering stressors, by controlling for race, gender, and time
frame (before and after COVID-19). Using this dataset, we conduct semantic and
lexical analyses to (1) identify the predominant stressors for each demographic
group; and (2) compare our synthetic data to a human-generated dataset. We
present the procedures to generate queries to develop depression data using
GPT-3, and conduct analyzes to uncover the types of stressors it assigns to
demographic groups, which could be used to test the limitations of LLMs for
synthetic data generation for depression data. Our findings show that synthetic
data mimics some of the human-generated data distribution for the predominant
depression stressors across diverse demographics.
\\ ( https://arxiv.org/abs/2403.16909 ,  1033kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16984
Date: Mon, 25 Mar 2024 17:44:45 GMT   (7719kb,D)

Title: Modelling Commonsense Commonalities with Multi-Facet Concept Embeddings
Authors: Hanane Kteich, Na Li, Usashi Chatterjee, Zied Bouraoui, Steven
  Schockaert
Categories: cs.AI cs.CL
\\
  Concept embeddings offer a practical and efficient mechanism for injecting
commonsense knowledge into downstream tasks. Their core purpose is often not to
predict the commonsense properties of concepts themselves, but rather to
identify commonalities, i.e.\ sets of concepts which share some property of
interest. Such commonalities are the basis for inductive generalisation, hence
high-quality concept embeddings can make learning easier and more robust.
Unfortunately, standard embeddings primarily reflect basic taxonomic
categories, making them unsuitable for finding commonalities that refer to more
specific aspects (e.g.\ the colour of objects or the materials they are made
of). In this paper, we address this limitation by explicitly modelling the
different facets of interest when learning concept embeddings. We show that
this leads to embeddings which capture a more diverse range of commonsense
properties, and consistently improves results in downstream tasks such as
ultra-fine entity typing and ontology completion.
\\ ( https://arxiv.org/abs/2403.16984 ,  7719kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15391
Date: Wed, 7 Feb 2024 13:41:22 GMT   (788kb)

Title: CapsF: Capsule Fusion for Extracting psychiatric stressors for suicide
  from twitter
Authors: Mohammad Ali Dadgostarnia, Ramin Mousa, Saba Hesaraki
Categories: cs.CL cs.SI
\\
  Along with factors such as cancer, blood pressure, street accidents and
stroke, suicide has been one of Iran main causes of death. One of the main
reasons for suicide is psychological stressors. Identifying psychological
stressors in an at risk population can help in the early prevention of suicidal
and suicidal behaviours. In recent years, the widespread popularity and flow of
real time information sharing of social media have allowed for potential early
intervention in large scale and even small scale populations. However, some
automated approaches to extract psychiatric stressors from Twitter have been
presented, but most of this research has been for non Persian languages. This
study aims to investigate the techniques of detecting psychological stress
related to suicide from Persian tweets using learning based methods. The
proposed capsule based approach achieved a binary classification accuracy of
0.83.
\\ ( https://arxiv.org/abs/2403.15391 ,  788kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15393
Date: Fri, 9 Feb 2024 22:12:20 GMT   (802kb,D)

Title: Detection of Opioid Users from Reddit Posts via an Attention-based
  Bidirectional Recurrent Neural Network
Authors: Yuchen Wang, Zhengyu Fang, Wei Du, Shuai Xu, Rong Xu, and Jing Li
Categories: cs.CL cs.LG cs.SI
\\
  The opioid epidemic, referring to the growing hospitalizations and deaths
because of overdose of opioid usage and addiction, has become a severe health
problem in the United States. Many strategies have been developed by the
federal and local governments and health communities to combat this crisis.
Among them, improving our understanding of the epidemic through better health
surveillance is one of the top priorities. In addition to direct testing,
machine learning approaches may also allow us to detect opioid users by
analyzing data from social media because many opioid users may choose not to do
the tests but may share their experiences on social media anonymously. In this
paper, we take advantage of recent advances in machine learning, collect and
analyze user posts from a popular social network Reddit with the goal to
identify opioid users. Posts from more than 1,000 users who have posted on
three sub-reddits over a period of one month have been collected. In addition
to the ones that contain keywords such as opioid, opiate, or heroin, we have
also collected posts that contain slang words of opioid such as black or
chocolate. We apply an attention-based bidirectional long short memory model to
identify opioid users. Experimental results show that the approaches
significantly outperform competitive algorithms in terms of F1-score.
Furthermore, the model allows us to extract most informative words, such as
opiate, opioid, and black, from posts via the attention layer, which provides
more insights on how the machine learning algorithm works in distinguishing
drug users from non-drug users.
\\ ( https://arxiv.org/abs/2403.15393 ,  802kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15407
Date: Thu, 29 Feb 2024 05:16:19 GMT   (7508kb,D)

Title: X-AMR Annotation Tool
Authors: Shafiuddin Rehan Ahmed, Jon Z. Cai, Martha Palmer, James H. Martin
Categories: cs.CL cs.AI
Comments: EACL 2024 System Demonstration
\\
  This paper presents a novel Cross-document Abstract Meaning Representation
(X-AMR) annotation tool designed for annotating key corpus-level event
semantics. Leveraging machine assistance through the Prodigy Annotation Tool,
we enhance the user experience, ensuring ease and efficiency in the annotation
process. Through empirical analyses, we demonstrate the effectiveness of our
tool in augmenting an existing event corpus, highlighting its advantages when
integrated with GPT-4. Code and annotations:
https://github.com/ahmeshaf/gpt_coref
\\ ( https://arxiv.org/abs/2403.15407 ,  7508kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15430
Date: Wed, 13 Mar 2024 15:38:55 GMT   (3575kb,D)

Title: Distilling Named Entity Recognition Models for Endangered Species from
  Large Language Models
Authors: Jesse Atuhurra, Seiveright Cargill Dujohn, Hidetaka Kamigaito,
  Hiroyuki Shindo, Taro Watanabe
Categories: cs.CL
\\
  Natural language processing (NLP) practitioners are leveraging large language
models (LLM) to create structured datasets from semi-structured and
unstructured data sources such as patents, papers, and theses, without having
domain-specific knowledge. At the same time, ecological experts are searching
for a variety of means to preserve biodiversity. To contribute to these
efforts, we focused on endangered species and through in-context learning, we
distilled knowledge from GPT-4. In effect, we created datasets for both named
entity recognition (NER) and relation extraction (RE) via a two-stage process:
1) we generated synthetic data from GPT-4 of four classes of endangered
species, 2) humans verified the factual accuracy of the synthetic data,
resulting in gold data. Eventually, our novel dataset contains a total of 3.6K
sentences, evenly divided between 1.8K NER and 1.8K RE sentences. The
constructed dataset was then used to fine-tune both general BERT and
domain-specific BERT variants, completing the knowledge distillation process
from GPT-4 to BERT, because GPT-4 is resource intensive. Experiments show that
our knowledge transfer approach is effective at creating a NER model suitable
for detecting endangered species from texts.
\\ ( https://arxiv.org/abs/2403.15430 ,  3575kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15434
Date: Fri, 15 Mar 2024 09:15:22 GMT   (3239kb,D)

Title: ChatPattern: Layout Pattern Customization via Natural Language
Authors: Zixiao Wang, Yunheng Shen, Xufeng Yao, Wenqian Zhao, Yang Bai, Farzan
  Farnia, Bei Yu
Categories: cs.CL cs.AI
Comments: Accepted by DAC24
\\
  Existing works focus on fixed-size layout pattern generation, while the more
practical free-size pattern generation receives limited attention. In this
paper, we propose ChatPattern, a novel Large-Language-Model (LLM) powered
framework for flexible pattern customization. ChatPattern utilizes a two-part
system featuring an expert LLM agent and a highly controllable layout pattern
generator. The LLM agent can interpret natural language requirements and
operate design tools to meet specified needs, while the generator excels in
conditional layout generation, pattern modification, and memory-friendly
patterns extension. Experiments on challenging pattern generation setting shows
the ability of ChatPattern to synthesize high-quality large-scale patterns.
\\ ( https://arxiv.org/abs/2403.15434 ,  3239kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15436
Date: Fri, 15 Mar 2024 20:12:32 GMT   (31kb)

Title: Using Contextual Information for Sentence-level Morpheme Segmentation
Authors: Prabin Bhandari and Abhishek Paudel
Categories: cs.CL
Comments: 5 pages, 3 tables
\\
  Recent advancements in morpheme segmentation primarily emphasize word-level
segmentation, often neglecting the contextual relevance within the sentence. In
this study, we redefine the morpheme segmentation task as a
sequence-to-sequence problem, treating the entire sentence as input rather than
isolating individual words. Our findings reveal that the multilingual model
consistently exhibits superior performance compared to monolingual
counterparts. While our model did not surpass the performance of the current
state-of-the-art, it demonstrated comparable efficacy with high-resource
languages while revealing limitations in low-resource language scenarios.
\\ ( https://arxiv.org/abs/2403.15436 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15440
Date: Sat, 16 Mar 2024 23:10:42 GMT   (1712kb,D)

Title: Linguistics from a topological viewpoint
Authors: Rui Dong
Categories: cs.CL
Comments: 14 pages, 17 figures
MSC-class: 91F20, 62R40, 55N31, 55U10
\\
  Typological databases in linguistics are usually categorical-valued. As a
result, it is difficult to have a clear visualization of the data. In this
paper, we describe a workflow to analyze the topological shapes of South
American languages by applying multiple correspondence analysis technique and
topological data analysis methods.
\\ ( https://arxiv.org/abs/2403.15440 ,  1712kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15445
Date: Mon, 18 Mar 2024 00:01:10 GMT   (2880kb)

Title: Decoding Multilingual Topic Dynamics and Trend Identification through
  ARIMA Time Series Analysis on Social Networks: A Novel Data Translation
  Framework Enhanced by LDA/HDP Models
Authors: Samawel Jaballi, Azer Mahjoubi, Manar Joundy Hazar, Salah Zrigui,
  Henri Nicolas, Mounir Zrigui
Categories: cs.CL cs.AI cs.LG cs.SI
\\
  In this study, the authors present a novel methodology adept at decoding
multilingual topic dynamics and identifying communication trends during crises.
We focus on dialogues within Tunisian social networks during the Coronavirus
Pandemic and other notable themes like sports and politics. We start by
aggregating a varied multilingual corpus of comments relevant to these
subjects. This dataset undergoes rigorous refinement during data preprocessing.
We then introduce our No-English-to-English Machine Translation approach to
handle linguistic differences. Empirical tests of this method showed high
accuracy and F1 scores, highlighting its suitability for linguistically
coherent tasks. Delving deeper, advanced modeling techniques, specifically LDA
and HDP models are employed to extract pertinent topics from the translated
content. This leads to applying ARIMA time series analysis to decode evolving
topic trends. Applying our method to a multilingual Tunisian dataset, we
effectively identified key topics mirroring public sentiment. Such insights
prove vital for organizations and governments striving to understand public
perspectives during crises. Compared to standard approaches, our model
outperforms, as confirmed by metrics like Coherence Score, U-mass, and Topic
Coherence. Additionally, an in-depth assessment of the identified topics
revealed notable thematic shifts in discussions, with our trends identification
indicating impressive accuracy, backed by RMSE-based analysis.
\\ ( https://arxiv.org/abs/2403.15445 ,  2880kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15447
Date: Mon, 18 Mar 2024 01:38:19 GMT   (1676kb,D)

Title: Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient
  LLMs Under Compression
Authors: Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie,
  Kelsey Lieberman, James Diffenderfer, Brian Bartoldson, Ajay Jaiswal, Kaidi
  Xu, Bhavya Kailkhura, Dan Hendrycks, Dawn Song, Zhangyang Wang, Bo Li
Categories: cs.CL cs.AI
Comments: Preprint under work
\\
  Compressing high-capability Large Language Models (LLMs) has emerged as a
favored strategy for resource-efficient inferences. While state-of-the-art
(SoTA) compression methods boast impressive advancements in preserving benign
task performance, the potential risks of compression in terms of safety and
trustworthiness have been largely neglected. This study conducts the first,
thorough evaluation of three (3) leading LLMs using five (5) SoTA compression
techniques across eight (8) trustworthiness dimensions. Our experiments
highlight the intricate interplay between compression and trustworthiness,
revealing some interesting patterns. We find that quantization is currently a
more effective approach than pruning in achieving efficiency and
trustworthiness simultaneously. For instance, a 4-bit quantized model retains
the trustworthiness of its original counterpart, but model pruning
significantly degrades trustworthiness, even at 50% sparsity. Moreover,
employing quantization within a moderate bit range could unexpectedly improve
certain trustworthiness dimensions such as ethics and fairness. Conversely,
extreme quantization to very low bit levels (3 bits) tends to significantly
reduce trustworthiness. This increased risk cannot be uncovered by looking at
benign performance alone, in turn, mandating comprehensive trustworthiness
evaluation in practice. These findings culminate in practical recommendations
for simultaneously achieving high utility, efficiency, and trustworthiness in
LLMs. Models and code are available at https://decoding-comp-trust.github.io/.
\\ ( https://arxiv.org/abs/2403.15447 ,  1676kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15449
Date: Mon, 18 Mar 2024 07:20:35 GMT   (5984kb,D)

Title: Hatred Stems from Ignorance! Distillation of the Persuasion Modes in
  Countering Conversational Hate Speech
Authors: Ghadi Alyahya, Abeer Aldayel
Categories: cs.CL cs.AI
\\
  Examining the factors that the counter-speech uses is at the core of
understanding the optimal methods for confronting hate speech online. Various
studies assess the emotional base factor used in counter speech, such as
emotion-empathy, offensiveness, and level of hostility. To better understand
the counter-speech used in conversational interactions, this study distills
persuasion modes into reason, emotion, and credibility and then evaluates their
use in two types of conversation interactions: closed (multi-turn) and open
(single-turn) conversation interactions concerning racism, sexism, and
religion. The evaluation covers the distinct behaviors of human versus
generated counter-speech. We also assess the interplay between the replies'
stance and each mode of persuasion in the counter-speech. Notably, we observe
nuanced differences in the counter-speech persuasion modes for open and closed
interactions -- especially on the topic level -- with a general tendency to use
reason as a persuasion mode to express the counterpoint to hate comments. The
generated counter-speech tends to exhibit an emotional persuasion mode, while
human counters lean towards using reasoning. Furthermore, our study shows that
reason as a persuasion mode tends to obtain more supportive replies than do
other persuasion types. The findings highlight the potential of incorporating
persuasion modes into studies about countering hate speech, as these modes can
serve as an optimal means of explainability and paves the way for the further
adoption of the reply's stance and the role it plays in assessing what
comprises the optimal counter-speech.
\\ ( https://arxiv.org/abs/2403.15449 ,  5984kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15450
Date: Mon, 18 Mar 2024 15:19:17 GMT   (165kb,D)

Title: Loops On Retrieval Augmented Generation (LoRAG)
Authors: Ayush Thakur and Rashmi Vashisth
Categories: cs.CL cs.IR
\\
  This paper presents Loops On Retrieval Augmented Generation (LoRAG), a new
framework designed to enhance the quality of retrieval-augmented text
generation through the incorporation of an iterative loop mechanism. The
architecture integrates a generative model, a retrieval mechanism, and a
dynamic loop module, allowing for iterative refinement of the generated text
through interactions with relevant information retrieved from the input
context. Experimental evaluations on benchmark datasets demonstrate that LoRAG
surpasses existing state-of-the-art models in terms of BLEU score, ROUGE score,
and perplexity, showcasing its effectiveness in achieving both coherence and
relevance in generated text. The qualitative assessment further illustrates
LoRAG's capability to produce contextually rich and coherent outputs. This
research contributes valuable insights into the potential of iterative loops in
mitigating challenges in text generation, positioning LoRAG as a promising
advancement in the field.
\\ ( https://arxiv.org/abs/2403.15450 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15451
Date: Mon, 18 Mar 2024 16:46:00 GMT   (34kb,D)

Title: Towards Enabling FAIR Dataspaces Using Large Language Models
Authors: Benedikt T. Arnold, Johannes Theissen-Lipp, Diego Collarana, Christoph
  Lange, Sandra Geisler, Edward Curry, Stefan Decker
Categories: cs.CL
Comments: 8 pages. Preprint. Under review
\\
  Dataspaces have recently gained adoption across various sectors, including
traditionally less digitized domains such as culture. Leveraging Semantic Web
technologies helps to make dataspaces FAIR, but their complexity poses a
significant challenge to the adoption of dataspaces and increases their cost.
The advent of Large Language Models (LLMs) raises the question of how these
models can support the adoption of FAIR dataspaces. In this work, we
demonstrate the potential of LLMs in dataspaces with a concrete example. We
also derive a research agenda for exploring this emerging field.
\\ ( https://arxiv.org/abs/2403.15451 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15452
Date: Mon, 18 Mar 2024 17:20:07 GMT   (1245kb,D)

Title: What Are Tools Anyway? A Survey from the Language Model Perspective
Authors: Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, Graham Neubig
Categories: cs.CL cs.AI
\\
  Language models (LMs) are powerful yet mostly for text generation tasks.
Tools have substantially enhanced their performance for tasks that require
complex skills. However, many works adopt the term "tool" in different ways,
raising the question: What is a tool anyway? Subsequently, where and how do
tools help LMs? In this survey, we provide a unified definition of tools as
external programs used by LMs, and perform a systematic review of LM tooling
scenarios and approaches. Grounded on this review, we empirically study the
efficiency of various tooling methods by measuring their required compute and
performance gains on various benchmarks, and highlight some challenges and
potential future research in the field.
\\ ( https://arxiv.org/abs/2403.15452 ,  1245kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15453
Date: Mon, 18 Mar 2024 20:10:44 GMT   (2405kb,D)

Title: Span-Oriented Information Extraction -- A Unifying Perspective on
  Information Extraction
Authors: Yifan Ding, Michael Yankoski, Tim Weninger
Categories: cs.CL cs.AI
Comments: 35 Pages, 1 Figure
\\
  Information Extraction refers to a collection of tasks within Natural
Language Processing (NLP) that identifies sub-sequences within text and their
labels. These tasks have been used for many years to link extract relevant
information and to link free text to structured data. However, the
heterogeneity among information extraction tasks impedes progress in this area.
We therefore offer a unifying perspective centered on what we define to be
spans in text. We then re-orient these seemingly incongruous tasks into this
unified perspective and then re-present the wide assortment of information
extraction tasks as variants of the same basic Span-Oriented Information
Extraction task.
\\ ( https://arxiv.org/abs/2403.15453 ,  2405kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15454
Date: Mon, 18 Mar 2024 23:22:50 GMT   (568kb)

Title: Emotion Detection with Transformers: A Comparative Study
Authors: Mahdi Rezapour
Categories: cs.CL stat.AP
\\
  In this study, we explore the application of transformer-based models for
emotion classification on text data. We train and evaluate several pre-trained
transformer models, on the Emotion dataset using different variants of
transformers. The paper also analyzes some factors that in-fluence the
performance of the model, such as the fine-tuning of the transformer layer, the
trainability of the layer, and the preprocessing of the text data. Our analysis
reveals that commonly applied techniques like removing punctuation and stop
words can hinder model performance. This might be because transformers strength
lies in understanding contextual relationships within text. Elements like
punctuation and stop words can still convey sentiment or emphasis and removing
them might disrupt this context.
\\ ( https://arxiv.org/abs/2403.15454 ,  568kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15455
Date: Mon, 18 Mar 2024 23:41:52 GMT   (1813kb,D)

Title: Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams
Authors: Cristiano Mesquita Garcia, Alessandro Lameiras Koerich, Alceu de Souza
  Britto Jr and Jean Paul Barddal
Categories: cs.CL cs.LG
\\
  The proliferation of textual data on the Internet presents a unique
opportunity for institutions and companies to monitor public opinion about
their services and products. Given the rapid generation of such data, the text
stream mining setting, which handles sequentially arriving, potentially
infinite text streams, is often more suitable than traditional batch learning.
While pre-trained language models are commonly employed for their high-quality
text vectorization capabilities in streaming contexts, they face challenges
adapting to concept drift - the phenomenon where the data distribution changes
over time, adversely affecting model performance. Addressing the issue of
concept drift, this study explores the efficacy of seven text sampling methods
designed to selectively fine-tune language models, thereby mitigating
performance degradation. We precisely assess the impact of these methods on
fine-tuning the SBERT model using four different loss functions. Our
evaluation, focused on Macro F1-score and elapsed time, employs two text stream
datasets and an incremental SVM classifier to benchmark performance. Our
findings indicate that Softmax loss and Batch All Triplets loss are
particularly effective for text stream classification, demonstrating that
larger sample sizes generally correlate with improved macro F1-scores. Notably,
our proposed WordPieceToken ratio sampling method significantly enhances
performance with the identified loss functions, surpassing baseline results.
\\ ( https://arxiv.org/abs/2403.15455 ,  1813kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15458
Date: Tue, 19 Mar 2024 11:36:53 GMT   (381kb)

Title: Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks
Authors: Daniel Fesalbon, Arvin De La Cruz, Marvin Mallari, and Nelson Rodelas
Categories: cs.CL cs.LG
Journal-ref: IJFMR Volume 6, Issue 2, March-April 2024
DOI: 10.36948/ijfmr.2024.v06i02.14927
\\
  Common problems in playing online mobile and computer games were related to
toxic behavior and abusive communication among players. Based on different
reports and studies, the study also discusses the impact of online hate speech
and toxicity on players' in-game performance and overall well-being. This study
investigates the capability of pre-trained language models to classify or
detect trash talk or toxic in-game messages The study employs and evaluates the
performance of pre-trained BERT and GPT language models in detecting toxicity
within in-game chats. Using publicly available APIs, in-game chat data from
DOTA 2 game matches were collected, processed, reviewed, and labeled as
non-toxic, mild (toxicity), and toxic. The study was able to collect around two
thousand in-game chats to train and test BERT (Base-uncased), BERT
(Large-uncased), and GPT-3 models. Based on the three models' state-of-the-art
performance, this study concludes pre-trained language models' promising
potential for addressing online hate speech and in-game insulting trash talk.
\\ ( https://arxiv.org/abs/2403.15458 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15459
Date: Tue, 19 Mar 2024 11:49:03 GMT   (1631kb)

Title: Assessing effect sizes, variability, and power in the on-line study of
  language production
Authors: B\"urki Audrey and Vasishth Shravan
Categories: cs.CL
\\
  With the pandemic, many experimental psychologists and linguists have started
to collect data over the internet (hereafter on-line data). The feasibility of
such experiments and the sample sizes required to achieve sufficient
statistical power in future experiments have to be assessed. This in turn
requires information on effect sizes and variability. In a series of analyses,
we compare response time data obtained in the same word production experiment
conducted in the lab and on-line. These analyses allow us to determine whether
the two settings differ in effect sizes, in the consistency of responses over
the course of the experiment, in the variability of average response times
across participants, in the magnitude of effect sizes across participants, or
in the amount of unexplained variability. We assess the impact of these
differences on the power of the design in a series of simulations. Our findings
temper the enthusiasm raised by previous studies and suggest that on-line
production studies might be feasible but at a non-negligible cost. The sample
sizes required to achieve sufficient power in on-line language production
studies come with a non-negligible increase in the amount of manual labour.
\\ ( https://arxiv.org/abs/2403.15459 ,  1631kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15464
Date: Tue, 19 Mar 2024 18:10:13 GMT   (543kb,D)

Title: LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach
  Combining Predictive Agent Reasoning and Critical Agent Instruction
Authors: Hejie Cui, Zhuocheng Shen, Jieyu Zhang, Hui Shao, Lianhui Qin, Joyce
  C. Ho, Carl Yang
Categories: cs.CL cs.AI cs.LG cs.MA
ACM-class: J.3; I.2.7
\\
  Electronic health records (EHRs) contain valuable patient data for
health-related prediction tasks, such as disease prediction. Traditional
approaches rely on supervised learning methods that require large labeled
datasets, which can be expensive and challenging to obtain. In this study, we
investigate the feasibility of applying Large Language Models (LLMs) to convert
structured patient visit data (e.g., diagnoses, labs, prescriptions) into
natural language narratives. We evaluate the zero-shot and few-shot performance
of LLMs using various EHR-prediction-oriented prompting strategies.
Furthermore, we propose a novel approach that utilizes LLM agents with
different roles: a predictor agent that makes predictions and generates
reasoning processes and a critic agent that analyzes incorrect predictions and
provides guidance for improving the reasoning of the predictor agent. Our
results demonstrate that with the proposed approach, LLMs can achieve decent
few-shot performance compared to traditional supervised learning methods in
EHR-based disease predictions, suggesting its potential for health-oriented
applications.
\\ ( https://arxiv.org/abs/2403.15464 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15467
Date: Wed, 20 Mar 2024 06:28:09 GMT   (8391kb,D)

Title: Don't be a Fool: Pooling Strategies in Offensive Language Detection from
  User-Intended Adversarial Attacks
Authors: Seunguk Yu, Juhwan Choi and Youngbin Kim
Categories: cs.CL
Comments: NAACL 2024 Findings; Camera-Ready Version
\\
  Offensive language detection is an important task for filtering out abusive
expressions and improving online user experiences. However, malicious users
often attempt to avoid filtering systems through the involvement of textual
noises. In this paper, we propose these evasions as user-intended adversarial
attacks that insert special symbols or leverage the distinctive features of the
Korean language. Furthermore, we introduce simple yet effective pooling
strategies in a layer-wise manner to defend against the proposed attacks,
focusing on the preceding layers not just the last layer to capture both
offensiveness and token embeddings. We demonstrate that these pooling
strategies are more robust to performance degradation even when the attack rate
is increased, without directly training of such patterns. Notably, we found
that models pre-trained on clean texts could achieve a comparable performance
in detecting attacked offensive language, to models pre-trained on noisy texts
by employing these pooling strategies.
\\ ( https://arxiv.org/abs/2403.15467 ,  8391kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15469
Date: Wed, 20 Mar 2024 08:52:40 GMT   (8730kb,D)

Title: Isometric Neural Machine Translation using Phoneme Count Ratio
  Reward-based Reinforcement Learning
Authors: Shivam Ratnakant Mhaskar, Nirmesh J. Shah, Mohammadi Zaki, Ashishkumar
  P. Gudmalwar, Pankaj Wasnik, Rajiv Ratn Shah
Categories: cs.CL cs.LG eess.AS
Comments: Accepted in NAACL2024 Findings
\\
  Traditional Automatic Video Dubbing (AVD) pipeline consists of three key
modules, namely, Automatic Speech Recognition (ASR), Neural Machine Translation
(NMT), and Text-to-Speech (TTS). Within AVD pipelines, isometric-NMT algorithms
are employed to regulate the length of the synthesized output text. This is
done to guarantee synchronization with respect to the alignment of video and
audio subsequent to the dubbing process. Previous approaches have focused on
aligning the number of characters and words in the source and target language
texts of Machine Translation models. However, our approach aims to align the
number of phonemes instead, as they are closely associated with speech
duration. In this paper, we present the development of an isometric NMT system
using Reinforcement Learning (RL), with a focus on optimizing the alignment of
phoneme counts in the source and target language sentence pairs. To evaluate
our models, we propose the Phoneme Count Compliance (PCC) score, which is a
measure of length compliance. Our approach demonstrates a substantial
improvement of approximately 36% in the PCC score compared to the
state-of-the-art models when applied to English-Hindi language pairs. Moreover,
we propose a student-teacher architecture within the framework of our RL
approach to maintain a trade-off between the phoneme count and translation
quality.
\\ ( https://arxiv.org/abs/2403.15469 ,  8730kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15470
Date: Wed, 20 Mar 2024 10:14:13 GMT   (141kb,D)

Title: Vi-Mistral-X: Building a Vietnamese Language Model with Advanced
  Continual Pre-training
Authors: James Vo
Categories: cs.CL
Comments: The model is currently under development
\\
  The advancement of Large Language Models (LLMs) has significantly transformed
the field of natural language processing, although the focus on English-centric
models has created a noticeable research gap for specific languages, including
Vietnamese. To address this issue, this paper presents vi-mistral-x, an
innovative Large Language Model designed expressly for the Vietnamese language.
It utilizes a unique method of continual pre-training, based on the Mistral
architecture, which incorporates grouped-query attention and sliding window
attention techniques. This model, vi-Mistral-X, marks a significant step
forward in improving the understanding and generation of the Vietnamese
language. It introduces an additional phase of continual pre-training,
specifically adapted for Vietnamese, enhancing the model's capability in
understanding complex language nuances and generating accurate, context-aware
Vietnamese text. Through comprehensive testing on various benchmarks,
vi-mistral-x has shown to outperform existing Vietnamese LLMs in several key
areas, including text classification, question answering, and text generation.
Particularly, in the Vietnamese Multitask Language Understanding (VMLU)
benchmark, vi-mistral-x sets a new standard, outperforming other available
models significantly. This paper highlights the critical role of continual
pre-training in advancing language-specific LLMs and opens new avenues for the
development of multilingual models. We aim for vi-mistral-x to not just be an
important asset for processing the Vietnamese language but also to encourage
more advancements in creating large language models for languages that are less
represented.
\\ ( https://arxiv.org/abs/2403.15470 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15473
Date: Wed, 20 Mar 2024 16:24:10 GMT   (34kb)

Title: Efficient argument classification with compact language models and
  ChatGPT-4 refinements
Authors: Marcin Pietron, Rafa{\l} Olszowski, Jakub Gomu{\l}ka
Categories: cs.CL
\\
  Argument mining (AM) is defined as the task of automatically identifying and
extracting argumentative components (e.g. premises, claims, etc.) and detecting
the existing relations among them (i.e., support, attack, no relations). Deep
learning models enable us to analyze arguments more efficiently than
traditional methods and extract their semantics. This paper presents
comparative studies between a few deep learning-based models in argument
mining. The work concentrates on argument classification. The research was done
on a wide spectrum of datasets (Args.me, UKP, US2016). The main novelty of this
paper is the ensemble model which is based on BERT architecture and ChatGPT-4
as fine tuning model. The presented results show that BERT+ChatGPT-4
outperforms the rest of the models including other Transformer-based and
LSTM-based models. The observed improvement is, in most cases, greater than
10The presented analysis can provide crucial insights into how the models for
argument classification should be further improved. Additionally, it can help
develop a prompt-based algorithm to eliminate argument classification errors.
\\ ( https://arxiv.org/abs/2403.15473 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15478
Date: Wed, 20 Mar 2024 21:16:10 GMT   (743kb)

Title: Integrating Supervised Extractive and Generative Language Models for
  Suicide Risk Evidence Summarization
Authors: Rika Tanaka, Yusuke Fukazawa
Categories: cs.CL
\\
  We propose a method that integrates supervised extractive and generative
language models for providing supporting evidence of suicide risk in the
CLPsych 2024 shared task. Our approach comprises three steps. Initially, we
construct a BERT-based model for estimating sentence-level suicide risk and
negative sentiment. Next, we precisely identify high suicide risk sentences by
emphasizing elevated probabilities of both suicide risk and negative sentiment.
Finally, we integrate generative summaries using the MentaLLaMa framework and
extractive summaries from identified high suicide risk sentences and a
specialized dictionary of suicidal risk words. SophiaADS, our team, achieved
1st place for highlight extraction and ranked 10th for summary generation, both
based on recall and consistency metrics, respectively.
\\ ( https://arxiv.org/abs/2403.15478 ,  743kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15482
Date: Thu, 21 Mar 2024 04:23:56 GMT   (12760kb,D)

Title: Multi-Level Feedback Generation with Large Language Models for
  Empowering Novice Peer Counselors
Authors: Alicja Chaszczewicz, Raj Sanjay Shah, Ryan Louie, Bruce A Arnow,
  Robert Kraut, Diyi Yang
Categories: cs.CL cs.LG
\\
  Realistic practice and tailored feedback are key processes for training peer
counselors with clinical skills. However, existing mechanisms of providing
feedback largely rely on human supervision. Peer counselors often lack
mechanisms to receive detailed feedback from experienced mentors, making it
difficult for them to support the large number of people with mental health
issues who use peer counseling. Our work aims to leverage large language models
to provide contextualized and multi-level feedback to empower peer counselors,
especially novices, at scale. To achieve this, we co-design with a group of
senior psychotherapy supervisors to develop a multi-level feedback taxonomy,
and then construct a publicly available dataset with comprehensive feedback
annotations of 400 emotional support conversations. We further design a
self-improvement method on top of large language models to enhance the
automatic generation of feedback. Via qualitative and quantitative evaluation
with domain experts, we demonstrate that our method minimizes the risk of
potentially harmful and low-quality feedback generation which is desirable in
such high-stakes scenarios.
\\ ( https://arxiv.org/abs/2403.15482 ,  12760kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15484
Date: Thu, 21 Mar 2024 06:56:07 GMT   (35kb,D)

Title: RakutenAI-7B: Extending Large Language Models for Japanese
Authors: Rakuten Group Inc., Aaron Levine, Connie Huang, Chenguang Wang,
  Eduardo Batista, Ewa Szymanska, Hongyi Ding, Hou Wei Chou, Jean-Fran\c{c}ois
  Pessiot, Johanes Effendi, Justin Chiu, Kai Torben Ohlhus, Karan Chopra, Keiji
  Shinzato, Koji Murakami, Lee Xiong, Lei Chen, Maki Kubota, Maksim Tkachenko,
  Miroku Lee, Naoki Takahashi, Prathyusha Jwalapuram, Ryutaro Tatsushima,
  Saurabh Jain, Sunil Kumar Yadav, Ting Cai, Wei-Te Chen, Yandi Xia, Yuki
  Nakayama, Yutaka Higashiyama
Categories: cs.CL cs.LG
\\
  We introduce RakutenAI-7B, a suite of Japanese-oriented large language models
that achieve the best performance on the Japanese LM Harness benchmarks among
the open 7B models. Along with the foundation model, we release instruction-
and chat-tuned models, RakutenAI-7B-instruct and RakutenAI-7B-chat
respectively, under the Apache 2.0 license.
\\ ( https://arxiv.org/abs/2403.15484 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15485
Date: Thu, 21 Mar 2024 07:45:58 GMT   (1120kb,D)

Title: MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression
  Detection
Authors: Junyeop Cha, Seoyun Kim, Dongjae Kim, and Eunil Park
Categories: cs.CL cs.AI cs.LG
Comments: 12 pages, 3 figures, 4 tables
\\
  Early detection plays a crucial role in the treatment of depression.
Therefore, numerous studies have focused on social media platforms, where
individuals express their emotions, aiming to achieve early detection of
depression. However, the majority of existing approaches often rely on specific
features, leading to limited scalability across different types of social media
datasets, such as text, images, or videos. To overcome this limitation, we
introduce a Multimodal Object-Oriented Graph Attention Model (MOGAM), which can
be applied to diverse types of data, offering a more scalable and versatile
solution. Furthermore, to ensure that our model can capture authentic symptoms
of depression, we only include vlogs from users with a clinical diagnosis. To
leverage the diverse features of vlogs, we adopt a multimodal approach and
collect additional metadata such as the title, description, and duration of the
vlogs. To effectively aggregate these multimodal features, we employed a
cross-attention mechanism. MOGAM achieved an accuracy of 0.871 and an F1-score
of 0.888. Moreover, to validate the scalability of MOGAM, we evaluated its
performance with a benchmark dataset and achieved comparable results with prior
studies (0.61 F1-score). In conclusion, we believe that the proposed model,
MOGAM, is an effective solution for detecting depression in social media,
offering potential benefits in the early detection and treatment of this mental
health condition.
\\ ( https://arxiv.org/abs/2403.15485 ,  1120kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15486
Date: Thu, 21 Mar 2024 08:27:49 GMT   (9609kb,D)

Title: Sequence-to-Sequence Language Models for Character and Emotion Detection
  in Dream Narratives
Authors: Gustave Cortal (ENS Paris Saclay, LISN)
Categories: cs.CL cs.AI
\\
  The study of dreams has been central to understanding human
(un)consciousness, cognition, and culture for centuries. Analyzing dreams
quantitatively depends on labor-intensive, manual annotation of dream
narratives. We automate this process through a natural language
sequence-to-sequence generation framework. This paper presents the first study
on character and emotion detection in the English portion of the open DreamBank
corpus of dream narratives. Our results show that language models can
effectively address this complex task. To get insight into prediction
performance, we evaluate the impact of model size, prediction order of
characters, and the consideration of proper names and character traits. We
compare our approach with a large language model using in-context learning. Our
supervised models perform better while having 28 times fewer parameters. Our
model and its generated annotations are made publicly available.
\\ ( https://arxiv.org/abs/2403.15486 ,  9609kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15491
Date: Thu, 21 Mar 2024 15:41:02 GMT   (1192kb,D)

Title: Open Source Conversational LLMs do not know most Spanish words
Authors: Javier Conde, Miguel Gonz\'alez, Nina Melero, Raquel Ferrando, Gonzalo
  Mart\'inez, Elena Merino-G\'omez, Jos\'e Alberto Hern\'andez and Pedro
  Reviriego
Categories: cs.CL
Comments: Under Review at SEPLN-2024
\\
  The growing interest in Large Language Models (LLMs) and in particular in
conversational models with which users can interact has led to the development
of a large number of open-source chat LLMs. These models are evaluated on a
wide range of benchmarks to assess their capabilities in answering questions or
solving problems on almost any possible topic or to test their ability to
reason or interpret texts. Instead, the evaluation of the knowledge that these
models have of the languages has received much less attention. For example, the
words that they can recognize and use in different languages. In this paper, we
evaluate the knowledge that open-source chat LLMs have of Spanish words by
testing a sample of words in a reference dictionary. The results show that
open-source chat LLMs produce incorrect meanings for an important fraction of
the words and are not able to use most of the words correctly to write
sentences with context. These results show how Spanish is left behind in the
open-source LLM race and highlight the need to push for linguistic fairness in
conversational LLMs ensuring that they provide similar performance across
languages.
\\ ( https://arxiv.org/abs/2403.15491 ,  1192kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15492
Date: Thu, 21 Mar 2024 17:26:28 GMT   (4466kb,D)

Title: Visual Analytics for Fine-grained Text Classification Models and
  Datasets
Authors: Munkhtulga Battogtokh, Yiwen Xing, Cosmin Davidescu, Alfie
  Abdul-Rahman, Michael Luck, Rita Borgo
Categories: cs.CL
\\
  In natural language processing (NLP), text classification tasks are
increasingly fine-grained, as datasets are fragmented into a larger number of
classes that are more difficult to differentiate from one another. As a
consequence, the semantic structures of datasets have become more complex, and
model decisions more difficult to explain. Existing tools, suited for
coarse-grained classification, falter under these additional challenges. In
response to this gap, we worked closely with NLP domain experts in an iterative
design-and-evaluation process to characterize and tackle the growing
requirements in their workflow of developing fine-grained text classification
models. The result of this collaboration is the development of SemLa, a novel
visual analytics system tailored for 1) dissecting complex semantic structures
in a dataset when it is spatialized in model embedding space, and 2)
visualizing fine-grained nuances in the meaning of text samples to faithfully
explain model reasoning. This paper details the iterative design study and the
resulting innovations featured in SemLa. The final design allows contrastive
analysis at different levels by unearthing lexical and conceptual patterns
including biases and artifacts in data. Expert feedback on our final design and
case studies confirm that SemLa is a useful tool for supporting model
validation and debugging as well as data annotation.
\\ ( https://arxiv.org/abs/2403.15492 ,  4466kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15501
Date: Thu, 21 Mar 2024 21:28:07 GMT   (426kb)

Title: Enhancing Medical Support in the Arabic Language Through Personalized
  ChatGPT Assistance
Authors: Mohamed Issa and Ahmed Abdelwahed
Categories: cs.CL
Comments: This paper was presented at The International conference for Arabic
  language and applied linguistics
\\
  This Paper discusses the growing popularity of online medical diagnosis as an
alternative to traditional doctor visits. It highlights the limitations of
existing tools and emphasizes the advantages of using ChatGPT, which provides
real-time, personalized medical diagnosis at no cost. The paragraph summarizes
a research study that evaluated the performance of ChatGPT in Arabic medical
diagnosis. The study involved compiling a dataset of disease information and
generating multiple messages for each disease using different prompting
techniques. ChatGPT's performance was assessed by measuring the similarity
between its responses and the actual diseases. The results showed promising
performance, with average scores of around 76% for similarity measures. Various
prompting techniques were used, and chain prompting demonstrated a relative
advantage. The study also recorded an average response time of 6.12 seconds for
the ChatGPT API, which is considered acceptable but has room for improvement.
While ChatGPT cannot replace human doctors entirely, the findings suggest its
potential in emergency cases and addressing general medical inquiries. Overall,
the study highlights ChatGPT's viability as a valuable tool in the medical
field.
\\ ( https://arxiv.org/abs/2403.15501 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15502
Date: Thu, 21 Mar 2024 22:33:16 GMT   (3453kb,D)

Title: Sequential Decision-Making for Inline Text Autocomplete
Authors: Rohan Chitnis and Shentao Yang and Alborz Geramifard
Categories: cs.CL cs.HC cs.LG
\\
  Autocomplete suggestions are fundamental to modern text entry systems, with
applications in domains such as messaging and email composition. Typically,
autocomplete suggestions are generated from a language model with a confidence
threshold. However, this threshold does not directly take into account the
cognitive load imposed on the user by surfacing suggestions, such as the effort
to switch contexts from typing to reading the suggestion, and the time to
decide whether to accept the suggestion. In this paper, we study the problem of
improving inline autocomplete suggestions in text entry systems via a
sequential decision-making formulation, and use reinforcement learning to learn
suggestion policies through repeated interactions with a target user over time.
This formulation allows us to factor cognitive load into the objective of
training an autocomplete model, through a reward function based on text entry
speed. We acquired theoretical and experimental evidence that, under certain
objectives, the sequential decision-making formulation of the autocomplete
problem provides a better suggestion policy than myopic single-step reasoning.
However, aligning these objectives with real users requires further
exploration. In particular, we hypothesize that the objectives under which
sequential decision-making can improve autocomplete systems are not tailored
solely to text entry speed, but more broadly to metrics such as user
satisfaction and convenience.
\\ ( https://arxiv.org/abs/2403.15502 ,  3453kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15503
Date: Thu, 21 Mar 2024 23:40:42 GMT   (1118kb)

Title: Evaluating the Performance of LLMs on Technical Language Processing
  tasks
Authors: Andrew Kernycky, David Coleman, Christopher Spence, Udayan Das
Categories: cs.CL
\\
  In this paper we present the results of an evaluation study of the
perfor-mance of LLMs on Technical Language Processing tasks. Humans are often
confronted with tasks in which they have to gather information from dispar-ate
sources and require making sense of large bodies of text. These tasks can be
significantly complex for humans and often require deep study including
rereading portions of a text. Towards simplifying the task of gathering
in-formation we evaluated LLMs with chat interfaces for their ability to
provide answers to standard questions that a human can be expected to answer
based on their reading of a body of text. The body of text under study is Title
47 of the United States Code of Federal Regulations (CFR) which describes
regula-tions for commercial telecommunications as governed by the Federal
Com-munications Commission (FCC). This has been a body of text of interest
be-cause our larger research concerns the issue of making sense of information
related to Wireless Spectrum Governance and usage in an automated manner to
support Dynamic Spectrum Access. The information concerning this wireless
spectrum domain is found in many disparate sources, with Title 47 of the CFR
being just one of many.
  Using a range of LLMs and providing the required CFR text as context we were
able to quantify the performance of those LLMs on the specific task of
answering the questions below.
\\ ( https://arxiv.org/abs/2403.15503 ,  1118kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15512
Date: Fri, 22 Mar 2024 05:18:08 GMT   (916kb,D)

Title: Enhancing Effectiveness and Robustness in a Low-Resource Regime via
  Decision-Boundary-aware Data Augmentation
Authors: Kyohoon Jin, Junho Lee, Juhwan Choi, Sangmin Song and Youngbin Kim
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at LREC-COLING 2024
\\
  Efforts to leverage deep learning models in low-resource regimes have led to
numerous augmentation studies. However, the direct application of methods such
as mixup and cutout to text data, is limited due to their discrete
characteristics. While methods using pretrained language models have exhibited
efficiency, they require additional considerations for robustness. Inspired by
recent studies on decision boundaries, this paper proposes a
decision-boundary-aware data augmentation strategy to enhance robustness using
pretrained language models. The proposed technique first focuses on shifting
the latent features closer to the decision boundary, followed by reconstruction
to generate an ambiguous version with a soft label. Additionally, mid-K
sampling is suggested to enhance the diversity of the generated sentences. This
paper demonstrates the performance of the proposed augmentation strategy
compared to other methods through extensive experiments. Furthermore, the
ablation study reveals the effect of soft labels and mid-K sampling and the
extensibility of the method with curriculum data augmentation.
\\ ( https://arxiv.org/abs/2403.15512 ,  916kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15516
Date: Fri, 22 Mar 2024 10:45:13 GMT   (374kb,D)

Title: CTSM: Combining Trait and State Emotions for Empathetic Response Model
Authors: Wang Yufeng, Chen Chao, Yang Zhou, Wang Shuhui, Liao Xiangwen
Categories: cs.CL cs.AI
Comments: 9 pages, 3 figures. Has been accepted by LREC-COLING2024
\\
  Empathetic response generation endeavors to empower dialogue systems to
perceive speakers' emotions and generate empathetic responses accordingly.
Psychological research demonstrates that emotion, as an essential factor in
empathy, encompasses trait emotions, which are static and context-independent,
and state emotions, which are dynamic and context-dependent. However, previous
studies treat them in isolation, leading to insufficient emotional perception
of the context, and subsequently, less effective empathetic expression. To
address this problem, we propose Combining Trait and State emotions for
Empathetic Response Model (CTSM). Specifically, to sufficiently perceive
emotions in dialogue, we first construct and encode trait and state emotion
embeddings, and then we further enhance emotional perception capability through
an emotion guidance module that guides emotion representation. In addition, we
propose a cross-contrastive learning decoder to enhance the model's empathetic
expression capability by aligning trait and state emotions between generated
responses and contexts. Both automatic and manual evaluation results
demonstrate that CTSM outperforms state-of-the-art baselines and can generate
more empathetic responses. Our code is available at
https://github.com/wangyufeng-empty/CTSM
\\ ( https://arxiv.org/abs/2403.15516 ,  374kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15529
Date: Fri, 22 Mar 2024 17:31:43 GMT   (665kb,D)

Title: LimGen: Probing the LLMs for Generating Suggestive Limitations of
  Research Papers
Authors: Abdur Rahman Bin Md Faizullah, Ashok Urlana, Rahul Mishra
Categories: cs.CL cs.AI
Comments: 16 pages, 3 figures
\\
  Examining limitations is a crucial step in the scholarly research reviewing
process, revealing aspects where a study might lack decisiveness or require
enhancement. This aids readers in considering broader implications for further
research. In this article, we present a novel and challenging task of
Suggestive Limitation Generation (SLG) for research papers. We compile a
dataset called LimGen, encompassing 4068 research papers and their associated
limitations from the ACL anthology. We investigate several approaches to
harness large language models (LLMs) for producing suggestive limitations, by
thoroughly examining the related challenges, practical insights, and potential
opportunities. Our LimGen dataset and code can be accessed at
https://github.com/armbf/LimGen.
\\ ( https://arxiv.org/abs/2403.15529 ,  665kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15615
Date: Fri, 22 Mar 2024 21:05:54 GMT   (1399kb)

Title: NaturalTurn: A Method to Segment Transcripts into Naturalistic
  Conversational Turns
Authors: Gus Cooney and Andrew Reece
Categories: cs.CL
Comments: 39 pages, 6 figures
\\
  Conversation is the subject of increasing interest in the social, cognitive,
and computational sciences. And yet, as conversational datasets continue to
increase in size and complexity, researchers lack scalable methods to segment
speech-to-text transcripts into conversational turns--the basic building blocks
of social interaction. We introduce "NaturalTurn," a turn segmentation
algorithm designed to accurately capture the dynamics of naturalistic exchange.
NaturalTurn operates by distinguishing speakers' primary conversational turns
from listeners' secondary utterances, such as backchannels, brief
interjections, and other forms of parallel speech that characterize
conversation. Using data from a large conversation corpus, we show how
NaturalTurn-derived transcripts demonstrate favorable statistical and
inferential characteristics compared to transcripts derived from existing
methods. The NaturalTurn algorithm represents an improvement in
machine-generated transcript processing methods, or "turn models" that will
enable researchers to associate turn-taking dynamics with the broader outcomes
that result from social interaction, a central goal of conversation science.
\\ ( https://arxiv.org/abs/2403.15615 ,  1399kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15673
Date: Sat, 23 Mar 2024 01:40:22 GMT   (812kb,D)

Title: AI for Biomedicine in the Era of Large Language Models
Authors: Zhenyu Bi, Sajib Acharjee Dip, Daniel Hajialigol, Sindhura Kommu,
  Hanwen Liu, Meng Lu, Xuan Wang
Categories: cs.CL
Comments: 8 pages, 3 figures
\\
  The capabilities of AI for biomedicine span a wide spectrum, from the atomic
level, where it solves partial differential equations for quantum systems, to
the molecular level, predicting chemical or protein structures, and further
extending to societal predictions like infectious disease outbreaks. Recent
advancements in large language models, exemplified by models like ChatGPT, have
showcased significant prowess in natural language tasks, such as translating
languages, constructing chatbots, and answering questions. When we consider
biomedical data, we observe a resemblance to natural language in terms of
sequences: biomedical literature and health records presented as text,
biological sequences or sequencing data arranged in sequences, or sensor data
like brain signals as time series. The question arises: Can we harness the
potential of recent large language models to drive biomedical knowledge
discoveries? In this survey, we will explore the application of large language
models to three crucial categories of biomedical data: 1) textual data, 2)
biological sequences, and 3) brain signals. Furthermore, we will delve into
large language model challenges in biomedical research, including ensuring
trustworthiness, achieving personalization, and adapting to multi-modal data
representation
\\ ( https://arxiv.org/abs/2403.15673 ,  812kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15690
Date: Sat, 23 Mar 2024 02:44:20 GMT   (16990kb,D)

Title: EAGLE: A Domain Generalization Framework for AI-generated Text Detection
Authors: Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu
Categories: cs.CL cs.AI cs.LG
\\
  With the advancement in capabilities of Large Language Models (LLMs), one
major step in the responsible and safe use of such LLMs is to be able to detect
text generated by these models. While supervised AI-generated text detectors
perform well on text generated by older LLMs, with the frequent release of new
LLMs, building supervised detectors for identifying text from such new models
would require new labeled training data, which is infeasible in practice. In
this work, we tackle this problem and propose a domain generalization framework
for the detection of AI-generated text from unseen target generators. Our
proposed framework, EAGLE, leverages the labeled data that is available so far
from older language models and learns features invariant across these
generators, in order to detect text generated by an unknown target generator.
EAGLE learns such domain-invariant features by combining the representational
power of self-supervised contrastive learning with domain adversarial training.
Through our experiments we demonstrate how EAGLE effectively achieves
impressive performance in detecting text generated by unseen target generators,
including recent state-of-the-art ones such as GPT-4 and Claude, reaching
detection scores of within 4.7% of a fully supervised detector.
\\ ( https://arxiv.org/abs/2403.15690 ,  16990kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15699
Date: Sat, 23 Mar 2024 03:32:26 GMT   (725kb)

Title: FEEL: A Framework for Evaluating Emotional Support Capability with Large
  Language Models
Authors: Huaiwen Zhang, Yu Chen, Ming Wang and Shi Feng
Categories: cs.CL
Comments: 14 pages,3 figures and 4 tables
\\
  Emotional Support Conversation (ESC) is a typical dialogue that can
effec-tively assist the user in mitigating emotional pressures. However, owing
to the inherent subjectivity involved in analyzing emotions, current
non-artificial methodologies face challenges in effectively appraising the
emo-tional support capability. These metrics exhibit a low correlation with
human judgments. Concurrently, manual evaluation methods extremely will cause
high costs. To solve these problems, we propose a novel model FEEL (Framework
for Evaluating Emotional Support Capability with Large Lan-guage Models),
employing Large Language Models (LLMs) as evaluators to assess emotional
support capabilities. The model meticulously considers var-ious evaluative
aspects of ESC to apply a more comprehensive and accurate evaluation method for
ESC. Additionally, it employs a probability distribu-tion approach for a more
stable result and integrates an ensemble learning strategy, leveraging multiple
LLMs with assigned weights to enhance evalua-tion accuracy. To appraise the
performance of FEEL, we conduct extensive experiments on existing ESC model
dialogues. Experimental results demon-strate our model exhibits a substantial
enhancement in alignment with human evaluations compared to the baselines. Our
source code is available at https://github.com/Ansisy/FEEL.
\\ ( https://arxiv.org/abs/2403.15699 ,  725kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15715
Date: Sat, 23 Mar 2024 04:29:29 GMT   (2508kb,D)

Title: EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance
  Detection
Authors: Daijun Ding, Li Dong, Zhichao Huang, Guangning Xu, Xu Huang, Bo Liu,
  Liwen Jing, Bowen Zhang
Categories: cs.CL
\\
  Stance detection aims to determine the attitude expressed in text towards a
given target. Zero-shot stance detection (ZSSD) has emerged to classify stances
towards unseen targets during inference. Recent data augmentation techniques
for ZSSD increase transferable knowledge between targets through text or target
augmentation. However, these methods exhibit limitations. Target augmentation
lacks logical connections between generated targets and source text, while text
augmentation relies solely on training data, resulting in insufficient
generalization. To address these issues, we propose an encoder-decoder data
augmentation (EDDA) framework. The encoder leverages large language models and
chain-of-thought prompting to summarize texts into target-specific if-then
rationales, establishing logical relationships. The decoder generates new
samples based on these expressions using a semantic correlation word
replacement strategy to increase syntactic diversity. We also analyze the
generated expressions to develop a rationale-enhanced network that fully
utilizes the augmented data. Experiments on benchmark datasets demonstrate our
approach substantially improves over state-of-the-art ZSSD techniques. The
proposed EDDA framework increases semantic relevance and syntactic variety in
augmented texts while enabling interpretable rationale-based learning.
\\ ( https://arxiv.org/abs/2403.15715 ,  2508kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15724
Date: Sat, 23 Mar 2024 05:20:36 GMT   (1203kb,D)

Title: PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on
  Scientific Documents
Authors: Nan Zhang, Connor Heaton, Sean Timothy Okonsky, Prasenjit Mitra, Hilal
  Ezgi Toraman
Categories: cs.CL cs.AI
Comments: LREC-COLING 2024
\\
  Optical Character Recognition (OCR) is an established task with the objective
of identifying the text present in an image. While many off-the-shelf OCR
models exist, they are often trained for either scientific (e.g., formulae) or
generic printed English text. Extracting text from chemistry publications
requires an OCR model that is capable in both realms. Nougat, a recent tool,
exhibits strong ability to parse academic documents, but is unable to parse
tables in PubMed articles, which comprises a significant part of the academic
community and is the focus of this work. To mitigate this gap, we present the
Printed English and Chemical Equations (PEaCE) dataset, containing both
synthetic and real-world records, and evaluate the efficacy of
transformer-based OCR models when trained on this resource. Given that
real-world records contain artifacts not present in synthetic records, we
propose transformations that mimic such qualities. We perform a suite of
experiments to explore the impact of patch size, multi-domain training, and our
proposed transformations, ultimately finding that models with a small patch
size trained on multiple domains using the proposed transformations yield the
best performance. Our dataset and code is available at
https://github.com/ZN1010/PEaCE.
\\ ( https://arxiv.org/abs/2403.15724 ,  1203kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15729
Date: Sat, 23 Mar 2024 05:32:46 GMT   (4267kb,D)

Title: Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion
  Collider
Authors: Karthik Suresh, Neeltje Kackar, Luke Schleck, Cristiano Fanelli
Categories: cs.CL cs.AI hep-ex physics.ins-det
\\
  The complexity and sheer volume of information encompassing documents,
papers, data, and other resources from large-scale experiments demand
significant time and effort to navigate, making the task of accessing and
utilizing these varied forms of information daunting, particularly for new
collaborators and early-career scientists. To tackle this issue, a Retrieval
Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under
development. This AI-Agent not only condenses information but also effectively
references relevant responses, offering substantial advantages for
collaborators. Our project involves a two-step approach: first, querying a
comprehensive vector database containing all pertinent experiment information;
second, utilizing a Large Language Model (LLM) to generate concise summaries
enriched with citations based on user queries and retrieved data. We describe
the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to
assess the effectiveness of responses. Furthermore, we describe the concept of
prompt template-based instruction-tuning which provides flexibility and
accuracy in summarization. Importantly, the implementation relies on LangChain,
which serves as the foundation of our entire workflow. This integration ensures
efficiency and scalability, facilitating smooth deployment and accessibility
for various user groups within the Electron Ion Collider (EIC) community. This
innovative AI-driven framework not only simplifies the understanding of vast
datasets but also encourages collaborative participation, thereby empowering
researchers. As a demonstration, a web application has been developed to
explain each stage of the RAG Agent development in detail.
\\ ( https://arxiv.org/abs/2403.15729 ,  4267kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15736
Date: Sat, 23 Mar 2024 06:03:36 GMT   (909kb,D)

Title: LLMs Instruct LLMs:An Extraction and Editing Method
Authors: Xin Zhang, Tianjie Ju, Huijia Liang, Ying Fu, Qin Zhang
Categories: cs.CL
Comments: Working in progress
\\
  The interest in updating Large Language Models (LLMs) without retraining from
scratch is substantial, yet it comes with some challenges.This is especially
true for situations demanding complex reasoning with limited samples, a
scenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation
for LLMs (PCRA-LLM).Traditional methods like Low-Rank Adaptation (LoRA) and
Retrieval-Augmented Generation (RAG) are inadequate for this critical issue,
particularly evident in our exploration of a specific medical context that
epitomize the PCRA-LLM's distinct needs.To address the issue, we propose a
Sequential Fusion method to incorporate knowledge from complex context into
LLMs. This method employs a two-stage framework: initially, it leverages
general LLMs to construct knowledge graphs (KGs) for extracting knowledge from
complex texts; subsequently, it updates the domain LLMs through knowledge edit.
According to our method, the domain LLM achieved a 71.69\% accuracy in question
answering tasks. Subsequently, we broadened our assessment to a novel dataset
we developed in the economics and management field, where our method realized a
75\% accuracy. These outcomes underline the efficacy and adaptability of our
approach for PCRA-LLM across various domains.
\\ ( https://arxiv.org/abs/2403.15736 ,  909kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15737
Date: Sat, 23 Mar 2024 06:03:37 GMT   (7750kb,D)

Title: Few-shot Dialogue Strategy Learning for Motivational Interviewing via
  Inductive Reasoning
Authors: Zhouhang Xie, Bodhisattwa Prasad Majumder, Mengjie Zhao, Yoshinori
  Maeda, Keiichi Yamada, Hiromi Wakaki, Julian McAuley
Categories: cs.CL
\\
  We consider the task of building a dialogue system that can motivate users to
adopt positive lifestyle changes: Motivational Interviewing. Addressing such a
task requires a system that can infer \textit{how} to motivate a user
effectively. We propose DIIT, a framework that is capable of learning and
applying conversation strategies in the form of natural language inductive
rules from expert demonstrations. Automatic and human evaluation on
instruction-following large language models show natural language strategy
descriptions discovered by DIIR can improve active listening skills, reduce
unsolicited advice, and promote more collaborative and less authoritative
responses, outperforming various demonstration utilization methods.
\\ ( https://arxiv.org/abs/2403.15737 ,  7750kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15740
Date: Sat, 23 Mar 2024 06:36:32 GMT   (3243kb,D)

Title: Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large
  Language Models
Authors: Shuai Zhao, Linchao Zhu, Ruijie Quan, Yi Yang
Categories: cs.CL cs.CR cs.LG
Comments: Preprint, work in progress
\\
  Web user data plays a central role in the ecosystem of pre-trained large
language models (LLMs) and their fine-tuned variants. Billions of data are
crawled from the web and fed to LLMs. How can \textit{\textbf{everyday web
users}} confirm if LLMs misuse their data without permission? In this work, we
suggest that users repeatedly insert personal passphrases into their documents,
enabling LLMs to memorize them. These concealed passphrases in user documents,
referred to as \textit{ghost sentences}, once they are identified in the
generated content of LLMs, users can be sure that their data is used for
training. To explore the effectiveness and usage of this copyrighting tool, we
define the \textit{user training data identification} task with ghost
sentences. Multiple datasets from various sources at different scales are
created and tested with LLMs of different sizes. For evaluation, we introduce a
last $k$ words verification manner along with two metrics: document and user
identification accuracy. In the specific case of instruction tuning of a 3B
LLaMA model, 11 out of 16 users with ghost sentences identify their data within
the generation content. These 16 users contribute 383 examples to $\sim$1.8M
training documents. For continuing pre-training of a 1.1B TinyLlama model, 61
out of 64 users with ghost sentences identify their data within the LLM output.
These 64 users contribute 1156 examples to $\sim$10M training documents.
\\ ( https://arxiv.org/abs/2403.15740 ,  3243kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15776
Date: Sat, 23 Mar 2024 09:18:53 GMT   (1000kb,D)

Title: Modeling Unified Semantic Discourse Structure for High-quality Headline
  Generation
Authors: Minghui Xu, Hao Fei, Fei Li, Shengqiong Wu, Rui Sun, Chong Teng,
  Donghong Ji
Categories: cs.CL cs.AI
\\
  Headline generation aims to summarize a long document with a short, catchy
title that reflects the main idea. This requires accurately capturing the core
document semantics, which is challenging due to the lengthy and background
information-rich na ture of the texts. In this work, We propose using a unified
semantic discourse structure (S3) to represent document semantics, achieved by
combining document-level rhetorical structure theory (RST) trees with
sentence-level abstract meaning representation (AMR) graphs to construct S3
graphs. The hierarchical composition of sentence, clause, and word
intrinsically characterizes the semantic meaning of the overall document. We
then develop a headline generation framework, in which the S3 graphs are
encoded as contextual features. To consolidate the efficacy of S3 graphs, we
further devise a hierarchical structure pruning mechanism to dynamically screen
the redundant and nonessential nodes within the graph. Experimental results on
two headline generation datasets demonstrate that our method outperforms
existing state-of-art methods consistently. Our work can be instructive for a
broad range of document modeling tasks, more than headline or summarization
generation.
\\ ( https://arxiv.org/abs/2403.15776 ,  1000kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15796
Date: Sat, 23 Mar 2024 11:03:31 GMT   (865kb,D)

Title: Understanding Emergent Abilities of Language Models from the Loss
  Perspective
Authors: Zhengxiao Du, Aohan Zeng, Yuxiao Dong, Jie Tang
Categories: cs.CL cs.AI cs.LG
Comments: 18 pages, 6 figures
\\
  Recent studies have put into question the belief that emergent abilities in
language models are exclusive to large models. This skepticism arises from two
observations: 1) smaller models can also exhibit high performance on emergent
abilities and 2) there is doubt on the discontinuous metrics used to measure
these abilities. In this paper, we propose to study emergent abilities in the
lens of pre-training loss, instead of model size or training compute. We
demonstrate that the models with the same pre-training loss, but different
model and data sizes, generate the same performance on various downstream
tasks. We also discover that a model exhibits emergent abilities on certain
tasks -- regardless of the continuity of metrics -- when its pre-training loss
falls below a specific threshold. Before reaching this threshold, its
performance remains at the level of random guessing. This inspires us to
redefine emergent abilities as those that manifest in models with lower
pre-training losses, highlighting that these abilities cannot be predicted by
merely extrapolating the performance trends of models with higher pre-training
losses.
\\ ( https://arxiv.org/abs/2403.15796 ,  865kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15800
Date: Sat, 23 Mar 2024 11:14:02 GMT   (1662kb,D)

Title: MRC-based Nested Medical NER with Co-prediction and Adaptive
  Pre-training
Authors: Xiaojing Du, Hanjie Zhao, Danyan Xing, Yuxiang Jia, Hongying Zan
Categories: cs.CL
\\
  In medical information extraction, medical Named Entity Recognition (NER) is
indispensable, playing a crucial role in developing medical knowledge graphs,
enhancing medical question-answering systems, and analyzing electronic medical
records. The challenge in medical NER arises from the complex nested structures
and sophisticated medical terminologies, distinguishing it from its
counterparts in traditional domains. In response to these complexities, we
propose a medical NER model based on Machine Reading Comprehension (MRC), which
uses a task-adaptive pre-training strategy to improve the model's capability in
the medical field. Meanwhile, our model introduces multiple word-pair
embeddings and multi-granularity dilated convolution to enhance the model's
representation ability and uses a combined predictor of Biaffine and MLP to
improve the model's recognition performance. Experimental evaluations conducted
on the CMeEE, a benchmark for Chinese nested medical NER, demonstrate that our
proposed model outperforms the compared state-of-the-art (SOTA) models.
\\ ( https://arxiv.org/abs/2403.15800 ,  1662kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15822
Date: Sat, 23 Mar 2024 12:19:49 GMT   (6158kb,D)

Title: Computational Sentence-level Metrics Predicting Human Sentence
  Comprehension
Authors: Kun Sun, and Rong Wang
Categories: cs.CL stat.ML
\\
  The majority of research in computational psycholinguistics has concentrated
on the processing of words. This study introduces innovative methods for
computing sentence-level metrics using multilingual large language models. The
metrics developed sentence surprisal and sentence relevance and then are tested
and compared to validate whether they can predict how humans comprehend
sentences as a whole across languages. These metrics offer significant
interpretability and achieve high accuracy in predicting human sentence reading
speeds. Our results indicate that these computational sentence-level metrics
are exceptionally effective at predicting and elucidating the processing
difficulties encountered by readers in comprehending sentences as a whole
across a variety of languages. Their impressive performance and generalization
capabilities provide a promising avenue for future research in integrating LLMs
and cognitive science.
\\ ( https://arxiv.org/abs/2403.15822 ,  6158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15872
Date: Sat, 23 Mar 2024 15:43:30 GMT   (902kb,D)

Title: RAAMove: A Corpus for Analyzing Moves in Research Article Abstracts
Authors: Hongzheng Li, Ruojin Wang, Ge Shi, Xing Lv, Lei Lei, Chong Feng, Fang
  Liu, Jinkun Lin, Yangguang Mei, Lingnan Xu
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\
  Move structures have been studied in English for Specific Purposes (ESP) and
English for Academic Purposes (EAP) for decades. However, there are few move
annotation corpora for Research Article (RA) abstracts. In this paper, we
introduce RAAMove, a comprehensive multi-domain corpus dedicated to the
annotation of move structures in RA abstracts. The primary objective of RAAMove
is to facilitate move analysis and automatic move identification. This paper
provides a thorough discussion of the corpus construction process, including
the scheme, data collection, annotation guidelines, and annotation procedures.
The corpus is constructed through two stages: initially, expert annotators
manually annotate high-quality data; subsequently, based on the human-annotated
data, a BERT-based model is employed for automatic annotation with the help of
experts' modification. The result is a large-scale and high-quality corpus
comprising 33,988 annotated instances. We also conduct preliminary move
identification experiments using the BERT-based model to verify the
effectiveness of the proposed corpus and model. The annotated corpus is
available for academic research purposes and can serve as essential resources
for move analysis, English language teaching and writing, as well as
move/discourse-related tasks in Natural Language Processing (NLP).
\\ ( https://arxiv.org/abs/2403.15872 ,  902kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15882
Date: Sat, 23 Mar 2024 16:26:49 GMT   (7275kb,D)

Title: VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for
  Vietnamese Natural Language Understanding
Authors: Phong Nguyen-Thuan Do, Son Quoc Tran, Phu Gia Hoang, Kiet Van Nguyen,
  Ngan Luu-Thuy Nguyen
Categories: cs.CL
Comments: Accepted at NAACL 2024 (Findings)
\\
  The success of Natural Language Understanding (NLU) benchmarks in various
languages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and
IndoNLU for Indonesian, has facilitated the evaluation of new NLU models across
a wide range of tasks. To establish a standardized set of benchmarks for
Vietnamese NLU, we introduce the first Vietnamese Language Understanding
Evaluation (VLUE) benchmark. The VLUE benchmark encompasses five datasets
covering different NLU tasks, including text classification, span extraction,
and natural language understanding. To provide an insightful overview of the
current state of Vietnamese NLU, we then evaluate seven state-of-the-art
pre-trained models, including both multilingual and Vietnamese monolingual
models, on our proposed VLUE benchmark. Furthermore, we present CafeBERT, a new
state-of-the-art pre-trained model that achieves superior results across all
tasks in the VLUE benchmark. Our model combines the proficiency of a
multilingual pre-trained model with Vietnamese linguistic knowledge. CafeBERT
is developed based on the XLM-RoBERTa model, with an additional pretraining
step utilizing a significant amount of Vietnamese textual data to enhance its
adaptation to the Vietnamese language. For the purpose of future research,
CafeBERT is made publicly available for research purposes.
\\ ( https://arxiv.org/abs/2403.15882 ,  7275kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15885
Date: Sat, 23 Mar 2024 16:45:22 GMT   (800kb,D)

Title: STEntConv: Predicting Disagreement with Stance Detection and a Signed
  Graph Convolutional Network
Authors: Isabelle Lorge, Li Zhang, Xiaowen Dong, Janet B. Pierrehumbert
Categories: cs.CL
Comments: Accepted for the 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)
\\
  The rise of social media platforms has led to an increase in polarised online
discussions, especially on political and socio-cultural topics such as
elections and climate change. We propose a simple and novel unsupervised method
to predict whether the authors of two posts agree or disagree, leveraging user
stances about named entities obtained from their posts. We present STEntConv, a
model which builds a graph of users and named entities weighted by stance and
trains a Signed Graph Convolutional Network (SGCN) to detect disagreement
between comment and reply posts. We run experiments and ablation studies and
show that including this information improves disagreement detection
performance on a dataset of Reddit posts for a range of controversial subreddit
topics, without the need for platform-specific features or user history.
\\ ( https://arxiv.org/abs/2403.15885 ,  800kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15886
Date: Sat, 23 Mar 2024 16:51:52 GMT   (5324kb,D)

Title: Leveraging Zero-Shot Prompting for Efficient Language Model Distillation
Authors: Lukas V\"oge, Vincent Gurgul, and Stefan Lessmann
Categories: cs.CL cs.AI cs.LG
\\
  This paper introduces a novel approach for efficiently distilling LLMs into
smaller, application-specific models, significantly reducing operational costs
and manual labor. Addressing the challenge of deploying computationally
intensive LLMs in specific applications or edge devices, this technique
utilizes LLMs' reasoning capabilities to generate labels and natural language
rationales for unlabeled data. Our approach enhances both finetuning and
distillation by employing a multi-task training framework where student models
mimic these rationales alongside teacher predictions. Key contributions include
the employment of zero-shot prompting to elicit teacher model rationales,
reducing the necessity for handcrafted few-shot examples and lowering the
overall token count required, which directly translates to cost savings given
the pay-per-token billing model of major tech companies' LLM APIs.
Additionally, the paper investigates the impact of explanation properties on
distillation efficiency, demonstrating that minimal performance loss occurs
even when rationale augmentation is not applied across the entire dataset,
facilitating further reductions of tokens. This research marks a step toward
the efficient training of task-specific models with minimal human intervention,
offering substantial cost-savings while maintaining, or even enhancing,
performance.
\\ ( https://arxiv.org/abs/2403.15886 ,  5324kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15938
Date: Sat, 23 Mar 2024 21:54:34 GMT   (71kb,D)

Title: LlamBERT: Large-scale low-cost data annotation in NLP
Authors: B\'alint Csan\'ady, Lajos Muzsai, P\'eter Vedres, Zolt\'an N\'adasdy,
  Andr\'as Luk\'acs
Categories: cs.CL cs.AI cs.LG
Comments: 11 pages, 1 figure
ACM-class: I.2.7; F.1.1
\\
  Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable
proficiency in a wide range of natural language processing (NLP) tasks. Despite
their effectiveness, the high costs associated with their use pose a challenge.
We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small
subset of large, unlabeled databases and uses the results for fine-tuning
transformer encoders like BERT and RoBERTa. This strategy is evaluated on two
diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our
results indicate that the LlamBERT approach slightly compromises on accuracy
while offering much greater cost-effectiveness.
\\ ( https://arxiv.org/abs/2403.15938 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15940
Date: Sat, 23 Mar 2024 22:02:56 GMT   (151kb,D)

Title: Geotokens and Geotransformers
Authors: Eren Unlu
Categories: cs.CL cs.AI
\\
  In transformer architectures, position encoding primarily provides a sense of
sequence for input tokens. While the original transformer paper's method has
shown satisfactory results in general language processing tasks, there have
been new proposals, such as Rotary Position Embedding (RoPE), for further
improvement. This paper presents geotokens, input components for transformers,
each linked to a specific geological location. Unlike typical language
sequences, for these tokens, the order is not as vital as the geographical
coordinates themselves. To represent the relative position in this context and
to keep a balance between the real world distance and the distance in the
embedding space, we design a position encoding approach drawing from the RoPE
structure but tailored for spherical coordinates.
\\ ( https://arxiv.org/abs/2403.15940 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16008
Date: Sun, 24 Mar 2024 04:34:34 GMT   (432kb,D)

Title: CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral
  Therapy-based Mental Health Question Answering
Authors: Hongbin Na
Categories: cs.CL
Comments: Accepted at COLING 2024
\\
  The recent advancements in artificial intelligence highlight the potential of
language models in psychological health support. While models trained on data
from mental health service platform have achieved preliminary success,
challenges persist in areas such as data scarcity, quality, and ensuring a
solid foundation in psychological techniques. To address these challenges, this
study introduces a novel approach to enhance the precision and efficacy of
psychological support through large language models. Specifically, we design a
specific prompt derived from principles of Cognitive Behavioral Therapy (CBT)
and have generated the CBT QA dataset, specifically for Chinese psychological
health Q&A based on CBT structured intervention strategies. Unlike previous
methods, our dataset emphasizes professional and structured response. Utilizing
this dataset, we fine-tuned the large language model, giving birth to CBT-LLM,
the large-scale language model specifically designed for Cognitive Behavioral
Therapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in
generating structured, professional, and highly relevant responses in
psychological health support tasks, showcasing its practicality and quality.
The model is available on Hugging Face:
https://huggingface.co/Hongbin37/CBT-LLM.
\\ ( https://arxiv.org/abs/2403.16008 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16038
Date: Sun, 24 Mar 2024 06:49:07 GMT   (8001kb,D)

Title: Monotonic Paraphrasing Improves Generalization of Language Model
  Prompting
Authors: Qin Liu, Fei Wang, Nan Xu, Tianyi Yan, Tao Meng, Muhao Chen
Categories: cs.CL
\\
  Performance of large language models (LLMs) may vary with different prompts
or instructions of even the same task. One commonly recognized factor for this
phenomenon is the model's familiarity with the given prompt or instruction,
which is typically estimated by its perplexity. However, finding the prompt
with the lowest perplexity is challenging, given the enormous space of possible
prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),
an end-to-end decoding strategy that paraphrases given prompts or instructions
into their lower perplexity counterparts based on an ensemble of a paraphrase
LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or
instruction executor) that constrains the generation for lower perplexity. The
ensemble decoding process can efficiently paraphrase the original prompt
without altering its semantic meaning, while monotonically decreasing the
perplexity of each generation as calculated by the target LM. We explore in
detail both greedy and search-based decoding as two alternative decoding
schemes of MonoPara. Notably, MonoPara does not require any training and can
monotonically lower the perplexity of the paraphrased prompt or instruction,
leading to improved performance of zero-shot LM prompting as evaluated on a
wide selection of tasks. In addition, MonoPara is also shown to effectively
improve LMs' generalization on perturbed and unseen task instructions.
\\ ( https://arxiv.org/abs/2403.16038 ,  8001kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16056
Date: Sun, 24 Mar 2024 07:48:05 GMT   (874kb,D)

Title: Qibo: A Large Language Model for Traditional Chinese Medicine
Authors: Heyi Zhang and Xin Wang and Zhaopeng Meng and Yongzhe Jia and Dawei Xu
Categories: cs.CL cs.AI
\\
  In the field of Artificial Intelligence, Large Language Models (LLMs) have
demonstrated significant advances in user intent understanding and response in
a number of specialized domains, including medicine, law, and finance. However,
in the unique domain of traditional Chinese medicine (TCM), the performance
enhancement of LLMs is challenged by the essential differences between its
theories and modern medicine, as well as the lack of specialized corpus
resources. In this paper, we aim to construct and organize a professional
corpus in the field of TCM, to endow the large model with professional
knowledge that is characteristic of TCM theory, and to successfully develop the
Qibo model based on LLaMA, which is the first LLM in the field of TCM to
undergo a complete training process from pre-training to Supervised Fine-Tuning
(SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for
evaluating the performance of LLMs, which is a specialized tool for evaluating
the performance of LLMs in the TCM domain. This tool will provide an important
basis for quantifying and comparing the understanding and application
capabilities of different models in the field of traditional Chinese medicine,
and provide guidance for future research directions and practical applications
of intelligent assistants for traditional Chinese medicine. Finally, we
conducted sufficient experiments to prove that Qibo has good performance in the
field of traditional Chinese medicine.
\\ ( https://arxiv.org/abs/2403.16056 ,  874kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16084
Date: Sun, 24 Mar 2024 10:43:21 GMT   (415kb,D)

Title: Argument Quality Assessment in the Age of Instruction-Following Large
  Language Models
Authors: Henning Wachsmuth, Gabriella Lapesa, Elena Cabrio, Anne Lauscher,
  Joonsuk Park, Eva Maria Vecchi, Serena Villata, Timon Ziegenbein
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  The computational treatment of arguments on controversial issues has been
subject to extensive NLP research, due to its envisioned impact on opinion
formation, decision making, writing education, and the like. A critical task in
any such application is the assessment of an argument's quality - but it is
also particularly challenging. In this position paper, we start from a brief
survey of argument quality research, where we identify the diversity of quality
notions and the subjectiveness of their perception as the main hurdles towards
substantial progress on argument quality assessment. We argue that the
capabilities of instruction-following large language models (LLMs) to leverage
knowledge across contexts enable a much more reliable assessment. Rather than
just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they
need to be instructed systematically with argumentation theories and scenarios
as well as with ways to solve argument-related problems. We discuss the
real-world opportunities and ethical issues emerging thereby.
\\ ( https://arxiv.org/abs/2403.16084 ,  415kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16099
Date: Sun, 24 Mar 2024 11:29:55 GMT   (2407kb,D)

Title: A Multi-Label Dataset of French Fake News: Human and Machine Insights
Authors: Benjamin Icard, Fran\c{c}ois Maine, Morgane Casanova, G\'eraud Faye,
  Julien Chanson, Guillaume Gadek, Ghislain Atemezing, Fran\c{c}ois Bancilhon,
  Paul \'Egr\'e
Categories: cs.CL cs.LG
Comments: Paper to appear in the Proceedings of the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)
\\
  We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of
French press considered unreliable by expert agencies, annotated using 11
labels by 8 annotators. By collecting more labels than usual, by more
annotators than is typically done, we can identify features that humans
consider as characteristic of fake news, and compare them to the predictions of
automated classifiers. We present a topic and genre analysis using Gate Cloud,
indicative of the prevalence of satire-like text in the corpus. We then use the
subjectivity analyzer VAGO, and a neural version of it, to clarify the link
between ascriptions of the label Subjective and ascriptions of the label Fake
News. The annotated dataset is available online at the following url:
https://github.com/obs-info/obsinfox
  Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion,
Exaggeration, French Press
\\ ( https://arxiv.org/abs/2403.16099 ,  2407kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16127
Date: Sun, 24 Mar 2024 12:49:30 GMT   (963kb,D)

Title: WangchanLion and WangchanX MRC Eval
Authors: Wannaphong Phatthiyaphaibun, Surapon Nonesung, Patomporn
  Payoungkhamdee, Peerat Limkonchotiwat, Can Udomcharoenchaikit, Ekapol
  Chuangsuwanich, Sarana Nutanong
Categories: cs.CL cs.AI
\\
  This technical report describes the development of WangchanLion, an
instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in
the Thai language. Our model is based on SEA-LION and a collection of
instruction following datasets. To promote open research and reproducibility,
we publically release all training data, code, and the final model weights
under the Apache-2 license. To assess the contextual understanding capability,
we conducted extensive experimental studies using two Thai MRC datasets, XQuAD
and Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to
comprehend the context and produce an answer faithful to the reference one in
0-shot and 1-shot settings. In addition, our evaluation goes beyond the
traditional MRC. We propose a new evaluation scheme assessing the answer's
correctness, helpfulness, conciseness, and contextuality. Evaluation results
provide insight into how we can improve our model in the future. Our code is
public at https://github.com/vistec-AI/WangchanLion.
\\ ( https://arxiv.org/abs/2403.16127 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16129
Date: Sun, 24 Mar 2024 12:58:48 GMT   (372kb)

Title: A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation
Authors: Miuru Abeysiriwardana, Deshan Sumanathilaka
Categories: cs.CL
Comments: 6 pages, 5 figures, 3 tables, Accepted by 20th IEEE International
  Colloquium on Signal Processing & its Applications (CSPA 2024)
\\
  This paper explores techniques that focus on understanding and resolving
ambiguity in language within the field of natural language processing (NLP),
highlighting the complexity of linguistic phenomena such as polysemy and
homonymy and their implications for computational models. Focusing extensively
on Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from
deep learning techniques to leveraging lexical resources and knowledge graphs
like WordNet. The paper introduces cutting-edge methodologies like word sense
extension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy
by predicting new word senses. It examines specific applications in biomedical
disambiguation and language specific optimisation and discusses the
significance of cognitive metaphors in discourse analysis. The research
identifies persistent challenges in the field, such as the scarcity of sense
annotated corpora and the complexity of informal clinical texts. It concludes
by suggesting future directions, including using large language models, visual
WSD, and multilingual WSD systems, emphasising the ongoing evolution in
addressing lexical complexities in NLP. This thinking perspective highlights
the advancement in this field to enable computers to understand language more
accurately.
\\ ( https://arxiv.org/abs/2403.16129 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16139
Date: Sun, 24 Mar 2024 13:21:58 GMT   (391kb,D)

Title: A Little Leak Will Sink a Great Ship: Survey of Transparency for Large
  Language Models from Start to Finish
Authors: Masahiro Kaneko, Timothy Baldwin
Categories: cs.CL
\\
  Large Language Models (LLMs) are trained on massive web-crawled corpora. This
poses risks of leakage, including personal information, copyrighted texts, and
benchmark datasets. Such leakage leads to undermining human trust in AI due to
potential unauthorized generation of content or overestimation of performance.
We establish the following three criteria concerning the leakage issues: (1)
leakage rate: the proportion of leaked data in training data, (2) output rate:
the ease of generating leaked data, and (3) detection rate: the detection
performance of leaked versus non-leaked data. Despite the leakage rate being
the origin of data leakage issues, it is not understood how it affects the
output rate and detection rate. In this paper, we conduct an experimental
survey to elucidate the relationship between the leakage rate and both the
output rate and detection rate for personal information, copyrighted texts, and
benchmark data. Additionally, we propose a self-detection approach that uses
few-shot learning in which LLMs detect whether instances are present or absent
in their training data, in contrast to previous methods that do not employ
explicit learning. To explore the ease of generating leaked information, we
create a dataset of prompts designed to elicit personal information,
copyrighted text, and benchmarks from LLMs. Our experiments reveal that LLMs
produce leaked information in most cases despite less such data in their
training set. This indicates even small amounts of leaked data can greatly
affect outputs. Our self-detection method showed superior performance compared
to existing detection methods.
\\ ( https://arxiv.org/abs/2403.16139 ,  391kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16142
Date: Sun, 24 Mar 2024 13:28:27 GMT   (72kb,D)

Title: What Happens to a Dataset Transformed by a Projection-based Concept
  Removal Method?
Authors: Richard Johansson
Categories: cs.CL cs.AI
\\
  We investigate the behavior of methods that use linear projections to remove
information about a concept from a language representation, and we consider the
question of what happens to a dataset transformed by such a method. A
theoretical analysis and experiments on real-world and synthetic data show that
these methods inject strong statistical dependencies into the transformed
datasets. After applying such a method, the representation space is highly
structured: in the transformed space, an instance tends to be located near
instances of the opposite label. As a consequence, the original labeling can in
some cases be reconstructed by applying an anti-clustering method.
\\ ( https://arxiv.org/abs/2403.16142 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16158
Date: Sun, 24 Mar 2024 13:51:05 GMT   (1029kb,D)

Title: Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition
Authors: Sungjoo Byun, Jiseung Hong, Sumin Park, Dongjun Jang, Jean Seo,
  Minseok Kim, Chaeyoung Oh, Hyopil Shin
Categories: cs.CL
Journal-ref: LREC-COLING 2024
\\
  Named Entity Recognition (NER) plays a pivotal role in medical Natural
Language Processing (NLP). Yet, there has not been an open-source medical NER
dataset specifically for the Korean language. To address this, we utilized
ChatGPT to assist in constructing the KBMC (Korean Bio-Medical Corpus), which
we are now presenting to the public. With the KBMC dataset, we noticed an
impressive 20% increase in medical NER performance compared to models trained
on general Korean NER datasets. This research underscores the significant
benefits and importance of using specialized tools and datasets, like ChatGPT,
to enhance language processing in specialized fields such as healthcare.
\\ ( https://arxiv.org/abs/2403.16158 ,  1029kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16187
Date: Sun, 24 Mar 2024 15:09:55 GMT   (330kb,D)

Title: ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language
  Models
Authors: Zequan Liu, Jiawen Lyn, Wei Zhu, Xing Tian, Yvette Graham
Categories: cs.CL
Comments: Accepted by NAACL-2024
\\
  Parameter-efficient fine-tuning (PEFT) is widely studied for its
effectiveness and efficiency in the era of large language models. Low-rank
adaptation (LoRA) has demonstrated commendable performance as a popular and
representative method. However, it is implemented with a fixed intrinsic rank
that might not be the ideal setting for the downstream tasks. Recognizing the
need for more flexible downstream task adaptation, we extend the methodology of
LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA)
that enables dynamic adjustments to the intrinsic rank during the adaptation
process. First, we propose a novel method, AB-LoRA, that can effectively
estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we
gradually prune abundant and negatively impacting LoRA ranks and allocate the
pruned LoRA budgets to important Transformer modules needing higher ranks. We
have conducted experiments on various tasks, and the experimental results
demonstrate that our ALoRA method can outperform the recent baselines with
comparable tunable parameters.
\\ ( https://arxiv.org/abs/2403.16187 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16204
Date: Sun, 24 Mar 2024 15:57:24 GMT   (7600kb,D)

Title: SQL-Encoder: Improving NL2SQL In-Context Learning Through a
  Context-Aware Encoder
Authors: Mohammadreza Pourreza, Davood Rafiei, Yuxi Feng, Raymond Li, Zhenan
  Fan, Weiwei Zhang
Categories: cs.CL cs.DB cs.HC
\\
  Detecting structural similarity between queries is essential for selecting
examples in in-context learning models. However, assessing structural
similarity based solely on the natural language expressions of queries, without
considering SQL queries, presents a significant challenge. This paper explores
the significance of this similarity metric and proposes a model for accurately
estimating it. To achieve this, we leverage a dataset comprising 170k question
pairs, meticulously curated to train a similarity prediction model. Our
comprehensive evaluation demonstrates that the proposed model adeptly captures
the structural similarity between questions, as evidenced by improvements in
Kendall-Tau distance and precision@k metrics. Notably, our model outperforms
strong competitive embedding models from OpenAI and Cohere. Furthermore,
compared to these competitive models, our proposed encoder enhances the
downstream performance of NL2SQL models in 1-shot in-context learning scenarios
by 1-2\% for GPT-3.5-turbo, 4-8\% for CodeLlama-7B, and 2-3\% for
CodeLlama-13B.
\\ ( https://arxiv.org/abs/2403.16204 ,  7600kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16247
Date: Sun, 24 Mar 2024 17:39:36 GMT   (1407kb,D)

Title: Improving Sequence-to-Sequence Models for Abstractive Text Summarization
  Using Meta Heuristic Approaches
Authors: Aditya Saxena, Ashutosh Ranjan
Categories: cs.CL cs.LG cs.NE
\\
  As human society transitions into the information age, reduction in our
attention span is a contingency, and people who spend time reading lengthy news
articles are decreasing rapidly and the need for succinct information is higher
than ever before. Therefore, it is essential to provide a quick overview of
important news by concisely summarizing the top news article and the most
intuitive headline. When humans try to make summaries, they extract the
essential information from the source and add useful phrases and grammatical
annotations from the original extract. Humans have a unique ability to create
abstractions. However, automatic summarization is a complicated problem to
solve. The use of sequence-to-sequence (seq2seq) models for neural abstractive
text summarization has been ascending as far as prevalence. Numerous innovative
strategies have been proposed to develop the current seq2seq models further,
permitting them to handle different issues like saliency, familiarity, and
human lucidness and create excellent synopses. In this article, we aimed toward
enhancing the present architectures and models for abstractive text
summarization. The modifications have been aimed at fine-tuning
hyper-parameters, attempting specific encoder-decoder combinations. We examined
many experiments on an extensively used CNN/DailyMail dataset to check the
effectiveness of various models.
\\ ( https://arxiv.org/abs/2403.16247 ,  1407kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16248
Date: Sun, 24 Mar 2024 17:39:51 GMT   (6374kb,D)

Title: Large Language Models Offer an Alternative to the Traditional Approach
  of Topic Modelling
Authors: Yida Mu, Chun Dong, Kalina Bontcheva, Xingyi Song
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Topic modelling, as a well-established unsupervised technique, has found
extensive use in automatically detecting significant topics within a corpus of
documents. However, classic topic modelling approaches (e.g., LDA) have certain
drawbacks, such as the lack of semantic understanding and the presence of
overlapping topics. In this work, we investigate the untapped potential of
large language models (LLMs) as an alternative for uncovering the underlying
topics within extensive text corpora. To this end, we introduce a framework
that prompts LLMs to generate topics from a given set of documents and
establish evaluation protocols to assess the clustering efficacy of LLMs. Our
findings indicate that LLMs with appropriate prompts can stand out as a viable
alternative, capable of generating relevant topic titles and adhering to human
guidelines to refine and merge topics. Through in-depth experiments and
evaluation, we summarise the advantages and constraints of employing LLMs in
topic extraction.
\\ ( https://arxiv.org/abs/2403.16248 ,  6374kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16265
Date: Sun, 24 Mar 2024 18:59:38 GMT   (7889kb,D)

Title: Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved
  Phrase Graphs
Authors: Zhuoyi Peng, Yi Yang
Categories: cs.CL
Comments: Findings of NAACL 2024
\\
  We study the patent phrase similarity inference task, which measures the
semantic similarity between two patent phrases. As patent documents employ
legal and highly technical language, existing semantic textual similarity
methods that use localized contextual information do not perform satisfactorily
in inferring patent phrase similarity. To address this, we introduce a
graph-augmented approach to amplify the global contextual information of the
patent phrases. For each patent phrase, we construct a phrase graph that links
to its focal patents and a list of patents that are either cited by or cite
these focal patents. The augmented phrase embedding is then derived from
combining its localized contextual embedding with its global embedding within
the phrase graph. We further propose a self-supervised learning objective that
capitalizes on the retrieved topology to refine both the contextualized
embedding and the graph parameters in an end-to-end manner. Experimental
results from a unique patent phrase similarity dataset demonstrate that our
approach significantly enhances the representation of patent phrases, resulting
in marked improvements in similarity inference in a self-supervised fashion.
Substantial improvements are also observed in the supervised setting,
underscoring the potential benefits of leveraging retrieved phrase graph
augmentation.
\\ ( https://arxiv.org/abs/2403.16265 ,  7889kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16295
Date: Sun, 24 Mar 2024 21:02:35 GMT   (1026kb,D)

Title: LexDrafter: Terminology Drafting for Legislative Documents using
  Retrieval Augmented Generation
Authors: Ashish Chouhan and Michael Gertz
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  With the increase in legislative documents at the EU, the number of new terms
and their definitions is increasing as well. As per the Joint Practical Guide
of the European Parliament, the Council and the Commission, terms used in legal
documents shall be consistent, and identical concepts shall be expressed
without departing from their meaning in ordinary, legal, or technical language.
Thus, while drafting a new legislative document, having a framework that
provides insights about existing definitions and helps define new terms based
on a document's context will support such harmonized legal definitions across
different regulations and thus avoid ambiguities. In this paper, we present
LexDrafter, a framework that assists in drafting Definitions articles for
legislative documents using retrieval augmented generation (RAG) and existing
term definitions present in different legislative documents. For this,
definition elements are built by extracting definitions from existing
documents. Using definition elements and RAG, a Definitions article can be
suggested on demand for a legislative document that is being drafted. We
demonstrate and evaluate the functionality of LexDrafter using a collection of
EU documents from the energy domain. The code for LexDrafter framework is
available at https://github.com/achouhan93/LexDrafter.
\\ ( https://arxiv.org/abs/2403.16295 ,  1026kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16345
Date: Mon, 25 Mar 2024 00:43:44 GMT   (102kb,D)

Title: Enhanced Facet Generation with LLM Editing
Authors: Joosung Lee, Jinhong Kim
Categories: cs.CL cs.AI cs.IR
Comments: Accepted at LREC-COLING 2024
\\
  In information retrieval, facet identification of a user query is an
important task. If a search service can recognize the facets of a user's query,
it has the potential to offer users a much broader range of search results.
Previous studies can enhance facet prediction by leveraging retrieved documents
and related queries obtained through a search engine. However, there are
challenges in extending it to other applications when a search engine operates
as part of the model. First, search engines are constantly updated. Therefore,
additional information may change during training and test, which may reduce
performance. The second challenge is that public search engines cannot search
for internal documents. Therefore, a separate search system needs to be built
to incorporate documents from private domains within the company. We propose
two strategies that focus on a framework that can predict facets by taking only
queries as input without a search engine. The first strategy is multi-task
learning to predict SERP. By leveraging SERP as a target instead of a source,
the proposed model deeply understands queries without relying on external
modules. The second strategy is to enhance the facets by combining Large
Language Model (LLM) and the small model. Overall performance improves when
small model and LLM are combined rather than facet generation individually.
\\ ( https://arxiv.org/abs/2403.16345 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16394
Date: Mon, 25 Mar 2024 03:18:39 GMT   (7630kb,D)

Title: Skews in the Phenomenon Space Hinder Generalization in Text-to-Image
  Generation
Authors: Yingshan Chang, Yasi Zhang, Zhiyuan Fang, Yingnian Wu, Yonatan Bisk,
  Feng Gao
Categories: cs.CL cs.AI
\\
  The literature on text-to-image generation is plagued by issues of faithfully
composing entities with relations. But there lacks a formal understanding of
how entity-relation compositions can be effectively learned. Moreover, the
underlying phenomenon space that meaningfully reflects the problem structure is
not well-defined, leading to an arms race for larger quantities of data in the
hope that generalization emerges out of large-scale pretraining. We hypothesize
that the underlying phenomenological coverage has not been proportionally
scaled up, leading to a skew of the presented phenomenon which harms
generalization. We introduce statistical metrics that quantify both the
linguistic and visual skew of a dataset for relational learning, and show that
generalization failures of text-to-image generation are a direct result of
incomplete or unbalanced phenomenological coverage. We first perform
experiments in a synthetic domain and demonstrate that systematically
controlled metrics are strongly predictive of generalization performance. Then
we move to natural images and show that simple distribution perturbations in
light of our theories boost generalization without enlarging the absolute data
size. This work informs an important direction towards quality-enhancing the
data diversity or balance orthogonal to scaling up the absolute size. Our
discussions point out important open questions on 1) Evaluation of generated
entity-relation compositions, and 2) Better models for reasoning with abstract
relations.
\\ ( https://arxiv.org/abs/2403.16394 ,  7630kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16396
Date: Mon, 25 Mar 2024 03:19:20 GMT   (7385kb,D)

Title: Is There a One-Model-Fits-All Approach to Information Extraction?
  Revisiting Task Definition Biases
Authors: Wenhao Huang, Qianyu He, Zhixu Li, Jiaqing Liang, Yanghua Xiao
Categories: cs.CL
Comments: 15 pages, 4 figures
\\
  Definition bias is a negative phenomenon that can mislead models. Definition
bias in information extraction appears not only across datasets from different
domains but also within datasets sharing the same domain. We identify two types
of definition bias in IE: bias among information extraction datasets and bias
between information extraction datasets and instruction tuning datasets. To
systematically investigate definition bias, we conduct three probing
experiments to quantitatively analyze it and discover the limitations of
unified information extraction and large language models in solving definition
bias. To mitigate definition bias in information extraction, we propose a
multi-stage framework consisting of definition bias measurement, bias-aware
fine-tuning, and task-specific bias mitigation. Experimental results
demonstrate the effectiveness of our framework in addressing definition bias.
Resources of this paper can be found at
https://github.com/EZ-hwh/definition-bias
\\ ( https://arxiv.org/abs/2403.16396 ,  7385kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16432
Date: Mon, 25 Mar 2024 05:27:35 GMT   (7539kb,D)

Title: $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on
  Prompt-based Language Models
Authors: Yue Xu, Wenjie Wang
Categories: cs.CL cs.AI
Comments: Accepted to the main conference of NAACL2024
\\
  Prompt-based learning is a new language model training paradigm that adapts
the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes
the performance benchmarks across various natural language processing (NLP)
tasks. Instead of using a fixed prompt template to fine-tune the model, some
research demonstrates the effectiveness of searching for the prompt via
optimization. Such prompt optimization process of prompt-based learning on PLMs
also gives insight into generating adversarial prompts to mislead the model,
raising concerns about the adversarial vulnerability of this paradigm. Recent
studies have shown that universal adversarial triggers (UATs) can be generated
to alter not only the predictions of the target PLMs but also the prediction of
corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based
learning paradigm. However, UATs found in previous works are often unreadable
tokens or characters and can be easily distinguished from natural texts with
adaptive defenses. In this work, we consider the naturalness of the UATs and
develop $\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs
by a gradient-based beam search algorithm that not only effectively attacks the
target PLMs and PFMs but also maintains the naturalness among the trigger
tokens. Extensive results demonstrate the effectiveness of
$\textit{LinkPrompt}$, as well as the transferability of UATs generated by
\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and
API-accessed LLM GPT-3.5-turbo.
\\ ( https://arxiv.org/abs/2403.16432 ,  7539kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16435
Date: Mon, 25 Mar 2024 05:31:22 GMT   (7307kb,D)

Title: InstUPR : Instruction-based Unsupervised Passage Reranking with Large
  Language Models
Authors: Chao-Wei Huang and Yun-Nung Chen
Categories: cs.CL cs.IR
Comments: Preprint. This manuscript was originally written and submitted in
  June 2023
\\
  This paper introduces InstUPR, an unsupervised passage reranking method based
on large language models (LLMs). Different from existing approaches that rely
on extensive training with query-document pairs or retrieval-specific
instructions, our method leverages the instruction-following capabilities of
instruction-tuned LLMs for passage reranking without any additional
fine-tuning. To achieve this, we introduce a soft score aggregation technique
and employ pairwise reranking for unsupervised passage reranking. Experiments
on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised
baselines as well as an instruction-tuned reranker, highlighting its
effectiveness and superiority. Source code to reproduce all experiments is
open-sourced at https://github.com/MiuLab/InstUPR
\\ ( https://arxiv.org/abs/2403.16435 ,  7307kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16442
Date: Mon, 25 Mar 2024 06:05:50 GMT   (4961kb,D)

Title: If CLIP Could Talk: Understanding Vision-Language Model Representations
  Through Their Preferred Concept Descriptions
Authors: Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach
Categories: cs.CL cs.CV cs.LG
Comments: Code: https://github.com/BatsResearch/ex2
\\
  Recent works often assume that Vision-Language Model (VLM) representations
are based on visual attributes like shape. However, it is unclear to what
extent VLMs prioritize this information to represent concepts. We propose
Extract and Explore (EX2), a novel approach to characterize important textual
features for VLMs. EX2 uses reinforcement learning to align a large language
model with VLM preferences and generates descriptions that incorporate the
important features for the VLM. Then, we inspect the descriptions to identify
the features that contribute to VLM representations. We find that spurious
descriptions have a major role in VLM representations despite providing no
helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,
among informative descriptions, VLMs rely significantly on non-visual
attributes like habitat to represent visual concepts. Also, our analysis
reveals that different VLMs prioritize different attributes in their
representations. Overall, we show that VLMs do not simply match images to scene
descriptions and that non-visual or even spurious descriptions significantly
influence their representations.
\\ ( https://arxiv.org/abs/2403.16442 ,  4961kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16443
Date: Mon, 25 Mar 2024 06:09:55 GMT   (1844kb,D)

Title: CodeS: Natural Language to Code Repository via Multi-Layer Sketch
Authors: Daoguang Zan and Ailun Yu and Wei Liu and Dong Chen and Bo Shen and
  Wei Li and Yafen Yao and Yongshun Gong and Xiaolin Chen and Bei Guan and
  Zhiguang Yang and Yongji Wang and Qianxiang Wang and Lizhen Cui
Categories: cs.CL cs.AI cs.SE
Comments: https://github.com/NL2Code/CodeS
\\
  The impressive performance of large language models (LLMs) on code-related
tasks has shown the potential of fully automated software development. In light
of this, we introduce a new software engineering task, namely Natural Language
to code Repository (NL2Repo). This task aims to generate an entire code
repository from its natural language requirements. To address this task, we
propose a simple yet effective framework CodeS, which decomposes NL2Repo into
multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three
modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first
generates a repository's directory structure for given requirements;
FileSketcher then generates a file sketch for each file in the generated
structure; SketchFiller finally fills in the details for each function in the
generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry
out evaluations through both automated benchmarking and manual feedback
analysis. For benchmark-based evaluation, we craft a repository-oriented
benchmark, SketchEval, and design an evaluation metric, SketchBLEU. For
feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30
participants in conducting empirical studies. Extensive experiments prove the
effectiveness and practicality of CodeS on the NL2Repo task.
\\ ( https://arxiv.org/abs/2403.16443 ,  1844kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16444
Date: Mon, 25 Mar 2024 06:15:21 GMT   (854kb,D)

Title: KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for
  Fine-Tuning Korean Large Language Models
Authors: Dongjun Jang, Sungjoo Byun, Hyemi Jo, Hyopil Shin
Categories: cs.CL
\\
  Instruction Tuning on Large Language Models is an essential process for model
to function well and achieve high performance in specific tasks. Accordingly,
in mainstream languages such as English, instruction-based datasets are being
constructed and made publicly available. In the case of Korean, publicly
available models and datasets all rely on using the output of ChatGPT or
translating datasets built in English. In this paper, We introduce
\textit{KIT-19} as an instruction dataset for the development of LLM in Korean.
\textit{KIT-19} is a dataset created in an instruction format, comprising 19
existing open-source datasets for Korean NLP tasks. In this paper, we train a
Korean Pretrained LLM using \textit{KIT-19} to demonstrate its effectiveness.
The experimental results show that the model trained on \textit{KIT-19}
significantly outperforms existing Korean LLMs. Based on the its quality and
empirical results, this paper proposes that \textit{KIT-19} has the potential
to make a substantial contribution to the future improvement of Korean LLMs'
performance.
\\ ( https://arxiv.org/abs/2403.16444 ,  854kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16446
Date: Mon, 25 Mar 2024 06:17:54 GMT   (4361kb,D)

Title: Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric,
  Data, and Algorithm
Authors: Lei Liu and Xiaoyan Yang and Fangzhou Li and Chenfei Chi and Yue Shen
  and Shiwei Lyu Ming Zhang and Xiaowei Ma and Xiangguo Lyu and Liya Ma and
  Zhiqiang Zhang and Wei Xue and Yiran Huang and Jinjie Gu
Categories: cs.CL
\\
  Large language models (LLMs) are gaining increasing interests to improve
clinical efficiency for medical diagnosis, owing to their unprecedented
performance in modelling natural language. Ensuring the safe and reliable
clinical applications, the evaluation of LLMs indeed becomes critical for
better mitigating the potential risks, e.g., hallucinations. However, current
evaluation methods heavily rely on labor-intensive human participation to
achieve human-preferred judgements. To overcome this challenge, we propose an
automatic evaluation paradigm tailored to assess the LLMs' capabilities in
delivering clinical services, e.g., disease diagnosis and treatment. The
evaluation paradigm contains three basic elements: metric, data, and algorithm.
Specifically, inspired by professional clinical practice pathways, we formulate
a LLM-specific clinical pathway (LCP) to define the clinical capabilities that
a doctor agent should possess. Then, Standardized Patients (SPs) from the
medical education are introduced as the guideline for collecting medical data
for evaluation, which can well ensure the completeness of the evaluation
procedure. Leveraging these steps, we develop a multi-agent framework to
simulate the interactive environment between SPs and a doctor agent, which is
equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the
behaviors of a doctor agent are in accordance with LCP. The above paradigm can
be extended to any similar clinical scenarios to automatically evaluate the
LLMs' medical capabilities. Applying such paradigm, we construct an evaluation
benchmark in the field of urology, including a LCP, a SPs dataset, and an
automated RAE. Extensive experiments are conducted to demonstrate the
effectiveness of the proposed approach, providing more insights for LLMs' safe
and reliable deployments in clinical practice.
\\ ( https://arxiv.org/abs/2403.16446 ,  4361kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16447
Date: Mon, 25 Mar 2024 06:18:18 GMT   (362kb)

Title: A Study on How Attention Scores in the BERT Model are Aware of Lexical
  Categories in Syntactic and Semantic Tasks on the GLUE Benchmark
Authors: Dongjun Jang, Sungjoo Byun, Hyopil Shin
Categories: cs.CL
\\
  This study examines whether the attention scores between tokens in the BERT
model significantly vary based on lexical categories during the fine-tuning
process for downstream tasks. Drawing inspiration from the notion that in human
language processing, syntactic and semantic information is parsed differently,
we categorize tokens in sentences according to their lexical categories and
focus on changes in attention scores among these categories. Our hypothesis
posits that in downstream tasks that prioritize semantic information, attention
scores centered on content words are enhanced, while in cases emphasizing
syntactic information, attention scores centered on function words are
intensified. Through experimentation conducted on six tasks from the GLUE
benchmark dataset, we substantiate our hypothesis regarding the fine-tuning
process. Furthermore, our additional investigations reveal the presence of BERT
layers that consistently assign more bias to specific lexical categories,
irrespective of the task, highlighting the existence of task-agnostic lexical
category preferences.
\\ ( https://arxiv.org/abs/2403.16447 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16463
Date: Mon, 25 Mar 2024 06:45:09 GMT   (704kb,D)

Title: Few-shot Named Entity Recognition via Superposition Concept
  Discrimination
Authors: Jiawei Chen, Hongyu Lin, Xianpei Han, Yaojie Lu, Shanshan Jiang, Bin
  Dong, Le Sun
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Few-shot NER aims to identify entities of target types with only limited
number of illustrative instances. Unfortunately, few-shot NER is severely
challenged by the intrinsic precise generalization problem, i.e., it is hard to
accurately determine the desired target type due to the ambiguity stemming from
information deficiency. In this paper, we propose Superposition Concept
Discriminator (SuperCD), which resolves the above challenge via an active
learning paradigm. Specifically, a concept extractor is first introduced to
identify superposition concepts from illustrative instances, with each concept
corresponding to a possible generalization boundary. Then a superposition
instance retriever is applied to retrieve corresponding instances of these
superposition concepts from large-scale text corpus. Finally, annotators are
asked to annotate the retrieved instances and these annotated instances
together with original illustrative instances are used to learn FS-NER models.
To this end, we learn a universal concept extractor and superposition instance
retriever using a large-scale openly available knowledge bases. Experiments
show that SuperCD can effectively identify superposition concepts from
illustrative instances, retrieve superposition instances from large-scale
corpus, and significantly improve the few-shot NER performance with minimal
additional efforts.
\\ ( https://arxiv.org/abs/2403.16463 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16483
Date: Mon, 25 Mar 2024 07:08:13 GMT   (693kb,D)

Title: Automatic Construction of a Large-Scale Corpus for Geoparsing Using
  Wikipedia Hyperlinks
Authors: Keyaki Ohno, Hirotaka Kameko, Keisuke Shirai, Taichi Nishimura,
  Shinsuke Mori
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  Geoparsing is the task of estimating the latitude and longitude (coordinates)
of location expressions in texts. Geoparsing must deal with the ambiguity of
the expressions that indicate multiple locations with the same notation. For
evaluating geoparsing systems, several corpora have been proposed in previous
work. However, these corpora are small-scale and suffer from the coverage of
location expressions on general domains. In this paper, we propose Wikipedia
Hyperlink-based Location Linking (WHLL), a novel method to construct a
large-scale corpus for geoparsing from Wikipedia articles. WHLL leverages
hyperlinks in Wikipedia to annotate multiple location expressions with
coordinates. With this method, we constructed the WHLL corpus, a new
large-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles,
each containing about 7.8 unique location expressions. 45.6% of location
expressions are ambiguous and refer to more than one location with the same
notation. In each article, location expressions of the article title and those
hyperlinks to other articles are assigned with coordinates. By utilizing
hyperlinks, we can accurately assign location expressions with coordinates even
with ambiguous location expressions in the texts. Experimental results show
that there remains room for improvement by disambiguating location expressions.
\\ ( https://arxiv.org/abs/2403.16483 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16504
Date: Mon, 25 Mar 2024 07:38:40 GMT   (1632kb,D)

Title: LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent
  Classification
Authors: Liu Junhua, Tan Yong Keat, Fu Bin
Categories: cs.CL cs.IR
\\
  Following the significant achievements of large language models (LLMs),
researchers have employed in-context learning for text classification tasks.
However, these studies focused on monolingual, single-turn classification
tasks. In this paper, we introduce LARA (Linguistic-Adaptive
Retrieval-Augmented Language Models), designed to enhance accuracy in
multi-turn classification tasks across six languages, accommodating numerous
intents in chatbot interactions. Multi-turn intent classification is notably
challenging due to the complexity and evolving nature of conversational
contexts. LARA tackles these issues by combining a fine-tuned smaller model
with a retrieval-augmented mechanism, integrated within the architecture of
LLMs. This integration allows LARA to dynamically utilize past dialogues and
relevant intents, thereby improving the understanding of the context.
Furthermore, our adaptive retrieval techniques bolster the cross-lingual
capabilities of LLMs without extensive retraining and fine-tune. Comprehensive
experiments demonstrate that LARA achieves state-of-the-art performance on
multi-turn intent classification tasks, enhancing the average accuracy by 3.67%
compared to existing methods.
\\ ( https://arxiv.org/abs/2403.16504 ,  1632kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16512
Date: Mon, 25 Mar 2024 07:55:29 GMT   (10055kb,D)

Title: LLMs Are Few-Shot In-Context Low-Resource Language Learners
Authors: Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
Categories: cs.CL cs.AI
\\
  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages.
\\ ( https://arxiv.org/abs/2403.16512 ,  10055kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16516
Date: Mon, 25 Mar 2024 08:00:43 GMT   (16909kb,D)

Title: Visually Guided Generative Text-Layout Pre-training for Document
  Intelligence
Authors: Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu,
  Kam-Fai Wong
Categories: cs.CL cs.CV
Comments: Accepted to NAACL 2024 main conference. The first version of this
  paper was submitted to OpenReview
  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023
\\
  Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.
\\ ( https://arxiv.org/abs/2403.16516 ,  16909kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16543
Date: Mon, 25 Mar 2024 08:36:06 GMT   (7987kb,D)

Title: Efficient Information Extraction in Few-Shot Relation Classification
  through Contrastive Representation Learning
Authors: Philipp Borchert, Jochen De Weerdt, Marie-Francine Moens
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\
  Differentiating relationships between entity pairs with limited labeled
instances poses a significant challenge in few-shot relation classification.
Representations of textual data extract rich information spanning the domain,
entities, and relations. In this paper, we introduce a novel approach to
enhance information extraction combining multiple sentence representations and
contrastive learning. While representations in relation classification are
commonly extracted using entity marker tokens, we argue that substantial
information within the internal model representations remains untapped. To
address this, we propose aligning multiple sentence representations, such as
the [CLS] token, the [MASK] token used in prompting, and entity marker tokens.
Our method employs contrastive learning to extract complementary discriminative
information from these individual representations. This is particularly
relevant in low-resource settings where information is scarce. Leveraging
multiple sentence representations is especially effective in distilling
discriminative information for relation classification when additional
information, like relation descriptions, are not available. We validate the
adaptability of our approach, maintaining robust performance in scenarios that
include relation descriptions, and showcasing its flexibility to adapt to
different resource constraints.
\\ ( https://arxiv.org/abs/2403.16543 ,  7987kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16554
Date: Mon, 25 Mar 2024 09:04:14 GMT   (373kb,D)

Title: PE: A Poincare Explanation Method for Fast Text Hierarchy Generation
Authors: Qian Chen, Xiaofeng He, Hongzhao Li, Hongyu Yi
Categories: cs.CL cs.AI
Comments: 12 pages, 10 figures
\\
  The black-box nature of deep learning models in NLP hinders their widespread
application. The research focus has shifted to Hierarchical Attribution (HA)
for its ability to model feature interactions. Recent works model
non-contiguous combinations with a time-costly greedy search in Eculidean
spaces, neglecting underlying linguistic information in feature
representations. In this work, we introduce a novel method, namely Poincar\'e
Explanation (PE), for modeling feature interactions using hyperbolic spaces in
an $O(n^2logn)$ time complexity. Inspired by Poincar\'e model, we propose a
framework to project the embeddings into hyperbolic spaces, which exhibit
better inductive biases for syntax and semantic hierarchical structures.
Eventually, we prove that the hierarchical clustering process in the projected
space could be viewed as building a minimum spanning tree and propose a time
efficient algorithm. Experimental results demonstrate the effectiveness of our
approach.
\\ ( https://arxiv.org/abs/2403.16554 ,  373kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16571
Date: Mon, 25 Mar 2024 09:36:51 GMT   (926kb,D)

Title: NSINA: A News Corpus for Sinhala
Authors: Hansi Hettiarachchi, Damith Premasiri, Lasitha Uyangodage, Tharindu
  Ranasinghe
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)
\\
  The introduction of large language models (LLMs) has advanced natural
language processing (NLP), but their effectiveness is largely dependent on
pre-training resources. This is especially evident in low-resource languages,
such as Sinhala, which face two primary challenges: the lack of substantial
training data and limited benchmarking datasets. In response, this study
introduces NSINA, a comprehensive news corpus of over 500,000 articles from
popular Sinhala news websites, along with three NLP tasks: news media
identification, news category prediction, and news headline generation. The
release of NSINA aims to provide a solution to challenges in adapting LLMs to
Sinhala, offering valuable resources and benchmarks for improving NLP in the
Sinhala language. NSINA is the largest news corpus for Sinhala, available up to
date.
\\ ( https://arxiv.org/abs/2403.16571 ,  926kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16584
Date: Mon, 25 Mar 2024 09:51:54 GMT   (55kb,D)

Title: Can Large Language Models (or Humans) Distill Text?
Authors: Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak, Moa
  Johansson, Richard Johansson
Categories: cs.CL
\\
  We investigate the potential of large language models (LLMs) to distill text:
to remove the textual traces of an undesired forbidden variable. We employ a
range of LLMs with varying architectures and training approaches to distill
text by identifying and removing information about the target variable while
preserving other relevant signals. Our findings shed light on the strengths and
limitations of LLMs in addressing the distillation and provide insights into
the strategies for leveraging these models in computational social science
investigations involving text data. In particular, we show that in the strong
test of removing sentiment, the statistical association between the processed
text and sentiment is still clearly detectable to machine learning classifiers
post-LLM-distillation. Furthermore, we find that human annotators also struggle
to distill sentiment while preserving other semantic content. This suggests
there may be limited separability between concept variables in some text
contexts, highlighting limitations of methods relying on text-level
transformations and also raising questions about the robustness of distillation
methods that achieve statistical independence in representation space if this
is difficult for human coders operating on raw text to attain.
\\ ( https://arxiv.org/abs/2403.16584 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16592
Date: Mon, 25 Mar 2024 10:09:03 GMT   (7820kb,D)

Title: TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain
  Machine Generated Text Detection Techniques
Authors: Ashok Urlana, Aditya Saibewar, Bala Mallikarjunarao Garlapati, Charaka
  Vinayak Kumar, Ajeet Kumar Singh, Srinivasa Rao Chalamala
Categories: cs.CL
Comments: 8 pages, 1 Figure
ACM-class: I.2.7
\\
  The Large Language Models (LLMs) exhibit remarkable ability to generate
fluent content across a wide spectrum of user queries. However, this capability
has raised concerns regarding misinformation and personal information leakage.
In this paper, we present our methods for the SemEval2024 Task8, aiming to
detect machine-generated text across various domains in both mono-lingual and
multi-lingual contexts. Our study comprehensively analyzes various methods to
detect machine-generated text, including statistical, neural, and pre-trained
model approaches. We also detail our experimental setup and perform a in-depth
error analysis to evaluate the effectiveness of these methods. Our methods
obtain an accuracy of 86.9\% on the test set of subtask-A mono and 83.7\% for
subtask-B. Furthermore, we also highlight the challenges and essential factors
for consideration in future studies.
\\ ( https://arxiv.org/abs/2403.16592 ,  7820kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16609
Date: Mon, 25 Mar 2024 10:39:18 GMT   (1400kb,D)

Title: Conversational Grounding: Annotation and Analysis of Grounding Acts and
  Grounding Units
Authors: Biswesh Mohapatra, Seemab Hassan, Laurent Romary and Justine Cassell
Categories: cs.CL
Journal-ref: LREC-COLING 2024
\\
  Successful conversations often rest on common understanding, where all
parties are on the same page about the information being shared. This process,
known as conversational grounding, is crucial for building trustworthy dialog
systems that can accurately keep track of and recall the shared information.
The proficiencies of an agent in grounding the conveyed information
significantly contribute to building a reliable dialog system. Despite recent
advancements in dialog systems, there exists a noticeable deficit in their
grounding capabilities. Traum provided a framework for conversational grounding
introducing Grounding Acts and Grounding Units, but substantial progress,
especially in the realm of Large Language Models, remains lacking. To bridge
this gap, we present the annotation of two dialog corpora employing Grounding
Acts, Grounding Units, and a measure of their degree of grounding. We discuss
our key findings during the annotation and also provide a baseline model to
test the performance of current Language Models in categorizing the grounding
acts of the dialogs. Our work aims to provide a useful resource for further
research in making conversations with machines better understood and more
reliable in natural day-to-day collaborative dialogs.
\\ ( https://arxiv.org/abs/2403.16609 ,  1400kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16614
Date: Mon, 25 Mar 2024 10:44:38 GMT   (361kb,D)

Title: Semantically Enriched Cross-Lingual Sentence Embeddings for
  Crisis-related Social Media Texts
Authors: Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera
Categories: cs.CL
Comments: Accepted to ISCRAM 2024
\\
  Tasks such as semantic search and clustering on crisis-related social media
texts enhance our comprehension of crisis discourse, aiding decision-making and
targeted interventions. Pre-trained language models have advanced performance
in crisis informatics, but their contextual embeddings lack semantic
meaningfulness. Although the CrisisTransformers family includes a sentence
encoder to address the semanticity issue, it remains monolingual, processing
only English texts. Furthermore, employing separate models for different
languages leads to embeddings in distinct vector spaces, introducing challenges
when comparing semantic similarities between multi-lingual texts. Therefore, we
propose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed
crisis-related social media texts for over 50 languages, such that texts with
similar meanings are in close proximity within the same vector space,
irrespective of language diversity. Results in sentence encoding and sentence
matching tasks are promising, suggesting these models could serve as robust
baselines when embedding multi-lingual crisis-related social media texts. The
models are publicly available at: https://huggingface.co/crisistransformers.
\\ ( https://arxiv.org/abs/2403.16614 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16630
Date: Mon, 25 Mar 2024 11:20:23 GMT   (40kb)

Title: A comparative analysis of embedding models for patent similarity
Authors: Grazia Sveva Ascione and Valerio Sterzi
Categories: cs.CL cs.IR cs.LG
\\
  This paper makes two contributions to the field of text-based patent
similarity. First, it compares the performance of different kinds of
patent-specific pretrained embedding models, namely static word embeddings
(such as word2vec and doc2vec models) and contextual word embeddings (such as
transformers based models), on the task of patent similarity calculation.
Second, it compares specifically the performance of Sentence Transformers
(SBERT) architectures with different training phases on the patent similarity
task. To assess the models' performance, we use information about patent
interferences, a phenomenon in which two or more patent claims belonging to
different patent applications are proven to be overlapping by patent examiners.
Therefore, we use these interferences cases as a proxy for maximum similarity
between two patents, treating them as ground-truth to evaluate the performance
of the different embedding models. Our results point out that, first, Patent
SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer
architecture proposed in this research, outperforms the current
state-of-the-art in patent similarity. Second, they show that, in some cases,
large static models performances are still comparable to contextual ones when
trained on extensive data; thus, we believe that the superiority in the
performance of contextual embeddings may not be related to the actual
architecture but rather to the way the training phase is performed.
\\ ( https://arxiv.org/abs/2403.16630 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16655
Date: Mon, 25 Mar 2024 11:45:21 GMT   (749kb)

Title: Grammatical vs Spelling Error Correction: An Investigation into the
  Responsiveness of Transformer-based Language Models using BART and MarianMT
Authors: Rohit Raju, Peeta Basa Pati, SA Gandheesh, Gayatri Sanjana Sannala and
  Suriya KS
Categories: cs.CL
Journal-ref: Journal of Information & Knowledge Management, 2024, World
  Scientific
DOI: 10.1142/S0219649224500370
\\
  Text continues to remain a relevant form of representation for information.
Text documents are created either in digital native platforms or through the
conversion of other media files such as images and speech. While the digital
native text is invariably obtained through physical or virtual keyboards,
technologies such as OCR and speech recognition are utilized to transform the
images and speech signals into text content. All these variety of mechanisms of
text generation also introduce errors into the captured text.
  This project aims at analyzing different kinds of error that occurs in text
documents. The work employs two of the advanced deep neural network-based
language models, namely, BART and MarianMT, to rectify the anomalies present in
the text. Transfer learning of these models with available dataset is performed
to finetune their capacity for error correction. A comparative study is
conducted to investigate the effectiveness of these models in handling each of
the defined error categories. It is observed that while both models can bring
down the erroneous sentences by 20+%, BART can handle spelling errors far
better (24.6%) than grammatical errors (8.8%).
\\ ( https://arxiv.org/abs/2403.16655 ,  749kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16662
Date: Mon, 25 Mar 2024 11:56:29 GMT   (4089kb,D)

Title: RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking
  on Russia-Ukraine Conflict
Authors: Yirong Zeng, Xiao Ding, Yi Zhao, Xiangyu Li, Jie Zhang, Chao Yao, Ting
  Liu and Bing Qin
Categories: cs.CL
Comments: 12 pages, 3 figures, accepted by lrec-coling2024
\\
  Fact-checking is the task of verifying the factuality of a given claim by
examining the available evidence. High-quality evidence plays a vital role in
enhancing fact-checking systems and facilitating the generation of explanations
that are understandable to humans. However, the provision of both sufficient
and relevant evidence for explainable fact-checking systems poses a challenge.
To tackle this challenge, we propose a method based on a Large Language Model
to automatically retrieve and summarize evidence from the Web. Furthermore, we
construct RU22Fact, a novel multilingual explainable fact-checking dataset on
the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world
claims, optimized evidence, and referenced explanation. To establish a baseline
for our dataset, we also develop an end-to-end explainable fact-checking system
to verify claims and generate explanations. Experimental results demonstrate
the prospect of optimized evidence in increasing fact-checking performance and
also indicate the possibility of further progress in the end-to-end claim
verification and explanation generation tasks.
\\ ( https://arxiv.org/abs/2403.16662 ,  4089kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16668
Date: Mon, 25 Mar 2024 12:07:21 GMT   (1862kb,D)

Title: Who is bragging more online? A large scale analysis of bragging in
  social media
Authors: Mali Jin, Daniel Preo\c{t}iuc-Pietro, A. Seza Do\u{g}ru\"oz, Nikolaos
  Aletras
Categories: cs.CL cs.SI
Comments: Accepted at LREC-COLING 2024
\\
  Bragging is the act of uttering statements that are likely to be positively
viewed by others and it is extensively employed in human communication with the
aim to build a positive self-image of oneself. Social media is a natural
platform for users to employ bragging in order to gain admiration, respect,
attention and followers from their audiences. Yet, little is known about the
scale of bragging online and its characteristics. This paper employs
computational sociolinguistics methods to conduct the first large scale study
of bragging behavior on Twitter (U.S.) by focusing on its overall prevalence,
temporal dynamics and impact of demographic factors. Our study shows that the
prevalence of bragging decreases over time within the same population of users.
In addition, younger, more educated and popular users in the U.S. are more
likely to brag. Finally, we conduct an extensive linguistics analysis to unveil
specific bragging themes associated with different user traits.
\\ ( https://arxiv.org/abs/2403.16668 ,  1862kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16685
Date: Mon, 25 Mar 2024 12:21:38 GMT   (138kb,D)

Title: ToXCL: A Unified Framework for Toxic Speech Detection and Explanation
Authors: Nhat M. Hoang, Xuan Long Do, Duc Anh Do, Duc Anh Vu, Luu Anh Tuan
Categories: cs.CL cs.CY
Comments: Accepted at NAACL 2024 (Main Conference)
\\
  The proliferation of online toxic speech is a pertinent problem posing
threats to demographic groups. While explicit toxic speech contains offensive
lexical signals, implicit one consists of coded or indirect language.
Therefore, it is crucial for models not only to detect implicit toxic speech
but also to explain its toxicity. This draws a unique need for unified
frameworks that can effectively detect and explain implicit toxic speech. Prior
works mainly formulated the task of toxic speech detection and explanation as a
text generation problem. Nonetheless, models trained using this strategy can be
prone to suffer from the consequent error propagation problem. Moreover, our
experiments reveal that the detection results of such models are much lower
than those that focus only on the detection task. To bridge these gaps, we
introduce ToXCL, a unified framework for the detection and explanation of
implicit toxic speech. Our model consists of three modules: a (i) Target Group
Generator to generate the targeted demographic group(s) of a given post; an
(ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit
toxic speech and is boosted by a (iii) Teacher Classifier via knowledge
distillation, and the decoder generates the necessary explanation. ToXCL
achieves new state-of-the-art effectiveness, and outperforms baselines
significantly.
\\ ( https://arxiv.org/abs/2403.16685 ,  138kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16702
Date: Mon, 25 Mar 2024 12:34:33 GMT   (900kb,D)

Title: ProCQA: A Large-scale Community-based Programming Question Answering
  Dataset for Code Search
Authors: Zehan Li, Jianfei Zhang, Chuantao Yin, Yuanxin Ouyang, Wenge Rong
Categories: cs.CL cs.IR cs.SE
Comments: Accepted to LREC-COLING 2024
\\
  Retrieval-based code question answering seeks to match user queries in
natural language to relevant code snippets. Previous approaches typically rely
on pretraining models using crafted bi-modal and uni-modal datasets to align
text and code representations. In this paper, we introduce ProCQA, a
large-scale programming question answering dataset extracted from the
StackOverflow community, offering naturally structured mixed-modal QA pairs. To
validate its effectiveness, we propose a modality-agnostic contrastive
pre-training approach to improve the alignment of text and code representations
of current code language models. Compared to previous models that primarily
employ bimodal and unimodal pairs extracted from CodeSearchNet for
pre-training, our model exhibits significant performance improvements across a
wide range of code retrieval benchmarks.
\\ ( https://arxiv.org/abs/2403.16702 ,  900kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16771
Date: Mon, 25 Mar 2024 13:50:11 GMT   (372kb)

Title: Synthetic Data Generation and Joint Learning for Robust Code-Mixed
  Translation
Authors: Kartik, Sanjana Soni, Anoop Kunchukuttan, Tanmoy Chakraborty, Md Shad
  Akhtar
Categories: cs.CL cs.LG
Comments: 9 pages, 2 figures, to be published in LREC-COLING 2024
\\
  The widespread online communication in a modern multilingual world has
provided opportunities to blend more than one language (aka code-mixed
language) in a single utterance. This has resulted a formidable challenge for
the computational models due to the scarcity of annotated data and presence of
noise. A potential solution to mitigate the data scarcity problem in
low-resource setup is to leverage existing data in resource-rich language
through translation. In this paper, we tackle the problem of code-mixed
(Hinglish and Bengalish) to English machine translation. First, we
synthetically develop HINMIX, a parallel corpus of Hinglish to English, with
~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation
based joint-training model that learns to handle noise in the real-world
code-mixed text by parameter sharing across clean and noisy words. Further, we
show the adaptability of RCMT in a zero-shot setup for Bengalish to English
translation. Our evaluation and comprehensive analyses qualitatively and
quantitatively demonstrate the superiority of RCMT over state-of-the-art
code-mixed and robust translation methods.
\\ ( https://arxiv.org/abs/2403.16771 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16777
Date: Mon, 25 Mar 2024 13:53:04 GMT   (1253kb,D)

Title: Can Machine Translation Bridge Multilingual Pretraining and
  Cross-lingual Transfer Learning?
Authors: Shaoxiong Ji, Timothee Mickus, Vincent Segonne, J\"org Tiedemann
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  Multilingual pretraining and fine-tuning have remarkably succeeded in various
natural language processing tasks. Transferring representations from one
language to another is especially crucial for cross-lingual learning. One can
expect machine translation objectives to be well suited to fostering such
capabilities, as they involve the explicit alignment of semantically equivalent
sentences from different languages. This paper investigates the potential
benefits of employing machine translation as a continued training objective to
enhance language representation learning, bridging multilingual pretraining and
cross-lingual applications. We study this question through two lenses: a
quantitative evaluation of the performance of existing models and an analysis
of their latent representations. Our results show that, contrary to
expectations, machine translation as the continued training fails to enhance
cross-lingual representation learning in multiple cross-lingual natural
language understanding tasks. We conclude that explicit sentence-level
alignment in the cross-lingual scenario is detrimental to cross-lingual
transfer pretraining, which has important implications for future cross-lingual
transfer studies. We furthermore provide evidence through similarity measures
and investigation of parameters that this lack of positive influence is due to
output separability -- which we argue is of use for machine translation but
detrimental elsewhere.
\\ ( https://arxiv.org/abs/2403.16777 ,  1253kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16792
Date: Mon, 25 Mar 2024 14:07:27 GMT   (4725kb,D)

Title: Iterative Refinement of Project-Level Code Context for Precise Code
  Generation with Compiler Feedback
Authors: Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin
  Lu, Zili Zhang, Yulei Sui, Xuanhua Shi, Hai Jin
Categories: cs.CL cs.SE
\\
  Large language models (LLMs) have shown remarkable progress in automated code
generation. Yet, incorporating LLM-based code generation into real-life
software projects poses challenges, as the generated code may contain errors in
API usage, class, data structure, or missing project-specific information. As
much of this project-specific context cannot fit into the prompts of LLMs, we
must find ways to allow the model to explore the project-level code context. To
this end, this paper puts forward a novel approach, termed ProCoder, which
iteratively refines the project-level code context for precise code generation,
guided by the compiler feedback. In particular, ProCoder first leverages
compiler techniques to identify a mismatch between the generated code and the
project's context. It then iteratively aligns and fixes the identified errors
using information extracted from the code repository. We integrate ProCoder
with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and
apply it to Python code generation. Experimental results show that ProCoder
significantly improves the vanilla LLMs by over 80% in generating code
dependent on project context, and consistently outperforms the existing
retrieval-based code generation baselines.
\\ ( https://arxiv.org/abs/2403.16792 ,  4725kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16804
Date: Mon, 25 Mar 2024 14:23:03 GMT   (839kb)

Title: TEI2GO: A Multilingual Approach for Fast Temporal Expression
  Identification
Authors: Hugo Sousa, Ricardo Campos, Al\'ipio Jorge
Categories: cs.CL
DOI: 10.1145/3583780.3615130
\\
  Temporal expression identification is crucial for understanding texts written
in natural language. Although highly effective systems such as HeidelTime
exist, their limited runtime performance hampers adoption in large-scale
applications and production environments. In this paper, we introduce the
TEI2GO models, matching HeidelTime's effectiveness but with significantly
improved runtime, supporting six languages, and achieving state-of-the-art
results in four of them. To train the TEI2GO models, we used a combination of
manually annotated reference corpus and developed ``Professor HeidelTime'', a
comprehensive weakly labeled corpus of news texts annotated with HeidelTime.
This corpus comprises a total of $138,069$ documents (over six languages) with
$1,050,921$ temporal expressions, the largest open-source annotated dataset for
temporal expression identification to date. By describing how the models were
produced, we aim to encourage the research community to further explore,
refine, and extend the set of models to additional languages and domains. Code,
annotations, and models are openly available for community exploration and use.
The models are conveniently on HuggingFace for seamless integration and
application.
\\ ( https://arxiv.org/abs/2403.16804 ,  839kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16820
Date: Mon, 25 Mar 2024 14:46:51 GMT   (8156kb,D)

Title: Cross-lingual Contextualized Phrase Retrieval
Authors: Huayang Li, Deng Cai, Zhi Qu, Qu Cui, Hidetaka Kamigaito, Lemao Liu,
  Taro Watanabe
Categories: cs.CL
Comments: preprint
\\
  Phrase-level dense retrieval has shown many appealing characteristics in
downstream NLP tasks by leveraging the fine-grained information that phrases
offer. In our work, we propose a new task formulation of dense retrieval,
cross-lingual contextualized phrase retrieval, which aims to augment
cross-lingual applications by addressing polysemy using context information.
However, the lack of specific training data and models are the primary
challenges to achieve our goal. As a result, we extract pairs of cross-lingual
phrases using word alignment information automatically induced from parallel
sentences. Subsequently, we train our Cross-lingual Contextualized Phrase
Retriever (CCPR) using contrastive learning, which encourages the hidden
representations of phrases with similar contexts and semantics to align
closely. Comprehensive experiments on both the cross-lingual phrase retrieval
task and a downstream task, i.e, machine translation, demonstrate the
effectiveness of CCPR. On the phrase retrieval task, CCPR surpasses baselines
by a significant margin, achieving a top-1 accuracy that is at least 13 points
higher. When utilizing CCPR to augment the large-language-model-based
translator, it achieves average gains of 0.7 and 1.5 in BERTScore for
translations from X=>En and vice versa, respectively, on WMT16 dataset. Our
code and data are available at \url{https://github.com/ghrua/ccpr_release}.
\\ ( https://arxiv.org/abs/2403.16820 ,  8156kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16852
Date: Mon, 25 Mar 2024 15:15:41 GMT   (8008kb,D)

Title: Towards Explainability in Legal Outcome Prediction Models
Authors: Josef Valvoda, Ryan Cotterell
Categories: cs.CL cs.AI
\\
  Current legal outcome prediction models - a staple of legal NLP - do not
explain their reasoning. However, to employ these models in the real world,
human legal actors need to be able to understand their decisions. In the case
of common law, legal practitioners reason towards the outcome of a case by
referring to past case law, known as precedent. We contend that precedent is,
therefore, a natural way of facilitating explainability for legal NLP models.
In this paper, we contribute a novel method for identifying the precedent
employed by legal outcome prediction models. Furthermore, by developing a
taxonomy of legal precedent, we are able to compare human judges and our models
with respect to the different types of precedent they rely on. We find that
while the models learn to predict outcomes reasonably well, their use of
precedent is unlike that of human judges.
\\ ( https://arxiv.org/abs/2403.16852 ,  8008kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16854
Date: Mon, 25 Mar 2024 15:17:05 GMT   (449kb,D)

Title: An Expert is Worth One Token: Synergizing Multiple Expert LLMs as
  Generalist via Expert Token Routing
Authors: Ziwei Chai, Guoyin Wang, Jing Su, Tianjie Zhang, Xuanwen Huang, Xuwu
  Wang, Jingjing Xu, Jianbo Yuan, Hongxia Yang, Fei Wu, Yang Yang
Categories: cs.CL cs.AI
\\
  We present Expert-Token-Routing, a unified generalist framework that
facilitates seamless integration of multiple expert LLMs. Our framework
represents expert LLMs as special expert tokens within the vocabulary of a meta
LLM. The meta LLM can route to an expert LLM like generating new tokens.
Expert-Token-Routing not only supports learning the implicit expertise of
expert LLMs from existing instruction dataset but also allows for dynamic
extension of new expert LLMs in a plug-and-play manner. It also conceals the
detailed collaboration process from the user's perspective, facilitating
interaction as though it were a singular LLM. Our framework outperforms various
existing multi-LLM collaboration paradigms across benchmarks that incorporate
six diverse expert domains, demonstrating effectiveness and robustness in
building generalist LLM system via synergizing multiple expert LLMs.
\\ ( https://arxiv.org/abs/2403.16854 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16865
Date: Mon, 25 Mar 2024 15:28:38 GMT   (886kb,D)

Title: Encoding of lexical tone in self-supervised models of spoken language
Authors: Gaofei Shen, Michaela Watkins, Afra Alishahi, Arianna Bisazza,
  Grzegorz Chrupa{\l}a
Categories: cs.CL eess.AS
Comments: Accepted to NAACL 2024
\\
  Interpretability research has shown that self-supervised Spoken Language
Models (SLMs) encode a wide variety of features in human speech from the
acoustic, phonetic, phonological, syntactic and semantic levels, to speaker
characteristics. The bulk of prior research on representations of phonology has
focused on segmental features such as phonemes; the encoding of suprasegmental
phonology (such as tone and stress patterns) in SLMs is not yet well
understood. Tone is a suprasegmental feature that is present in more than half
of the world's languages. This paper aims to analyze the tone encoding
capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show
that SLMs encode lexical tone to a significant degree even when they are
trained on data from non-tonal languages. We further find that SLMs behave
similarly to native and non-native human participants in tone and consonant
perception studies, but they do not follow the same developmental trajectory.
\\ ( https://arxiv.org/abs/2403.16865 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16913
Date: Mon, 25 Mar 2024 16:31:55 GMT   (2169kb,D)

Title: New Intent Discovery with Attracting and Dispersing Prototype
Authors: Shun Zhang, Jian Yang, Jiaqi Bai, Chaoran Yan, Tongliang Li, Zhao Yan,
  Zhoujun Li
Categories: cs.CL
Comments: COLING 2024
\\
  New Intent Discovery (NID) aims to recognize known and infer new intent
categories with the help of limited labeled and large-scale unlabeled data. The
task is addressed as a feature-clustering problem and recent studies augment
instance representation. However, existing methods fail to capture
cluster-friendly representations, since they show less capability to
effectively control and coordinate within-cluster and between-cluster
distances. Tailored to the NID problem, we propose a Robust and Adaptive
Prototypical learning (RAP) framework for globally distinct decision boundaries
for both known and new intent categories. Specifically, a robust prototypical
attracting learning (RPAL) method is designed to compel instances to gravitate
toward their corresponding prototype, achieving greater within-cluster
compactness. To attain larger between-cluster separation, another adaptive
prototypical dispersing learning (APDL) method is devised to maximize the
between-cluster distance from the prototype-to-prototype perspective.
Experimental results evaluated on three challenging benchmarks (CLINC, BANKING,
and StackOverflow) of our method with better cluster-friendly representation
demonstrate that RAP brings in substantial improvements over the current
state-of-the-art methods (even large language model) by a large margin (average
+5.5% improvement).
\\ ( https://arxiv.org/abs/2403.16913 ,  2169kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16941
Date: Mon, 25 Mar 2024 17:04:02 GMT   (2439kb,D)

Title: SPACE-IDEAS: A Dataset for Salient Information Detection in Space
  Innovation
Authors: Andr\'es Garc\'ia-Silva, Cristian Berr\'io, Jos\'e Manuel
  G\'omez-P\'erez
Categories: cs.CL cs.AI cs.DL
Comments: Accepted in LREC-COLING 2024
\\
  Detecting salient parts in text using natural language processing has been
widely used to mitigate the effects of information overflow. Nevertheless, most
of the datasets available for this task are derived mainly from academic
publications. We introduce SPACE-IDEAS, a dataset for salient information
detection from innovation ideas related to the Space domain. The text in
SPACE-IDEAS varies greatly and includes informal, technical, academic and
business-oriented writing styles. In addition to a manually annotated dataset
we release an extended version that is annotated using a large generative
language model. We train different sentence and sequential sentence
classifiers, and show that the automatically annotated dataset can be leveraged
using multitask learning to train better classifiers.
\\ ( https://arxiv.org/abs/2403.16941 ,  2439kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16950
Date: Mon, 25 Mar 2024 17:11:28 GMT   (3373kb,D)

Title: Aligning with Human Judgement: The Role of Pairwise Preference in Large
  Language Model Evaluators
Authors: Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna
  Korhonen and Nigel Collier
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs) have demonstrated promising capabilities as
automatic evaluators in assessing the quality of generated natural language.
However, LLMs still exhibit biases in evaluation and often struggle to generate
coherent evaluations that align with human assessments. In this work, we first
conduct a systematic study of the misalignment between LLM evaluators and human
judgement, revealing that existing calibration methods aimed at mitigating
biases are insufficient for effectively aligning LLM evaluators. Inspired by
the use of preference data in RLHF, we formulate the evaluation as a ranking
problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided
search method that employs LLMs to conduct pairwise comparisons and efficiently
ranks candidate texts. PAIRS achieves state-of-the-art performance on
representative evaluation tasks and demonstrates significant improvements over
direct scoring. Furthermore, we provide insights into the role of pairwise
preference in quantifying the transitivity of LLMs and demonstrate how PAIRS
benefits from calibration.
\\ ( https://arxiv.org/abs/2403.16950 ,  3373kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16952
Date: Mon, 25 Mar 2024 17:14:00 GMT   (5412kb,D)

Title: Data Mixing Laws: Optimizing Data Mixtures by Predicting Language
  Modeling Performance
Authors: Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, Xipeng
  Qiu
Categories: cs.CL cs.AI cs.LG
\\
  Pretraining data of large language models composes multiple domains (e.g.,
web texts, academic papers, codes), whose mixture proportions crucially impact
the competence of outcome models. While existing endeavors rely on heuristics
or qualitative strategies to tune the proportions, we discover the quantitative
predictability of model performance regarding the mixture proportions in
function forms, which we refer to as the data mixing laws. Fitting such
functions on sample mixtures unveils model performance on unseen mixtures
before actual runs, thus guiding the selection of an ideal data mixture.
Furthermore, we propose nested use of the scaling laws of training steps, model
sizes, and our data mixing law to enable predicting the performance of large
models trained on massive data under various mixtures with only small-scale
training. Moreover, experimental results verify that our method effectively
optimizes the training mixture of a 1B model trained for 100B tokens in
RedPajama, reaching a performance comparable to the one trained for 48% more
steps on the default mixture. Extending the application of data mixing laws to
continual training accurately predicts the critical mixture proportion that
avoids catastrophic forgetting and outlooks the potential for dynamic data
schedules
\\ ( https://arxiv.org/abs/2403.16952 ,  5412kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16968
Date: Mon, 25 Mar 2024 17:28:24 GMT   (6877kb)

Title: Evaluating Shortest Edit Script Methods for Contextual Lemmatization
Authors: Olia Toporkov, Rodrigo Agerri
Categories: cs.CL
Comments: 13 pages, 7 tables. Accepted to LREC-COLING 2024
\\
  Modern contextual lemmatizers often rely on automatically induced Shortest
Edit Scripts (SES), namely, the number of edit operations to transform a word
form into its lemma. In fact, different methods of computing SES have been
proposed as an integral component in the architecture of several
state-of-the-art contextual lemmatizers currently available. However, previous
work has not investigated the direct impact of SES in the final lemmatization
performance. In this paper we address this issue by focusing on lemmatization
as a token classification task where the only input that the model receives is
the word-label pairs in context, where the labels correspond to previously
induced SES. Thus, by modifying in our lemmatization system only the SES labels
that the model needs to learn, we may then objectively conclude which SES
representation produces the best lemmatization results. We experiment with
seven languages of different morphological complexity, namely, English,
Spanish, Basque, Russian, Czech, Turkish and Polish, using multilingual and
language-specific pre-trained masked language encoder-only models as a backbone
to build our lemmatizers. Comprehensive experimental results, both in- and
out-of-domain, indicate that computing the casing and edit operations
separately is beneficial overall, but much more clearly for languages with
high-inflected morphology. Notably, multilingual pre-trained language models
consistently outperform their language-specific counterparts in every
evaluation setting.
\\ ( https://arxiv.org/abs/2403.16968 ,  6877kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16977
Date: Mon, 25 Mar 2024 17:41:02 GMT   (594kb,D)

Title: A comparison of Human, GPT-3.5, and GPT-4 Performance in a
  University-Level Coding Course
Authors: Will Yeadon, Alex Peach, Craig P. Testrow
Categories: cs.CL
Comments: 9 pages, 3 figures
\\
  This study evaluates the performance of ChatGPT variants, GPT-3.5 and GPT-4,
both with and without prompt engineering, against solely student work and a
mixed category containing both student and GPT-4 contributions in
university-level physics coding assignments using the Python language.
Comparing 50 student submissions to 50 AI-generated submissions across
different categories, and marked blindly by three independent markers, we
amassed $n = 300$ data points. Students averaged 91.9% (SE:0.4), surpassing the
highest performing AI submission category, GPT-4 with prompt engineering, which
scored 81.1% (SE:0.8) - a statistically significant difference (p = $2.482
\times 10^{-10}$). Prompt engineering significantly improved scores for both
GPT-4 (p = $1.661 \times 10^{-4}$) and GPT-3.5 (p = $4.967 \times 10^{-9}$).
Additionally, the blinded markers were tasked with guessing the authorship of
the submissions on a four-point Likert scale from `Definitely AI' to
`Definitely Human'. They accurately identified the authorship, with 92.1% of
the work categorized as 'Definitely Human' being human-authored. Simplifying
this to a binary `AI' or `Human' categorization resulted in an average accuracy
rate of 85.3%. These findings suggest that while AI-generated work closely
approaches the quality of university students' work, it often remains
detectable by human evaluators.
\\ ( https://arxiv.org/abs/2403.16977 ,  594kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16995
Date: Mon, 25 Mar 2024 17:58:22 GMT   (188kb,D)

Title: Language Rectified Flow: Advancing Diffusion Language Generation with
  Probabilistic Flows
Authors: Shujian Zhang, Lemeng Wu, Chengyue Gong, Xingchao Liu
Categories: cs.CL cs.AI cs.LG stat.ML
Comments: Accepted to NAACL 2024
\\
  Recent works have demonstrated success in controlling sentence attributes
($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the
diffusion language model. A key component that drives theimpressive performance
for generating high-quality samples from noise is iteratively denoise for
thousands of steps. While beneficial, the complexity of starting from the noise
and the learning steps has limited its implementation to many NLP real-world
applications. This paper proposes Language Rectified Flow ({\ours}). Our method
is based on the reformulation of the standard probabilistic flow models.
Language rectified flow learns (neural) ordinary differential equation models
to transport between the source distribution and the target distribution, hence
providing a unified and effective solution to generative modeling and domain
transfer. From the source distribution, our language rectified flow yields fast
simulation and effectively decreases the inference time. Experiments on three
challenging fine-grained control tasks and multiple high-quality text editing
show that our method consistently outperforms its baselines. Extensive
experiments and ablation studies demonstrate that our method can be general,
effective, and beneficial for many NLP tasks.
\\ ( https://arxiv.org/abs/2403.16995 ,  188kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15416
Date: Fri, 8 Mar 2024 07:47:27 GMT   (846kb,D)

Title: Fuzzy hyperparameters update in a second order optimization
Authors: Abdelaziz Bensadok, Muhammad Zeeshan Babar
Categories: cs.LG cs.AI math.OC
\\
  This research will present a hybrid approach to accelerate convergence in a
second order optimization. An online finite difference approximation of the
diagonal Hessian matrix will be introduced, along with fuzzy inferencing of
several hyperparameters. Competitive results have been achieved
\\ ( https://arxiv.org/abs/2403.15416 ,  846kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15419
Date: Sun, 10 Mar 2024 11:28:33 GMT   (777kb,D)

Title: Attention is all you need for boosting graph convolutional neural
  network
Authors: Yinwei Wu
Categories: cs.LG cs.GR cs.SI
\\
  Graph Convolutional Neural Networks (GCNs) possess strong capabilities for
processing graph data in non-grid domains. They can capture the topological
logical structure and node features in graphs and integrate them into nodes'
final representations. GCNs have been extensively studied in various fields,
such as recommendation systems, social networks, and protein molecular
structures. With the increasing application of graph neural networks, research
has focused on improving their performance while compressing their size. In
this work, a plug-in module named Graph Knowledge Enhancement and Distillation
Module (GKEDM) is proposed. GKEDM can enhance node representations and improve
the performance of GCNs by extracting and aggregating graph information via
multi-head attention mechanism. Furthermore, GKEDM can serve as an auxiliary
transferor for knowledge distillation. With a specially designed attention
distillation method, GKEDM can distill the knowledge of large teacher models
into high-performance and compact student models. Experiments on multiple
datasets demonstrate that GKEDM can significantly improve the performance of
various GCNs with minimal overhead. Furthermore, it can efficiently transfer
distilled knowledge from large teacher networks to small student networks via
attention distillation.
\\ ( https://arxiv.org/abs/2403.15419 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15426
Date: Wed, 13 Mar 2024 05:38:39 GMT   (5105kb,D)

Title: A Three-Phases SFT Hybrid Model Integrated Strong Prior Module and Data
  Overlap Estimation in the Eduation Context
Authors: Zhangquan Chen, Chunjiang Liu, Haobin Duan
Categories: cs.LG cs.AI cs.CL
Comments: 9 pages, 2 figures
ACM-class: I.2.7
\\
  In this paper, we propose an end-to-end prior-based three-phases supervised
fine-tuned model, which is proved more competitive than traditional fine-tuning
method. More specifically, our model realizes the structural disassembly and
incremental guided output of educational knowledge. To this end, we robustify
data classification of three types via a sampler and overlap estimation neural
network, and inject the preprocessing datasets into pre-trained model in three
batches for LORA fine-tuning. Then, we design a prior module couples system
prompt, vector databases, and abstract syntax tree task segmentation. Finally,
the compression method and regularization constraint are applied to the
prior-based fine-tuned model, followed by text filter at the output end to
obtain incremental guided results. Our model represents the first research
effort to truly embody the tutor role with the features of abundant educational
knowledge, step-by-step incremental guided outputs and non-disclosure of
answers. Extensive experiments report that our model also achieves
state-of-the-art in code abilities compared to open-source models, reaching an
impressive 75.10% on the HumanEval (@pass 1) benchmark. Additionally, our model
maintains strong conversational capabilities, with the 13B quantized version
achieving scores of 56.34, 50.60, and 45.27 respectively on the MMLU, C-Eval,
and AGIEval (5 shot) dialogue evaluation benchmarks.
\\ ( https://arxiv.org/abs/2403.15426 ,  5105kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15439
Date: Sat, 16 Mar 2024 14:35:03 GMT   (1340kb)

Title: Federated Learning based on Pruning and Recovery
Authors: Chengjie Ma
Categories: cs.LG
\\
  A novel federated learning training framework for heterogeneous environments
is presented, taking into account the diverse network speeds of clients in
realistic settings. This framework integrates asynchronous learning algorithms
and pruning techniques, effectively addressing the inefficiencies of
traditional federated learning algorithms in scenarios involving heterogeneous
devices, as well as tackling the staleness issue and inadequate training of
certain clients in asynchronous algorithms. Through the incremental restoration
of model size during training, the framework expedites model training while
preserving model accuracy. Furthermore, enhancements to the federated learning
aggregation process are introduced, incorporating a buffering mechanism to
enable asynchronous federated learning to operate akin to synchronous learning.
Additionally, optimizations in the process of the server transmitting the
global model to clients reduce communication overhead. Our experiments across
various datasets demonstrate that: (i) significant reductions in training time
and improvements in convergence accuracy are achieved compared to conventional
asynchronous FL and HeteroFL; (ii) the advantages of our approach are more
pronounced in scenarios with heterogeneous clients and non-IID client data.
\\ ( https://arxiv.org/abs/2403.15439 ,  1340kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15465
Date: Tue, 19 Mar 2024 19:58:46 GMT   (15669kb,D)

Title: Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and
  Markov Chains, by Using Rollout Algorithms
Authors: Yuchao Li, Dimitri Bertsekas
Categories: cs.LG cs.AI cs.CL cs.SY eess.SY
\\
  In this paper we consider a transformer with an $n$-gram structure, such as
the one underlying ChatGPT. The transformer provides next word probabilities,
which can be used to generate word sequences. We consider methods for computing
word sequences that are highly likely, based on these probabilities. Computing
the optimal (i.e., most likely) word sequence starting with a given initial
state is an intractable problem, so we propose methods to compute highly likely
sequences of $N$ words in time that is a low order polynomial in $N$ and in the
vocabulary size of the $n$-gram. These methods are based on the rollout
approach from approximate dynamic programming, a form of single policy
iteration, which can improve the performance of any given heuristic policy. In
our case we use a greedy heuristic that generates as next word one that has the
highest probability. We show with analysis, examples, and computational
experimentation that our methods are capable of generating highly likely
sequences with a modest increase in computation over the greedy heuristic.
While our analysis and experiments are focused on Markov chains of the type
arising in transformer and ChatGPT-like models, our methods apply to general
finite-state Markov chains, and related inference applications of Hidden Markov
Models (HMM), where Viterbi decoding is used extensively.
\\ ( https://arxiv.org/abs/2403.15465 ,  15669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15498
Date: Thu, 21 Mar 2024 18:53:23 GMT   (449kb,D)

Title: Emergent World Models and Latent Variable Estimation in Chess-Playing
  Language Models
Authors: Adam Karvonen
Categories: cs.LG cs.CL
Comments: Submitted to the Conference on Language Modeling 2024
\\
  Language models have shown unprecedented capabilities, sparking debate over
the source of their performance. Is it merely the outcome of learning syntactic
patterns and surface level statistics, or do they extract semantics and a world
model from the text? Prior work by Li et al. investigated this by training a
GPT model on synthetic, randomly generated Othello games and found that the
model learned an internal representation of the board state. We extend this
work into the more complex domain of chess, training on real games and
investigating our model's internal representations using linear probes and
contrastive activations. The model is given no a priori knowledge of the game
and is solely trained on next character prediction, yet we find evidence of
internal representations of board state. We validate these internal
representations by using them to make interventions on the model's activations
and edit its internal board state. Unlike Li et al's prior synthetic dataset
approach, our analysis finds that the model also learns to estimate latent
variables like player skill to better predict the next character. We derive a
player skill vector and add it to the model, improving the model's win rate by
up to 2.6 times.
\\ ( https://arxiv.org/abs/2403.15498 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15499
Date: Thu, 21 Mar 2024 18:55:05 GMT   (179kb,D)

Title: A Causal Analysis of CO2 Reduction Strategies in Electricity Markets
  Through Machine Learning-Driven Metalearners
Authors: Iman Emtiazi Naeini, Zahra Saberi, and Khadijeh Hassanzadeh
Categories: cs.LG cs.AI cs.CY stat.ME
\\
  This study employs the Causal Machine Learning (CausalML) statistical method
to analyze the influence of electricity pricing policies on carbon dioxide
(CO2) levels in the household sector. Investigating the causality between
potential outcomes and treatment effects, where changes in pricing policies are
the treatment, our analysis challenges the conventional wisdom surrounding
incentive-based electricity pricing. The study's findings suggest that adopting
such policies may inadvertently increase CO2 intensity. Additionally, we
integrate a machine learning-based meta-algorithm, reflecting a contemporary
statistical approach, to enhance the depth of our causal analysis. The study
conducts a comparative analysis of learners X, T, S, and R to ascertain the
optimal methods based on the defined question's specified goals and contextual
nuances. This research contributes valuable insights to the ongoing dialogue on
sustainable development practices, emphasizing the importance of considering
unintended consequences in policy formulation.
\\ ( https://arxiv.org/abs/2403.15499 ,  179kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15511
Date: Fri, 22 Mar 2024 03:54:04 GMT   (1141kb,D)

Title: Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion
  Detection Systems
Authors: Phai Vu Dinh, Diep N. Nguyen, Dinh Thai Hoang, Quang Uy Nguyen, Eryk
  Dutkiewicz, and Son Pham Bao
Categories: cs.LG cs.AI
\\
  While intrusion detection systems (IDSs) benefit from the diversity and
generalization of IoT data features, the data diversity (e.g., the
heterogeneity and high dimensions of data) also makes it difficult to train
effective machine learning models in IoT IDSs. This also leads to potentially
redundant/noisy features that may decrease the accuracy of the detection engine
in IDSs. This paper first introduces a novel neural network architecture called
Multiple-Input Auto-Encoder (MIAE). MIAE consists of multiple sub-encoders that
can process inputs from different sources with different characteristics. The
MIAE model is trained in an unsupervised learning mode to transform the
heterogeneous inputs into lower-dimensional representation, which helps
classifiers distinguish between normal behaviour and different types of
attacks. To distil and retain more relevant features but remove less
important/redundant ones during the training process, we further design and
embed a feature selection layer right after the representation layer of MIAE
resulting in a new model called MIAEFS. This layer learns the importance of
features in the representation vector, facilitating the selection of
informative features from the representation vector. The results on three IDS
datasets, i.e., NSLKDD, UNSW-NB15, and IDS2017, show the superior performance
of MIAE and MIAEFS compared to other methods, e.g., conventional classifiers,
dimensionality reduction models, unsupervised representation learning methods
with different input dimensions, and unsupervised feature selection models.
Moreover, MIAE and MIAEFS combined with the Random Forest (RF) classifier
achieve accuracy of 96.5% in detecting sophisticated attacks, e.g., Slowloris.
The average running time for detecting an attack sample using RF with the
representation of MIAE and MIAEFS is approximate 1.7E-6 seconds, whilst the
model size is lower than 1 MB.
\\ ( https://arxiv.org/abs/2403.15511 ,  1141kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15517
Date: Fri, 22 Mar 2024 11:14:30 GMT   (5128kb,D)

Title: Improving Forward Compatibility in Class Incremental Learning by
  Increasing Representation Rank and Feature Richness
Authors: Jaeill Kim, Wonseok Lee, Moonjung Eo, Wonjong Rhee
Categories: cs.LG cs.CV
\\
  Class Incremental Learning (CIL) constitutes a pivotal subfield within
continual learning, aimed at enabling models to progressively learn new
classification tasks while retaining knowledge obtained from prior tasks.
Although previous studies have predominantly focused on backward compatible
approaches to mitigate catastrophic forgetting, recent investigations have
introduced forward compatible methods to enhance performance on novel tasks and
complement existing backward compatible methods. In this study, we introduce an
effective-Rank based Feature Richness enhancement (RFR) method, designed for
improving forward compatibility. Specifically, this method increases the
effective rank of representations during the base session, thereby facilitating
the incorporation of more informative features pertinent to unseen novel tasks.
Consequently, RFR achieves dual objectives in backward and forward
compatibility: minimizing feature extractor modifications and enhancing novel
task performance, respectively. To validate the efficacy of our approach, we
establish a theoretical connection between effective rank and the Shannon
entropy of representations. Subsequently, we conduct comprehensive experiments
by integrating RFR into eleven well-known CIL methods. Our results demonstrate
the effectiveness of our approach in enhancing novel-task performance while
mitigating catastrophic forgetting. Furthermore, our method notably improves
the average incremental accuracy across all eleven cases examined.
\\ ( https://arxiv.org/abs/2403.15517 ,  5128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15520
Date: Fri, 22 Mar 2024 12:22:44 GMT   (4318kb,D)

Title: GTC: GNN-Transformer Co-contrastive Learning for Self-supervised
  Heterogeneous Graph Representation
Authors: Yundong Sun, Dongjie Zhu, Yansong Wang and Zhaoshuo Tian
Categories: cs.LG cs.IR
\\
  Graph Neural Networks (GNNs) have emerged as the most powerful weapon for
various graph tasks due to the message-passing mechanism's great local
information aggregation ability. However, over-smoothing has always hindered
GNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs,
Transformers can model global information and multi-hop interactions via
multi-head self-attention and a proper Transformer structure can show more
immunity to the over-smoothing problem. So, can we propose a novel framework to
combine GNN and Transformer, integrating both GNN's local information
aggregation and Transformer's global information modeling ability to eliminate
the over-smoothing problem? To realize this, this paper proposes a
collaborative learning scheme for GNN-Transformer and constructs GTC
architecture. GTC leverages the GNN and Transformer branch to encode node
information from different views respectively, and establishes contrastive
learning tasks based on the encoded cross-view information to realize
self-supervised heterogeneous graph representation. For the Transformer branch,
we propose Metapath-aware Hop2Token and CG-Hetphormer, which can cooperate with
GNN to attentively encode neighborhood information from different levels. As
far as we know, this is the first attempt in the field of graph representation
learning to utilize both GNN and Transformer to collaboratively capture
different view information and conduct cross-view contrastive learning. The
experiments on real datasets show that GTC exhibits superior performance
compared with state-of-the-art methods. Codes can be available at
https://github.com/PHD-lanyu/GTC.
\\ ( https://arxiv.org/abs/2403.15520 ,  4318kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15567
Date: Fri, 22 Mar 2024 18:43:46 GMT   (8904kb,D)

Title: Do not trust what you trust: Miscalibration in Semi-supervised Learning
Authors: Shambhavi Mishra, Balamurali Murugesan, Ismail Ben Ayed, Marco
  Pedersoli, Jose Dolz
Categories: cs.LG cs.CV
\\
  State-of-the-art semi-supervised learning (SSL) approaches rely on highly
confident predictions to serve as pseudo-labels that guide the training on
unlabeled samples. An inherent drawback of this strategy stems from the quality
of the uncertainty estimates, as pseudo-labels are filtered only based on their
degree of uncertainty, regardless of the correctness of their predictions.
Thus, assessing and enhancing the uncertainty of network predictions is of
paramount importance in the pseudo-labeling process. In this work, we
empirically demonstrate that SSL methods based on pseudo-labels are
significantly miscalibrated, and formally demonstrate the minimization of the
min-entropy, a lower bound of the Shannon entropy, as a potential cause for
miscalibration. To alleviate this issue, we integrate a simple penalty term,
which enforces the logit distances of the predictions on unlabeled samples to
remain low, preventing the network predictions to become overconfident.
Comprehensive experiments on a variety of SSL image classification benchmarks
demonstrate that the proposed solution systematically improves the calibration
performance of relevant SSL models, while also enhancing their discriminative
power, being an appealing addition to tackle SSL tasks.
\\ ( https://arxiv.org/abs/2403.15567 ,  8904kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15576
Date: Fri, 22 Mar 2024 19:04:02 GMT   (15735kb,D)

Title: Data-centric Prediction Explanation via Kernelized Stein Discrepancy
Authors: Mahtab Sarvmaili, Hassan Sajjad, Ga Wu
Categories: cs.LG
\\
  Existing example-based prediction explanation methods often bridge test and
training data points through the model's parameters or latent representations.
While these methods offer clues to the causes of model predictions, they often
exhibit innate shortcomings, such as incurring significant computational
overhead or producing coarse-grained explanations. This paper presents a
Highly-precise and Data-centric Explanation (HD-Explain), a straightforward
prediction explanation method exploiting properties of Kernelized Stein
Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized
kernel function for a trained model that encodes model-dependent data
correlation. By leveraging the kernel function, one can identify training
samples that provide the best predictive support to a test point efficiently.
We conducted thorough analyses and experiments across multiple classification
domains, where we show that HD-Explain outperforms existing methods from
various aspects, including 1) preciseness (fine-grained explanation), 2)
consistency, and 3) computation efficiency, leading to a surprisingly simple,
effective, and robust prediction explanation solution.
\\ ( https://arxiv.org/abs/2403.15576 ,  15735kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15652
Date: Fri, 22 Mar 2024 23:56:40 GMT   (10752kb,D)

Title: Parametric Encoding with Attention and Convolution Mitigate Spectral
  Bias of Neural Partial Differential Equation Solvers
Authors: Mehdi Shishehbor, Shirin Hosseinmardi, Ramin Bostanabad
Categories: cs.LG
Comments: 21 pages, 10 Figures, 2 tables
\\
  Deep neural networks (DNNs) are increasingly used to solve partial
differential equations (PDEs) that naturally arise while modeling a wide range
of systems and physical phenomena. However, the accuracy of such DNNs decreases
as the PDE complexity increases and they also suffer from spectral bias as they
tend to learn the low-frequency solution characteristics. To address these
issues, we introduce Parametric Grid Convolutional Attention Networks (PGCANs)
that can solve PDE systems without leveraging any labeled data in the domain.
The main idea of PGCAN is to parameterize the input space with a grid-based
encoder whose parameters are connected to the output via a DNN decoder that
leverages attention to prioritize feature training. Our encoder provides a
localized learning ability and uses convolution layers to avoid overfitting and
improve information propagation rate from the boundaries to the interior of the
domain. We test the performance of PGCAN on a wide range of PDE systems and
show that it effectively addresses spectral bias and provides more accurate
solutions compared to competing methods.
\\ ( https://arxiv.org/abs/2403.15652 ,  10752kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15654
Date: Sat, 23 Mar 2024 00:01:34 GMT   (3008kb)

Title: The Effectiveness of Local Updates for Decentralized Learning under Data
  Heterogeneity
Authors: Tongle Wu, Ying Sun
Categories: cs.LG math.OC
\\
  We revisit two fundamental decentralized optimization methods, Decentralized
Gradient Tracking (DGT) and Decentralized Gradient Descent (DGD), with multiple
local updates. We consider two settings and demonstrate that incorporating $K >
1$ local update steps can reduce communication complexity. Specifically, for
$\mu$-strongly convex and $L$-smooth loss functions, we proved that local DGT
achieves communication complexity $\tilde{\mathcal{O}} \Big(\frac{L}{\mu K} +
\frac{\delta}{\mu (1 - \rho)} + \frac{\rho }{(1 - \rho)^2} \cdot \frac{L+
\delta}{\mu}\Big)$, where $\rho$ measures the network connectivity and $\delta$
measures the second-order heterogeneity of the local loss. Our result reveals
the tradeoff between communication and computation and shows increasing $K$ can
effectively reduce communication costs when the data heterogeneity is low and
the network is well-connected. We then consider the over-parameterization
regime where the local losses share the same minimums, we proved that employing
local updates in DGD, even without gradient correction, can yield a similar
effect as DGT in reducing communication complexity. Numerical experiments
validate our theoretical results.
\\ ( https://arxiv.org/abs/2403.15654 ,  3008kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15694
Date: Sat, 23 Mar 2024 03:06:19 GMT   (1301kb,D)

Title: Group Benefits Instances Selection for Data Purification
Authors: Zhenhuang Cai, Chuanyi Zhang, Dan Huang, Yuanbo Chen, Xiuyun Guan,
  Yazhou Yao
Categories: cs.LG cs.MM
Comments: accepted by IEEE Intelligent Systems
\\
  Manually annotating datasets for training deep models is very labor-intensive
and time-consuming. To overcome such inferiority, directly leveraging web
images to conduct training data becomes a natural choice. Nevertheless, the
presence of label noise in web data usually degrades the model performance.
Existing methods for combating label noise are typically designed and tested on
synthetic noisy datasets. However, they tend to fail to achieve satisfying
results on real-world noisy datasets. To this end, we propose a method named
GRIP to alleviate the noisy label problem for both synthetic and real-world
datasets. Specifically, GRIP utilizes a group regularization strategy that
estimates class soft labels to improve noise robustness. Soft label supervision
reduces overfitting on noisy labels and learns inter-class similarities to
benefit classification. Furthermore, an instance purification operation
globally identifies noisy labels by measuring the difference between each
training sample and its class soft label. Through operations at both group and
instance levels, our approach integrates the advantages of noise-robust and
noise-cleaning methods and remarkably alleviates the performance degradation
caused by noisy labels. Comprehensive experimental results on synthetic and
real-world datasets demonstrate the superiority of GRIP over the existing
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.15694 ,  1301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15706
Date: Sat, 23 Mar 2024 03:56:31 GMT   (739kb,D)

Title: G-ACIL: Analytic Learning for Exemplar-Free Generalized Class
  Incremental Learning
Authors: Huiping Zhuang, Yizhu Chen, Di Fang, Run He, Kai Tong, Hongxin Wei,
  Ziqian Zeng, Cen Chen
Categories: cs.LG cs.CV
\\
  Class incremental learning (CIL) trains a network on sequential tasks with
separated categories but suffers from catastrophic forgetting, where models
quickly lose previously learned knowledge when acquiring new tasks. The
generalized CIL (GCIL) aims to address the CIL problem in a more real-world
scenario, where incoming data have mixed data categories and unknown sample
size distribution, leading to intensified forgetting. Existing attempts for the
GCIL either have poor performance, or invade data privacy by saving historical
exemplars. To address this, in this paper, we propose an exemplar-free
generalized analytic class incremental learning (G-ACIL). The G-ACIL adopts
analytic learning (a gradient-free training technique), and delivers an
analytical solution (i.e., closed-form) to the GCIL scenario. This solution is
derived via decomposing the incoming data into exposed and unexposed classes,
allowing an equivalence between the incremental learning and its joint
training, i.e., the weight-invariant property. Such an equivalence is
theoretically validated through matrix analysis tools, and hence contributes
interpretability in GCIL. It is also empirically evidenced by experiments on
various datasets and settings of GCIL. The results show that the G-ACIL
exhibits leading performance with high robustness compared with existing
competitive GCIL methods. Codes will be ready at
https://github.com/ZHUANGHP/Analytic-continual-learning.
\\ ( https://arxiv.org/abs/2403.15706 ,  739kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15707
Date: Sat, 23 Mar 2024 03:57:28 GMT   (1309kb,D)

Title: Role of Locality and Weight Sharing in Image-Based Tasks: A Sample
  Complexity Separation between CNNs, LCNs, and FCNs
Authors: Aakash Lahoti, Stefani Karp, Ezra Winston, Aarti Singh, Yuanzhi Li
Categories: cs.LG cs.AI stat.ML
Comments: 40 pages, 4 figures, Accepted to ICLR 2024, Spotlight
\\
  Vision tasks are characterized by the properties of locality and translation
invariance. The superior performance of convolutional neural networks (CNNs) on
these tasks is widely attributed to the inductive bias of locality and weight
sharing baked into their architecture. Existing attempts to quantify the
statistical benefits of these biases in CNNs over locally connected
convolutional neural networks (LCNs) and fully connected neural networks (FCNs)
fall into one of the following categories: either they disregard the optimizer
and only provide uniform convergence upper bounds with no separating lower
bounds, or they consider simplistic tasks that do not truly mirror the locality
and translation invariance as found in real-world vision tasks. To address
these deficiencies, we introduce the Dynamic Signal Distribution (DSD)
classification task that models an image as consisting of $k$ patches, each of
dimension $d$, and the label is determined by a $d$-sparse signal vector that
can freely appear in any one of the $k$ patches. On this task, for any
orthogonally equivariant algorithm like gradient descent, we prove that CNNs
require $\tilde{O}(k+d)$ samples, whereas LCNs require $\Omega(kd)$ samples,
establishing the statistical advantages of weight sharing in translation
invariant tasks. Furthermore, LCNs need $\tilde{O}(k(k+d))$ samples, compared
to $\Omega(k^2d)$ samples for FCNs, showcasing the benefits of locality in
local tasks. Additionally, we develop information theoretic tools for analyzing
randomized algorithms, which may be of interest for statistical research.
\\ ( https://arxiv.org/abs/2403.15707 ,  1309kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15711
Date: Sat, 23 Mar 2024 04:13:55 GMT   (2960kb,D)

Title: Identifiable Latent Neural Causal Models
Authors: Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton
  van den Hengel, Kun Zhang, Javen Qinfeng Shi
Categories: cs.LG stat.ME stat.ML
\\
  Causal representation learning seeks to uncover latent, high-level causal
representations from low-level observed data. It is particularly good at
predictions under unseen distribution shifts, because these shifts can
generally be interpreted as consequences of interventions. Hence leveraging
{seen} distribution shifts becomes a natural strategy to help identifying
causal representations, which in turn benefits predictions where distributions
are previously {unseen}. Determining the types (or conditions) of such
distribution shifts that do contribute to the identifiability of causal
representations is critical. This work establishes a {sufficient} and
{necessary} condition characterizing the types of distribution shifts for
identifiability in the context of latent additive noise models. Furthermore, we
present partial identifiability results when only a portion of distribution
shifts meets the condition. In addition, we extend our findings to latent
post-nonlinear causal models. We translate our findings into a practical
algorithm, allowing for the acquisition of reliable latent causal
representations. Our algorithm, guided by our underlying theory, has
demonstrated outstanding performance across a diverse range of synthetic and
real-world datasets. The empirical observations align closely with the
theoretical findings, affirming the robustness and effectiveness of our
approach.
\\ ( https://arxiv.org/abs/2403.15711 ,  2960kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15717
Date: Sat, 23 Mar 2024 04:44:55 GMT   (30295kb,D)

Title: Ev-Edge: Efficient Execution of Event-based Vision Algorithms on
  Commodity Edge Platforms
Authors: Shrihari Sridharan, Surya Selvam, Kaushik Roy, Anand Raghunathan
Categories: cs.LG cs.CV
\\
  Event cameras have emerged as a promising sensing modality for autonomous
navigation systems, owing to their high temporal resolution, high dynamic range
and negligible motion blur. To process the asynchronous temporal event streams
from such sensors, recent research has shown that a mix of Artificial Neural
Networks (ANNs), Spiking Neural Networks (SNNs) as well as hybrid SNN-ANN
algorithms are necessary to achieve high accuracies across a range of
perception tasks. However, we observe that executing such workloads on
commodity edge platforms which feature heterogeneous processing elements such
as CPUs, GPUs and neural accelerators results in inferior performance. This is
due to the mismatch between the irregular nature of event streams and diverse
characteristics of algorithms on the one hand and the underlying hardware
platform on the other. We propose Ev-Edge, a framework that contains three key
optimizations to boost the performance of event-based vision systems on edge
platforms: (1) An Event2Sparse Frame converter directly transforms raw event
streams into sparse frames, enabling the use of sparse libraries with minimal
encoding overheads (2) A Dynamic Sparse Frame Aggregator merges sparse frames
at runtime by trading off the temporal granularity of events and computational
demand thereby improving hardware utilization (3) A Network Mapper maps
concurrently executing tasks to different processing elements while also
selecting layer precision by considering both compute and communication
overheads. On several state-of-art networks for a range of autonomous
navigation tasks, Ev-Edge achieves 1.28x-2.05x improvements in latency and
1.23x-2.15x in energy over an all-GPU implementation on the NVIDIA Jetson
Xavier AGX platform for single-task execution scenarios. Ev-Edge also achieves
1.43x-1.81x latency improvements over round-robin scheduling methods in
multi-task execution scenarios.
\\ ( https://arxiv.org/abs/2403.15717 ,  30295kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15726
Date: Sat, 23 Mar 2024 05:26:36 GMT   (369kb,D)

Title: Convection-Diffusion Equation: A Theoretically Certified Framework for
  Neural Networks
Authors: Tangjun Wang, Chenglong Bao, Zuoqiang Shi
Categories: cs.LG
\\
  In this paper, we study the partial differential equation models of neural
networks. Neural network can be viewed as a map from a simple base model to a
complicate function. Based on solid analysis, we show that this map can be
formulated by a convection-diffusion equation. This theoretically certified
framework gives mathematical foundation and more understanding of neural
networks. Moreover, based on the convection-diffusion equation model, we design
a novel network structure, which incorporates diffusion mechanism into network
architecture. Extensive experiments on both benchmark datasets and real-world
applications validate the performance of the proposed model.
\\ ( https://arxiv.org/abs/2403.15726 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15744
Date: Sat, 23 Mar 2024 07:16:23 GMT   (2415kb,D)

Title: On the Fragility of Active Learners
Authors: Abhishek Ghose and Emma Nguyen
Categories: cs.LG cs.CL
\\
  Active learning (AL) techniques aim to maximally utilize a labeling budget by
iteratively selecting instances that are most likely to improve prediction
accuracy. However, their benefit compared to random sampling has not been
consistent across various setups, e.g., different datasets, classifiers. In
this empirical study, we examine how a combination of different factors might
obscure any gains from an AL technique.
  Focusing on text classification, we rigorously evaluate AL techniques over
around 1000 experiments that vary wrt the dataset, batch size, text
representation and the classifier. We show that AL is only effective in a
narrow set of circumstances. We also address the problem of using metrics that
are better aligned with real world expectations.
  The impact of this study is in its insights for a practitioner: (a) the
choice of text representation and classifier is as important as that of an AL
technique, (b) choice of the right metric is critical in assessment of the
latter, and, finally, (c) reported AL results must be holistically interpreted,
accounting for variables other than just the query strategy.
\\ ( https://arxiv.org/abs/2403.15744 ,  2415kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15766
Date: Sat, 23 Mar 2024 08:40:38 GMT   (1225kb,D)

Title: BEND: Bagging Deep Learning Training Based on Efficient Neural Network
  Diffusion
Authors: Jia Wei, Xingjun Zhang, Witold Pedrycz
Categories: cs.LG cs.AI
\\
  Bagging has achieved great success in the field of machine learning by
integrating multiple base classifiers to build a single strong classifier to
reduce model variance. The performance improvement of bagging mainly relies on
the number and diversity of base classifiers. However, traditional deep
learning model training methods are expensive to train individually and
difficult to train multiple models with low similarity in a restricted dataset.
Recently, diffusion models, which have been tremendously successful in the
fields of imaging and vision, have been found to be effective in generating
neural network model weights and biases with diversity. We creatively propose a
Bagging deep learning training algorithm based on Efficient Neural network
Diffusion (BEND). The originality of BEND comes from the first use of a neural
network diffusion model to efficiently build base classifiers for bagging. Our
approach is simple but effective, first using multiple trained model weights
and biases as inputs to train autoencoder and latent diffusion model to realize
a diffusion model from noise to valid neural network parameters. Subsequently,
we generate several base classifiers using the trained diffusion model.
Finally, we integrate these ba se classifiers for various inference tasks using
the Bagging method. Resulting experiments on multiple models and datasets show
that our proposed BEND algorithm can consistently outperform the mean and
median accuracies of both the original trained model and the diffused model. At
the same time, new models diffused using the diffusion model have higher
diversity and lower cost than multiple models trained using traditional
methods. The BEND approach successfully introduces diffusion models into the
new deep learning training domain and provides a new paradigm for future deep
learning training and inference.
\\ ( https://arxiv.org/abs/2403.15766 ,  1225kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15790
Date: Sat, 23 Mar 2024 10:37:22 GMT   (6513kb,D)

Title: Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled
  Autoencoder for Mixed Tabular Datasets
Authors: Samuel Stocksieker, Denys Pommeret, Arthur Charpentier
Categories: cs.LG stat.ML
\\
  The field of imbalanced self-supervised learning, especially in the context
of tabular data, has not been extensively studied. Existing research has
predominantly focused on image datasets. This paper aims to fill this gap by
examining the specific challenges posed by data imbalance in self-supervised
learning in the domain of tabular data, with a primary focus on autoencoders.
Autoencoders are widely employed for learning and constructing a new
representation of a dataset, particularly for dimensionality reduction. They
are also often used for generative model learning, as seen in variational
autoencoders. When dealing with mixed tabular data, qualitative variables are
often encoded using a one-hot encoder with a standard loss function (MSE or
Cross Entropy). In this paper, we analyze the drawbacks of this approach,
especially when categorical variables are imbalanced. We propose a novel metric
to balance learning: a Multi-Supervised Balanced MSE. This approach reduces the
reconstruction error by balancing the influence of variables. Finally, we
empirically demonstrate that this new metric, compared to the standard MSE: i)
outperforms when the dataset is imbalanced, especially when the learning
process is insufficient, and ii) provides similar results in the opposite case.
\\ ( https://arxiv.org/abs/2403.15790 ,  6513kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15824
Date: Sat, 23 Mar 2024 12:33:12 GMT   (577kb)

Title: Carbon Intensity-Aware Adaptive Inference of DNNs
Authors: Jiwan Jung
Categories: cs.LG cs.AI
\\
  DNN inference, known for its significant energy consumption and the resulting
high carbon footprint, can be made more sustainable by adapting model size and
accuracy to the varying carbon intensity throughout the day. Our heuristic
algorithm uses larger, high-accuracy models during low-intensity periods and
smaller, lower-accuracy ones during high-intensity periods. We also introduce a
metric, carbon-emission efficiency, which quantitatively measures the efficacy
of adaptive model selection in terms of carbon footprint. The evaluation showed
that the proposed approach could improve the carbon emission efficiency in
improving the accuracy of vision recognition services by up to 80%.
\\ ( https://arxiv.org/abs/2403.15824 ,  577kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15839
Date: Sat, 23 Mar 2024 13:28:37 GMT   (1198kb,D)

Title: TablePuppet: A Generic Framework for Relational Federated Learning
Authors: Lijie Xu, Chulin Xie, Yiran Guo, Gustavo Alonso, Bo Li, Guoliang Li,
  Wei Wang, Wentao Wu, Ce Zhang
Categories: cs.LG cs.DB
Comments: 14 pages, 8 figures
\\
  Current federated learning (FL) approaches view decentralized training data
as a single table, divided among participants either horizontally (by rows) or
vertically (by columns). However, these approaches are inadequate for handling
distributed relational tables across databases. This scenario requires
intricate SQL operations like joins and unions to obtain the training data,
which is either costly or restricted by privacy concerns. This raises the
question: can we directly run FL on distributed relational tables?
  In this paper, we formalize this problem as relational federated learning
(RFL). We propose TablePuppet, a generic framework for RFL that decomposes the
learning process into two steps: (1) learning over join (LoJ) followed by (2)
learning over union (LoU). In a nutshell, LoJ pushes learning down onto the
vertical tables being joined, and LoU further pushes learning down onto the
horizontal partitions of each vertical table. TablePuppet incorporates
computation/communication optimizations to deal with the duplicate tuples
introduced by joins, as well as differential privacy (DP) to protect against
both feature and label leakages. We demonstrate the efficiency of TablePuppet
in combination with two widely-used ML training algorithms, stochastic gradient
descent (SGD) and alternating direction method of multipliers (ADMM), and
compare their computation/communication complexity. We evaluate the SGD/ADMM
algorithms developed atop TablePuppet by training diverse ML models. Our
experimental results show that TablePuppet achieves model accuracy comparable
to the centralized baselines running directly atop the SQL results. Moreover,
ADMM takes less communication time than SGD to converge to similar model
accuracy.
\\ ( https://arxiv.org/abs/2403.15839 ,  1198kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15855
Date: Sat, 23 Mar 2024 14:24:36 GMT   (317kb,D)

Title: Initialisation and Topology Effects in Decentralised Federated Learning
Authors: Arash Badie-Modiri, Chiara Boldrini, Lorenzo Valerio, J\'anos
  Kert\'esz and M\'arton Karsai
Categories: cs.LG cs.AI cs.DC physics.soc-ph
\\
  Fully decentralised federated learning enables collaborative training of
individual machine learning models on distributed devices on a network while
keeping the training data localised. This approach enhances data privacy and
eliminates both the single point of failure and the necessity for central
coordination. Our research highlights that the effectiveness of decentralised
federated learning is significantly influenced by the network topology of
connected devices. A simplified numerical model for studying the early
behaviour of these systems leads us to an improved artificial neural network
initialisation strategy, which leverages the distribution of eigenvector
centralities of the nodes of the underlying network, leading to a radically
improved training efficiency. Additionally, our study explores the scaling
behaviour and choice of environmental parameters under our proposed
initialisation strategy. This work paves the way for more efficient and
scalable artificial neural network training in a distributed and uncoordinated
environment, offering a deeper understanding of the intertwining roles of
network structure and learning dynamics.
\\ ( https://arxiv.org/abs/2403.15855 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15881
Date: Sat, 23 Mar 2024 16:21:22 GMT   (1660kb,D)

Title: Fast and Unified Path Gradient Estimators for Normalizing Flows
Authors: Lorenz Vaitl, Ludwig Winkler, Lorenz Richter, Pan Kessel
Categories: cs.LG stat.ML
\\
  Recent work shows that path gradient estimators for normalizing flows have
lower variance compared to standard estimators for variational inference,
resulting in improved training. However, they are often prohibitively more
expensive from a computational point of view and cannot be applied to maximum
likelihood training in a scalable manner, which severely hinders their
widespread adoption. In this work, we overcome these crucial limitations.
Specifically, we propose a fast path gradient estimator which improves
computational efficiency significantly and works for all normalizing flow
architectures of practical relevance. We then show that this estimator can also
be applied to maximum likelihood training for which it has a regularizing
effect as it can take the form of a given target energy function into account.
We empirically establish its superior performance and reduced variance for
several natural sciences applications.
\\ ( https://arxiv.org/abs/2403.15881 ,  1660kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15905
Date: Sat, 23 Mar 2024 18:19:02 GMT   (5298kb,D)

Title: Towards Low-Energy Adaptive Personalization for Resource-Constrained
  Devices
Authors: Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Hadaddi
Categories: cs.LG cs.CV
Comments: Accepetd to The 4th Workshop on Machine Learning and Systems
  (EuroMLSys '24)
\\
  The personalization of machine learning (ML) models to address data drift is
a significant challenge in the context of Internet of Things (IoT)
applications. Presently, most approaches focus on fine-tuning either the full
base model or its last few layers to adapt to new data, while often neglecting
energy costs. However, various types of data drift exist, and fine-tuning the
full base model or the last few layers may not result in optimal performance in
certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy
adaptive personalization framework designed for resource-constrained devices.
We categorize data drift and personalization into three types: input-level,
feature-level, and output-level. For each type, we fine-tune different blocks
of the model to achieve optimal performance with reduced energy costs.
Specifically, input-, feature-, and output-level correspond to fine-tuning the
front, middle, and rear blocks of the model. We evaluate TBFT on a ResNet
model, three datasets, three different training sizes, and a Raspberry Pi.
Compared with the $Block Avg$, where each block is fine-tuned individually and
their performance improvements are averaged, TBFT exhibits an improvement in
model accuracy by an average of 15.30% whilst saving 41.57% energy consumption
on average compared with full fine-tuning.
\\ ( https://arxiv.org/abs/2403.15905 ,  5298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15908
Date: Sat, 23 Mar 2024 18:42:22 GMT   (465kb,D)

Title: Deep Gaussian Covariance Network with Trajectory Sampling for
  Data-Efficient Policy Search
Authors: Can Bogoclu and Robert Vosshall and Kevin Cremanns and Dirk Roos
Categories: cs.LG stat.ML
DOI: 10.1109/ACDSA59508.2024.10467448
\\
  Probabilistic world models increase data efficiency of model-based
reinforcement learning (MBRL) by guiding the policy with their epistemic
uncertainty to improve exploration and acquire new samples. Moreover, the
uncertainty-aware learning procedures in probabilistic approaches lead to
robust policies that are less sensitive to noisy observations compared to
uncertainty unaware solutions. We propose to combine trajectory sampling and
deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL
problems in an optimal control setting. We compare trajectory sampling with
density-based approximation for uncertainty propagation using three different
probabilistic world models; Gaussian processes, Bayesian neural networks, and
DGCNs. We provide empirical evidence using four different well-known test
environments, that our method improves the sample-efficiency over other
combinations of uncertainty propagation methods and probabilistic models.
During our tests, we place particular emphasis on the robustness of the learned
policies with respect to noisy initial states.
\\ ( https://arxiv.org/abs/2403.15908 ,  465kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15928
Date: Sat, 23 Mar 2024 20:22:30 GMT   (1449kb,D)

Title: Safe Reinforcement Learning for Constrained Markov Decision Processes
  with Stochastic Stopping Time
Authors: Abhijit Mazumdar and Rafal Wisniewski and Manuela L. Bujorianu
Categories: cs.LG
\\
  In this paper, we present an online reinforcement learning algorithm for
constrained Markov decision processes with a safety constraint. Despite the
necessary attention of the scientific community, considering stochastic
stopping time, the problem of learning optimal policy without violating safety
constraints during the learning phase is yet to be addressed. To this end, we
propose an algorithm based on linear programming that does not require a
process model. We show that the learned policy is safe with high confidence. We
also propose a method to compute a safe baseline policy, which is central in
developing algorithms that do not violate the safety constraints. Finally, we
provide simulation results to show the efficacy of the proposed algorithm.
Further, we demonstrate that efficient exploration can be achieved by defining
a subset of the state-space called proxy set.
\\ ( https://arxiv.org/abs/2403.15928 ,  1449kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15935
Date: Sat, 23 Mar 2024 21:39:56 GMT   (5753kb,D)

Title: Sample and Communication Efficient Fully Decentralized MARL Policy
  Evaluation via a New Approach: Local TD update
Authors: Fnu Hairi, Zifan Zhang and Jia Liu
Categories: cs.LG
Comments: Main body of the paper appeared in AAMAS24
\\
  In actor-critic framework for fully decentralized multi-agent reinforcement
learning (MARL), one of the key components is the MARL policy evaluation (PE)
problem, where a set of $N$ agents work cooperatively to evaluate the value
function of the global states for a given policy through communicating with
their neighbors. In MARL-PE, a critical challenge is how to lower the sample
and communication complexities, which are defined as the number of training
samples and communication rounds needed to converge to some
$\epsilon$-stationary point. To lower communication complexity in MARL-PE, a
"natural'' idea is to perform multiple local TD-update steps between each
consecutive rounds of communication to reduce the communication frequency.
However, the validity of the local TD-update approach remains unclear due to
the potential "agent-drift'' phenomenon resulting from heterogeneous rewards
across agents in general. This leads to an interesting open question: Can the
local TD-update approach entail low sample and communication complexities? In
this paper, we make the first attempt to answer this fundamental question. We
focus on the setting of MARL-PE with average reward, which is motivated by many
multi-agent network optimization problems. Our theoretical and experimental
results confirm that allowing multiple local TD-update steps is indeed an
effective approach in lowering the sample and communication complexities of
MARL-PE compared to consensus-based MARL-PE algorithms. Specifically, the local
TD-update steps between two consecutive communication rounds can be as large as
$\mathcal{O}(1/\epsilon^{1/2}\log{(1/\epsilon)})$ in order to converge to an
$\epsilon$-stationary point of MARL-PE. Moreover, we show theoretically that in
order to reach the optimal sample complexity, the communication complexity of
local TD-update approach is $\mathcal{O}(1/\epsilon^{1/2}\log{(1/\epsilon)})$.
\\ ( https://arxiv.org/abs/2403.15935 ,  5753kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15953
Date: Sat, 23 Mar 2024 23:14:37 GMT   (5014kb,D)

Title: Understanding The Effectiveness of Lossy Compression in Machine Learning
  Training Sets
Authors: Robert Underwood, Jon C. Calhoun, Sheng Di, Franck Cappello
Categories: cs.LG cs.AI
Comments: 12 pages, 4 figures
ACM-class: I.2.6; E.2; C.4
\\
  Learning and Artificial Intelligence (ML/AI) techniques have become
increasingly prevalent in high performance computing (HPC). However, these
methods depend on vast volumes of floating point data for training and
validation which need methods to share the data on a wide area network (WAN) or
to transfer it from edge devices to data centers. Data compression can be a
solution to these problems, but an in-depth understanding of how lossy
compression affects model quality is needed. Prior work largely considers a
single application or compression method. We designed a systematic methodology
for evaluating data reduction techniques for ML/AI, and we use it to perform a
very comprehensive evaluation with 17 data reduction methods on 7 ML/AI
applications to show modern lossy compression methods can achieve a 50-100x
compression ratio improvement for a 1% or less loss in quality. We identify
critical insights that guide the future use and design of lossy compressors for
ML/AI.
\\ ( https://arxiv.org/abs/2403.15953 ,  5014kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15962
Date: Sat, 23 Mar 2024 23:49:01 GMT   (869kb)

Title: Detection of Problem Gambling with Less Features Using Machine Learning
  Methods
Authors: Yang Jiao, Gloria Wong-Padoongpatt, Mei Yang
Categories: cs.LG cs.AI cs.CY
Comments: 6 pages, 5 tables, 1 figure
\\
  Analytic features in gambling study are performed based on the amount of data
monitoring on user daily actions. While performing the detection of problem
gambling, existing datasets provide relatively rich analytic features for
building machine learning based model. However, considering the complexity and
cost of collecting the analytic features in real applications, conducting
precise detection with less features will tremendously reduce the cost of data
collection. In this study, we propose a deep neural networks PGN4 that performs
well when using limited analytic features. Through the experiment on two
datasets, we discover that PGN4 only experiences a mere performance drop when
cutting 102 features to 5 features. Besides, we find the commonality within the
top 5 features from two datasets.
\\ ( https://arxiv.org/abs/2403.15962 ,  869kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15989
Date: Sun, 24 Mar 2024 02:54:46 GMT   (2843kb,D)

Title: Knowledge-guided Machine Learning: Current Trends and Future Prospects
Authors: Anuj Karpatne, Xiaowei Jia, Vipin Kumar
Categories: cs.LG cs.AI cs.CE
\\
  This paper presents an overview of scientific modeling and discusses the
complementary strengths and weaknesses of ML methods for scientific modeling in
comparison to process-based models. It also provides an introduction to the
current state of research in the emerging field of scientific knowledge-guided
machine learning (KGML) that aims to use both scientific knowledge and data in
ML frameworks to achieve better generalizability, scientific consistency, and
explainability of results. We discuss different facets of KGML research in
terms of the type of scientific knowledge used, the form of knowledge-ML
integration explored, and the method for incorporating scientific knowledge in
ML. We also discuss some of the common categories of use cases in environmental
sciences where KGML methods are being developed, using illustrative examples in
each category.
\\ ( https://arxiv.org/abs/2403.15989 ,  2843kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16004
Date: Sun, 24 Mar 2024 04:23:43 GMT   (28253kb,D)

Title: A Federated Parameter Aggregation Method for Node Classification Tasks
  with Different Graph Network Structures
Authors: Hao Song, Jiacheng Yao, Zhengxi Li, Shaocong Xu, Shibo Jin, Jiajun
  Zhou, Chenbo Fu, Qi Xuan, Shanqing Yu
Categories: cs.LG cs.AI
\\
  Over the past few years, federated learning has become widely used in various
classical machine learning fields because of its collaborative ability to train
data from multiple sources without compromising privacy. However, in the area
of graph neural networks, the nodes and network structures of graphs held by
clients are different in many practical applications, and the aggregation
method that directly shares model gradients cannot be directly applied to this
scenario. Therefore, this work proposes a federated aggregation method FLGNN
applied to various graph federation scenarios and investigates the aggregation
effect of parameter sharing at each layer of the graph neural network model.
The effectiveness of the federated aggregation method FLGNN is verified by
experiments on real datasets. Additionally, for the privacy security of FLGNN,
this paper designs membership inference attack experiments and differential
privacy defense experiments. The results show that FLGNN performs good
robustness, and the success rate of privacy theft is further reduced by adding
differential privacy defense methods.
\\ ( https://arxiv.org/abs/2403.16004 ,  28253kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16024
Date: Sun, 24 Mar 2024 05:57:00 GMT   (6248kb,D)

Title: A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA
Authors: Ayush Thakur and Rashmi Vashisth
Categories: cs.LG cs.CV cs.GR
\\
  This paper presents a comprehensive study on the unified module for
accelerating stable-diffusion processes, specifically focusing on the lcm-lora
module. Stable-diffusion processes play a crucial role in various scientific
and engineering domains, and their acceleration is of paramount importance for
efficient computational performance. The standard iterative procedures for
solving fixed-source discrete ordinates problems often exhibit slow
convergence, particularly in optically thick scenarios. To address this
challenge, unconditionally stable diffusion-acceleration methods have been
developed, aiming to enhance the computational efficiency of transport
equations and discrete ordinates problems. This study delves into the
theoretical foundations and numerical results of unconditionally stable
diffusion synthetic acceleration methods, providing insights into their
stability and performance for model discrete ordinates problems. Furthermore,
the paper explores recent advancements in diffusion model acceleration,
including on device acceleration of large diffusion models via gpu aware
optimizations, highlighting the potential for significantly improved inference
latency. The results and analyses in this study provide important insights into
stable diffusion processes and have important ramifications for the creation
and application of acceleration methods specifically, the lcm-lora module in a
variety of computing environments.
\\ ( https://arxiv.org/abs/2403.16024 ,  6248kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16030
Date: Sun, 24 Mar 2024 06:10:56 GMT   (849kb,D)

Title: VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections
Authors: Dongqi Fu, Zhigang Hua, Yan Xie, Jin Fang, Si Zhang, Kaan Sancak, Hao
  Wu, Andrey Malevich, Jingrui He, Bo Long
Categories: cs.LG
\\
  Graph transformer has been proven as an effective graph learning method for
its adoption of attention mechanism that is capable of capturing expressive
representations from complex topological and feature information of graphs.
Graph transformer conventionally performs dense attention (or global attention)
for every pair of nodes to learn node representation vectors, resulting in
quadratic computational costs that are unaffordable for large-scale graph data.
Therefore, mini-batch training for graph transformers is a promising direction,
but limited samples in each mini-batch can not support effective dense
attention to encode informative representations. Facing this bottleneck, (1) we
start by assigning each node a token list that is sampled by personalized
PageRank (PPR) and then apply standard multi-head self-attention only on this
list to compute its node representations. This PPR tokenization method
decouples model training from complex graph topological information and makes
heavy feature engineering offline and independent, such that mini-batch
training of graph transformers is possible by loading each node's token list in
batches. We further prove this PPR tokenization is viable as a graph
convolution network with a fixed polynomial filter and jumping knowledge.
However, only using personalized PageRank may limit information carried by a
token list, which could not support different graph inductive biases for model
training. To this end, (2) we rewire graphs by introducing multiple types of
virtual connections through structure- and content-based super nodes that
enable PPR tokenization to encode local and global contexts, long-range
interaction, and heterophilous information into each node's token list, and
then formalize our Virtual Connection Ranking based Graph Transformer
(VCR-Graphormer).
\\ ( https://arxiv.org/abs/2403.16030 ,  849kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16033
Date: Sun, 24 Mar 2024 06:28:54 GMT   (159kb,D)

Title: Node Classification via Semantic-Structural Attention-Enhanced Graph
  Convolutional Networks
Authors: Hongyin Zhu
Categories: cs.LG cs.CL cs.SI
\\
  Graph data, also known as complex network data, is omnipresent across various
domains and applications. Prior graph neural network models primarily focused
on extracting task-specific structural features through supervised learning
objectives, but they fell short in capturing the inherent semantic and
structural features of the entire graph. In this paper, we introduce the
semantic-structural attention-enhanced graph convolutional network (SSA-GCN),
which not only models the graph structure but also extracts generalized
unsupervised features to enhance vertex classification performance. The
SSA-GCN's key contributions lie in three aspects: firstly, it derives semantic
information through unsupervised feature extraction from a knowledge graph
perspective; secondly, it obtains structural information through unsupervised
feature extraction from a complex network perspective; and finally, it
integrates these features through a cross-attention mechanism. By leveraging
these features, we augment the graph convolutional network, thereby enhancing
the model's generalization capabilities. Our experiments on the Cora and
CiteSeer datasets demonstrate the performance improvements achieved by our
proposed method. Furthermore, our approach also exhibits excellent accuracy
under privacy settings, making it a robust and effective solution for graph
data analysis.
\\ ( https://arxiv.org/abs/2403.16033 ,  159kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16049
Date: Sun, 24 Mar 2024 07:29:12 GMT   (10382kb,D)

Title: Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep
  Learning
Authors: Sangjoon Park, Yongsung Kwon, Hyungjoon Soh, Mi Jin Lee, and Seung-Woo
  Son
Categories: cs.LG physics.soc-ph
Comments: 10 pages, 7 figures
\\
  Predicting temporal patterns across various domains poses significant
challenges due to their nuanced and often nonlinear trajectories. To address
this challenge, prediction frameworks have been continuously refined, employing
data-driven statistical methods, mathematical models, and machine learning.
Recently, as one of the challenging systems, shared transport systems such as
public bicycles have gained prominence due to urban constraints and
environmental concerns. Predicting rental and return patterns at bicycle
stations remains a formidable task due to the system's openness and imbalanced
usage patterns across stations. In this study, we propose a deep learning
framework to predict rental and return patterns by leveraging cartogram
approaches. The cartogram approach facilitates the prediction of demand for
newly installed stations with no training data as well as long-period
prediction, which has not been achieved before. We apply this method to public
bicycle rental-and-return data in Seoul, South Korea, employing a
spatial-temporal convolutional graph attention network. Our improved
architecture incorporates batch attention and modified node feature updates for
better prediction accuracy across different time scales. We demonstrate the
effectiveness of our framework in predicting temporal patterns and its
potential applications.
\\ ( https://arxiv.org/abs/2403.16049 ,  10382kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16075
Date: Sun, 24 Mar 2024 09:33:45 GMT   (155kb,D)

Title: IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral
  Evolution History
Authors: Yi Xu, Weiran Shen, Xiao Zhang, Jun Xu
Categories: cs.LG
\\
  Traditional imitation learning focuses on modeling the behavioral mechanisms
of experts, which requires a large amount of interaction history generated by
some fixed expert. However, in many streaming applications, such as streaming
recommender systems, online decision-makers typically engage in online learning
during the decision-making process, meaning that the interaction history
generated by online decision-makers includes their behavioral evolution from
novice expert to experienced expert. This poses a new challenge for existing
imitation learning approaches that can only utilize data from experienced
experts. To address this issue, this paper proposes an inverse batched
contextual bandit (IBCB) framework that can efficiently perform estimations of
environment reward parameters and learned policy based on the expert's
behavioral evolution history. Specifically, IBCB formulates the inverse problem
into a simple quadratic programming problem by utilizing the behavioral
evolution history of the batched contextual bandit with inaccessible rewards.
We demonstrate that IBCB is a unified framework for both deterministic and
randomized bandit policies. The experimental results indicate that IBCB
outperforms several existing imitation learning algorithms on synthetic and
real-world data and significantly reduces running time. Additionally, empirical
analyses reveal that IBCB exhibits better out-of-distribution generalization
and is highly effective in learning the bandit policy from the interaction
history of novice experts.
\\ ( https://arxiv.org/abs/2403.16075 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16108
Date: Sun, 24 Mar 2024 11:52:39 GMT   (211kb,D)

Title: A Transformer approach for Electricity Price Forecasting
Authors: Oscar Llorente Gonzalez and Jose Portela
Categories: cs.LG cs.AI
Comments: 7 pages
\\
  This paper presents a novel approach to electricity price forecasting (EPF)
using a pure Transformer model. As opposed to other alternatives, no other
recurrent network is used in combination to the attention mechanism. Hence,
showing that the attention layer is enough for capturing the temporal patterns.
The paper also provides fair comparison of the models using the open-source EPF
toolbox and provide the code to enhance reproducibility and transparency in EPF
research. The results show that the Transformer model outperforms traditional
methods, offering a promising solution for reliable and sustainable power
system operation.
\\ ( https://arxiv.org/abs/2403.16108 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16130
Date: Sun, 24 Mar 2024 13:01:05 GMT   (671kb,D)

Title: AKBR: Learning Adaptive Kernel-based Representations for Graph
  Classification
Authors: Feifei Qian, Lixin Cui, Yue Wang, Hangyuan Du, Lu Bai, Edwin R.
  Hancock
Categories: cs.LG cs.AI
\\
  In this paper, we propose a new model to learn Adaptive Kernel-based
Representations (AKBR) for graph classification. Unlike state-of-the-art
R-convolution graph kernels that are defined by merely counting any pair of
isomorphic substructures between graphs and cannot provide an end-to-end
learning mechanism for the classifier, the proposed AKBR approach aims to
define an end-to-end representation learning model to construct an adaptive
kernel matrix for graphs. To this end, we commence by leveraging a novel
feature-channel attention mechanism to capture the interdependencies between
different substructure invariants of original graphs. The proposed AKBR model
can thus effectively identify the structural importance of different
substructures, and compute the R-convolution kernel between pairwise graphs
associated with the more significant substructures specified by their
structural attentions. Since each row of the resulting kernel matrix can be
theoretically seen as the embedding vector of a sample graph, the proposed AKBR
model is able to directly employ the resulting kernel matrix as the graph
feature matrix and input it into the classifier for classification (i.e., the
SoftMax layer), naturally providing an end-to-end learning architecture between
the kernel computation as well as the classifier. Experimental results show
that the proposed AKBR model outperforms existing state-of-the-art graph
kernels and deep learning methods on standard graph benchmarks.
\\ ( https://arxiv.org/abs/2403.16130 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16137
Date: Sun, 24 Mar 2024 13:10:09 GMT   (191kb,D)

Title: A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A
  Knowledge-Based Perspective
Authors: Ziwen Zhao, Yuhua Li, Yixiong Zou, Ruixuan Li, Rui Zhang
Categories: cs.LG cs.SI
Comments: Work in progress
\\
  Graph self-supervised learning is now a go-to method for pre-training graph
foundation models, including graph neural networks, graph transformers, and
more recent large language model (LLM)-based graph models. There is a wide
variety of knowledge patterns embedded in the structure and properties of
graphs which may be used for pre-training, but we lack a systematic overview of
self-supervised pre-training tasks from the perspective of graph knowledge. In
this paper, we comprehensively survey and analyze the pre-training tasks of
graph foundation models from a knowledge-based perspective, consisting of
microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global
structure, etc). It covers a total of 9 knowledge categories and 25
pre-training tasks, as well as various downstream task adaptation strategies.
Furthermore, an extensive list of the related papers with detailed metadata is
provided at https://github.com/Newiz430/Pretext.
\\ ( https://arxiv.org/abs/2403.16137 ,  191kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16153
Date: Sun, 24 Mar 2024 13:44:57 GMT   (2016kb,D)

Title: One Masked Model is All You Need for Sensor Fault Detection, Isolation
  and Accommodation
Authors: Yiwei Fu, Weizhong Yan
Categories: cs.LG cs.AI
Comments: Accepted by the 2024 International Joint Conference on Neural
  Networks (IJCNN 2024)
\\
  Accurate and reliable sensor measurements are critical for ensuring the
safety and longevity of complex engineering systems such as wind turbines. In
this paper, we propose a novel framework for sensor fault detection, isolation,
and accommodation (FDIA) using masked models and self-supervised learning. Our
proposed approach is a general time series modeling approach that can be
applied to any neural network (NN) model capable of sequence modeling, and
captures the complex spatio-temporal relationships among different sensors.
During training, the proposed masked approach creates a random mask, which acts
like a fault, for one or more sensors, making the training and inference task
unified: finding the faulty sensors and correcting them. We validate our
proposed technique on both a public dataset and a real-world dataset from GE
offshore wind turbines, and demonstrate its effectiveness in detecting,
diagnosing and correcting sensor faults. The masked model not only simplifies
the overall FDIA pipeline, but also outperforms existing approaches. Our
proposed technique has the potential to significantly improve the accuracy and
reliability of sensor measurements in complex engineering systems in real-time,
and could be applied to other types of sensors and engineering systems in the
future. We believe that our proposed framework can contribute to the
development of more efficient and effective FDIA techniques for a wide range of
applications.
\\ ( https://arxiv.org/abs/2403.16153 ,  2016kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16163
Date: Sun, 24 Mar 2024 14:08:24 GMT   (899kb,D)

Title: An Analytic Solution to Covariance Propagation in Neural Networks
Authors: Oren Wright, Yorie Nakahira, Jos\'e M. F. Moura
Categories: cs.LG cs.AI stat.ML
Comments: Accepted to AISTATS 2024
\\
  Uncertainty quantification of neural networks is critical to measuring the
reliability and robustness of deep learning systems. However, this often
involves costly or inaccurate sampling methods and approximations. This paper
presents a sample-free moment propagation technique that propagates mean
vectors and covariance matrices across a network to accurately characterize the
input-output distributions of neural networks. A key enabler of our technique
is an analytic solution for the covariance of random variables passed through
nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide
applicability and merits of the proposed technique are shown in experiments
analyzing the input-output distributions of trained neural networks and
training Bayesian neural networks.
\\ ( https://arxiv.org/abs/2403.16163 ,  899kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16176
Date: Sun, 24 Mar 2024 14:35:44 GMT   (8079kb,D)

Title: Subspace Defense: Discarding Adversarial Perturbations by Learning a
  Subspace for Clean Signals
Authors: Rui Zheng, Yuhao Zhou, Zhiheng Xi, Tao Gui, Qi Zhang and Xuanjing
  Huang
Categories: cs.LG cs.CL
Comments: Accepted by COLING 2024
\\
  Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks
that place carefully crafted perturbations on normal examples to fool DNNs. To
better understand such attacks, a characterization of the features carried by
adversarial examples is needed. In this paper, we tackle this challenge by
inspecting the subspaces of sample features through spectral analysis. We first
empirically show that the features of either clean signals or adversarial
perturbations are redundant and span in low-dimensional linear subspaces
respectively with minimal overlap, and the classical low-dimensional subspace
projection can suppress perturbation features out of the subspace of clean
signals. This makes it possible for DNNs to learn a subspace where only
features of clean signals exist while those of perturbations are discarded,
which can facilitate the distinction of adversarial examples. To prevent the
residual perturbations that is inevitable in subspace learning, we propose an
independence criterion to disentangle clean signals from perturbations.
Experimental results show that the proposed strategy enables the model to
inherently suppress adversaries, which not only boosts model robustness but
also motivates new directions of effective adversarial defense.
\\ ( https://arxiv.org/abs/2403.16176 ,  8079kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16201
Date: Sun, 24 Mar 2024 15:48:29 GMT   (809kb,D)

Title: From Discrete to Continuous: Deep Fair Clustering With Transferable
  Representations
Authors: Xiang Zhang
Categories: cs.LG cs.CV
\\
  We consider the problem of deep fair clustering, which partitions data into
clusters via the representations extracted by deep neural networks while hiding
sensitive data attributes. To achieve fairness, existing methods present a
variety of fairness-related objective functions based on the group fairness
criterion. However, these works typically assume that the sensitive attributes
are discrete and do not work for continuous sensitive variables, such as the
proportion of the female population in an area. Besides, the potential of the
representations learned from clustering tasks to improve performance on other
tasks is ignored by existing works. In light of these limitations, we propose a
flexible deep fair clustering method that can handle discrete and continuous
sensitive attributes simultaneously. Specifically, we design an information
bottleneck style objective function to learn fair and clustering-friendly
representations. Furthermore, we explore for the first time the transferability
of the extracted representations to other downstream tasks. Unlike existing
works, we impose fairness at the representation level, which could guarantee
fairness for the transferred task regardless of clustering results. To verify
the effectiveness of the proposed method, we perform extensive experiments on
datasets with discrete and continuous sensitive attributes, demonstrating the
advantage of our method in comparison with state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.16201 ,  809kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16215
Date: Sun, 24 Mar 2024 16:16:41 GMT   (3344kb,D)

Title: Systematic construction of continuous-time neural networks for linear
  dynamical systems
Authors: Chinmay Datar, Adwait Datar, Felix Dietrich, Wil Schilders
Categories: cs.LG cs.NA math.DS math.NA
Comments: 37 pages, 25 figures
MSC-class: 93B17, 65L70, 68T07
ACM-class: I.2.m; G.1.3; G.1.7
\\
  Discovering a suitable neural network architecture for modeling complex
dynamical systems poses a formidable challenge, often involving extensive trial
and error and navigation through a high-dimensional hyper-parameter space. In
this paper, we discuss a systematic approach to constructing neural
architectures for modeling a subclass of dynamical systems, namely, Linear
Time-Invariant (LTI) systems. We use a variant of continuous-time neural
networks in which the output of each neuron evolves continuously as a solution
of a first-order or second-order Ordinary Differential Equation (ODE). Instead
of deriving the network architecture and parameters from data, we propose a
gradient-free algorithm to compute sparse architecture and network parameters
directly from the given LTI system, leveraging its properties. We bring forth a
novel neural architecture paradigm featuring horizontal hidden layers and
provide insights into why employing conventional neural architectures with
vertical hidden layers may not be favorable. We also provide an upper bound on
the numerical errors of our neural networks. Finally, we demonstrate the high
accuracy of our constructed networks on three numerical examples.
\\ ( https://arxiv.org/abs/2403.16215 ,  3344kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16233
Date: Sun, 24 Mar 2024 16:49:55 GMT   (6730kb,D)

Title: An early warning indicator trained on stochastic disease-spreading
  models with different noises
Authors: Amit K. Chakraborty, Shan Gao, Reza Miry, Pouria Ramazi, Russell
  Greiner, Mark A. Lewis, Hao Wang
Categories: cs.LG q-bio.PE
\\
  The timely detection of disease outbreaks through reliable early warning
signals (EWSs) is indispensable for effective public health mitigation
strategies. Nevertheless, the intricate dynamics of real-world disease spread,
often influenced by diverse sources of noise and limited data in the early
stages of outbreaks, pose a significant challenge in developing reliable EWSs,
as the performance of existing indicators varies with extrinsic and intrinsic
noises. Here, we address the challenge of modeling disease when the
measurements are corrupted by additive white noise, multiplicative
environmental noise, and demographic noise into a standard epidemic
mathematical model. To navigate the complexities introduced by these noise
sources, we employ a deep learning algorithm that provides EWS in infectious
disease outbreak by training on noise-induced disease-spreading models. The
indicator's effectiveness is demonstrated through its application to real-world
COVID-19 cases in Edmonton and simulated time series derived from diverse
disease spread models affected by noise. Notably, the indicator captures an
impending transition in a time series of disease outbreaks and outperforms
existing indicators. This study contributes to advancing early warning
capabilities by addressing the intricate dynamics inherent in real-world
disease spread, presenting a promising avenue for enhancing public health
preparedness and response efforts.
\\ ( https://arxiv.org/abs/2403.16233 ,  6730kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16244
Date: Sun, 24 Mar 2024 17:21:32 GMT   (39717kb,D)

Title: On the Equivalency, Substitutability, and Flexibility of Synthetic Data
Authors: Che-Jui Chang, Danrui Li, Seonghyeon Moon, Mubbasir Kapadia
Categories: cs.LG cs.CV
\\
  We study, from an empirical standpoint, the efficacy of synthetic data in
real-world scenarios. Leveraging synthetic data for training perception models
has become a key strategy embraced by the community due to its efficiency,
scalability, perfect annotations, and low costs. Despite proven advantages, few
studies put their stress on how to efficiently generate synthetic datasets to
solve real-world problems and to what extent synthetic data can reduce the
effort for real-world data collection. To answer the questions, we
systematically investigate several interesting properties of synthetic data --
the equivalency of synthetic data to real-world data, the substitutability of
synthetic data for real data, and the flexibility of synthetic data generators
to close up domain gaps. Leveraging the M3Act synthetic data generator, we
conduct experiments on DanceTrack and MOT17. Our results suggest that synthetic
data not only enhances model performance but also demonstrates substitutability
for real data, with 60% to 80% replacement without performance loss. In
addition, our study of the impact of synthetic data distributions on downstream
performance reveals the importance of flexible data generators in narrowing
domain gaps for improved model adaptability.
\\ ( https://arxiv.org/abs/2403.16244 ,  39717kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16246
Date: Sun, 24 Mar 2024 17:33:22 GMT   (171kb,D)

Title: Partially Blinded Unlearning: Class Unlearning for Deep Networks a
  Bayesian Perspective
Authors: Subhodip Panda and Shashwat Sourav and Prathosh A.P
Categories: cs.LG cs.CV
\\
  In order to adhere to regulatory standards governing individual data privacy
and safety, machine learning models must systematically eliminate information
derived from specific subsets of a user's training data that can no longer be
utilized. The emerging discipline of Machine Unlearning has arisen as a pivotal
area of research, facilitating the process of selectively discarding
information designated to specific sets or classes of data from a pre-trained
model, thereby eliminating the necessity for extensive retraining from scratch.
The principal aim of this study is to formulate a methodology tailored for the
purposeful elimination of information linked to a specific class of data from a
pre-trained classification network. This intentional removal is crafted to
degrade the model's performance specifically concerning the unlearned data
class while concurrently minimizing any detrimental impacts on the model's
performance in other classes. To achieve this goal, we frame the class
unlearning problem from a Bayesian perspective, which yields a loss function
that minimizes the log-likelihood associated with the unlearned data with a
stability regularization in parameter space. This stability regularization
incorporates Mohalanobis distance with respect to the Fisher Information matrix
and $l_2$ distance from the pre-trained model parameters. Our novel approach,
termed \textbf{Partially-Blinded Unlearning (PBU)}, surpasses existing
state-of-the-art class unlearning methods, demonstrating superior
effectiveness. Notably, PBU achieves this efficacy without requiring awareness
of the entire training dataset but only to the unlearned data points, marking a
distinctive feature of its performance.
\\ ( https://arxiv.org/abs/2403.16246 ,  171kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16260
Date: Sun, 24 Mar 2024 18:43:04 GMT   (1760kb,D)

Title: Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble
Authors: Chenhui Xu, Fuxun Yu, Zirui Xu, Nathan Inkawhich, Xiang Chen
Categories: cs.LG cs.AI cs.CV stat.ML
\\
  Recent research underscores the pivotal role of the Out-of-Distribution (OOD)
feature representation field scale in determining the efficacy of models in OOD
detection. Consequently, the adoption of model ensembles has emerged as a
prominent strategy to augment this feature representation field, capitalizing
on anticipated model diversity.
  However, our introduction of novel qualitative and quantitative model
ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and
the Self-Coupling Index, reveals a critical drawback in existing ensemble
methods. We find that these methods incorporate weights that are
affine-transformable, exhibiting limited variability and thus failing to
achieve the desired diversity in feature representation.
  To address this limitation, we elevate the dimensions of traditional model
ensembles, incorporating various factors such as different weight
initializations, data holdout, etc., into distinct supervision tasks. This
innovative approach, termed Multi-Comprehension (MC) Ensemble, leverages
diverse training tasks to generate distinct comprehensions of the data and
labels, thereby extending the feature representation field.
  Our experimental results demonstrate the superior performance of the MC
Ensemble strategy in OOD detection compared to both the naive Deep Ensemble
method and a standalone model of comparable size. This underscores the
effectiveness of our proposed approach in enhancing the model's capability to
detect instances outside its training distribution.
\\ ( https://arxiv.org/abs/2403.16260 ,  1760kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16282
Date: Sun, 24 Mar 2024 20:08:16 GMT   (601kb)

Title: The Evolution of Football Betting- A Machine Learning Approach to Match
  Outcome Forecasting and Bookmaker Odds Estimation
Authors: Purnachandra Mandadapu
Categories: cs.LG
\\
  This paper explores the significant history of professional football and the
betting industry, tracing its evolution from clandestine beginnings to a
lucrative multi-million-pound enterprise. Initiated by the legalization of
gambling in 1960 and complemented by advancements in football data gathering
pioneered by Thorold Charles Reep, the symbiotic relationship between these
sectors has propelled rapid growth and innovation. Over the past six decades,
both industries have undergone radical transformations, with data collection
methods evolving from rudimentary notetaking to sophisticated technologies such
as high-definition cameras and Artificial Intelligence (AI)-driven analytics.
Therefore, the primary aim of this study is to utilize Machine Learning (ML)
algorithms to forecast premier league football match outcomes. By analyzing
historical data and investigating the significance of various features, the
study seeks to identify the most effective predictive models and discern key
factors influencing match results. Additionally, the study aims to utilize
these forecasting to inform the establishment of bookmaker odds, providing
insights into the impact of different variables on match outcomes. By
highlighting the potential for informed decision-making in sports forecasting
and betting, this study opens up new avenues for research and practical
applications in the domain of sports analytics.
\\ ( https://arxiv.org/abs/2403.16282 ,  601kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16293
Date: Sun, 24 Mar 2024 20:56:16 GMT   (951kb,D)

Title: Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling
Authors: Boyang Li, Zhiling Lan, Michael E. Papka
Categories: cs.LG cs.DC
DOI: 10.1109/MASCOTS59514.2023.10387651
\\
  In the field of high-performance computing (HPC), there has been recent
exploration into the use of deep reinforcement learning for cluster scheduling
(DRL scheduling), which has demonstrated promising outcomes. However, a
significant challenge arises from the lack of interpretability in deep neural
networks (DNN), rendering them as black-box models to system managers. This
lack of model interpretability hinders the practical deployment of DRL
scheduling. In this work, we present a framework called IRL (Interpretable
Reinforcement Learning) to address the issue of interpretability of DRL
scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a
decision tree by utilizing imitation learning. Unlike DNN, decision tree models
are non-parametric and easily comprehensible to humans. To extract an effective
and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger)
algorithm and introduces the notion of critical state to prune the derived
decision tree. Through trace-based experiments, we demonstrate that IRL is
capable of converting a black-box DNN policy into an interpretable rulebased
decision tree while maintaining comparable scheduling performance.
Additionally, IRL can contribute to the setting of rewards in DRL scheduling.
\\ ( https://arxiv.org/abs/2403.16293 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16334
Date: Mon, 25 Mar 2024 00:15:34 GMT   (356kb,D)

Title: Graphs Generalization under Distribution Shifts
Authors: Qin Tian, Wenjun Wang, Chen Zhao, Minglai Shao, Wang Zhang, Dong Li
Categories: cs.LG cs.AI
\\
  Traditional machine learning methods heavily rely on the independent and
identically distribution assumption, which imposes limitations when the test
distribution deviates from the training distribution. To address this crucial
issue, out-of-distribution (OOD) generalization, which aims to achieve
satisfactory generalization performance when faced with unknown distribution
shifts, has made a significant process. However, the OOD method for
graph-structured data currently lacks clarity and remains relatively unexplored
due to two primary challenges. Firstly, distribution shifts on graphs often
occur simultaneously on node attributes and graph topology. Secondly, capturing
invariant information amidst diverse distribution shifts proves to be a
formidable challenge. To overcome these obstacles, in this paper, we introduce
a novel framework, namely Graph Learning Invariant Domain genERation (GLIDER).
The goal is to (1) diversify variations across domains by modeling the
potential seen or unseen variations of attribute distribution and topological
structure and (2) minimize the discrepancy of the variation in a representation
space where the target is to predict semantic labels. Extensive experiment
results indicate that our model outperforms baseline methods on node-level OOD
generalization across domains in distribution shift on node features and
topological structures simultaneously.
\\ ( https://arxiv.org/abs/2403.16334 ,  356kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16365
Date: Mon, 25 Mar 2024 02:03:38 GMT   (5535kb,D)

Title: Generating Potent Poisons and Backdoors from Scratch with Guided
  Diffusion
Authors: Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha,
  Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah
  Goldblum
Categories: cs.LG cs.CR cs.CV
\\
  Modern neural networks are often trained on massive datasets that are web
scraped with minimal human inspection. As a result of this insecure curation
pipeline, an adversary can poison or backdoor the resulting model by uploading
malicious data to the internet and waiting for a victim to scrape and train on
it. Existing approaches for creating poisons and backdoors start with randomly
sampled clean data, called base samples, and then modify those samples to craft
poisons. However, some base samples may be significantly more amenable to
poisoning than others. As a result, we may be able to craft more potent poisons
by carefully choosing the base samples. In this work, we use guided diffusion
to synthesize base samples from scratch that lead to significantly more potent
poisons and backdoors than previous state-of-the-art attacks. Our Guided
Diffusion Poisoning (GDP) base samples can be combined with any downstream
poisoning or backdoor attack to boost its effectiveness. Our implementation
code is publicly available at: https://github.com/hsouri/GDP .
\\ ( https://arxiv.org/abs/2403.16365 ,  5535kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16369
Date: Mon, 25 Mar 2024 02:17:54 GMT   (6719kb,D)

Title: Learning Action-based Representations Using Invariance
Authors: Max Rudolph, Caleb Chuck, Kevin Black, Misha Lvovsky, Scott Niekum,
  Amy Zhang
Categories: cs.LG cs.AI
\\
  Robust reinforcement learning agents using high-dimensional observations must
be able to identify relevant state features amidst many exogeneous distractors.
A representation that captures controllability identifies these state elements
by determining what affects agent control. While methods such as inverse
dynamics and mutual information capture controllability for a limited number of
timesteps, capturing long-horizon elements remains a challenging problem.
Myopic controllability can capture the moment right before an agent crashes
into a wall, but not the control-relevance of the wall while the agent is still
some distance away. To address this we introduce action-bisimulation encoding,
a method inspired by the bisimulation invariance pseudometric, that extends
single-step controllability with a recursive invariance constraint. By doing
this, action-bisimulation learns a multi-step controllability metric that
smoothly discounts distant state features that are relevant for control. We
demonstrate that action-bisimulation pretraining on reward-free, uniformly
random data improves sample efficiency in several environments, including a
photorealistic 3D simulation domain, Habitat. Additionally, we provide
theoretical analysis and qualitative results demonstrating the information
captured by action-bisimulation.
\\ ( https://arxiv.org/abs/2403.16369 ,  6719kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16372
Date: Mon, 25 Mar 2024 02:32:43 GMT   (2790kb,D)

Title: SignSGD with Federated Voting
Authors: Chanho Park, H. Vincent Poor, Namyoon Lee
Categories: cs.LG eess.SP
\\
  Distributed learning is commonly used for accelerating model training by
harnessing the computational capabilities of multiple-edge devices. However, in
practical applications, the communication delay emerges as a bottleneck due to
the substantial information exchange required between workers and a central
parameter server. SignSGD with majority voting (signSGD-MV) is an effective
distributed learning algorithm that can significantly reduce communication
costs by one-bit quantization. However, due to heterogeneous computational
capabilities, it fails to converge when the mini-batch sizes differ among
workers. To overcome this, we propose a novel signSGD optimizer with
\textit{federated voting} (signSGD-FV). The idea of federated voting is to
exploit learnable weights to perform weighted majority voting. The server
learns the weights assigned to the edge devices in an online fashion based on
their computational capabilities. Subsequently, these weights are employed to
decode the signs of the aggregated local gradients in such a way to minimize
the sign decoding error probability. We provide a unified convergence rate
analysis framework applicable to scenarios where the estimated weights are
known to the parameter server either perfectly or imperfectly. We demonstrate
that the proposed signSGD-FV algorithm has a theoretical convergence guarantee
even when edge devices use heterogeneous mini-batch sizes. Experimental results
show that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence
rate, especially in heterogeneous mini-batch sizes.
\\ ( https://arxiv.org/abs/2403.16372 ,  2790kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16374
Date: Mon, 25 Mar 2024 02:38:34 GMT   (3232kb,D)

Title: ProIn: Learning to Predict Trajectory Based on Progressive Interactions
  for Autonomous Driving
Authors: Yinke Dong, Haifeng Yuan, Hongkun Liu, Wei Jing, Fangzhen Li, Hongmin
  Liu, Bin Fan
Categories: cs.LG
\\
  Accurate motion prediction of pedestrians, cyclists, and other surrounding
vehicles (all called agents) is very important for autonomous driving. Most
existing works capture map information through an one-stage interaction with
map by vector-based attention, to provide map constraints for social
interaction and multi-modal differentiation. However, these methods have to
encode all required map rules into the focal agent's feature, so as to retain
all possible intentions' paths while at the meantime to adapt to potential
social interaction. In this work, a progressive interaction network is proposed
to enable the agent's feature to progressively focus on relevant maps, in order
to better learn agents' feature representation capturing the relevant map
constraints. The network progressively encode the complex influence of map
constraints into the agent's feature through graph convolutions at the
following three stages: after historical trajectory encoder, after social
interaction, and after multi-modal differentiation. In addition, a weight
allocation mechanism is proposed for multi-modal training, so that each mode
can obtain learning opportunities from a single-mode ground truth. Experiments
have validated the superiority of progressive interactions to the existing
one-stage interaction, and demonstrate the effectiveness of each component.
Encouraging results were obtained in the challenging benchmarks.
\\ ( https://arxiv.org/abs/2403.16374 ,  3232kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16377
Date: Mon, 25 Mar 2024 02:47:29 GMT   (4700kb,D)

Title: Real-time Adaptation for Condition Monitoring Signal Prediction using
  Label-aware Neural Processes
Authors: Seokhyun Chung, Raed Al Kontar
Categories: cs.LG stat.ML
\\
  Building a predictive model that rapidly adapts to real-time condition
monitoring (CM) signals is critical for engineering systems/units.
Unfortunately, many current methods suffer from a trade-off between
representation power and agility in online settings. For instance, parametric
methods that assume an underlying functional form for CM signals facilitate
efficient online prediction updates. However, this simplification leads to
vulnerability to model specifications and an inability to capture complex
signals. On the other hand, approaches based on over-parameterized or
non-parametric models can excel at explaining complex nonlinear signals, but
real-time updates for such models pose a challenging task. In this paper, we
propose a neural process-based approach that addresses this trade-off. It
encodes available observations within a CM signal into a representation space
and then reconstructs the signal's history and evolution for prediction. Once
trained, the model can encode an arbitrary number of observations without
requiring retraining, enabling on-the-spot real-time predictions along with
quantified uncertainty and can be readily updated as more online data is
gathered. Furthermore, our model is designed to incorporate qualitative
information (i.e., labels) from individual units. This integration not only
enhances individualized predictions for each unit but also enables joint
inference for both signals and their associated labels. Numerical studies on
both synthetic and real-world data in reliability engineering highlight the
advantageous features of our model in real-time adaptation, enhanced signal
prediction with uncertainty quantification, and joint prediction for labels and
signals.
\\ ( https://arxiv.org/abs/2403.16377 ,  4700kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16398
Date: Mon, 25 Mar 2024 03:26:01 GMT   (15616kb,D)

Title: Rethinking the Representation in Federated Unsupervised Learning with
  Non-IID Data
Authors: Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Fengyuan Yu,
  Huabin Zhu, Binhui Yao, Tao Wang, Xiaolin Zheng, and Yanchao Tan
Categories: cs.LG cs.AI
Comments: CVPR 2024
\\
  Federated learning achieves effective performance in modeling decentralized
data. In practice, client data are not well-labeled, which makes it potential
for federated unsupervised learning (FUSL) with non-IID data. However, the
performance of existing FUSL methods suffers from insufficient representations,
i.e., (1) representation collapse entanglement among local and global models,
and (2) inconsistent representation spaces among local models. The former
indicates that representation collapse in local model will subsequently impact
the global model and other local models. The latter means that clients model
data representation with inconsistent parameters due to the deficiency of
supervision signals. In this work, we propose FedU2 which enhances generating
uniform and unified representation in FUSL with non-IID data. Specifically,
FedU2 consists of flexible uniform regularizer (FUR) and efficient unified
aggregator (EUA). FUR in each client avoids representation collapse via
dispersing samples uniformly, and EUA in server promotes unified representation
by constraining consistent client model updating. To extensively validate the
performance of FedU2, we conduct both cross-device and cross-silo evaluation
experiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100.
\\ ( https://arxiv.org/abs/2403.16398 ,  15616kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16405
Date: Mon, 25 Mar 2024 03:44:36 GMT   (1079kb,D)

Title: Ensemble Adversarial Defense via Integration of Multiple Dispersed Low
  Curvature Models
Authors: Kaikang Zhao, Xi Chen, Wei Huang, Liuxin Ding, Xianglong Kong, Fan
  Zhang
Categories: cs.LG cs.CV
Comments: Accepted to The 2024 International Joint Conference on Neural
  Networks (IJCNN)
\\
  The integration of an ensemble of deep learning models has been extensively
explored to enhance defense against adversarial attacks. The diversity among
sub-models increases the attack cost required to deceive the majority of the
ensemble, thereby improving the adversarial robustness. While existing
approaches mainly center on increasing diversity in feature representations or
dispersion of first-order gradients with respect to input, the limited
correlation between these diversity metrics and adversarial robustness
constrains the performance of ensemble adversarial defense. In this work, we
aim to enhance ensemble diversity by reducing attack transferability. We
identify second-order gradients, which depict the loss curvature, as a key
factor in adversarial robustness. Computing the Hessian matrix involved in
second-order gradients is computationally expensive. To address this, we
approximate the Hessian-vector product using differential approximation. Given
that low curvature provides better robustness, our ensemble model was designed
to consider the influence of curvature among different sub-models. We introduce
a novel regularizer to train multiple more-diverse low-curvature network
models. Extensive experiments across various datasets demonstrate that our
ensemble model exhibits superior robustness against a range of attacks,
underscoring the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2403.16405 ,  1079kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16418
Date: Mon, 25 Mar 2024 04:43:47 GMT   (222kb)

Title: An incremental MaxSAT-based model to learn balanced rules
Authors: Ant\^onio Carlos Souza Ferreira J\'unior, Thiago Alves Rocha
Categories: cs.LG cs.AI cs.LO
Comments: 16 pages, 5 tables, submitted to BRACIS 2023 (Brazilian Conference on
  Intelligent Systems), accepted version published in Intelligent Systems,
  LNCS, vol 14195
ACM-class: I.2.4; I.2.6
Journal-ref: Intelligent Systems (2023), LNCS, vol 14195 (pp. 227-242),
  Springer Nature
DOI: 10.1007/978-3-031-45368-7_15
\\
  The increasing advancements in the field of machine learning have led to the
development of numerous applications that effectively address a wide range of
problems with accurate predictions. However, in certain cases, accuracy alone
may not be sufficient. Many real-world problems also demand explanations and
interpretability behind the predictions. One of the most popular interpretable
models that are classification rules. This work aims to propose an incremental
model for learning interpretable and balanced rules based on MaxSAT, called
IMLIB. This new model was based on two other approaches, one based on SAT and
the other on MaxSAT. The one based on SAT limits the size of each generated
rule, making it possible to balance them. We suggest that such a set of rules
seem more natural to be understood compared to a mixture of large and small
rules. The approach based on MaxSAT, called IMLI, presents a technique to
increase performance that involves learning a set of rules by incrementally
applying the model in a dataset. Finally, IMLIB and IMLI are compared using
diverse databases. IMLIB obtained results comparable to IMLI in terms of
accuracy, generating more balanced rules with smaller sizes.
\\ ( https://arxiv.org/abs/2403.16418 ,  222kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16451
Date: Mon, 25 Mar 2024 06:30:54 GMT   (28061kb,D)

Title: DeepMachining: Online Prediction of Machining Errors of Lathe Machines
Authors: Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, and Chen-Hsin
  Lee
Categories: cs.LG cs.AI
\\
  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
\\ ( https://arxiv.org/abs/2403.16451 ,  28061kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16459
Date: Mon, 25 Mar 2024 06:42:02 GMT   (59kb,D)

Title: On the rates of convergence for learning with convolutional neural
  networks
Authors: Yunfei Yang, Han Feng, Ding-Xuan Zhou
Categories: cs.LG math.ST stat.ML stat.TH
\\
  We study the approximation and learning capacities of convolutional neural
networks (CNNs). Our first result proves a new approximation bound for CNNs
with certain constraint on the weights. Our second result gives a new analysis
on the covering number of feed-forward neural networks, which include CNNs as
special cases. The analysis carefully takes into account the size of the
weights and hence gives better bounds than existing literature in some
situations. Using these two results, we are able to derive rates of convergence
for estimators based on CNNs in many learning problems. In particular, we
establish minimax optimal convergence rates of the least squares based on CNNs
for learning smooth functions in the nonparametric regression setting. For
binary classification, we derive convergence rates for CNN classifiers with
hinge loss and logistic loss. It is also shown that the obtained rates are
minimax optimal in several settings.
\\ ( https://arxiv.org/abs/2403.16459 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16460
Date: Mon, 25 Mar 2024 06:43:28 GMT   (1699kb,D)

Title: FedAC: A Adaptive Clustered Federated Learning Framework for
  Heterogeneous Data
Authors: Yuxin Zhang, Haoyu Chen, Zheng Lin, Zhe Chen, Jin Zhao
Categories: cs.LG cs.AI cs.DC
Comments: 14 pages, 4 figures
\\
  Clustered federated learning (CFL) is proposed to mitigate the performance
deterioration stemming from data heterogeneity in federated learning (FL) by
grouping similar clients for cluster-wise model training. However, current CFL
methods struggle due to inadequate integration of global and intra-cluster
knowledge and the absence of an efficient online model similarity metric, while
treating the cluster count as a fixed hyperparameter limits flexibility and
robustness. In this paper, we propose an adaptive CFL framework, named FedAC,
which (1) efficiently integrates global knowledge into intra-cluster learning
by decoupling neural networks and utilizing distinct aggregation methods for
each submodule, significantly enhancing performance; (2) includes a
costeffective online model similarity metric based on dimensionality reduction;
(3) incorporates a cluster number fine-tuning module for improved adaptability
and scalability in complex, heterogeneous environments. Extensive experiments
show that FedAC achieves superior empirical performance, increasing the test
accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets,
respectively, under different non-IID settings compared to SOTA methods.
\\ ( https://arxiv.org/abs/2403.16460 ,  1699kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16469
Date: Mon, 25 Mar 2024 06:50:25 GMT   (817kb,D)

Title: Learning from Reduced Labels for Long-Tailed Data
Authors: Meng Wei, Zhongnian Li, Yong Zhou, Xinzheng Xu
Categories: cs.LG
Comments: 12 pages, 3 figures
\\
  Long-tailed data is prevalent in real-world classification tasks and heavily
relies on supervised information, which makes the annotation process
exceptionally labor-intensive and time-consuming. Unfortunately, despite being
a common approach to mitigate labeling costs, existing weakly supervised
learning methods struggle to adequately preserve supervised information for
tail samples, resulting in a decline in accuracy for the tail classes. To
alleviate this problem, we introduce a novel weakly supervised labeling setting
called Reduced Label. The proposed labeling setting not only avoids the decline
of supervised information for the tail samples, but also decreases the labeling
costs associated with long-tailed data. Additionally, we propose an
straightforward and highly efficient unbiased framework with strong theoretical
guarantees to learn from these Reduced Labels. Extensive experiments conducted
on benchmark datasets including ImageNet validate the effectiveness of our
approach, surpassing the performance of state-of-the-art weakly supervised
methods.
\\ ( https://arxiv.org/abs/2403.16469 ,  817kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16482
Date: Mon, 25 Mar 2024 07:08:01 GMT   (850kb,D)

Title: Determined Multi-Label Learning via Similarity-Based Prompt
Authors: Meng Wei, Zhongnian Li, Peng Ying, Yong Zhou, Xinzheng Xu
Categories: cs.LG
Comments: 10 pages, 4 figures
\\
  In multi-label classification, each training instance is associated with
multiple class labels simultaneously. Unfortunately, collecting the fully
precise class labels for each training instance is time- and labor-consuming
for real-world applications. To alleviate this problem, a novel labeling
setting termed \textit{Determined Multi-Label Learning} (DMLL) is proposed,
aiming to effectively alleviate the labeling cost inherent in multi-label
tasks. In this novel labeling setting, each training instance is associated
with a \textit{determined label} (either "Yes" or "No"), which indicates
whether the training instance contains the provided class label. The provided
class label is randomly and uniformly selected from the whole candidate labels
set. Besides, each training instance only need to be determined once, which
significantly reduce the annotation cost of the labeling task for multi-label
datasets. In this paper, we theoretically derive an risk-consistent estimator
to learn a multi-label classifier from these determined-labeled training data.
Additionally, we introduce a similarity-based prompt learning method for the
first time, which minimizes the risk-consistent loss of large-scale pre-trained
models to learn a supplemental prompt with richer semantic information.
Extensive experimental validation underscores the efficacy of our approach,
demonstrating superior performance compared to existing state-of-the-art
methods.
\\ ( https://arxiv.org/abs/2403.16482 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16495
Date: Mon, 25 Mar 2024 07:23:23 GMT   (6108kb,D)

Title: LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural
  Network for Traffic Flow Forecasting
Authors: Qinyao Luo, Silu He, Xing Han, Yuhan Wang, Haifeng Li
Categories: cs.LG cs.AI cs.SI
Comments: 15 pages, 10 figures, 6 tables
Journal-ref: Knowledge-Based Systems 2024
DOI: 10.1016/j.knosys.2024.111637
\\
  Accurate traffic forecasting is a fundamental problem in intelligent
transportation systems and learning long-range traffic representations with key
information through spatiotemporal graph neural networks (STGNNs) is a basic
assumption of current traffic flow prediction models. However, due to
structural limitations, existing STGNNs can only utilize short-range traffic
flow data; therefore, the models cannot adequately learn the complex trends and
periodic features in traffic flow. Besides, it is challenging to extract the
key temporal information from the long historical traffic series and obtain a
compact representation. To solve the above problems, we propose a novel LSTTN
(Long-Short Term Transformer-based Network) framework comprehensively
considering the long- and short-term features in historical traffic flow.
First, we employ a masked subseries Transformer to infer the content of masked
subseries from a small portion of unmasked subseries and their temporal context
in a pretraining manner, forcing the model to efficiently learn compressed and
contextual subseries temporal representations from long historical series.
Then, based on the learned representations, long-term trend is extracted by
using stacked 1D dilated convolution layers, and periodic features are
extracted by dynamic graph convolution layers. For the difficulties in making
time-step level prediction, LSTTN adopts a short-term trend extractor to learn
fine-grained short-term temporal features. Finally, LSTTN fuses the long-term
trend, periodic features and short-term features to obtain the prediction
results. Experiments on four real-world datasets show that in 60-minute-ahead
long-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\%
and a maximum improvement of 16.78\% over baseline models. The source code is
available at https://github.com/GeoX-Lab/LSTTN.
\\ ( https://arxiv.org/abs/2403.16495 ,  6108kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16509
Date: Mon, 25 Mar 2024 07:48:34 GMT   (144kb,D)

Title: Human Understanding AI Paper Challenge 2024 -- Dataset Design
Authors: Se Won Oh, Hyuntae Jeong, Jeong Mook Lim, Seungeun Chung, Kyoung Ju
  Noh
Categories: cs.LG
Comments: 7 pages, 3 figures
ACM-class: J.7; E.m
\\
  In 2024, we will hold a research paper competition (the third Human
Understanding AI Paper Challenge) for the research and development of
artificial intelligence technologies to understand human daily life. This
document introduces the datasets that will be provided to participants in the
competition, and summarizes the issues to consider in data processing and
learning model development.
\\ ( https://arxiv.org/abs/2403.16509 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16542
Date: Mon, 25 Mar 2024 08:35:19 GMT   (423kb,D)

Title: Differentially Private Online Federated Learning with Correlated Noise
Authors: Jiaojiao Zhang and Linglingzhi Zhu and Mikael Johansson
Categories: cs.LG cs.CR cs.DC
Comments: 11 pages
\\
  We propose a novel differentially private algorithm for online federated
learning that employs temporally correlated noise to improve the utility while
ensuring the privacy of the continuously released models. To address challenges
stemming from DP noise and local updates with streaming noniid data, we develop
a perturbed iterate analysis to control the impact of the DP noise on the
utility. Moreover, we demonstrate how the drift errors from local updates can
be effectively managed under a quasi-strong convexity condition. Subject to an
$(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the
entire time horizon that quantifies the impact of key parameters and the
intensity of changes in dynamic environments. Numerical experiments validate
the efficacy of the proposed algorithm.
\\ ( https://arxiv.org/abs/2403.16542 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16557
Date: Mon, 25 Mar 2024 09:16:59 GMT   (7561kb)

Title: Accelerating Federated Learning by Selecting Beneficial Herd of Local
  Gradients
Authors: Ping Luo, Xiaoge Deng, Ziqing Wen, Tao Sun, Dongsheng Li
Categories: cs.LG cs.DC
\\
  Federated Learning (FL) is a distributed machine learning framework in
communication network systems. However, the systems' Non-Independent and
Identically Distributed (Non-IID) data negatively affect the convergence
efficiency of the global model, since only a subset of these data samples are
beneficial for model convergence. In pursuit of this subset, a reliable
approach involves determining a measure of validity to rank the samples within
the dataset. In this paper, We propose the BHerd strategy which selects a
beneficial herd of local gradients to accelerate the convergence of the FL
model. Specifically, we map the distribution of the local dataset to the local
gradients and use the Herding strategy to obtain a permutation of the set of
gradients, where the more advanced gradients in the permutation are closer to
the average of the set of gradients. These top portion of the gradients will be
selected and sent to the server for global aggregation. We conduct experiments
on different datasets, models and scenarios by building a prototype system, and
experimental results demonstrate that our BHerd strategy is effective in
selecting beneficial local gradients to mitigate the effects brought by the
Non-IID dataset, thus accelerating model convergence.
\\ ( https://arxiv.org/abs/2403.16557 ,  7561kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16561
Date: Mon, 25 Mar 2024 09:24:05 GMT   (505kb,D)

Title: FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning
Authors: Xinyuan Ji and Zhaowei Zhu and Wei Xi and Olga Gadyatskaya and Zilong
  Song and Yong Cai and Yang Liu
Categories: cs.LG cs.AI
Comments: accepted by AAA24
\\
  Federated Learning (FL) heavily depends on label quality for its performance.
However, the label distribution among individual clients is always both noisy
and heterogeneous. The high loss incurred by client-specific samples in
heterogeneous label noise poses challenges for distinguishing between
client-specific and noisy label samples, impacting the effectiveness of
existing label noise learning approaches. To tackle this issue, we propose
FedFixer, where the personalized model is introduced to cooperate with the
global model to effectively select clean client-specific samples. In the dual
models, updating the personalized model solely at a local level can lead to
overfitting on noisy data due to limited samples, consequently affecting both
the local and global models' performance. To mitigate overfitting, we address
this concern from two perspectives. Firstly, we employ a confidence regularizer
to alleviate the impact of unconfident predictions caused by label noise.
Secondly, a distance regularizer is implemented to constrain the disparity
between the personalized and global models. We validate the effectiveness of
FedFixer through extensive experiments on benchmark datasets. The results
demonstrate that FedFixer can perform well in filtering noisy label samples on
different clients, especially in highly heterogeneous label noise scenarios.
\\ ( https://arxiv.org/abs/2403.16561 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16569
Date: Mon, 25 Mar 2024 09:36:10 GMT   (7209kb,D)

Title: Revealing Vulnerabilities of Neural Networks in Parameter Learning and
  Defense Against Explanation-Aware Backdoors
Authors: Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag
Categories: cs.LG cs.CV
\\
  Explainable Artificial Intelligence (XAI) strategies play a crucial part in
increasing the understanding and trustworthiness of neural networks.
Nonetheless, these techniques could potentially generate misleading
explanations. Blinding attacks can drastically alter a machine learning
algorithm's prediction and explanation, providing misleading information by
adding visually unnoticeable artifacts into the input, while maintaining the
model's accuracy. It poses a serious challenge in ensuring the reliability of
XAI methods. To ensure the reliability of XAI methods poses a real challenge,
we leverage statistical analysis to highlight the changes in CNN weights within
a CNN following blinding attacks. We introduce a method specifically designed
to limit the effectiveness of such attacks during the evaluation phase,
avoiding the need for extra training. The method we suggest defences against
most modern explanation-aware adversarial attacks, achieving an approximate
decrease of ~99\% in the Attack Success Rate (ASR) and a ~91\% reduction in the
Mean Square Error (MSE) between the original explanation and the defended
(post-attack) explanation across three unique types of attacks.
\\ ( https://arxiv.org/abs/2403.16569 ,  7209kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16582
Date: Mon, 25 Mar 2024 09:49:42 GMT   (5837kb,D)

Title: In the Search for Optimal Multi-view Learning Models for Crop
  Classification with Global Remote Sensing Data
Authors: Francisco Mena, Diego Arenas, Andreas Dengel
Categories: cs.LG cs.AI cs.CV
Comments: submitted to journal
\\
  Crop classification is of critical importance due to its role in studying
crop pattern changes, resource management, and carbon sequestration. When
employing data-driven techniques for its prediction, utilizing various temporal
data sources is necessary. Deep learning models have proven to be effective for
this task by mapping time series data to high-level representation for
prediction. However, they face substantial challenges when dealing with
multiple input patterns. The literature offers limited guidance for Multi-View
Learning (MVL) scenarios, as it has primarily focused on exploring fusion
strategies with specific encoders and validating them in local regions. In
contrast, we investigate the impact of simultaneous selection of the fusion
strategy and the encoder architecture evaluated on a global-scale cropland and
crop-type classifications. We use a range of five fusion strategies (Input,
Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures
(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The
validation is on the CropHarvest dataset that provides optical, radar, and
weather time series, and topographic information as input data. We found that
in scenarios with a limited number of labeled samples, a unique configuration
is insufficient for all the cases. Instead, a specialized combination,
including encoder and fusion strategy, should be meticulously sought. To
streamline this search process, we suggest initially identifying the optimal
encoder architecture tailored for a particular fusion strategy, and then
determining the most suitable fusion strategy for the classification task. We
provide a technical framework for researchers exploring crop classification or
related tasks through a MVL approach.
\\ ( https://arxiv.org/abs/2403.16582 ,  5837kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16591
Date: Mon, 25 Mar 2024 10:06:45 GMT   (948kb,D)

Title: Deciphering the Interplay between Local Differential Privacy, Average
  Bayesian Privacy, and Maximum Bayesian Privacy
Authors: Xiaojin Zhang, Yulin Fei, Wei Chen, Hai Jin
Categories: cs.LG cs.AI
\\
  The swift evolution of machine learning has led to emergence of various
definitions of privacy due to the threats it poses to privacy, including the
concept of local differential privacy (LDP). Although widely embraced and
utilized across numerous domains, this conventional approach to measure privacy
still exhibits certain limitations, spanning from failure to prevent
inferential disclosure to lack of consideration for the adversary's background
knowledge. In this comprehensive study, we introduce Bayesian privacy and delve
into the intricate relationship between local differential privacy and its
Bayesian counterparts, unveiling novel insights into utility-privacy
trade-offs. We introduce a framework that encapsulates both attack and defense
strategies, highlighting their interplay and effectiveness. Our theoretical
contributions are anchored in the rigorous definitions and relationships
between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),
encapsulated by equations $\epsilon_{p,a} \leq
\frac{1}{\sqrt{2}}\sqrt{(\epsilon_{p,m} + \epsilon)\cdot(e^{\epsilon_{p,m} +
\epsilon} - 1)}$ and the equivalence between $\xi$-MBP and $2\xi$-LDP
established under uniform prior distribution. These relationships fortify our
understanding of the privacy guarantees provided by various mechanisms, leading
to the realization that a mechanism satisfying $\xi$-LDP also confers
$\xi$-MBP, and vice versa. Our work not only lays the groundwork for future
empirical exploration but also promises to enhance the design of
privacy-preserving algorithms that do not compromise on utility, thereby
fostering the development of trustworthy machine learning solutions.
\\ ( https://arxiv.org/abs/2403.16591 ,  948kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16607
Date: Mon, 25 Mar 2024 10:38:17 GMT   (5377kb,D)

Title: Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction
  and Defect-Focus
Authors: Chen Li, Ruijie Ma, Xiang Qian, Xiaohao Wang, Xinghui Li
Categories: cs.LG cs.CV
Comments: 17 pages, 11 figures,4 tables
\\
  Addressing the challenge of data scarcity in industrial domains, transfer
learning emerges as a pivotal paradigm. This work introduces Style Filter, a
tailored methodology for industrial contexts. By selectively filtering source
domain data before knowledge transfer, Style Filter reduces the quantity of
data while maintaining or even enhancing the performance of transfer learning
strategy. Offering label-free operation, minimal reliance on prior knowledge,
independence from specific models, and re-utilization, Style Filter is
evaluated on authentic industrial datasets, highlighting its effectiveness when
employed before conventional transfer strategies in the deep learning domain.
The results underscore the effectiveness of Style Filter in real-world
industrial applications.
\\ ( https://arxiv.org/abs/2403.16607 ,  5377kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16612
Date: Mon, 25 Mar 2024 10:42:48 GMT   (8352kb,D)

Title: Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting
Authors: Busra Asan, Abdullah Akgul, Alper Unal, Melih Kandemir, Gozde Unal
Categories: cs.LG cs.CV
Comments: Accepted as a workshop paper at "ICLR 2024 Tackling Climate Change
  with Machine Learning"
\\
  Seasonal forecasting is a crucial task when it comes to detecting the extreme
heat and colds that occur due to climate change. Confidence in the predictions
should be reliable since a small increase in the temperatures in a year has a
big impact on the world. Calibration of the neural networks provides a way to
ensure our confidence in the predictions. However, calibrating regression
models is an under-researched topic, especially in forecasters. We calibrate a
UNet++ based architecture, which was shown to outperform physics-based models
in temperature anomalies. We show that with a slight trade-off between
prediction error and calibration error, it is possible to get more reliable and
sharper forecasts. We believe that calibration should be an important part of
safety-critical machine learning applications such as weather forecasters.
\\ ( https://arxiv.org/abs/2403.16612 ,  8352kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16654
Date: Mon, 25 Mar 2024 11:42:01 GMT   (47kb)

Title: A Novel Loss Function-based Support Vector Machine for Binary
  Classification
Authors: Yan Li and Liping Zhang
Categories: cs.LG math.OC
\\
  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss
SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the
degree of penalty for the correctly classified samples within the margin. This
oversight affects the generalization ability of the SVM classifier to some
extent. To address this limitation, from the perspective of confidence margin,
we propose a novel Slide loss function ($\ell_s$) to construct the support
vector machine classifier($\ell_s$-SVM). By introducing the concept of proximal
stationary point, and utilizing the property of Lipschitz continuity, we derive
the first-order optimality conditions for $\ell_s$-SVM. Based on this, we
define the $\ell_s$ support vectors and working set of $\ell_s$-SVM. To
efficiently handle $\ell_s$-SVM, we devise a fast alternating direction method
of multipliers with the working set ($\ell_s$-ADMM), and provide the
convergence analysis. The numerical experiments on real world datasets confirm
the robustness and effectiveness of the proposed method.
\\ ( https://arxiv.org/abs/2403.16654 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16656
Date: Mon, 25 Mar 2024 11:47:53 GMT   (2165kb,D)

Title: Graph Augmentation for Recommendation
Authors: Qianru Zhang and Lianghao Xia and Xuheng Cai and Siuming Yiu and Chao
  Huang and Christian S. Jensen
Categories: cs.LG
Comments: 13 pages and accepted by ICDE 2024
Journal-ref: ICDE 2024
\\
  Graph augmentation with contrastive learning has gained significant attention
in the field of recommendation systems due to its ability to learn expressive
user representations, even when labeled data is limited. However, directly
applying existing GCL models to real-world recommendation environments poses
challenges. There are two primary issues to address. Firstly, the lack of
consideration for data noise in contrastive learning can result in noisy
self-supervised signals, leading to degraded performance. Secondly, many
existing GCL approaches rely on graph neural network (GNN) architectures, which
can suffer from over-smoothing problems due to non-adaptive message passing. To
address these challenges, we propose a principled framework called GraphAug.
This framework introduces a robust data augmentor that generates denoised
self-supervised signals, enhancing recommender systems. The GraphAug framework
incorporates a graph information bottleneck (GIB)-regularized augmentation
paradigm, which automatically distills informative self-supervision information
and adaptively adjusts contrastive view generation. Through rigorous
experimentation on real-world datasets, we thoroughly assessed the performance
of our novel GraphAug model. The outcomes consistently unveil its superiority
over existing baseline methods. The source code for our model is publicly
available at: https://github.com/HKUDS/GraphAug.
\\ ( https://arxiv.org/abs/2403.16656 ,  2165kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16677
Date: Mon, 25 Mar 2024 12:14:48 GMT   (4385kb,D)

Title: FOOL: Addressing the Downlink Bottleneck in Satellite Computing with
  Neural Feature Compression
Authors: Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter,
  Shangguang Wang, Schahram Dustdar
Categories: cs.LG cs.CV cs.DC cs.NI eess.IV
Comments: 18 pages, double column, 19 figures, 7 tables, Initial Submission to
  IEEE Transactions on Mobile Computing
\\
  Nanosatellite constellations equipped with sensors capturing large geographic
regions provide unprecedented opportunities for Earth observation. As
constellation sizes increase, network contention poses a downlink bottleneck.
Orbital Edge Computing (OEC) leverages limited onboard compute resources to
reduce transfer costs by processing the raw captures at the source. However,
current solutions have limited practicability due to reliance on crude
filtering methods or over-prioritizing particular downstream tasks.
  This work presents FOOL, an OEC-native and task-agnostic feature compression
method that preserves prediction performance. FOOL partitions high-resolution
satellite imagery to maximize throughput. Further, it embeds context and
leverages inter-tile dependencies to lower transfer costs with negligible
overhead. While FOOL is a feature compressor, it can recover images with
competitive scores on perceptual quality measures at lower bitrates. We
extensively evaluate transfer cost reduction by including the peculiarity of
intermittently available network connections in low earth orbit. Lastly, we
test the feasibility of our system for standardized nanosatellite form factors.
We demonstrate that FOOL permits downlinking over 100x the data volume without
relying on prior information on the downstream tasks.
\\ ( https://arxiv.org/abs/2403.16677 ,  4385kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16680
Date: Mon, 25 Mar 2024 12:15:47 GMT   (44910kb,D)

Title: Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics
Authors: Rene Winchenbach and Nils Thuerey
Categories: cs.LG physics.comp-ph
Comments: Published at International Conference on Learning Representation
  (ICLR) 2024, 54 pages, 39 figures
\\
  Learning physical simulations has been an essential and central aspect of
many recent research efforts in machine learning, particularly for
Navier-Stokes-based fluid mechanics. Classic numerical solvers have
traditionally been computationally expensive and challenging to use in inverse
problems, whereas Neural solvers aim to address both concerns through machine
learning. We propose a general formulation for continuous convolutions using
separable basis functions as a superset of existing methods and evaluate a
large set of basis functions in the context of (a) a compressible 1D SPH
simulation, (b) a weakly compressible 2D SPH simulation, and (c) an
incompressible 2D SPH Simulation. We demonstrate that even and odd symmetries
included in the basis functions are key aspects of stability and accuracy. Our
broad evaluation shows that Fourier-based continuous convolutions outperform
all other architectures regarding accuracy and generalization. Finally, using
these Fourier-based networks, we show that prior inductive biases, such as
window functions, are no longer necessary. An implementation of our approach,
as well as complete datasets and solver implementations, is available at
https://github.com/tum-pbs/SFBC.
\\ ( https://arxiv.org/abs/2403.16680 ,  44910kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16707
Date: Mon, 25 Mar 2024 12:44:52 GMT   (745kb,D)

Title: One-Shot Domain Incremental Learning
Authors: Yasushi Esaki and Satoshi Koide and Takuro Kutsuna
Categories: cs.LG cs.AI
Comments: accepted at IEEE International Joint Conference on Neural Networks
  (IJCNN) 2024
\\
  Domain incremental learning (DIL) has been discussed in previous studies on
deep neural network models for classification. In DIL, we assume that samples
on new domains are observed over time. The models must classify inputs on all
domains. In practice, however, we may encounter a situation where we need to
perform DIL under the constraint that the samples on the new domain are
observed only infrequently. Therefore, in this study, we consider the extreme
case where we have only one sample from the new domain, which we call one-shot
DIL. We first empirically show that existing DIL methods do not work well in
one-shot DIL. We have analyzed the reason for this failure through various
investigations. According to our analysis, we clarify that the difficulty of
one-shot DIL is caused by the statistics in the batch normalization layers.
Therefore, we propose a technique regarding these statistics and demonstrate
the effectiveness of our technique through experiments on open datasets.
\\ ( https://arxiv.org/abs/2403.16707 ,  745kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16768
Date: Mon, 25 Mar 2024 13:46:09 GMT   (685kb,D)

Title: DeepKnowledge: Generalisation-Driven Deep Learning Testing
Authors: Sondess Missaoui, Simos Gerasimou, Nikolaos Matragkas
Categories: cs.LG cs.AI cs.SE
Comments: 10 pages
\\
  Despite their unprecedented success, DNNs are notoriously fragile to small
shifts in data distribution, demanding effective testing techniques that can
assess their dependability. Despite recent advances in DNN testing, there is a
lack of systematic testing approaches that assess the DNN's capability to
generalise and operate comparably beyond data in their training distribution.
We address this gap with DeepKnowledge, a systematic testing methodology for
DNN-based systems founded on the theory of knowledge generalisation, which aims
to enhance DNN robustness and reduce the residual risk of 'black box' models.
Conforming to this theory, DeepKnowledge posits that core computational DNN
units, termed Transfer Knowledge neurons, can generalise under domain shift.
DeepKnowledge provides an objective confidence measurement on testing
activities of DNN given data distribution shifts and uses this information to
instrument a generalisation-informed test adequacy criterion to check the
transfer knowledge capacity of a test set. Our empirical evaluation of several
DNNs, across multiple datasets and state-of-the-art adversarial generation
techniques demonstrates the usefulness and effectiveness of DeepKnowledge and
its ability to support the engineering of more dependable DNNs. We report
improvements of up to 10 percentage points over state-of-the-art coverage
criteria for detecting adversarial attacks on several benchmarks, including
MNIST, SVHN, and CIFAR.
\\ ( https://arxiv.org/abs/2403.16768 ,  685kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16782
Date: Mon, 25 Mar 2024 13:57:45 GMT   (4238kb,D)

Title: The Anatomy of Adversarial Attacks: Concept-based XAI Dissection
Authors: Georgii Mikriukov, Gesina Schwalbe, Franz Motzkus, Korinna Bade
Categories: cs.LG cs.AI cs.CV
\\
  Adversarial attacks (AAs) pose a significant threat to the reliability and
robustness of deep neural networks. While the impact of these attacks on model
predictions has been extensively studied, their effect on the learned
representations and concepts within these models remains largely unexplored. In
this work, we perform an in-depth analysis of the influence of AAs on the
concepts learned by convolutional neural networks (CNNs) using eXplainable
artificial intelligence (XAI) techniques. Through an extensive set of
experiments across various network architectures and targeted AA techniques, we
unveil several key findings. First, AAs induce substantial alterations in the
concept composition within the feature space, introducing new concepts or
modifying existing ones. Second, the adversarial perturbation itself can be
linearly decomposed into a set of latent vector components, with a subset of
these being responsible for the attack's success. Notably, we discover that
these components are target-specific, i.e., are similar for a given target
class throughout different AA techniques and starting classes. Our findings
provide valuable insights into the nature of AAs and their impact on learned
representations, paving the way for the development of more robust and
interpretable deep learning models, as well as effective defenses against
adversarial threats.
\\ ( https://arxiv.org/abs/2403.16782 ,  4238kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16790
Date: Mon, 25 Mar 2024 14:05:52 GMT   (5485kb,D)

Title: Iso-Diffusion: Improving Diffusion Probabilistic Models Using the
  Isotropy of the Additive Gaussian Noise
Authors: Dilum Fernando, Dhananjaya jayasundara, Roshan Godaliyadda, Chaminda
  Bandara, Parakrama Ekanayake, Vijitha Herath
Categories: cs.LG
\\
  Denoising Diffusion Probabilistic Models (DDPMs) have accomplished much in
the realm of generative AI. Despite their high performance, there is room for
improvement, especially in terms of sample fidelity by utilizing statistical
properties that impose structural integrity, such as isotropy. Minimizing the
mean squared error between the additive and predicted noise alone does not
impose constraints on the predicted noise to be isotropic. Thus, we were
motivated to utilize the isotropy of the additive noise as a constraint on the
objective function to enhance the fidelity of DDPMs. Our approach is simple and
can be applied to any DDPM variant. We validate our approach by presenting
experiments conducted on four synthetic 2D datasets as well as on unconditional
image generation. As demonstrated by the results, the incorporation of this
constraint improves the fidelity metrics, Precision and Density for the 2D
datasets as well as for the unconditional image generation.
\\ ( https://arxiv.org/abs/2403.16790 ,  5485kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16798
Date: Mon, 25 Mar 2024 14:17:38 GMT   (8154kb,D)

Title: Cluster-Based Normalization Layer for Neural Networks
Authors: Bilal Faye, Hanane Azzag, Mustapha Lebbah
Categories: cs.LG cs.AI
\\
  Deep learning faces significant challenges during the training of neural
networks, including internal covariate shift, label shift, vanishing/exploding
gradients, overfitting, and computational complexity. While conventional
normalization methods, such as Batch Normalization, aim to tackle some of these
issues, they often depend on assumptions that constrain their adaptability.
Mixture Normalization faces computational hurdles in its pursuit of handling
multiple Gaussian distributions.
  This paper introduces Cluster-Based Normalization (CB-Norm) in two variants -
Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised
Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step
normalization approach. CB-Norm leverages a Gaussian mixture model to
specifically address challenges related to gradient stability and learning
acceleration.
  For SCB-Norm, a supervised variant, the novel mechanism involves introducing
predefined data partitioning, termed clusters, to normalize activations based
on the assigned cluster. This cluster-driven approach creates a space that
conforms to a Gaussian mixture model. On the other hand, UCB-Norm, an
unsupervised counterpart, dynamically clusters neuron activations during
training, adapting to task-specific challenges without relying on predefined
data partitions (clusters). This dual approach ensures flexibility in
addressing diverse learning scenarios.
  CB-Norm innovatively uses a one-step normalization approach, where parameters
of each mixture component (cluster in activation space) serve as weights for
deep neural networks. This adaptive clustering process tackles both clustering
and resolution of deep neural network tasks concurrently during training,
signifying a notable advancement in the field.
\\ ( https://arxiv.org/abs/2403.16798 ,  8154kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16818
Date: Mon, 25 Mar 2024 14:46:24 GMT   (222kb,D)

Title: Multiple-Source Localization from a Single-Snapshot Observation Using
  Graph Bayesian Optimization
Authors: Zonghan Zhang, Zijian Zhang, Zhiqian Chen
Categories: cs.LG
Comments: Proceedings of the AAAI Conference on Artificial Intelligence, 2024
\\
  Due to the significance of its various applications, source localization has
garnered considerable attention as one of the most important means to confront
diffusion hazards. Multi-source localization from a single-snapshot observation
is especially relevant due to its prevalence. However, the inherent
complexities of this problem, such as limited information, interactions among
sources, and dependence on diffusion models, pose challenges to resolution.
Current methods typically utilize heuristics and greedy selection, and they are
usually bonded with one diffusion model. Consequently, their effectiveness is
constrained. To address these limitations, we propose a simulation-based method
termed BOSouL. Bayesian optimization (BO) is adopted to approximate the results
for its sample efficiency. A surrogate function models uncertainty from the
limited information. It takes sets of nodes as the input instead of individual
nodes. BOSouL can incorporate any diffusion model in the data acquisition
process through simulations. Empirical studies demonstrate that its performance
is robust across graph structures and diffusion models. The code is available
at https://github.com/XGraph-Team/BOSouL.
\\ ( https://arxiv.org/abs/2403.16818 ,  222kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16825
Date: Mon, 25 Mar 2024 14:49:01 GMT   (49kb)

Title: Weak Convergence Analysis of Online Neural Actor-Critic Algorithms
Authors: Samuel Chun-Hei Lam, Justin Sirignano, Ziheng Wang
Categories: cs.LG math.OC math.PR stat.ML
\\
  We prove that a single-layer neural network trained with the online actor
critic algorithm converges in distribution to a random ordinary differential
equation (ODE) as the number of hidden units and the number of training steps
$\rightarrow \infty$. In the online actor-critic algorithm, the distribution of
the data samples dynamically changes as the model is updated, which is a key
challenge for any convergence analysis. We establish the geometric ergodicity
of the data samples under a fixed actor policy. Then, using a Poisson equation,
we prove that the fluctuations of the model updates around the limit
distribution due to the randomly-arriving data samples vanish as the number of
parameter updates $\rightarrow \infty$. Using the Poisson equation and weak
convergence techniques, we prove that the actor neural network and critic
neural network converge to the solutions of a system of ODEs with random
initial conditions. Analysis of the limit ODE shows that the limit critic
network will converge to the true value function, which will provide the actor
an asymptotically unbiased estimate of the policy gradient. We then prove that
the limit actor network will converge to a stationary point.
\\ ( https://arxiv.org/abs/2403.16825 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16829
Date: Mon, 25 Mar 2024 14:54:42 GMT   (25kb)

Title: Convergence of a model-free entropy-regularized inverse reinforcement
  learning algorithm
Authors: Titouan Renard, Andreas Schlaginhaufen, Tingting Ni, Maryam Kamgarpour
Categories: cs.LG cs.AI
\\
  Given a dataset of expert demonstrations, inverse reinforcement learning
(IRL) aims to recover a reward for which the expert is optimal. This work
proposes a model-free algorithm to solve entropy-regularized IRL problem. In
particular, we employ a stochastic gradient descent update for the reward and a
stochastic soft policy iteration update for the policy. Assuming access to a
generative model, we prove that our algorithm is guaranteed to recover a reward
for which the expert is $\varepsilon$-optimal using
$\mathcal{O}(1/\varepsilon^{2})$ samples of the Markov decision process (MDP).
Furthermore, with $\mathcal{O}(1/\varepsilon^{4})$ samples we prove that the
optimal policy corresponding to the recovered reward is $\varepsilon$-close to
the expert policy in total variation distance.
\\ ( https://arxiv.org/abs/2403.16829 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16843
Date: Mon, 25 Mar 2024 15:04:11 GMT   (8530kb,D)

Title: Do LLM Agents Have Regret? A Case Study in Online Learning and Games
Authors: Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang
Categories: cs.LG cs.AI cs.GT
\\
  Large language models (LLMs) have been increasingly employed for
(interactive) decision-making, via the development of LLM-based autonomous
agents. Despite their emerging successes, the performance of LLM agents in
decision-making has not been fully investigated through quantitative metrics,
especially in the multi-agent setting when they interact with each other, a
typical scenario in real-world LLM-agent applications. To better understand the
limits of LLM agents in these interactive environments, we propose to study
their interactions in benchmark decision-making settings in online learning and
game theory, through the performance metric of \emph{regret}. We first
empirically study the {no-regret} behaviors of LLMs in canonical
(non-stationary) online learning problems, as well as the emergence of
equilibria when LLM agents interact through playing repeated games. We then
provide some theoretical insights into the no-regret behaviors of LLM agents,
under certain assumptions on the supervised pre-training and the rationality
model of human decision-makers who generate the data. Notably, we also identify
(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To
promote the no-regret behaviors, we propose a novel \emph{unsupervised}
training loss of \emph{regret-loss}, which, in contrast to the supervised
pre-training loss, does not require the labels of (optimal) actions. We then
establish the statistical guarantee of generalization bound for regret-loss
minimization, followed by the optimization guarantee that minimizing such a
loss may automatically lead to known no-regret learning algorithms. Our further
experiments demonstrate the effectiveness of our regret-loss, especially in
addressing the above ``regrettable'' cases.
\\ ( https://arxiv.org/abs/2403.16843 ,  8530kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16846
Date: Mon, 25 Mar 2024 15:07:50 GMT   (1237kb,D)

Title: GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs
Authors: Zhan Qu, Daniel Gomm, Michael F\"arber
Categories: cs.LG cs.AI
\\
  Temporal Graph Neural Networks (TGNNs), crucial for modeling dynamic graphs
with time-varying interactions, face a significant challenge in explainability
due to their complex model structure. Counterfactual explanations, crucial for
understanding model decisions, examine how input graph changes affect outcomes.
This paper introduces two novel counterfactual explanation methods for TGNNs:
GreeDy (Greedy Explainer for Dynamic Graphs) and CoDy (Counterfactual Explainer
for Dynamic Graphs). They treat explanations as a search problem, seeking input
graph alterations that alter model predictions. GreeDy uses a simple, greedy
approach, while CoDy employs a sophisticated Monte Carlo Tree Search algorithm.
Experiments show both methods effectively generate clear explanations. Notably,
CoDy outperforms GreeDy and existing factual methods, with up to 59\% higher
success rate in finding significant counterfactual inputs. This highlights
CoDy's potential in clarifying TGNN decision-making, increasing their
transparency and trustworthiness in practice.
\\ ( https://arxiv.org/abs/2403.16846 ,  1237kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16883
Date: Mon, 25 Mar 2024 15:53:32 GMT   (2426kb,D)

Title: Discrete Latent Graph Generative Modeling with Diffusion Bridges
Authors: Van Khoa Nguyen, Yoann Boget, Frantzeska Lavda, Alexandros Kalousis
Categories: cs.LG stat.ML
\\
  Learning graph generative models over latent spaces has received less
attention compared to models that operate on the original data space and has so
far demonstrated lacklustre performance. We present GLAD a latent space graph
generative model. Unlike most previous latent space graph generative models,
GLAD operates on a discrete latent space that preserves to a significant extent
the discrete nature of the graph structures making no unnatural assumptions
such as latent space continuity. We learn the prior of our discrete latent
space by adapting diffusion bridges to its structure. By operating over an
appropriately constructed latent space we avoid relying on decompositions that
are often used in models that operate in the original data space. We present
experiments on a series of graph benchmark datasets which clearly show the
superiority of the discrete latent space and obtain state of the art graph
generative performance, making GLAD the first latent space graph generative
model with competitive performance. Our source code is published at:
\url{https://github.com/v18nguye/GLAD}.
\\ ( https://arxiv.org/abs/2403.16883 ,  2426kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16916
Date: Mon, 25 Mar 2024 16:36:13 GMT   (1009kb,D)

Title: SCOD: From Heuristics to Theory
Authors: Vojtech Franc and Jakub Paplham and Daniel Prusa
Categories: cs.LG
\\
  This paper addresses the problem of designing reliable prediction models that
abstain from predictions when faced with uncertain or out-of-distribution
samples - a recently proposed problem known as Selective Classification in the
presence of Out-of-Distribution data (SCOD). We make three key contributions to
SCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes
classifier for in-distribution (ID) data and a selector represented as a
stochastic linear classifier in a 2D space, using i) the conditional risk of
the ID classifier, and ii) the likelihood ratio of ID and out-of-distribution
(OOD) data as input. This contrasts with suboptimal strategies from current OOD
detection methods and the Softmax Information Retaining Combination (SIRC),
specifically developed for SCOD. Secondly, we establish that in a
distribution-free setting, the SCOD problem is not Probably Approximately
Correct learnable when relying solely on an ID data sample. Third, we introduce
POSCOD, a simple method for learning a plugin estimate of the optimal SCOD
strategy from both an ID data sample and an unlabeled mixture of ID and OOD
data. Our empirical results confirm the theoretical findings and demonstrate
that our proposed method, POSCOD, out performs existing OOD methods in
effectively addressing the SCOD problem.
\\ ( https://arxiv.org/abs/2403.16916 ,  1009kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16930
Date: Mon, 25 Mar 2024 16:49:38 GMT   (551kb,D)

Title: FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN
Authors: Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic
Categories: cs.LG
\\
  Federated Learning (FL) provides a privacy-preserving mechanism for
distributed training of machine learning models on networked devices (e.g.,
mobile devices, IoT edge nodes). It enables Artificial Intelligence (AI) at the
edge by creating models without sharing the actual data across the network.
Existing research works typically focus on generic aspects of non-IID data and
heterogeneity in client's system characteristics, but they often neglect the
issue of insufficient data for model development, which can arise from uneven
class label distribution and highly variable data volumes across edge nodes. In
this work, we propose FLIGAN, a novel approach to address the issue of data
incompleteness in FL. First, we leverage Generative Adversarial Networks (GANs)
to adeptly capture complex data distributions and generate synthetic data that
closely resemble the real-world data. Then, we use synthetic data to enhance
the robustness and completeness of datasets across nodes. Our methodology
adheres to FL's privacy requirements by generating synthetic data in a
federated manner without sharing the actual data in the process. We incorporate
techniques such as classwise sampling and node grouping, designed to improve
the federated GAN's performance, enabling the creation of high-quality
synthetic datasets and facilitating efficient FL training. Empirical results
from our experiments demonstrate that FLIGAN significantly improves the model
accuracy, especially in scenarios with high class imbalances, achieving up to a
20% increase in model accuracy over traditional FL baselines.
\\ ( https://arxiv.org/abs/2403.16930 ,  551kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.14763 (*cross-listing*)
Date: Thu, 21 Mar 2024 18:07:32 GMT   (507kb,D)

Title: Gravitational Duals from Equations of State
Authors: Yago Bea, Raul Jimenez, David Mateos, Shuheng Liu, Pavlos Protopapas,
  Pedro Taranc\'on-\'Alvarez, Pablo Tejerina-P\'erez
Categories: hep-th astro-ph.CO cs.AI cs.LG gr-qc
\\
  Holography relates gravitational theories in five dimensions to
four-dimensional quantum field theories in flat space. Under this map, the
equation of state of the field theory is encoded in the black hole solutions of
the gravitational theory. Solving the five-dimensional Einstein's equations to
determine the equation of state is an algorithmic, direct problem. Determining
the gravitational theory that gives rise to a prescribed equation of state is a
much more challenging, inverse problem. We present a novel approach to solve
this problem based on physics-informed neural networks. The resulting algorithm
is not only data-driven but also informed by the physics of the Einstein's
equations. We successfully apply it to theories with crossovers, first- and
second-order phase transitions.
\\ ( https://arxiv.org/abs/2403.14763 ,  507kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15396 (*cross-listing*)
Date: Fri, 16 Feb 2024 08:10:41 GMT   (487kb)

Title: I would love this to be like an assistant, not the teacher: a voice of
  the customer perspective of what distance learning students want from an
  Artificial Intelligence Digital Assistant
Authors: Bart Rienties, John Domingue, Subby Duttaroy, Christothea Herodotou,
  Felipe Tessarolo, Denise Whitelock
Categories: cs.CY cs.AI
Comments: 23 pages, 1 figure, submitted to Distance Education
\\
  With the release of Generative AI systems such as ChatGPT, an increasing
interest in using Artificial Intelligence (AI) has been observed across
domains, including higher education. While emerging statistics show the
popularity of using AI amongst undergraduate students, little is yet known
about students' perceptions regarding AI including self-reported benefits and
concerns from their actual usage, in particular in distance learning contexts.
Using a two-step, mixed-methods approach, we examined the perceptions of ten
online and distance learning students from diverse disciplines regarding the
design of a hypothetical AI Digital Assistant (AIDA). In the first step, we
captured students' perceptions via interviews, while the second step supported
the triangulation of data by enabling students to share, compare, and contrast
perceptions with those of peers. All participants agreed on the usefulness of
such an AI tool while studying and reported benefits from using it for
real-time assistance and query resolution, support for academic tasks,
personalisation and accessibility, together with emotional and social support.
Students' concerns related to the ethical and social implications of
implementing AIDA, data privacy and data use, operational challenges, academic
integrity and misuse, and the future of education. Implications for the design
of AI-tailored systems are also discussed.
\\ ( https://arxiv.org/abs/2403.15396 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15397 (*cross-listing*)
Date: Fri, 16 Feb 2024 21:49:17 GMT   (351kb)

Title: Regulating Large Language Models: A Roundtable Report
Authors: Gabriel Nicholas and Paul Friedl
Categories: cs.CY cs.AI cs.CL
Comments: 24 pages
\\
  On July 20, 2023, a group of 27 scholars and digital rights advocates with
expertise in law, computer science, political science, and other disciplines
gathered for the Large Language Models, Law and Policy Roundtable, co-hosted by
the NYU School of Law's Information Law Institute and the Center for Democracy
& Technology. The roundtable convened to discuss how law and policy can help
address some of the larger societal problems posed by large language models
(LLMs). The discussion focused on three policy topic areas in particular:
  1. Truthfulness: What risks do LLMs pose in terms of generating mis- and
disinformation? How can these risks be mitigated from a technical and/or
regulatory perspective?
  2. Privacy: What are the biggest privacy risks involved in the creation,
deployment, and use of LLMs? How can these risks be mitigated from a technical
and/or regulatory perspective?
  3. Market concentration: What threats do LLMs pose concerning market/power
concentration? How can these risks be mitigated from a technical and/or
regulatory perspective?
  In this paper, we provide a detailed summary of the day's proceedings. We
first recap what we deem to be the most important contributions made during the
issue framing discussions. We then provide a list of potential legal and
regulatory interventions generated during the brainstorming discussions.
\\ ( https://arxiv.org/abs/2403.15397 ,  351kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15401 (*cross-listing*)
Date: Mon, 19 Feb 2024 17:58:41 GMT   (855kb)

Title: Large Language Model for Mental Health: A Systematic Review
Authors: Zhijun Guo, Alvina Lai, Johan Hilge Thygesen, Joseph Farrington,
  Thomas Keen and Kezhi Li
Categories: cs.CY cs.AI cs.CL
\\
  Large language models (LLMs) have received much attention and shown their
potential in digital health, while their application in mental health is
subject to ongoing debate. This systematic review aims to summarize and
characterize the use of LLMs in mental health by investigating the strengths
and limitations of the latest work in LLMs and discusses the challenges and
opportunities for early screening, digital interventions, and other clinical
applications in mental health. Following PRISMA guidelines, we examined English
articles from PubMed, DBLP Computer Science Bibliography, and IEEE Xplore,
published between 1 January 2017, and 1 September 2023, focusing on mental
health and LLMs. The review analyzed 32 articles, including mental health
analysis using social media datasets (n=13), mental health chatbots (n=10), and
other mental health applications (n=9). Findings reveal LLMs' effectiveness in
mental health issue detection and the enhancement of telepsychological services
through personalised healthcare. Nonetheless, risks like text inconsistencies,
hallucinatory content, and the lack of an ethical framework raise concerns
about their clinical use. Despite these challenges, the advancement of LLMs
underscores their potential as innovative clinical tools, necessitating further
research and development. The review emphasizes that LLMs should complement,
not replace, professional mental health services.
\\ ( https://arxiv.org/abs/2403.15401 ,  855kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15403 (*cross-listing*)
Date: Mon, 19 Feb 2024 22:43:19 GMT   (9527kb)

Title: AI Ethics and Governance in Practice: An Introduction
Authors: David Leslie, Cami Rincon, Morgan Briggs, Antonella Perini, Smera
  Jayadeva, Ann Borda, SJ Bennett, Christopher Burr, Mhairi Aitken, Michael
  Katell, Claudia Fischer
Categories: cs.CY cs.AI cs.HC
DOI: 10.5281/zenodo.10679891
\\
  AI systems may have transformative and long-term effects on individuals and
society. To manage these impacts responsibly and direct the development of AI
systems toward optimal public benefit, considerations of AI ethics and
governance must be a first priority.
  In this workbook, we introduce and describe our PBG Framework, a multi-tiered
governance model that enables project teams to integrate ethical values and
practical principles into their innovation practices and to have clear
mechanisms for demonstrating and documenting this.
\\ ( https://arxiv.org/abs/2403.15403 ,  9527kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15404 (*cross-listing*)
Date: Mon, 19 Feb 2024 22:58:05 GMT   (7467kb)

Title: AI Sustainability in Practice Part Two: Sustainability Throughout the AI
  Workflow
Authors: David Leslie, Cami Rincon, Morgan Briggs, Antonella Perini, Smera
  Jayadeva, Ann Borda, SJ Bennett, Christopher Burr, Mhairi Aitken, Michael
  Katell, Claudia Fischer, Janis Wong, Ismael Kherroubi Garcia
Categories: cs.CY cs.AI cs.HC
DOI: 10.5281/zenodo.10680345
\\
  The sustainability of AI systems depends on the capacity of project teams to
proceed with a continuous sensitivity to their potential real-world impacts and
transformative effects. Stakeholder Impact Assessments (SIAs) are governance
mechanisms that enable this kind of responsiveness. They are tools that create
a procedure for, and a means of documenting, the collaborative evaluation and
reflective anticipation of the possible harms and benefits of AI innovation
projects. SIAs are not one-off governance actions. They require project teams
to pay continuous attention to the dynamic and changing character of AI
production and use and to the shifting conditions of the real-world
environments in which AI technologies are embedded. This workbook is part two
of two workbooks on AI Sustainability. It provides a template of the SIA and
activities that allow a deeper dive into crucial parts of it. It discusses
methods for weighing values and considering trade-offs during the SIA. And, it
highlights the need to treat the SIA as an end-to-end process of responsive
evaluation and re-assessment.
\\ ( https://arxiv.org/abs/2403.15404 ,  7467kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15408 (*cross-listing*)
Date: Fri, 1 Mar 2024 01:16:27 GMT   (2276kb,D)

Title: Multi-modal Heart Failure Risk Estimation based on Short ECG and Sampled
  Long-Term HRV
Authors: Sergio Gonz\'alez, Abel Ko-Chun Yi, Wan-Ting Hsieh, Wei-Chao Chen,
  Chun-Li Wang, Victor Chien-Chia Wu, Shang-Hung Chang
Categories: eess.SP cs.AI cs.LG
Journal-ref: S. Gonz\'alez, A. K.-C. Yi, W.-T. Hsieh, W.-C. Chen, C.-L. Wang,
  V. C.-C. Wu, S.-H. Chang, Multi-modal heart failure risk estimation based on
  short ECG and sampled long-term HRV, Information Fusion 107 (2024) 102337
DOI: 10.1016/j.inffus.2024.102337
\\
  Cardiovascular diseases, including Heart Failure (HF), remain a leading
global cause of mortality, often evading early detection. In this context,
accessible and effective risk assessment is indispensable. Traditional
approaches rely on resource-intensive diagnostic tests, typically administered
after the onset of symptoms. The widespread availability of electrocardiogram
(ECG) technology and the power of Machine Learning are emerging as viable
alternatives within smart healthcare. In this paper, we propose several
multi-modal approaches that combine 30-second ECG recordings and approximate
long-term Heart Rate Variability (HRV) data to estimate the risk of HF
hospitalization. We introduce two survival models: an XGBoost model with
Accelerated Failure Time (AFT) incorporating comprehensive ECG features and a
ResNet model that learns from the raw ECG. We extend these with our novel
long-term HRVs extracted from the combination of ultra-short-term beat-to-beat
measurements taken over the day. To capture their temporal dynamics, we propose
a survival model comprising ResNet and Transformer architectures (TFM-ResNet).
Our experiments demonstrate high model performance for HF risk assessment with
a concordance index of 0.8537 compared to 14 survival models and competitive
discrimination power on various external ECG datasets. After transferability
tests with Apple Watch data, our approach implemented in the myHeartScore App
offers cost-effective and highly accessible HF risk assessment, contributing to
its prevention and management.
\\ ( https://arxiv.org/abs/2403.15408 ,  2276kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15412 (*cross-listing*)
Date: Tue, 5 Mar 2024 08:29:36 GMT   (1275kb,D)

Title: Towards Measuring and Modeling "Culture" in LLMs: A Survey
Authors: Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania,
  Siddhant Singh, Ashutosh Dwivedi, Alham Fikri Aji, Jacki O'Neill, Ashutosh
  Modi, Monojit Choudhury
Categories: cs.CY cs.AI cs.CL
\\
  We present a survey of 39 recent papers that aim to study cultural
representation and inclusion in large language models. We observe that none of
the studies define "culture," which is a complex, multifaceted concept;
instead, they probe the models on some specially designed datasets which
represent certain aspects of "culture." We call these aspects the proxies of
cultures, and organize them across three dimensions of demographic, semantic
and linguistic-cultural interaction proxies. We also categorize the probing
methods employed. Our analysis indicates that only certain aspects of
"culture," such as values and objectives, have been studied, leaving several
other interesting and important facets, especially the multitude of semantic
domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022),
unexplored. Two other crucial gaps are the lack of robustness and situatedness
of the current methods. Based on these observations, we provide several
recommendations for a holistic and practically useful research agenda for
furthering cultural inclusion in LLMs and LLM-based applications.
\\ ( https://arxiv.org/abs/2403.15412 ,  1275kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15413 (*cross-listing*)
Date: Wed, 6 Mar 2024 12:38:18 GMT   (747kb,D)

Title: Playing With Neuroscience: Past, Present and Future of Neuroimaging and
  Games
Authors: Paolo Burelli and Laurits Dixen
Categories: q-bio.NC cs.AI
\\
  Videogames have been a catalyst for advances in many research fields, such as
artificial intelligence, human-computer interaction or virtual reality. Over
the years, research in fields such as artificial intelligence has enabled the
design of new types of games, while games have often served as a powerful tool
for testing and simulation. Can this also happen with neuroscience? What is the
current relationship between neuroscience and games research? what can we
expect from the future? In this article, we'll try to answer these questions,
analysing the current state-of-the-art at the crossroads between neuroscience
and games and envisioning future directions.
\\ ( https://arxiv.org/abs/2403.15413 ,  747kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15422 (*cross-listing*)
Date: Tue, 12 Mar 2024 22:22:14 GMT   (167kb,D)

Title: Machine Learning Techniques for Sensor-based Human Activity Recognition
  with Data Heterogeneity -- A Review
Authors: Xiaozhou Ye, Kouichi Sakurai, Nirmal Nair, Kevin I-Kai Wang
Categories: eess.SP cs.AI cs.HC cs.LG
\\
  Sensor-based Human Activity Recognition (HAR) is crucial in ubiquitous
computing, analysing behaviours through multi-dimensional observations. Despite
research progress, HAR confronts challenges, particularly in data distribution
assumptions. Most studies often assume uniform data distributions across
datasets, contrasting with the varied nature of practical sensor data in human
activities. Addressing data heterogeneity issues can improve performance,
reduce computational costs, and aid in developing personalized, adaptive models
with less annotated data. This review investigates how machine learning
addresses data heterogeneity in HAR, by categorizing data heterogeneity types,
applying corresponding suitable machine learning methods, summarizing available
datasets, and discussing future challenges.
\\ ( https://arxiv.org/abs/2403.15422 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15423 (*cross-listing*)
Date: Tue, 12 Mar 2024 22:33:56 GMT   (1007kb,D)

Title: Cross-user activity recognition via temporal relation optimal transport
Authors: Xiaozhou Ye, Kevin I-Kai Wang
Categories: eess.SP cs.AI cs.CV cs.HC cs.LG
\\
  Current research on human activity recognition (HAR) mainly assumes that
training and testing data are drawn from the same distribution to achieve a
generalised model, which means all the data are considered to be independent
and identically distributed $\displaystyle (i.i.d.) $. In many real-world
applications, this assumption does not hold, and collected training and target
testing datasets have non-uniform distribution, such as in the case of
cross-user HAR. Domain adaptation is a promising approach for cross-user HAR
tasks. Existing domain adaptation works based on the assumption that samples in
each domain are $\displaystyle i.i.d. $ and do not consider the knowledge of
temporal relation hidden in time series data for aligning data distribution.
This strong assumption of $\displaystyle i.i.d. $ may not be suitable for time
series-related domain adaptation methods because the samples formed by time
series segmentation and feature extraction techniques are only coarse
approximations to $\displaystyle i.i.d. $ assumption in each domain. In this
paper, we propose the temporal relation optimal transport (TROT) method to
utilise temporal relation and relax the $\displaystyle i.i.d. $ assumption for
the samples in each domain for accurate and efficient knowledge transfer. We
obtain the temporal relation representation and implement temporal relation
alignment of activities via the Hidden Markov model (HMM) and optimal transport
(OT) techniques. Besides, a new regularisation term that preserves temporal
relation order information for an improved optimal transport mapping is
proposed to enhance the domain adaptation performance. Comprehensive
experiments are conducted on three public activity recognition datasets (i.e.
OPPT, PAMAP2 and DSADS), demonstrating that TROT outperforms other
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.15423 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15424 (*cross-listing*)
Date: Tue, 12 Mar 2024 22:38:09 GMT   (1839kb,D)

Title: Cross-user activity recognition using deep domain adaptation with
  temporal relation information
Authors: Xiaozhou Ye, Waleed H. Abdulla, Nirmal Nair, Kevin I-Kai Wang
Categories: eess.SP cs.AI cs.CV cs.HC cs.LG
\\
  Human Activity Recognition (HAR) is a cornerstone of ubiquitous computing,
with promising applications in diverse fields such as health monitoring and
ambient assisted living. Despite significant advancements, sensor-based HAR
methods often operate under the assumption that training and testing data have
identical distributions. However, in many real-world scenarios, particularly in
sensor-based HAR, this assumption is invalidated by out-of-distribution
($\displaystyle o.o.d.$) challenges, including differences from heterogeneous
sensors, change over time, and individual behavioural variability. This paper
centres on the latter, exploring the cross-user HAR problem where behavioural
variability across individuals results in differing data distributions. To
address this challenge, we introduce the Deep Temporal State Domain Adaptation
(DTSDA) model, an innovative approach tailored for time series domain
adaptation in cross-user HAR. Contrary to the common assumption of sample
independence in existing domain adaptation approaches, DTSDA recognizes and
harnesses the inherent temporal relations in the data. Therefore, we introduce
'Temporal State', a concept that defined the different sub-activities within an
activity, consistent across different users. We ensure these sub-activities
follow a logical time sequence through 'Temporal Consistency' property and
propose the 'Pseudo Temporal State Labeling' method to identify the
user-invariant temporal relations. Moreover, the design principle of DTSDA
integrates adversarial learning for better domain adaptation. Comprehensive
evaluations on three HAR datasets demonstrate DTSDA's superior performance in
cross-user HAR applications by briding individual behavioral variability using
temporal relations across sub-activities.
\\ ( https://arxiv.org/abs/2403.15424 ,  1839kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15432 (*cross-listing*)
Date: Thu, 14 Mar 2024 15:43:48 GMT   (24346kb,D)

Title: BRIEDGE: EEG-Adaptive Edge AI for Multi-Brain to Multi-Robot Interaction
Authors: Jinhui Ouyang and Mingzhu Wu and Xinglin Li and Hanhui Deng and Di Wu
Categories: eess.SP cs.AI cs.HC cs.LG
\\
  Recent advances in EEG-based BCI technologies have revealed the potential of
brain-to-robot collaboration through the integration of sensing, computing,
communication, and control. In this paper, we present BRIEDGE as an end-to-end
system for multi-brain to multi-robot interaction through an EEG-adaptive
neural network and an encoding-decoding communication framework, as illustrated
in Fig.1. As depicted, the edge mobile server or edge portable server will
collect EEG data from the users and utilize the EEG-adaptive neural network to
identify the users' intentions. The encoding-decoding communication framework
then encodes the EEG-based semantic information and decodes it into commands in
the process of data transmission. To better extract the joint features of
heterogeneous EEG data as well as enhance classification accuracy, BRIEDGE
introduces an informer-based ProbSparse self-attention mechanism. Meanwhile,
parallel and secure transmissions for multi-user multi-task scenarios under
physical channels are addressed by dynamic autoencoder and autodecoder
communications. From mobile computing and edge AI perspectives, model
compression schemes composed of pruning, weight sharing, and quantization are
also used to deploy lightweight EEG-adaptive models running on both transmitter
and receiver sides. Based on the effectiveness of these components, a code map
representing various commands enables multiple users to control multiple
intelligent agents concurrently. Our experiments in comparison with
state-of-the-art works show that BRIEDGE achieves the best classification
accuracy of heterogeneous EEG data, and more stable performance under noisy
environments.
\\ ( https://arxiv.org/abs/2403.15432 ,  24346kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15433 (*cross-listing*)
Date: Fri, 15 Mar 2024 02:30:00 GMT   (4797kb,D)

Title: HyPer-EP: Meta-Learning Hybrid Personalized Models for Cardiac
  Electrophysiology
Authors: Xiajun Jiang, Sumeet Vadhavkar, Yubo Ye, Maryam Toloubidokhti, Ryan
  Missel, Linwei Wang
Categories: eess.SP cs.AI cs.LG eess.IV
\\
  Personalized virtual heart models have demonstrated increasing potential for
clinical use, although the estimation of their parameters given
patient-specific data remain a challenge. Traditional physics-based modeling
approaches are computationally costly and often neglect the inherent structural
errors in these models due to model simplifications and assumptions. Modern
deep learning approaches, on the other hand, rely heavily on data supervision
and lacks interpretability. In this paper, we present a novel hybrid modeling
framework to describe a personalized cardiac digital twin as a combination of a
physics-based known expression augmented by neural network modeling of its
unknown gap to reality. We then present a novel meta-learning framework to
enable the separate identification of both the physics-based and neural
components in the hybrid model. We demonstrate the feasibility and generality
of this hybrid modeling framework with two examples of instantiations and their
proof-of-concept in synthetic experiments.
\\ ( https://arxiv.org/abs/2403.15433 ,  4797kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15441 (*cross-listing*)
Date: Sun, 17 Mar 2024 08:40:06 GMT   (4480kb,D)

Title: Unified Generative Modeling of 3D Molecules via Bayesian Flow Networks
Authors: Yuxuan Song, Jingjing Gong, Yanru Qu, Hao Zhou, Mingyue Zheng,
  Jingjing Liu, Wei-Ying Ma
Categories: physics.chem-ph cs.AI cs.LG
Comments: ICLR 2024
\\
  Advanced generative model (e.g., diffusion model) derived from simplified
continuity assumptions of data distribution, though showing promising progress,
has been difficult to apply directly to geometry generation applications due to
the multi-modality and noise-sensitive nature of molecule geometry. This work
introduces Geometric Bayesian Flow Networks (GeoBFN), which naturally fits
molecule geometry by modeling diverse modalities in the differentiable
parameter space of distributions. GeoBFN maintains the SE-(3) invariant density
modeling property by incorporating equivariant inter-dependency modeling on
parameters of distributions and unifying the probabilistic modeling of
different modalities. Through optimized training and sampling techniques, we
demonstrate that GeoBFN achieves state-of-the-art performance on multiple 3D
molecule generation benchmarks in terms of generation quality (90.87% molecule
stability in QM9 and 85.6% atom stability in GEOM-DRUG. GeoBFN can also conduct
sampling with any number of steps to reach an optimal trade-off between
efficiency and quality (e.g., 20-times speedup without sacrificing
performance).
\\ ( https://arxiv.org/abs/2403.15441 ,  4480kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15442 (*cross-listing*)
Date: Sun, 17 Mar 2024 11:28:23 GMT   (5805kb,D)

Title: Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review
  of Healthcare Strategies, Challenges, and Perspectives
Authors: Billel Essaid, Hamza Kheddar, Noureddine Batel, Abderrahmane Lakas,
  Muhammad E.H.Chowdhury
Categories: eess.AS cs.AI cs.CV eess.IV
\\
  Automatic speech recognition (ASR) plays a pivotal role in our daily lives,
offering utility not only for interacting with machines but also for
facilitating communication for individuals with either partial or profound
hearing impairments. The process involves receiving the speech signal in
analogue form, followed by various signal processing algorithms to make it
compatible with devices of limited capacity, such as cochlear implants (CIs).
Unfortunately, these implants, equipped with a finite number of electrodes,
often result in speech distortion during synthesis. Despite efforts by
researchers to enhance received speech quality using various state-of-the-art
signal processing techniques, challenges persist, especially in scenarios
involving multiple sources of speech, environmental noise, and other
circumstances. The advent of new artificial intelligence (AI) methods has
ushered in cutting-edge strategies to address the limitations and difficulties
associated with traditional signal processing techniques dedicated to CIs. This
review aims to comprehensively review advancements in CI-based ASR and speech
enhancement, among other related aspects. The primary objective is to provide a
thorough overview of metrics and datasets, exploring the capabilities of AI
algorithms in this biomedical field, summarizing and commenting on the best
results obtained. Additionally, the review will delve into potential
applications and suggest future directions to bridge existing research gaps in
this domain.
\\ ( https://arxiv.org/abs/2403.15442 ,  5805kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15443 (*cross-listing*)
Date: Sun, 17 Mar 2024 16:12:50 GMT   (773kb)

Title: Introducing an ensemble method for the early detection of Alzheimer's
  disease through the analysis of PET scan images
Authors: Arezoo Borji, Taha-Hossein Hejazi, Abbas Seifi
Categories: eess.SP cs.AI cs.CV cs.LG eess.IV
\\
  Alzheimer's disease is a progressive neurodegenerative disorder that
primarily affects cognitive functions such as memory, thinking, and behavior.
In this disease, there is a critical phase, mild cognitive impairment, that is
really important to be diagnosed early since some patients with progressive MCI
will develop the disease. This study delves into the challenging task of
classifying Alzheimer's disease into four distinct groups: control normal (CN),
progressive mild cognitive impairment (pMCI), stable mild cognitive impairment
(sMCI), and Alzheimer's disease (AD). This classification is based on a
thorough examination of PET scan images obtained from the ADNI dataset, which
provides a thorough understanding of the disease's progression. Several
deep-learning and traditional machine-learning models have been used to detect
Alzheimer's disease. In this paper, three deep-learning models, namely VGG16
and AlexNet, and a custom Convolutional neural network (CNN) with 8-fold
cross-validation have been used for classification. Finally, an ensemble
technique is used to improve the overall result of these models. The results
show that using deep-learning models to tell the difference between MCI
patients gives an overall average accuracy of 93.13% and an AUC of 94.4%.
\\ ( https://arxiv.org/abs/2403.15443 ,  773kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15444 (*cross-listing*)
Date: Sun, 17 Mar 2024 22:31:14 GMT   (7298kb,D)

Title: A Survey of IMU Based Cross-Modal Transfer Learning in Human Activity
  Recognition
Authors: Abhi Kamboj, Minh Do
Categories: eess.SP cs.AI cs.CV cs.LG eess.IV
\\
  Despite living in a multi-sensory world, most AI models are limited to
textual and visual understanding of human motion and behavior. In fact, full
situational awareness of human motion could best be understood through a
combination of sensors. In this survey we investigate how knowledge can be
transferred and utilized amongst modalities for Human Activity/Action
Recognition (HAR), i.e. cross-modality transfer learning. We motivate the
importance and potential of IMU data and its applicability in cross-modality
learning as well as the importance of studying the HAR problem. We categorize
HAR related tasks by time and abstractness and then compare various types of
multimodal HAR datasets. We also distinguish and expound on many related but
inconsistently used terms in the literature, such as transfer learning, domain
adaptation, representation learning, sensor fusion, and multimodal learning,
and describe how cross-modal learning fits with all these concepts. We then
review the literature in IMU-based cross-modal transfer for HAR. The two main
approaches for cross-modal transfer are instance-based transfer, where
instances of one modality are mapped to another (e.g. knowledge is transferred
in the input space), or feature-based transfer, where the model relates the
modalities in an intermediate latent space (e.g. knowledge is transferred in
the feature space). Finally, we discuss future research directions and
applications in cross-modal HAR.
\\ ( https://arxiv.org/abs/2403.15444 ,  7298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15457 (*cross-listing*)
Date: Tue, 19 Mar 2024 08:27:04 GMT   (35097kb,D)

Title: The Journey to Trustworthy AI- Part 1: Pursuit of Pragmatic Frameworks
Authors: Mohamad M Nasr-Azadani and Jean-Luc Chatelain
Categories: cs.CY cs.AI cs.HC
\\
  This paper reviews Trustworthy Artificial Intelligence (TAI) and its various
definitions. Considering the principles respected in any society, TAI is often
characterized by a few attributes, some of which have led to confusion in
regulatory or engineering contexts. We argue against using terms such as
Responsible or Ethical AI as substitutes for TAI. And to help clarify any
confusion, we suggest leaving them behind. Given the subjectivity and
complexity inherent in TAI, developing a universal framework is deemed
infeasible. Instead, we advocate for approaches centered on addressing key
attributes and properties such as fairness, bias, risk, security,
explainability, and reliability. We examine the ongoing regulatory landscape,
with a focus on initiatives in the EU, China, and the USA. We recognize that
differences in AI regulations based on geopolitical and geographical reasons
pose an additional challenge for multinational companies. We identify risk as a
core factor in AI regulation and TAI. For example, as outlined in the EU-AI
Act, organizations must gauge the risk level of their AI products to act
accordingly (or risk hefty fines). We compare modalities of TAI implementation
and how multiple cross-functional teams are engaged in the overall process.
Thus, a brute force approach for enacting TAI renders its efficiency and
agility, moot. To address this, we introduce our framework
Set-Formalize-Measure-Act (SFMA). Our solution highlights the importance of
transforming TAI-aware metrics, drivers of TAI, stakeholders, and
business/legal requirements into actual benchmarks or tests. Finally,
over-regulation driven by panic of powerful AI models can, in fact, harm TAI
too. Based on GitHub user-activity data, in 2023, AI open-source projects rose
to top projects by contributor account. Enabling innovation in TAI hinges on
the independent contributions of the open-source community.
\\ ( https://arxiv.org/abs/2403.15457 ,  35097kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15472 (*cross-listing*)
Date: Wed, 20 Mar 2024 15:47:28 GMT   (1103kb,D)

Title: Enhancing Programming Education with ChatGPT: A Case Study on Student
  Perceptions and Interactions in a Python Course
Authors: Boxaun Ma, Li Chen and Shin'ichi Konomi
Categories: cs.CY cs.AI cs.PL
\\
  The integration of ChatGPT as a supportive tool in education, notably in
programming courses, addresses the unique challenges of programming education
by providing assistance with debugging, code generation, and explanations.
Despite existing research validating ChatGPT's effectiveness, its application
in university-level programming education and a detailed understanding of
student interactions and perspectives remain limited. This paper explores
ChatGPT's impact on learning in a Python programming course tailored for
first-year students over eight weeks. By analyzing responses from surveys,
open-ended questions, and student-ChatGPT dialog data, we aim to provide a
comprehensive view of ChatGPT's utility and identify both its advantages and
limitations as perceived by students. Our study uncovers a generally positive
reception toward ChatGPT and offers insights into its role in enhancing the
programming education experience. These findings contribute to the broader
discourse on AI's potential in education, suggesting paths for future research
and application.
\\ ( https://arxiv.org/abs/2403.15472 ,  1103kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15474 (*cross-listing*)
Date: Wed, 20 Mar 2024 16:25:49 GMT   (19232kb,D)

Title: EC-IoU: Orienting Safety for Object Detectors via Ego-Centric
  Intersection-over-Union
Authors: Brian Hsuan-Cheng Liao, Chih-Hong Cheng, Hasan Esen, Alois Knoll
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: 8 pages (IEEE double column format), 7 figures, 2 tables, submitted
  to IROS 2024
\\
  This paper presents safety-oriented object detection via a novel Ego-Centric
Intersection-over-Union (EC-IoU) measure, addressing practical concerns when
applying state-of-the-art learning-based perception models in safety-critical
domains such as autonomous driving. Concretely, we propose a weighting
mechanism to refine the widely used IoU measure, allowing it to assign a higher
score to a prediction that covers closer points of a ground-truth object from
the ego agent's perspective. The proposed EC-IoU measure can be used in typical
evaluation processes to select object detectors with higher safety-related
performance for downstream tasks. It can also be integrated into common loss
functions for model fine-tuning. While geared towards safety, our experiment
with the KITTI dataset demonstrates the performance of a model trained on
EC-IoU can be better than that of a variant trained on IoU in terms of mean
Average Precision as well.
\\ ( https://arxiv.org/abs/2403.15474 ,  19232kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15476 (*cross-listing*)
Date: Wed, 20 Mar 2024 17:29:58 GMT   (2628kb,D)

Title: Learning to Infer Generative Template Programs for Visual Concepts
Authors: R. Kenny Jones, Siddhartha Chaudhuri, Daniel Ritchie
Categories: cs.CV cs.AI cs.GR cs.LG
\\
  People grasp flexible visual concepts from a few examples. We explore a
neurosymbolic system that learns how to infer programs that capture visual
concepts in a domain-general fashion. We introduce Template Programs:
programmatic expressions from a domain-specific language that specify
structural and parametric patterns common to an input concept. Our framework
supports multiple concept-related tasks, including few-shot generation and
co-segmentation through parsing. We develop a learning paradigm that allows us
to train networks that infer Template Programs directly from visual datasets
that contain concept groupings. We run experiments across multiple visual
domains: 2D layouts, Omniglot characters, and 3D shapes. We find that our
method outperforms task-specific alternatives, and performs competitively
against domain-specific approaches for the limited domains where they exist.
\\ ( https://arxiv.org/abs/2403.15476 ,  2628kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15479 (*cross-listing*)
Date: Thu, 21 Mar 2024 02:12:03 GMT   (2503kb)

Title: Antisocial Analagous Behavior, Alignment and Human Impact of Google AI
  Systems: Evaluating through the lens of modified Antisocial Behavior Criteria
  by Human Interaction, Independent LLM Analysis, and AI Self-Reflection
Authors: Alan D. Ogilvie
Categories: cs.CY cs.AI
Comments: 48 pages including addendum of transcripts
\\
  Google AI systems exhibit patterns mirroring antisocial personality disorder
(ASPD), consistent across models from Bard on PaLM to Gemini Advanced, meeting
5 out of 7 ASPD modified criteria. These patterns, along with comparable
corporate behaviors, are scrutinized using an ASPD-inspired framework,
emphasizing the heuristic value in assessing AI's human impact. Independent
analyses by ChatGPT 4 and Claude 3.0 Opus of the Google interactions, alongside
AI self-reflection, validate these concerns, highlighting behaviours analogous
to deceit, manipulation, and safety neglect.
  The analogy of ASPD underscores the dilemma: just as we would hesitate to
entrust our homes or personal devices to someone with psychopathic traits, we
must critically evaluate the trustworthiness of AI systems and their
creators.This research advocates for an integrated AI ethics approach, blending
technological evaluation, human-AI interaction, and corporate behavior
scrutiny. AI self-analysis sheds light on internal biases, stressing the need
for multi-sectoral collaboration for robust ethical guidelines and oversight.
  Given the persistent unethical behaviors in Google AI, notably with potential
Gemini integration in iOS affecting billions, immediate ethical scrutiny is
imperative. The trust we place in AI systems, akin to the trust in individuals,
necessitates rigorous ethical evaluation. Would we knowingly trust our home,
our children or our personal computer to human with ASPD.?
  Urging Google and the AI community to address these ethical challenges
proactively, this paper calls for transparent dialogues and a commitment to
higher ethical standards, ensuring AI's societal benefit and moral integrity.
The urgency for ethical action is paramount, reflecting the vast influence and
potential of AI technologies in our lives.
\\ ( https://arxiv.org/abs/2403.15479 ,  2503kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15481 (*cross-listing*)
Date: Thu, 21 Mar 2024 03:44:59 GMT   (766kb,D)

Title: Navigating Fairness: Practitioners' Understanding, Challenges, and
  Strategies in AI/ML Development
Authors: Aastha Pant, Rashina Hoda, Chakkrit Tantithamthavorn, Burak Turhan
Categories: cs.CY cs.AI cs.SE
Comments: 31 pages, 8 figures, 2 tables
\\
  The rise in the use of AI/ML applications across industries has sparked more
discussions about the fairness of AI/ML in recent times. While prior research
on the fairness of AI/ML exists, there is a lack of empirical studies focused
on understanding the views and experiences of AI practitioners in developing a
fair AI/ML. Understanding AI practitioners' views and experiences on the
fairness of AI/ML is important because they are directly involved in its
development and deployment and their insights can offer valuable real-world
perspectives on the challenges associated with ensuring fairness in AI/ML. We
conducted semi-structured interviews with 22 AI practitioners to investigate
their understanding of what a 'fair AI/ML' is, the challenges they face in
developing a fair AI/ML, the consequences of developing an unfair AI/ML, and
the strategies they employ to ensure AI/ML fairness. We developed a framework
showcasing the relationship between AI practitioners' understanding of 'fair
AI/ML' and (i) their challenges in its development, (ii) the consequences of
developing an unfair AI/ML, and (iii) strategies used to ensure AI/ML fairness.
Additionally, we also identify areas for further investigation and offer
recommendations to aid AI practitioners and AI companies in navigating
fairness.
\\ ( https://arxiv.org/abs/2403.15481 ,  766kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15489 (*cross-listing*)
Date: Thu, 21 Mar 2024 13:38:59 GMT   (208kb)

Title: EEG decoding with conditional identification information
Authors: Pengfei Sun and Jorg De Winne and Paul Devos and Dick Botteldooren
Categories: eess.SP cs.AI cs.HC cs.LG
Comments: Accepted by 6th International Conference on Advances in Signal
  Processing and Artificial Intelligence (ASPAI' 2024)
\\
  Decoding EEG signals is crucial for unraveling human brain and advancing
brain-computer interfaces. Traditional machine learning algorithms have been
hindered by the high noise levels and inherent inter-person variations in EEG
signals. Recent advances in deep neural networks (DNNs) have shown promise,
owing to their advanced nonlinear modeling capabilities. However, DNN still
faces challenge in decoding EEG samples of unseen individuals. To address this,
this paper introduces a novel approach by incorporating the conditional
identification information of each individual into the neural network, thereby
enhancing model representation through the synergistic interaction of EEG and
personal traits. We test our model on the WithMe dataset and demonstrated that
the inclusion of these identifiers substantially boosts accuracy for both
subjects in the training set and unseen subjects. This enhancement suggests
promising potential for improving for EEG interpretability and understanding of
relevant identification features.
\\ ( https://arxiv.org/abs/2403.15489 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15509 (*cross-listing*)
Date: Fri, 22 Mar 2024 03:39:40 GMT   (2617kb,D)

Title: Twin Auto-Encoder Model for Learning Separable Representation in
  Cyberattack Detection
Authors: Phai Vu Dinh, Quang Uy Nguyen, Thai Hoang Dinh, Diep N. Nguyen, Bao
  Son Pham, and Eryk Dutkiewicz
Categories: cs.CR cs.AI cs.LG
\\
  Representation Learning (RL) plays a pivotal role in the success of many
problems including cyberattack detection. Most of the RL methods for
cyberattack detection are based on the latent vector of Auto-Encoder (AE)
models. An AE transforms raw data into a new latent representation that better
exposes the underlying characteristics of the input data. Thus, it is very
useful for identifying cyberattacks. However, due to the heterogeneity and
sophistication of cyberattacks, the representation of AEs is often
entangled/mixed resulting in the difficulty for downstream attack detection
models. To tackle this problem, we propose a novel mod called Twin Auto-Encoder
(TAE). TAE deterministically transforms the latent representation into a more
distinguishable representation namely the \textit{separable representation} and
the reconstructsuct the separable representation at the output. The output of
TAE called the \textit{reconstruction representation} is input to downstream
models to detect cyberattacks. We extensively evaluate the effectiveness of TAE
using a wide range of bench-marking datasets. Experiment results show the
superior accuracy of TAE over state-of-the-art RL models and well-known machine
learning algorithms. Moreover, TAE also outperforms state-of-the-art models on
some sophisticated and challenging attacks. We then investigate various
characteristics of TAE to further demonstrate its superiority.
\\ ( https://arxiv.org/abs/2403.15509 ,  2617kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15523 (*cross-listing*)
Date: Fri, 22 Mar 2024 13:35:34 GMT   (366kb,D)

Title: Towards auditory attention decoding with noise-tagging: A pilot study
Authors: H. A. Scheppink, S. Ahmadi, P. Desain, M. Tangermann, J. Thielen
Categories: q-bio.NC cs.AI cs.LG cs.SD eess.AS
Comments: 6 pages, 2 figures, 9th Graz Brain-Computer Interface Conference 2024
\\
  Auditory attention decoding (AAD) aims to extract from brain activity the
attended speaker amidst candidate speakers, offering promising applications for
neuro-steered hearing devices and brain-computer interfacing. This pilot study
makes a first step towards AAD using the noise-tagging stimulus protocol, which
evokes reliable code-modulated evoked potentials, but is minimally explored in
the auditory modality. Participants were sequentially presented with two Dutch
speech stimuli that were amplitude modulated with a unique binary pseudo-random
noise-code, effectively tagging these with additional decodable information. We
compared the decoding of unmodulated audio against audio modulated with various
modulation depths, and a conventional AAD method against a standard method to
decode noise-codes. Our pilot study revealed higher performances for the
conventional method with 70 to 100 percent modulation depths compared to
unmodulated audio. The noise-code decoder did not further improve these
results. These fundamental insights highlight the potential of integrating
noise-codes in speech to enhance auditory speaker detection when multiple
speakers are presented simultaneously.
\\ ( https://arxiv.org/abs/2403.15523 ,  366kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15528 (*cross-listing*)
Date: Fri, 22 Mar 2024 17:27:18 GMT   (451kb,D)

Title: Evaluating GPT-4 with Vision on Detection of Radiological Findings on
  Chest Radiographs
Authors: Yiliang Zhou, Hanley Ong, Patrick Kennedy, Carol Wu, Jacob Kazam,
  Keith Hentel, Adam Flanders, George Shih, Yifan Peng
Categories: eess.IV cs.AI cs.CV
\\
  The study examines the application of GPT-4V, a multi-modal large language
model equipped with visual recognition, in detecting radiological findings from
a set of 100 chest radiographs and suggests that GPT-4V is currently not ready
for real-world diagnostic usage in interpreting chest radiographs.
\\ ( https://arxiv.org/abs/2403.15528 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15551 (*cross-listing*)
Date: Fri, 22 Mar 2024 18:05:33 GMT   (267kb,D)

Title: Language-Based Depth Hints for Monocular Depth Estimation
Authors: Dylan Auty and Krystian Mikolajczyk
Categories: cs.CV cs.AI
Comments: 8 pages, 1 figure. Work originally done in June 2022
\\
  Monocular depth estimation (MDE) is inherently ambiguous, as a given image
may result from many different 3D scenes and vice versa. To resolve this
ambiguity, an MDE system must make assumptions about the most likely 3D scenes
for a given input. These assumptions can be either explicit or implicit. In
this work, we demonstrate the use of natural language as a source of an
explicit prior about the structure of the world. The assumption is made that
human language encodes the likely distribution in depth-space of various
objects. We first show that a language model encodes this implicit bias during
training, and that it can be extracted using a very simple learned approach. We
then show that this prediction can be provided as an explicit source of
assumption to an MDE system, using an off-the-shelf instance segmentation model
that provides the labels used as the input to the language model. We
demonstrate the performance of our method on the NYUD2 dataset, showing
improvement compared to the baseline and to random controls.
\\ ( https://arxiv.org/abs/2403.15551 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15559 (*cross-listing*)
Date: Fri, 22 Mar 2024 18:28:04 GMT   (39641kb,D)

Title: An Optimization Framework to Enforce Multi-View Consistency for
  Texturing 3D Meshes Using Pre-Trained Text-to-Image Models
Authors: Zhengyi Zhao and Chen Song and Xiaodong Gu and Yuan Dong and Qi Zuo
  and Weihao Yuan and Zilong Dong and Liefeng Bo and Qixing Huang
Categories: cs.CV cs.AI
\\
  A fundamental problem in the texturing of 3D meshes using pre-trained
text-to-image models is to ensure multi-view consistency. State-of-the-art
approaches typically use diffusion models to aggregate multi-view inputs, where
common issues are the blurriness caused by the averaging operation in the
aggregation step or inconsistencies in local features. This paper introduces an
optimization framework that proceeds in four stages to achieve multi-view
consistency. Specifically, the first stage generates an over-complete set of 2D
textures from a predefined set of viewpoints using an MV-consistent diffusion
process. The second stage selects a subset of views that are mutually
consistent while covering the underlying 3D model. We show how to achieve this
goal by solving semi-definite programs. The third stage performs non-rigid
alignment to align the selected views across overlapping regions. The fourth
stage solves an MRF problem to associate each mesh face with a selected view.
In particular, the third and fourth stages are iterated, with the cuts obtained
in the fourth stage encouraging non-rigid alignment in the third stage to focus
on regions close to the cuts. Experimental results show that our approach
significantly outperforms baseline approaches both qualitatively and
quantitatively.
\\ ( https://arxiv.org/abs/2403.15559 ,  39641kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15569 (*cross-listing*)
Date: Fri, 22 Mar 2024 18:47:54 GMT   (220kb,D)

Title: Music to Dance as Language Translation using Sequence Models
Authors: Andr\'e Correia, Lu\'is A. Alexandre
Categories: cs.SD cs.AI cs.RO eess.AS
\\
  Synthesising appropriate choreographies from music remains an open problem.
We introduce MDLT, a novel approach that frames the choreography generation
problem as a translation task. Our method leverages an existing data set to
learn to translate sequences of audio into corresponding dance poses. We
present two variants of MDLT: one utilising the Transformer architecture and
the other employing the Mamba architecture. We train our method on AIST++ and
PhantomDance data sets to teach a robotic arm to dance, but our method can be
applied to a full humanoid robot. Evaluation metrics, including Average Joint
Error and Frechet Inception Distance, consistently demonstrate that, when given
a piece of music, MDLT excels at producing realistic and high-quality
choreography. The code can be found at github.com/meowatthemoon/MDLT.
\\ ( https://arxiv.org/abs/2403.15569 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15585 (*cross-listing*)
Date: Fri, 22 Mar 2024 19:19:51 GMT   (8600kb,D)

Title: MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis
Authors: Mai A. Shaaban, Adnan Khan, Mohammad Yaqub
Categories: cs.CV cs.AI
\\
  Chest X-ray images are commonly used for predicting acute and chronic
cardiopulmonary conditions, but efforts to integrate them with structured
clinical data face challenges due to incomplete electronic health records
(EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate
multimodal large language models (MLLMs), few-shot prompting (FP) and visual
grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A
pre-trained MLLM is utilized to complement the missing EHR information,
providing a comprehensive understanding of patients' medical history.
Additionally, FP reduces the necessity for extensive training of MLLMs while
effectively tackling the issue of hallucination. Nevertheless, the process of
determining the optimal number of few-shot examples and selecting high-quality
candidates can be burdensome, yet it profoundly influences model performance.
Hence, we propose a new technique that dynamically refines few-shot data for
real-time adjustment to new patient scenarios. Moreover, VG aids in focusing
the model's attention on relevant regions of interest in X-ray images,
enhancing the identification of abnormalities. We release MedPromptX-VQA, a new
in-context visual question answering dataset encompassing interleaved image and
EHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the
SOTA performance of MedPromptX, achieving an 11% improvement in F1-score
compared to the baselines. Code and data are available at
\url{https://github.com/BioMedIA-MBZUAI/MedPromptX}.
\\ ( https://arxiv.org/abs/2403.15585 ,  8600kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15600 (*cross-listing*)
Date: Fri, 22 Mar 2024 20:06:41 GMT   (330kb,D)

Title: Just another copy and paste? Comparing the security vulnerabilities of
  ChatGPT generated code and StackOverflow answers
Authors: Sivana Hamer, Marcelo d'Amorim, Laurie Williams
Categories: cs.SE cs.AI cs.CR
Comments: 8 pages, 2 figures, accepted at Deep Learning Security and Privacy
  Workshop (DLSP) part of IEEE Symposium on Security and Privacy Workshops
  (SPW) for 2024
\\
  Sonatype's 2023 report found that 97% of developers and security leads
integrate generative Artificial Intelligence (AI), particularly Large Language
Models (LLMs), into their development process. Concerns about the security
implications of this trend have been raised. Developers are now weighing the
benefits and risks of LLMs against other relied-upon information sources, such
as StackOverflow (SO), requiring empirical data to inform their choice. In this
work, our goal is to raise software developers awareness of the security
implications when selecting code snippets by empirically comparing the
vulnerabilities of ChatGPT and StackOverflow. To achieve this, we used an
existing Java dataset from SO with security-related questions and answers.
Then, we asked ChatGPT the same SO questions, gathering the generated code for
comparison. After curating the dataset, we analyzed the number and types of
Common Weakness Enumeration (CWE) vulnerabilities of 108 snippets from each
platform using CodeQL. ChatGPT-generated code contained 248 vulnerabilities
compared to the 302 vulnerabilities found in SO snippets, producing 20% fewer
vulnerabilities with a statistically significant difference. Additionally,
ChatGPT generated 19 types of CWE, fewer than the 22 found in SO. Our findings
suggest developers are under-educated on insecure code propagation from both
platforms, as we found 274 unique vulnerabilities and 25 types of CWE. Any code
copied and pasted, created by AI or humans, cannot be trusted blindly,
requiring good software engineering practices to reduce risk. Future work can
help minimize insecure code propagation from any platform.
\\ ( https://arxiv.org/abs/2403.15600 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15601 (*cross-listing*)
Date: Fri, 22 Mar 2024 20:07:58 GMT   (397kb,D)

Title: From Guidelines to Governance: A Study of AI Policies in Education
Authors: Aashish Ghimire and John Edwards
Categories: cs.CY cs.AI
\\
  Emerging technologies like generative AI tools, including ChatGPT, are
increasingly utilized in educational settings, offering innovative approaches
to learning while simultaneously posing new challenges. This study employs a
survey methodology to examine the policy landscape concerning these
technologies, drawing insights from 102 high school principals and higher
education provosts. Our results reveal a prominent policy gap: the majority of
institutions lack specialized guide-lines for the ethical deployment of AI
tools such as ChatGPT. Moreover,we observed that high schools are less inclined
to work on policies than higher educational institutions. Where such policies
do exist, they often overlook crucial issues, including student privacy and
algorithmic transparency. Administrators overwhelmingly recognize the necessity
of these policies, primarily to safeguard student safety and mitigate
plagiarism risks. Our findings underscore the urgent need for flexible and
iterative policy frameworks in educational contexts.
\\ ( https://arxiv.org/abs/2403.15601 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15603 (*cross-listing*)
Date: Fri, 22 Mar 2024 20:11:19 GMT   (4881kb,D)

Title: Forward Learning for Gradient-based Black-box Saliency Map Generation
Authors: Zeliang Zhang, Mingqian Feng, Jinyang Jiang, Rongyi Zhu, Yijie Peng,
  Chenliang Xu
Categories: cs.CV cs.AI
\\
  Gradient-based saliency maps are widely used to explain deep neural network
decisions. However, as models become deeper and more black-box, such as in
closed-source APIs like ChatGPT, computing gradients become challenging,
hindering conventional explanation methods. In this work, we introduce a novel
unified framework for estimating gradients in black-box settings and generating
saliency maps to interpret model decisions. We employ the likelihood ratio
method to estimate output-to-input gradients and utilize them for saliency map
generation. Additionally, we propose blockwise computation techniques to
enhance estimation accuracy. Extensive experiments in black-box settings
validate the effectiveness of our method, demonstrating accurate gradient
estimation and explainability of generated saliency maps. Furthermore, we
showcase the scalability of our approach by applying it to explain GPT-Vision,
revealing the continued relevance of gradient-based explanation methods in the
era of large, closed-source, and black-box models.
\\ ( https://arxiv.org/abs/2403.15603 ,  4881kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15604 (*cross-listing*)
Date: Fri, 22 Mar 2024 20:16:55 GMT   (17724kb)

Title: Investigating Use Cases of AI-Powered Scene Description Applications for
  Blind and Low Vision People
Authors: Ricardo Gonzalez, Jazmin Collins, Shiri Azenkot, and Cynthia Bennett
Categories: cs.HC cs.AI
Comments: 21 pages, 18 figures, 5 tables, to appear CHI2024
\\
  "Scene description" applications that describe visual content in a photo are
useful daily tools for blind and low vision (BLV) people. Researchers have
studied their use, but they have only explored those that leverage remote
sighted assistants; little is known about applications that use AI to generate
their descriptions. Thus, to investigate their use cases, we conducted a
two-week diary study where 16 BLV participants used an AI-powered scene
description application we designed. Through their diary entries and follow-up
interviews, users shared their information goals and assessments of the visual
descriptions they received. We analyzed the entries and found frequent use
cases, such as identifying visual features of known objects, and surprising
ones, such as avoiding contact with dangerous objects. We also found users
scored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for
satisfaction and 2.43 out of 4 (SD=1.16) for trust, showing that descriptions
still need significant improvements to deliver satisfying and trustworthy
experiences. We discuss future opportunities for AI as it becomes a more
powerful accessibility tool for BLV users.
\\ ( https://arxiv.org/abs/2403.15604 ,  17724kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15646 (*cross-listing*)
Date: Fri, 22 Mar 2024 23:07:11 GMT   (1822kb)

Title: Application of the NIST AI Risk Management Framework to Surveillance
  Technology
Authors: Nandhini Swaminathan, David Danks
Categories: cs.CY cs.AI
Comments: 14 pages, 2 figures
MSC-class: K.4.1, K.5.2
\\
  This study offers an in-depth analysis of the application and implications of
the National Institute of Standards and Technology's AI Risk Management
Framework (NIST AI RMF) within the domain of surveillance technologies,
particularly facial recognition technology. Given the inherently high-risk and
consequential nature of facial recognition systems, our research emphasizes the
critical need for a structured approach to risk management in this sector. The
paper presents a detailed case study demonstrating the utility of the NIST AI
RMF in identifying and mitigating risks that might otherwise remain unnoticed
in these technologies. Our primary objective is to develop a comprehensive risk
management strategy that advances the practice of responsible AI utilization in
feasible, scalable ways. We propose a six-step process tailored to the specific
challenges of surveillance technology that aims to produce a more systematic
and effective risk management practice. This process emphasizes continual
assessment and improvement to facilitate companies in managing AI-related risks
more robustly and ensuring ethical and responsible deployment of AI systems.
Additionally, our analysis uncovers and discusses critical gaps in the current
framework of the NIST AI RMF, particularly concerning its application to
surveillance technologies. These insights contribute to the evolving discourse
on AI governance and risk management, highlighting areas for future refinement
and development in frameworks like the NIST AI RMF.
\\ ( https://arxiv.org/abs/2403.15646 ,  1822kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15648 (*cross-listing*)
Date: Fri, 22 Mar 2024 23:12:28 GMT   (1691kb,D)

Title: SRLM: Human-in-Loop Interactive Social Robot Navigation with Large
  Language Model and Deep Reinforcement Learning
Authors: Weizheng Wang, Le Mao, Ruiqi Wang, and Byung-Cheol Min
Categories: cs.RO cs.AI
\\
  An interactive social robotic assistant must provide services in complex and
crowded spaces while adapting its behavior based on real-time human language
commands or feedback. In this paper, we propose a novel hybrid approach called
Social Robot Planner (SRLM), which integrates Large Language Models (LLM) and
Deep Reinforcement Learning (DRL) to navigate through human-filled public
spaces and provide multiple social services. SRLM infers global planning from
human-in-loop commands in real-time, and encodes social information into a
LLM-based large navigation model (LNM) for low-level motion execution.
Moreover, a DRL-based planner is designed to maintain benchmarking performance,
which is blended with LNM by a large feedback model (LFM) to address the
instability of current text and LLM-driven LNM. Finally, SRLM demonstrates
outstanding performance in extensive experiments. More details about this work
are available at: https://sites.google.com/view/navi-srlm
\\ ( https://arxiv.org/abs/2403.15648 ,  1691kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15698 (*cross-listing*)
Date: Sat, 23 Mar 2024 03:23:29 GMT   (29082kb,D)

Title: SceneX:Procedural Controllable Large-scale Scene Generation via
  Large-language Models
Authors: Mengqi Zhou and Jun Hou and Chuanchen Luo and Yuxi Wang and Zhaoxiang
  Zhang and Junran Peng
Categories: cs.CV cs.AI
\\
  Due to its great application potential, large-scale scene generation has
drawn extensive attention in academia and industry. Recent research employs
powerful generative models to create desired scenes and achieves promising
results. However, most of these methods represent the scene using 3D primitives
(e.g. point cloud or radiance field) incompatible with the industrial pipeline,
which leads to a substantial gap between academic research and industrial
deployment. Procedural Controllable Generation (PCG) is an efficient technique
for creating scalable and high-quality assets, but it is unfriendly for
ordinary users as it demands profound domain expertise. To address these
issues, we resort to using the large language model (LLM) to drive the
procedural modeling. In this paper, we introduce a large-scale scene generation
framework, SceneX, which can automatically produce high-quality procedural
models according to designers' textual descriptions.Specifically, the proposed
method comprises two components, PCGBench and PCGPlanner. The former
encompasses an extensive collection of accessible procedural assets and
thousands of hand-craft API documents. The latter aims to generate executable
actions for Blender to produce controllable and precise 3D assets guided by the
user's instructions. Our SceneX can generate a city spanning 2.5 km times 2.5
km with delicate layout and geometric structures, drastically reducing the time
cost from several weeks for professional PCG engineers to just a few hours for
an ordinary user. Extensive experiments demonstrated the capability of our
method in controllable large-scale scene generation and editing, including
asset placement and season translation.
\\ ( https://arxiv.org/abs/2403.15698 ,  29082kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15709 (*cross-listing*)
Date: Sat, 23 Mar 2024 04:08:39 GMT   (8596kb,D)

Title: Contact-aware Human Motion Generation from Textual Descriptions
Authors: Sihan Ma, Qiong Cao, Jing Zhang, Dacheng Tao
Categories: cs.CV cs.AI
Comments: Project page: https://xymsh.github.io/RICH-CAT/
\\
  This paper addresses the problem of generating 3D interactive human motion
from text. Given a textual description depicting the actions of different body
parts in contact with objects, we synthesize sequences of 3D body poses that
are visually natural and physically plausible. Yet, this task poses a
significant challenge due to the inadequate consideration of interactions by
physical contacts in both motion and textual descriptions, leading to unnatural
and implausible sequences. To tackle this challenge, we create a novel dataset
named RICH-CAT, representing ``Contact-Aware Texts'' constructed from the RICH
dataset. RICH-CAT comprises high-quality motion, accurate human-object contact
labels, and detailed textual descriptions, encompassing over 8,500 motion-text
pairs across 26 indoor/outdoor actions. Leveraging RICH-CAT, we propose a novel
approach named CATMO for text-driven interactive human motion synthesis that
explicitly integrates human body contacts as evidence. We employ two VQ-VAE
models to encode motion and body contact sequences into distinct yet
complementary latent spaces and an intertwined GPT for generating human motions
and contacts in a mutually conditioned manner. Additionally, we introduce a
pre-trained text encoder to learn textual embeddings that better discriminate
among various contact types, allowing for more precise control over synthesized
motions and contacts. Our experiments demonstrate the superior performance of
our approach compared to existing text-to-motion methods, producing stable,
contact-aware motion sequences. Code and data will be available for research
purposes.
\\ ( https://arxiv.org/abs/2403.15709 ,  8596kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15716 (*cross-listing*)
Date: Sat, 23 Mar 2024 04:36:12 GMT   (1729kb,D)

Title: Distributed Robust Learning based Formation Control of Mobile Robots
  based on Bioinspired Neural Dynamics
Authors: Zhe Xu, Tao Yan, Simon X. Yang, S. Andrew Gadsden, Mohammad
  Biglarbegian
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: This paper is accepted by IEEE Transactions on Intelligent Vehicles
DOI: 10.1109/TIV.2024.3380000
\\
  This paper addresses the challenges of distributed formation control in
multiple mobile robots, introducing a novel approach that enhances real-world
practicability. We first introduce a distributed estimator using a variable
structure and cascaded design technique, eliminating the need for derivative
information to improve the real time performance. Then, a kinematic tracking
control method is developed utilizing a bioinspired neural dynamic-based
approach aimed at providing smooth control inputs and effectively resolving the
speed jump issue. Furthermore, to address the challenges for robots operating
with completely unknown dynamics and disturbances, a learning-based robust
dynamic controller is developed. This controller provides real time parameter
estimates while maintaining its robustness against disturbances. The overall
stability of the proposed method is proved with rigorous mathematical analysis.
At last, multiple comprehensive simulation studies have shown the advantages
and effectiveness of the proposed method.
\\ ( https://arxiv.org/abs/2403.15716 ,  1729kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15743 (*cross-listing*)
Date: Sat, 23 Mar 2024 07:14:27 GMT   (774kb,D)

Title: A Comparative Study of Artificial Potential Fields and Safety Filters
Authors: Ming Li and Zhiyong Sun
Categories: eess.SY cs.AI cs.SY math.DS
\\
  In this paper, we have demonstrated that the controllers designed by a
classical motion planning tool, namely artificial potential fields (APFs), can
be derived from a recently prevalent approach: control barrier function
quadratic program (CBF-QP) safety filters. By integrating APF information into
the CBF-QP framework, we establish a bridge between these two methodologies.
Specifically, this is achieved by employing the attractive potential field as a
control Lyapunov function (CLF) to guide the design of the nominal controller,
and then the repulsive potential field serves as a reciprocal CBF (RCBF) to
define a CBF-QP safety filter. Building on this integration, we extend the
design of the CBF-QP safety filter to accommodate a more general class of
dynamical models featuring a control-affine structure. This extension yields a
special CBF-QP safety filter and a general APF solution suitable for
control-affine dynamical models. Through a reach-avoid navigation example, we
showcase the efficacy of the developed approaches.
\\ ( https://arxiv.org/abs/2403.15743 ,  774kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15747 (*cross-listing*)
Date: Sat, 23 Mar 2024 07:29:41 GMT   (387kb,D)

Title: CodeShell Technical Report
Authors: Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, Wei Ye
Categories: cs.SE cs.AI
\\
  Code large language models mark a pivotal breakthrough in artificial
intelligence. They are specifically crafted to understand and generate
programming languages, significantly boosting the efficiency of coding
development workflows. In this technical report, we present CodeShell-Base, a
seven billion-parameter foundation model with 8K context length, showcasing
exceptional proficiency in code comprehension. By incorporating Grouped-Query
Attention and Rotary Positional Embedding into GPT-2, CodeShell-Base integrates
the structural merits of StarCoder and CodeLlama and forms its unique
architectural design. We then carefully built a comprehensive data
pre-processing process, including similar data deduplication, perplexity-based
data filtering, and model-based data filtering. Through this process, We have
curated 100 billion high-quality pre-training data from GitHub. Benefiting from
the high-quality data, CodeShell-Base outperforms CodeLlama in Humaneval after
training on just 500 billion tokens (5 epochs). We have conducted extensive
experiments across multiple language datasets, including Python, Java, and C++,
and the results indicate that our model possesses robust foundational
capabilities in code comprehension and generation.
\\ ( https://arxiv.org/abs/2403.15747 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15756 (*cross-listing*)
Date: Sat, 23 Mar 2024 07:59:30 GMT   (420kb)

Title: Leveraging Large Language Models for Preliminary Security Risk Analysis:
  A Mission-Critical Case Study
Authors: Matteo Esposito and Francesco Palagiano
Categories: cs.SE cs.AI cs.CL cs.CR cs.CY
\\
  Preliminary security risk analysis (PSRA) provides a quick approach to
identify, evaluate and propose remeditation to potential risks in specific
scenarios. The extensive expertise required for an effective PSRA and the
substantial ammount of textual-related tasks hinder quick assessments in
mission-critical contexts, where timely and prompt actions are essential. The
speed and accuracy of human experts in PSRA significantly impact response time.
A large language model can quickly summarise information in less time than a
human. To our knowledge, no prior study has explored the capabilities of
fine-tuned models (FTM) in PSRA. Our case study investigates the proficiency of
FTM to assist practitioners in PSRA. We manually curated 141 representative
samples from over 50 mission-critical analyses archived by the industrial
context team in the last five years.We compared the proficiency of the FTM
versus seven human experts. Within the industrial context, our approach has
proven successful in reducing errors in PSRA, hastening security risk
detection, and minimizing false positives and negatives. This translates to
cost savings for the company by averting unnecessary expenses associated with
implementing unwarranted countermeasures. Therefore, experts can focus on more
comprehensive risk analysis, leveraging LLMs for an effective preliminary
assessment within a condensed timeframe.
\\ ( https://arxiv.org/abs/2403.15756 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15757 (*cross-listing*)
Date: Sat, 23 Mar 2024 08:03:50 GMT   (24654kb,D)

Title: User-Side Realization
Authors: Ryoma Sato
Categories: cs.IR cs.AI cs.CL cs.CR cs.LG
Comments: Doctoral Thesis
\\
  Users are dissatisfied with services. Since the service is not tailor-made
for a user, it is natural for dissatisfaction to arise. The problem is, that
even if users are dissatisfied, they often do not have the means to resolve
their dissatisfaction. The user cannot alter the source code of the service,
nor can they force the service provider to change. The user has no choice but
to remain dissatisfied or quit the service. User-side realization offers
proactive solutions to this problem by providing general algorithms to deal
with common problems on the user's side. These algorithms run on the user's
side and solve the problems without having the service provider change the
service itself.
\\ ( https://arxiv.org/abs/2403.15757 ,  24654kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15765 (*cross-listing*)
Date: Sat, 23 Mar 2024 08:40:35 GMT   (5158kb,D)

Title: Towards Human-Like Machine Comprehension: Few-Shot Relational Learning
  in Visually-Rich Documents
Authors: Hao Wang, Tang Li, Chenhui Chu, Nengjun Zhu, Rui Wang, and Pinpin Zhu
Categories: cs.CV cs.AI cs.IR
Comments: 13 pages, 7 figures, accepted by LERC-COLING2024
\\
  Key-value relations are prevalent in Visually-Rich Documents (VRDs), often
depicted in distinct spatial regions accompanied by specific color and font
styles. These non-textual cues serve as important indicators that greatly
enhance human comprehension and acquisition of such relation triplets. However,
current document AI approaches often fail to consider this valuable prior
information related to visual and spatial features, resulting in suboptimal
performance, particularly when dealing with limited examples. To address this
limitation, our research focuses on few-shot relational learning, specifically
targeting the extraction of key-value relation triplets in VRDs. Given the
absence of a suitable dataset for this task, we introduce two new few-shot
benchmarks built upon existing supervised benchmark datasets. Furthermore, we
propose a variational approach that incorporates relational 2D-spatial priors
and prototypical rectification techniques. This approach aims to generate
relation representations that are more aware of the spatial context and unseen
relation in a manner similar to human perception. Experimental results
demonstrate the effectiveness of our proposed method by showcasing its ability
to outperform existing methods. This study also opens up new possibilities for
practical applications.
\\ ( https://arxiv.org/abs/2403.15765 ,  5158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15769 (*cross-listing*)
Date: Sat, 23 Mar 2024 08:54:03 GMT   (4036kb,D)

Title: FusionINN: Invertible Image Fusion for Brain Tumor Monitoring
Authors: Nishant Kumar, Ziyan Tao, Jaikirat Singh, Yang Li, Peiwen Sun, Binghui
  Zhao, Stefan Gumhold
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: Source code coming soon
\\
  Image fusion typically employs non-invertible neural networks to merge
multiple source images into a single fused image. However, for clinical
experts, solely relying on fused images may be insufficient for making
diagnostic decisions, as the fusion mechanism blends features from source
images, thereby making it difficult to interpret the underlying tumor
pathology. We introduce FusionINN, a novel invertible image fusion framework,
capable of efficiently generating fused images and also decomposing them back
to the source images by solving the inverse of the fusion process. FusionINN
guarantees lossless one-to-one pixel mapping by integrating a normally
distributed latent image alongside the fused image to facilitate the generative
modeling of the decomposition process. To the best of our knowledge, we are the
first to investigate the decomposability of fused images, which is particularly
crucial for life-sensitive applications such as medical image fusion compared
to other tasks like multi-focus or multi-exposure image fusion. Our extensive
experimentation validates FusionINN over existing discriminative and generative
fusion methods, both subjectively and objectively. Moreover, compared to a
recent denoising diffusion-based fusion model, our approach offers faster and
qualitatively better fusion results. We also exhibit the clinical utility of
our results in aiding disease prognosis.
\\ ( https://arxiv.org/abs/2403.15769 ,  4036kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15807 (*cross-listing*)
Date: Sat, 23 Mar 2024 11:34:17 GMT   (1184kb,D)

Title: Efficient Data Access Paths for Mixed Vector-Relational Search
Authors: Viktor Sanca and Anastasia Ailamaki
Categories: cs.DB cs.AI cs.LG
\\
  The rapid growth of machine learning capabilities and the adoption of data
processing methods using vector embeddings sparked a great interest in creating
systems for vector data management. While the predominant approach of vector
data management is to use specialized index structures for fast search over the
entirety of the vector embeddings, once combined with other (meta)data, the
search queries can also become selective on relational attributes - typical for
analytical queries. As using vector indexes differs from traditional relational
data access, we revisit and analyze alternative access paths for efficient
mixed vector-relational search.
  We first evaluate the accurate but exhaustive scan-based search and propose
hardware optimizations and alternative tensor-based formulation and batching to
offset the cost. We outline the complex access-path design space, primarily
driven by relational selectivity, and the decisions to consider when selecting
an exhaustive scan-based search against an approximate index-based approach.
Since the vector index primarily avoids expensive computation across the entire
dataset, contrary to the common relational knowledge, it is better to scan at
lower selectivity and probe at higher, with a cross-point between the two
approaches dictated by data dimensionality and the number of concurrent search
queries.
\\ ( https://arxiv.org/abs/2403.15807 ,  1184kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15812 (*cross-listing*)
Date: Sat, 23 Mar 2024 11:50:20 GMT   (4386kb,D)

Title: The Impact of Evolutionary Computation on Robotic Design: A Case Study
  with an Underactuated Hand Exoskeleton
Authors: Baris Akbas, Huseyin Taner Yuksel, Aleyna Soylemez, Mazhar Eid Zyada,
  Mine Sarac, Fabio Stroppa
Categories: cs.RO cs.AI cs.HC
Comments: 6 pages (+ref), 4 figures, IEEE International Conference on Robotics
  and Automation (ICRA) 2024
\\
  Robotic exoskeletons can enhance human strength and aid people with physical
disabilities. However, designing them to ensure safety and optimal performance
presents significant challenges. Developing exoskeletons should incorporate
specific optimization algorithms to find the best design. This study
investigates the potential of Evolutionary Computation (EC) methods in robotic
design optimization, with an underactuated hand exoskeleton (U-HEx) used as a
case study. We propose improving the performance and usability of the U-HEx
design, which was initially optimized using a naive brute-force approach, by
integrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch
Algorithm. Comparative analysis revealed that EC methods consistently yield
more precise and optimal solutions than brute force in a significantly shorter
time. This allowed us to improve the optimization by increasing the number of
variables in the design, which was impossible with naive methods. The results
show significant improvements in terms of the torque magnitude the device
transfers to the user, enhancing its efficiency. These findings underline the
importance of performing proper optimization while designing exoskeletons, as
well as providing a significant improvement to this specific robotic design.
\\ ( https://arxiv.org/abs/2403.15812 ,  4386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15826 (*cross-listing*)
Date: Sat, 23 Mar 2024 12:53:51 GMT   (2042kb,D)

Title: Scaling Learning based Policy Optimization for Temporal Tasks via
  Dropout
Authors: Navid Hashemi, Bardh Hoxha, Danil Prokhorov, Georgios Fainekos,
  Jyotirmoy Deshmukh
Categories: eess.SY cs.AI cs.LG cs.RO cs.SY
\\
  This paper introduces a model-based approach for training feedback
controllers for an autonomous agent operating in a highly nonlinear
environment. We desire the trained policy to ensure that the agent satisfies
specific task objectives, expressed in discrete-time Signal Temporal Logic
(DT-STL). One advantage for reformulation of a task via formal frameworks, like
DT-STL, is that it permits quantitative satisfaction semantics. In other words,
given a trajectory and a DT-STL formula, we can compute the robustness, which
can be interpreted as an approximate signed distance between the trajectory and
the set of trajectories satisfying the formula. We utilize feedback
controllers, and we assume a feed forward neural network for learning these
feedback controllers. We show how this learning problem is similar to training
recurrent neural networks (RNNs), where the number of recurrent units is
proportional to the temporal horizon of the agent's task objectives. This poses
a challenge: RNNs are susceptible to vanishing and exploding gradients, and
na\"{i}ve gradient descent-based strategies to solve long-horizon task
objectives thus suffer from the same problems. To tackle this challenge, we
introduce a novel gradient approximation algorithm based on the idea of dropout
or gradient sampling. We show that, the existing smooth semantics for
robustness are inefficient regarding gradient computation when the
specification becomes complex. To address this challenge, we propose a new
smooth semantics for DT-STL that under-approximates the robustness value and
scales well for backpropagation over a complex specification. We show that our
control synthesis methodology, can be quite helpful for stochastic gradient
descent to converge with less numerical issues, enabling scalable
backpropagation over long time horizons and trajectories over high dimensional
state spaces.
\\ ( https://arxiv.org/abs/2403.15826 ,  2042kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15834 (*cross-listing*)
Date: Sat, 23 Mar 2024 13:21:09 GMT   (636kb,D)

Title: ARO: Large Language Model Supervised Robotics Text2Skill Autonomous
  Learning
Authors: Yiwen Chen, Yuyao Ye, Ziyi Chen, Chuheng Zhang, Marcelo H. Ang
Categories: cs.RO cs.AI
Comments: 6 pages, 2 figures
\\
  Robotics learning highly relies on human expertise and efforts, such as
demonstrations, design of reward functions in reinforcement learning,
performance evaluation using human feedback, etc. However, reliance on human
assistance can lead to expensive learning costs and make skill learning
difficult to scale. In this work, we introduce the Large Language Model
Supervised Robotics Text2Skill Autonomous Learning (ARO) framework, which aims
to replace human participation in the robot skill learning process with
large-scale language models that incorporate reward function design and
performance evaluation. We provide evidence that our approach enables fully
autonomous robot skill learning, capable of completing partial tasks without
human intervention. Furthermore, we also analyze the limitations of this
approach in task understanding and optimization stability.
\\ ( https://arxiv.org/abs/2403.15834 ,  636kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15852 (*cross-listing*)
Date: Sat, 23 Mar 2024 14:04:48 GMT   (2341kb,D)

Title: When LLM-based Code Generation Meets the Software Development Process
Authors: Feng Lin, Dong Jae Kim, Tse-Husn (Peter) Chen
Categories: cs.SE cs.AI
\\
  Software process models play a pivotal role in fostering collaboration and
communication within software teams, enabling them to tackle intricate
development tasks effectively. This paper introduces LCG, a code generation
framework inspired by established software engineering practices. LCG leverages
multiple Large Language Model (LLM) agents to emulate various software process
models, namely LCGWaterfall, LCGTDD, and LCGScrum. Each model assigns LLM
agents specific roles such as requirement engineer, architect, developer,
tester, and scrum master, mirroring typical development activities and
communication patterns. Through collaborative efforts utilizing
chain-of-thought and prompt composition techniques, the agents continuously
refine themselves to enhance code quality. Utilizing GPT3.5 as the underlying
LLM and baseline (GPT), we evaluate LCG across four code generation benchmarks:
HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Results indicate LCGScrum
outperforms other models, achieving Pass@1 scores of 75.2, 65.5, 82.5, and 56.7
in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively - an average 15%
improvement over GPT. Analysis reveals distinct impacts of development
activities on generated code, with design and code reviews contributing to
enhanced exception handling, while design, testing, and code reviews mitigate
code smells. Furthermore, temperature values exhibit negligible influence on
Pass@1 across all models. However, variations in Pass@1 are notable for
different GPT3.5 model versions, ranging from 5 to over 60 in HumanEval,
highlighting the stability of LCG across model versions. This stability
underscores the importance of adopting software process models to bolster the
quality and consistency of LLM-generated code.
\\ ( https://arxiv.org/abs/2403.15852 ,  2341kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15857 (*cross-listing*)
Date: Sat, 23 Mar 2024 14:47:26 GMT   (2302kb,D)

Title: Automated System-level Testing of Unmanned Aerial Systems
Authors: Hassan Sartaj, Asmar Muqeet, Muhammad Zohaib Iqbal, Muhammad Uzair
  Khan
Categories: cs.SE cs.AI cs.RO
\\
  Unmanned aerial systems (UAS) rely on various avionics systems that are
safety-critical and mission-critical. A major requirement of international
safety standards is to perform rigorous system-level testing of avionics
software systems. The current industrial practice is to manually create test
scenarios, manually/automatically execute these scenarios using simulators, and
manually evaluate outcomes. The test scenarios typically consist of setting
certain flight or environment conditions and testing the system under test in
these settings. The state-of-the-art approaches for this purpose also require
manual test scenario development and evaluation. In this paper, we propose a
novel approach to automate the system-level testing of the UAS. The proposed
approach (AITester) utilizes model-based testing and artificial intelligence
(AI) techniques to automatically generate, execute, and evaluate various test
scenarios. The test scenarios are generated on the fly, i.e., during test
execution based on the environmental context at runtime. The approach is
supported by a toolset. We empirically evaluate the proposed approach on two
core components of UAS, an autopilot system of an unmanned aerial vehicle (UAV)
and cockpit display systems (CDS) of the ground control station (GCS). The
results show that the AITester effectively generates test scenarios causing
deviations from the expected behavior of the UAV autopilot and reveals
potential flaws in the GCS-CDS.
\\ ( https://arxiv.org/abs/2403.15857 ,  2302kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15876 (*cross-listing*)
Date: Sat, 23 Mar 2024 15:53:00 GMT   (24233kb,D)

Title: Cognitive resilience: Unraveling the proficiency of image-captioning
  models to interpret masked visual content
Authors: Zhicheng Du, Zhaotian Xie, Huazhang Ying, Likun Zhang, Peiwu Qin
Categories: cs.CV cs.AI
Comments: Accepted as tiny paper in ICLR 2024
\\
  This study explores the ability of Image Captioning (IC) models to decode
masked visual content sourced from diverse datasets. Our findings reveal the IC
model's capability to generate captions from masked images, closely resembling
the original content. Notably, even in the presence of masks, the model adeptly
crafts descriptive textual information that goes beyond what is observable in
the original image-generated captions. While the decoding performance of the IC
model experiences a decline with an increase in the masked region's area, the
model still performs well when important regions of the image are not masked at
high coverage.
\\ ( https://arxiv.org/abs/2403.15876 ,  24233kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15931 (*cross-listing*)
Date: Sat, 23 Mar 2024 20:30:28 GMT   (15623kb,D)

Title: X-Portrait: Expressive Portrait Animation with Hierarchical Motion
  Attention
Authors: You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, Linjie Luo
Categories: cs.CV cs.AI
\\
  We propose X-Portrait, an innovative conditional diffusion model tailored for
generating expressive and temporally coherent portrait animation. Specifically,
given a single portrait as appearance reference, we aim to animate it with
motion derived from a driving video, capturing both highly dynamic and subtle
facial expressions along with wide-range head movements. As its core, we
leverage the generative prior of a pre-trained diffusion model as the rendering
backbone, while achieve fine-grained head pose and expression control with
novel controlling signals within the framework of ControlNet. In contrast to
conventional coarse explicit controls such as facial landmarks, our motion
control module is learned to interpret the dynamics directly from the original
driving RGB inputs. The motion accuracy is further enhanced with a patch-based
local control module that effectively enhance the motion attention to
small-scale nuances like eyeball positions. Notably, to mitigate the identity
leakage from the driving signals, we train our motion control modules with
scaling-augmented cross-identity images, ensuring maximized disentanglement
from the appearance reference modules. Experimental results demonstrate the
universal effectiveness of X-Portrait across a diverse range of facial
portraits and expressive driving sequences, and showcase its proficiency in
generating captivating portrait animations with consistently maintained
identity characteristics.
\\ ( https://arxiv.org/abs/2403.15931 ,  15623kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15941 (*cross-listing*)
Date: Sat, 23 Mar 2024 22:04:03 GMT   (7374kb,D)

Title: Explore until Confident: Efficient Exploration for Embodied Question
  Answering
Authors: Allen Z. Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha
  Majumdar, Dorsa Sadigh
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: Under review
\\
  We consider the problem of Embodied Question Answering (EQA), which refers to
settings where an embodied agent such as a robot needs to actively explore an
environment to gather information until it is confident about the answer to a
question. In this work, we leverage the strong semantic reasoning capabilities
of large vision-language models (VLMs) to efficiently explore and answer such
questions. However, there are two main challenges when using VLMs in EQA: they
do not have an internal memory for mapping the scene to be able to plan how to
explore over time, and their confidence can be miscalibrated and can cause the
robot to prematurely stop exploration or over-explore. We propose a method that
first builds a semantic map of the scene based on depth information and via
visual prompting of a VLM - leveraging its vast knowledge of relevant regions
of the scene for exploration. Next, we use conformal prediction to calibrate
the VLM's question answering confidence, allowing the robot to know when to
stop exploration - leading to a more calibrated and efficient exploration
strategy. To test our framework in simulation, we also contribute a new EQA
dataset with diverse, realistic human-robot scenarios and scenes built upon the
Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot
experiments show our proposed approach improves the performance and efficiency
over baselines that do no leverage VLM for exploration or do not calibrate its
confidence. Webpage with experiment videos and code:
https://explore-eqa.github.io/
\\ ( https://arxiv.org/abs/2403.15941 ,  7374kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15944 (*cross-listing*)
Date: Sat, 23 Mar 2024 22:14:38 GMT   (8941kb,D)

Title: Adaptive Super Resolution For One-Shot Talking-Head Generation
Authors: Luchuan Song, Pinxin Liu, Guojun Yin, Chenliang Xu
Categories: cs.CV cs.AI eess.IV
Comments: 5 pages, 3 figures
\\
  The one-shot talking-head generation learns to synthesize a talking-head
video with one source portrait image under the driving of same or different
identity video. Usually these methods require plane-based pixel transformations
via Jacobin matrices or facial image warps for novel poses generation. The
constraints of using a single image source and pixel displacements often
compromise the clarity of the synthesized images. Some methods try to improve
the quality of synthesized videos by introducing additional super-resolution
modules, but this will undoubtedly increase computational consumption and
destroy the original data distribution. In this work, we propose an adaptive
high-quality talking-head video generation method, which synthesizes
high-resolution video without additional pre-trained modules. Specifically,
inspired by existing super-resolution methods, we down-sample the one-shot
source image, and then adaptively reconstruct high-frequency details via an
encoder-decoder module, resulting in enhanced video clarity. Our method
consistently improves the quality of generated videos through a straightforward
yet effective strategy, substantiated by quantitative and qualitative
evaluations. The code and demo video are available on:
\url{https://github.com/Songluchuan/AdaSR-TalkingHead/}.
\\ ( https://arxiv.org/abs/2403.15944 ,  8941kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15955 (*cross-listing*)
Date: Sat, 23 Mar 2024 23:22:54 GMT   (2200kb,D)

Title: Finding needles in a haystack: A Black-Box Approach to Invisible
  Watermark Detection
Authors: Minzhou Pan, Zhengting Wang, Xin Dong, Vikash Sehwag, Lingjuan Lyu,
  Xue Lin
Categories: cs.CV cs.AI
\\
  In this paper, we propose WaterMark Detection (WMD), the first invisible
watermark detection method under a black-box and annotation-free setting. WMD
is capable of detecting arbitrary watermarks within a given reference dataset
using a clean non-watermarked dataset as a reference, without relying on
specific decoding methods or prior knowledge of the watermarking techniques. We
develop WMD using foundations of offset learning, where a clean non-watermarked
dataset enables us to isolate the influence of only watermarked samples in the
reference dataset. Our comprehensive evaluations demonstrate the effectiveness
of WMD, significantly outperforming naive detection methods, which only yield
AUC scores around 0.5. In contrast, WMD consistently achieves impressive
detection AUC scores, surpassing 0.9 in most single-watermark datasets and
exceeding 0.7 in more challenging multi-watermark scenarios across diverse
datasets and watermarking methods. As invisible watermarks become increasingly
prevalent, while specific decoding techniques remain undisclosed, our approach
provides a versatile solution and establishes a path toward increasing
accountability, transparency, and trust in our digital visual content.
\\ ( https://arxiv.org/abs/2403.15955 ,  2200kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15974 (*cross-listing*)
Date: Sun, 24 Mar 2024 00:46:40 GMT   (1402kb,D)

Title: CBGT-Net: A Neuromimetic Architecture for Robust Classification of
  Streaming Data
Authors: Shreya Sharma, Dana Hughes, Katia Sycara
Categories: cs.NE cs.AI cs.CV cs.LG
\\
  This paper describes CBGT-Net, a neural network model inspired by the
cortico-basal ganglia-thalamic (CBGT) circuits found in mammalian brains.
Unlike traditional neural network models, which either generate an output for
each provided input, or an output after a fixed sequence of inputs, the
CBGT-Net learns to produce an output after a sufficient criteria for evidence
is achieved from a stream of observed data. For each observation, the CBGT-Net
generates a vector that explicitly represents the amount of evidence the
observation provides for each potential decision, accumulates the evidence over
time, and generates a decision when the accumulated evidence exceeds a
pre-defined threshold. We evaluate the proposed model on two image
classification tasks, where models need to predict image categories based on a
stream of small patches extracted from the image. We show that the CBGT-Net
provides improved accuracy and robustness compared to models trained to
classify from a single patch, and models leveraging an LSTM layer to classify
from a fixed sequence length of patches.
\\ ( https://arxiv.org/abs/2403.15974 ,  1402kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15977 (*cross-listing*)
Date: Sun, 24 Mar 2024 01:20:08 GMT   (35329kb,D)

Title: Towards Two-Stream Foveation-based Active Vision Learning
Authors: Timur Ibrayev, Amitangshu Mukherjee, Sai Aparna Aketi, and Kaushik Roy
Categories: cs.CV cs.AI
Comments: 18 pages, 14 figures, Under consideration at IEEE Transactions on
  Cognitive and Developmental Systems
\\
  Deep neural network (DNN) based machine perception frameworks process the
entire input in a one-shot manner to provide answers to both "what object is
being observed" and "where it is located". In contrast, the "two-stream
hypothesis" from neuroscience explains the neural processing in the human
visual cortex as an active vision system that utilizes two separate regions of
the brain to answer the what and the where questions. In this work, we propose
a machine learning framework inspired by the "two-stream hypothesis" and
explore the potential benefits that it offers. Specifically, the proposed
framework models the following mechanisms: 1) ventral (what) stream focusing on
the input regions perceived by the fovea part of an eye (foveation), 2) dorsal
(where) stream providing visual guidance, and 3) iterative processing of the
two streams to calibrate visual focus and process the sequence of focused image
patches. The training of the proposed framework is accomplished by label-based
DNN training for the ventral stream model and reinforcement learning for the
dorsal stream model. We show that the two-stream foveation-based learning is
applicable to the challenging task of weakly-supervised object localization
(WSOL), where the training data is limited to the object class or its
attributes. The framework is capable of both predicting the properties of an
object and successfully localizing it by predicting its bounding box. We also
show that, due to the independent nature of the two streams, the dorsal model
can be applied on its own to unseen images to localize objects from different
datasets.
\\ ( https://arxiv.org/abs/2403.15977 ,  35329kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15994 (*cross-listing*)
Date: Sun, 24 Mar 2024 03:10:39 GMT   (5537kb,D)

Title: Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial
  Expression Spotting
Authors: Yicheng Deng, Hideaki Hayashi, Hajime Nagahara
Categories: cs.CV cs.AI
Comments: Accepted by FG2024
\\
  Facial expression spotting is a significant but challenging task in facial
expression analysis. The accuracy of expression spotting is affected not only
by irrelevant facial movements but also by the difficulty of perceiving subtle
motions in micro-expressions. In this paper, we propose a Multi-Scale
Spatio-Temporal Graph Convolutional Network (SpoT-GCN) for facial expression
spotting. To extract more robust motion features, we track both short- and
long-term motion of facial muscles in compact sliding windows whose window
length adapts to the temporal receptive field of the network. This strategy,
termed the receptive field adaptive sliding window strategy, effectively
magnifies the motion features while alleviating the problem of severe head
movement. The subtle motion features are then converted to a facial graph
representation, whose spatio-temporal graph patterns are learned by a graph
convolutional network. This network learns both local and global features from
multiple scales of facial graph structures using our proposed facial local
graph pooling (FLGP). Furthermore, we introduce supervised contrastive learning
to enhance the discriminative capability of our model for difficult-to-classify
frames. The experimental results on the SAMM-LV and CAS(ME)^2 datasets
demonstrate that our method achieves state-of-the-art performance, particularly
in micro-expression spotting. Ablation studies further verify the effectiveness
of our proposed modules.
\\ ( https://arxiv.org/abs/2403.15994 ,  5537kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16003 (*cross-listing*)
Date: Sun, 24 Mar 2024 04:22:37 GMT   (994kb,D)

Title: Diverse Representation Embedding for Lifelong Person Re-Identification
Authors: Shiben Liu, Huijie Fan, Qiang Wang, Xiai Chen, Zhi Han, and Yandong
  Tang
Categories: cs.CV cs.AI
Comments: 11 pages,7 Tables,3 Figures
\\
  Lifelong Person Re-Identification (LReID) aims to continuously learn from
successive data streams, matching individuals across multiple cameras. The key
challenge for LReID is how to effectively preserve old knowledge while learning
new information incrementally. Task-level domain gaps and limited old task
datasets are key factors leading to catastrophic forgetting in ReLD, which are
overlooked in existing methods. To alleviate this problem, we propose a novel
Diverse Representation Embedding (DRE) framework for LReID. The proposed DRE
preserves old knowledge while adapting to new information based on
instance-level and task-level layout. Concretely, an Adaptive Constraint Module
(ACM) is proposed to implement integration and push away operations between
multiple representations, obtaining dense embedding subspace for each instance
to improve matching ability on limited old task datasets. Based on the
processed diverse representation, we interact knowledge between the adjustment
model and the learner model through Knowledge Update (KU) and Knowledge
Preservation (KP) strategies at the task-level layout, which reduce the
task-wise domain gap on both old and new tasks, and exploit diverse
representation of each instance in limited datasets from old tasks, improving
model performance for extended periods. Extensive experiments were conducted on
eleven Re-ID datasets, including five seen datasets for training in order-1 and
order-2 orders and six unseen datasets for inference. Compared to
state-of-the-art methods, our method achieves significantly improved
performance in holistic, large-scale, and occluded datasets.
\\ ( https://arxiv.org/abs/2403.16003 ,  994kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16016 (*cross-listing*)
Date: Sun, 24 Mar 2024 05:26:55 GMT   (9226kb,D)

Title: Fill in the ____ (a Diffusion-based Image Inpainting Pipeline)
Authors: Eyoel Gebre, Krishna Saxena, Timothy Tran
Categories: cs.CV cs.AI
\\
  Image inpainting is the process of taking an image and generating lost or
intentionally occluded portions. Inpainting has countless applications
including restoring previously damaged pictures, restoring the quality of
images that have been degraded due to compression, and removing unwanted
objects/text. Modern inpainting techniques have shown remarkable ability in
generating sensible completions for images with mask occlusions. In our paper,
an overview of the progress of inpainting techniques will be provided, along
with identifying current leading approaches, focusing on their strengths and
weaknesses. A critical gap in these existing models will be addressed, focusing
on the ability to prompt and control what exactly is generated. We will
additionally justify why we think this is the natural next progressive step
that inpainting models must take, and provide multiple approaches to
implementing this functionality. Finally, we will evaluate the results of our
approaches by qualitatively checking whether they generate high-quality images
that correctly inpaint regions with the objects that they are instructed to
produce.
\\ ( https://arxiv.org/abs/2403.16016 ,  9226kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16023 (*cross-listing*)
Date: Sun, 24 Mar 2024 05:55:39 GMT   (7884kb,D)

Title: RPMArt: Towards Robust Perception and Manipulation for Articulated
  Objects
Authors: Junbo Wang, Wenhai Liu, Qiaojun Yu, Yang You, Liu Liu, Weiming Wang,
  Cewu Lu
Categories: cs.RO cs.AI cs.CV
Comments: 8 pages, 7 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024), project website at
  https://r-pmart.github.io
\\
  Articulated objects are commonly found in daily life. It is essential that
robots can exhibit robust perception and manipulation skills for articulated
objects in real-world robotic applications. However, existing methods for
articulated objects insufficiently address noise in point clouds and struggle
to bridge the gap between simulation and reality, thus limiting the practical
deployment in real-world scenarios. To tackle these challenges, we propose a
framework towards Robust Perception and Manipulation for Articulated Objects
(RPMArt), which learns to estimate the articulation parameters and manipulate
the articulation part from the noisy point cloud. Our primary contribution is a
Robust Articulation Network (RoArtNet) that is able to predict both joint
parameters and affordable points robustly by local feature learning and point
tuple voting. Moreover, we introduce an articulation-aware classification
scheme to enhance its ability for sim-to-real transfer. Finally, with the
estimated affordable point and articulation joint constraint, the robot can
generate robust actions to manipulate articulated objects. After learning only
from synthetic data, RPMArt is able to transfer zero-shot to real-world
articulated objects. Experimental results confirm our approach's effectiveness,
with our framework achieving state-of-the-art performance in both noise-added
simulation and real-world environments. The code and data will be open-sourced
for reproduction. More results are published on the project website at
https://r-pmart.github.io .
\\ ( https://arxiv.org/abs/2403.16023 ,  7884kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16043 (*cross-listing*)
Date: Sun, 24 Mar 2024 07:04:08 GMT   (1479kb,D)

Title: Semantic Is Enough: Only Semantic Information For NeRF Reconstruction
Authors: Ruibo Wang, Song Zhang, Ping Huang, Donghai Zhang, Wei Yan
Categories: cs.CV cs.AI
DOI: 10.1109/ICUS58632.2023.10318339
\\
  Recent research that combines implicit 3D representation with semantic
information, like Semantic-NeRF, has proven that NeRF model could perform
excellently in rendering 3D structures with semantic labels. This research aims
to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing
solely on semantic output and removing the RGB output component. We reformulate
the model and its training procedure to leverage only the cross-entropy loss
between the model semantic output and the ground truth semantic images,
removing the colour data traditionally used in the original Semantic-NeRF
approach. We then conduct a series of identical experiments using the original
and the modified Semantic-NeRF model. Our primary objective is to obverse the
impact of this modification on the model performance by Semantic-NeRF, focusing
on tasks such as scene understanding, object detection, and segmentation. The
results offer valuable insights into the new way of rendering the scenes and
provide an avenue for further research and development in semantic-focused 3D
scene understanding.
\\ ( https://arxiv.org/abs/2403.16043 ,  1479kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16067 (*cross-listing*)
Date: Sun, 24 Mar 2024 08:34:08 GMT   (971kb,D)

Title: Robust Diffusion Models for Adversarial Purification
Authors: Guang Lin, Zerui Tao, Jianhai Zhang, Toshihisa Tanaka and Qibin Zhao
Categories: cs.CV cs.AI
\\
  Diffusion models (DMs) based adversarial purification (AP) has shown to be
the most powerful alternative to adversarial training (AT). However, these
methods neglect the fact that pre-trained diffusion models themselves are not
robust to adversarial attacks as well. Additionally, the diffusion process can
easily destroy semantic information and generate a high quality image but
totally different from the original input image after the reverse process,
leading to degraded standard accuracy. To overcome these issues, a natural idea
is to harness adversarial training strategy to retrain or fine-tune the
pre-trained diffusion model, which is computationally prohibitive. We propose a
novel robust reverse process with adversarial guidance, which is independent of
given pre-trained DMs and avoids retraining or fine-tuning the DMs. This robust
guidance can not only ensure to generate purified examples retaining more
semantic content but also mitigate the accuracy-robustness trade-off of DMs for
the first time, which also provides DM-based AP an efficient adaptive ability
to new attacks. Extensive experiments are conducted to demonstrate that our
method achieves the state-of-the-art results and exhibits generalization
against different attacks.
\\ ( https://arxiv.org/abs/2403.16067 ,  971kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16081 (*cross-listing*)
Date: Sun, 24 Mar 2024 10:07:46 GMT   (865kb)

Title: The Interplay of Learning, Analytics, and Artificial Intelligence in
  Education
Authors: Mutlu Cukurova
Categories: cs.CY cs.AI
Comments: 20 pages, 7 figures, this paper is based on the keynote talk given by
  the author at the ACM International Conference on Learning Analytics &
  Knowledge (LAK) 2024 in Kyoto, Japan.
  https://www.solaresearch.org/events/lak/lak24/keynotes/
\\
  This paper presents a multi dimensional view of AI's role in learning and
education, emphasizing the intricate interplay between AI, analytics, and the
learning processes. Here, I challenge the prevalent narrow conceptualization of
AI as stochastic tools, as exemplified in generative AI, and argue for the
importance of alternative conceptualisations of AI. I highlight the differences
between human intelligence and artificial information processing, the cognitive
diversity inherent in AI algorithms, and posit that AI can also serve as an
instrument for understanding human learning. Early learning sciences and AI in
Education research, which saw AI as an analogy for human intelligence, have
diverged from this perspective, prompting a need to rekindle this connection.
The paper presents three unique conceptualizations of AI in education: the
externalization of human cognition, the internalization of AI models to
influence human thought processes, and the extension of human cognition via
tightly integrated human-AI systems. Examples from current research and
practice are examined as instances of the three conceptualisations,
highlighting the potential value and limitations of each conceptualisation for
education, as well as the perils of overemphasis on externalising human
cognition as exemplified in today's hype surrounding generative AI tools. The
paper concludes with an advocacy for a broader educational approach that
includes educating people about AI and innovating educational systems to remain
relevant in an AI enabled world.
\\ ( https://arxiv.org/abs/2403.16081 ,  865kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16112 (*cross-listing*)
Date: Sun, 24 Mar 2024 12:05:23 GMT   (5543kb,D)

Title: Opportunities and challenges in the application of large artificial
  intelligence models in radiology
Authors: Liangrui Pan, Zhenyu Zhao, Ying Lu, Kewei Tang, Liyong Fu, Qingchun
  Liang and Shaoliang Peng
Categories: cs.CV cs.AI cs.LG
\\
  Influenced by ChatGPT, artificial intelligence (AI) large models have
witnessed a global upsurge in large model research and development. As people
enjoy the convenience by this AI large model, more and more large models in
subdivided fields are gradually being proposed, especially large models in
radiology imaging field. This article first introduces the development history
of large models, technical details, workflow, working principles of multimodal
large models and working principles of video generation large models. Secondly,
we summarize the latest research progress of AI large models in radiology
education, radiology report generation, applications of unimodal and multimodal
radiology. Finally, this paper also summarizes some of the challenges of large
AI models in radiology, with the aim of better promoting the rapid revolution
in the field of radiography.
\\ ( https://arxiv.org/abs/2403.16112 ,  5543kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16116 (*cross-listing*)
Date: Sun, 24 Mar 2024 12:15:28 GMT   (1750kb,D)

Title: Self-Supervised Multi-Frame Neural Scene Flow
Authors: Dongrui Liu, Daqi Liu, Xueqian Li, Sihao Lin, Hongwei xie, Bing Wang,
  Xiaojun Chang, and Lei Chu
Categories: cs.CV cs.AI cs.LG
\\
  Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown
remarkable adaptability in the context of large out-of-distribution autonomous
driving. Despite their success, the underlying reasons for their astonishing
generalization capabilities remain unclear. Our research addresses this gap by
examining the generalization capabilities of NSFP through the lens of uniform
stability, revealing that its performance is inversely proportional to the
number of input point clouds. This finding sheds light on NSFP's effectiveness
in handling large-scale point cloud scene flow estimation tasks. Motivated by
such theoretical insights, we further explore the improvement of scene flow
estimation by leveraging historical point clouds across multiple frames, which
inherently increases the number of point clouds. Consequently, we propose a
simple and effective method for multi-frame point cloud scene flow estimation,
along with a theoretical evaluation of its generalization abilities. Our
analysis confirms that the proposed method maintains a limited generalization
error, suggesting that adding multiple frames to the scene flow optimization
process does not detract from its generalizability. Extensive experimental
results on large-scale autonomous driving Waymo Open and Argoverse lidar
datasets demonstrate that the proposed method achieves state-of-the-art
performance.
\\ ( https://arxiv.org/abs/2403.16116 ,  1750kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16135 (*cross-listing*)
Date: Sun, 24 Mar 2024 13:06:05 GMT   (1264kb,D)

Title: Complementary Recommendation in E-commerce: Definition, Approaches, and
  Future Directions
Authors: Linyue Li, Zhijuan Du
Categories: cs.IR cs.AI cs.LG
Comments: 20 pages,9 figures
\\
  In recent years, complementary recommendation has received extensive
attention in the e-commerce domain. In this paper, we comprehensively summarize
and compare 34 representative studies conducted between 2009 and 2024. Firstly,
we compare the data and methods used for modeling complementary relationships
between products, including simple complementarity and more complex scenarios
such as asymmetric complementarity, the coexistence of substitution and
complementarity relationships between products, and varying degrees of
complementarity between different pairs of products. Next, we classify and
compare the models based on the research problems of complementary
recommendation, such as diversity, personalization, and cold-start.
Furthermore, we provide a comparative analysis of experimental results from
different studies conducted on the same dataset, which helps identify the
strengths and weaknesses of the research. Compared to previous surveys, this
paper provides a more updated and comprehensive summary of the research,
discusses future research directions, and contributes to the advancement of
this field.
\\ ( https://arxiv.org/abs/2403.16135 ,  1264kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16149 (*cross-listing*)
Date: Sun, 24 Mar 2024 13:43:43 GMT   (5754kb,D)

Title: A Survey on Consumer IoT Traffic: Security and Privacy
Authors: Yan Jia, Yuxin Song, Zihou Liu, Qingyin Tan, Fangming Wang, Yu Zhang,
  Zheli Liu
Categories: cs.CR cs.AI cs.LG
\\
  For the past few years, the Consumer Internet of Things (CIoT) has entered
public lives. While CIoT has improved the convenience of people's daily lives,
it has also brought new security and privacy concerns. In this survey, we try
to figure out what researchers can learn about the security and privacy of CIoT
by traffic analysis, a popular method in the security community. From the
security and privacy perspective, this survey seeks out the new characteristics
in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic
analysis, and the challenges yet to be solved. We collected 310 papers from
January 2018 to December 2023 related to CIoT traffic analysis from the
security and privacy perspective and summarized the process of CIoT traffic
analysis in which the new characteristics of CIoT are identified. Then, we
detail existing works based on five application goals: device fingerprinting,
user activity inference, malicious traffic analysis, security analysis, and
measurement. At last, we discuss the new challenges and future research
directions.
\\ ( https://arxiv.org/abs/2403.16149 ,  5754kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16178 (*cross-listing*)
Date: Sun, 24 Mar 2024 14:38:18 GMT   (2837kb,D)

Title: Mixed-Initiative Human-Robot Teaming under Suboptimality with Online
  Bayesian Adaptation
Authors: Manisha Natarajan, Chunyue Xue, Sanne van Waveren, Karen Feigh,
  Matthew Gombolay
Categories: cs.RO cs.AI
Comments: 8 pages, 4 pages for supplementary
\\
  For effective human-agent teaming, robots and other artificial intelligence
(AI) agents must infer their human partner's abilities and behavioral response
patterns and adapt accordingly. Most prior works make the unrealistic
assumption that one or more teammates can act near-optimally. In real-world
collaboration, humans and autonomous agents can be suboptimal, especially when
each only has partial domain knowledge. In this work, we develop computational
modeling and optimization techniques for enhancing the performance of
suboptimal human-agent teams, where the human and the agent have asymmetric
capabilities and act suboptimally due to incomplete environmental knowledge. We
adopt an online Bayesian approach that enables a robot to infer people's
willingness to comply with its assistance in a sequential decision-making game.
Our user studies show that user preferences and team performance indeed vary
with robot intervention styles, and our approach for mixed-initiative
collaborations enhances objective team performance ($p<.001$) and subjective
measures, such as user's trust ($p<.001$) and perceived likeability of the
robot ($p<.001$).
\\ ( https://arxiv.org/abs/2403.16178 ,  2837kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16209 (*cross-listing*)
Date: Sun, 24 Mar 2024 16:08:10 GMT   (1577kb)

Title: Image Captioning in news report scenario
Authors: Tianrui Liu, Qi Cai, Changxin Xu, Zhanxin Zhou, Jize Xiong, Yuxin
  Qiao, and Tsungwei Yang
Categories: cs.CV cs.AI
Comments: 10 pages, 4 figures
\\
  Image captioning strives to generate pertinent captions for specified images,
situating itself at the crossroads of Computer Vision (CV) and Natural Language
Processing (NLP). This endeavor is of paramount importance with far-reaching
applications in recommendation systems, news outlets, social media, and beyond.
Particularly within the realm of news reporting, captions are expected to
encompass detailed information, such as the identities of celebrities captured
in the images. However, much of the existing body of work primarily centers
around understanding scenes and actions. In this paper, we explore the realm of
image captioning specifically tailored for celebrity photographs, illustrating
its broad potential for enhancing news industry practices. This exploration
aims to augment automated news content generation, thereby facilitating a more
nuanced dissemination of information. Our endeavor shows a broader horizon,
enriching the narrative in news reporting through a more intuitive image
captioning framework.
\\ ( https://arxiv.org/abs/2403.16209 ,  1577kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16210 (*cross-listing*)
Date: Sun, 24 Mar 2024 16:09:21 GMT   (7287kb,D)

Title: Frankenstein: Generating Semantic-Compositional 3D Scenes in One
  Tri-Plane
Authors: Han Yan, Yang Li, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang
  Shang, Weizhe Liu, Tian Chen, Xiaqiang Dai, Chao Ma, Hongdong Li, Pan Ji
Categories: cs.CV cs.AI cs.GR
Comments: Video: https://youtu.be/lRn-HqyCrLI
\\
  We present Frankenstein, a diffusion-based framework that can generate
semantic-compositional 3D scenes in a single pass. Unlike existing methods that
output a single, unified 3D shape, Frankenstein simultaneously generates
multiple separated shapes, each corresponding to a semantically meaningful
part. The 3D scene information is encoded in one single tri-plane tensor, from
which multiple Singed Distance Function (SDF) fields can be decoded to
represent the compositional shapes. During training, an auto-encoder compresses
tri-planes into a latent space, and then the denoising diffusion process is
employed to approximate the distribution of the compositional scenes.
Frankenstein demonstrates promising results in generating room interiors as
well as human avatars with automatically separated parts. The generated scenes
facilitate many downstream applications, such as part-wise re-texturing, object
rearrangement in the room or avatar cloth re-targeting.
\\ ( https://arxiv.org/abs/2403.16210 ,  7287kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16218 (*cross-listing*)
Date: Sun, 24 Mar 2024 16:18:27 GMT   (871kb,D)

Title: CoverUp: Coverage-Guided LLM-Based Test Generation
Authors: Juan Altmayer Pizzorno and Emery D. Berger
Categories: cs.SE cs.AI cs.LG cs.PL
Comments: 11 pages
\\
  This paper presents CoverUp, a novel system that drives the generation of
high-coverage Python regression tests via a combination of coverage analysis
and large-language models (LLMs). CoverUp iteratively improves coverage,
interleaving coverage analysis with dialogs with the LLM to focus its attention
on as yet uncovered lines and branches. The resulting test suites significantly
improve coverage over the current state of the art: compared to CodaMosa, a
hybrid LLM / search-based software testing system, CoverUp substantially
improves coverage across the board. On a per-module basis, CoverUp achieves
median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and
line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative,
coverage-guided approach is crucial to its effectiveness, contributing to
nearly half of its successes.
\\ ( https://arxiv.org/abs/2403.16218 ,  871kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16230 (*cross-listing*)
Date: Sun, 24 Mar 2024 16:48:10 GMT   (1048kb)

Title: On machine learning analysis of atomic force microscopy images for image
  classification, sample surface recognition
Authors: Igor Sokolov
Categories: physics.bio-ph cs.AI physics.ins-det physics.med-ph
Comments: perspective; mini-review; method description; Physical Chemistry
  Chemical Physics (PCCP) in press, 2024
MSC-class: 92C05, 92C55, 92C10, 92C50
ACM-class: J.2; J.3; I.2; I.5
\\
  Atomic force microscopy (AFM or SPM) imaging is one of the best matches with
machine learning (ML) analysis among microscopy techniques. The digital format
of AFM images allows for direct utilization in ML algorithms without the need
for additional processing. Additionally, AFM enables the simultaneous imaging
of distributions of over a dozen different physicochemical properties of sample
surfaces, a process known as multidimensional imaging. While this wealth of
information can be challenging to analyze using traditional methods, ML
provides a seamless approach to this task. However, the relatively slow speed
of AFM imaging poses a challenge in applying deep learning methods broadly used
in image recognition. This Prospective is focused on ML
recognition/classification when using a relatively small number of AFM images,
small database. We discuss ML methods other than popular deep-learning neural
networks. The described approach has already been successfully used to analyze
and classify the surfaces of biological cells. It can be applied to recognize
medical images, specific material processing, in forensic studies, even to
identify the authenticity of arts. A general template for ML analysis specific
to AFM is suggested, with a specific example of the identification of cell
phenotype. Special attention is given to the analysis of the statistical
significance of the obtained results, an important feature that is often
overlooked in papers dealing with machine learning. A simple method for finding
statistical significance is also described.
\\ ( https://arxiv.org/abs/2403.16230 ,  1048kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16272 (*cross-listing*)
Date: Sun, 24 Mar 2024 19:34:33 GMT   (13193kb,D)

Title: L-MAE: Longitudinal masked auto-encoder with time and severity-aware
  encoding for diabetic retinopathy progression prediction
Authors: Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Yihao Li,
  Alireza Rezaei, Hugo Le Boit\'e, Ramin Tadayoni, Pascal Massin, B\'eatrice
  Cochener, Ikram Brahim, Gwenol\'e Quellec, Mathieu Lamard
Categories: cs.CV cs.AI
\\
  Pre-training strategies based on self-supervised learning (SSL) have proven
to be effective pretext tasks for many downstream tasks in computer vision. Due
to the significant disparity between medical and natural images, the
application of typical SSL is not straightforward in medical imaging.
Additionally, those pretext tasks often lack context, which is critical for
computer-aided clinical decision support. In this paper, we developed a
longitudinal masked auto-encoder (MAE) based on the well-known
Transformer-based MAE. In particular, we explored the importance of time-aware
position embedding as well as disease progression-aware masking. Taking into
account the time between examinations instead of just scheduling them offers
the benefit of capturing temporal changes and trends. The masking strategy, for
its part, evolves during follow-up to better capture pathological changes,
ensuring a more accurate assessment of disease progression. Using OPHDIAT, a
large follow-up screening dataset targeting diabetic retinopathy (DR), we
evaluated the pre-trained weights on a longitudinal task, which is to predict
the severity label of the next visit within 3 years based on the past time
series examinations. Our results demonstrated the relevancy of both time-aware
position embedding and masking strategies based on disease progression
knowledge. Compared to popular baseline models and standard longitudinal
Transformers, these simple yet effective extensions significantly enhance the
predictive ability of deep classification models.
\\ ( https://arxiv.org/abs/2403.16272 ,  13193kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16276 (*cross-listing*)
Date: Sun, 24 Mar 2024 19:50:49 GMT   (12696kb,D)

Title: AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary
  Alignment for Temporal Referential Dialogue
Authors: Yunlong Tang, Daiki Shimada, Jing Bi, Chenliang Xu
Categories: cs.CV cs.AI
\\
  In everyday communication, humans frequently use speech and gestures to refer
to specific areas or objects, a process known as Referential Dialogue (RD).
While prior studies have investigated RD through Large Language Models (LLMs)
or Large Multimodal Models (LMMs) in static contexts, the exploration of
Temporal Referential Dialogue (TRD) within audio-visual media remains limited.
Two primary challenges hinder progress in this field: (1) the absence of
comprehensive, untrimmed audio-visual video datasets with precise temporal
annotations, and (2) the need for methods to integrate complex temporal
auditory and visual cues effectively. To address these challenges, we introduce
a novel framework to generate PU-VALOR, an extensive audio-visual dataset
comprising over 114,000 untrimmed videos with accurate temporal demarcations.
We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI)
that ensures the temporal alignment of audio-visual information. Additionally,
we develop the A5-222K dataset, encompassing more than 200,000 audio-text
pairings, to facilitate the audio and text alignments. Our experiments
demonstrate that AVicuna can effectively handle TRD in audio-visual videos and
achieve state-of-the-art performance on various audio-visual video
understanding tasks, particularly in untrimmed videos. We further investigate
the optimal audio-interleaving rate for interleaved audio-visual inputs, which
maximizes performance on the Audio-Visual Event Dense Localization task.
\\ ( https://arxiv.org/abs/2403.16276 ,  12696kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16291 (*cross-listing*)
Date: Sun, 24 Mar 2024 20:43:29 GMT   (12055kb,D)

Title: Guessing human intentions to avoid dangerous situations in caregiving
  robots
Authors: No\'e Zapata, Gerardo P\'erez, Lucas Bonilla, Pedro N\'u\~nez, Pilar
  Bachiller and Pablo Bustos
Categories: cs.RO cs.AI
Comments: 8 pages, 6 figures. Submitted to IROS
\\
  For robots to interact socially, they must interpret human intentions and
anticipate their potential outcomes accurately. This is particularly important
for social robots designed for human care, which may face potentially dangerous
situations for people, such as unseen obstacles in their way, that should be
avoided. This paper explores the Artificial Theory of Mind (ATM) approach to
inferring and interpreting human intentions. We propose an algorithm that
detects risky situations for humans, selecting a robot action that removes the
danger in real time. We use the simulation-based approach to ATM and adopt the
'like-me' policy to assign intentions and actions to people. Using this
strategy, the robot can detect and act with a high rate of success under
time-constrained situations. The algorithm has been implemented as part of an
existing robotics cognitive architecture and tested in simulation scenarios.
Three experiments have been conducted to test the implementation's robustness,
precision and real-time response, including a simulated scenario, a
human-in-the-loop hybrid configuration and a real-world scenario.
\\ ( https://arxiv.org/abs/2403.16291 ,  12055kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16303 (*cross-listing*)
Date: Sun, 24 Mar 2024 21:29:39 GMT   (4201kb)

Title: Large Language Models in Biomedical and Health Informatics: A
  Bibliometric Review
Authors: Huizi Yu, Lizhou Fan, Lingyao Li, Jiayan Zhou, Zihui Ma, Lu Xian,
  Wenyue Hua, Sijia He, Mingyu Jin, Yongfeng Zhang, Ashvin Gandhi, Xin Ma
Categories: cs.DL cs.AI cs.CL cs.SI
Comments: 50 pages, 7 figures, 4 tables
\\
  Large Language Models (LLMs) have rapidly become important tools in
Biomedical and Health Informatics (BHI), enabling new ways to analyze data,
treat patients, and conduct research. This bibliometric review aims to provide
a panoramic view of how LLMs have been used in BHI by examining research
articles and collaboration networks from 2022 to 2023. It further explores how
LLMs can improve Natural Language Processing (NLP) applications in various BHI
areas like medical diagnosis, patient engagement, electronic health record
management, and personalized medicine. To do this, our bibliometric review
identifies key trends, maps out research networks, and highlights major
developments in this fast-moving field. Lastly, it discusses the ethical
concerns and practical challenges of using LLMs in BHI, such as data privacy
and reliable medical recommendations. Looking ahead, we consider how LLMs could
further transform biomedical research as well as healthcare delivery and
patient outcomes. This comprehensive review serves as a resource for
stakeholders in healthcare, including researchers, clinicians, and
policymakers, to understand the current state and future potential of LLMs in
BHI.
\\ ( https://arxiv.org/abs/2403.16303 ,  4201kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16327 (*cross-listing*)
Date: Sun, 24 Mar 2024 23:22:02 GMT   (4635kb,D)

Title: Artificial Neural Microcircuits as Building Blocks: Concept and
  Challenges
Authors: Andrew Walter, Shimeng Wu, Andy M. Tyrrell, Liam McDaid, Malachy
  McElholm, Nidhin Thandassery Sumithran, Jim Harkin, Martin A. Trefzer
Categories: cs.NE cs.AI cs.LG
Comments: 12 pages, 31 figures, 3 tables, submitted to A-Life Journal for
  review
\\
  Artificial Neural Networks (ANNs) are one of the most widely employed forms
of bio-inspired computation. However the current trend is for ANNs to be
structurally homogeneous. Furthermore, this structural homogeneity requires the
application of complex training and learning tools that produce application
specific ANNs, susceptible to pitfalls such as overfitting. In this paper, an
new approach is explored, inspired by the role played in biology by Neural
Microcircuits, the so called ``fundamental processing elements'' of organic
nervous systems. How large neural networks, particularly Spiking Neural
Networks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs),
intended as off-the-shelf components, is articulated; the results of initial
work to produce a catalogue of such Microcircuits though the use of Novelty
Search is shown; followed by efforts to expand upon this initial work,
including a discussion of challenges uncovered during these efforts and
explorations of methods by which they might be overcome.
\\ ( https://arxiv.org/abs/2403.16327 ,  4635kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16338 (*cross-listing*)
Date: Mon, 25 Mar 2024 00:24:10 GMT   (22646kb,D)

Title: Impact of Video Compression Artifacts on Fisheye Camera Visual
  Perception Tasks
Authors: Madhumitha Sakthi, Louis Kerofsky, Varun Ravi Kumar and Senthil
  Yogamani
Categories: cs.CV cs.AI
\\
  Autonomous driving systems require extensive data collection schemes to cover
the diverse scenarios needed for building a robust and safe system. The data
volumes are in the order of Exabytes and have to be stored for a long period of
time (i.e., more than 10 years of the vehicle's life cycle). Lossless
compression doesn't provide sufficient compression ratios, hence, lossy video
compression has been explored. It is essential to prove that lossy video
compression artifacts do not impact the performance of the perception
algorithms. However, there is limited work in this area to provide a solid
conclusion. In particular, there is no such work for fisheye cameras, which
have high radial distortion and where compression may have higher artifacts.
Fisheye cameras are commonly used in automotive systems for 3D object detection
task. In this work, we provide the first analysis of the impact of standard
video compression codecs on wide FOV fisheye camera images. We demonstrate that
the achievable compression with negligible impact depends on the dataset and
temporal prediction of the video codec. We propose a radial distortion-aware
zonal metric to evaluate the performance of artifacts in fisheye images. In
addition, we present a novel method for estimating affine mode parameters of
the latest VVC codec, and suggest some areas for improvement in video codecs
for the application to fisheye imagery.
\\ ( https://arxiv.org/abs/2403.16338 ,  22646kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16347 (*cross-listing*)
Date: Mon, 25 Mar 2024 00:50:27 GMT   (213kb,D)

Title: ChatGPT Incorrectness Detection in Software Reviews
Authors: Minaoar Hossain Tanzil, Junaed Younus Khan, Gias Uddin
Categories: cs.SE cs.AI cs.LG
Journal-ref: IEEE/ACM 46th International Conference on Software Engineering
  (ICSE 2024)
DOI: 10.1145/3597503.3639194
\\
  We conducted a survey of 135 software engineering (SE) practitioners to
understand how they use Generative AI-based chatbots like ChatGPT for SE tasks.
We find that they want to use ChatGPT for SE tasks like software library
selection but often worry about the truthfulness of ChatGPT responses. We
developed a suite of techniques and a tool called CID (ChatGPT Incorrectness
Detector) to automatically test and detect the incorrectness in ChatGPT
responses. CID is based on the iterative prompting to ChatGPT by asking it
contextually similar but textually divergent questions (using an approach that
utilizes metamorphic relationships in texts). The underlying principle in CID
is that for a given question, a response that is different from other responses
(across multiple incarnations of the question) is likely an incorrect response.
In a benchmark study of library selection, we show that CID can detect
incorrect responses from ChatGPT with an F1-score of 0.74 - 0.75.
\\ ( https://arxiv.org/abs/2403.16347 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16354 (*cross-listing*)
Date: Mon, 25 Mar 2024 01:12:57 GMT   (3073kb,D)

Title: ChatDBG: An AI-Powered Debugging Assistant
Authors: Kyla Levin and Nicolas van Kempen and Emery D. Berger and Stephen N.
  Freund
Categories: cs.SE cs.AI cs.LG cs.PL
Comments: 11 pages
\\
  This paper presents ChatDBG, the first AI-powered debugging assistant.
ChatDBG integrates large language models (LLMs) to significantly enhance the
capabilities and user-friendliness of conventional debuggers. ChatDBG lets
programmers engage in a collaborative dialogue with the debugger, allowing them
to pose complex questions about program state, perform root cause analysis for
crashes or assertion failures, and explore open-ended queries like "why is x
null?". To handle these queries, ChatDBG grants the LLM autonomy to take the
wheel and drive debugging by issuing commands to navigate through stacks and
inspect program state; it then reports its findings and yields back control to
the programmer. Our ChatDBG prototype integrates with standard debuggers
including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our
evaluation across a diverse set of code, including C/C++ code with known bugs
and a suite of Python code including standalone scripts and Jupyter notebooks,
demonstrates that ChatDBG can successfully analyze root causes, explain bugs,
and generate accurate fixes for a wide range of real-world errors. For the
Python programs, a single query led to an actionable bug fix 67% of the time;
one additional follow-up query increased the success rate to 85%. ChatDBG has
seen rapid uptake; it has already been downloaded nearly 30,000 times.
\\ ( https://arxiv.org/abs/2403.16354 ,  3073kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16386 (*cross-listing*)
Date: Mon, 25 Mar 2024 03:02:51 GMT   (2657kb,D)

Title: Dia-LLaMA: Towards Large Language Model-driven CT Report Generation
Authors: Zhixuan Chen, Luyang Luo, Yequan Bie, Hao Chen
Categories: cs.CV cs.AI
Comments: 10 pages
\\
  Medical report generation has achieved remarkable advancements yet has still
been faced with several challenges. First, the inherent imbalance in the
distribution of normal and abnormal cases may lead models to exhibit a biased
focus on normal samples, resulting in unreliable diagnoses. Second, the
frequent occurrence of common template sentences in the reports may overwhelm
the critical abnormal information. Moreover, existing works focus on 2D chest
X-rays, leaving CT report generation underexplored due to the high-dimensional
nature of CT images and the limited availability of CT-report pairs. Recently,
LLM has shown a great ability to generate reliable answers with appropriate
prompts, which shed light on addressing the aforementioned challenges. In this
paper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report
generation by incorporating diagnostic information as guidance prompts.
Considering the high dimension of CT, we leverage a pre-trained ViT3D with
perceiver to extract the visual information. To tailor the LLM for report
generation and emphasize abnormality, we extract additional diagnostic
information by referring to a disease prototype memory bank, which is updated
during training to capture common disease representations. Furthermore, we
introduce disease-aware attention to enable the model to adjust attention for
different diseases. Experiments on the chest CT dataset demonstrated that our
proposed method outperformed previous methods and achieved state-of-the-art on
both clinical efficacy performance and natural language generation metrics. The
code will be made publically available.
\\ ( https://arxiv.org/abs/2403.16386 ,  2657kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16397 (*cross-listing*)
Date: Mon, 25 Mar 2024 03:23:10 GMT   (8668kb,D)

Title: RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band
  Radiomap Reconstruction via Graph Attention Networks
Authors: Xiaojie Li, Songyang Zhang, Hang Li, Xiaoyang Li, Lexi Xu, Haigao Xu,
  Hui Mei, Guangxu Zhu, Nan Qi, and Ming Xiao
Categories: eess.SP cs.AI
Comments: submitted to IEEE journal for possible publication
\\
  Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless
communications for tasks such as spectrum management and network planning.
However, traditional machine-learning-based MB-RMR methods, which rely heavily
on simulated data or complete structured ground truth, face significant
deployment challenges. These challenges stem from the differences between
simulated and actual data, as well as the scarcity of real-world measurements.
To address these challenges, our study presents RadioGAT, a novel framework
based on Graph Attention Network (GAT) tailored for MB-RMR within a single
area, eliminating the need for multi-region datasets. RadioGAT innovatively
merges model-based spatial-spectral correlation encoding with data-driven
radiomap generalization, thus minimizing the reliance on extensive data
sources. The framework begins by transforming sparse multi-band data into a
graph structure through an innovative encoding strategy that leverages radio
propagation models to capture the spatial-spectral correlation inherent in the
data. This graph-based representation not only simplifies data handling but
also enables tailored label sampling during training, significantly enhancing
the framework's adaptability for deployment. Subsequently, The GAT is employed
to generalize the radiomap information across various frequency bands.
Extensive experiments using raytracing datasets based on real-world
environments have demonstrated RadioGAT's enhanced accuracy in supervised
learning settings and its robustness in semi-supervised scenarios. These
results underscore RadioGAT's effectiveness and practicality for MB-RMR in
environments with limited data availability.
\\ ( https://arxiv.org/abs/2403.16397 ,  8668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16422 (*cross-listing*)
Date: Mon, 25 Mar 2024 04:54:49 GMT   (8142kb,D)

Title: Refining Text-to-Image Generation: Towards Accurate Training-Free
  Glyph-Enhanced Image Generation
Authors: Sanyam Lakhanpal, Shivang Chopra, Vinija Jain, Aman Chadha, Man Luo
Categories: cs.CV cs.AI
\\
  Over the past few years, Text-to-Image (T2I) generation approaches based on
diffusion models have gained significant attention. However, vanilla diffusion
models often suffer from spelling inaccuracies in the text displayed within the
generated images. The capability to generate visual text is crucial, offering
both academic interest and a wide range of practical applications. To produce
accurate visual text images, state-of-the-art techniques adopt a
glyph-controlled image generation approach, consisting of a text layout
generator followed by an image generator that is conditioned on the generated
text layout. Nevertheless, our study reveals that these models still face three
primary challenges, prompting us to develop a testbed to facilitate future
research. We introduce a benchmark, LenCom-Eval, specifically designed for
testing models' capability in generating images with Lengthy and Complex visual
text. Subsequently, we introduce a training-free framework to enhance the
two-stage generation approaches. We examine the effectiveness of our approach
on both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable
improvements across a range of evaluation metrics, including CLIPScore, OCR
precision, recall, F1 score, accuracy, and edit distance scores. For instance,
our proposed framework improves the backbone model, TextDiffuser, by more than
23\% and 13.5\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval,
respectively. Our work makes a unique contribution to the field by focusing on
generating images with long and rare text sequences, a niche previously
unexplored by existing literature
\\ ( https://arxiv.org/abs/2403.16422 ,  8142kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16431 (*cross-listing*)
Date: Mon, 25 Mar 2024 05:22:34 GMT   (1851kb,D)

Title: DOCTR: Disentangled Object-Centric Transformer for Point Scene
  Understanding
Authors: Xiaoxuan Yu, Hao Wang, Weiming Li, Qiang Wang, Soonyong Cho, Younghun
  Sung
Categories: cs.CV cs.AI
\\
  Point scene understanding is a challenging task to process real-world scene
point cloud, which aims at segmenting each object, estimating its pose, and
reconstructing its mesh simultaneously. Recent state-of-the-art method first
segments each object and then processes them independently with multiple stages
for the different sub-tasks. This leads to a complex pipeline to optimize and
makes it hard to leverage the relationship constraints between multiple
objects. In this work, we propose a novel Disentangled Object-Centric
TRansformer (DOCTR) that explores object-centric representation to facilitate
learning with multiple objects for the multiple sub-tasks in a unified manner.
Each object is represented as a query, and a Transformer decoder is adapted to
iteratively optimize all the queries involving their relationship. In
particular, we introduce a semantic-geometry disentangled query (SGDQ) design
that enables the query features to attend separately to semantic information
and geometric information relevant to the corresponding sub-tasks. A hybrid
bipartite matching module is employed to well use the supervisions from all the
sub-tasks during training. Qualitative and quantitative experimental results
demonstrate that our method achieves state-of-the-art performance on the
challenging ScanNet dataset. Code is available at
https://github.com/SAITPublic/DOCTR.
\\ ( https://arxiv.org/abs/2403.16431 ,  1851kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16523 (*cross-listing*)
Date: Mon, 25 Mar 2024 08:06:08 GMT   (2621kb,D)

Title: Causal Discovery from Poisson Branching Structural Causal Model Using
  High-Order Cumulant with Path Analysis
Authors: Jie Qiao, Yu Xiang, Zhengming Chen, Ruichu Cai, Zhifeng Hao
Categories: stat.ML cs.AI cs.LG
Comments: Accepted by AAAI-2024
\\
  Count data naturally arise in many fields, such as finance, neuroscience, and
epidemiology, and discovering causal structure among count data is a crucial
task in various scientific and industrial scenarios. One of the most common
characteristics of count data is the inherent branching structure described by
a binomial thinning operator and an independent Poisson distribution that
captures both branching and noise. For instance, in a population count
scenario, mortality and immigration contribute to the count, where survival
follows a Bernoulli distribution, and immigration follows a Poisson
distribution. However, causal discovery from such data is challenging due to
the non-identifiability issue: a single causal pair is Markov equivalent, i.e.,
$X\rightarrow Y$ and $Y\rightarrow X$ are distributed equivalent. Fortunately,
in this work, we found that the causal order from $X$ to its child $Y$ is
identifiable if $X$ is a root vertex and has at least two directed paths to
$Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed
path to $Y$ without passing $X$. Specifically, we propose a Poisson Branching
Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using
high-order cumulants. Theoretical results establish the connection between the
path and cumulant and demonstrate that the path information can be obtained
from the cumulant. With the path information, causal order is identifiable
under some graphical conditions. A practical algorithm for learning causal
structure under PB-SCM is proposed and the experiments demonstrate and verify
the effectiveness of the proposed method.
\\ ( https://arxiv.org/abs/2403.16523 ,  2621kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16530 (*cross-listing*)
Date: Mon, 25 Mar 2024 08:16:06 GMT   (16635kb,D)

Title: An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in
  Diffusion Models
Authors: Zizhao Hu, Shaochong Jia, Mohammad Rostami
Categories: cs.CV cs.AI
\\
  Diffusion models have been widely used for conditional data cross-modal
generation tasks such as text-to-image and text-to-video. However,
state-of-the-art models still fail to align the generated visual concepts with
high-level semantics in a language such as object count, spatial relationship,
etc. We approach this problem from a multimodal data fusion perspective and
investigate how different fusion strategies can affect vision-language
alignment. We discover that compared to the widely used early fusion of
conditioning text in a pretrained image feature space, a specially designed
intermediate fusion can: (i) boost text-to-image alignment with improved
generation quality and (ii) improve training and inference efficiency by
reducing low-rank text-to-image attention calculations. We perform experiments
using a text-to-image generation task on the MS-COCO dataset. We compare our
intermediate fusion mechanism with the classic early fusion mechanism on two
common conditioning methods on a U-shaped ViT backbone. Our intermediate fusion
model achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and
50% increased training speed compared to a strong U-ViT baseline with an early
fusion.
\\ ( https://arxiv.org/abs/2403.16530 ,  16635kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16552 (*cross-listing*)
Date: Mon, 25 Mar 2024 08:57:27 GMT   (1120kb,D)

Title: QKFormer: Hierarchical Spiking Transformer using Q-K Attention
Authors: Chenlin Zhou, Han Zhang, Zhaokun Zhou, Liutao Yu, Liwei Huang,
  Xiaopeng Fan, Li Yuan, Zhengyu Ma, Huihui Zhou, Yonghong Tian
Categories: cs.NE cs.AI cs.CV
Comments: 10 pages, code: https://github.com/zhouchenlin2096/QKFormer
\\
  Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with
Transformer architectures, have attracted significant attention due to their
potential for energy efficiency and high performance. However, existing models
in this domain still suffer from suboptimal performance. We introduce several
innovations to improve the performance: i) We propose a novel spike-form Q-K
attention mechanism, tailored for SNNs, which efficiently models the importance
of token or channel dimensions through binary vectors with linear complexity.
ii) We incorporate the hierarchical structure, which significantly benefits the
performance of both the brain and artificial neural networks, into spiking
transformers to obtain multi-scale spiking representation. iii) We design a
versatile and powerful patch embedding module with a deformed shortcut
specifically for spiking transformers. Together, we develop QKFormer, a
hierarchical spiking transformer based on Q-K attention with direct training.
QKFormer shows significantly superior performance over existing
state-of-the-art SNN models on various mainstream datasets. Notably, with
comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a
groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially
outperforming Spikformer by 10.84%. To our best knowledge, this is the first
time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The
code and models are publicly available at
https://github.com/zhouchenlin2096/QKFormer
\\ ( https://arxiv.org/abs/2403.16552 ,  1120kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16578 (*cross-listing*)
Date: Mon, 25 Mar 2024 09:43:56 GMT   (3411kb,D)

Title: SegICL: A Universal In-context Learning Framework for Enhanced
  Segmentation in Medical Imaging
Authors: Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shining
  Xiang
Categories: cs.CV cs.AI
\\
  Medical image segmentation models adapting to new tasks in a training-free
manner through in-context learning is an exciting advancement. Universal
segmentation models aim to generalize across the diverse modality of medical
images, yet their effectiveness often diminishes when applied to
out-of-distribution (OOD) data modalities and tasks, requiring intricate
fine-tuning of model for optimal performance. For addressing this challenge, we
introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for
image segmentation. Unlike existing methods, SegICL has the capability to
employ text-guided segmentation and conduct in-context learning with a small
set of image-mask pairs, eliminating the need for training the model from
scratch or fine-tuning for OOD tasks (including OOD modality and dataset).
Extensive experimental validation of SegICL demonstrates a positive correlation
between the number of prompt samples and segmentation performance on OOD
modalities and tasks. This indicates that SegICL effectively address new
segmentation tasks based on contextual information. Additionally, SegICL also
exhibits comparable segmentation performance to mainstream models on OOD and
in-distribution tasks. Our code will be released soon.
\\ ( https://arxiv.org/abs/2403.16578 ,  3411kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16666 (*cross-listing*)
Date: Mon, 25 Mar 2024 12:01:27 GMT   (52kb,D)

Title: Revisiting the Sleeping Beauty problem
Authors: Paulo S. Piva and Gabriel Ruffolo
Categories: math.HO cs.AI
Comments: 14 pages, 1 figure
\\
  The Sleeping Beauty problem is a probability riddle with no definite solution
for more than two decades and its solution is of great interest in many fields
of knowledge. There are two main competing solutions to the problem: the halfer
approach, and the thirder approach. The main reason for disagreement in the
literature is connected to the use of different probability spaces to represent
the same probabilistic riddle. In this work, we analyse the problem from a
mathematical perspective, identifying probability distributions induced
directly from the thought experiment's rules. The precise choices of
probability spaces provide both halfer and thirder solutions to the problem. To
try and decide on which approach to follow, a criterion involving the
information available to Sleeping Beauty is proposed.
\\ ( https://arxiv.org/abs/2403.16666 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16674 (*cross-listing*)
Date: Mon, 25 Mar 2024 12:13:20 GMT   (10509kb,D)

Title: Understanding the Functional Roles of Modelling Components in Spiking
  Neural Networks
Authors: Huifeng Yin, Hanle Zheng, Jiayi Mao, Siyuan Ding, Xing Liu, Mingkun
  Xu, Yifan Hu, Jing Pei, Lei Deng
Categories: cs.NE cs.AI cs.LG
\\
  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,
are promising in achieving high computational efficiency with biological
fidelity. Nevertheless, it is quite difficult to optimize SNNs because the
functional roles of their modelling components remain unclear. By designing and
evaluating several variants of the classic model, we systematically investigate
the functional roles of key modelling components, leakage, reset, and
recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive
experiments, we demonstrate how these components influence the accuracy,
generalization, and robustness of SNNs. Specifically, we find that the leakage
plays a crucial role in balancing memory retention and robustness, the reset
mechanism is essential for uninterrupted temporal processing and computational
efficiency, and the recurrence enriches the capability to model complex
dynamics at a cost of robustness degradation. With these interesting
observations, we provide optimization suggestions for enhancing the performance
of SNNs in different scenarios. This work deepens the understanding of how SNNs
work, which offers valuable guidance for the development of more effective and
robust neuromorphic models.
\\ ( https://arxiv.org/abs/2403.16674 ,  10509kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16687 (*cross-listing*)
Date: Mon, 25 Mar 2024 12:23:12 GMT   (454kb)

Title: Investigation of the effectiveness of applying ChatGPT in Dialogic
  Teaching Using Electroencephalography
Authors: Jiayue Zhang, Yiheng Liu, Wenqi Cai, Yali Peng, Senqing Qi, Taotao
  Long, Bao Ge
Categories: cs.CY cs.AI physics.ed-ph
\\
  In recent years, the rapid development of artificial intelligence technology,
especially the emergence of large language models (LLMs) such as ChatGPT, has
presented significant prospects for application in the field of education. LLMs
possess the capability to interpret knowledge, answer questions, and consider
context, thus providing support for dialogic teaching to students. Therefore,
an examination of the capacity of LLMs to effectively fulfill instructional
roles, thereby facilitating student learning akin to human educators within
dialogic teaching scenarios, is an exceptionally valuable research topic. This
research recruited 34 undergraduate students as participants, who were randomly
divided into two groups. The experimental group engaged in dialogic teaching
using ChatGPT, while the control group interacted with human teachers. Both
groups learned the histogram equalization unit in the information-related
course "Digital Image Processing". The research findings show comparable scores
between the two groups on the retention test. However, students who engaged in
dialogue with ChatGPT exhibited lower performance on the transfer test.
Electroencephalography data revealed that students who interacted with ChatGPT
exhibited higher levels of cognitive activity, suggesting that ChatGPT could
help students establish a knowledge foundation and stimulate cognitive
activity. However, its strengths on promoting students. knowledge application
and creativity were insignificant. Based upon the research findings, it is
evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the
dialogue teaching in information related courses. Combining ChatGPT with
traditional human teachers might be a more ideal approach. The synergistic use
of both can provide students with more comprehensive learning support, thus
contributing to enhancing the quality of teaching.
\\ ( https://arxiv.org/abs/2403.16687 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16719 (*cross-listing*)
Date: Mon, 25 Mar 2024 12:56:48 GMT   (105kb)

Title: Towards a Formalisation of Value-based Actions and Consequentialist
  Ethics
Authors: Adam Wyner, Tomasz Zurek, DOrota Stachura-Zurek
Categories: cs.MA cs.AI
\\
  Agents act to bring about a state of the world that is more compatible with
their personal or institutional values. To formalise this intuition, the paper
proposes an action framework based on the STRIPS formalisation. Technically,
the contribution expresses actions in terms of Value-based Formal Reasoning
(VFR), which provides a set of propositions derived from an Agent's value
profile and the Agent's assessment of propositions with respect to the profile.
Conceptually, the contribution provides a computational framework for a form of
consequentialist ethics which is satisficing, luralistic, act-based, and
preferential.
\\ ( https://arxiv.org/abs/2403.16719 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16757 (*cross-listing*)
Date: Mon, 25 Mar 2024 13:36:20 GMT   (58kb)

Title: Bi-objective Optimization in Role Mining
Authors: Jason Crampton and Eduard Eiben and Gregory Gutin and Daniel
  Karapetyan and Diptapriyo Majumdar
Categories: cs.CR cs.AI cs.CC
\\
  Role mining is a technique used to derive a role-based authorization policy
from an existing policy. Given a set of users $U$, a set of permissions $P$ and
a user-permission authorization relation $\mahtit{UPA}\subseteq U\times P$, a
role mining algorithm seeks to compute a set of roles $R$, a user-role
authorization relation $\mathit{UA}\subseteq U\times R$ and a permission-role
authorization relation $\mathit{PA}\subseteq R\times P$, such that the
composition of $\mathit{UA}$ and $\mathit{PA}$ is close (in some appropriate
sense) to $\mathit{UPA}$.
  In this paper, we first introduce the Generalized Noise Role Mining problem
(GNRM) -- a generalization of the MinNoise Role Mining problem -- which we
believe has considerable practical relevance. Extending work of Fomin et al.,
we show that GNRM is fixed parameter tractable, with parameter $r + k$, where
$r$ is the number of roles in the solution and $k$ is the number of
discrepancies between $\mathit{UPA}$ and the relation defined by the
composition of $\mathit{UA}$ and $\mathit{PA}$. We further introduce a
bi-objective optimization variant of GNRM, where we wish to minimize both $r$
and $k$ subject to upper bounds $r\le \bar{r}$ and $k\le \bar{k}$, where
$\bar{r}$ and $\bar{k}$ are constants. We show that the Pareto front of this
bi-objective optimization problem (BO-GNRM) can be computed in fixed-parameter
tractable time with parameter $\bar{r}+\bar{k}$.
  We then report the results of our experimental work using the integer
programming solver Gurobi to solve instances of BO-GNRM. Our key findings are
that (a) we obtained strong support that Gurobi's performance is
fixed-parameter tractable, (b) our results suggest that our techniques may be
useful for role mining in practice, based on our experiments in the context of
three well-known real-world authorization policies.
\\ ( https://arxiv.org/abs/2403.16757 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16760 (*cross-listing*)
Date: Mon, 25 Mar 2024 13:39:33 GMT   (359kb)

Title: As Good As A Coin Toss Human detection of AI-generated images, videos,
  audio, and audiovisual stimuli
Authors: Di Cooke, Abigail Edwards, Sophia Barkoff, and Kathryn Kelly
Categories: cs.HC cs.AI cs.SD
Comments: For study pre-registration, see https://osf.io/fnhr3
MSC-class: 68T01
ACM-class: I.2
\\
  As synthetic media becomes progressively more realistic and barriers to using
it continue to lower, the technology has been increasingly utilized for
malicious purposes, from financial fraud to nonconsensual pornography. Today,
the principal defense against being misled by synthetic media relies on the
ability of the human observer to visually and auditorily discern between real
and fake. However, it remains unclear just how vulnerable people actually are
to deceptive synthetic media in the course of their day to day lives. We
conducted a perceptual study with 1276 participants to assess how accurate
people were at distinguishing synthetic images, audio only, video only, and
audiovisual stimuli from authentic. To reflect the circumstances under which
people would likely encounter synthetic media in the wild, testing conditions
and stimuli emulated a typical online platform, while all synthetic media used
in the survey was sourced from publicly accessible generative AI technology.
  We find that overall, participants struggled to meaningfully discern between
synthetic and authentic content. We also find that detection performance
worsens when the stimuli contains synthetic content as compared to authentic
content, images featuring human faces as compared to non face objects, a single
modality as compared to multimodal stimuli, mixed authenticity as compared to
being fully synthetic for audiovisual stimuli, and features foreign languages
as compared to languages the observer is fluent in. Finally, we also find that
prior knowledge of synthetic media does not meaningfully impact their detection
performance. Collectively, these results indicate that people are highly
susceptible to being tricked by synthetic media in their daily lives and that
human perceptual detection capabilities can no longer be relied upon as an
effective counterdefense.
\\ ( https://arxiv.org/abs/2403.16760 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16809 (*cross-listing*)
Date: Mon, 25 Mar 2024 14:32:28 GMT   (2832kb,D)

Title: An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems
Authors: Hanqing Yang, Marie Siew, Carlee Joe-Wong
Categories: eess.SY cs.AI cs.LG cs.SY
Comments: Accepted at International Workshop on Foundation Models for
  Cyber-Physical Systems & Internet of Things (FMSys) 2024, Co-located at
  CPS-IoT Week 2024
\\
  The increasing prevalence of Cyber-Physical Systems and the Internet of
Things (CPS-IoT) applications and Foundation Models are enabling new
applications that leverage real-time control of the environment. For example,
real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems
can reduce its usage when not needed for the comfort of human occupants, hence
reducing energy consumption. Collecting real-time feedback on human preferences
in such human-in-the-loop (HITL) systems, however, is difficult in practice. We
propose the use of large language models (LLMs) to deal with the challenges of
dynamic environments and difficult-to-obtain data in CPS optimization. In this
paper, we present a case study that employs LLM agents to mimic the behaviors
and thermal preferences of various population groups (e.g. young families, the
elderly) in a shopping mall. The aggregated thermal preferences are integrated
into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which
employs the LLM as a dynamic simulation of the physical environment to learn
how to balance between energy savings and occupant comfort. Our results show
that LLMs are capable of simulating complex population movements within large
open spaces. Besides, AitL-RL demonstrates superior performance compared to the
popular existing policy of set point control, suggesting that adaptive and
personalized decision-making is critical for efficient optimization in CPS-IoT
applications. Through this case study, we demonstrate the potential of
integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system
adaptability and efficiency. The project's code can be found on our GitHub
repository.
\\ ( https://arxiv.org/abs/2403.16809 ,  2832kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16812 (*cross-listing*)
Date: Mon, 25 Mar 2024 14:34:06 GMT   (938kb,D)

Title: Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered
  Deliberative AI for AI-Assisted Decision-Making
Authors: Shuai Ma, Qiaoyi Chen, Xinru Wang, Chengbo Zheng, Zhenhui Peng, Ming
  Yin, Xiaojuan Ma
Categories: cs.HC cs.AI
\\
  In AI-assisted decision-making, humans often passively review AI's suggestion
and decide whether to accept or reject it as a whole. In such a paradigm,
humans are found to rarely trigger analytical thinking and face difficulties in
communicating the nuances of conflicting opinions to the AI when disagreements
occur. To tackle this challenge, we propose Human-AI Deliberation, a novel
framework to promote human reflection and discussion on conflicting human-AI
opinions in decision-making. Based on theories in human deliberation, this
framework engages humans and AI in dimension-level opinion elicitation,
deliberative discussion, and decision updates. To empower AI with deliberative
capabilities, we designed Deliberative AI, which leverages large language
models (LLMs) as a bridge between humans and domain-specific models to enable
flexible conversational interactions and faithful information provision. An
exploratory evaluation on a graduate admissions task shows that Deliberative AI
outperforms conventional explainable AI (XAI) assistants in improving humans'
appropriate reliance and task performance. Based on a mixed-methods analysis of
participant behavior, perception, user experience, and open-ended feedback, we
draw implications for future AI-assisted decision tool design.
\\ ( https://arxiv.org/abs/2403.16812 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16831 (*cross-listing*)
Date: Mon, 25 Mar 2024 14:57:18 GMT   (5761kb,D)

Title: UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation
  Model for Urban Indicator Prediction
Authors: Xixuan Hao, Wei Chen, Yibo Yan, Siru Zhong, Kun Wang, Qingsong Wen,
  Yuxuan Liang
Categories: cs.CV cs.AI
\\
  Urban indicator prediction aims to infer socio-economic metrics in diverse
urban landscapes using data-driven methods. However, prevalent pre-trained
models, particularly those reliant on satellite imagery, face dual challenges.
Firstly, concentrating solely on macro-level patterns from satellite data may
introduce bias, lacking nuanced details at micro levels, such as architectural
details at a place. Secondly, the lack of interpretability in pre-trained
models limits their utility in providing transparent evidence for urban
planning. In response to these issues, we devise a novel Vision-Language
Pre-Trained Model (UrbanVLP) in this paper. Our UrbanVLP seamlessly integrates
multi-granularity information from both macro (satellite) and micro
(street-view) levels, overcoming the limitations of prior pre-trained models.
Moreover, it introduces automatic text generation and calibration, elevating
interpretability in downstream applications by producing high-quality text
descriptions of urban imagery. Rigorous experiments conducted across six
socio-economic tasks underscore UrbanVLP's superior performance. We also deploy
a web platform to verify its practicality.
\\ ( https://arxiv.org/abs/2403.16831 ,  5761kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16851 (*cross-listing*)
Date: Mon, 25 Mar 2024 15:15:09 GMT   (655kb)

Title: Can ChatGPT predict article retraction based on Twitter mentions?
Authors: Er-Te Zheng, Hui-Zhen Fu, Zhichao Fang
Categories: cs.DL cs.AI cs.CL cs.LG
\\
  Detecting problematic research articles timely is a vital task. This study
explores whether Twitter mentions of retracted articles can signal potential
problems with the articles prior to retraction, thereby playing a role in
predicting future retraction of problematic articles. A dataset comprising
3,505 retracted articles and their associated Twitter mentions is analyzed,
alongside 3,505 non-retracted articles with similar characteristics obtained
using the Coarsened Exact Matching method. The effectiveness of Twitter
mentions in predicting article retraction is evaluated by four prediction
methods, including manual labelling, keyword identification, machine learning
models, and ChatGPT. Manual labelling results indicate that there are indeed
retracted articles with their Twitter mentions containing recognizable evidence
signaling problems before retraction, although they represent only a limited
share of all retracted articles with Twitter mention data (approximately 16%).
Using the manual labelling results as the baseline, ChatGPT demonstrates
superior performance compared to other methods, implying its potential in
assisting human judgment for predicting article retraction. This study uncovers
both the potential and limitation of social media events as an early warning
system for article retraction, shedding light on a potential application of
generative artificial intelligence in promoting research integrity.
\\ ( https://arxiv.org/abs/2403.16851 ,  655kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16863 (*cross-listing*)
Date: Mon, 25 Mar 2024 15:26:50 GMT   (1148kb,D)

Title: SIP: Autotuning GPU Native Schedules via Stochastic Instruction
  Perturbation
Authors: Guoliang He and Eiko Yoneki
Categories: cs.AR cs.AI
Comments: EuroMLSys 24, April 22, 2024, Athens, Greece
\\
  Large language models (LLMs) have become a significant workload since their
appearance. However, they are also computationally expensive as they have
billions of parameters and are trained with massive amounts of data. Thus,
recent works have developed dedicated CUDA kernels for LLM training and
inference instead of relying on compilergenerated ones, so that hardware
resources are as fully utilized as possible. In this work, we explore the
possibility of GPU native instruction optimization to further push the CUDA
kernels to extreme performance. Contrary to prior works, we adopt an automatic
optimization approach by defining a search space of possible GPU native
instruction schedules, and then we apply stochastic search to perform
optimization. Experiments show that SIP can further improve CUDA kernel
throughput by automatically discovering better GPU native instruction schedules
and the optimized schedules are tested by 10 million test samples.
\\ ( https://arxiv.org/abs/2403.16863 ,  1148kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16877 (*cross-listing*)
Date: Mon, 25 Mar 2024 15:42:09 GMT   (5761kb,D)

Title: Proprioception Is All You Need: Terrain Classification for Boreal
  Forests
Authors: Damien LaRocque, William Guimont-Martin, David-Alexandre Duclos,
  Philippe Gigu\`ere, Fran\c{c}ois Pomerleau
Categories: cs.RO cs.AI cs.LG
Comments: Submitted to the 2024 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2024)
\\
  Recent works in field robotics highlighted the importance of resiliency
against different types of terrains. Boreal forests, in particular, are home to
many mobility-impeding terrains that should be considered for off-road
autonomous navigation. Also, being one of the largest land biomes on Earth,
boreal forests are an area where autonomous vehicles are expected to become
increasingly common. In this paper, we address this issue by introducing
BorealTC, a publicly available dataset for proprioceptive-based terrain
classification (TC). Recorded with a Husky A200, our dataset contains 116 min
of Inertial Measurement Unit (IMU), motor current, and wheel odometry data,
focusing on typical boreal forest terrains, notably snow, ice, and silty loam.
Combining our dataset with another dataset from the state-of-the-art, we
evaluate both a Convolutional Neural Network (CNN) and the novel state space
model (SSM)-based Mamba architecture on a TC task. Interestingly, we show that
while CNN outperforms Mamba on each separate dataset, Mamba achieves greater
accuracy when trained on a combination of both. In addition, we demonstrate
that Mamba's learning capacity is greater than a CNN for increasing amounts of
data. We show that the combination of two TC datasets yields a latent space
that can be interpreted with the properties of the terrains. We also discuss
the implications of merging datasets on classification. Our source code and
dataset are publicly available online:
https://github.com/norlab-ulaval/BorealTC.
\\ ( https://arxiv.org/abs/2403.16877 ,  5761kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16895 (*cross-listing*)
Date: Mon, 25 Mar 2024 16:06:31 GMT   (453kb)

Title: "It is there, and you need it, so why do you not use it?" Achieving
  better adoption of AI systems by domain experts, in the case study of natural
  science research
Authors: Auste Simkute, Ewa Luger, Michael Evans, Rhianne Jones
Categories: cs.HC cs.AI
\\
  Artificial Intelligence (AI) is becoming ubiquitous in domains such as
medicine and natural science research. However, when AI systems are implemented
in practice, domain experts often refuse them. Low acceptance hinders effective
human-AI collaboration, even when it is essential for progress. In natural
science research, scientists' ineffective use of AI-enabled systems can impede
them from analysing their data and advancing their research. We conducted an
ethnographically informed study of 10 in-depth interviews with AI practitioners
and natural scientists at the organisation facing low adoption of algorithmic
systems. Results were consolidated into recommendations for better AI adoption:
i) actively supporting experts during the initial stages of system use, ii)
communicating the capabilities of a system in a user-relevant way, and iii)
following predefined collaboration rules. We discuss the broader implications
of our findings and expand on how our proposed requirements could support
practitioners and experts across domains.
\\ ( https://arxiv.org/abs/2403.16895 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16903 (*cross-listing*)
Date: Mon, 25 Mar 2024 16:14:22 GMT   (32kb,D)

Title: Towards Secure and Trusted-by-Design Smart Contracts
Authors: Zaynah Dargaye, \"Onder G\"urcan, Florent Kirchner, Sara
  Tucci-Piergiovanni
Categories: cs.CR cs.AI cs.DC
Comments: 17 pages, 1 algorithm, The 29th Francophone Days of Application
  Languages - JFLA 2018
\\
  Distributed immutable ledgers, or blockchains, allow the secure digitization
of evidential transactions without relying on a trusted third-party. Evidential
transactions involve the exchange of any form of physical evidence, such as
money, birth certificate, visas, tickets, etc. Most of the time, evidential
transactions occur in the context of complex procedures, called evidential
protocols, among physical agents. The blockchain provides the mechanisms to
transfer evidence, while smart contracts - programs executing within the
blockchain in a decentralized and replicated fashion - allow encoding
evidential protocols on top of a blockchain.
  As a smart contract foregoes trusted third-parties and runs on several
machines anonymously, it constitutes a highly critical program that has to be
secure and trusted-by-design. While most of the current smart contract
languages focus on easy programmability, they do not directly address the need
of guaranteeing trust and accountability, which becomes a significant issue
when evidential protocols are encoded as smart contracts.
\\ ( https://arxiv.org/abs/2403.16903 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16915 (*cross-listing*)
Date: Mon, 25 Mar 2024 16:32:50 GMT   (95kb,D)

Title: Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language
  Models
Authors: Atsushi Keyaki and Ribeka Keyaki
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: Accepted at LREC-COLING 2024
\\
  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
\\ ( https://arxiv.org/abs/2403.16915 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16933 (*cross-listing*)
Date: Mon, 25 Mar 2024 16:57:02 GMT   (2269kb,D)

Title: Backpropagation through space, time, and the brain
Authors: Benjamin Ellenberger, Paul Haider, Jakob Jordan, Kevin Max, Ismael
  Jaras, Laura Kriener, Federico Benitez, Mihai A. Petrovici
Categories: q-bio.NC cs.AI cs.LG cs.NE eess.SP
Comments: 15 pages, 7 figures
\\
  Effective learning in neuronal networks requires the adaptation of individual
synapses given their relative contribution to solving a task. However, physical
neuronal systems -- whether biological or artificial -- are constrained by
spatio-temporal locality. How such networks can perform efficient credit
assignment, remains, to a large extent, an open question. In Machine Learning,
the answer is almost universally given by the error backpropagation algorithm,
through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely
on biologically implausible assumptions, in particular with respect to
spatiotemporal (non-)locality, while forward-propagation models such as
real-time recurrent learning (RTRL) suffer from prohibitive memory constraints.
We introduce Generalized Latent Equilibrium (GLE), a computational framework
for fully local spatio-temporal credit assignment in physical, dynamical
networks of neurons. We start by defining an energy based on neuron-local
mismatches, from which we derive both neuronal dynamics via stationarity and
parameter dynamics via gradient descent. The resulting dynamics can be
interpreted as a real-time, biologically plausible approximation of BPTT in
deep cortical networks with continuous-time neuronal dynamics and continuously
active, local synaptic plasticity. In particular, GLE exploits the ability of
biological neurons to phase-shift their output rate with respect to their
membrane potential, which is essential in both directions of information
propagation. For the forward computation, it enables the mapping of
time-continuous inputs to neuronal space, performing an effective
spatiotemporal convolution. For the backward computation, it permits the
temporal inversion of feedback signals, which consequently approximate the
adjoint states necessary for useful parameter updates.
\\ ( https://arxiv.org/abs/2403.16933 ,  2269kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16971 (*cross-listing*)
Date: Mon, 25 Mar 2024 17:32:23 GMT   (394kb,D)

Title: LLM Agent Operating System
Authors: Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng
  Zhang
Categories: cs.OS cs.AI cs.CL
Comments: 14 pages, 5 figures, 5 tables; comments and suggestions are
  appreciated
\\
  The integration and deployment of large language model (LLM)-based
intelligent agents have been fraught with challenges that compromise their
efficiency and efficacy. Among these issues are sub-optimal scheduling and
resource allocation of agent requests over the LLM, the difficulties in
maintaining context during interactions between agent and LLM, and the
complexities inherent in integrating heterogeneous agents with different
capabilities and specializations. The rapid increase of agent quantity and
complexity further exacerbates these issues, often leading to bottlenecks and
sub-optimal utilization of resources. Inspired by these challenges, this paper
presents AIOS, an LLM agent operating system, which embeds large language model
into operating systems (OS). Specifically, AIOS is designed to optimize
resource allocation, facilitate context switch across agents, enable concurrent
execution of agents, provide tool service for agents, and maintain access
control for agents. We present the architecture of such an operating system,
outline the core challenges it aims to resolve, and provide the basic design
and implementation of the AIOS. Our experiments on concurrent execution of
multiple agents demonstrate the reliability and efficiency of our AIOS modules.
Through this, we aim to not only improve the performance and efficiency of LLM
agents but also to pioneer for better development and deployment of the AIOS
ecosystem in the future. The project is open-source at
https://github.com/agiresearch/AIOS.
\\ ( https://arxiv.org/abs/2403.16971 ,  394kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16973 (*cross-listing*)
Date: Mon, 25 Mar 2024 17:38:32 GMT   (9071kb,D)

Title: VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild
Authors: Puyuan Peng, Po-Yao Huang, Daniel Li, Abdelrahman Mohamed, David
  Harwath
Categories: eess.AS cs.AI cs.CL cs.LG cs.SD
Comments: Data, code, and model weights are available at
  https://github.com/jasonppy/VoiceCraft
\\
  We introduce VoiceCraft, a token infilling neural codec language model, that
achieves state-of-the-art performance on both speech editing and zero-shot
text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft
employs a Transformer decoder architecture and introduces a token rearrangement
procedure that combines causal masking and delayed stacking to enable
generation within an existing sequence. On speech editing tasks, VoiceCraft
produces edited speech that is nearly indistinguishable from unedited
recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS,
our model outperforms prior SotA models including VALLE and the popular
commercial model XTTS-v2. Crucially, the models are evaluated on challenging
and realistic datasets, that consist of diverse accents, speaking styles,
recording conditions, and background noise and music, and our model performs
consistently well compared to other models and real recordings. In particular,
for speech editing evaluation, we introduce a high quality, challenging, and
realistic dataset named RealEdit. We encourage readers to listen to the demos
at https://jasonppy.github.io/VoiceCraft_web.
\\ ( https://arxiv.org/abs/2403.16973 ,  9071kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16990 (*cross-listing*)
Date: Mon, 25 Mar 2024 17:52:07 GMT   (36601kb,D)

Title: Be Yourself: Bounded Attention for Multi-Subject Text-to-Image
  Generation
Authors: Omer Dahary, Or Patashnik, Kfir Aberman, Daniel Cohen-Or
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: Project page: https://omer11a.github.io/bounded-attention/
\\
  Text-to-image diffusion models have an unprecedented ability to generate
diverse and high-quality images. However, they often struggle to faithfully
capture the intended semantics of complex input prompts that include multiple
subjects. Recently, numerous layout-to-image extensions have been introduced to
improve user control, aiming to localize subjects represented by specific
tokens. Yet, these methods often produce semantically inaccurate images,
especially when dealing with multiple semantically or visually similar
subjects. In this work, we study and analyze the causes of these limitations.
Our exploration reveals that the primary issue stems from inadvertent semantic
leakage between subjects in the denoising process. This leakage is attributed
to the diffusion model's attention layers, which tend to blend the visual
features of different subjects. To address these issues, we introduce Bounded
Attention, a training-free method for bounding the information flow in the
sampling process. Bounded Attention prevents detrimental leakage among subjects
and enables guiding the generation to promote each subject's individuality,
even with complex multi-subject conditioning. Through extensive
experimentation, we demonstrate that our method empowers the generation of
multiple subjects that better align with given prompts and layouts.
\\ ( https://arxiv.org/abs/2403.16990 ,  36601kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15399 (*cross-listing*)
Date: Sun, 18 Feb 2024 07:35:01 GMT   (6049kb,D)

Title: ChatGPT in Linear Algebra: Strides Forward, Steps to Go
Authors: Eli Bagno, Thierry Dana-Picard and Shulamit Reches
Categories: cs.CY cs.CL cs.LG
Comments: arXiv admin note: text overlap with arXiv:2306.16282
\\
  As soon as a new technology emerges, the education community explores its
affordances and the possibilities to apply it in education. In this paper, we
analyze sessions with ChatGPT around topics in basic Linear Algebra. We reflect
the process undertaken by the ChatGPT along the recent year in our area of
interest, emphasising the vast improvement that has been done in grappling with
Linear Algebra problems. In particular, the question whether this software can
be a teaching assistant or even somehow replace the human teacher, is
addressed. As of the time this paper is written, the answer is generally
negative. For the small part where the answer can be positive, some reflections
about an original instrumental genesis are given.
  Communication with the software gives the impression to talk to a human, and
sometimes the question is whether the software understands the question or not.
Therefore, the reader's attention is drawn to the fact that ChatGPT works on a
statistical basis and not according to reflection and understanding.
\\ ( https://arxiv.org/abs/2403.15399 ,  6049kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15638 (*cross-listing*)
Date: Fri, 22 Mar 2024 22:27:44 GMT   (740kb,D)

Title: Differentially Private Next-Token Prediction of Large Language Models
Authors: James Flemings, Meisam Razaviyayn, Murali Annavaram
Categories: cs.CR cs.CL cs.LG
\\
  Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly
important. The most widely adopted technique to accomplish this is DP-SGD,
which trains a model in such a way that guarantees Differential Privacy (DP).
However, DP-SGD requires longer training times and larger memory requirements
than SGD, while overestimating an adversary's capabilities in having white box
access to the model. A more realistic scenario assumes only black-box access to
a privacy-sensitive LLM. Motivated by these observations, we present Private
Mixing of Ensemble Distributions (PMixED): a private prediction protocol that
achieves practical next-token prediction by projecting each of the model's
output distribution from an ensemble of fine-tuned LLMs onto a set around a
public LLM's output distribution, then averaging the projected distributions
and sampling from it. Our approach is more lightweight than DP-SGD in that it
is model agnostic, instead providing differential privacy at prediction rather
than during training. Our results show that PMixED achieves a stronger privacy
guarantee than sample-level privacy and outperforms DP-SGD for privacy
$\epsilon = 8$ on large-scale datasets.
\\ ( https://arxiv.org/abs/2403.15638 ,  740kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15676 (*cross-listing*)
Date: Sat, 23 Mar 2024 01:44:57 GMT   (851kb,D)

Title: AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs
Authors: Hao Chen, Minyu Chen, Ruibang Liu, Guoqiang Li
Categories: cs.SE cs.CL cs.CR
Comments: 20 pages, 4 figures
\\
  ZKP systems have surged attention and held a fundamental role in contemporary
cryptography. Zk-SNARK protocols dominate the ZKP usage, often implemented
through arithmetic circuit programming paradigm. However, underconstrained or
overconstrained circuits may lead to bugs. Underconstrained circuits refer to
circuits that lack the necessary constraints, resulting in unexpected solutions
in the circuit and causing the verifier to accept a bogus witness.
Overconstrained circuits refer to circuits that are constrained excessively,
resulting in the circuit lacking necessary solutions and causing the verifier
to accept no witness, rendering the circuit meaningless. This paper introduces
a novel approach for pinpointing two distinct types of bugs in ZKP circuits.
The method involves encoding the arithmetic circuit constraints to polynomial
equation systems and solving polynomial equation systems over a finite field by
algebraic computation. The classification of verification results is refined,
greatly enhancing the expressive power of the system. We proposed a tool, AC4,
to represent the implementation of this method. Experiments demonstrate that
AC4 represents a substantial 29% increase in the checked ratio compared to
prior work. Within a solvable range, the checking time of AC4 has also
exhibited noticeable improvement, demonstrating a magnitude increase compared
to previous efforts.
\\ ( https://arxiv.org/abs/2403.15676 ,  851kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15837 (*cross-listing*)
Date: Sat, 23 Mar 2024 13:24:31 GMT   (3154kb,D)

Title: Centered Masking for Language-Image Pre-Training
Authors: Mingliang Liang, Martha Larson
Categories: cs.CV cs.CL cs.LG
\\
  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
\\ ( https://arxiv.org/abs/2403.15837 ,  3154kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15952 (*cross-listing*)
Date: Sat, 23 Mar 2024 23:06:32 GMT   (3101kb,D)

Title: IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language
  Models
Authors: Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi
  Uddin Ahmad, Yue Dong, Rifat Shahriyar
Categories: cs.CV cs.CL
\\
  The advent of Vision Language Models (VLM) has allowed researchers to
investigate the visual understanding of a neural network using natural
language. Beyond object classification and detection, VLMs are capable of
visual comprehension and common-sense reasoning. This naturally led to the
question: How do VLMs respond when the image itself is inherently unreasonable?
To this end, we present IllusionVQA: a diverse dataset of challenging optical
illusions and hard-to-interpret scenes to test the capability of VLMs in two
distinct multiple-choice VQA tasks - comprehension and soft localization.
GPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the
comprehension task and 49.7% on the localization task (4-shot and
Chain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100%
accuracy in comprehension and localization. We discover that In-Context
Learning (ICL) and Chain-of-Thought reasoning substantially degrade the
performance of GeminiPro on the localization task. Tangentially, we discover a
potential weakness in the ICL capabilities of VLMs: they fail to locate optical
illusions even when the correct answer is in the context window as a few-shot
example.
\\ ( https://arxiv.org/abs/2403.15952 ,  3101kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15992 (*cross-listing*)
Date: Sun, 24 Mar 2024 03:10:07 GMT   (5694kb,D)

Title: BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval
Authors: Yinda Chen, Che Liu, Xiaoyu Liu, Rossella Arcucci, Zhiwei Xiong
Categories: cs.CV cs.CL
\\
  The burgeoning integration of 3D medical imaging into healthcare has led to a
substantial increase in the workload of medical professionals. To assist
clinicians in their diagnostic processes and alleviate their workload, the
development of a robust system for retrieving similar case studies presents a
viable solution. While the concept holds great promise, the field of 3D medical
text-image retrieval is currently limited by the absence of robust evaluation
benchmarks and curated datasets. To remedy this, our study presents a
groundbreaking dataset, BIMCV-R (This dataset will be released upon
acceptance.), which includes an extensive collection of 8,069 3D CT volumes,
encompassing over 2 million slices, paired with their respective radiological
reports. Expanding upon the foundational work of our dataset, we craft a
retrieval strategy, MedFinder. This approach employs a dual-stream network
architecture, harnessing the potential of large language models to advance the
field of medical image retrieval beyond existing text-image retrieval
solutions. It marks our preliminary step towards developing a system capable of
facilitating text-to-image, image-to-text, and keyword-based retrieval tasks.
\\ ( https://arxiv.org/abs/2403.15992 ,  5694kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16087 (*cross-listing*)
Date: Sun, 24 Mar 2024 10:57:08 GMT   (1509kb,D)

Title: LLMs as Compiler for Arabic Programming Language
Authors: Serry Sibaee, Omar Najar, Lahouri Ghouti, Anis Koubaa
Categories: cs.SE cs.CL cs.LG
\\
  In this paper we introduce APL (Arabic Programming Language) that uses Large
language models (LLM) as semi-compiler to covert Arabic text code to python
code then run the code. Designing a full pipeline from the structure of the APL
text then a prompt (using prompt engineering) then running the prodcued python
code using PyRunner. This project has a three parts first python library, a
playground with simple interface and this research paper.
\\ ( https://arxiv.org/abs/2403.16087 ,  1509kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16167 (*cross-listing*)
Date: Sun, 24 Mar 2024 14:21:06 GMT   (3512kb,D)

Title: Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-Language Models
Authors: Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru
  Chang
Categories: cs.CV cs.CL
\\
  Hallucinations in vision-language models pose a significant challenge to
their reliability, particularly in the generation of long captions. Current
methods fall short of accurately identifying and mitigating these
hallucinations. To address this issue, we introduce ESREAL, a novel
unsupervised learning framework designed to suppress the generation of
hallucinations through accurate localization and penalization of hallucinated
tokens. Initially, ESREAL creates a reconstructed image based on the generated
caption and aligns its corresponding regions with those of the original image.
This semantic reconstruction aids in identifying both the presence and type of
token-level hallucinations within the generated caption. Subsequently, ESREAL
computes token-level hallucination scores by assessing the semantic similarity
of aligned regions based on the type of hallucination. Finally, ESREAL employs
a proximal policy optimization algorithm, where it selectively penalizes
hallucinated tokens according to their token-level hallucination scores. Our
framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2
by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved
solely through signals derived from the image itself, without the need for any
image-text pairs.
\\ ( https://arxiv.org/abs/2403.16167 ,  3512kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16385 (*cross-listing*)
Date: Mon, 25 Mar 2024 03:02:27 GMT   (4089kb,D)

Title: Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators
  for Reasoning-Based Chart VQA
Authors: Li Zhuowan, Jasani Bhavan, Tang Peng, Ghadar Shabnam
Categories: cs.CV cs.CL
Comments: Accepted to CVPR 2024
\\
  Understanding data visualizations like charts and plots requires reasoning
about both visual elements and numerics. Although strong in extractive
questions, current chart visual question answering (chart VQA) models suffer on
complex reasoning questions. In this work, we address the lack of reasoning
ability by data augmentation. We leverage Large Language Models (LLMs), which
have shown to have strong reasoning ability, as an automatic data annotator
that generates question-answer annotations for chart images. The key innovation
in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data
generator learns to decompose the complex question into step-by-step
sub-questions (rationales), which are then used to derive the final answer
using external tools, i.e. Python. This step-wise generation procedure is
trained on synthetic data generated using a template-based QA generation
pipeline. Experimental results highlight the significance of the proposed
step-by-step generation. By training with the LLM-augmented data (LAMENDA), we
significantly enhance the chart VQA models, achieving the state-of-the-art
accuracy on the ChartQA and PlotQA datasets. In particular, our approach
improves the accuracy of the previous state-of-the-art approach from 38% to 54%
on the human-written questions in the ChartQA dataset, which needs strong
reasoning. We hope our work underscores the potential of synthetic data and
encourages further exploration of data augmentation using LLMs for
reasoning-heavy tasks.
\\ ( https://arxiv.org/abs/2403.16385 ,  4089kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16437 (*cross-listing*)
Date: Mon, 25 Mar 2024 05:37:16 GMT   (710kb,D)

Title: Evaluating Large Language Models with Runtime Behavior of Program
  Execution
Authors: Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, Xin Xia
Categories: cs.SE cs.CL
\\
  Large language models for code (i.e., code LLMs) have shown strong code
understanding and generation capabilities. To evaluate the capabilities of code
LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval
and ClassEval). Code reasoning is one of the most essential abilities of code
LLMs, but existing benchmarks for code reasoning are not sufficient. Typically,
they focus on predicting the input and output of a program, ignoring the
evaluation of the intermediate behavior during program execution, as well as
the logical consistency (e.g., the model should not give the correct output if
the prediction of execution path is wrong) when performing the reasoning. To
address these problems, in this paper, we propose a framework, namely REval,
for evaluating code reasoning abilities and consistency of code LLMs with
program execution. We utilize existing code benchmarks and adapt them to new
benchmarks within our framework. A large-scale empirical study is conducted and
most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning
(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation
(i.e., an average IC score of 10.3). Evaluation results of current code LLMs
reflect the urgent need for the community to strengthen the code reasoning
capability of code LLMs.
\\ ( https://arxiv.org/abs/2403.16437 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16899 (*cross-listing*)
Date: Mon, 25 Mar 2024 16:10:47 GMT   (814kb,D)

Title: State Space Models as Foundation Models: A Control Theoretic Overview
Authors: Carmen Amo Alonso, Jerome Sieber, Melanie N. Zeilinger
Categories: eess.SY cs.LG cs.SY
\\
  In recent years, there has been a growing interest in integrating linear
state-space models (SSM) in deep neural network architectures of foundation
models. This is exemplified by the recent success of Mamba, showing better
performance than the state-of-the-art Transformer architectures in language
tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a
latent space in order to learn a compressed representation of the data. The
same goal has been pursued by control theorists using SSMs to efficiently model
dynamical systems. Therefore, SSMs can be naturally connected to deep sequence
modeling, offering the opportunity to create synergies between the
corresponding research areas. This paper is intended as a gentle introduction
to SSM-based architectures for control theorists and summarizes the latest
research developments. It provides a systematic review of the most successful
SSM proposals and highlights their main features from a control theoretic
perspective. Additionally, we present a comparative analysis of these models,
evaluating their performance on a standardized benchmark designed for assessing
a model's efficiency at learning long sequences.
\\ ( https://arxiv.org/abs/2403.16899 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15394 (*cross-listing*)
Date: Thu, 15 Feb 2024 14:56:00 GMT   (361kb)

Title: "Model Cards for Model Reporting" in 2024: Reclassifying Category of
  Ethical Considerations in Terms of Trustworthiness and Risk Management
Authors: DeBrae Kennedy-Mayo and Jake Gord
Categories: cs.CY cs.LG
Comments: 14 pages, 2 figures, submitted to ACM Conference on Fairness,
  Accountability, and Transparency 2024 (ACM FAccT '24)
\\
  In 2019, the paper entitled "Model Cards for Model Reporting" introduced a
new tool for documenting model performance and encouraged the practice of
transparent reporting for a defined list of categories. One of the categories
detailed in that paper is ethical considerations, which includes the
subcategories of data, human life, mitigations, risks and harms, and use cases.
We propose to reclassify this category in the original model card due to the
recent maturing of the field known as trustworthy AI, a term which analyzes
whether the algorithmic properties of the model indicate that the AI system is
deserving of trust from its stakeholders. In our examination of trustworthy AI,
we highlight three respected organizations - the European Commission's
High-Level Expert Group on AI, the OECD, and the U.S.-based NIST - that have
written guidelines on various aspects of trustworthy AI. These recent
publications converge on numerous characteristics of the term, including
accountability, explainability, fairness, privacy, reliability, robustness,
safety, security, and transparency, while recognizing that the implementation
of trustworthy AI varies by context. Our reclassification of the original
model-card category known as ethical considerations involves a two-step
process: 1) adding a new category known as trustworthiness, where the
subcategories will be derived from the discussion of trustworthy AI in our
paper, and 2) maintaining the subcategories of ethical considerations under a
renamed category known as risk environment and risk management, a title which
we believe better captures today's understanding of the essence of these
topics. We hope that this reclassification will further the goals of the
original paper and continue to prompt those releasing trained models to
accompany these models with documentation that will assist in the evaluation of
their algorithmic properties.
\\ ( https://arxiv.org/abs/2403.15394 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15409 (*cross-listing*)
Date: Sat, 2 Mar 2024 12:09:16 GMT   (2278kb,D)

Title: Coupled generator decomposition for fusion of electro- and
  magnetoencephalography data
Authors: Anders Stevnhoved Olsen, Jesper Duemose Nielsen, Morten M{\o}rup
Categories: eess.SP cs.LG
\\
  Data fusion modeling can identify common features across diverse data sources
while accounting for source-specific variability. Here we introduce the concept
of a \textit{coupled generator decomposition} and demonstrate how it
generalizes sparse principal component analysis (SPCA) for data fusion.
Leveraging data from a multisubject, multimodal (electro- and
magnetoencephalography (EEG and MEG)) neuroimaging experiment, we demonstrate
the efficacy of the framework in identifying common features in response to
face perception stimuli, while accommodating modality- and subject-specific
variability. Through split-half cross-validation of EEG/MEG trials, we
investigate the optimal model order and regularization strengths for models of
varying complexity, comparing these to a group-level model assuming shared
brain responses to stimuli. Our findings reveal altered $\sim170ms$ fusiform
face area activation for scrambled faces, as opposed to real faces,
particularly evident in the multimodal, multisubject model. Model parameters
were inferred using stochastic optimization in PyTorch, demonstrating
comparable performance to conventional quadratic programming inference for SPCA
but with considerably faster execution. We provide an easily accessible toolbox
for coupled generator decomposition that includes data fusion for SPCA,
archetypal analysis and directional archetypal analysis. Overall, our approach
offers a promising new avenue for data fusion.
\\ ( https://arxiv.org/abs/2403.15409 ,  2278kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15415 (*cross-listing*)
Date: Thu, 7 Mar 2024 16:17:33 GMT   (546kb,D)

Title: Physics-informed and Unsupervised Riemannian Domain Adaptation for
  Machine Learning on Heterogeneous EEG Datasets
Authors: Apolline Mellot, Antoine Collas, Sylvain Chevallier, Denis Engemann,
  Alexandre Gramfort
Categories: eess.SP cs.LG
\\
  Combining electroencephalogram (EEG) datasets for supervised machine learning
(ML) is challenging due to session, subject, and device variability. ML
algorithms typically require identical features at train and test time,
complicating analysis due to varying sensor numbers and positions across
datasets. Simple channel selection discards valuable data, leading to poorer
performance, especially with datasets sharing few channels. To address this, we
propose an unsupervised approach leveraging EEG signal physics. We map EEG
channels to fixed positions using field interpolation, facilitating source-free
domain adaptation. Leveraging Riemannian geometry classification pipelines and
transfer learning steps, our method demonstrates robust performance in
brain-computer interface (BCI) tasks and potential biomarker applications.
Comparative analysis against a statistical-based approach known as
Dimensionality Transcending, a signal-based imputation called ComImp,
source-dependent methods, as well as common channel selection and spherical
spline interpolation, was conducted with leave-one-dataset-out validation on
six public BCI datasets for a right-hand/left-hand classification task.
Numerical experiments show that in the presence of few shared channels in train
and test, the field interpolation consistently outperforms other methods,
demonstrating enhanced classification performance across all datasets. When
more channels are shared, field interpolation was found to be competitive with
other methods and faster to compute than source-dependent methods.
\\ ( https://arxiv.org/abs/2403.15415 ,  546kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15417 (*cross-listing*)
Date: Fri, 8 Mar 2024 21:33:03 GMT   (1147kb,D)

Title: Enhancing Automatic Modulation Recognition for IoT Applications Using
  Transformers
Authors: Narges Rashvand, Kenneth Witham, Gabriel Maldonado, Vinit Katariya,
  Nishanth Marer Prabhu, Gunar Schirner, Hamed Tabkhi
Categories: eess.SP cs.LG
\\
  Automatic modulation recognition (AMR) is critical for determining the
modulation type of incoming signals. Integrating advanced deep learning
approaches enables rapid processing and minimal resource usage, essential for
IoT applications. We have introduced a novel method using Transformer networks
for efficient AMR, designed specifically to address the constraints on model
size prevalent in IoT environments. Our extensive experiments reveal that our
proposed method outperformed advanced deep learning techniques, achieving the
highest recognition accuracy.
\\ ( https://arxiv.org/abs/2403.15417 ,  1147kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15421 (*cross-listing*)
Date: Tue, 12 Mar 2024 19:34:18 GMT   (3620kb,D)

Title: Agile gesture recognition for low-power applications: customisation for
  generalisation
Authors: Ying Liu, Liucheng Guo, Valeri A. Makarovc, Alexander Gorbana, Evgeny
  Mirkesa, Ivan Y. Tyukin
Categories: eess.SP cs.LG stat.AP
\\
  Automated hand gesture recognition has long been a focal point in the AI
community. Traditionally, research in this field has predominantly focused on
scenarios with access to a continuous flow of hand's images. This focus has
been driven by the widespread use of cameras and the abundant availability of
image data. However, there is an increasing demand for gesture recognition
technologies that operate on low-power sensor devices. This is due to the
rising concerns for data leakage and end-user privacy, as well as the limited
battery capacity and the computing power in low-cost devices. Moreover, the
challenge in data collection for individually designed hardware also hinders
the generalisation of a gesture recognition model.
  In this study, we unveil a novel methodology for pattern recognition systems
using adaptive and agile error correction, designed to enhance the performance
of legacy gesture recognition models on devices with limited battery capacity
and computing power. This system comprises a compact Support Vector Machine as
the base model for live gesture recognition. Additionally, it features an
adaptive agile error corrector that employs few-shot learning within the
feature space induced by high-dimensional kernel mappings. The error corrector
can be customised for each user, allowing for dynamic adjustments to the
gesture prediction based on their movement patterns while maintaining the agile
performance of its base model on a low-cost and low-power micro-controller.
This proposed system is distinguished by its compact size, rapid processing
speed, and low power consumption, making it ideal for a wide range of embedded
systems.
\\ ( https://arxiv.org/abs/2403.15421 ,  3620kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15431 (*cross-listing*)
Date: Thu, 14 Mar 2024 09:49:00 GMT   (3814kb,D)

Title: Transferring BCI models from calibration to control: Observing shifts in
  EEG features
Authors: Ivo Pascal de Jong, L\"uke Luna van den Wittenboer, Matias
  Valdenegro-Toro, Andreea Ioana Sburlea
Categories: eess.SP cs.HC cs.LG
\\
  Public Motor Imagery-based brain-computer interface (BCI) datasets are being
used to develop increasingly good classifiers. However, they usually follow
discrete paradigms where participants perform Motor Imagery at regularly timed
intervals. It is often unclear what changes may happen in the EEG patterns when
users attempt to perform a control task with such a BCI. This may lead to
generalisation errors. We demonstrate a new paradigm containing a standard
calibration session and a novel BCI control session based on EMG. This allows
us to observe similarities in sensorimotor rhythms, and observe the additional
preparation effects introduced by the control paradigm. In the Movement Related
Cortical Potentials we found large differences between the calibration and
control sessions. We demonstrate a CSP-based Machine Learning model trained on
the calibration data that can make surprisingly good predictions on the
BCI-controlled driving data.
\\ ( https://arxiv.org/abs/2403.15431 ,  3814kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15438 (*cross-listing*)
Date: Fri, 15 Mar 2024 22:22:10 GMT   (239kb)

Title: Unsupervised Adaptive Deep Learning Method For BCI Motor Imagery
  Decoding
Authors: Yassine El Ouahidi, Giulia Lioi, Nicolas Farrugia, Bastien Pasdeloup
  and Vincent Gripon
Categories: eess.SP cs.LG
\\
  In the context of Brain-Computer Interfaces, we propose an adaptive method
that reaches offline performance level while being usable online without
requiring supervision. Interestingly, our method does not require retraining
the model, as it consists in using a frozen efficient deep learning backbone
while continuously realigning data, both at input and latent spaces, based on
streaming observations. We demonstrate its efficiency for Motor Imagery brain
decoding from electroencephalography data, considering challenging
cross-subject scenarios. For reproducibility, we share the code of our
experiments.
\\ ( https://arxiv.org/abs/2403.15438 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15448 (*cross-listing*)
Date: Mon, 18 Mar 2024 03:01:53 GMT   (993kb,D)

Title: What is Wrong with End-to-End Learning for Phase Retrieval?
Authors: Wenjie Zhang, Yuxiang Wan, Zhong Zhuang and Ju Sun
Categories: eess.SP cs.LG
\\
  For nonlinear inverse problems that are prevalent in imaging science,
symmetries in the forward model are common. When data-driven deep learning
approaches are used to solve such problems, these intrinsic symmetries can
cause substantial learning difficulties. In this paper, we explain how such
difficulties arise and, more importantly, how to overcome them by preprocessing
the training set before any learning, i.e., symmetry breaking. We take
far-field phase retrieval (FFPR), which is central to many areas of scientific
imaging, as an example and show that symmetric breaking can substantially
improve data-driven learning. We also formulate the mathematical principle of
symmetry breaking.
\\ ( https://arxiv.org/abs/2403.15448 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15462 (*cross-listing*)
Date: Tue, 19 Mar 2024 14:20:46 GMT   (28604kb,D)

Title: FUELVISION: A Multimodal Data Fusion and Multimodel Ensemble Algorithm
  for Wildfire Fuels Mapping
Authors: Riyaaz Uddien Shaik, Mohamad Alipour, Eric Rowell, Bharathan Balaji,
  Adam Watts, Ertugrul Taciroglu
Categories: eess.IV cs.LG
Comments: 40 pages
ACM-class: I.4.9
\\
  Accurate assessment of fuel conditions is a prerequisite for fire ignition
and behavior prediction, and risk management. The method proposed herein
leverages diverse data sources including Landsat-8 optical imagery, Sentinel-1
(C-band) Synthetic Aperture Radar (SAR) imagery, PALSAR (L-band) SAR imagery,
and terrain features to capture comprehensive information about fuel types and
distributions. An ensemble model was trained to predict landscape-scale fuels
such as the 'Scott and Burgan 40' using the as-received Forest Inventory and
Analysis (FIA) field survey plot data obtained from the USDA Forest Service.
However, this basic approach yielded relatively poor results due to the
inadequate amount of training data. Pseudo-labeled and fully synthetic datasets
were developed using generative AI approaches to address the limitations of
ground truth data availability. These synthetic datasets were used for
augmenting the FIA data from California to enhance the robustness and coverage
of model training. The use of an ensemble of methods including deep learning
neural networks, decision trees, and gradient boosting offered a fuel mapping
accuracy of nearly 80\%. Through extensive experimentation and evaluation, the
effectiveness of the proposed approach was validated for regions of the 2021
Dixie and Caldor fires. Comparative analyses against high-resolution data from
the National Agriculture Imagery Program (NAIP) and timber harvest maps
affirmed the robustness and reliability of the proposed approach, which is
capable of near-real-time fuel mapping.
\\ ( https://arxiv.org/abs/2403.15462 ,  28604kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15463 (*cross-listing*)
Date: Tue, 19 Mar 2024 15:53:57 GMT   (840kb,D)

Title: Unveiling the Anomalies in an Ever-Changing World: A Benchmark for
  Pixel-Level Anomaly Detection in Continual Learning
Authors: Nikola Bugarin, Jovana Bugaric, Manuel Barusco, Davide Dalle Pezze,
  Gian Antonio Susto
Categories: cs.CV cs.LG
\\
  Anomaly Detection is a relevant problem in numerous real-world applications,
especially when dealing with images. However, little attention has been paid to
the issue of changes over time in the input data distribution, which may cause
a significant decrease in performance. In this study, we investigate the
problem of Pixel-Level Anomaly Detection in the Continual Learning setting,
where new data arrives over time and the goal is to perform well on new and old
data. We implement several state-of-the-art techniques to solve the Anomaly
Detection problem in the classic setting and adapt them to work in the
Continual Learning setting. To validate the approaches, we use a real-world
dataset of images with pixel-based anomalies to provide a reliable benchmark
and serve as a foundation for further advancements in the field. We provide a
comprehensive analysis, discussing which Anomaly Detection methods and which
families of approaches seem more suitable for the Continual Learning setting.
\\ ( https://arxiv.org/abs/2403.15463 ,  840kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15466 (*cross-listing*)
Date: Wed, 20 Mar 2024 03:42:15 GMT   (1229kb)

Title: Using Super-Resolution Imaging for Recognition of Low-Resolution Blurred
  License Plates: A Comparative Study of Real-ESRGAN, A-ESRGAN, and StarSRGAN
Authors: Ching-Hsiang Wang
Categories: cs.CV cs.LG eess.IV
Comments: Master's thesis
\\
  With the robust development of technology, license plate recognition
technology can now be properly applied in various scenarios, such as road
monitoring, tracking of stolen vehicles, detection at parking lot entrances and
exits, and so on. However, the precondition for these applications to function
normally is that the license plate must be 'clear' enough to be recognized by
the system with the correct license plate number. If the license plate becomes
blurred due to some external factors, then the accuracy of recognition will be
greatly reduced. Although there are many road surveillance cameras in Taiwan,
the quality of most cameras is not good, often leading to the inability to
recognize license plate numbers due to low photo resolution. Therefore, this
study focuses on using super-resolution technology to process blurred license
plates. This study will mainly fine-tune three super-resolution models:
Real-ESRGAN, A-ESRGAN, and StarSRGAN, and compare their effectiveness in
enhancing the resolution of license plate photos and enabling accurate license
plate recognition. By comparing different super-resolution models, it is hoped
to find the most suitable model for this task, providing valuable references
for future researchers.
\\ ( https://arxiv.org/abs/2403.15466 ,  1229kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15480 (*cross-listing*)
Date: Thu, 21 Mar 2024 03:11:53 GMT   (867kb,D)

Title: SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph
  Attention
Authors: Yundong Sun, Dongjie Zhu, Yansong Wang, Zhaoshuo Tian, Ning Cao and
  Gregory O'Hared
Categories: cs.NE cs.LG
\\
  Recently, Graph Transformers have emerged as a promising solution to
alleviate the inherent limitations of Graph Neural Networks (GNNs) and enhance
graph representation performance. Unfortunately, Graph Transformers are
computationally expensive due to the quadratic complexity inherent in
self-attention when applied over large-scale graphs, especially for node tasks.
In contrast, spiking neural networks (SNNs), with event-driven and binary
spikes properties, can perform energy-efficient computation. In this work, we
propose a novel insight into integrating SNNs with Graph Transformers and
design a Spiking Graph Attention (SGA) module. The matrix multiplication is
replaced by sparse addition and mask operations. The linear complexity enables
all-pair node interactions on large-scale graphs with limited GPU memory. To
our knowledge, our work is the first attempt to introduce SNNs into Graph
Transformers. Furthermore, we design SpikeGraphormer, a Dual-branch
architecture, combining a sparse GNN branch with our SGA-driven Graph
Transformer branch, which can simultaneously perform all-pair node interactions
and capture local neighborhoods. SpikeGraphormer consistently outperforms
existing state-of-the-art approaches across various datasets and makes
substantial improvements in training time, inference time, and GPU memory cost
(10 ~ 20x lower than vanilla self-attention). It also performs well in
cross-domain applications (image and text classification). We release our code
at https://github.com/PHD-lanyu/SpikeGraphormer.
\\ ( https://arxiv.org/abs/2403.15480 ,  867kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15483 (*cross-listing*)
Date: Thu, 21 Mar 2024 06:42:35 GMT   (971kb)

Title: Rolling bearing fault diagnosis method based on generative adversarial
  enhanced multi-scale convolutional neural network model
Authors: Maoxuan Zhou, Wei Kang and Kun He
Categories: eess.SP cs.LG
\\
  In order to solve the problem that current convolutional neural networks can
not capture the correlation features between the time domain signals of rolling
bearings effectively, and the model accuracy is limited by the number and
quality of samples, a rolling bearing fault diagnosis method based on
generative adversarial enhanced multi-scale convolutional neural network model
is proposed. Firstly, Gram angular field coding technique is used to encode the
time domain signal of the rolling bearing and generate the feature map to
retain the complete information of the vibration signal. Then, the re-sulting
data is divided into a training set, a validation set, and a test set. Among
them, the training set is input into the gradient penalty Wasserstein distance
generation adversarial network to complete the training, and a new sample with
similar features to the training sample is obtained, and then the original
training set is expanded. Next, multi-scale convolution is used to extract the
fault features of the extended training set, and the feature graph is
normalized by example to overcome the influence of the difference in feature
distribution. Finally, the attention mechanism is applied to the adaptive
weighting of normalized features and the extraction of deep features, and the
fault diagnosis is completed by the softmax classifier. Compared with ResNet
method, the experimental results show that the proposed method has better
generalization performance and anti-noise performance.
\\ ( https://arxiv.org/abs/2403.15483 ,  971kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15494 (*cross-listing*)
Date: Thu, 21 Mar 2024 17:36:53 GMT   (4618kb,D)

Title: Multiple and Gyro-Free Inertial Datasets
Authors: Zeev Yampolsky, Yair Stolero, Nitzan Pri-Hadash, Dan Solodar, Shira
  Massas, Itai Savin, and Itzik Klein
Categories: eess.SP cs.LG cs.RO
Comments: 10 pages, 16 figures, 6 tables
\\
  An inertial navigation system (INS) utilizes three orthogonal accelerometers
and gyroscopes to determine platform position, velocity, and orientation. There
are countless applications for INS, including robotics, autonomous platforms,
and the internet of things. Recent research explores the integration of
data-driven methods with INS, highlighting significant innovations, improving
accuracy and efficiency. Despite the growing interest in this field and the
availability of INS datasets, no datasets are available for gyro-free INS
(GFINS) and multiple inertial measurement unit (MIMU) architectures. To fill
this gap and to stimulate further research in this field, we designed and
recorded GFINS and MIMU datasets using 54 inertial sensors grouped in nine
inertial measurement units. These sensors can be used to define and evaluate
different types of MIMU and GFINS architectures. The inertial sensors were
arranged in three different sensor configurations and mounted on a mobile robot
and a passenger car. In total, the dataset contains 35 hours of inertial data
and corresponding ground truth trajectories. The data and code are freely
accessible through our GitHub repository.
\\ ( https://arxiv.org/abs/2403.15494 ,  4618kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15497 (*cross-listing*)
Date: Thu, 21 Mar 2024 18:31:47 GMT   (2073kb,D)

Title: On the Detection of Anomalous or Out-Of-Distribution Data in Vision
  Models Using Statistical Techniques
Authors: Laura O'Mahony, David JP O'Sullivan, Nikola S. Nikolov
Categories: cs.CV cs.LG
\\
  Out-of-distribution data and anomalous inputs are vulnerabilities of machine
learning systems today, often causing systems to make incorrect predictions.
The diverse range of data on which these models are used makes detecting
atypical inputs a difficult and important task. We assess a tool, Benford's
law, as a method used to quantify the difference between real and corrupted
inputs. We believe that in many settings, it could function as a filter for
anomalous data points and for signalling out-of-distribution data. We hope to
open a discussion on these applications and further areas where this technique
is underexplored.
\\ ( https://arxiv.org/abs/2403.15497 ,  2073kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15500 (*cross-listing*)
Date: Thu, 21 Mar 2024 21:27:43 GMT   (2031kb,D)

Title: Gene Regulatory Network Inference in the Presence of Dropouts: a Causal
  View
Authors: Haoyue Dai, Ignavier Ng, Gongxu Luo, Peter Spirtes, Petar Stojanov,
  Kun Zhang
Categories: q-bio.QM cs.LG q-bio.MN
Comments: Appears at ICLR 2024 (oral)
\\
  Gene regulatory network inference (GRNI) is a challenging problem,
particularly owing to the presence of zeros in single-cell RNA sequencing data:
some are biological zeros representing no gene expression, while some others
are technical zeros arising from the sequencing procedure (aka dropouts), which
may bias GRNI by distorting the joint distribution of the measured gene
expressions. Existing approaches typically handle dropout error via imputation,
which may introduce spurious relations as the true joint distribution is
generally unidentifiable. To tackle this issue, we introduce a causal graphical
model to characterize the dropout mechanism, namely, Causal Dropout Model. We
provide a simple yet effective theoretical result: interestingly, the
conditional independence (CI) relations in the data with dropouts, after
deleting the samples with zero values (regardless if technical or not) for the
conditioned variables, are asymptotically identical to the CI relations in the
original data without dropouts. This particular test-wise deletion procedure,
in which we perform CI tests on the samples without zeros for the conditioned
variables, can be seamlessly integrated with existing structure learning
approaches including constraint-based and greedy score-based methods, thus
giving rise to a principled framework for GRNI in the presence of dropouts. We
further show that the causal dropout model can be validated from data, and many
existing statistical models to handle dropouts fit into our model as specific
parametric instances. Empirical evaluation on synthetic, curated, and
real-world experimental transcriptomic data comprehensively demonstrate the
efficacy of our method.
\\ ( https://arxiv.org/abs/2403.15500 ,  2031kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15510 (*cross-listing*)
Date: Fri, 22 Mar 2024 03:41:57 GMT   (3215kb,D)

Title: Privacy-Preserving End-to-End Spoken Language Understanding
Authors: Yinggui Wang, Wei Huang and Le Yang
Categories: cs.CR cs.LG eess.AS
Comments: Accepted by IJCAI
\\
  Spoken language understanding (SLU), one of the key enabling technologies for
human-computer interaction in IoT devices, provides an easy-to-use user
interface. Human speech can contain a lot of user-sensitive information, such
as gender, identity, and sensitive content. New types of security and privacy
breaches have thus emerged. Users do not want to expose their personal
sensitive information to malicious attacks by untrusted third parties. Thus,
the SLU system needs to ensure that a potential malicious attacker cannot
deduce the sensitive attributes of the users, while it should avoid greatly
compromising the SLU accuracy. To address the above challenge, this paper
proposes a novel SLU multi-task privacy-preserving model to prevent both the
speech recognition (ASR) and identity recognition (IR) attacks. The model uses
the hidden layer separation technique so that SLU information is distributed
only in a specific portion of the hidden layer, and the other two types of
information are removed to obtain a privacy-secure hidden layer. In order to
achieve good balance between efficiency and privacy, we introduce a new
mechanism of model pre-training, namely joint adversarial training, to further
enhance the user privacy. Experiments over two SLU datasets show that the
proposed method can reduce the accuracy of both the ASR and IR attacks close to
that of a random guess, while leaving the SLU performance largely unaffected.
\\ ( https://arxiv.org/abs/2403.15510 ,  3215kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15521 (*cross-listing*)
Date: Fri, 22 Mar 2024 13:20:33 GMT   (74kb,D)

Title: Learning to walk on new ground: Calibration-free decoding for c-VEP BCI
Authors: J. Thielen, J. Sosulski, M. Tangermann
Categories: eess.SP cs.LG
Comments: 6 pages, 2 figures, 9th Graz Brain-Computer Interface Conference 2024
\\
  This study explores two zero-training methods aimed at enhancing the
usability of brain-computer interfaces (BCIs) by eliminating the need for a
calibration session. We introduce a novel method rooted in the event-related
potential (ERP) domain, unsupervised mean maximization (UMM), to the fast
code-modulated visual evoked potential (c-VEP) stimulus protocol. We compare
UMM to the state-of-the-art c-VEP zero-training method that uses canonical
correlation analysis (CCA). The comparison includes instantaneous
classification and classification with cumulative learning from previously
classified trials for both CCA and UMM. Our study shows the effectiveness of
both methods in navigating the complexities of a c-VEP dataset, highlighting
their differences and distinct strengths. This research not only provides
insights into the practical implementation of calibration-free BCI methods but
also paves the way for further exploration and refinement. Ultimately, the
fusion of CCA and UMM holds promise for enhancing the accessibility and
usability of BCI systems across various application domains and a multitude of
stimulus protocols.
\\ ( https://arxiv.org/abs/2403.15521 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15524 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:13:11 GMT   (639kb,D)

Title: PPA-Game: Characterizing and Learning Competitive Dynamics Among Online
  Content Creators
Authors: Renzhe Xu, Haotian Wang, Xingxuan Zhang, Bo Li, Peng Cui
Categories: cs.GT cs.LG
\\
  We introduce the Proportional Payoff Allocation Game (PPA-Game) to model how
agents, akin to content creators on platforms like YouTube and TikTok, compete
for divisible resources and consumers' attention. Payoffs are allocated to
agents based on heterogeneous weights, reflecting the diversity in content
quality among creators. Our analysis reveals that although a pure Nash
equilibrium (PNE) is not guaranteed in every scenario, it is commonly observed,
with its absence being rare in our simulations. Beyond analyzing static
payoffs, we further discuss the agents' online learning about resource payoffs
by integrating a multi-player multi-armed bandit framework. We propose an
online algorithm facilitating each agent's maximization of cumulative payoffs
over $T$ rounds. Theoretically, we establish that the regret of any agent is
bounded by $O(\log^{1 + \eta} T)$ for any $\eta > 0$. Empirical results further
validate the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2403.15524 ,  639kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15525 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:15:28 GMT   (114kb,D)

Title: Latent Neural Cellular Automata for Resource-Efficient Image Restoration
Authors: Andrea Menta, Alberto Archetti, Matteo Matteucci
Categories: eess.IV cs.LG cs.NE
\\
  Neural cellular automata represent an evolution of the traditional cellular
automata model, enhanced by the integration of a deep learning-based transition
function. This shift from a manual to a data-driven approach significantly
increases the adaptability of these models, enabling their application in
diverse domains, including content generation and artificial life. However,
their widespread application has been hampered by significant computational
requirements. In this work, we introduce the Latent Neural Cellular Automata
(LNCA) model, a novel architecture designed to address the resource limitations
of neural cellular automata. Our approach shifts the computation from the
conventional input space to a specially designed latent space, relying on a
pre-trained autoencoder. We apply our model in the context of image
restoration, which aims to reconstruct high-quality images from their degraded
versions. This modification not only reduces the model's resource consumption
but also maintains a flexible framework suitable for various applications. Our
model achieves a significant reduction in computational requirements while
maintaining high reconstruction fidelity. This increase in efficiency allows
for inputs up to 16 times larger than current state-of-the-art neural cellular
automata models, using the same resources.
\\ ( https://arxiv.org/abs/2403.15525 ,  114kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15527 (*cross-listing*)
Date: Fri, 22 Mar 2024 15:40:06 GMT   (1013kb,D)

Title: Conformal online model aggregation
Authors: Matteo Gasparin and Aaditya Ramdas
Categories: stat.ML cs.LG
Comments: 15 pages, 8 figures. arXiv admin note: substantial text overlap with
  arXiv:2401.09379
\\
  Conformal prediction equips machine learning models with a reasonable notion
of uncertainty quantification without making strong distributional assumptions.
It wraps around any black-box prediction model and converts point predictions
into set predictions that have a predefined marginal coverage guarantee.
However, conformal prediction only works if we fix the underlying machine
learning model in advance. A relatively unaddressed issue in conformal
prediction is that of model selection and/or aggregation: for a given problem,
which of the plethora of prediction methods (random forests, neural nets,
regularized linear models, etc.) should we conformalize? This paper proposes a
new approach towards conformal model aggregation in online settings that is
based on combining the prediction sets from several algorithms by voting, where
weights on the models are adapted over time based on past performance.
\\ ( https://arxiv.org/abs/2403.15527 ,  1013kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15560 (*cross-listing*)
Date: Fri, 22 Mar 2024 18:31:24 GMT   (589kb)

Title: A2DMN: Anatomy-Aware Dilated Multiscale Network for Breast Ultrasound
  Semantic Segmentation
Authors: Kyle Lucke, Aleksandar Vakanski, Min Xian
Categories: eess.IV cs.CV cs.LG
\\
  In recent years, convolutional neural networks for semantic segmentation of
breast ultrasound (BUS) images have shown great success; however, two major
challenges still exist. 1) Most current approaches inherently lack the ability
to utilize tissue anatomy, resulting in misclassified image regions. 2) They
struggle to produce accurate boundaries due to the repeated down-sampling
operations. To address these issues, we propose a novel breast anatomy-aware
network for capturing fine image details and a new smoothness term that encodes
breast anatomy. It incorporates context information across multiple spatial
scales to generate more accurate semantic boundaries. Extensive experiments are
conducted to compare the proposed method and eight state-of-the-art approaches
using a BUS dataset with 325 images. The results demonstrate the proposed
method significantly improves the segmentation of the muscle, mammary, and
tumor classes and produces more accurate fine details of tissue boundaries.
\\ ( https://arxiv.org/abs/2403.15560 ,  589kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15593 (*cross-listing*)
Date: Fri, 22 Mar 2024 19:41:26 GMT   (1751kb,D)

Title: FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in
  RKHSs
Authors: Sepehr Dehdashtian, Lan Wang, Vishnu Naresh Boddeti
Categories: cs.CV cs.LG
Comments: The Twelfth International Conference on Learning Representations
  (ICLR) 2024
\\
  Large pre-trained vision-language models such as CLIP provide compact and
general-purpose representations of text and images that are demonstrably
effective across multiple downstream zero-shot prediction tasks. However, owing
to the nature of their training process, these models have the potential to 1)
propagate or amplify societal biases in the training data and 2) learn to rely
on spurious features. This paper proposes FairerCLIP, a general approach for
making zero-shot predictions of CLIP more fair and robust to spurious
correlations. We formulate the problem of jointly debiasing CLIP's image and
text representations in reproducing kernel Hilbert spaces (RKHSs), which
affords multiple benefits: 1) Flexibility: Unlike existing approaches, which
are specialized to either learn with or without ground-truth labels, FairerCLIP
is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP
lends itself to an iterative optimization involving closed-form solvers, which
leads to $4\times$-$10\times$ faster training than the existing methods. 3)
Sample Efficiency: Under sample-limited conditions, FairerCLIP significantly
outperforms baselines when they fail entirely. And, 4) Performance:
Empirically, FairerCLIP achieves appreciable accuracy gains on benchmark
fairness and spurious correlation datasets over their respective baselines.
\\ ( https://arxiv.org/abs/2403.15593 ,  1751kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15594 (*cross-listing*)
Date: Fri, 22 Mar 2024 19:53:21 GMT   (4440kb,D)

Title: Analyzing Male Domestic Violence through Exploratory Data Analysis and
  Explainable Machine Learning Insights
Authors: Md Abrar Jahin, Saleh Akram Naife, Fatema Tuj Johora Lima, M. F.
  Mridha, and Jungpil Shin
Categories: cs.CY cs.LG
\\
  Domestic violence, which is often perceived as a gendered issue among female
victims, has gained increasing attention in recent years. Despite this focus,
male victims of domestic abuse remain primarily overlooked, particularly in
Bangladesh. Our study represents a pioneering exploration of the underexplored
realm of male domestic violence (MDV) within the Bangladeshi context, shedding
light on its prevalence, patterns, and underlying factors. Existing literature
predominantly emphasizes female victimization in domestic violence scenarios,
leading to an absence of research on male victims. We collected data from the
major cities of Bangladesh and conducted exploratory data analysis to
understand the underlying dynamics. We implemented 11 traditional machine
learning models with default and optimized hyperparameters, 2 deep learning,
and 4 ensemble models. Despite various approaches, CatBoost has emerged as the
top performer due to its native support for categorical features, efficient
handling of missing values, and robust regularization techniques, achieving 76%
accuracy. In contrast, other models achieved accuracy rates in the range of
58-75%. The eXplainable AI techniques, SHAP and LIME, were employed to gain
insights into the decision-making of black-box machine learning models. By
shedding light on this topic and identifying factors associated with domestic
abuse, the study contributes to identifying groups of people vulnerable to MDV,
raising awareness, and informing policies and interventions aimed at reducing
MDV. Our findings challenge the prevailing notion that domestic abuse primarily
affects women, thus emphasizing the need for tailored interventions and support
systems for male victims. ML techniques enhance the analysis and understanding
of the data, providing valuable insights for developing effective strategies to
combat this pressing social issue.
\\ ( https://arxiv.org/abs/2403.15594 ,  4440kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15598 (*cross-listing*)
Date: Fri, 22 Mar 2024 20:01:53 GMT   (4575kb,D)

Title: An ensemble of data-driven weather prediction models for operational
  sub-seasonal forecasting
Authors: Jonathan A. Weyn, Divya Kumar, Jeremy Berman, Najeeb Kazmi, Sylwester
  Klocek, Pete Luferenko, Kit Thambiratnam
Categories: physics.ao-ph cs.LG
\\
  We present an operations-ready multi-model ensemble weather forecasting
system which uses hybrid data-driven weather prediction models coupled with the
European Centre for Medium-range Weather Forecasts (ECMWF) ocean model to
predict global weather at 1-degree resolution for 4 weeks of lead time. For
predictions of 2-meter temperature, our ensemble on average outperforms the raw
ECMWF extended-range ensemble by 4-17%, depending on the lead time. However,
after applying statistical bias corrections, the ECMWF ensemble is about 3%
better at 4 weeks. For other surface parameters, our ensemble is also within a
few percentage points of ECMWF's ensemble. We demonstrate that it is possible
to achieve near-state-of-the-art subseasonal-to-seasonal forecasts using a
multi-model ensembling approach with data-driven weather prediction models.
\\ ( https://arxiv.org/abs/2403.15598 ,  4575kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15605 (*cross-listing*)
Date: Fri, 22 Mar 2024 20:22:08 GMT   (3242kb,D)

Title: Efficiently Assemble Normalization Layers and Regularization for
  Federated Domain Generalization
Authors: Khiem Le, Long Ho, Cuong Do, Danh Le-Phuoc, Kok-Seng Wong
Categories: cs.CV cs.LG
\\
  Domain shift is a formidable issue in Machine Learning that causes a model to
suffer from performance degradation when tested on unseen domains. Federated
Domain Generalization (FedDG) attempts to train a global model using
collaborative clients in a privacy-preserving manner that can generalize well
to unseen clients possibly with domain shift. However, most existing FedDG
methods either cause additional privacy risks of data leakage or induce
significant costs in client communication and computation, which are major
concerns in the Federated Learning paradigm. To circumvent these challenges,
here we introduce a novel architectural method for FedDG, namely gPerXAN, which
relies on a normalization scheme working with a guiding regularizer. In
particular, we carefully design Personalized eXplicitly Assembled Normalization
to enforce client models selectively filtering domain-specific features that
are biased towards local data while retaining discrimination of those features.
Then, we incorporate a simple yet effective regularizer to guide these models
in directly capturing domain-invariant representations that the global model's
classifier can leverage. Extensive experimental results on two benchmark
datasets, i.e., PACS and Office-Home, and a real-world medical dataset,
Camelyon17, indicate that our proposed method outperforms other existing
methods in addressing this particular problem.
\\ ( https://arxiv.org/abs/2403.15605 ,  3242kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15681 (*cross-listing*)
Date: Sat, 23 Mar 2024 02:13:22 GMT   (2102kb,D)

Title: Differentiable Information Bottleneck for Deterministic Multi-view
  Clustering
Authors: Xiaoqiang Yan and Zhixiang Jin and Fengshou Han and Yangdong Ye
Categories: cs.IT cs.LG math.IT
Comments: 10 pages, 5 figures, cvpr 2024
\\
  In recent several years, the information bottleneck (IB) principle provides
an information-theoretic framework for deep multi-view clustering (MVC) by
compressing multi-view observations while preserving the relevant information
of multiple views. Although existing IB-based deep MVC methods have achieved
huge success, they rely on variational approximation and distribution
assumption to estimate the lower bound of mutual information, which is a
notoriously hard and impractical problem in high-dimensional multi-view spaces.
In this work, we propose a new differentiable information bottleneck (DIB)
method, which provides a deterministic and analytical MVC solution by fitting
the mutual information without the necessity of variational approximation.
Specifically, we first propose to directly fit the mutual information of
high-dimensional spaces by leveraging normalized kernel Gram matrix, which does
not require any auxiliary neural estimator to estimate the lower bound of
mutual information. Then, based on the new mutual information measurement, a
deterministic multi-view neural network with analytical gradients is explicitly
trained to parameterize IB principle, which derives a deterministic compression
of input variables from different views. Finally, a triplet consistency
discovery mechanism is devised, which is capable of mining the feature
consistency, cluster consistency and joint consistency based on the
deterministic and compact representations. Extensive experimental results show
the superiority of our DIB method on 6 benchmarks compared with 13
state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2403.15681 ,  2102kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15734 (*cross-listing*)
Date: Sat, 23 Mar 2024 06:01:45 GMT   (2591kb,D)

Title: Space Group Informed Transformer for Crystalline Materials Generation
Authors: Zhendong Cao, Xiaoshan Luo, Jian Lv and Lei Wang
Categories: cond-mat.mtrl-sci cs.LG physics.comp-ph
Comments: 17 pages, 8 figures
\\
  We introduce CrystalFormer, a transformer-based autoregressive model
specifically designed for space group-controlled generation of crystalline
materials. The space group symmetry significantly simplifies the crystal space,
which is crucial for data and compute efficient generative modeling of
crystalline materials. Leveraging the prominent discrete and sequential nature
of the Wyckoff positions, CrystalFormer learns to generate crystals by directly
predicting the species and locations of symmetry-inequivalent atoms in the unit
cell. Our results demonstrate that CrystalFormer matches state-of-the-art
performance on standard benchmarks for both validity, novelty, and stability of
the generated crystalline materials. Our analysis also shows that CrystalFormer
ingests sensible solid-state chemistry information from data for generative
modeling. The CrystalFormer unifies symmetry-based structure search and
generative pre-training in the realm of crystalline materials. The simplicity,
generality, and flexibility of CrystalFormer position it as a promising
architecture to be the foundational model of the entire crystalline materials
space, heralding a new era in materials modeling and discovery.
\\ ( https://arxiv.org/abs/2403.15734 ,  2591kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15749 (*cross-listing*)
Date: Sat, 23 Mar 2024 07:34:18 GMT   (10kb)

Title: Horoballs and the subgradient method
Authors: Adrian S. Lewis and Genaro Lopez-Acedo and Adriana Nicolae
Categories: math.OC cs.CC cs.LG
MSC-class: 90C48, 65Y20, 49M29
ACM-class: G.1.6
\\
  To explore convex optimization on Hadamard spaces, we consider an iteration
in the style of a subgradient algorithm. Traditionally, such methods assume
that the underlying spaces are manifolds and that the objectives are
geodesically convex: the methods are described using tangent spaces and
exponential maps. By contrast, our iteration applies in a general Hadamard
space, is framed in the underlying space itself, and relies instead on
horospherical convexity of the objective level sets. For this restricted class
of objectives, we prove a complexity result of the usual form. Notably, the
complexity does not depend on a lower bound on the space curvature.
\\ ( https://arxiv.org/abs/2403.15749 ,  10kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15778 (*cross-listing*)
Date: Sat, 23 Mar 2024 09:24:29 GMT   (5566kb,D)

Title: Supervised Learning via Ensembles of Diverse Functional Representations:
  the Functional Voting Classifier
Authors: Donato Riccio and Fabrizio Maturo and Elvira Romano
Categories: stat.ME cs.LG stat.ML
Comments: 35 pages, 20 figures
MSC-class: 46N30, 62-08
\\
  Many conventional statistical and machine learning methods face challenges
when applied directly to high dimensional temporal observations. In recent
decades, Functional Data Analysis (FDA) has gained widespread popularity as a
framework for modeling and analyzing data that are, by their nature, functions
in the domain of time. Although supervised classification has been extensively
explored in recent decades within the FDA literature, ensemble learning of
functional classifiers has only recently emerged as a topic of significant
interest. Thus, the latter subject presents unexplored facets and challenges
from various statistical perspectives. The focal point of this paper lies in
the realm of ensemble learning for functional data and aims to show how
different functional data representations can be used to train ensemble members
and how base model predictions can be combined through majority voting. The
so-called Functional Voting Classifier (FVC) is proposed to demonstrate how
different functional representations leading to augmented diversity can
increase predictive accuracy. Many real-world datasets from several domains are
used to display that the FVC can significantly enhance performance compared to
individual models. The framework presented provides a foundation for voting
ensembles with functional data and can stimulate a highly encouraging line of
research in the FDA context.
\\ ( https://arxiv.org/abs/2403.15778 ,  5566kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15780 (*cross-listing*)
Date: Sat, 23 Mar 2024 09:32:23 GMT   (115kb,D)

Title: A Fairness-Oriented Reinforcement Learning Approach for the Operation
  and Control of Shared Micromobility Services
Authors: Luca Vittorio Piron, Matteo Cederle, Marina Ceccon, Federico
  Chiariotti, Alessandro Fabris, Marco Fabris, and Gian Antonio Susto
Categories: eess.SY cs.LG cs.SY
Comments: 8 pages, 4 figures, submitted to the 63rd Conference on Decision and
  Control, Dec. 16-19, 2024, Milan, Italy
\\
  As Machine Learning systems become increasingly popular across diverse
application domains, including those with direct human implications, the
imperative of equity and algorithmic fairness has risen to prominence in the
Artificial Intelligence community. On the other hand, in the context of Shared
Micromobility Systems, the exploration of fairness-oriented approaches remains
limited. Addressing this gap, we introduce a pioneering investigation into the
balance between performance optimization and algorithmic fairness in the
operation and control of Shared Micromobility Services. Our study leverages the
Q-Learning algorithm in Reinforcement Learning, benefiting from its convergence
guarantees to ensure the robustness of our proposed approach. Notably, our
methodology stands out for its ability to achieve equitable outcomes, as
measured by the Gini index, across different station categories--central,
peripheral, and remote. Through strategic rebalancing of vehicle distribution,
our approach aims to maximize operator performance while simultaneously
upholding fairness principles for users. In addition to theoretical insights,
we substantiate our findings with a case study or simulation based on synthetic
data, validating the efficacy of our approach. This paper underscores the
critical importance of fairness considerations in shaping control strategies
for Shared Micromobility Services, offering a pragmatic framework for enhancing
equity in urban transportation systems.
\\ ( https://arxiv.org/abs/2403.15780 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15999 (*cross-listing*)
Date: Sun, 24 Mar 2024 03:57:21 GMT   (41kb)

Title: Near-Optimal differentially private low-rank trace regression with
  guaranteed private initialization
Authors: Mengyue Zha
Categories: stat.ML cs.LG
\\
  We study differentially private (DP) estimation of a rank-$r$ matrix $M \in
\RR^{d_1\times d_2}$ under the trace regression model with Gaussian measurement
matrices. Theoretically, the sensitivity of non-private spectral initialization
is precisely characterized, and the differential-privacy-constrained minimax
lower bound for estimating $M$ under the Schatten-$q$ norm is established.
Methodologically, the paper introduces a computationally efficient algorithm
for DP-initialization with a sample size of $n \geq \wt O (r^2 (d_1\vee d_2))$.
Under certain regularity conditions, the DP-initialization falls within a local
ball surrounding $M$. We also propose a differentially private algorithm for
estimating $M$ based on Riemannian optimization (DP-RGrad), which achieves a
near-optimal convergence rate with the DP-initialization and sample size of $n
\geq \wt O(r (d_1 + d_2))$. Finally, the paper discusses the non-trivial gap
between the minimax lower bound and the upper bound of low-rank matrix
estimation under the trace regression model. It is shown that the estimator
given by DP-RGrad attains the optimal convergence rate in a weaker notion of
differential privacy. Our powerful technique for analyzing the sensitivity of
initialization requires no eigengap condition between $r$ non-zero singular
values.
\\ ( https://arxiv.org/abs/2403.15999 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16028 (*cross-listing*)
Date: Sun, 24 Mar 2024 06:10:22 GMT   (13082kb,D)

Title: Exploring the Impact of Dataset Bias on Dataset Distillation
Authors: Yao Lu, Jianyang Gu, Xuguang Chen, Saeed Vahidian, Qi Xuan
Categories: cs.CV cs.LG
\\
  Dataset Distillation (DD) is a promising technique to synthesize a smaller
dataset that preserves essential information from the original dataset. This
synthetic dataset can serve as a substitute for the original large-scale one,
and help alleviate the training workload. However, current DD methods typically
operate under the assumption that the dataset is unbiased, overlooking
potential bias issues within the dataset itself. To fill in this blank, we
systematically investigate the influence of dataset bias on DD. To the best of
our knowledge, this is the first exploration in the DD domain. Given that there
are no suitable biased datasets for DD, we first construct two biased datasets,
CMNIST-DD and CCIFAR10-DD, to establish a foundation for subsequent analysis.
Then we utilize existing DD methods to generate synthetic datasets on CMNIST-DD
and CCIFAR10-DD, and evaluate their performance following the standard process.
Experiments demonstrate that biases present in the original dataset
significantly impact the performance of the synthetic dataset in most cases,
which highlights the necessity of identifying and mitigating biases in the
original datasets during DD. Finally, we reformulate DD within the context of a
biased dataset. Our code along with biased datasets are available at
https://github.com/yaolu-zjut/Biased-DD.
\\ ( https://arxiv.org/abs/2403.16028 ,  13082kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16031 (*cross-listing*)
Date: Sun, 24 Mar 2024 06:14:50 GMT   (418kb,D)

Title: Learning Directed Acyclic Graphs from Partial Orderings
Authors: Ali Shojaie and Wenyu Chen
Categories: stat.ML cs.LG stat.ME
Comments: 29 pages, 5 figures
\\
  Directed acyclic graphs (DAGs) are commonly used to model causal
relationships among random variables. In general, learning the DAG structure is
both computationally and statistically challenging. Moreover, without
additional information, the direction of edges may not be estimable from
observational data. In contrast, given a complete causal ordering of the
variables, the problem can be solved efficiently, even in high dimensions. In
this paper, we consider the intermediate problem of learning DAGs when a
partial causal ordering of variables is available. We propose a general
estimation framework for leveraging the partial ordering and present efficient
estimation algorithms for low- and high-dimensional problems. The advantages of
the proposed framework are illustrated via numerical studies.
\\ ( https://arxiv.org/abs/2403.16031 ,  418kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16059 (*cross-listing*)
Date: Sun, 24 Mar 2024 08:06:34 GMT   (1013kb,D)

Title: Manifold Regularization Classification Model Based On Improved Diffusion
  Map
Authors: Hongfu Guo, Wencheng Zou, Zeyu Zhang, Shuishan Zhang, Ruitong Wang,
  Jintao Zhang
Categories: stat.ML cs.LG math.OC
Comments: 20 pages, 24figures
\\
  Manifold regularization model is a semi-supervised learning model that
leverages the geometric structure of a dataset, comprising a small number of
labeled samples and a large number of unlabeled samples, to generate
classifiers. However, the original manifold norm limits the performance of
models to local regions. To address this limitation, this paper proposes an
approach to improve manifold regularization based on a label propagation model.
We initially enhance the probability transition matrix of the diffusion map
algorithm, which can be used to estimate the Neumann heat kernel, enabling it
to accurately depict the label propagation process on the manifold. Using this
matrix, we establish a label propagation function on the dataset to describe
the distribution of labels at different time steps. Subsequently, we extend the
label propagation function to the entire data manifold. We prove that the
extended label propagation function converges to a stable distribution after a
sufficiently long time and can be considered as a classifier. Building upon
this concept, we propose a viable improvement to the manifold regularization
model and validate its superiority through experiments.
\\ ( https://arxiv.org/abs/2403.16059 ,  1013kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16125 (*cross-listing*)
Date: Sun, 24 Mar 2024 12:43:04 GMT   (816kb,D)

Title: A Codesign of Scheduling and Parallelization for Large Model Training in
  Heterogeneous Clusters
Authors: Chunyu Xue, Weihao Cui, Han Zhao, Quan Chen, Shulai Zhang, Pengyu
  Yang, Jing Yang, Shaobo Li, Minyi Guo
Categories: cs.DC cs.LG
\\
  Joint consideration of scheduling and adaptive parallelism offers great
opportunities for improving the training efficiency of large models on
heterogeneous GPU clusters. However, integrating adaptive parallelism into a
cluster scheduler expands the cluster scheduling space. The new space is the
product of the original scheduling space and the parallelism exploration space
of adaptive parallelism (also a product of pipeline, data, and tensor
parallelism). The exponentially enlarged scheduling space and ever-changing
optimal parallelism plan from adaptive parallelism together result in the
contradiction between low-overhead and accurate performance data acquisition
for efficient cluster scheduling. This paper presents Crius, a training system
for efficiently scheduling multiple large models with adaptive parallelism in a
heterogeneous cluster. Crius proposes a novel scheduling granularity called
Cell. It represents a job with deterministic resources and pipeline stages. The
exploration space of Cell is shrunk to the product of only data and tensor
parallelism, thus exposing the potential for accurate and low-overhead
performance estimation. Crius then accurately estimates Cells and efficiently
schedules training jobs. When a Cell is selected as a scheduling choice, its
represented job runs with the optimal parallelism plan explored. Experimental
results show that Crius reduces job completion time by up to 48.9% and
schedules large models with up to 1.49x cluster throughput improvement.
\\ ( https://arxiv.org/abs/2403.16125 ,  816kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16132 (*cross-listing*)
Date: Sun, 24 Mar 2024 13:03:27 GMT   (1866kb)

Title: Runtime Monitoring and Fault Detection for Neural Network-Controlled
  Systems
Authors: Jianglin Lan, Siyuan Zhan, Ron Patton, Xianxian Zhao
Categories: eess.SY cs.LG cs.SY
Comments: Accepted to SAFEPROCESS 2024
\\
  There is an emerging trend in applying deep learning methods to control
complex nonlinear systems. This paper considers enhancing the runtime safety of
nonlinear systems controlled by neural networks in the presence of disturbance
and measurement noise. A robustly stable interval observer is designed to
generate sound and precise lower and upper bounds for the neural network,
nonlinear function, and system state. The obtained interval is utilised to
monitor the real-time system safety and detect faults in the system outputs or
actuators. An adaptive cruise control vehicular system is simulated to
demonstrate effectiveness of the proposed design.
\\ ( https://arxiv.org/abs/2403.16132 ,  1866kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16143 (*cross-listing*)
Date: Sun, 24 Mar 2024 13:31:31 GMT   (8752kb,D)

Title: CFAT: Unleashing TriangularWindows for Image Super-resolution
Authors: Abhisek Ray, Gaurav Kumar, and Maheshkumar H. Kolekar
Categories: eess.IV cs.CV cs.LG cs.MM
Comments: Accepted to CVPR 2024
\\
  Transformer-based models have revolutionized the field of image
super-resolution (SR) by harnessing their inherent ability to capture complex
contextual features. The overlapping rectangular shifted window technique used
in transformer architecture nowadays is a common practice in super-resolution
models to improve the quality and robustness of image upscaling. However, it
suffers from distortion at the boundaries and has limited unique shifting
modes. To overcome these weaknesses, we propose a non-overlapping triangular
window technique that synchronously works with the rectangular one to mitigate
boundary-level distortion and allows the model to access more unique sifting
modes. In this paper, we propose a Composite Fusion Attention Transformer
(CFAT) that incorporates triangular-rectangular window-based local attention
with a channel-based global attention technique in image super-resolution. As a
result, CFAT enables attention mechanisms to be activated on more image pixels
and captures long-range, multi-scale features to improve SR performance. The
extensive experimental results and ablation study demonstrate the effectiveness
of CFAT in the SR domain. Our proposed model shows a significant 0.7 dB
performance improvement over other state-of-the-art SR architectures.
\\ ( https://arxiv.org/abs/2403.16143 ,  8752kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16144 (*cross-listing*)
Date: Sun, 24 Mar 2024 13:32:42 GMT   (1406kb,D)

Title: Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural
  Network Approach
Authors: Diego A. de Aguiar and Hugo L. Fran\c{c}a and Cassio M. Oishi
Categories: physics.flu-dyn cs.LG
\\
  Neural networks in fluid mechanics offer an efficient approach for exploring
complex flows, including multiphase and free surface flows. The recurrent
neural network, particularly the Long Short-Term Memory (LSTM) model, proves
attractive for learning mappings from transient inputs to dynamic outputs. This
study applies LSTM to predict transient and static outputs for fluid flows
under surface tension effects. Specifically, we explore two distinct droplet
dynamic scenarios: droplets with diverse initial shapes impacting with solid
surfaces, as well as the coalescence of two droplets following collision. Using
only dimensionless numbers and geometric time series data from numerical
simulations, LSTM predicts the energy budget. The marker-and-cell
front-tracking methodology combined with a marker-and-cell finite-difference
strategy is adopted for simulating the droplet dynamics. Using a recurrent
neural network (RNN) architecture fed with time series data derived from
geometrical parameters, as for example droplet diameter variation, our study
shows the accuracy of our approach in predicting energy budgets, as for
instance the kinetic, dissipation, and surface energy trends, across a range of
Reynolds and Weber numbers in droplet dynamic problems. Finally, a two-phase
sequential neural network using only geometric data, which is readily available
in experimental settings, is employed to predict the energies and then use them
to estimate static parameters, such as the Reynolds and Weber numbers. While
our methodology has been primarily validated with simulation data, its
adaptability to experimental datasets is a promising avenue for future
exploration. We hope that our strategy can be useful for diverse applications,
spanning from inkjet printing to combustion engines, where the prediction of
energy budgets or dissipation energies is crucial.
\\ ( https://arxiv.org/abs/2403.16144 ,  1406kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16208 (*cross-listing*)
Date: Sun, 24 Mar 2024 16:05:57 GMT   (28kb)

Title: Convergence analysis of OT-Flow for sample generation
Authors: Yang Jing and Lei Li
Categories: math.NA cs.LG cs.NA
\\
  Deep generative models aim to learn the underlying distribution of data and
generate new ones. Despite the diversity of generative models and their
high-quality generation performance in practice, most of them lack rigorous
theoretical convergence proofs. In this work, we aim to establish some
convergence results for OT-Flow, one of the deep generative models. First, by
reformulating the framework of OT-Flow model, we establish the
$\Gamma$-convergence of the formulation of OT-flow to the corresponding optimal
transport (OT) problem as the regularization term parameter $\alpha$ goes to
infinity. Second, since the loss function will be approximated by Monte Carlo
method in training, we established the convergence between the discrete loss
function and the continuous one when the sample number $N$ goes to infinity as
well. Meanwhile, the approximation capability of the neural network provides an
upper bound for the discrete loss function of the minimizers. The proofs in
both aspects provide convincing assurances for OT-Flow.
\\ ( https://arxiv.org/abs/2403.16208 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16212 (*cross-listing*)
Date: Sun, 24 Mar 2024 16:11:27 GMT   (1273kb,D)

Title: Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI
  Classification in Alzheimer Diagnosis
Authors: Shaojie Li, Haichen Qu, Xinqi Dong, Bo Dang, Hengyi Zang, and Yulu
  Gong
Categories: eess.IV cs.CV cs.LG
\\
  Exploring the application of deep learning technologies in the field of
medical diagnostics, Magnetic Resonance Imaging (MRI) provides a unique
perspective for observing and diagnosing complex neurodegenerative diseases
such as Alzheimer Disease (AD). With advancements in deep learning,
particularly in Convolutional Neural Networks (CNNs) and the Xception network
architecture, we are now able to analyze and classify vast amounts of MRI data
with unprecedented accuracy. The progress of this technology not only enhances
our understanding of brain structural changes but also opens up new avenues for
monitoring disease progression through non-invasive means and potentially
allows for precise diagnosis in the early stages of the disease.
  This study aims to classify MRI images using deep learning models to identify
different stages of Alzheimer Disease through a series of innovative data
processing and model construction steps. Our experimental results show that the
deep learning framework based on the Xception model achieved a 99.6% accuracy
rate in the multi-class MRI image classification task, demonstrating its
potential application value in assistive diagnosis. Future research will focus
on expanding the dataset, improving model interpretability, and clinical
validation to further promote the application of deep learning technology in
the medical field, with the hope of bringing earlier diagnosis and more
personalized treatment plans to Alzheimer Disease patients.
\\ ( https://arxiv.org/abs/2403.16212 ,  1273kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16258 (*cross-listing*)
Date: Sun, 24 Mar 2024 18:33:16 GMT   (34921kb,D)

Title: Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated
  Synthesis
Authors: Atefeh Khoshkhahtinat, Ali Zafari, Piyush M. Mehta, Nasser M.
  Nasrabadi
Categories: eess.IV cs.CV cs.IT cs.LG math.IT
Comments: Accepted by CVPR2024
\\
  While replacing Gaussian decoders with a conditional diffusion model enhances
the perceptual quality of reconstructions in neural image compression, their
lack of inductive bias for image data restricts their ability to achieve
state-of-the-art perceptual levels. To address this limitation, we adopt a
non-isotropic diffusion model at the decoder side. This model imposes an
inductive bias aimed at distinguishing between frequency contents, thereby
facilitating the generation of high-quality images. Moreover, our framework is
equipped with a novel entropy model that accurately models the probability
distribution of latent representation by exploiting spatio-channel correlations
in latent space, while accelerating the entropy decoding step. This
channel-wise entropy model leverages both local and global spatial contexts
within each channel chunk. The global spatial context is built upon the
Transformer, which is specifically designed for image compression tasks. The
designed Transformer employs a Laplacian-shaped positional encoding, the
learnable parameters of which are adaptively adjusted for each channel cluster.
Our experiments demonstrate that our proposed framework yields better
perceptual quality compared to cutting-edge generative-based codecs, and the
proposed entropy model contributes to notable bitrate savings.
\\ ( https://arxiv.org/abs/2403.16258 ,  34921kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16317 (*cross-listing*)
Date: Sun, 24 Mar 2024 22:42:40 GMT   (136kb,D)

Title: Optimization on a Finer Scale: Bounded Local Subgradient Variation
  Perspective
Authors: Jelena Diakonikolas and Crist\'obal Guzm\'an
Categories: math.OC cs.DS cs.LG
\\
  We initiate the study of nonsmooth optimization problems under bounded local
subgradient variation, which postulates bounded difference between
(sub)gradients in small local regions around points, in either average or
maximum sense. The resulting class of objective functions encapsulates the
classes of objective functions traditionally studied in optimization, which are
defined based on either Lipschitz continuity of the objective or
H\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class
contains functions that are neither Lipschitz continuous nor have a H\"{o}lder
continuous gradient. When restricted to the traditional classes of optimization
problems, the parameters defining the studied classes lead to more fine-grained
complexity bounds, recovering traditional oracle complexity bounds in the worst
case but generally leading to lower oracle complexity for functions that are
not ``worst case.'' Some highlights of our results are that: (i) it is possible
to obtain complexity results for both convex and nonconvex problems with the
(local or global) Lipschitz constant being replaced by a constant of local
subgradient variation and (ii) mean width of the subdifferential set around the
optima plays a role in the complexity of nonsmooth optimization, particularly
in parallel settings. A consequence of (ii) is that for any error parameter
$\epsilon > 0$, parallel oracle complexity of nonsmooth Lipschitz convex
optimization is lower than its sequential oracle complexity by a factor
$\tilde{\Omega}\big(\frac{1}{\epsilon}\big)$ whenever the objective function is
piecewise linear with polynomially many pieces in the input size. This is
particularly surprising as existing parallel complexity lower bounds are based
on such classes of functions. The seeming contradiction is resolved by
considering the region in which the algorithm is allowed to query the
objective.
\\ ( https://arxiv.org/abs/2403.16317 ,  136kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16331 (*cross-listing*)
Date: Sun, 24 Mar 2024 23:50:15 GMT   (200kb,D)

Title: Modeling Analog Dynamic Range Compressors using Deep Learning and
  State-space Models
Authors: Hanzhi Yin, Gang Cheng, Christian J. Steinmetz, Ruibin Yuan, Richard
  M. Stern and Roger B. Dannenberg
Categories: cs.SD cs.LG eess.SP
\\
  We describe a novel approach for developing realistic digital models of
dynamic range compressors for digital audio production by analyzing their
analog prototypes. While realistic digital dynamic compressors are potentially
useful for many applications, the design process is challenging because the
compressors operate nonlinearly over long time scales. Our approach is based on
the structured state space sequence model (S4), as implementing the state-space
model (SSM) has proven to be efficient at learning long-range dependencies and
is promising for modeling dynamic range compressors. We present in this paper a
deep learning model with S4 layers to model the Teletronix LA-2A analog dynamic
range compressor. The model is causal, executes efficiently in real time, and
achieves roughly the same quality as previous deep-learning models but with
fewer parameters.
\\ ( https://arxiv.org/abs/2403.16331 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16335 (*cross-listing*)
Date: Mon, 25 Mar 2024 00:17:43 GMT   (3567kb,D)

Title: MEDDAP: Medical Dataset Enhancement via Diversified Augmentation
  Pipeline
Authors: Yasamin Medghalchi, Niloufar Zakariaei, Arman Rahmim, Ilker
  Hacihaliloglu
Categories: eess.IV cs.CV cs.LG
Comments: submitted to miccai 2024 submitted to miccai 2024 Submitted to
  MICCAI-2024
\\
  The effectiveness of Deep Neural Networks (DNNs) heavily relies on the
abundance and accuracy of available training data. However, collecting and
annotating data on a large scale is often both costly and time-intensive,
particularly in medical cases where practitioners are already occupied with
their duties. Moreover, ensuring that the model remains robust across various
scenarios of image capture is crucial in medical domains, especially when
dealing with ultrasound images that vary based on the settings of different
devices and the manual operation of the transducer. To address this challenge,
we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion
(SD) models to augment existing small datasets by automatically generating new
informative labeled samples. Pretrained checkpoints for SD are typically based
on natural images, and training them for medical images requires significant
GPU resources due to their heavy parameters. To overcome this challenge, we
introduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method
tailored specifically for ultrasound applications. USLoRA allows for selective
fine-tuning of weights within SD, requiring fewer than 0.1\% of parameters
compared to fully fine-tuning only the UNet portion of SD. To enhance dataset
diversity, we incorporate different adjectives into the generation process
prompts, thereby desensitizing the classifiers to intensity changes across
different images. This approach is inspired by clinicians' decision-making
processes regarding breast tumors, where tumor shape often plays a more crucial
role than intensity. In conclusion, our pipeline not only outperforms
classifiers trained on the original dataset but also demonstrates superior
performance when encountering unseen datasets. The source code is available at
https://github.com/yasamin-med/MEDDAP.
\\ ( https://arxiv.org/abs/2403.16335 ,  3567kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16336 (*cross-listing*)
Date: Mon, 25 Mar 2024 00:21:34 GMT   (242kb,D)

Title: Predictive Inference in Multi-environment Scenarios
Authors: John C. Duchi, Suyash Gupta, Kuanhao Jiang, Pragya Sur
Categories: stat.ML cs.LG math.ST stat.ME stat.TH
\\
  We address the challenge of constructing valid confidence intervals and sets
in problems of prediction across multiple environments. We investigate two
types of coverage suitable for these problems, extending the jackknife and
split-conformal methods to show how to obtain distribution-free coverage in
such non-traditional, hierarchical data-generating scenarios. Our contributions
also include extensions for settings with non-real-valued responses and a
theory of consistency for predictive inference in these general problems. We
demonstrate a novel resizing method to adapt to problem difficulty, which
applies both to existing approaches for predictive inference with hierarchical
data and the methods we develop; this reduces prediction set sizes using
limited information from the test environment, a key to the methods' practical
performance, which we evaluate through neurochemical sensing and species
classification datasets.
\\ ( https://arxiv.org/abs/2403.16336 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16391 (*cross-listing*)
Date: Mon, 25 Mar 2024 03:13:56 GMT   (502kb)

Title: Physics-informed RL for Maximal Safety Probability Estimation
Authors: Hikaru Hoshino and Yorie Nakahira
Categories: eess.SY cs.LG cs.SY
\\
  Accurate risk quantification and reachability analysis are crucial for safe
control and learning, but sampling from rare events, risky states, or long-term
trajectories can be prohibitively costly. Motivated by this, we study how to
estimate the long-term safety probability of maximally safe actions without
sufficient coverage of samples from risky states and long-term trajectories.
The use of maximal safety probability in control and learning is expected to
avoid conservative behaviors due to over-approximation of risk. Here, we first
show that long-term safety probability, which is multiplicative in time, can be
converted into additive costs and be solved using standard reinforcement
learning methods. We then derive this probability as solutions of partial
differential equations (PDEs) and propose Physics-Informed Reinforcement
Learning (PIRL) algorithm. The proposed method can learn using sparse rewards
because the physics constraints help propagate risk information through
neighbors. This suggests that, for the purpose of extracting more information
for efficient learning, physics constraints can serve as an alternative to
reward shaping. The proposed method can also estimate long-term risk using
short-term samples and deduce the risk of unsampled states. This feature is in
stark contrast with the unconstrained deep RL that demands sufficient data
coverage. These merits of the proposed method are demonstrated in numerical
simulation.
\\ ( https://arxiv.org/abs/2403.16391 ,  502kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16439 (*cross-listing*)
Date: Mon, 25 Mar 2024 05:58:33 GMT   (20396kb,D)

Title: Producing and Leveraging Online Map Uncertainty in Trajectory Prediction
Authors: Xunjiang Gu, Guanyu Song, Igor Gilitschenski, Marco Pavone, Boris
  Ivanovic
Categories: cs.RO cs.CV cs.LG
Comments: 14 pages, 14 figures, 6 tables. CVPR 2024
\\
  High-definition (HD) maps have played an integral role in the development of
modern autonomous vehicle (AV) stacks, albeit with high associated labeling and
maintenance costs. As a result, many recent works have proposed methods for
estimating HD maps online from sensor data, enabling AVs to operate outside of
previously-mapped regions. However, current online map estimation approaches
are developed in isolation of their downstream tasks, complicating their
integration in AV stacks. In particular, they do not produce uncertainty or
confidence estimates. In this work, we extend multiple state-of-the-art online
map estimation methods to additionally estimate uncertainty and show how this
enables more tightly integrating online mapping with trajectory forecasting. In
doing so, we find that incorporating uncertainty yields up to 50% faster
training convergence and up to 15% better prediction performance on the
real-world nuScenes driving dataset.
\\ ( https://arxiv.org/abs/2403.16439 ,  20396kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16464 (*cross-listing*)
Date: Mon, 25 Mar 2024 06:46:27 GMT   (112kb,D)

Title: Training Generative Adversarial Network-Based Vocoder with Limited Data
  Using Augmentation-Conditional Discriminator
Authors: Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka
Categories: cs.SD cs.LG eess.AS
Comments: Accepted to ICASSP 2024. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/
\\
  A generative adversarial network (GAN)-based vocoder trained with an
adversarial discriminator is commonly used for speech synthesis because of its
fast, lightweight, and high-quality characteristics. However, this data-driven
model requires a large amount of training data incurring high data-collection
costs. This fact motivates us to train a GAN-based vocoder on limited data. A
promising solution is to augment the training data to avoid overfitting.
However, a standard discriminator is unconditional and insensitive to
distributional changes caused by data augmentation. Thus, augmented speech
(which can be extraordinary) may be considered real speech. To address this
issue, we propose an augmentation-conditional discriminator (AugCondD) that
receives the augmentation state as input in addition to speech, thereby
assessing the input speech according to the augmentation state, without
inhibiting the learning of the original non-augmented distribution.
Experimental results indicate that AugCondD improves speech quality under
limited data conditions while achieving comparable speech quality under
sufficient data conditions. Audio samples are available at
https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.
\\ ( https://arxiv.org/abs/2403.16464 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16497 (*cross-listing*)
Date: Mon, 25 Mar 2024 07:29:18 GMT   (935kb,D)

Title: PathoTune: Adapting Visual Foundation Model to Pathological Specialists
Authors: Jiaxuan Lu, Fang Yan, Xiaofan Zhang, Yue Gao, Shaoting Zhang
Categories: cs.CV cs.LG
Comments: Submitted to MICCAI 2024
\\
  As natural image understanding moves towards the pretrain-finetune era,
research in pathology imaging is concurrently evolving. Despite the predominant
focus on pretraining pathological foundation models, how to adapt foundation
models to downstream tasks is little explored. For downstream adaptation, we
propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the
Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework
designed to efficiently adapt pathological or even visual foundation models to
pathology-specific tasks via multi-modal prompt tuning. The proposed framework
leverages Task-specific Visual Prompts and Task-specific Textual Prompts to
identify task-relevant features, along with Instance-specific Visual Prompts
for encoding single pathological image features. Results across multiple
datasets at both patch-level and WSI-level demonstrate its superior performance
over single-modality prompt tuning approaches. Significantly, PathoTune
facilitates the direct adaptation of natural visual foundation models to
pathological tasks, drastically outperforming pathological foundation models
with simple linear probing. The code will be available upon acceptance.
\\ ( https://arxiv.org/abs/2403.16497 ,  935kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16576 (*cross-listing*)
Date: Mon, 25 Mar 2024 09:41:49 GMT   (4287kb,D)

Title: Antigen-Specific Antibody Design via Direct Energy-based Preference
  Optimization
Authors: Xiangxin Zhou, Dongyu Xue, Ruizhe Chen, Zaixiang Zheng, Liang Wang,
  Quanquan Gu
Categories: q-bio.BM cs.LG
\\
  Antibody design, a crucial task with significant implications across various
disciplines such as therapeutics and biology, presents considerable challenges
due to its intricate nature. In this paper, we tackle antigen-specific antibody
design as a protein sequence-structure co-design problem, considering both
rationality and functionality. Leveraging a pre-trained conditional diffusion
model that jointly models sequences and structures of
complementarity-determining regions (CDR) in antibodies with equivariant neural
networks, we propose direct energy-based preference optimization to guide the
generation of antibodies with both rational structures and considerable binding
affinities to given antigens. Our method involves fine-tuning the pre-trained
diffusion model using a residue-level decomposed energy preference.
Additionally, we employ gradient surgery to address conflicts between various
types of energy, such as attraction and repulsion. Experiments on RAbD
benchmark show that our approach effectively optimizes the energy of generated
antibodies and achieves state-of-the-art performance in designing high-quality
antibodies with low total energy and high binding affinity, demonstrating the
superiority of our approach.
\\ ( https://arxiv.org/abs/2403.16576 ,  4287kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16594 (*cross-listing*)
Date: Mon, 25 Mar 2024 10:13:52 GMT   (4381kb,D)

Title: EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for
  Medical Image Segmentation
Authors: Kudaibergen Abutalip, Numan Saeed, Ikboljon Sobirov, Vincent
  Andrearczyk, Adrien Depeursinge, and Mohammad Yaqub
Categories: eess.IV cs.CV cs.LG
\\
  Deploying deep learning (DL) models in medical applications relies on
predictive performance and other critical factors, such as conveying
trustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide
potential solutions for evaluating prediction reliability and improving the
model confidence calibration. Despite increasing interest in UE, challenges
persist, such as the need for explicit methods to capture aleatoric uncertainty
and align uncertainty estimates with real-life disagreements among domain
experts. This paper proposes an Expert Disagreement-Guided Uncertainty
Estimation (EDUE) for medical image segmentation. By leveraging variability in
ground-truth annotations from multiple raters, we guide the model during
training and incorporate random sampling-based strategies to enhance
calibration confidence. Our method achieves 55% and 23% improvement in
correlation on average with expert disagreements at the image and pixel levels,
respectively, better calibration, and competitive segmentation performance
compared to the state-of-the-art deep ensembles, requiring only a single
forward pass.
\\ ( https://arxiv.org/abs/2403.16594 ,  4381kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16610 (*cross-listing*)
Date: Mon, 25 Mar 2024 10:40:04 GMT   (18kb)

Title: Distributed collaborative anomalous sound detection by embedding sharing
Authors: Kota Dohi and Yohei Kawaguchi
Categories: eess.AS cs.CR cs.LG cs.SD
\\
  To develop a machine sound monitoring system, a method for detecting
anomalous sound is proposed. In this paper, we explore a method for multiple
clients to collaboratively learn an anomalous sound detection model while
keeping their raw data private from each other. In the context of industrial
machine anomalous sound detection, each client possesses data from different
machines or different operational states, making it challenging to learn
through federated learning or split learning. In our proposed method, each
client calculates embeddings using a common pre-trained model developed for
sound data classification, and these calculated embeddings are aggregated on
the server to perform anomalous sound detection through outlier exposure.
Experiments showed that our proposed method improves the AUC of anomalous sound
detection by an average of 6.8%.
\\ ( https://arxiv.org/abs/2403.16610 ,  18kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16640 (*cross-listing*)
Date: Mon, 25 Mar 2024 11:28:52 GMT   (12027kb,D)

Title: Multi-Scale Texture Loss for CT denoising with GANs
Authors: Francesco Di Feola, Lorenzo Tronchin, Valerio Guarrasi, Paolo Soda
Categories: eess.IV cs.CV cs.LG
\\
  Generative Adversarial Networks (GANs) have proved as a powerful framework
for denoising applications in medical imaging. However, GAN-based denoising
algorithms still suffer from limitations in capturing complex relationships
within the images. In this regard, the loss function plays a crucial role in
guiding the image generation process, encompassing how much a synthetic image
differs from a real image. To grasp highly complex and non-linear textural
relationships in the training process, this work presents a loss function that
leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence
Matrix (GLCM). Although the recent advances in deep learning have demonstrated
superior performance in classification and detection tasks, we hypothesize that
its information content can be valuable when integrated into GANs' training. To
this end, we propose a differentiable implementation of the GLCM suited for
gradient-based optimization. Our approach also introduces a self-attention
layer that dynamically aggregates the multi-scale texture information extracted
from the images. We validate our approach by carrying out extensive experiments
in the context of low-dose CT denoising, a challenging application that aims to
enhance the quality of noisy CT scans. We utilize three publicly available
datasets, including one simulated and two real datasets. The results are
promising as compared to other well-established loss functions, being also
consistent across three different GAN architectures. The code is available at:
https://github.com/FrancescoDiFeola/DenoTextureLoss
\\ ( https://arxiv.org/abs/2403.16640 ,  12027kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16644 (*cross-listing*)
Date: Mon, 25 Mar 2024 11:29:32 GMT   (8594kb,D)

Title: Bridging the Sim-to-Real Gap with Bayesian Inference
Authors: Jonas Rothfuss, Bhavya Sukhija, Lenart Treven, Florian D\"orfler,
  Stelian Coros, Andreas Krause
Categories: cs.RO cs.LG
\\
  We present SIM-FSVGD for learning robot dynamics from data. As opposed to
traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in
the form of simulators, to regularize the training of neural network models.
While learning accurate dynamics already in the low data regime, SIM-FSVGD
scales and excels also when more data is available. We empirically show that
learning with implicit physical priors results in accurate mean model
estimation as well as precise uncertainty quantification. We demonstrate the
effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a
high-performance RC racecar system. Using model-based RL, we demonstrate a
highly dynamic parking maneuver with drifting, using less than half the data
compared to the state of the art.
\\ ( https://arxiv.org/abs/2403.16644 ,  8594kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16678 (*cross-listing*)
Date: Mon, 25 Mar 2024 12:15:42 GMT   (1375kb)

Title: DeepGleason: a System for Automated Gleason Grading of Prostate Cancer
  using Deep Neural Networks
Authors: Dominik M\"uller, Philip Meyer, Lukas Rentschler, Robin Manz, Jonas
  B\"acker, Samantha Cramer, Christoph Wengenmayr, Bruno M\"arkl, Ralf Huss,
  I\~naki Soto-Rey, Johannes Raffler
Categories: eess.IV cs.CV cs.LG q-bio.TO
\\
  Advances in digital pathology and artificial intelligence (AI) offer
promising opportunities for clinical decision support and enhancing diagnostic
workflows. Previous studies already demonstrated AI's potential for automated
Gleason grading, but lack state-of-the-art methodology and model reusability.
To address this issue, we propose DeepGleason: an open-source deep neural
network based image classification system for automated Gleason grading using
whole-slide histopathology images from prostate tissue sections. Implemented
with the standardized AUCMEDI framework, our tool employs a tile-wise
classification approach utilizing fine-tuned image preprocessing techniques in
combination with a ConvNeXt architecture which was compared to various
state-of-the-art architectures. The neural network model was trained and
validated on an in-house dataset of 34,264 annotated tiles from 369 prostate
carcinoma slides. We demonstrated that DeepGleason is capable of highly
accurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,
AUC of 0.991, and Accuracy of 0.974. The internal architecture comparison
revealed that the ConvNeXt model was superior performance-wise on our dataset
to established and other modern architectures like transformers. Furthermore,
we were able to outperform the current state-of-the-art in tile-wise
fine-classification with a sensitivity and specificity of 0.94 and 0.98 for
benign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs
Gleason 4 & 5 classification, respectively. Our tool contributes to the wider
adoption of AI-based Gleason grading within the research community and paves
the way for broader clinical application of deep learning models in digital
pathology. DeepGleason is open-source and publicly available for research
application in the following Git repository:
https://github.com/frankkramer-lab/DeepGleason.
\\ ( https://arxiv.org/abs/2403.16678 ,  1375kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16681 (*cross-listing*)
Date: Mon, 25 Mar 2024 12:15:55 GMT   (354kb,D)

Title: A note on generalization bounds for losses with finite moments
Authors: Borja Rodr\'iguez-G\'alvez, Omar Rivasplata, Ragnar Thobaben, and
  Mikael Skoglund
Categories: stat.ML cs.LG
Comments: 9 pages: 5 of main text, 1 of references, and 3 of appendices
\\
  This paper studies the truncation method from Alquier [1] to derive
high-probability PAC-Bayes bounds for unbounded losses with heavy tails.
Assuming that the $p$-th moment is bounded, the resulting bounds interpolate
between a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p
\to \infty$ and the loss is essentially bounded. Moreover, the paper derives a
high-probability PAC-Bayes bound for losses with a bounded variance. This bound
has an exponentially better dependence on the confidence parameter and the
dependency measure than previous bounds in the literature. Finally, the paper
extends all results to guarantees in expectation and single-draw PAC-Bayes. In
order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded
losses from [2] in these settings.
\\ ( https://arxiv.org/abs/2403.16681 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16689 (*cross-listing*)
Date: Mon, 25 Mar 2024 12:23:39 GMT   (8513kb,D)

Title: Synapse: Learning Preferential Concepts from Visual Demonstrations
Authors: Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas
Categories: cs.RO cs.CV cs.LG cs.PL
Comments: 23 pages, 7 figures; Preprint
\\
  This paper addresses the problem of preference learning, which aims to learn
user-specific preferences (e.g., "good parking spot", "convenient drop-off
location") from visual input. Despite its similarity to learning factual
concepts (e.g., "red cube"), preference learning is a fundamentally harder
problem due to its subjective nature and the paucity of person-specific
training data. We address this problem using a new framework called Synapse,
which is a neuro-symbolic approach designed to efficiently learn preferential
concepts from limited demonstrations. Synapse represents preferences as
neuro-symbolic programs in a domain-specific language (DSL) that operates over
images, and leverages a novel combination of visual parsing, large language
models, and program synthesis to learn programs representing individual
preferences. We evaluate Synapse through extensive experimentation including a
user case study focusing on mobility-related concepts in mobile robotics and
autonomous driving. Our evaluation demonstrates that Synapse significantly
outperforms existing baselines as well as its own ablations. The code and other
details can be found on the project website https://amrl.cs.utexas.edu/synapse .
\\ ( https://arxiv.org/abs/2403.16689 ,  8513kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16695 (*cross-listing*)
Date: Mon, 25 Mar 2024 12:26:32 GMT   (543kb)

Title: Assessing the Performance of Deep Learning for Automated Gleason Grading
  in Prostate Cancer
Authors: Dominik M\"uller, Philip Meyer, Lukas Rentschler, Robin Manz, Daniel
  Hieber, Jonas B\"acker, Samantha Cramer, Christoph Wengenmayr, Bruno M\"arkl,
  Ralf Huss, Frank Kramer, I\~naki Soto-Rey, Johannes Raffler
Categories: eess.IV cs.CV cs.LG q-bio.TO
\\
  Prostate cancer is a dominant health concern calling for advanced diagnostic
tools. Utilizing digital pathology and artificial intelligence, this study
explores the potential of 11 deep neural network architectures for automated
Gleason grading in prostate carcinoma focusing on comparing traditional and
recent architectures. A standardized image classification pipeline, based on
the AUCMEDI framework, facilitated robust evaluation using an in-house dataset
consisting of 34,264 annotated tissue tiles. The results indicated varying
sensitivity across architectures, with ConvNeXt demonstrating the strongest
performance. Notably, newer architectures achieved superior performance, even
though with challenges in differentiating closely related Gleason grades. The
ConvNeXt model was capable of learning a balance between complexity and
generalizability. Overall, this study lays the groundwork for enhanced Gleason
grading systems, potentially improving diagnostic efficiency for prostate
cancer.
\\ ( https://arxiv.org/abs/2403.16695 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16776 (*cross-listing*)
Date: Mon, 25 Mar 2024 13:52:48 GMT   (21832kb,D)

Title: Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases
Authors: Sophie Starck, Vasiliki Sideri-Lampretsa, Bernhard Kainz, Martin
  Menten, Tamara Mueller, and Daniel Rueckert
Categories: eess.IV cs.CV cs.LG
\\
  Anatomical atlases are widely used for population analysis. Conditional
atlases target a particular sub-population defined via certain conditions (e.g.
demographics or pathologies) and allow for the investigation of fine-grained
anatomical differences - such as morphological changes correlated with age.
Existing approaches use either registration-based methods that are unable to
handle large anatomical variations or generative models, which can suffer from
training instabilities and hallucinations. To overcome these limitations, we
use latent diffusion models to generate deformation fields, which transform a
general population atlas into one representing a specific sub-population. By
generating a deformation field and registering the conditional atlas to a
neighbourhood of images, we ensure structural plausibility and avoid
hallucinations, which can occur during direct image synthesis. We compare our
method to several state-of-the-art atlas generation methods in experiments
using 5000 brain as well as whole-body MR images from UK Biobank. Our method
generates highly realistic atlases with smooth transformations and high
anatomical fidelity, outperforming the baselines.
\\ ( https://arxiv.org/abs/2403.16776 ,  21832kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16823 (*cross-listing*)
Date: Mon, 25 Mar 2024 14:48:00 GMT   (395kb)

Title: Resource and Mobility Management in Hybrid LiFi and WiFi Networks: A
  User-Centric Learning Approach
Authors: Han Ji, Xiping Wu
Categories: eess.SY cs.LG cs.SY
Comments: 12 pages, 12 figures, 3 tables, submitted to IEEE TWC
\\
  Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks (HLWNets)
are an emerging indoor wireless communication paradigm, which combines the
advantages of the capacious optical spectra of LiFi and ubiquitous coverage of
WiFi. Meanwhile, load balancing (LB) becomes a key challenge in resource
management for such hybrid networks. The existing LB methods are mostly
network-centric, relying on a central unit to make a solution for the users all
at once. Consequently, the solution needs to be updated for all users at the
same pace, regardless of their moving status. This would affect the network
performance in two aspects: i) when the update frequency is low, it would
compromise the connectivity of fast-moving users; ii) when the update frequency
is high, it would cause unnecessary handovers as well as hefty feedback costs
for slow-moving users. Motivated by this, we investigate user-centric LB which
allows users to update their solutions at different paces. The research is
developed upon our previous work on adaptive target-condition neural network
(ATCNN), which can conduct LB for individual users in quasi-static channels. In
this paper, a deep neural network (DNN) model is designed to enable an adaptive
update interval for each individual user. This new model is termed as
mobility-supporting neural network (MSNN). Associating MSNN with ATCNN, a
user-centric LB framework named mobility-supporting ATCNN (MS-ATCNN) is
proposed to handle resource management and mobility management simultaneously.
Results show that at the same level of average update interval, MS-ATCNN can
achieve a network throughput up to 215\% higher than conventional LB methods
such as game theory, especially for a larger number of users. In addition,
MS-ATCNN costs an ultra low runtime at the level of 100s $\mu$s, which is two
to three orders of magnitude lower than game theory.
\\ ( https://arxiv.org/abs/2403.16823 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16855 (*cross-listing*)
Date: Mon, 25 Mar 2024 15:18:23 GMT   (1610kb,D)

Title: Semantic-Aware Remote Estimation of Multiple Markov Sources Under
  Constraints
Authors: Jiping Luo and Nikolaos Pappas
Categories: eess.SY cs.IT cs.LG cs.NI cs.SY math.IT
\\
  This paper studies semantic-aware communication for remote estimation of
multiple Markov sources over a lossy and rate-constrained channel. Unlike most
existing studies that treat all source states equally, we exploit the semantics
of information and consider that the remote actuator has different tolerances
for the estimation errors of different states. We aim to find an optimal
scheduling policy that minimizes the long-term state-dependent costs of
estimation errors under a transmission frequency constraint. We theoretically
show the structure of the optimal policy by leveraging the average-cost
Constrained Markov Decision Process (CMDP) theory and the Lagrangian dynamic
programming. By exploiting the optimal structural results, we develop a novel
policy search algorithm, termed intersection search plus relative value
iteration (Insec-RVI), that can find the optimal policy using only a few
iterations. To avoid the ``curse of dimensionality'' of MDPs, we propose an
online low-complexity drift-plus-penalty (DPP) scheduling algorithm based on
the Lyapunov optimization theorem. We also design an efficient average-cost
Q-learning algorithm to estimate the optimal policy without knowing a priori
the channel and source statistics. Numerical results show that continuous
transmission is inefficient, and remarkably, our semantic-aware policies can
attain the optimum by strategically utilizing fewer transmissions by exploiting
the timing of the important information.
\\ ( https://arxiv.org/abs/2403.16855 ,  1610kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16861 (*cross-listing*)
Date: Mon, 25 Mar 2024 15:26:10 GMT   (40kb)

Title: DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts
Authors: Gabriele Morello, Mojtaba Eshghie, Sofia Bobadilla, Martin Monperrus
Categories: cs.SE cs.DC cs.LG
\\
  The DISL dataset features a collection of $514,506$ unique Solidity files
that have been deployed to Ethereum mainnet. It caters to the need for a large
and diverse dataset of real-world smart contracts. DISL serves as a resource
for developing machine learning systems and for benchmarking software
engineering tools designed for smart contracts. By aggregating every verified
smart contract from Etherscan up to January 15, 2024, DISL surpasses existing
datasets in size and recency.
\\ ( https://arxiv.org/abs/2403.16861 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16862 (*cross-listing*)
Date: Mon, 25 Mar 2024 15:26:32 GMT   (24085kb,D)

Title: INPC: Implicit Neural Point Clouds for Radiance Field Rendering
Authors: Florian Hahlbohm and Linus Franke and Moritz Kappel and Susana
  Castillo and Marc Stamminger and Marcus Magnor
Categories: cs.CV cs.GR cs.LG
Comments: Project page: https://fhahlbohm.github.io/inpc/
\\
  We introduce a new approach for reconstruction and novel-view synthesis of
unbounded real-world scenes. In contrast to previous methods using either
volumetric fields, grid-based models, or discrete point cloud proxies, we
propose a hybrid scene representation, which implicitly encodes a point cloud
in a continuous octree-based probability field and a multi-resolution hash
grid. In doing so, we combine the benefits of both worlds by retaining
favorable behavior during optimization: Our novel implicit point cloud
representation and differentiable bilinear rasterizer enable fast rendering
while preserving fine geometric detail without depending on initial priors like
structure-from-motion point clouds. Our method achieves state-of-the-art image
quality on several common benchmark datasets. Furthermore, we achieve fast
inference at interactive frame rates, and can extract explicit point clouds to
further enhance performance.
\\ ( https://arxiv.org/abs/2403.16862 ,  24085kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16871 (*cross-listing*)
Date: Mon, 25 Mar 2024 15:37:43 GMT   (522kb,D)

Title: Conformal Off-Policy Prediction for Multi-Agent Systems
Authors: Tom Kuipers, Renukanandan Tumu, Shuo Yang, Milad Kazemi, Rahul
  Mangharam and Nicola Paoletti
Categories: cs.MA cs.LG
Comments: Submitted to the 63rd IEEE Conference on Decision and Control (CDC)
\\
  Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy
using only data collected under a nominal (behavioural) policy, is a paramount
problem in data-driven analysis of safety-critical systems where the deployment
of a new policy may be unsafe. To achieve dependable off-policy predictions,
recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal
prediction framework to derive prediction regions with probabilistic guarantees
under the target process. Existing COPP methods can account for the
distribution shifts induced by policy switching, but are limited to
single-agent systems and scalar outcomes (e.g., rewards). In this work, we
introduce MA-COPP, the first conformal prediction method to solve OPP problems
involving multi-agent systems, deriving joint prediction regions for all
agents' trajectories when one or more "ego" agents change their policies.
Unlike the single-agent scenario, this setting introduces higher complexity as
the distribution shifts affect predictions for all agents, not just the ego
agents, and the prediction task involves full multi-dimensional trajectories,
not just reward values. A key contribution of MA-COPP is to avoid enumeration
or exhaustive search of the output space of agent trajectories, which is
instead required by existing COPP methods to construct the prediction region.
We achieve this by showing that an over-approximation of the true JPR can be
constructed, without enumeration, from the maximum density ratio of the JPR
trajectories. We evaluate the effectiveness of MA-COPP in multi-agent systems
from the PettingZoo library and the F1TENTH autonomous racing environment,
achieving nominal coverage in higher dimensions and various shift settings.
\\ ( https://arxiv.org/abs/2403.16871 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16967 (*cross-listing*)
Date: Mon, 25 Mar 2024 17:26:08 GMT   (28573kb,D)

Title: Visual Whole-Body Control for Legged Loco-Manipulation
Authors: Minghuan Liu, Zixuan Chen, Xuxin Cheng, Yandong Ji, Ruihan Yang,
  Xiaolong Wang
Categories: cs.RO cs.CV cs.LG
Comments: The first two authors contribute equally. Project page:
  https://wholebody-b1.github.io
\\
  We study the problem of mobile manipulation using legged robots equipped with
an arm, namely legged loco-manipulation. The robot legs, while usually utilized
for mobility, offer an opportunity to amplify the manipulation capabilities by
conducting whole-body control. That is, the robot can control the legs and the
arm at the same time to extend its workspace. We propose a framework that can
conduct the whole-body control autonomously with visual observations. Our
approach, namely \ourFull~(\our), is composed of a low-level policy using all
degrees of freedom to track the end-effector manipulator position and a
high-level policy proposing the end-effector position based on visual inputs.
We train both levels of policies in simulation and perform Sim2Real transfer
for real robot deployment. We perform extensive experiments and show
significant improvements over baselines in picking up diverse objects in
different configurations (heights, locations, orientations) and environments.
Project page: https://wholebody-b1.github.io
\\ ( https://arxiv.org/abs/2403.16967 ,  28573kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16970 (*cross-listing*)
Date: Mon, 25 Mar 2024 17:31:12 GMT   (2071kb,D)

Title: Joint chest X-ray diagnosis and clinical visual attention prediction
  with multi-stage cooperative learning: enhancing interpretability
Authors: Zirui Qiu, Hassan Rivaz, Yiming Xiao
Categories: eess.IV cs.CV cs.LG
\\
  As deep learning has become the state-of-the-art for computer-assisted
diagnosis, interpretability of the automatic decisions is crucial for clinical
deployment. While various methods were proposed in this domain, visual
attention maps of clinicians during radiological screening offer a unique asset
to provide important insights and can potentially enhance the quality of
computer-assisted diagnosis. With this paper, we introduce a novel
deep-learning framework for joint disease diagnosis and prediction of
corresponding visual saliency maps for chest X-ray scans. Specifically, we
designed a novel dual-encoder multi-task UNet, which leverages both a
DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based
encoder to extract diverse features for saliency map prediction, and a
multi-scale feature-fusion classifier to perform disease classification. To
tackle the issue of asynchronous training schedules of individual tasks in
multi-task learning, we proposed a multi-stage cooperative learning strategy,
with contrastive learning for feature encoder pretraining to boost performance.
Experiments show that our proposed method outperformed existing techniques for
chest X-ray diagnosis and the quality of visual saliency map prediction.
\\ ( https://arxiv.org/abs/2403.16970 ,  2071kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16974 (*cross-listing*)
Date: Mon, 25 Mar 2024 17:40:32 GMT   (2496kb,D)

Title: Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution
  Microscopy
Authors: Yair Ben Sahel, Yonina C. Eldar
Categories: eess.IV cs.CV cs.LG
\\
  The use of fluorescent molecules to create long sequences of low-density,
diffraction-limited images enables highly-precise molecule localization.
However, this methodology requires lengthy imaging times, which limits the
ability to view dynamic interactions of live cells on short time scales. Many
techniques have been developed to reduce the number of frames needed for
localization, from classic iterative optimization to deep neural networks.
Particularly, deep algorithm unrolling utilizes both the structure of iterative
sparse recovery algorithms and the performance gains of supervised deep
learning. However, the robustness of this approach is highly dependant on
having sufficient training data. In this paper we introduce deep unrolled
self-supervised learning, which alleviates the need for such data by training a
sequence-specific, model-based autoencoder that learns only from given
measurements. Our proposed method exceeds the performance of its supervised
counterparts, thus allowing for robust, dynamic imaging well below the
diffraction limit without any labeled training samples. Furthermore, the
suggested model-based autoencoder scheme can be utilized to enhance
generalization in any sparse recovery framework, without the need for external
training data.
\\ ( https://arxiv.org/abs/2403.16974 ,  2496kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16986 (*cross-listing*)
Date: Mon, 25 Mar 2024 17:48:06 GMT   (398kb,D)

Title: Dynamic Relative Representations for Goal-Oriented Semantic
  Communications
Authors: Simone Fiorellino, Claudio Battiloro, Emilio Calvanese Strinati, Paolo
  Di Lorenzo
Categories: cs.NI cs.IT cs.LG math.IT
\\
  In future 6G wireless networks, semantic and effectiveness aspects of
communications will play a fundamental role, incorporating meaning and
relevance into transmissions. However, obstacles arise when devices employ
diverse languages, logic, or internal representations, leading to semantic
mismatches that might jeopardize understanding. In latent space communication,
this challenge manifests as misalignment within high-dimensional
representations where deep neural networks encode data. This paper presents a
novel framework for goal-oriented semantic communication, leveraging relative
representations to mitigate semantic mismatches via latent space alignment. We
propose a dynamic optimization strategy that adapts relative representations,
communication parameters, and computation resources for energy-efficient,
low-latency, goal-oriented semantic communications. Numerical results
demonstrate our methodology's effectiveness in mitigating mismatches among
devices, while optimizing energy consumption, delay, and effectiveness.
\\ ( https://arxiv.org/abs/2403.16986 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17010 (*cross-listing*)
Date: Mon, 25 Mar 2024 17:59:59 GMT   (6922kb,D)

Title: Calib3D: Calibrating Model Preferences for Reliable 3D Scene
  Understanding
Authors: Lingdong Kong and Xiang Xu and Jun Cen and Wenwei Zhang and Liang Pan
  and Kai Chen and Ziwei Liu
Categories: cs.CV cs.LG cs.RO
Comments: Preprint; 37 pages, 8 figures, 11 tables; Code at
  https://github.com/ldkong1205/Calib3D
\\
  Safety-critical 3D scene understanding tasks necessitate not only accurate
but also confident predictions from 3D perception models. This study introduces
Calib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D
scene understanding models from an uncertainty estimation viewpoint. We
comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D
datasets, uncovering insightful phenomena that cope with both the aleatoric and
epistemic uncertainties in 3D scene understanding. We discover that despite
achieving impressive levels of accuracy, existing models frequently fail to
provide reliable uncertainty estimates -- a pitfall that critically undermines
their applicability in safety-sensitive contexts. Through extensive analysis of
key factors such as network capacity, LiDAR representations, rasterization
resolutions, and 3D data augmentation techniques, we correlate these aspects
directly with the model calibration efficacy. Furthermore, we introduce DeptS,
a novel depth-aware scaling approach aimed at enhancing 3D model calibration.
Extensive experiments across a wide range of configurations validate the
superiority of our method. We hope this work could serve as a cornerstone for
fostering reliable 3D scene understanding. Code and benchmark toolkits are
publicly available.
\\ ( https://arxiv.org/abs/2403.17010 ,  6922kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2011.12340
replaced with revised version Sat, 23 Mar 2024 17:53:43 GMT   (13144kb,D)

Title: mForms : Multimodal Form-Filling with Question Answering
Authors: Larry Heck, Simon Heck, Anirudh Sundar
Categories: cs.AI
Comments: 5 pages, 6 figures, 4 tables
\\ ( https://arxiv.org/abs/2011.12340 ,  13144kb)
------------------------------------------------------------------------------
\\
arXiv:2204.10438
replaced with revised version Mon, 25 Mar 2024 02:37:09 GMT   (2154kb,D)

Title: EVOTER: Evolution of Transparent Explainable Rule-sets
Authors: Hormoz Shahrzad, Babak Hodjat, Risto Miikkulainen
Categories: cs.AI cs.LG cs.NE
\\ ( https://arxiv.org/abs/2204.10438 ,  2154kb)
------------------------------------------------------------------------------
\\
arXiv:2205.11624
replaced with revised version Sun, 24 Mar 2024 20:03:38 GMT   (1091kb,D)

Title: Effective Integration of Weighted Cost-to-go and Conflict Heuristic
  within Suboptimal CBS
Authors: Rishi Veerapaneni, Tushar Kusnur, Maxim Likhachev
Categories: cs.AI cs.MA cs.RO
Comments: Published in AAAI 2023
DOI: 10.1609/aaai.v37i10.26381
\\ ( https://arxiv.org/abs/2205.11624 ,  1091kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11118
replaced with revised version Mon, 25 Mar 2024 15:10:57 GMT   (348kb,D)

Title: Dual Box Embeddings for the Description Logic EL++
Authors: Mathias Jackermeier, Jiaoyan Chen, Ian Horrocks
Categories: cs.AI cs.LG cs.LO
Comments: Updated license information
\\ ( https://arxiv.org/abs/2301.11118 ,  348kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14670
replaced with revised version Sat, 23 Mar 2024 10:10:18 GMT   (1818kb,D)

Title: Prompt Engineering for Healthcare: Methodologies and Applications
Authors: Jiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing Dai,
  Qiushi Yang, Yanqing Kang, Jinru Wu, Huawen Hu, Chenxi Yue, Haiyang Zhang,
  Yiheng Liu, Yi Pan, Zhengliang Liu, Lichao Sun, Xiang Li, Bao Ge, Xi Jiang,
  Dajiang Zhu, Yixuan Yuan, Dinggang Shen, Tianming Liu, Shu Zhang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2304.14670 ,  1818kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17196
replaced with revised version Mon, 25 Mar 2024 05:50:33 GMT   (1159kb,D)

Title: A Knowledge Engineering Primer
Authors: Agnieszka {\L}awrynowicz
Categories: cs.AI
\\ ( https://arxiv.org/abs/2305.17196 ,  1159kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07942
replaced with revised version Sun, 24 Mar 2024 12:00:31 GMT   (2329kb,D)

Title: Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis
Authors: Akash Anil, V\'ictor Guti\'errez-Basulto, Yazm\'in
  Iba\~n\'ez-Garc\'ia, Steven Schockaert
Categories: cs.AI cs.LG cs.SI
\\ ( https://arxiv.org/abs/2308.07942 ,  2329kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11432
replaced with revised version Mon, 25 Mar 2024 02:56:58 GMT   (4914kb,D)

Title: A Survey on Large Language Model based Autonomous Agents
Authors: Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and
  Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and
  Wayne Xin Zhao and Zhewei Wei and Ji-Rong Wen
Categories: cs.AI cs.CL
Comments: 35 pages, 5 figures, 3 tables, has been accepted by frontiers of
  computer science (FCS), doi={10.1007/s11704-024-40231-1}
\\ ( https://arxiv.org/abs/2308.11432 ,  4914kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11585
replaced with revised version Sat, 23 Mar 2024 14:07:54 GMT   (19112kb,D)

Title: Causal Intersectionality and Dual Form of Gradient Descent for
  Multimodal Analysis: a Case Study on Hateful Memes
Authors: Yosuke Miyanishi, Minh Le Nguyen
Categories: cs.AI cs.CL
Comments: Accepted to LREC-COLING 2024
\\ ( https://arxiv.org/abs/2308.11585 ,  19112kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11528
replaced with revised version Sun, 24 Mar 2024 10:27:10 GMT   (14206kb,D)

Title: Learning Complete Topology-Aware Correlations Between Relations for
  Inductive Link Prediction
Authors: Jie Wang, Hanzhu Chen, Qitan Lv, Zhihao Shi, Jiajun Chen, Huarui He,
  Hongtao Xie, Yongdong Zhang, and Feng Wu
Categories: cs.AI
Comments: arXiv admin note: text overlap with arXiv:2103.03642
\\ ( https://arxiv.org/abs/2309.11528 ,  14206kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11667
replaced with revised version Fri, 22 Mar 2024 18:52:15 GMT   (8201kb,D)

Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language
  Agents
Authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang
  Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig,
  Maarten Sap
Categories: cs.AI cs.CL cs.LG
Comments: Preprint, 43 pages. The first two authors contribute equally
\\ ( https://arxiv.org/abs/2310.11667 ,  8201kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13192
replaced with revised version Sat, 23 Mar 2024 13:21:58 GMT   (7022kb,D)

Title: The opaque law of artificial intelligence
Authors: Vincenzo Calderonio
Categories: cs.AI
Comments: 17 pages, 7 figures
ACM-class: F.0; I.2; J.4; K.4; K.5
\\ ( https://arxiv.org/abs/2310.13192 ,  7022kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02760
replaced with revised version Mon, 25 Mar 2024 08:57:47 GMT   (116kb,D)

Title: Causal Question Answering with Reinforcement Learning
Authors: Lukas Bl\"ubaum, Stefan Heindorf
Categories: cs.AI cs.LG
Comments: Accepted at WWW 2024
DOI: 10.1145/3589334.3645610
\\ ( https://arxiv.org/abs/2311.02760 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07954
replaced with revised version Sat, 23 Mar 2024 13:54:44 GMT   (407kb,D)

Title: A Closer Look at the Self-Verification Abilities of Large Language
  Models in Logical Reasoning
Authors: Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, Changshui Zhang
Categories: cs.AI cs.CL
Comments: NAACL 2024 Main Conference
\\ ( https://arxiv.org/abs/2311.07954 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03009
replaced with revised version Mon, 25 Mar 2024 05:04:04 GMT   (1572kb,D)

Title: I-PHYRE: Interactive Physical Reasoning
Authors: Shiqian Li, Kewen Wu, Chi Zhang, Yixin Zhu
Categories: cs.AI cs.CV cs.LG cs.RO
Comments: 21 pages, ICLR 2024
\\ ( https://arxiv.org/abs/2312.03009 ,  1572kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06203
replaced with revised version Sat, 23 Mar 2024 06:38:37 GMT   (29kb)

Title: Offloading and Quality Control for AI Generated Content Services in 6G
  Mobile Edge Computing Networks
Authors: Yitong Wang, Chang Liu, Jun Zhao
Categories: cs.AI cs.NI cs.PF
Comments: This paper appears in the 2024 IEEE 99th Vehicular Technology
  Conference (VTC)
\\ ( https://arxiv.org/abs/2312.06203 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15911
replaced with revised version Sat, 23 Mar 2024 02:15:31 GMT   (46kb)

Title: Distribution-consistency Structural Causal Models
Authors: Heyang Gong, Chaochao Lu, Yu Zhang
Categories: cs.AI math.ST stat.TH
\\ ( https://arxiv.org/abs/2401.15911 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00591
replaced with revised version Mon, 25 Mar 2024 10:52:20 GMT   (873kb,D)

Title: Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations
Authors: Nicolas Lazzari, Stefano De Giorgis, Aldo Gangemi, Valentina Presutti
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.00591 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08644
replaced with revised version Sat, 23 Mar 2024 11:29:17 GMT   (647kb,D)

Title: Tandem Transformers for Inference Efficient LLMs
Authors: Aishwarya P S and Pranav Ajit Nair and Yashas Samaga and Toby Boyd and
  Sanjiv Kumar and Prateek Jain and Praneeth Netrapalli
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.08644 ,  647kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09132
replaced with revised version Mon, 25 Mar 2024 08:46:02 GMT   (105kb,D)

Title: Exploring the Adversarial Capabilities of Large Language Models
Authors: Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.09132 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04369
replaced with revised version Sun, 24 Mar 2024 10:08:13 GMT   (1837kb,D)

Title: From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge
  Prediction
Authors: Ang Li, Qiangchao Chen, Yiquan Wu, Ming Cai, Xiang Zhou, Fei Wu, Kun
  Kuang
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2403.04369 ,  1837kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14566
replaced with revised version Sat, 23 Mar 2024 09:50:23 GMT   (223kb,D)

Title: A survey on Concept-based Approaches For Model Improvement
Authors: Avani Gupta, P J Narayanan
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.14566 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14589
replaced with revised version Mon, 25 Mar 2024 15:45:35 GMT   (340kb,D)

Title: ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for
  Contrastive Self-Training
Authors: Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.14589 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2007.01359
replaced with revised version Sat, 23 Mar 2024 22:22:54 GMT   (399kb)

Title: A Bayesian Multilingual Document Model for Zero-shot Topic
  Identification and Discovery
Authors: Santosh Kesiraju, Sangeet Sagar, Ond\v{r}ej Glembek, Luk\'a\v{s}
  Burget, J\'an \v{C}ernock\'y, Suryakanth V Gangashetty
Categories: cs.CL
\\ ( https://arxiv.org/abs/2007.01359 ,  399kb)
------------------------------------------------------------------------------
\\
arXiv:2211.13883
replaced with revised version Fri, 22 Mar 2024 19:09:52 GMT   (0kb,I)

Title: Learning with Silver Standard Data for Zero-shot Relation Extraction
Authors: Tianyin Wang, Jianwei Wang, Ziqian Zeng
Categories: cs.CL
Comments: Our research group conducted further research and discussions on the
  same topic and the latest paper is here arXiv:2402.18061, which has been
  accepted in LREC-COLING 2024
\\ ( https://arxiv.org/abs/2211.13883 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2212.10471
replaced with revised version Mon, 25 Mar 2024 17:54:21 GMT   (852kb,D)

Title: Little Red Riding Hood Goes Around the Globe:Crosslingual Story Planning
  and Generation with Large Language Models
Authors: Evgeniia Razumovskaia, Joshua Maynez, Annie Louis, Mirella Lapata,
  Shashi Narayan
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\ ( https://arxiv.org/abs/2212.10471 ,  852kb)
------------------------------------------------------------------------------
\\
arXiv:2301.06627
replaced with revised version Sat, 23 Mar 2024 19:52:33 GMT   (9037kb,D)

Title: Dissociating language and thought in large language models
Authors: Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua
  B. Tenenbaum, Evelina Fedorenko
Categories: cs.CL cs.AI
Comments: The two lead authors contributed equally to this work; published in
  "Trends in Cognnitive Sciences", March 2024
\\ ( https://arxiv.org/abs/2301.06627 ,  9037kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10451
replaced with revised version Mon, 25 Mar 2024 13:55:20 GMT   (423kb,D)

Title: Knowledge-augmented Graph Neural Networks with Concept-aware Attention
  for Adverse Drug Event Detection
Authors: Shaoxiong Ji and Ya Gao and Pekka Marttinen
Categories: cs.CL
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2301.10451 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2304.02541
replaced with revised version Sun, 24 Mar 2024 13:39:45 GMT   (671kb,D)

Title: PWESuite: Phonetic Word Embeddings and Tasks They Facilitate
Authors: Vil\'em Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson,
  Nathaniel Robinson, Mrinmaya Sachan, David Mortensen
Categories: cs.CL
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2304.02541 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2304.04806
replaced with revised version Sun, 24 Mar 2024 16:46:06 GMT   (2064kb,D)

Title: Examining Temporalities on Stance Detection towards COVID-19 Vaccination
Authors: Yida Mu, Mali Jin, Kalina Bontcheva, Xingyi Song
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2304.04806 ,  2064kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10666
replaced with revised version Mon, 25 Mar 2024 10:59:04 GMT   (364kb,D)

Title: A unified front-end framework for English text-to-speech synthesis
Authors: Zelin Ying, Chen Li, Yu Dong, Qiuqiang Kong, Qiao Tian, Yuanyuan Huo,
  Yuxuan Wang
Categories: cs.CL cs.SD eess.AS
Comments: Accepted in ICASSP 2024
DOI: 10.1109/ICASSP48485.2024.10447144
\\ ( https://arxiv.org/abs/2305.10666 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12519
replaced with revised version Sat, 23 Mar 2024 11:34:49 GMT   (6950kb,D)

Title: LLM Paternity Test: Generated Text Detection with LLM Genetic
  Inheritance
Authors: Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu,
  Weiming Zhang and Nenghai Yu
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2305.12519 ,  6950kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14012
replaced with revised version Mon, 25 Mar 2024 12:51:40 GMT   (2413kb,D)

Title: When your Cousin has the Right Connections: Unsupervised Bilingual
  Lexicon Induction for Related Data-Imbalanced Languages
Authors: Niyati Bafna, Cristina Espa\~na-Bonet, Josef van Genabith, Beno\^it
  Sagot, Rachel Bawden
Categories: cs.CL
Comments: 9 pages, Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2305.14012 ,  2413kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14310
replaced with revised version Sun, 24 Mar 2024 18:03:10 GMT   (1698kb)

Title: Navigating Prompt Complexity for Zero-Shot Classification: A Study of
  Large Language Models in Computational Social Science
Authors: Yida Mu, Ben P. Wu, William Thorne, Ambrose Robinson, Nikolaos
  Aletras, Carolina Scarton, Kalina Bontcheva, Xingyi Song
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2305.14310 ,  1698kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14328
replaced with revised version Sat, 23 Mar 2024 02:20:02 GMT   (2900kb,D)

Title: Benchmarking LLM-based Machine Translation on Cultural Awareness
Authors: Binwei Yao, Ming Jiang, Diyi Yang, Junjie Hu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.14328 ,  2900kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14965
replaced with revised version Mon, 25 Mar 2024 15:18:06 GMT   (7054kb,D)

Title: Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting
  Jailbreaks
Authors: Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit
  Choudhury
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024 - The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
\\ ( https://arxiv.org/abs/2305.14965 ,  7054kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16582
replaced with revised version Sat, 23 Mar 2024 03:06:54 GMT   (1291kb,D)

Title: Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in
  Language Models
Authors: Yao Yao, Zuchao Li and Hai Zhao
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.16582 ,  1291kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02531
replaced with revised version Fri, 22 Mar 2024 23:36:57 GMT   (3707kb,D)

Title: PLANNER: Generating Diversified Paragraph via Latent Language Diffusion
  Model
Authors: Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Josh Susskind,
  Navdeep Jaitly
Categories: cs.CL
Comments: Accepted by NeurIPS 2023, code at https://github.com/apple/ml-planner
\\ ( https://arxiv.org/abs/2306.02531 ,  3707kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04337
replaced with revised version Fri, 22 Mar 2024 18:41:02 GMT   (307kb,D)

Title: A study on the impact of Self-Supervised Learning on automatic
  dysarthric speech assessment
Authors: Xavier F. Cadet, Ranya Aloufi, Sara Ahmadi-Abhari, Hamed Haddadi
Categories: cs.CL
Comments: Accepted as a workshop paper at ICASSP SASB 2024
\\ ( https://arxiv.org/abs/2306.04337 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04357
replaced with revised version Mon, 25 Mar 2024 06:54:10 GMT   (176kb,D)

Title: Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue
  Systems
Authors: Zhenpeng Su and Xing Wu and Wei Zhou and Guangyuan Ma and Songlin Hu
Categories: cs.CL cs.AI
Comments: This paper has been accepted by NAACL 2024
\\ ( https://arxiv.org/abs/2306.04357 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02591
replaced with revised version Fri, 22 Mar 2024 20:01:04 GMT   (164kb,D)

Title: ODD: A Benchmark Dataset for the Natural Language Processing based
  Opioid Related Aberrant Behavior Detection
Authors: Sunjae Kwon, Xun Wang, Weisong Liu, Emily Druhl, Minhee L. Sung, Joel
  I. Reisman, Wenjun Li, Robert D. Kerns, William Becker, Hong Yu
Categories: cs.CL cs.AI
Comments: To be appeared at NAACL 2024
\\ ( https://arxiv.org/abs/2307.02591 ,  164kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06708
replaced with revised version Mon, 25 Mar 2024 08:44:53 GMT   (5673kb,D)

Title: To share or not to share: What risks would laypeople accept to give
  sensitive data to differentially-private NLP systems?
Authors: Christopher Weiss, Frauke Kreuter, Ivan Habernal
Categories: cs.CL cs.CR
Comments: Accepted at LREC-COLING 2024; final camera-ready version
\\ ( https://arxiv.org/abs/2307.06708 ,  5673kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14115
replaced with revised version Mon, 25 Mar 2024 03:54:48 GMT   (900kb,D)

Title: Situated Natural Language Explanations
Authors: Zining Zhu, Haoming Jiang, Jingfeng Yang, Sreyashi Nag, Chao Zhang,
  Jie Huang, Yifan Gao, Frank Rudzicz, Bing Yin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.14115 ,  900kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03103
replaced with revised version Sat, 23 Mar 2024 03:15:42 GMT   (482kb,D)

Title: ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation
  Following the Metaphor Identification Procedure
Authors: Mohamad Elzohbi, Richard Zhao
Categories: cs.CL cs.LG
Comments: 9 pages, 2 figures, accepted for the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)
\\ ( https://arxiv.org/abs/2309.03103 ,  482kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08503
replaced with revised version Mon, 25 Mar 2024 08:33:37 GMT   (888kb,D)

Title: HealthFC: Verifying Health Claims with Evidence-Based Medical
  Fact-Checking
Authors: Juraj Vladika, Phillip Schneider, Florian Matthes
Categories: cs.CL cs.AI
Comments: Accepted to LREC-COLING 2024
\\ ( https://arxiv.org/abs/2309.08503 ,  888kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09369
replaced with revised version Fri, 22 Mar 2024 22:54:04 GMT   (9091kb,D)

Title: Embrace Divergence for Richer Insights: A Multi-document Summarization
  Benchmark and a Case Study on Summarizing Diverse Information from News
  Articles
Authors: Kung-Hsiang Huang, Philippe Laban, Alexander R. Fabbri, Prafulla Kumar
  Choubey, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu
Categories: cs.CL
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2309.09369 ,  9091kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11093
replaced with revised version Sat, 23 Mar 2024 13:33:06 GMT   (1932kb,D)

Title: K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling
Authors: Haven Kim, Jongmin Jung, Dasaem Jeong, and Juhan Nam
Categories: cs.CL cs.LG cs.MM
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2309.11093 ,  1932kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11576
replaced with revised version Sun, 24 Mar 2024 00:06:49 GMT   (13203kb,D)

Title: Examining the Limitations of Computational Rumor Detection Models
  Trained on Static Datasets
Authors: Yida Mu, Xingyi Song, Kalina Bontcheva, Nikolaos Aletras
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2309.11576 ,  13203kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12263
replaced with revised version Mon, 25 Mar 2024 10:52:14 GMT   (6144kb,D)

Title: On the Relationship between Skill Neurons and Robustness in Prompt
  Tuning
Authors: Leon Ackermann, Xenia Ohmer
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.12263 ,  6144kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13182
replaced with revised version Mon, 25 Mar 2024 06:49:16 GMT   (1844kb,D)

Title: Effective Distillation of Table-based Reasoning Ability from LLMs
Authors: Bohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao, Chenghua Lin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.13182 ,  1844kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13339
replaced with revised version Sun, 24 Mar 2024 04:17:28 GMT   (312kb,D)

Title: Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models
  through Logic
Authors: Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun
  Chu, Stefan Wermter
Categories: cs.CL cs.AI cs.LG cs.SC
Comments: Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT
\\ ( https://arxiv.org/abs/2309.13339 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01777
replaced with revised version Mon, 25 Mar 2024 04:04:05 GMT   (14843kb,D)

Title: SEA: Sparse Linear Attention with Estimated Attention Mask
Authors: Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang
Categories: cs.CL cs.LG
Comments: 9 main pages
\\ ( https://arxiv.org/abs/2310.01777 ,  14843kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06707
replaced with revised version Mon, 25 Mar 2024 13:27:16 GMT   (323kb,D)

Title: Quality-Aware Translation Models: Efficient Generation and Quality
  Estimation in a Single Model
Authors: Christian Tomani, David Vilar, Markus Freitag, Colin Cherry, Subhajit
  Naskar, Mara Finkelstein, Xavier Garcia and Daniel Cremers
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.06707 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09725
replaced with revised version Sat, 23 Mar 2024 11:45:55 GMT   (3321kb,D)

Title: KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large
  Language Models
Authors: Yuyang Bai, Shangbin Feng, Vidhisha Balachandran, Zhaoxuan Tan, Shiqi
  Lou, Tianxing He, Yulia Tsvetkov
Categories: cs.CL
Comments: TheWebConf 2024
\\ ( https://arxiv.org/abs/2310.09725 ,  3321kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10648
replaced with revised version Mon, 25 Mar 2024 16:56:39 GMT   (1863kb,D)

Title: Bridging the Novice-Expert Gap via Models of Decision-Making: A Case
  Study on Remediating Math Mistakes
Authors: Rose E. Wang, Qingyang Zhang, Carly Robinson, Susanna Loeb, Dorottya
  Demszky
Categories: cs.CL cs.AI
Comments: NAACL 2024. Code: https://github.com/rosewang2008/bridge
\\ ( https://arxiv.org/abs/2310.10648 ,  1863kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13895
replaced with revised version Mon, 25 Mar 2024 13:41:32 GMT   (5562kb,D)

Title: RTSUM: Relation Triple-based Interpretable Summarization with
  Multi-level Salience Visualization
Authors: Seonglae Cho, Yonggi Cho, HoonJae Lee, Myungha Jang, Jinyoung Yeo,
  Dongha Lee
Categories: cs.CL cs.LG
Comments: 8 pages, 2 figures
\\ ( https://arxiv.org/abs/2310.13895 ,  5562kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16049
replaced with revised version Sat, 23 Mar 2024 21:21:44 GMT   (11764kb,D)

Title: MuSR: Testing the Limits of Chain-of-thought with Multistep Soft
  Reasoning
Authors: Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett
Categories: cs.CL
Journal-ref: ICLR 2024 (Spotlight)
\\ ( https://arxiv.org/abs/2310.16049 ,  11764kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16450
replaced with revised version Sun, 24 Mar 2024 17:14:11 GMT   (925kb,D)

Title: CLEX: Continuous Length Extrapolation for Large Language Models
Authors: Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, Lidong Bing
Categories: cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.16450 ,  925kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18340
replaced with revised version Sun, 24 Mar 2024 09:09:00 GMT   (2896kb,D)

Title: UrbanCLIP: Learning Text-enhanced Urban Region Profiling with
  Contrastive Language-Image Pretraining from the Web
Authors: Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong
  Wen, Roger Zimmermann, Yuxuan Liang
Categories: cs.CL cs.AI
Comments: Accepted by The Web Conference 2024
\\ ( https://arxiv.org/abs/2310.18340 ,  2896kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08298
replaced with revised version Mon, 25 Mar 2024 06:01:49 GMT   (7960kb,D)

Title: A Survey of Confidence Estimation and Calibration in Large Language
  Models
Authors: Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov,
  Iryna Gurevych
Categories: cs.CL cs.AI
Comments: 16 pages, 1 page, 1 table
\\ ( https://arxiv.org/abs/2311.08298 ,  7960kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08849
replaced with revised version Mon, 25 Mar 2024 15:49:53 GMT   (304kb,D)

Title: OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient
  Large-scale Multilingual Continued Pretraining
Authors: Yihong Liu, Peiqin Lin, Mingyang Wang, Hinrich Sch\"utze
Categories: cs.CL
Comments: NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2311.08849 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09122
replaced with revised version Mon, 25 Mar 2024 17:14:35 GMT   (792kb,D)

Title: Universal NER: A Gold-Standard Multilingual Named Entity Recognition
  Benchmark
Authors: Stephen Mayhew, Terra Blevins, Shuheng Liu, Marek \v{S}uppa, Hila
  Gonen, Joseph Marvin Imperial, B\"orje F. Karlsson, Peiqin Lin, Nikola
  Ljube\v{s}i\'c, LJ Miranda, Barbara Plank, Arij Riabi, Yuval Pinter
Categories: cs.CL
Comments: NAACL 2024 Camera-ready
\\ ( https://arxiv.org/abs/2311.09122 ,  792kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09214
replaced with revised version Mon, 25 Mar 2024 17:52:30 GMT   (8364kb,D)

Title: Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive
  Thinking from Large Language Models
Authors: Weize Liu, Guocong Li, Kai Zhang, Bang Du, Qiyuan Chen, Xuming Hu,
  Hongxia Xu, Jintai Chen, Jian Wu
Categories: cs.CL
Comments: Accepted to NAACL 2024 Main Conference
\\ ( https://arxiv.org/abs/2311.09214 ,  8364kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07527
replaced with revised version Sat, 23 Mar 2024 21:43:04 GMT   (19kb,D)

Title: BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy
  and Reasoning Ability
Authors: Peter Clark, Bhavana Dalvi Mishra, Oyvind Tafjord
Categories: cs.CL cs.AI
Comments: Added note about how dataset sampling was performed
\\ ( https://arxiv.org/abs/2312.07527 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10647
replaced with revised version Sun, 24 Mar 2024 06:46:19 GMT   (1052kb,D)

Title: Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language
  Models
Authors: Rima Hazra, Sayan Layek, Somnath Banerjee, Soujanya Poria
Categories: cs.CL
Comments: Under review.
  {https://huggingface.co/datasets/SoftMINER-Group/NicheHazardQA}
\\ ( https://arxiv.org/abs/2401.10647 ,  1052kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11504
replaced with revised version Mon, 25 Mar 2024 08:16:06 GMT   (131kb,D)

Title: With Greater Text Comes Greater Necessity: Inference-Time Training Helps
  Long Text Generation
Authors: Y. Wang, D. Ma, D. Cai
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.11504 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11624
replaced with revised version Sat, 23 Mar 2024 16:35:45 GMT   (183kb,D)

Title: In-context Learning with Retrieved Demonstrations for Language Models: A
  Survey
Authors: Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2401.11624 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15770
replaced with revised version Sat, 23 Mar 2024 21:13:35 GMT   (7549kb,D)

Title: PILOT: Legal Case Outcome Prediction with Case Law
Authors: Lang Cao, Zifeng Wang, Cao Xiao, Jimeng Sun
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.15770 ,  7549kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17919
replaced with revised version Mon, 25 Mar 2024 12:52:42 GMT   (7711kb,D)

Title: LOCOST: State-Space Models for Long Document Abstractive Summarization
Authors: Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen,
  Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick
  Gallinari
Categories: cs.CL cs.LG
Comments: 9 pages, 5 figures, 7 tables, EACL 2024 conference
\\ ( https://arxiv.org/abs/2401.17919 ,  7711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00157
replaced with revised version Sat, 23 Mar 2024 15:45:57 GMT   (6944kb,D)

Title: Large Language Models for Mathematical Reasoning: Progresses and
  Challenges
Authors: Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin
Categories: cs.CL
Comments: EACL 2024 Student Research Workshop, 8 pages
\\ ( https://arxiv.org/abs/2402.00157 ,  6944kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09282
replaced with revised version Sun, 24 Mar 2024 07:06:19 GMT   (454kb,D)

Title: Leveraging Large Language Models for Enhanced NLP Task Performance
  through Knowledge Distillation and Optimized Training Strategies
Authors: Yining Huang, Keke Tang, Meilian Chen
Categories: cs.CL
Comments: 16 pages, 3 figures
\\ ( https://arxiv.org/abs/2402.09282 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10670
replaced with revised version Mon, 25 Mar 2024 02:52:43 GMT   (12899kb,D)

Title: OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via
  Vision-Language Foundation Models
Authors: Yuxuan Kuang, Hai Lin, Meng Jiang
Categories: cs.CL cs.RO
Comments: NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2402.10670 ,  12899kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10685
replaced with revised version Mon, 25 Mar 2024 11:50:32 GMT   (9808kb,D)

Title: LongHeads: Multi-Head Attention is Secretly a Long Context Processor
Authors: Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing
  Huang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.10685 ,  9808kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13571
replaced with revised version Sat, 23 Mar 2024 08:22:58 GMT   (327kb)

Title: Multilingual Coreference Resolution in Low-resource South Asian
  Languages
Authors: Ritwik Mishra, Pooja Desur, Rajiv Ratn Shah, Ponnurangam Kumaraguru
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.13571 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15248
replaced with revised version Mon, 25 Mar 2024 09:36:54 GMT   (242kb,D)

Title: Chitchat as Interference: Adding User Backstories to Task-Oriented
  Dialogues
Authors: Armand Stricker, Patrick Paroubek
Categories: cs.CL
Comments: Accepted @ LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.15248 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16694
replaced with revised version Sun, 24 Mar 2024 15:41:21 GMT   (3303kb,D)

Title: HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual
  Natural Language Generalization
Authors: Qiwei Peng, Yekun Chai, Xuhong Li
Categories: cs.CL cs.PL cs.SE
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.16694 ,  3303kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17914
replaced with revised version Sat, 23 Mar 2024 20:21:04 GMT   (2289kb,D)

Title: Extracting Lexical Features from Dialects via Interpretable Dialect
  Classifiers
Authors: Roy Xie, Orevaoghene Ahia, Yulia Tsvetkov, Antonios Anastasopoulos
Categories: cs.CL cs.AI
Comments: Code is available at
  https://github.com/ruoyuxie/interpretable_dialect_classifier
\\ ( https://arxiv.org/abs/2402.17914 ,  2289kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01479
replaced with revised version Mon, 25 Mar 2024 08:46:15 GMT   (248kb,D)

Title: Align-to-Distill: Trainable Attention Alignment for Knowledge
  Distillation in Neural Machine Translation
Authors: Heegon Jin, Seonil Son, Jemin Park, Youngseok Kim, Hyungjong Noh,
  Yeonsoo Lee
Categories: cs.CL cs.AI
Comments: Accepted to LREC-COLING 2024
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2403.01479 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02691
replaced with revised version Mon, 25 Mar 2024 17:25:10 GMT   (2151kb,D)

Title: InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated
  Large Language Model Agents
Authors: Qiusi Zhan, Zhixiang Liang, Zifan Ying, Daniel Kang
Categories: cs.CL cs.CR
Comments: 28 pages, 5 figures, 9 tables
\\ ( https://arxiv.org/abs/2403.02691 ,  2151kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02930
replaced with revised version Mon, 25 Mar 2024 12:07:13 GMT   (66kb,D)

Title: A Second Look on BASS -- Boosting Abstractive Summarization with Unified
  Semantic Graphs -- A Replication Study
Authors: Osman Alperen Kora\c{s}, J\"org Schl\"otterer, Christin Seifert
Categories: cs.CL cs.LG
Comments: This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in Advances in Information Retrieval, 46th European Conference on
  Information Retrieval, ECIR 2024. 16 pages, 4 figures
\\ ( https://arxiv.org/abs/2403.02930 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07311
replaced with revised version Sat, 23 Mar 2024 19:09:27 GMT   (2262kb,D)

Title: Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
Authors: Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Chong Zhang, Mengnan
  Du, Yongfeng Zhang
Categories: cs.CL cs.LG
Comments: 23 pages, 2 figures
\\ ( https://arxiv.org/abs/2403.07311 ,  2262kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10827
replaced with revised version Sat, 23 Mar 2024 02:19:18 GMT   (0kb,I)

Title: Multi-party Response Generation with Relation Disentanglement
Authors: Tianhao Dai, Chengyu Huang, and Lizi Liao
Categories: cs.CL
Comments: The paper needs systematic polishment to consider recent development
  in dialogue
\\ ( https://arxiv.org/abs/2403.10827 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10963
replaced with revised version Mon, 25 Mar 2024 12:37:16 GMT   (3443kb,D)

Title: Pointer-Generator Networks for Low-Resource Machine Translation: Don't
  Copy That!
Authors: Niyati Bafna, Philipp Koehn, and David Yarowsky
Categories: cs.CL
Comments: 4 pages
\\ ( https://arxiv.org/abs/2403.10963 ,  3443kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11311
replaced with revised version Sun, 24 Mar 2024 06:21:27 GMT   (1277kb,D)

Title: Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding
Authors: Zichen Wu, Hsiu-Yuan Huang, Fanyi Qu and Yunfang Wu
Categories: cs.CL cs.MM
Comments: LREC-COLING 2024, Long Paper
\\ ( https://arxiv.org/abs/2403.11311 ,  1277kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11752
replaced with revised version Mon, 25 Mar 2024 16:33:12 GMT   (217kb,D)

Title: Revisiting The Classics: A Study on Identifying and Rectifying Gender
  Stereotypes in Rhymes and Poems
Authors: Aditya Narayan Sankaran, Vigneshwaran Shankaran, Sampath Lonka, Rajesh
  Sharma
Categories: cs.CL
Comments: Accepted to appear at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.11752 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11802
replaced with revised version Mon, 25 Mar 2024 14:58:41 GMT   (845kb,D)

Title: Counting-Stars: A Simple, Efficient, and Reasonable Strategy for
  Evaluating Long-Context Large Language Models
Authors: Mingyang Song, Mao Zheng, Xuan Luo
Categories: cs.CL
Comments: a technical report
\\ ( https://arxiv.org/abs/2403.11802 ,  845kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11838
replaced with revised version Sat, 23 Mar 2024 06:26:41 GMT   (9067kb,D)

Title: Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for
  Language Models
Authors: Yi Luo, Zhenghao Lin, Yuhao Zhang, Jiashuo Sun, Chen Lin, Chengjin Xu,
  Xiangdong Su, Yelong Shen, Jian Guo, Yeyun Gong
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024 main conference
\\ ( https://arxiv.org/abs/2403.11838 ,  9067kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12027
replaced with revised version Mon, 25 Mar 2024 17:39:10 GMT   (454kb,D)

Title: From Pixels to Insights: A Survey on Automatic Chart Understanding in
  the Era of Large Foundation Models
Authors: Kung-Hsiang Huang, Hou Pong Chan, Yi R. Fung, Haoyi Qiu, Mingyang
  Zhou, Shafiq Joty, Shih-Fu Chang, Heng Ji
Categories: cs.CL cs.AI cs.CV
\\ ( https://arxiv.org/abs/2403.12027 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13737
replaced with revised version Fri, 22 Mar 2024 20:49:51 GMT   (846kb)

Title: EthioLLM: Multilingual Large Language Models for Ethiopian Languages
  with Task Evaluation
Authors: Atnafu Lambebo Tonja, Israel Abebe Azime, Tadesse Destaw Belay, Mesay
  Gemeda Yigezu, Moges Ahmed Mehamed, Abinew Ali Ayele, Ebrahim Chekol Jibril,
  Michael Melese Woldeyohannis, Olga Kolesnikova, Philipp Slusallek, Dietrich
  Klakow, Shengwu Xiong, Seid Muhie Yimam
Categories: cs.CL
Comments: Accepted at LREC-Coling 2024
\\ ( https://arxiv.org/abs/2403.13737 ,  846kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13786
replaced with revised version Sat, 23 Mar 2024 04:02:01 GMT   (904kb,D)

Title: Chain-of-Interaction: Enhancing Large Language Models for Psychiatric
  Behavior Understanding by Dyadic Contexts
Authors: Guangzeng Han and Weisi Liu and Xiaolei Huang and Brian Borsari
Categories: cs.CL
Comments: Accepted to IEEE ICHI 2024
\\ ( https://arxiv.org/abs/2403.13786 ,  904kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14050
replaced with revised version Sat, 23 Mar 2024 14:46:59 GMT   (385kb)

Title: Extracting Emotion Phrases from Tweets using BART
Authors: Mahdi Rezapour
Categories: cs.CL cs.LG stat.AP
\\ ( https://arxiv.org/abs/2403.14050 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14253
replaced with revised version Sat, 23 Mar 2024 15:53:50 GMT   (8596kb,D)

Title: K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional
  Expression
Authors: Kyuhee Kim, Surin Lee and Sangah Lee
Categories: cs.CL
Comments: 10 pages
\\ ( https://arxiv.org/abs/2403.14253 ,  8596kb)
------------------------------------------------------------------------------
\\
arXiv:1902.05605
replaced with revised version Mon, 25 Mar 2024 10:20:18 GMT   (3298kb,D)

Title: CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater
  Sample Efficiency and Simplicity
Authors: Aditya Bhatt, Daniel Palenicek, Boris Belousov, Max Argus, Artemij
  Amiranashvili, Thomas Brox, Jan Peters
Categories: cs.LG stat.ML
Comments: Published at ICLR 2024. Project page at
  http://aditya.bhatts.org/CrossQ and code release at
  https://github.com/adityab/CrossQ
\\ ( https://arxiv.org/abs/1902.05605 ,  3298kb)
------------------------------------------------------------------------------
\\
arXiv:2111.10933
replaced with revised version Sat, 23 Mar 2024 03:41:10 GMT   (302kb,D)

Title: Decentralized Multi-Armed Bandit Can Outperform Classic Upper Confidence
  Bound: A Homogeneous Case over Strongly Connected Graphs
Authors: Jingxuan Zhu, Ji Liu
Categories: cs.LG cs.SY eess.SY
Comments: Submitted to the 63rd IEEE Conference on Decision and Control
\\ ( https://arxiv.org/abs/2111.10933 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2112.15400
replaced with revised version Mon, 25 Mar 2024 12:22:53 GMT   (3750kb,D)

Title: A Theoretical Understanding of Gradient Bias in Meta-Reinforcement
  Learning
Authors: Xidong Feng, Bo Liu, Jie Ren, Luo Mai, Rui Zhu, Haifeng Zhang, Jun
  Wang, Yaodong Yang
Categories: cs.LG cs.AI
Comments: NeurIPS 2022
\\ ( https://arxiv.org/abs/2112.15400 ,  3750kb)
------------------------------------------------------------------------------
\\
arXiv:2205.07250
replaced with revised version Sun, 24 Mar 2024 03:33:39 GMT   (8115kb,D)

Title: A cGAN Ensemble-based Uncertainty-aware Surrogate Model for Offline
  Model-based Optimization in Industrial Control Problems
Authors: Cheng Feng
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: Accepted in IJCNN 2024
\\ ( https://arxiv.org/abs/2205.07250 ,  8115kb)
------------------------------------------------------------------------------
\\
arXiv:2207.04913
replaced with revised version Sat, 23 Mar 2024 10:32:22 GMT   (4417kb,D)

Title: Generalizing to Unseen Domains with Wasserstein Distributional
  Robustness under Limited Source Knowledge
Authors: Jingge Wang, Liyan Xie, Yao Xie, Shao-Lun Huang, Yang Li
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2207.04913 ,  4417kb)
------------------------------------------------------------------------------
\\
arXiv:2208.08270
replaced with revised version Sat, 23 Mar 2024 03:33:32 GMT   (423kb,D)

Title: On the Privacy Effect of Data Enhancement via the Lens of Memorization
Authors: Xiao Li and Qiongxiu Li and Zhanhao Hu and Xiaolin Hu
Categories: cs.LG cs.CR cs.CV
Comments: Accepted by IEEE TIFS, 17 pages
\\ ( https://arxiv.org/abs/2208.08270 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2211.07556
replaced with revised version Mon, 25 Mar 2024 15:11:56 GMT   (4370kb,D)

Title: Utilizing Synthetic Data in Supervised Learning for Robust 5-DoF
  Magnetic Marker Localization
Authors: Mengfan Wu, Thomas Langerak, Otmar Hilliges and Juan Zarate
Categories: cs.LG
\\ ( https://arxiv.org/abs/2211.07556 ,  4370kb)
------------------------------------------------------------------------------
\\
arXiv:2211.12328
replaced with revised version Sat, 23 Mar 2024 08:48:14 GMT   (1197kb,D)

Title: A survey on knowledge-enhanced multimodal learning
Authors: Maria Lymperaiou, Giorgos Stamou
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2211.12328 ,  1197kb)
------------------------------------------------------------------------------
\\
arXiv:2212.06370
replaced with revised version Fri, 22 Mar 2024 02:30:02 GMT   (4403kb,D)

Title: Dual Accuracy-Quality-Driven Neural Network for Prediction Interval
  Generation
Authors: Giorgio Morales and John W. Sheppard
Categories: cs.LG stat.ML
Comments: Accepted at the IEEE Transactions on Neural Networks and Learning
  Systems
Journal-ref: G. Morales and J. W. Sheppard, "Dual Accuracy-Quality-Driven
  Neural Network for Prediction Interval Generation," in IEEE Transactions on
  Neural Networks and Learning Systems, 2023
DOI: 10.1109/TNNLS.2023.3339470
\\ ( https://arxiv.org/abs/2212.06370 ,  4403kb)
------------------------------------------------------------------------------
\\
arXiv:2212.09962
replaced with revised version Mon, 25 Mar 2024 16:07:31 GMT   (417kb,D)

Title: Distributional Robustness Bounds Generalization Errors
Authors: Shixiong Wang and Haowei Wang
Categories: cs.LG stat.ML
Comments: Updated Version
MSC-class: 62F35, 62G35, 62C12
\\ ( https://arxiv.org/abs/2212.09962 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2301.06626
replaced with revised version Mon, 25 Mar 2024 00:45:30 GMT   (0kb,I)

Title: Masked Vector Quantization
Authors: David D. Nguyen, David Leibowitz, Surya Nepal, Salil S. Kanhere
Categories: cs.LG cs.CV
Comments: A newer version of this manuscript was archived under 2312.11735
\\ ( https://arxiv.org/abs/2301.06626 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2301.07482
replaced with revised version Sun, 24 Mar 2024 14:48:59 GMT   (1457kb,D)

Title: FreshGNN: Reducing Memory Access via Stable Historical Embeddings for
  Graph Neural Network Training
Authors: Kezhao Huang, Haitian Jiang, Minjie Wang, Guangxuan Xiao, David Wipf,
  Xiang Song, Quan Gan, Zengfeng Huang, Jidong Zhai, Zheng Zhang
Categories: cs.LG
Comments: Accepted by VLDB 2024
\\ ( https://arxiv.org/abs/2301.07482 ,  1457kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10956
replaced with revised version Sat, 23 Mar 2024 08:17:35 GMT   (2281kb,D)

Title: Graph Neural Networks can Recover the Hidden Features Solely from the
  Graph Structure
Authors: Ryoma Sato
Categories: cs.LG cs.SI
Comments: ICML 2023
\\ ( https://arxiv.org/abs/2301.10956 ,  2281kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12528
replaced with revised version Sat, 23 Mar 2024 22:57:18 GMT   (1693kb,D)

Title: Sequential Estimation of Gaussian Process-based Deep State-Space Models
Authors: Yuhao Liu, Marzieh Ajirak, Petar Djuric
Categories: cs.LG
DOI: 10.1109/TSP.2023.3303648
\\ ( https://arxiv.org/abs/2301.12528 ,  1693kb)
------------------------------------------------------------------------------
\\
arXiv:2302.00924
replaced with revised version Sat, 23 Mar 2024 15:40:58 GMT   (916kb,D)

Title: LMC: Fast Training of GNNs via Subgraph Sampling with Provable
  Convergence
Authors: Zhihao Shi, Xize Liang, Jie Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2302.00924 ,  916kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05049
replaced with revised version Sun, 24 Mar 2024 22:00:52 GMT   (413kb,D)

Title: Principled Federated Domain Adaptation: Gradient Projection and
  Auto-Weighting
Authors: Enyi Jiang, Yibo Jacky Zhang, Sanmi Koyejo
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2302.05049 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05656
replaced with revised version Sun, 24 Mar 2024 02:43:55 GMT   (2673kb,D)

Title: EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models
Authors: Hongyi Yuan, Songchi Zhou, Sheng Yu
Categories: cs.LG cs.CV
Comments: Accepted by TMLR, preprint of camera-ready version
\\ ( https://arxiv.org/abs/2303.05656 ,  2673kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17245
replaced with revised version Mon, 25 Mar 2024 12:20:02 GMT   (5400kb,D)

Title: Investigating and Mitigating the Side Effects of Noisy Views for
  Self-Supervised Clustering Algorithms in Practical Multi-View Scenarios
Authors: Jie Xu, Yazhou Ren, Xiaolong Wang, Lei Feng, Zheng Zhang, Gang Niu,
  Xiaofeng Zhu
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2303.17245 ,  5400kb)
------------------------------------------------------------------------------
\\
arXiv:2304.01391
replaced with revised version Mon, 25 Mar 2024 01:55:36 GMT   (6014kb,D)

Title: Counterfactual Learning on Graphs: A Survey
Authors: Zhimeng Guo, Teng Xiao, Zongyu Wu, Charu Aggarwal, Hui Liu, Suhang
  Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2304.01391 ,  6014kb)
------------------------------------------------------------------------------
\\
arXiv:2304.07537
replaced with revised version Sun, 24 Mar 2024 09:42:39 GMT   (1835kb,D)

Title: Gradient-less Federated Gradient Boosting Trees with Learnable Learning
  Rates
Authors: Chenyang Ma, Xinchi Qiu, Daniel J. Beutel, Nicholas D. Lane
Categories: cs.LG cs.AI cs.DC cs.DS
Comments: Accepted at the 3rd ACM Workshop on Machine Learning and Systems
  (EuroMLSys), May 8th 2023, Rome, Italy
DOI: 10.1145/3578356.3592579
\\ ( https://arxiv.org/abs/2304.07537 ,  1835kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14822
replaced with revised version Sun, 24 Mar 2024 13:01:18 GMT   (199kb,D)

Title: Can Copyright be Reduced to Privacy?
Authors: Niva Elkin-Koren and Uri Hacohen and Roi Livni and Shay Moran
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2305.14822 ,  199kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15253
replaced with revised version Sat, 23 Mar 2024 07:14:23 GMT   (80kb,D)

Title: Rethinking the Evaluation Protocol of Domain Generalization
Authors: Han Yu, Xingxuan Zhang, Renzhe Xu, Jiashuo Liu, Yue He, Peng Cui
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2305.15253 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16943
replaced with revised version Sun, 24 Mar 2024 22:00:04 GMT   (33903kb,D)

Title: DiffusionNAG: Predictor-guided Neural Architecture Generation with
  Diffusion Models
Authors: Sohyun An, Hayeon Lee, Jaehyeong Jo, Seanie Lee, Sung Ju Hwang
Categories: cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2305.16943 ,  33903kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04212
replaced with revised version Sat, 23 Mar 2024 11:37:21 GMT   (855kb,D)

Title: Migrate Demographic Group For Fair GNNs
Authors: YanMing Hu, TianChi Liao, JiaLong Chen, Jing Bian, ZiBin Zheng, and
  Chuan Chen
Categories: cs.LG cs.CY
\\ ( https://arxiv.org/abs/2306.04212 ,  855kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08318
replaced with revised version Mon, 25 Mar 2024 16:06:34 GMT   (6133kb,D)

Title: Identification of Energy Management Configuration Concepts from a Set of
  Pareto-optimal Solutions
Authors: Felix Lanfermann and Qiqi Liu and Yaochu Jin and Sebastian Schmitt
Categories: cs.LG cs.SY eess.SY
Comments: 18 pages, 8 figures, accepted at Energy Conversion and Management: X
\\ ( https://arxiv.org/abs/2306.08318 ,  6133kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16788
replaced with revised version Sat, 23 Mar 2024 07:05:26 GMT   (1518kb,D)

Title: Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging
Authors: Max Zimmer, Christoph Spiegel, Sebastian Pokutta
Categories: cs.LG cs.AI
Comments: ICLR24 Camera Ready, 9 pages, 5 pages references, 16 pages appendix
\\ ( https://arxiv.org/abs/2306.16788 ,  1518kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12851
replaced with revised version Mon, 25 Mar 2024 14:48:55 GMT   (2438kb,D)

Title: Early Neuron Alignment in Two-layer ReLU Networks with Small
  Initialization
Authors: Hancheng Min, Enrique Mallada, Ren\'e Vidal
Categories: cs.LG
Comments: iclr 2024 camera-ready
\\ ( https://arxiv.org/abs/2307.12851 ,  2438kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03472
replaced with revised version Mon, 25 Mar 2024 10:58:22 GMT   (8120kb,D)

Title: Improving the forecast accuracy of wind power by leveraging multiple
  hierarchical structure
Authors: Lucas English, Mahdi Abolghasemi
Categories: cs.LG
Comments: 41 pages, 14 figures
\\ ( https://arxiv.org/abs/2308.03472 ,  8120kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10457
replaced with revised version Sun, 24 Mar 2024 10:04:37 GMT   (2458kb,D)

Title: ALI-DPFL: Differentially Private Federated Learning with Adaptive Local
  Iterations
Authors: Xinpeng Ling, Jie Fu, Kuncan Wang, Haitao Liu, Zhili Chen
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2308.10457 ,  2458kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11841
replaced with revised version Sat, 23 Mar 2024 08:45:03 GMT   (7676kb,D)

Title: A Survey for Federated Learning Evaluations: Goals and Measures
Authors: Di Chai, Leye Wang, Liu Yang, Junxue Zhang, Kai Chen, and Qiang Yang
Categories: cs.LG cs.CR cs.DC
\\ ( https://arxiv.org/abs/2308.11841 ,  7676kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12581
replaced with revised version Mon, 25 Mar 2024 11:58:04 GMT   (193kb,D)

Title: A Huber Loss Minimization Approach to Byzantine Robust Federated
  Learning
Authors: Puning Zhao, Fei Yu, Zhiguo Wan
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.12581 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01069
replaced with revised version Mon, 25 Mar 2024 17:39:18 GMT   (3619kb)

Title: Separable Hamiltonian Neural Networks
Authors: Zi-Yu Khoo, Dawen Wu, Jonathan Sze Choong Low and St\'ephane Bressan
Categories: cs.LG cs.AI
Comments: 11 pages
\\ ( https://arxiv.org/abs/2309.01069 ,  3619kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06380
replaced with revised version Sat, 23 Mar 2024 14:22:39 GMT   (43466kb,D)

Title: InstaFlow: One Step is Enough for High-Quality Diffusion-Based
  Text-to-Image Generation
Authors: Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, Qiang Liu
Categories: cs.LG cs.CV
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2309.06380 ,  43466kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09317
replaced with revised version Fri, 22 Mar 2024 18:59:15 GMT   (1468kb,D)

Title: Kinematics-aware Trajectory Generation and Prediction with Latent
  Stochastic Differential Modeling
Authors: Ruochen Jiao, Yixuan Wang, Xiangguo Liu, Chao Huang, Qi Zhu
Categories: cs.LG cs.RO
Comments: 8 pages, conference paper in motion generation
\\ ( https://arxiv.org/abs/2309.09317 ,  1468kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09574
replaced with revised version Sat, 23 Mar 2024 02:16:40 GMT   (3665kb,D)

Title: Latent assimilation with implicit neural representations for unknown
  dynamics
Authors: Zhuoyuan Li, Bin Dong, and Pingwen Zhang
Categories: cs.LG math-ph math.MP math.OC physics.ao-ph
Comments: 40 pages
MSC-class: 68T07, 49N45, 33C55
DOI: 10.1016/j.jcp.2024.112953
\\ ( https://arxiv.org/abs/2309.09574 ,  3665kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15531
replaced with revised version Sun, 24 Mar 2024 20:18:28 GMT   (2946kb,D)

Title: Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight
  Quantization of Large Language Models
Authors: Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung
  Kwon, Dongsoo Lee
Categories: cs.LG
Comments: ICLR 2024. 19 pages, 11 figures, 10 tables
\\ ( https://arxiv.org/abs/2309.15531 ,  2946kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00290
replaced with revised version Mon, 25 Mar 2024 01:07:23 GMT   (10kb)

Title: Universality of almost periodicity in bounded discrete time series
Authors: Chikara Nakayama and Tsuyoshi Yoneda
Categories: cs.LG cs.IT math.AP math.DS math.FA math.IT
\\ ( https://arxiv.org/abs/2310.00290 ,  10kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00724
replaced with revised version Sat, 23 Mar 2024 09:41:47 GMT   (6928kb,D)

Title: Subtractive Mixture Models via Squaring: Representation and Learning
Authors: Lorenzo Loconte, Aleksanteri M. Sladek, Stefan Mengel, Martin Trapp,
  Arno Solin, Nicolas Gillis, Antonio Vergari
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.00724 ,  6928kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01327
replaced with revised version Mon, 25 Mar 2024 15:55:22 GMT   (5176kb,D)

Title: TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate
  Time Series
Authors: Arjun Ashok, \'Etienne Marcotte, Valentina Zantedeschi, Nicolas
  Chapados, Alexandre Drouin
Categories: cs.LG cs.AI stat.ML
Comments: 28 pages, 15 figures, The Twelfth International Conference on
  Learning Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2310.01327 ,  5176kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02710
replaced with revised version Fri, 22 Mar 2024 18:49:46 GMT   (4098kb,D)

Title: Local Search GFlowNets
Authors: Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua
  Bengio, Sungsoo Ahn, Jinkyoo Park
Categories: cs.LG stat.ML
Comments: ICLR 2024 (Spotlight paper), 18 pages, 17 figures
\\ ( https://arxiv.org/abs/2310.02710 ,  4098kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05884
replaced with revised version Mon, 25 Mar 2024 17:58:36 GMT   (6094kb,D)

Title: A Meta-Learning Perspective on Transformers for Causal Language Modeling
Authors: Xinbo Wu, Lav R. Varshney
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2310.05884 ,  6094kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08282
replaced with revised version Mon, 25 Mar 2024 06:21:37 GMT   (9683kb,D)

Title: Data driven modeling for self-similar dynamics
Authors: Ruyi Tao, Ningning Tao, Yi-zhuang You, Jiang Zhang
Categories: cs.LG cond-mat.stat-mech
Comments: 10 pages,7 figures,1 table
\\ ( https://arxiv.org/abs/2310.08282 ,  9683kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08446
replaced with revised version Sat, 23 Mar 2024 14:01:39 GMT   (6369kb,D)

Title: Towards Robust Multi-Modal Reasoning via Model Selection
Authors: Xiangyan Liu, Rongxue Li, Wei Ji, Tao Lin
Categories: cs.LG cs.AI
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.08446 ,  6369kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08685
replaced with revised version Sat, 23 Mar 2024 18:52:44 GMT   (38485kb,D)

Title: Kernel-Elastic Autoencoder for Molecular Design
Authors: Haote Li, Yu Shee, Brandon Allen, Federica Maschietto, Victor Batista
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.08685 ,  38485kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11959
replaced with revised version Sun, 24 Mar 2024 11:50:28 GMT   (479kb,D)

Title: A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis
Authors: Shuhan Zhong, Sizhe Song, Weipeng Zhuo, Guanyao Li, Yang Liu, S.-H.
  Gary Chan
Categories: cs.LG cs.AI
Comments: Accepted for VLDB 2024
\\ ( https://arxiv.org/abs/2310.11959 ,  479kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14714
replaced with revised version Mon, 25 Mar 2024 08:58:39 GMT   (1428kb,D)

Title: BatteryML:An Open-source platform for Machine Learning on Battery
  Degradation
Authors: Han Zhang, Xiaofan Gui, Shun Zheng, Ziheng Lu, Yuqi Li, Jiang Bian
Categories: cs.LG cs.AI
MSC-class: 68T05
\\ ( https://arxiv.org/abs/2310.14714 ,  1428kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18411
replaced with revised version Sat, 23 Mar 2024 07:35:03 GMT   (880kb,D)

Title: A general learning scheme for classical and quantum Ising machines
Authors: Ludwig Schmid, Enrico Zardini, Davide Pastorello
Categories: cs.LG quant-ph
Comments: 25 pages, 9 figures, updated to the version published on SciPost
Journal-ref: SciPost Phys. Core 7, 013 (2024)
DOI: 10.21468/SciPostPhysCore.7.1.013
\\ ( https://arxiv.org/abs/2310.18411 ,  880kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08118
replaced with revised version Sun, 24 Mar 2024 12:21:35 GMT   (93kb,D)

Title: Evaluating Neighbor Explainability for Graph Neural Networks
Authors: Oscar Llorente Gonzalez, Rana Fawzy, Jared Keown, Michal Horemuz,
  P\'eter Vaderna, S\'andor Laki, Roland Kotrocz\'o, Rita Csoma and J\'anos
  M\'ark Szalai-Gindl
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2311.08118 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10610
replaced with revised version Mon, 25 Mar 2024 16:05:04 GMT   (552kb)

Title: A Poincar\'e Inequality and Consistency Results for Signal Sampling on
  Large Graphs
Authors: Thien Le, Luana Ruiz, Stefanie Jegelka
Categories: cs.LG stat.ML
Comments: 23 pages
\\ ( https://arxiv.org/abs/2311.10610 ,  552kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11058
replaced with revised version Sat, 23 Mar 2024 14:49:30 GMT   (74kb,D)

Title: Tactics2D: A Reinforcement Learning Environment Library with Generative
  Scenarios for Driving Decision-making
Authors: Yueyuan Li, Songan Zhang, Mingyang Jiang, Xingyuan Chen, Ming Yang
Categories: cs.LG
Comments: 6 pages, 1 figure, 1 table
\\ ( https://arxiv.org/abs/2311.11058 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11202
replaced with revised version Sun, 24 Mar 2024 22:02:47 GMT   (193kb)

Title: Unmasking and Improving Data Credibility: A Study with Datasets for
  Training Harmless Language Models
Authors: Zhaowei Zhu, Jialu Wang, Hao Cheng, Yang Liu
Categories: cs.LG cs.AI cs.CL cs.CY
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2311.11202 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13231
replaced with revised version Sat, 23 Mar 2024 05:23:00 GMT   (23515kb,D)

Title: Using Human Feedback to Fine-tune Diffusion Models without Any Reward
  Model
Authors: Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li,
  Weihan Shen, Xiaolong Zhu, Xiu Li
Categories: cs.LG cs.AI cs.CV
Comments: CVPR 2024 accepted; huggingface daily paper
\\ ( https://arxiv.org/abs/2311.13231 ,  23515kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15001
replaced with revised version Mon, 25 Mar 2024 17:01:08 GMT   (1119kb,D)

Title: Discovering modular solutions that generalize compositionally
Authors: Simon Schug, Seijin Kobayashi, Yassir Akram, Maciej Wo{\l}czyk,
  Alexandra Proca, Johannes von Oswald, Razvan Pascanu, Jo\~ao Sacramento,
  Angelika Steger
Categories: cs.LG cs.NE
Comments: Published as a conference paper at ICLR 2024; Code available at
  https://github.com/smonsays/modular-hyperteacher
\\ ( https://arxiv.org/abs/2312.15001 ,  1119kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16430
replaced with revised version Mon, 25 Mar 2024 06:32:49 GMT   (123kb,D)

Title: Preference as Reward, Maximum Preference Optimization with Importance
  Sampling
Authors: Zaifan Jiang, Xing Huang, Chao Wei
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.16430 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03695
replaced with revised version Sun, 24 Mar 2024 02:03:55 GMT   (1150kb,D)

Title: A Large-Scale Empirical Study on Improving the Fairness of Image
  Classification Models
Authors: Junjie Yang, Jiajun Jiang, Zeyu Sun, Junjie Chen
Categories: cs.LG cs.AI cs.CV
Comments: Accepted by the 33rd ACM SIGSOFT International Symposium on Software
  Testing and Analysis (ISSTA 2024). Please include ISSTA in any citations
\\ ( https://arxiv.org/abs/2401.03695 ,  1150kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11565
replaced with revised version Fri, 22 Mar 2024 21:33:47 GMT   (4080kb,D)

Title: Thompson Sampling for Stochastic Bandits with Noisy Contexts: An
  Information-Theoretic Regret Analysis
Authors: Sharu Theresa Jose and Shana Moothedath
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2401.11565 ,  4080kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17548
replaced with revised version Sun, 24 Mar 2024 13:29:40 GMT   (3732kb,D)

Title: Rethinking Channel Dependence for Multivariate Time Series Forecasting:
  Learning from Leading Indicators
Authors: Lifan Zhao, Yanyan Shen
Categories: cs.LG cs.AI
Comments: Accepted to ICLR 2024
Journal-ref: The Twelfth International Conference on Learning Representations,
  2024
\\ ( https://arxiv.org/abs/2401.17548 ,  3732kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01295
replaced with revised version Mon, 25 Mar 2024 15:17:45 GMT   (6451kb,D)

Title: ExtremeCast: Boosting Extreme Value Prediction for Global Weather
  Forecast
Authors: Wanghan Xu, Kang Chen, Tao Han, Hao Chen, Wanli Ouyang, Lei Bai
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.01295 ,  6451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02023
replaced with revised version Sun, 24 Mar 2024 04:01:18 GMT   (8393kb,D)

Title: Self-Supervised Contrastive Learning for Long-term Forecasting
Authors: Junwoo Park, Daehoon Gwak, Jaegul Choo, Edward Choi
Categories: cs.LG cs.AI
Comments: Accepted at International Conference on Learning Representations
  (ICLR) 2024
\\ ( https://arxiv.org/abs/2402.02023 ,  8393kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02423
replaced with revised version Mon, 25 Mar 2024 13:20:46 GMT   (4487kb,D)

Title: Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement
  Learning with Diverse Human Feedback
Authors: Yifu Yuan, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu,
  Zhixin Feng, Kai Zhao, Yan Zheng
Categories: cs.LG cs.AI cs.HC cs.RO
Comments: Published as a conference paper at ICLR 2024. The website is
  available at https://uni-rlhf.github.io/
\\ ( https://arxiv.org/abs/2402.02423 ,  4487kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11922
replaced with revised version Mon, 25 Mar 2024 11:39:57 GMT   (3834kb,D)

Title: Spatio-Temporal Few-Shot Learning via Diffusive Neural Network
  Generation
Authors: Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.11922 ,  3834kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03967
replaced with revised version Sat, 23 Mar 2024 11:22:00 GMT   (893kb,D)

Title: Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability
Authors: Rajdeep Haldar, Yue Xing, Qifan Song
Categories: cs.LG cs.CR stat.ML
Comments: AISTATS 2024
\\ ( https://arxiv.org/abs/2403.03967 ,  893kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04810
replaced with revised version Sat, 23 Mar 2024 07:03:34 GMT   (50kb,D)

Title: Restricted Bayesian Neural Network
Authors: Sourav Ganguly
Categories: cs.LG cs.AI cs.NE
\\ ( https://arxiv.org/abs/2403.04810 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09863
replaced with revised version Mon, 25 Mar 2024 17:55:25 GMT   (787kb,D)

Title: Towards White Box Deep Learning
Authors: Maciej Satkiewicz
Categories: cs.LG cs.AI cs.NE
Comments: 13 pages, 9 figures, independent research, v2 changes: more adequate
  title; added: related research in Introduction, Ablation Study, Discussion,
  examples in Further Research, Appendix C; minor wording changes (including
  abstract)
\\ ( https://arxiv.org/abs/2403.09863 ,  787kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10855
replaced with revised version Mon, 25 Mar 2024 16:07:24 GMT   (526kb,D)

Title: Reinforcement Learning with Options and State Representation
Authors: Ayoub Ghriss and Masashi Sugiyama and Alessandro Lazaric
Categories: cs.LG cs.RO
Comments: Master Thesis 2018, MVA ENS Paris-Saclay, Tokyo RIKEN AIP
\\ ( https://arxiv.org/abs/2403.10855 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11259
replaced with revised version Sat, 23 Mar 2024 08:27:02 GMT   (743kb)

Title: A learning-based solution approach to the application placement problem
  in mobile edge computing under uncertainty
Authors: Taha-Hossein Hejazi, Zahra Ghadimkhani, Arezoo Borji
Categories: cs.LG cs.AI cs.DC eess.SP
\\ ( https://arxiv.org/abs/2403.11259 ,  743kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11722
replaced with revised version Mon, 25 Mar 2024 13:34:40 GMT   (113kb,D)

Title: Time Series Compression using Quaternion Valued Neural Networks and
  Quaternion Backpropagation
Authors: Johannes P\"oppelbaum and Andreas Schwung
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.11722 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11782
replaced with revised version Sun, 24 Mar 2024 10:12:42 GMT   (2669kb,D)

Title: A tutorial on learning from preferences and choices with Gaussian
  Processes
Authors: Alessio Benavoli and Dario Azzimonti
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2403.11782 ,  2669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13247
replaced with revised version Sun, 24 Mar 2024 20:58:50 GMT   (6511kb,D)

Title: FedNMUT -- Federated Noisy Model Update Tracking Convergence Analysis
Authors: Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl Hashemi, and
  Stanislaw H. \.Zak
Categories: cs.LG cs.DC
Comments: arXiv admin note: text overlap with arXiv:2303.10695
\\ ( https://arxiv.org/abs/2403.13247 ,  6511kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13841
replaced with revised version Sat, 23 Mar 2024 18:27:49 GMT   (4393kb,D)

Title: Integrating Wearable Sensor Data and Self-reported Diaries for
  Personalized Affect Forecasting
Authors: Zhongqi Yang, Yuning Wang, Ken S. Yamashita, Maryam Sabah, Elahe
  Khatibi, Iman Azimi, Nikil Dutt, Jessica L. Borelli, and Amir M. Rahmani
Categories: cs.LG cs.AI
Comments: Accepted by Connected Health: Applications, Systems and Engineering
  Technologies (CHASE) 2024
\\ ( https://arxiv.org/abs/2403.13841 ,  4393kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14092
replaced with revised version Mon, 25 Mar 2024 17:49:07 GMT   (1214kb)

Title: Carbon Footprint Reduction for Sustainable Data Centers in Real-Time
Authors: Soumyendu Sarkar, Avisek Naug, Ricardo Luna, Antonio Guillen, Vineet
  Gundecha, Sahand Ghorbanpour, Sajad Mousavi, Dejan Markovikj, Ashwin Ramesh
  Babu
Categories: cs.LG cs.AI cs.MA cs.SY eess.SY
Journal-ref: 2024 Proceedings of the AAAI Conference on Artificial Intelligence
\\ ( https://arxiv.org/abs/2403.14092 ,  1214kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14587
replaced with revised version Mon, 25 Mar 2024 12:00:19 GMT   (844kb,D)

Title: An Analysis of Linear Time Series Forecasting Models
Authors: William Toner, Luke Darlow
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.14587 ,  844kb)
------------------------------------------------------------------------------
\\
arXiv:1909.03820
replaced with revised version Sat, 23 Mar 2024 22:14:50 GMT   (139kb,D)

Title: Learning Concepts Definable in First-Order Logic with Counting
Authors: Steffen van Bergerem
Categories: cs.LO cs.AI cs.LG
Journal-ref: 34th Annual ACM/IEEE Symposium on Logic in Computer Science, LICS
  2019, Vancouver, BC, Canada, June 24-27, 2019
DOI: 10.1109/LICS.2019.8785811
\\ ( https://arxiv.org/abs/1909.03820 ,  139kb)
------------------------------------------------------------------------------
\\
arXiv:2012.04132 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 01:23:11 GMT   (18941kb,D)

Title: A Number Sense as an Emergent Property of the Manipulating Brain
Authors: Neehar Kondapaneni, Pietro Perona
Categories: q-bio.NC cs.AI cs.CV cs.RO
Comments: 16 pages, 5 figures, 15 supplemental figures
Journal-ref: Scientific reports, 14(6858) 2024
DOI: 10.1038/s41598-024-56828-2
\\ ( https://arxiv.org/abs/2012.04132 ,  18941kb)
------------------------------------------------------------------------------
\\
arXiv:2202.12074
replaced with revised version Sat, 23 Mar 2024 10:24:47 GMT   (171kb)

Title: On The Effectiveness of One-Class Support Vector Machine in Different
  Defect Prediction Scenarios
Authors: Rebecca Moussa, Danielle Azar and Federica Sarro
Categories: cs.SE cs.AI cs.LG
Comments: Published at SANER'24 (Winner of the Best RENE paper award) see
  https://conf.researchr.org/details/saner-2024/saner-2024-reproducibility-studies-and-negative-results--rene--track-/78/On-The-Effectiveness-of-One-Class-Support-Vector-Machine-in-Different-Defect-Predicti
\\ ( https://arxiv.org/abs/2202.12074 ,  171kb)
------------------------------------------------------------------------------
\\
arXiv:2209.00915
replaced with revised version Sun, 24 Mar 2024 19:39:00 GMT   (3968kb,D)

Title: Detection of diabetic retinopathy using longitudinal self-supervised
  learning
Authors: Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Ramin
  Tadayoni, Pascal Massin, B\'eatrice Cochener, Gwenol\'e Quellec, Mathieu
  Lamard
Categories: cs.CV cs.AI cs.LG
Comments: Accepted preprint for presentation at MICCAI-OMIA
\\ ( https://arxiv.org/abs/2209.00915 ,  3968kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10681 (*cross-listing*)
replaced with revised version Sat, 23 Mar 2024 15:51:29 GMT   (9091kb,D)

Title: FrankenSplit: Efficient Neural Feature Compression with Shallow
  Variational Bottleneck Injection for Mobile Edge Computing
Authors: Alireza Furutanpey, Philipp Raith, Schahram Dustdar
Categories: eess.IV cs.AI cs.DC cs.LG
Comments: Submission to IEEE Transactions on Mobile Computing
\\ ( https://arxiv.org/abs/2302.10681 ,  9091kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06928
replaced with revised version Mon, 25 Mar 2024 03:40:19 GMT   (38503kb,D)

Title: CiPR: An Efficient Framework with Cross-instance Positive Relations for
  Generalized Category Discovery
Authors: Shaozhe Hao, Kai Han, Kwan-Yee K. Wong
Categories: cs.CV cs.AI
Comments: Accepted to TMLR. Code: https://github.com/haoosz/CiPR
\\ ( https://arxiv.org/abs/2304.06928 ,  38503kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11959
replaced with revised version Mon, 25 Mar 2024 15:15:41 GMT   (8584kb,D)

Title: A Forward and Backward Compatible Framework for Few-shot
  Class-incremental Pill Recognition
Authors: Jinghua Zhang, Li Liu, Kai Gao, and Dewen Hu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2304.11959 ,  8584kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09985
replaced with revised version Sun, 24 Mar 2024 15:53:57 GMT   (5411kb,D)

Title: Our Model Achieves Excellent Performance on MovieLens: What Does it
  Mean?
Authors: Yu-chen Fan, Yitong Ji, Jie Zhang, Aixin Sun
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2307.09985 ,  5411kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02935
replaced with revised version Mon, 25 Mar 2024 11:45:58 GMT   (3997kb,D)

Title: Unveiling the Blind Spots: A Critical Examination of Fairness in
  Autonomous Driving Systems
Authors: Xinyue Li and Zhenpeng Chen and Jie M. Zhang and Federica Sarro and
  Ying Zhang and Xuanzhe Liu
Categories: cs.CY cs.AI cs.CV cs.SE
Comments: Update the models evaluated and the experimental results
\\ ( https://arxiv.org/abs/2308.02935 ,  3997kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15991
replaced with revised version Sun, 24 Mar 2024 04:45:03 GMT   (5883kb,D)

Title: DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous
  Driving
Authors: Yinda Xu, Lidong Yu
Categories: cs.RO cs.AI
Comments: Technical report. Code:
  https://github.com/MARMOTatZJU/drl-based-trajectory-tracking Documentation:
  https://drl-based-trajectory-tracking.readthedocs.io
\\ ( https://arxiv.org/abs/2308.15991 ,  5883kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01157
replaced with revised version Sat, 23 Mar 2024 17:05:42 GMT   (853kb,D)

Title: Large Language Models for Generative Recommendation: A Survey and
  Visionary Discussions
Authors: Lei Li, Yongfeng Zhang, Dugang Liu, Li Chen
Categories: cs.IR cs.AI cs.CL
Comments: Published as a conference paper at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2309.01157 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14552
replaced with revised version Sat, 23 Mar 2024 14:15:02 GMT   (4449kb,D)

Title: Tactile Estimation of Extrinsic Contact Patch for Stable Placement
Authors: Kei Ota, Devesh K. Jha, Krishna Murthy Jatavallabhula, Asako Kanezaki,
  and Joshua B. Tenenbaum
Categories: cs.RO cs.AI cs.LG
Comments: Accepted at ICRA2024
\\ ( https://arxiv.org/abs/2309.14552 ,  4449kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04662
replaced with revised version Fri, 22 Mar 2024 22:35:18 GMT   (40107kb,D)

Title: HalluciDet: Hallucinating RGB Modality for Person Detection Through
  Privileged Information
Authors: Heitor Rapela Medeiros, Fidel A. Guerrero Pena, Masih Aminbeidokhti,
  Thomas Dubail, Eric Granger, Marco Pedersoli
Categories: cs.CV cs.AI
Comments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
  2024
Journal-ref: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision 2024
\\ ( https://arxiv.org/abs/2310.04662 ,  40107kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04870
replaced with revised version Sun, 24 Mar 2024 18:10:03 GMT   (2714kb,D)

Title: Lemur: Integrating Large Language Models in Automated Program
  Verification
Authors: Haoze Wu, Clark Barrett, Nina Narodytska
Categories: cs.FL cs.AI cs.LG cs.LO
Comments: Accepted at ICLR'24
\\ ( https://arxiv.org/abs/2310.04870 ,  2714kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05022
replaced with revised version Sat, 23 Mar 2024 06:58:58 GMT   (10324kb,D)

Title: Fully Spiking Neural Network for Legged Robots
Authors: Xiaoyang Jiang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Jingtong Ma,
  Renjing Xu
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2310.05022 ,  10324kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07471
replaced with revised version Mon, 25 Mar 2024 11:07:13 GMT   (4142kb,D)

Title: The Implications of Decentralization in Blockchained Federated Learning:
  Evaluating the Impact of Model Staleness and Inconsistencies
Authors: Francesc Wilhelmi, Nima Afraz, Elia Guerra, Paolo Dini
Categories: cs.NI cs.AI cs.DC
\\ ( https://arxiv.org/abs/2310.07471 ,  4142kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01623
replaced with revised version Sun, 24 Mar 2024 23:13:06 GMT   (10619kb,D)

Title: VQPy: An Object-Oriented Approach to Modern Video Analytics
Authors: Shan Yu, Zhenting Zhu, Yu Chen, Hanchen Xu, Pengzhan Zhao, Yang Wang,
  Arthi Padmanabhan, Hugo Latapie, Harry Xu
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: MLSys'24
\\ ( https://arxiv.org/abs/2311.01623 ,  10619kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02787
replaced with revised version Sun, 24 Mar 2024 23:36:06 GMT   (29852kb,D)

Title: Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable
  Manipulation with Tools
Authors: Yang You, Bokui Shen, Congyue Deng, Haoran Geng, Songlin Wei, He Wang,
  Leonidas Guibas
Categories: cs.RO cs.AI
Comments: 8 pages
\\ ( https://arxiv.org/abs/2311.02787 ,  29852kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03056 (*cross-listing*)
replaced with revised version Mon, 25 Mar 2024 15:00:57 GMT   (737kb)

Title: LitSumm: Large language models for literature summarisation of
  non-coding RNAs
Authors: Andrew Green, Carlos Ribas, Nancy Ontiveros-Palacios, Sam
  Griffiths-Jones, Anton I. Petrov, Alex Bateman and Blake Sweeney
Categories: q-bio.GN cs.AI
\\ ( https://arxiv.org/abs/2311.03056 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06623
replaced with revised version Sat, 23 Mar 2024 16:27:17 GMT   (1463kb,D)

Title: VT-Former: An Exploratory Study on Vehicle Trajectory Prediction for
  Highway Surveillance through Graph Isomorphism and Transformer
Authors: Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed
  Tabkhi
Categories: cs.CV cs.AI
Comments: Completely updated based on the reviews received for the paper
\\ ( https://arxiv.org/abs/2311.06623 ,  1463kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13614
replaced with revised version Mon, 25 Mar 2024 03:39:45 GMT   (2560kb,D)

Title: HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction
  Data
Authors: Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng
  Qin, Siliang Tang, Qi Tian, Yueting Zhuang
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024
\\ ( https://arxiv.org/abs/2311.13614 ,  2560kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16515
replaced with revised version Mon, 25 Mar 2024 12:01:59 GMT   (1477kb,D)

Title: Word4Per: Zero-shot Composed Person Retrieval
Authors: Delong Liu, Haiwen Li, Zhicheng Zhao, Fei Su, Yuan Dong
Categories: cs.CV cs.AI cs.IR
\\ ( https://arxiv.org/abs/2311.16515 ,  1477kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11834
replaced with revised version Mon, 25 Mar 2024 07:00:29 GMT   (570kb,D)

Title: Multi-agent reinforcement learning using echo-state network and its
  application to pedestrian dynamics
Authors: Hisato Komatsu
Categories: cs.MA cs.AI cs.LG physics.soc-ph
Comments: 26 pages, 17 figures
\\ ( https://arxiv.org/abs/2312.11834 ,  570kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13964
replaced with revised version Mon, 25 Mar 2024 05:18:04 GMT   (46063kb,D)

Title: PIA: Your Personalized Image Animator via Plug-and-Play Modules in
  Text-to-Image Models
Authors: Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen
Categories: cs.CV cs.AI
Comments: Project page: https://pi-animator.github.io/
\\ ( https://arxiv.org/abs/2312.13964 ,  46063kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14149
replaced with revised version Mon, 25 Mar 2024 15:06:33 GMT   (7122kb,D)

Title: TagAlign: Improving Vision-Language Alignment with Multi-Tag
  Classification
Authors: Qinying Liu, Wei Wu, Kecheng Zheng, Zhan Tong, Yu Liu, Wei Chen, Zilei
  Wang, Yujun Shen
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2312.14149 ,  7122kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14481
replaced with revised version Sat, 23 Mar 2024 03:13:35 GMT   (1159kb,D)

Title: SurgicalPart-SAM: Part-to-Whole Collaborative Prompting for Surgical
  Instrument Segmentation
Authors: Wenxi Yue, Jing Zhang, Kun Hu, Qiuxia Wu, Zongyuan Ge, Yong Xia, Jiebo
  Luo, Zhiyong Wang
Categories: cs.CV cs.AI cs.RO
Comments: Technical Report. The source code will be released at
  https://github.com/wenxi-yue/SurgicalPart-SAM
\\ ( https://arxiv.org/abs/2312.14481 ,  1159kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16476
replaced with revised version Mon, 25 Mar 2024 11:24:45 GMT   (45667kb,D)

Title: SVGDreamer: Text Guided SVG Generation with Diffusion Model
Authors: Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, Qian Yu
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024. project link:
  https://ximinng.github.io/SVGDreamer-project/
\\ ( https://arxiv.org/abs/2312.16476 ,  45667kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05010
replaced with revised version Sun, 24 Mar 2024 12:32:06 GMT   (5124kb,D)

Title: Less is More: A Closer Look at Semantic-based Few-Shot Learning
Authors: Chunpeng Zhou, Haishuai Wang, Xilu Yuan, Zhi Yu, Jiajun Bu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.05010 ,  5124kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05224
replaced with revised version Fri, 22 Mar 2024 18:39:41 GMT   (32634kb,D)

Title: Do Vision and Language Encoders Represent the World Similarly?
Authors: Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou
  Djilali, Sanath Narayan, Mohamed El Amine Seddik, Karttikeya Mangalam, Noel
  E. O'Connor
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Accepted CVPR 2024
\\ ( https://arxiv.org/abs/2401.05224 ,  32634kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06461
replaced with revised version Sun, 24 Mar 2024 01:20:49 GMT   (2719kb,D)

Title: Between Lines of Code: Unraveling the Distinct Patterns of Machine and
  Human Programmers
Authors: Yuling Shi, Hongyu Zhang, Chengcheng Wan, Xiaodong Gu
Categories: cs.SE cs.AI cs.CL
Comments: code available at https://github.com/YerbaPage/DetectCodeGPT
\\ ( https://arxiv.org/abs/2401.06461 ,  2719kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15842
replaced with revised version Sat, 23 Mar 2024 02:46:54 GMT   (1908kb)

Title: LCV2: An Efficient Pretraining-Free Framework for Grounded Visual
  Question Answering
Authors: Yuhan Chen, Lumei Su, Lihua Chen, Zhiwei Lin
Categories: cs.CV cs.AI
Comments: 21 pages,9 figures
\\ ( https://arxiv.org/abs/2401.15842 ,  1908kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04599
replaced with revised version Mon, 25 Mar 2024 13:30:37 GMT   (10370kb,D)

Title: Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via
  Temporal-Viewpoint Alignment
Authors: Lei Wang and Jun Liu and Liang Zheng and Tom Gedeon and Piotr Koniusz
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by the International Journal of Computer Vision (IJCV). An
  extension of our ACCV'22 paper [arXiv:arXiv:2210.16820] which was
  distinguished by the Sang Uk Lee Best Student Paper Award
\\ ( https://arxiv.org/abs/2402.04599 ,  10370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05355
replaced with revised version Fri, 22 Mar 2024 22:47:50 GMT   (596kb,D)

Title: A Survey on Safe Multi-Modal Learning System
Authors: Tianyi Zhao, Liangliang Zhang, Yao Ma and Lu Cheng
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2402.05355 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09494
replaced with revised version Mon, 25 Mar 2024 16:32:44 GMT   (850kb)

Title: Can AI and humans genuinely communicate?
Authors: Constant Bonard
Categories: cs.HC cs.AI
Comments: March 2024 preprint
\\ ( https://arxiv.org/abs/2402.09494 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11734
replaced with revised version Mon, 25 Mar 2024 03:23:01 GMT   (323kb,D)

Title: Solving Data-centric Tasks using Large Language Models
Authors: Shraddha Barke, Christian Poelitz, Carina Suzana Negreanu, Benjamin
  Zorn, Jos\'e Cambronero, Andrew D. Gordon, Vu Le, Elnaz Nouri, Nadia
  Polikarpova, Advait Sarkar, Brian Slininger, Neil Toronto, Jack Williams
Categories: cs.PL cs.AI cs.SE
Comments: Paper accepted to NAACL 2024 (Findings)
\\ ( https://arxiv.org/abs/2402.11734 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12928
replaced with revised version Sun, 24 Mar 2024 10:06:59 GMT   (5287kb,D)

Title: A Literature Review of Literature Reviews in Pattern Analysis and
  Machine Intelligence
Authors: Penghai Zhao, Xin Zhang, Ming-Ming Cheng, Jian Yang, Xiang Li
Categories: cs.DL cs.AI cs.CV
Comments: IEEE version v1. [February 19, 2024] IEEE version v2 with typos
  fixed. [February 23, 2024] IEEE version v3 with errors fixed. [February 29,
  2024] IEEE version v4 with improved quaility. [February 29, 2024]
\\ ( https://arxiv.org/abs/2402.12928 ,  5287kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00046
replaced with revised version Sat, 23 Mar 2024 16:51:11 GMT   (644kb,D)

Title: SEED: Customize Large Language Models with Sample-Efficient Adaptation
  for Code Generation
Authors: Xue Jiang, Yihong Dong, Zhi Jin, Ge Li
Categories: cs.SE cs.AI cs.CL
\\ ( https://arxiv.org/abs/2403.00046 ,  644kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03881
replaced with revised version Sun, 24 Mar 2024 21:38:49 GMT   (60221kb,D)

Title: Latent Dataset Distillation with Diffusion Models
Authors: Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov and
  Andreas Dengel
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.03881 ,  60221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05030
replaced with revised version Fri, 22 Mar 2024 19:49:42 GMT   (1142kb,D)

Title: Defending Against Unforeseen Failure Modes with Latent Adversarial
  Training
Authors: Stephen Casper, Lennart Schulze, Oam Patel, Dylan Hadfield-Menell
Categories: cs.CR cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.05030 ,  1142kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06054 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 19:14:59 GMT   (13983kb,D)

Title: Decoupled Data Consistency with Diffusion Purification for Image
  Restoration
Authors: Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar,
  Qing Qu
Categories: eess.IV cs.AI cs.CV cs.LG eess.SP
\\ ( https://arxiv.org/abs/2403.06054 ,  13983kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06725
replaced with revised version Sat, 23 Mar 2024 08:14:28 GMT   (455kb,D)

Title: Improving Low-Resource Knowledge Tracing Tasks by Supervised
  Pre-training and Importance Mechanism Fine-tuning
Authors: Hengyuan Zhang, Zitao Liu, Shuyan Huang, Chenming Shang, Bojun Zhan,
  Yong Jiang
Categories: cs.CY cs.AI cs.CL cs.LG
Comments: 29 pages, 4 figures
\\ ( https://arxiv.org/abs/2403.06725 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06764
replaced with revised version Mon, 25 Mar 2024 13:29:30 GMT   (17542kb,D)

Title: An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference
  Acceleration for Large Vision-Language Models
Authors: Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang
  Zhou, Baobao Chang
Categories: cs.CV cs.AI cs.CL
Comments: 21 papes, 8 figures, code is released at
  https://github.com/pkunlp-icler/FastV
\\ ( https://arxiv.org/abs/2403.06764 ,  17542kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09506
replaced with revised version Mon, 25 Mar 2024 02:45:35 GMT   (1783kb,D)

Title: Don't Judge by the Look: Towards Motion Coherent Video Representation
Authors: Yitian Zhang, Yue Bai, Huan Wang, Yizhou Wang, Yun Fu
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by ICLR2024
\\ ( https://arxiv.org/abs/2403.09506 ,  1783kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11401
replaced with revised version Fri, 22 Mar 2024 18:52:51 GMT   (1461kb,D)

Title: Scene-LLM: Extending Language Model for 3D Visual Understanding and
  Reasoning
Authors: Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, Wenhan Xiong
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.11401 ,  1461kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12061
replaced with revised version Mon, 25 Mar 2024 11:50:42 GMT   (42kb)

Title: Design-Space Exploration of SNN Models using Application-Specific
  Multi-Core Architectures
Authors: Sanaullah and Shamini Koravuna and Ulrich R\"uckert and Thorsten
  Jungeblut
Categories: cs.NE cs.AI
Comments: Abstract Presentation in 2023 Neuro-Inspired Computing Elements
  (NICE) Conference
DOI: 10.13140/RG.2.2.26328.88324
\\ ( https://arxiv.org/abs/2403.12061 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13597
replaced with revised version Sat, 23 Mar 2024 17:05:15 GMT   (368kb,D)

Title: No more optimization rules: LLM-enabled policy-based multi-modal query
  optimizer
Authors: Yifan Wang, Haodi Ma, Daisy Zhe Wang
Categories: cs.DB cs.AI cs.IR
Comments: Yifan and Haodi contribute equally to the work
\\ ( https://arxiv.org/abs/2403.13597 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13731
replaced with revised version Sat, 23 Mar 2024 06:31:11 GMT   (385kb,D)

Title: Emotion Recognition Using Transformers with Masked Learning
Authors: Seongjae Min, Junseok Yang, Sangjun Lim, Junyong Lee, Sangwon Lee and
  Sejoon Lim
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.13731 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14119
replaced with revised version Sun, 24 Mar 2024 17:16:53 GMT   (5980kb,D)

Title: C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via
  Text Feature Dispersion
Authors: Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark
  Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2403.14119 ,  5980kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14643
replaced with revised version Mon, 25 Mar 2024 05:35:12 GMT   (611kb)

Title: Exploring ChatGPT and its Impact on Society
Authors: Md. Asraful Haque and Shuai Li
Categories: cs.CY cs.AI cs.CL
Comments: 13 Pages
MSC-class: 68Txx
Journal-ref: AI and Ethics (2024)
DOI: 10.1007/s43681-024-00435-4
\\ ( https://arxiv.org/abs/2403.14643 ,  611kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14689
replaced with revised version Mon, 25 Mar 2024 04:21:13 GMT   (2157kb,D)

Title: Developing and Deploying Industry Standards for Artificial Intelligence
  in Education (AIED): Challenges, Strategies, and Future Directions
Authors: Richard Tong, Haoyang Li, Joleen Liang, Qingsong Wen
Categories: cs.CY cs.AI cs.LG
Comments: 12 pages
\\ ( https://arxiv.org/abs/2403.14689 ,  2157kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14692
replaced with revised version Mon, 25 Mar 2024 01:47:10 GMT   (433kb)

Title: The AI Assessment Scale (AIAS) in action: A pilot implementation of
  GenAI supported assessment
Authors: Leon Furze, Mike Perkins, Jasper Roe, Jason MacVaugh
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2403.14692 ,  433kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15048
replaced with revised version Mon, 25 Mar 2024 02:08:01 GMT   (12202kb,D)

Title: Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning
Authors: Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo
Categories: cs.CV cs.AI cs.LG cs.MM
Comments: 11 pages, 12 figures, 1 table, Project page:
  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/
\\ ( https://arxiv.org/abs/2403.15048 ,  12202kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15317
replaced with revised version Mon, 25 Mar 2024 16:45:41 GMT   (3683kb,D)

Title: Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for
  Weakly Semi-supervised 3D Object Detection
Authors: Hongzhi Gao, Zheng Chen, Zehui Chen, Lin Chen, Jiaming Liu, Shanghang
  Zhang, Feng Zhao
Categories: cs.CV cs.AI
Comments: Accepted by AAAI2024
\\ ( https://arxiv.org/abs/2403.15317 ,  3683kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15388
replaced with revised version Mon, 25 Mar 2024 17:59:55 GMT   (518kb,D)

Title: LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal
  Models
Authors: Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan
Categories: cs.CV cs.AI cs.CL
Comments: Project page: https://llava-prumerge.github.io/
\\ ( https://arxiv.org/abs/2403.15388 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2302.14534
replaced with revised version Sun, 24 Mar 2024 14:34:53 GMT   (1923kb,D)

Title: Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face
Authors: Christopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang,
  Akintunde Oladipo, Jimmy Lin, Martin Potthast
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2302.14534 ,  1923kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14566
replaced with revised version Mon, 25 Mar 2024 06:05:24 GMT   (23051kb,D)

Title: HallusionBench: An Advanced Diagnostic Suite for Entangled Language
  Hallucination and Visual Illusion in Large Vision-Language Models
Authors: Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu
  Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha,
  Tianyi Zhou
Categories: cs.CV cs.CL
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2310.14566 ,  23051kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04786
replaced with revised version Sat, 23 Mar 2024 05:21:49 GMT   (6918kb,D)

Title: Breaking Down the Defenses: A Comparative Survey of Attacks on Large
  Language Models
Authors: Arijit Ghosh Chowdhury, Md Mofijul Islam, Vaibhav Kumar, Faysal
  Hossain Shezan, Vaibhav Kumar, Vinija Jain, Aman Chadha
Categories: cs.CR cs.CL
\\ ( https://arxiv.org/abs/2403.04786 ,  6918kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06199
replaced with revised version Mon, 25 Mar 2024 05:36:56 GMT   (10904kb,D)

Title: Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small
  Language Models
Authors: Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen,
  Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2403.06199 ,  10904kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15365
replaced with revised version Mon, 25 Mar 2024 03:06:08 GMT   (9571kb,D)

Title: A Transfer Attack to Image Watermarks
Authors: Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong
Categories: cs.CR cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.15365 ,  9571kb)
------------------------------------------------------------------------------
\\
arXiv:1807.03162 (*cross-listing*)
replaced with revised version Mon, 25 Mar 2024 14:13:42 GMT   (1003kb,D)

Title: Deep Learning Based Sphere Decoding
Authors: Mostafa Mohammadkarimi, Mehrtash Mehrabi, Masoud Ardakani, and Yindi
  Jing
Categories: eess.SP cs.LG
Journal-ref: IEEE Trans. Wireless Commun. vol. 18, no. 9, pp. 4368-4378, June.
  2019
DOI: 10.1109/TWC.2019.2924220
\\ ( https://arxiv.org/abs/1807.03162 ,  1003kb)
------------------------------------------------------------------------------
\\
arXiv:1908.01978
replaced with revised version Sun, 24 Mar 2024 17:47:06 GMT   (8728kb,D)

Title: Multi-view Deep Subspace Clustering Networks
Authors: Pengfei Zhu, Xinjie Yao, Yu Wang, Binyuan Hui, Dawei Du, Qinghua Hu
Categories: cs.CV cs.LG
Comments: Accepted by T-CYB
\\ ( https://arxiv.org/abs/1908.01978 ,  8728kb)
------------------------------------------------------------------------------
\\
arXiv:2009.13961 (*cross-listing*)
replaced with revised version Sat, 23 Mar 2024 15:01:03 GMT   (0kb,I)

Title: Online Action Learning in High Dimensions: A Conservative Perspective
Authors: Claudio Cardoso Flores and Marcelo Cunha Medeiros
Categories: stat.ML cs.LG econ.EM
Comments: We found an error in the proof of the main theorem which cannot be
  fixed without completely changing the results in the paper
ACM-class: G.3.8
\\ ( https://arxiv.org/abs/2009.13961 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2202.00992 (*cross-listing*)
replaced with revised version Mon, 25 Mar 2024 13:31:05 GMT   (341kb,D)

Title: Tight Convergence Rate Bounds for Optimization Under Power Law Spectral
  Conditions
Authors: Maksim Velikanov and Dmitry Yarotsky
Categories: math.OC cs.LG cs.NE
\\ ( https://arxiv.org/abs/2202.00992 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2206.13254
replaced with revised version Mon, 25 Mar 2024 14:09:32 GMT   (461kb,D)

Title: Sample compression schemes for balls in graphs
Authors: J\'er\'emie Chalopin, Victor Chepoi, Fionn Mc Inerney, S\'ebastien
  Ratel, Yann Vax\`es
Categories: cs.DM cs.LG
Comments: 27 pages, 8 figures
Journal-ref: SIAM Journal on Discrete Mathematics, 37(4):2585-2616, 2023
\\ ( https://arxiv.org/abs/2206.13254 ,  461kb)
------------------------------------------------------------------------------
\\
arXiv:2208.00956 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 05:01:56 GMT   (753kb,D)

Title: An adjoint-free algorithm for conditional nonlinear optimal
  perturbations (CNOPs) via sampling
Authors: Bin Shi, Guodong Sun
Categories: math.OC cs.LG nlin.CD physics.ao-ph
Comments: 20 pages, 6 figures, 4 tables
Journal-ref: Nonlin. Processes Geophys.,30,263-276,2023
DOI: 10.5194/npg-30-263-2023
\\ ( https://arxiv.org/abs/2208.00956 ,  753kb)
------------------------------------------------------------------------------
\\
arXiv:2208.07243 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 07:48:18 GMT   (2684kb,D)

Title: Exponential Concentration in Stochastic Approximation
Authors: Kody Law and Neil Walton and Shangda Yang
Categories: stat.ML cs.LG math.OC
Comments: 35 pages, 11 Figures
\\ ( https://arxiv.org/abs/2208.07243 ,  2684kb)
------------------------------------------------------------------------------
\\
arXiv:2208.08233 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 12:11:18 GMT   (6159kb,D)

Title: Dynamical softassign and adaptive parameter tuning for graph matching
Authors: Binrui Shen, Qiang Niu, Shengxin Zhu
Categories: math.CO cs.LG
\\ ( https://arxiv.org/abs/2208.08233 ,  6159kb)
------------------------------------------------------------------------------
\\
arXiv:2212.09961 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 18:11:41 GMT   (649kb,D)

Title: Uncertainty Quantification of MLE for Entity Ranking with Covariates
Authors: Jianqing Fan, Jikai Hou, Mengxin Yu
Categories: stat.ME cs.LG math.ST stat.ML stat.TH
Comments: 81 pages, 3 figures
\\ ( https://arxiv.org/abs/2212.09961 ,  649kb)
------------------------------------------------------------------------------
\\
arXiv:2212.13332
replaced with revised version Sun, 24 Mar 2024 23:18:18 GMT   (7865kb,D)

Title: Development and Evaluation of a Learning-based Model for Real-time
  Haptic Texture Rendering
Authors: Negin Heravi, Heather Culbertson, Allison M. Okamura, Jeannette Bohg
Categories: cs.RO cs.HC cs.LG
Comments: Accepted for publication in IEEE Transactions on Haptics 2024. 12
  pages, 8 figures
\\ ( https://arxiv.org/abs/2212.13332 ,  7865kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08347 (*cross-listing*)
replaced with revised version Mon, 25 Mar 2024 13:18:39 GMT   (662kb,D)

Title: The autoregressive neural network architecture of the Boltzmann
  distribution of pairwise interacting spins systems
Authors: Indaco Biazzo
Categories: cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML
Comments: 20 pages, 10 figure plus the Supplementary Information
DOI: 10.1038/s42005-023-01416-5
\\ ( https://arxiv.org/abs/2302.08347 ,  662kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16971 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 17:10:42 GMT   (44kb)

Title: Sparse joint shift in multinomial classification
Authors: Dirk Tasche
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 27 pages
MSC-class: 68T10, 62G05
ACM-class: G.3; I.5.1
\\ ( https://arxiv.org/abs/2303.16971 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03111 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 18:43:38 GMT   (6793kb,D)

Title: Bootstrapped Training of Score-Conditioned Generator for Offline Design
  of Biological Sequences
Authors: Minsu Kim, Federico Berto, Sungsoo Ahn, Jinkyoo Park
Categories: q-bio.QM cs.LG stat.ML
Comments: NeurIPS 2023, 19 pages, 5 figures
\\ ( https://arxiv.org/abs/2306.03111 ,  6793kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06721 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 19:40:59 GMT   (249kb,D)

Title: Differentially Private Conditional Independence Testing
Authors: Iden Kalemaj, Shiva Prasad Kasiviswanathan, Aaditya Ramdas
Categories: stat.ML cs.CR cs.LG
\\ ( https://arxiv.org/abs/2306.06721 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08929
replaced with revised version Mon, 25 Mar 2024 08:00:38 GMT   (325kb,D)

Title: On the resilience of Collaborative Learning-based Recommender Systems
  Against Community Detection Attack
Authors: Yacine Belal, Sonia Ben Mokhtar, Mohamed Maouche and Anthony
  Simonet-Boulogne
Categories: cs.IR cs.CR cs.LG cs.SI
ACM-class: H.3.3; I.2.6; I.2.11; K.6.5
\\ ( https://arxiv.org/abs/2306.08929 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10180 (*cross-listing*)
replaced with revised version Mon, 25 Mar 2024 13:02:27 GMT   (22483kb,D)

Title: Samplet basis pursuit: Multiresolution scattered data approximation with
  sparsity constraints
Authors: Davide Baroli, Helmut Harbrecht, and Michael Multerer
Categories: stat.ML cs.LG cs.NA math.NA
\\ ( https://arxiv.org/abs/2306.10180 ,  22483kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04570
replaced with revised version Mon, 25 Mar 2024 13:31:33 GMT   (476kb,D)

Title: A Call to Reflect on Evaluation Practices for Age Estimation:
  Comparative Analysis of the State-of-the-Art and a Unified Benchmark
Authors: Jakub Paplham and Vojtech Franc
Categories: cs.CV cs.LG
Comments: CVPR 2024 Camera-Ready
\\ ( https://arxiv.org/abs/2307.04570 ,  476kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01562
replaced with revised version Sun, 24 Mar 2024 05:50:58 GMT   (7703kb,D)

Title: Hierarchical Federated Learning in Wireless Networks: Pruning Tackles
  Bandwidth Scarcity and System Heterogeneity
Authors: Md Ferdous Pervej, Richeng Jin, Huaiyu Dai
Categories: eess.SY cs.LG cs.NI cs.SY
Comments: Accepted for publications in the IEEE Transactions on Wireless
  Communications (TWC); \copyright 2024 IEEE. Personal use of this material is
  permitted. Permission from IEEE must be obtained for all other uses
\\ ( https://arxiv.org/abs/2308.01562 ,  7703kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02836
replaced with revised version Mon, 25 Mar 2024 03:17:30 GMT   (1242kb,D)

Title: BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial
  Network
Authors: Takashi Shibuya, Yuhta Takida, Yuki Mitsufuji
Categories: cs.SD cs.LG eess.AS
Comments: Accepted at ICASSP 2024. Equation (5) in the previous version is
  wrong. We modified it
\\ ( https://arxiv.org/abs/2309.02836 ,  1242kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05153 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 07:31:23 GMT   (28235kb,D)

Title: Learning Energy-Based Models by Cooperative Diffusion Recovery
  Likelihood
Authors: Yaxuan Zhu, Jianwen Xie, Yingnian Wu, Ruiqi Gao
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2309.05153 ,  28235kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07133 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 23:12:01 GMT   (1925kb)

Title: Assessing cognitive function among older adults using machine learning
  and wearable device data: a feasibility study
Authors: Collin Sakal, Tingyou Li, Juan Li, Xinyue Li
Categories: eess.SP cs.HC cs.LG
\\ ( https://arxiv.org/abs/2309.07133 ,  1925kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07289
replaced with revised version Fri, 22 Mar 2024 21:11:15 GMT   (3321kb,D)

Title: User Training with Error Augmentation for Electromyogram-based Gesture
  Classification
Authors: Yunus Bicer, Niklas Smedemark-Margulies, Basak Celik, Elifnur Sunger,
  Ryan Orendorff, Stephanie Naufel, Tales Imbiriba, Deniz Erdo\u{g}mu\c{s},
  Eugene Tunik, Mathew Yarossi
Categories: cs.HC cs.LG eess.SP
Comments: 10 pages, 10 figures. V2: Fix latex characters in author name. V3:
  Add published DOI and Copyright notice
Journal-ref: in IEEE Transactions on Neural Systems and Rehabilitation
  Engineering, vol. 32, pp. 1187-1197, 2024
DOI: 10.1109/TNSRE.2024.3372512
\\ ( https://arxiv.org/abs/2309.07289 ,  3321kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15366 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 05:06:16 GMT   (3290kb,D)

Title: Density Estimation via Measure Transport: Outlook for Applications in
  the Biological Sciences
Authors: Vanessa Lopez-Marrero, Patrick R. Johnstone, Gilchan Park, Xihaier Luo
Categories: q-bio.QM cs.LG physics.bio-ph
Comments: 46 pages; 18 figures; Introduction expanded; minor revisions
  throughout; results unchanged. sha256:
  e501c3ce7eb1b4228668c18b0194a601196173093d24a0e2cbd546a21be7f1b0
MSC-class: 62G07, 49Q22, 92-08
ACM-class: K.3.2; G.3
\\ ( https://arxiv.org/abs/2309.15366 ,  3290kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01099 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 12:55:31 GMT   (683kb)

Title: HyMNet: a Multimodal Deep Learning System for Hypertension
  Classification using Fundus Photographs and Cardiometabolic Risk Factors
Authors: Mohammed Baharoon, Hessa Almatar, Reema Alduhayan, Tariq Aldebasi,
  Badr Alahmadi, Yahya Bokhari, Mohammed Alawad, Ahmed Almazroa, Abdulrhman
  Aljouie
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2310.01099 ,  683kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07240
replaced with revised version Mon, 25 Mar 2024 16:49:18 GMT   (13491kb,D)

Title: CacheGen: KV Cache Compression and Streaming for Fast Language Model
  Serving
Authors: Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang,
  Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan,
  Michael Maire, Henry Hoffmann, Ari Holtzman, Junchen Jiang
Categories: cs.NI cs.LG
\\ ( https://arxiv.org/abs/2310.07240 ,  13491kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08774 (*cross-listing*)
replaced with revised version Mon, 25 Mar 2024 00:18:35 GMT   (3826kb,D)

Title: PhyloGFN: Phylogenetic inference with generative flow networks
Authors: Mingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai
  Zhang, Moksh Jain, Mathieu Blanchette, Yoshua Bengio
Categories: q-bio.PE cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.08774 ,  3826kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09484
replaced with revised version Sat, 23 Mar 2024 23:04:00 GMT   (2380kb,D)

Title: Fast-DiM: Towards Fast Diffusion Morphs
Authors: Zander W. Blasingame and Chen Liu
Categories: cs.CV cs.LG
Comments: Revised manuscript. Under review for publication
\\ ( https://arxiv.org/abs/2310.09484 ,  2380kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15168
replaced with revised version Sun, 24 Mar 2024 23:32:50 GMT   (11352kb,D)

Title: Ghost on the Shell: An Expressive Representation of General 3D Shapes
Authors: Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J.
  Black, Bernhard Sch\"olkopf
Categories: cs.CV cs.GR cs.LG
Comments: ICLR 2024 Oral (v3: 30 pages, 19 figures, Project Page:
  https://gshell3d.github.io/)
\\ ( https://arxiv.org/abs/2310.15168 ,  11352kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18847
replaced with revised version Sat, 23 Mar 2024 00:28:21 GMT   (5768kb,D)

Title: Bird's Eye View Based Pretrained World model for Visual Navigation
Authors: Kiran Lekkala, Chen Liu, Laurent Itti
Categories: cs.RO cs.LG
Comments: Under Review at the IROS 2024; Accepted at NeurIPS 2023, Robot
  Learning Workshop
\\ ( https://arxiv.org/abs/2310.18847 ,  5768kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00181 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 01:13:59 GMT   (302kb,D)

Title: Best of Both Worlds Guarantees for Smoothed Online Quadratic
  Optimization
Authors: Neelkamal Bhuyan, Debankur Mukherjee, Adam Wierman
Categories: math.OC cs.DS cs.LG math.PR
Comments: 48 pages, 9 figures
\\ ( https://arxiv.org/abs/2311.00181 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02695 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 17:00:40 GMT   (366kb,D)

Title: Identifying Linearly-Mixed Causal Representations from Multi-Node
  Interventions
Authors: Simon Bing, Urmi Ninad, Jonas Wahl, Jakob Runge
Categories: stat.ML cs.LG math.ST stat.ME stat.TH
Comments: Accepted for publication at CLeaR 2024
\\ ( https://arxiv.org/abs/2311.02695 ,  366kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12261
replaced with revised version Sun, 24 Mar 2024 14:18:36 GMT   (4064kb,D)

Title: EnduRL: Enhancing Safety, Stability, and Efficiency of Mixed Traffic
  Under Real-World Perturbations Via Reinforcement Learning
Authors: Bibek Poudel and Weizi Li and Kevin Heaslip
Categories: cs.RO cs.LG
\\ ( https://arxiv.org/abs/2311.12261 ,  4064kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05654
replaced with revised version Mon, 25 Mar 2024 04:32:19 GMT   (1853kb,D)

Title: Spectral methods for Neural Integral Equations
Authors: Emanuele Zappala
Categories: math.NA cs.LG cs.NA physics.comp-ph
Comments: 15 pages, 3 figures and 2 tables. v3: Missing hypotheses for the
  framework have been now added
\\ ( https://arxiv.org/abs/2312.05654 ,  1853kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07310
replaced with revised version Mon, 25 Mar 2024 12:58:45 GMT   (74724kb,D)

Title: BioNeRF: Biologically Plausible Neural Radiance Fields for View
  Synthesis
Authors: Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P.
  Costa, Ahsan Adeel, Jo\~ao Paulo Papa
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.07310 ,  74724kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11858 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 21:33:21 GMT   (500kb)

Title: Stochastic Hessian Fittings on Lie Groups
Authors: Xi-Lin Li
Categories: stat.ML cs.LG math.OC
\\ ( https://arxiv.org/abs/2402.11858 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15893
replaced with revised version Sun, 24 Mar 2024 19:34:34 GMT   (3843kb,D)

Title: Concurrent Learning of Policy and Unknown Safety Constraints in
  Reinforcement Learning
Authors: Lunet Yifru and Ali Baheri
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2402.15893 ,  3843kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02746
replaced with revised version Sat, 23 Mar 2024 09:51:35 GMT   (41914kb,D)

Title: Learning without Exact Guidance: Updating Large-scale High-resolution
  Land Cover Maps from Low-resolution Historical Labels
Authors: Zhuohong Li, Wei He, Jiepan Li, Fangxiao Lu, Hongyan Zhang
Categories: cs.CV cs.LG
Comments: 11 pages, 9 figures, accepted by CVPR 2024
\\ ( https://arxiv.org/abs/2403.02746 ,  41914kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06606
replaced with revised version Mon, 25 Mar 2024 06:57:57 GMT   (3503kb,D)

Title: Distributionally Generative Augmentation for Fair Facial Attribute
  Classification
Authors: Fengda Zhang, Qianpei He, Kun Kuang, Jiashuo Liu, Long Chen, Chao Wu,
  Jun Xiao, Hanwang Zhang
Categories: cs.CV cs.LG
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2403.06606 ,  3503kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09267 (*cross-listing*)
replaced with revised version Sun, 24 Mar 2024 17:24:45 GMT   (5403kb,D)

Title: Deep Limit Order Book Forecasting
Authors: Antonio Briola, Silvia Bartolucci, Tomaso Aste
Categories: q-fin.TR cs.LG
Comments: 43 pages, 14 figures, 12 Tables
\\ ( https://arxiv.org/abs/2403.09267 ,  5403kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10615
replaced with revised version Mon, 25 Mar 2024 09:42:13 GMT   (15191kb,D)

Title: LightIt: Illumination Modeling and Control for Diffusion Models
Authors: Peter Kocsis (1), Julien Philip (2), Kalyan Sunkavalli (2), Matthias
  Nie{\ss}ner (1), Yannick Hold-Geoffroy (2) ((1) Technical University of
  Munich, (2) Adobe Research)
Categories: cs.CV cs.GR cs.LG
Comments: Project page: https://peter-kocsis.github.io/LightIt/ Video:
  https://youtu.be/cCfSBD5aPLI
ACM-class: I.4.8; I.2.10
\\ ( https://arxiv.org/abs/2403.10615 ,  15191kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14137
replaced with revised version Sun, 24 Mar 2024 07:58:01 GMT   (9318kb,D)

Title: SynerMix: Synergistic Mixup Solution for Enhanced Intra-Class Cohesion
  and Inter-Class Separability in Image Classification
Authors: Ye Xu, Ya Gao, Xiaorong Qiu, Yang Chen, Ying Ji
Categories: cs.CV cs.LG
Comments: 25 pages,12 figures
\\ ( https://arxiv.org/abs/2403.14137 ,  9318kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
