240704
========

-------------
Benchmark (4)
-------------

`[2407.02936] GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models <https://arxiv.org/abs/2407.02936>`__ GraCoRe:大型语言模型图理解和复杂推理基准测试

::

    Wed, 3 Jul 2024 09:12:38 GMT
    Zike Yuan, Ming Liu, Hui Wang, Bing Qin

Evaluating the graph comprehension and reasoning abilities of Large Language Models (LLMs) is challenging and often incomplete. Existing benchmarks focus primarily on pure graph understanding, lacking a comprehensive evaluation across all graph types and detailed capability definitions. This paper presents GraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and reasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and test models on pure graph and heterogeneous graphs, subdividing capabilities into 10 distinct areas tested through 19 tasks. Our benchmark includes 11 datasets with 5,140 graphs of varying complexity. We evaluated three closed-source and seven open-source LLMs, conducting thorough analyses from both ability and task perspectives. Key findings reveal that semantic enrichment enhances reasoning performance, node ordering impacts task success, and the ability to process longer texts does not necessarily improve graph comprehension or reasoning. GraCoRe is open-sourced at https://github.com/ZIKEYUAN/GraCoRe

------------

`[2407.02842] MindBench: A Comprehensive Benchmark for Mind Map Structure Recognition and Analysis <https://arxiv.org/abs/2407.02842>`__ MindBench:思维导图结构识别与分析的综合基准

::

    Wed, 3 Jul 2024 06:39:18 GMT
    Lei Chen, Feng Yan, Yujie Zhong, Shaoxiang Chen, Zequn Jie, Lin Ma

Multimodal Large Language Models (MLLM) have made significant progress in the field of document analysis. Despite this, existing benchmarks typically focus only on extracting text and simple layout information, neglecting the complex interactions between elements in structured documents such as mind maps and flowcharts. To address this issue, we introduce the new benchmark named MindBench, which not only includes meticulously constructed bilingual authentic or synthetic images, detailed annotations, evaluation metrics and baseline models, but also specifically designs five types of structured understanding and parsing tasks. These tasks include full parsing, partial parsing, position-related parsing, structured Visual Question Answering (VQA), and position-related VQA, covering key areas such as text recognition, spatial awareness, relationship discernment, and structured parsing. Extensive experimental results demonstrate the substantial potential and significant room for improvement in current models' ability to handle structured document information. We anticipate that the launch of MindBench will significantly advance research and application development in structured document analysis technology. MindBench is available at: https://miasanlei.github.io/MindBench.github.io/.

------------

`[2402.01781] When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards <https://arxiv.org/abs/2402.01781>`__ 当基准为目标时:揭示大型语言模型排行榜的敏感性

::

    replaced with revised version Wed, 3 Jul 2024 11:20:43 GMT
    Submission history From: Haidar Khan [view email]
    [v1] Thu, 1 Feb 2024 19:12:25 UTC (2,723 KB)
    [v2] Wed, 3 Jul 2024 11:20:43 UTC (2,732 KB)
    Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan Alrashed, Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, Nouf Alotaibi, Nora Altwairesh, Areeb Alowisheq, M Saiful Bari, Haidar Khan

Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at this https URL.

------------

`[2406.06331] MedExQA: Medical Question Answering Benchmark with Multiple Explanations <https://arxiv.org/abs/2406.06331>`__ MedExQA:具有多种解释的医疗问答基准

::

    replaced with revised version Wed, 3 Jul 2024 10:13:56 GMT
    Submission history From: Yunsoo Kim [view email]
    [v1] Mon, 10 Jun 2024 14:47:04 UTC (2,074 KB)
    [v2] Wed, 3 Jul 2024 10:13:56 UTC (2,074 KB)
    Yunsoo Kim, Jinge Wu, Yusuf Abdulle, Honghan Wu

This paper introduces MedExQA, a novel benchmark in medical question-answering, to evaluate large language models' (LLMs) understanding of medical knowledge through explanations. By constructing datasets across five distinct medical specialties that are underrepresented in current datasets and further incorporating multiple explanations for each question-answer pair, we address a major gap in current medical QA benchmarks which is the absence of comprehensive assessments of LLMs' ability to generate nuanced medical explanations. Our work highlights the importance of explainability in medical LLMs, proposes an effective methodology for evaluating models beyond classification accuracy, and sheds light on one specific domain, speech language pathology, where current LLMs including GPT4 lack good understanding. Our results show generation evaluation with multiple explanations aligns better with human assessment, highlighting an opportunity for a more robust automated comprehension assessment for LLMs. To diversify open-source medical LLMs (currently mostly based on Llama2), this work also proposes a new medical model, MedPhi-2, based on Phi-2 (2.7B). The model outperformed medical LLMs based on Llama2-70B in generating explanations, showing its effectiveness in the resource-constrained medical domain. We will share our benchmark datasets and the trained model.

------------

--------------
Accelerate (4)
--------------

`[2407.02819] Efficient Training of Language Models with Compact and Consistent Next Token Distributions <https://arxiv.org/abs/2407.02819>`__ 基于紧凑一致的Next Token分布的语言模型高效训练

::

    Wed, 3 Jul 2024 05:40:41 GMT
    Ashutosh Sathe, Sunita Sarawagi

Maximizing the likelihood of the next token is an established, statistically sound objective for pre-training language models. In this paper we show that we can train better models faster by pre-aggregating the corpus with a collapsed $n$-gram distribution. Previous studies have proposed corpus-level $n$-gram statistics as a regularizer; however, the construction and querying of such $n$-grams, if done naively, prove to be costly and significantly impede training speed, thereby limiting their application in modern large language model pre-training.
We introduce an alternative compact representation of the next token distribution that, in expectation, aligns with the complete $n$-gram distribution while markedly reducing variance across mini-batches compared to the standard next-token loss. Empirically, we demonstrate that both the $n$-gram regularized model and our approximation yield substantial improvements in model quality and convergence rate compared to existing methods.
Furthermore, our approximation facilitates scalability of gains to larger datasets and models compared to the straightforward $n$-gram regularization method.

------------

`[2407.02987] LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models <https://arxiv.org/abs/2407.02987>`__ 

::

    Wed, 3 Jul 2024 10:38:40 GMT
    Hayder Elesedy, Pedro M. Esperan\c{c}a, Silviu Vlad Oprea, Mete Ozay

Guardrails have emerged as an alternative to safety alignment for content moderation of large language models (LLMs). Existing model-based guardrails have not been designed for resource-constrained computational portable devices, such as mobile phones, more and more of which are running LLM-based applications locally. We introduce LoRA-Guard, a parameter-efficient guardrail adaptation method that relies on knowledge sharing between LLMs and guardrail models. LoRA-Guard extracts language features from the LLMs and adapts them for the content moderation task using low-rank adapters, while a dual-path design prevents any performance degradation on the generative task. We show that LoRA-Guard outperforms existing approaches with 100-1000x lower parameter overhead while maintaining accuracy, enabling on-device content moderation.

------------

`[2401.09967] Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access <https://arxiv.org/abs/2401.09967>`__ 基于草图引导的约束解码增强无Logit访问的黑盒大型语言模型

::

    replaced with revised version Wed, 3 Jul 2024 02:55:37 GMT
    Submission history From: Saibo Geng [view email]
    [v1] Thu, 18 Jan 2024 13:31:24 UTC (2,568 KB)
    [v2] Wed, 29 May 2024 20:23:39 UTC (4,366 KB)
    [v3] Wed, 3 Jul 2024 02:55:37 UTC (1,419 KB)
    Saibo Geng, Berkay D\"oner, Chris Wendler, Martin Josifoski, Robert West

Constrained decoding, a technique for enforcing constraints on language model outputs, offers a way to control text generation without retraining or architectural modifications. Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a novel approach to constrained decoding for blackbox LLMs, which operates without access to the logits of the blackbox LLM. SGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a "sketch" for further elaboration. This approach is complementary to traditional logit-based techniques and enables the application of constrained decoding in settings where full model transparency is unavailable. We demonstrate the efficacy of SGCD through experiments in closed information extraction and constituency parsing, showing how it enhances the utility and flexibility of blackbox LLMs for complex NLP tasks.

------------

`[2403.08593] Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments <https://arxiv.org/abs/2403.08593>`__ 必要时打电话给我:llm可以高效和忠实地在结构化环境中进行推理

::

    replaced with revised version Wed, 3 Jul 2024 15:23:59 GMT
    Submission history From: Sitao Cheng [view email]
    [v1] Wed, 13 Mar 2024 14:59:07 UTC (4,274 KB)
    [v2] Wed, 3 Jul 2024 15:23:59 UTC (4,277 KB)
    Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting Qin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang

Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graph and table. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous methods leverage LLMs to incrementally build a reasoning path, where the LLMs either invoke tools or pick up schemas by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA and two TableQA datasets show the effectiveness of Readi, significantly surpassing previous LLM-based methods (by 9.1% Hit@1 on WebQSP, 12.4% on MQA-3H and 9.5% on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and 74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ). Our code will be available on this https URL.

------------

-----------------------
In-Context Learning (1)
-----------------------

`[2405.15585] Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems <https://arxiv.org/abs/2405.15585>`__ 基于提示的端到端任务导向对话系统协同上下文学习

::

    replaced with revised version Wed, 3 Jul 2024 05:26:24 GMT
    Submission history From: Vishal Saley [view email]
    [v1] Fri, 24 May 2024 14:13:54 UTC (2,712 KB)
    [v2] Wed, 3 Jul 2024 05:26:24 UTC (8,821 KB)
    Vishal Vivek Saley, Rocktim Jyoti Das, Dinesh Raghu, Mausam

End-to-end Task-Oriented Dialog (TOD) systems typically require extensive training datasets to perform well. In contrast, large language model (LLM) based TOD systems can excel even with limited data due to their ability to learn tasks through in-context exemplars. However, these models lack alignment with the style of responses in training data and often generate comprehensive responses, making it difficult for users to grasp the information quickly. In response, we propose SyncTOD that synergizes LLMs with task-specific hints to improve alignment in low-data settings. SyncTOD employs small auxiliary models to provide hints and select exemplars for in-context prompts. With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings.

------------

-------------
Reasoning (8)
-------------

`[2407.02678] Reasoning in Large Language Models: A Geometric Perspective <https://arxiv.org/abs/2407.02678>`__ 大型语言模型中的推理:几何视角

::

    Tue, 2 Jul 2024 21:39:53 GMT
    Romain Cosentino, Sarath Shekkizhar

The advancement of large language models (LLMs) for real-world applications hinges critically on enhancing their reasoning capabilities. In this work, we explore the reasoning abilities of large language models (LLMs) through their geometrical understanding. We establish a connection between the expressive power of LLMs and the density of their self-attention graphs. Our analysis demonstrates that the density of these graphs defines the intrinsic dimension of the inputs to the MLP blocks. We demonstrate through theoretical analysis and toy examples that a higher intrinsic dimension implies a greater expressive capacity of the LLM. We further provide empirical evidence linking this geometric framework to recent advancements in methods aimed at enhancing the reasoning capabilities of LLMs.

------------

`[2407.02936] GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models <https://arxiv.org/abs/2407.02936>`__ GraCoRe:大型语言模型图理解和复杂推理基准测试

::

    Wed, 3 Jul 2024 09:12:38 GMT
    Zike Yuan, Ming Liu, Hui Wang, Bing Qin

Evaluating the graph comprehension and reasoning abilities of Large Language Models (LLMs) is challenging and often incomplete. Existing benchmarks focus primarily on pure graph understanding, lacking a comprehensive evaluation across all graph types and detailed capability definitions. This paper presents GraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and reasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and test models on pure graph and heterogeneous graphs, subdividing capabilities into 10 distinct areas tested through 19 tasks. Our benchmark includes 11 datasets with 5,140 graphs of varying complexity. We evaluated three closed-source and seven open-source LLMs, conducting thorough analyses from both ability and task perspectives. Key findings reveal that semantic enrichment enhances reasoning performance, node ordering impacts task success, and the ability to process longer texts does not necessarily improve graph comprehension or reasoning. GraCoRe is open-sourced at https://github.com/ZIKEYUAN/GraCoRe

------------

`[2407.03061] ALTER: Augmentation for Large-Table-Based Reasoning <https://arxiv.org/abs/2407.03061>`__ ALTER:基于大表的推理扩展

::

    Wed, 3 Jul 2024 12:34:45 GMT
    Han Zhang, Yuheng Ma, Hanfang Yang

While extensive research has explored the use of large language models (LLMs) for table-based reasoning, most approaches struggle with scalability when applied to large tables. To maintain the superior comprehension abilities of LLMs in these scenarios, we introduce ALTER(Augmentation for Large-Table-Based Reasoning)-a framework designed to harness the latent augmentation potential in both free-form natural language (NL) questions, via the query augmentor, and semi-structured tabular data, through the table augmentor. By utilizing only a small subset of relevant data from the table and supplementing it with pre-augmented schema, semantic, and literal information, ALTER achieves outstanding performance on table-based reasoning benchmarks. We also provide a detailed analysis of large-table scenarios, comparing different methods and various partitioning principles. In these scenarios, our method outperforms all other approaches and exhibits robustness and efficiency against perturbations.

------------

`[2407.03181] Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models <https://arxiv.org/abs/2407.03181>`__ 用发散思维链进行微调，通过语言模型的自我纠正来促进推理

::

    Wed, 3 Jul 2024 15:01:18 GMT
    Haritz Puerto, Tilek Chubakov, Xiaodan Zhu, Harish Tayyar Madabushi, Iryna Gurevych

Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model performance. In this work, we present a novel method of further improving performance by requiring models to compare multiple reasoning chains before generating a solution in a single inference step. We call this method Divergent CoT (DCoT). We find that instruction tuning on DCoT datasets boosts the performance of even smaller, and therefore more accessible, LLMs. Through a rigorous set of experiments spanning a wide range of tasks that require various reasoning types, we show that fine-tuning on DCoT consistently improves performance over the CoT baseline across model families and scales (1.3B to 70B). Through a combination of empirical and manual evaluation, we additionally show that these performance gains stem from models generating multiple divergent reasoning chains in a single inference step, indicative of the enabling of self-correction in language models. Our code and data are publicly available at https://github.com/UKPLab/arxiv2024-divergent-cot.

------------

`[2407.01046] FRoG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models <https://arxiv.org/abs/2407.01046>`__ FRoG:评价大型语言模型中广义量词的模糊推理

::

    replaced with revised version Wed, 3 Jul 2024 03:37:53 GMT
    Submission history From: Yiyuan Li [view email]
    [v1] Mon, 1 Jul 2024 07:56:14 UTC (9,122 KB)
    [v2] Wed, 3 Jul 2024 03:37:53 UTC (9,121 KB)
    Yiyuan Li, Shichao Sun, Pengfei Liu

Fuzzy reasoning is vital due to the frequent use of imprecise information in daily contexts. However, the ability of current large language models (LLMs) to handle such reasoning remains largely uncharted. In this paper, we introduce a new benchmark, FRoG, for fuzzy reasoning, featuring real-world mathematical word problems that incorporate generalized quantifiers. Our experimental findings reveal that fuzzy reasoning continues to pose significant challenges for LLMs. Moreover, we find that existing methods designed to enhance reasoning do not consistently improve performance in tasks involving fuzzy logic. Additionally, our results show an inverse scaling effect in the performance of LLMs on FRoG. Interestingly, we also demonstrate that strong mathematical reasoning skills are not necessarily indicative of success on our benchmark.

------------

`[2402.11924] Evaluating LLMs' Inherent Multi-hop Reasoning Ability <https://arxiv.org/abs/2402.11924>`__ 评估llm固有的多跳推理能力

::

    replaced with revised version Wed, 3 Jul 2024 15:50:48 GMT
    Submission history From: Jian Wu [view email]
    [v1] Mon, 19 Feb 2024 08:12:30 UTC (1,642 KB)
    [v2] Sun, 3 Mar 2024 02:23:19 UTC (1,642 KB)
    [v3] Wed, 3 Jul 2024 15:50:48 UTC (18,721 KB)
    [v4] Fri, 5 Jul 2024 13:43:43 UTC (18,721 KB)
    Jian Wu, Linyi Yang, Zhen Wang, Manabu Okumura, Yue Zhang

While Large Language Models (LLMs) excel in question-answering (QA) tasks, their multi-step reasoning abilities on multiple evidence integration on Multi-hop QA tasks remain underexplored. LLMs sometimes generate answers that rely on internal memory rather than reasoning given context, which brings concerns about the evaluation quality of real reasoning abilities. The counterfactual QA task can separate internal memory from reasoning abilities, but focusing solely on final-QA performance without evaluating the multi-step reasoning process is insufficient for reporting LLMs' real reasoning abilities. Current Multi-hop QA (MHQA) benchmarks are factual and annotated on open-source corpora such as Wikipedia, although useful for multi-step reasoning evaluation, showing limitations due to potential data contamination in LLMs pre-training stage. To address this issue, we introduce the Inherent Reasoning Evaluation (IRE) method, a novel evaluation way that jointly evaluates the LLMs' chain-of-reasoning performance based on the first knowledge-edited counterfactual multi-hop QA data which involves editing the original Wikipedia passages, reducing data contamination risks. The IRE comprehensively assesses reasoning chains through sub-QA and final-QA evaluations. Our comparisons reveal significant performance gaps for several LLMs between Wikipedia-based benchmarks and IRE, deeming data contamination issues in existing benchmarks. We believe that the IRE benchmark will enhance and facilitate trustworthy LLM evaluations.

------------

`[2407.01964] Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction <https://arxiv.org/abs/2407.01964>`__ 在llm中实现判别推理以进行法律判决预测

::

    replaced with revised version Wed, 3 Jul 2024 02:25:23 GMT
    Submission history From: Chenlong Deng [view email]
    [v1] Tue, 2 Jul 2024 05:43:15 UTC (722 KB)
    [v2] Wed, 3 Jul 2024 02:25:23 UTC (722 KB)
    [v3] Thu, 4 Jul 2024 01:28:46 UTC (722 KB)
    Chenlong Deng, Kelong Mao, Yuyao Zhang, Zhicheng Dou

Legal judgment prediction is essential for enhancing judicial efficiency. In this work, we identify that existing large language models (LLMs) underperform in this domain due to challenges in understanding case complexities and distinguishing between similar charges. To adapt LLMs for effective legal judgment prediction, we introduce the Ask-Discriminate-Predict (ADAPT) reasoning framework inspired by human judicial reasoning. ADAPT involves decomposing case facts, discriminating among potential charges, and predicting the final judgment. We further enhance LLMs through fine-tuning with multi-task synthetic trajectories to improve legal judgment prediction accuracy and efficiency under our ADAPT framework. Extensive experiments conducted on two widely-used datasets demonstrate the superior performance of our framework in legal judgment prediction, particularly when dealing with complex and confusing charges.

------------

`[2403.16437] Reasoning Runtime Behavior of a Program with LLM: How Far Are We? <https://arxiv.org/abs/2403.16437>`__ 用LLM推理程序的运行时行为:我们离现在有多远?

::

    replaced with revised version Wed, 3 Jul 2024 04:10:43 GMT
    Submission history From: Junkai Chen [view email]
    [v1] Mon, 25 Mar 2024 05:37:16 UTC (710 KB)
    [v2] Wed, 3 Jul 2024 04:10:43 UTC (711 KB)
    Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, Xin Xia

Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs, but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely REval, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs. Our code, data, and \newname leaderboard are available at this https URL.

------------

-----------
ToolUse (2)
-----------

`[2407.03007] What Affects the Stability of Tool Learning? An Empirical Study on the Robustness of Tool Learning Frameworks <https://arxiv.org/abs/2407.03007>`__ 什么影响工具学习的稳定性?关于工具学习框架鲁棒性的实证研究

::

    Wed, 3 Jul 2024 11:06:05 GMT
    Chengrui Huang, Zhengliang Shi, Yuntao Wen, Xiuying Chen, Peng Han, Shen Gao, Shuo Shang

Tool learning methods have enhanced the ability of large language models (LLMs) to interact with real-world applications. Many existing works fine-tune LLMs or design prompts to enable LLMs to select appropriate tools and correctly invoke them to meet user requirements. However, it is observed in previous works that the performance of tool learning varies from tasks, datasets, training settings, and algorithms. Without understanding the impact of these factors, it can lead to inconsistent results, inefficient model deployment, and suboptimal tool utilization, ultimately hindering the practical integration and scalability of LLMs in real-world scenarios. Therefore, in this paper, we explore the impact of both internal and external factors on the performance of tool learning frameworks. Through extensive experiments on two benchmark datasets, we find several insightful conclusions for future work, including the observation that LLMs can benefit significantly from increased trial and exploration. We believe our empirical study provides a new perspective for future tool learning research.

------------

`[2406.19657] LLMEasyQuant -- An Easy to Use Toolkit for LLM Quantization <https://arxiv.org/abs/2406.19657>`__ LLMEasyQuant——一个易于使用的LLM量化工具包

::

    replaced with revised version Tue, 2 Jul 2024 20:34:50 GMT
    Submission history From: Dong Liu [view email]
    [v1] Fri, 28 Jun 2024 04:56:53 UTC (446 KB)
    [v2] Tue, 2 Jul 2024 20:34:50 UTC (447 KB)
    Dong Liu, Meng Jiang, Kaiser Pister

Currently, there are many quantization methods appeared for LLM quantization, yet few are user-friendly and easy to be deployed locally. Packages like TensorRT and Quantohave many underlying structures and self-invoking internal functions, which are not conducive to developers' personalized development and learning for deployment. Therefore, we develop LLMEasyQuant, it is a package aiming to for easy quantization deployment which is user-friendly and suitable for beginners' learning.

------------

-----------------------
Retrieval-Augmented (7)
-----------------------

`[2407.02604] D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions <https://arxiv.org/abs/2407.02604>`__ D-Rax:基于多模态数据和专家模型预测的特定领域放射学辅助

::

    Tue, 2 Jul 2024 18:43:10 GMT
    Hareem Nisar, Syed Muhammad Anwar, Zhifan Jiang, Abhijeet Parida, Vishwesh Nath, Holger R. Roth, Marius George Linguraru

Large vision language models (VLMs) have progressed incredibly from research to applicability for general-purpose use cases. LLaVA-Med, a pioneering large language and vision assistant for biomedicine, can perform multi-modal biomedical image and data analysis to provide a natural language interface for radiologists. While it is highly generalizable and works with multi-modal data, it is currently limited by well-known challenges that exist in the large language model space. Hallucinations and imprecision in responses can lead to misdiagnosis which currently hinder the clinical adaptability of VLMs. To create precise, user-friendly models in healthcare, we propose D-Rax -- a domain-specific, conversational, radiologic assistance tool that can be used to gain insights about a particular radiologic image. In this study, we enhance the conversational analysis of chest X-ray (CXR) images to support radiological reporting, offering comprehensive insights from medical imaging and aiding in the formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the LLaVA-Med architecture on our curated enhanced instruction-following data, comprising of images, instructions, as well as disease diagnosis and demographic predictions derived from MIMIC-CXR imaging data, CXR-related visual question answer (VQA) pairs, and predictive outcomes from multiple expert AI models. We observe statistically significant improvement in responses when evaluated for both open and close-ended conversations. Leveraging the power of state-of-the-art diagnostic models combined with VLMs, D-Rax empowers clinicians to interact with medical images using natural language, which could potentially streamline their decision-making process, enhance diagnostic accuracy, and conserve their time.

------------

`[2407.03227] Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning <https://arxiv.org/abs/2407.03227>`__ 使用基于ast的排序和模式修剪改进检索增强的Text-to-SQL

::

    Wed, 3 Jul 2024 15:55:14 GMT
    Zhili Shen and Pavlos Vougiouklis and Chenxin Diao and Kaustubh Vyas and Yuanyi Ji and Jeff Z. Pan

We focus on Text-to-SQL semantic parsing from the perspective of Large Language Models. Motivated by challenges related to the size of commercial database schemata and the deployability of business intelligence solutions, we propose an approach that dynamically retrieves input database information and uses abstract syntax trees to select few-shot examples for in-context learning.
Furthermore, we investigate the extent to which an in-parallel semantic parser can be leveraged for generating $\textit{approximated}$ versions of the expected SQL queries, to support our retrieval. We take this approach to the extreme--we adapt a model consisting of less than $500$M parameters, to act as an extremely efficient approximator, enhancing it with the ability to process schemata in a parallelised manner. We apply our approach to monolingual and cross-lingual benchmarks for semantic parsing, showing improvements over state-of-the-art baselines. Comprehensive experiments highlight the contribution of modules involved in this retrieval-augmented generation setting, revealing interesting directions for future work.

------------

`[2407.02742] A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation <https://arxiv.org/abs/2407.02742>`__ DSL代码生成的比较研究:微调与优化检索增强

::

    Wed, 3 Jul 2024 01:28:51 GMT
    Nastaran Bassamzadeh and Chhaya Methani

Natural Language to Code Generation has made significant progress in recent years with the advent of Large Language Models(LLMs). While generation for general-purpose languages like C, C++, and Python has improved significantly, LLMs struggle with custom function names in Domain Specific Languages or DSLs.
This leads to higher hallucination rates and syntax errors, specially for DSLs having a high number of custom function names. Additionally, constant updates to function names add to the challenge as LLMs need to stay up-to-date. In this paper, we present optimizations for using Retrieval Augmented Generation (or RAG) with LLMs for DSL generation along with an ablation study comparing these strategies. We generated a train as well as test dataset with a DSL to represent automation tasks across roughly 700 APIs in public domain. We used the training dataset to fine-tune a Codex model for this DSL. Our results showed that the fine-tuned model scored the best on code similarity metric.
With our RAG optimizations, we achieved parity for similarity metric. The compilation rate, however, showed that both the models still got the syntax wrong many times, with RAG-based method being 2 pts better. Conversely, hallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for API parameter keys. We conclude that an optimized RAG model can match the quality of fine-tuned models and offer advantages for new, unseen APIs.

------------

`[2407.00668] HRDE: Retrieval-Augmented Large Language Models for Chinese Health Rumor Detection and Explainability <https://arxiv.org/abs/2407.00668>`__ HRDE:面向中文健康谣言检测和可解释性的检索增强大型语言模型

::

    replaced with revised version Wed, 3 Jul 2024 15:18:40 GMT
    Submission history From: Zhiyu Li [view email]
    [v1] Sun, 30 Jun 2024 11:27:50 UTC (1,765 KB)
    [v2] Wed, 3 Jul 2024 15:18:40 UTC (1,765 KB)
    Yanfang Chen and Ding Chen and Shichao Song and Simin Niu and Hanyu Wang and Zeyun Tang and Feiyu Xiong and Zhiyu Li

As people increasingly prioritize their health, the speed and breadth of health information dissemination on the internet have also grown. At the same time, the presence of false health information (health rumors) intermingled with genuine content poses a significant potential threat to public health. However, current research on Chinese health rumors still lacks a large-scale, public, and open-source dataset of health rumor information, as well as effective and reliable rumor detection methods. This paper addresses this gap by constructing a dataset containing 1.12 million health-related rumors (HealthRCN) through web scraping of common health-related questions and a series of data processing steps. HealthRCN is the largest known dataset of Chinese health information rumors to date. Based on this dataset, we propose retrieval-augmented large language models for Chinese health rumor detection and explainability (HRDE). This model leverages retrieved relevant information to accurately determine whether the input health information is a rumor and provides explanatory responses, effectively aiding users in verifying the authenticity of health information. In evaluation experiments, we compared multiple models and found that HRDE outperformed them all, including GPT-4-1106-Preview, in rumor detection accuracy and answer quality. HRDE achieved an average accuracy of 91.04% and an F1 score of 91.58%.

------------

`[2407.01080] Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese <https://arxiv.org/abs/2407.01080>`__ Face4RAG:面向中文检索增强生成的事实一致性评估

::

    replaced with revised version Wed, 3 Jul 2024 12:49:34 GMT
    Submission history From: Yunqi Xu [view email]
    [v1] Mon, 1 Jul 2024 08:35:04 UTC (1,305 KB)
    [v2] Wed, 3 Jul 2024 12:49:34 UTC (1,305 KB)
    Yunqi Xu, Tianchi Cai, Jiyan Jiang, Xierui Song

The prevailing issue of factual inconsistency errors in conventional Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency Evaluation (FCE). Despite the various FCE methods proposed earlier, these methods are evaluated on datasets generated by specific Large Language Models (LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE methods perform on other LLMs with different error distributions or even unseen error types, as these methods may fail to detect the error types generated by other LLMs. To fill this gap, in this paper, we propose the first comprehensive FCE benchmark \emph{Face4RAG} for RAG independent of the underlying LLM. Our benchmark consists of a synthetic dataset built upon a carefully designed typology for factuality inconsistency error and a real-world dataset constructed from six commonly used LLMs, enabling evaluation of FCE methods on specific error types or real-world error distributions. On the proposed benchmark, we discover the failure of existing FCE methods to detect the logical fallacy, which refers to a mismatch of logic structures between the answer and the retrieved reference. To fix this issue, we further propose a new method called \emph{L-Face4RAG} with two novel designs of logic-preserving answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG substantially outperforms previous methods for factual inconsistency detection on a wide range of tasks, notably beyond the RAG task from which it is originally motivated. Both the benchmark and our proposed method are publicly available.\footnote{\url{this https URL}\label{link_face4rag}}

------------

`[2310.04673] LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT <https://arxiv.org/abs/2310.04673>`__ 听，参加，理解，并用GPT重新生成音频

::

    replaced with revised version Wed, 3 Jul 2024 02:38:03 GMT
    Submission history From: Zhihao Du [view email]
    [v1] Sat, 7 Oct 2023 03:17:59 UTC (382 KB)
    [v2] Tue, 10 Oct 2023 06:26:54 UTC (155 KB)
    [v3] Wed, 11 Oct 2023 02:55:54 UTC (155 KB)
    [v4] Wed, 3 Jul 2024 02:38:03 UTC (254 KB)
    Zhihao Du, Jiaming Wang, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, Wen Wang, Siqi Zheng, Chang Zhou, Zhijie Yan, Shiliang Zhang

Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.

------------

`[2407.00072] Pistis-RAG: A Scalable Cascading Framework Towards Content-Centric Retrieval-Augmented Generation <https://arxiv.org/abs/2407.00072>`__ Pistis-RAG:面向以内容为中心的检索增强生成的可扩展级联框架

::

    replaced with revised version Wed, 3 Jul 2024 06:54:16 GMT
    Submission history From: Yu Bai [view email]
    [v1] Fri, 21 Jun 2024 08:52:11 UTC (1,761 KB)
    [v2] Wed, 3 Jul 2024 06:54:16 UTC (1,003 KB)
    [v3] Thu, 11 Jul 2024 09:28:34 UTC (1,000 KB)
    Yu Bai, Yukai Miao, Li Chen, Dan Li, Yanyu Ren, Hongtao Xie, Ce Yang, Xuhui Cai

In Greek mythology, Pistis symbolized good faith, trust, and reliability. Drawing inspiration from these principles, Pistis-RAG is a scalable multi-stage framework designed to address the challenges of large-scale retrieval-augmented generation (RAG) systems. This framework consists of distinct stages: matching, pre-ranking, ranking, reasoning, and aggregating. Each stage contributes to narrowing the search space, prioritizing semantically relevant documents, aligning with the large language model's (LLM) preferences, supporting complex chain-of-thought (CoT) methods, and combining information from multiple sources.
Our ranking stage introduces a significant innovation by recognizing that semantic relevance alone may not lead to improved generation quality, due to the sensitivity of the few-shot prompt order, as noted in previous research. This critical aspect is often overlooked in current RAG frameworks.
We argue that the alignment issue between LLMs and external knowledge ranking methods is tied to the model-centric paradigm dominant in RAG systems. We propose a content-centric approach, emphasizing seamless integration between LLMs and external information sources to optimize content transformation for specific tasks.
Our novel ranking stage is designed specifically for RAG systems, incorporating principles of information retrieval while considering the unique business scenarios reflected in LLM preferences and user feedback. We simulated feedback signals on the MMLU benchmark, resulting in a 9.3% performance improvement. Our model and code will be open-sourced on GitHub. Additionally, experiments on real-world, large-scale data validate the scalability of our framework.

------------

---------
Agent (1)
---------

`[2407.02736] MentalAgora: A Gateway to Advanced Personalized Care in Mental Health through Multi-Agent Debating and Attribute Control <https://arxiv.org/abs/2407.02736>`__ MentalAgora:通过多智能体辩论和属性控制实现心理健康高级个性化护理的门户

::

    Wed, 3 Jul 2024 01:19:38 GMT
    Yeonji Lee, Sangjun Park, Kyunghyun Cho, JinYeong Bak

As mental health issues globally escalate, there is a tremendous need for advanced digital support systems. We introduce MentalAgora, a novel framework employing large language models enhanced by interaction between multiple agents for tailored mental health support. This framework operates through three stages: strategic debating, tailored counselor creation, and response generation, enabling the dynamic customization of responses based on individual user preferences and therapeutic needs. We conduct experiments utilizing a high-quality evaluation dataset TherapyTalk crafted with mental health professionals, shwoing that MentalAgora generates expert-aligned and user preference-enhanced responses. Our evaluations, including experiments and user studies, demonstrate that MentalAgora aligns with professional standards and effectively meets user preferences, setting a new benchmark for digital mental health interventions.

------------

----------
Other (82)
----------

`[2407.02552] RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs <https://arxiv.org/abs/2407.02552>`__ RLHF可以使用多种语言:为llm解锁多语言偏好优化

::

    Tue, 2 Jul 2024 17:42:30 GMT
    John Dang, Arash Ahmadian, Kelly Marchisio, Julia Kreutzer, Ahmet \"Ust\"un, Sara Hooker

Preference optimization techniques have become a standard final stage for training state-of-art large language models (LLMs). However, despite widespread adoption, the vast majority of work to-date has focused on first-class citizen languages like English and Chinese. This captures a small fraction of the languages in the world, but also makes it unclear which aspects of current state-of-the-art research transfer to a multilingual setting. In this work, we perform an exhaustive study to achieve a new state-of-the-art in aligning multilingual LLMs. We introduce a novel, scalable method for generating high-quality multilingual feedback data to balance data coverage. We establish the benefits of cross-lingual transfer and increased dataset size in preference training. Our preference-trained model achieves a 54.4% win-rate against Aya 23 8B, the current state-of-the-art multilingual LLM in its parameter class, and a 69.5% win-rate or higher against widely used models like Gemma-1.1-7B-it, Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.3. As a result of our study, we expand the frontier of alignment techniques to 23 languages covering half of the world's population.

------------

`[2407.02637] Change My Frame: Reframing in the Wild in r/ChangeMyView <https://arxiv.org/abs/2407.02637>`__ 改变我的框架:在r/ChangeMyView中进行野外重建

::

    Tue, 2 Jul 2024 20:09:11 GMT
    Arturo Mart\'inez Peguero and Taro Watanabe

Recent work in reframing, within the scope of text style transfer, has so far made use of out-of-context, task-prompted utterances in order to produce neutralizing or optimistic reframes. Our work aims to generalize reframing based on the subreddit r/ChangeMyView (CMV). We build a dataset that leverages CMV's community's interactions and conventions to identify high-value, community-recognized utterances that produce changes of perspective. With this data, we widen the scope of the direction of reframing since the changes in perspective do not only occur in neutral or positive directions. We fine tune transformer-based models, make use of a modern LLM to refine our dataset, and explore challenges in the dataset creation and evaluation around this type of reframing.

------------

`[2407.02659] Ensuring Responsible Sourcing of Large Language Model Training Data Through Knowledge Graph Comparison <https://arxiv.org/abs/2407.02659>`__ 

::

    Tue, 2 Jul 2024 20:49:21 GMT
    Devam Mondal, Carlo Lipizzi

In light of recent plagiarism allegations Brough by publishers, newspapers, and other creators of copyrighted corpora against large language model (LLM) developers, we propose a novel system, a variant of a plagiarism detection system, that assesses whether a knowledge source has been used in the training or fine-tuning of a large language model. Unlike current methods, we utilize an approach that uses Resource Description Framework (RDF) triples to create knowledge graphs from both a source document and a LLM continuation of that document. These graphs are then analyzed with respect to content using cosine similarity and with respect to structure using a normalized version of graph edit distance that shows the degree of isomorphism. Unlike traditional systems that focus on content matching and keyword identification between a source and target corpus, our approach enables a broader evaluation of similarity and thus a more accurate comparison of the similarity between a source document and LLM continuation by focusing on relationships between ideas and their organization with regards to others. Additionally, our approach does not require access to LLM metrics like perplexity that may be unavailable in closed large language modeling "black-box" systems, as well as the training corpus. A prototype of our system will be found on a hyperlinked GitHub repository.

------------

`[2407.02750] Learning to Reduce: Towards Improving Performance of Large Language Models on Structured Data <https://arxiv.org/abs/2407.02750>`__ 学习归约:提高大型语言模型在结构化数据上的性能

::

    Wed, 3 Jul 2024 01:51:50 GMT
    Younghun Lee, Sungchul Kim, Ryan A. Rossi, Tong Yu, Xiang Chen

Large Language Models (LLMs) have been achieving competent performance on a wide range of downstream tasks, yet existing work shows that inference on structured data is challenging for LLMs. This is because LLMs need to either understand long structured data or select the most relevant evidence before inference, and both approaches are not trivial. This paper proposes a framework, Learning to Reduce, that fine-tunes a language model with On-Policy Learning to generate a reduced version of an input structured data. When compared to state-of-the-art LLMs like GPT-4, Learning to Reduce not only achieves outstanding performance in reducing the input, but shows generalizability on different datasets. We further show that the model fine-tuned with our framework helps LLMs better perform on table QA tasks especially when the context is longer.

------------

`[2407.02783] 52B to 1T: Lessons Learned via Tele-FLM Series <https://arxiv.org/abs/2407.02783>`__ 52B到1T:通过Tele-FLM系列学到的经验

::

    Wed, 3 Jul 2024 03:21:02 GMT
    Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Chao Wang, Xinzhang Liu, Zihan Wang, Yu Zhao, Xin Wang, Yuyao Huang, Shuangyong Song, Yongxiang Li, Zheng Zhang, Bo Zhao, Aixin Sun, Yequan Wang, Zhongjiang He, Zhongyuan Wang, Xuelong Li, Tiejun Huang

Large Language Models (LLMs) represent a significant stride toward Artificial General Intelligence. As scaling laws underscore the potential of increasing model sizes, the academic community has intensified its investigations into LLMs with capacities exceeding 50 billion parameters. This technical report builds on our prior work with Tele-FLM (also known as FLM-2), a publicly available 52-billion-parameter model. We delve into two primary areas: we first discuss our observation of Supervised Fine-tuning (SFT) on Tele-FLM-52B, which supports the "less is more" approach for SFT data construction; second, we demonstrate our experiments and analyses on the best practices for progressively growing a model from 52 billion to 102 billion, and subsequently to 1 trillion parameters. We will open-source a 1T model checkpoint, namely Tele-FLM-1T, to advance further training and research.

------------

`[2407.02964] FSM: A Finite State Machine Based Zero-Shot Prompting Paradigm for Multi-Hop Question Answering <https://arxiv.org/abs/2407.02964>`__ FSM:一种基于有限状态机的零样本多跳问答模型

::

    Wed, 3 Jul 2024 10:01:01 GMT
    Xiaochen Wang, Junqing He, Zhe yang, Yiru Wang, Xiangdi Meng, Kunhao Pan, Zhifang Sui

Large Language Models (LLMs) with chain-of-thought (COT) prompting have demonstrated impressive abilities on simple nature language inference tasks.
However, they tend to perform poorly on Multi-hop Question Answering (MHQA) tasks due to several challenges, including hallucination, error propagation and limited context length. We propose a prompting method, Finite State Machine (FSM) to enhance the reasoning capabilities of LLM for complex tasks in addition to improved effectiveness and trustworthiness. Different from COT methods, FSM addresses MHQA by iteratively decomposing a question into multi-turn sub-questions, and self-correcting in time, improving the accuracy of answers in each step. Specifically, FSM addresses one sub-question at a time and decides on the next step based on its current result and state, in an automaton-like format. Experiments on benchmarks show the effectiveness of our method. Although our method performs on par with the baseline on relatively simpler datasets, it excels on challenging datasets like Musique. Moreover, this approach mitigates the hallucination phenomenon, wherein the correct final answer can be recovered despite errors in intermediate reasoning. Furthermore, our method improves LLMs' ability to follow specified output format requirements, significantly reducing the difficulty of answer interpretation and the need for reformatting.

------------

`[2407.02977] Large Language Models as Evaluators for Scientific Synthesis <https://arxiv.org/abs/2407.02977>`__ 作为科学综合评估器的大型语言模型

::

    Wed, 3 Jul 2024 10:21:27 GMT
    Julia Evans, Jennifer D'Souza and S\"oren Auer

Our study explores how well the state-of-the-art Large Language Models (LLMs), like GPT-4 and Mistral, can assess the quality of scientific summaries or, more fittingly, scientific syntheses, comparing their evaluations to those of human annotators. We used a dataset of 100 research questions and their syntheses made by GPT-4 from abstracts of five related papers, checked against human quality ratings. The study evaluates both the closed-source GPT-4 and the open-source Mistral model's ability to rate these summaries and provide reasons for their judgments. Preliminary results show that LLMs can offer logical explanations that somewhat match the quality ratings, yet a deeper statistical analysis shows a weak correlation between LLM and human ratings, suggesting the potential and current limitations of LLMs in scientific synthesis evaluation.

------------

`[2407.02978] Mast Kalandar at SemEval-2024 Task 8: On the Trail of Textual Origins: RoBERTa-BiLSTM Approach to Detect AI-Generated Text <https://arxiv.org/abs/2407.02978>`__ 

::

    Wed, 3 Jul 2024 10:22:23 GMT
    Jainit Sushil Bafna, Hardik Mittal, Suyash Sethia, Manish Shrivastava, Radhika Mamidi

Large Language Models (LLMs) have showcased impressive abilities in generating fluent responses to diverse user queries. However, concerns regarding the potential misuse of such texts in journalism, educational, and academic contexts have surfaced. SemEval 2024 introduces the task of Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection, aiming to develop automated systems for identifying machine-generated text and detecting potential misuse. In this paper, we i) propose a RoBERTa-BiLSTM based classifier designed to classify text into two categories: AI-generated or human ii) conduct a comparative study of our model with baseline approaches to evaluate its effectiveness. This paper contributes to the advancement of automatic text detection systems in addressing the challenges posed by machine-generated text misuse. Our architecture ranked 46th on the official leaderboard with an accuracy of 80.83 among 125.

------------

`[2407.02996] Are Large Language Models Consistent over Value-laden Questions? <https://arxiv.org/abs/2407.02996>`__ 大型语言模型对承载价值的问题是否一致?

::

    Wed, 3 Jul 2024 10:53:54 GMT
    Jared Moore, Tanvi Deshpande, Diyi Yang

Large language models (LLMs) appear to bias their survey answers toward certain values. Nonetheless, some argue that LLMs are too inconsistent to simulate particular values. Are they? To answer, we first define value consistency as the similarity of answers across (1) paraphrases of one question, (2) related questions under one topic, (3) multiple-choice and open-ended use-cases of one question, and (4) multilingual translations of a question to English, Chinese, German, and Japanese. We apply these measures to a few large ($>=34b$), open LLMs including llama-3, as well as gpt-4o, using eight thousand questions spanning more than 300 topics. Unlike prior work, we find that models are relatively consistent across paraphrases, use-cases, translations, and within a topic. Still, some inconsistencies remain. Models are more consistent on uncontroversial topics (e.g., in the U.S., "Thanksgiving") than on controversial ones ("euthanasia"). Base models are both more consistent compared to fine-tuned models and are uniform in their consistency across topics, while fine-tuned models are more inconsistent about some topics ("euthanasia") than others ("women's rights") like our human subjects (n=165).

------------

`[2407.03004] SemioLLM: Assessing Large Language Models for Semiological Analysis in Epilepsy Research <https://arxiv.org/abs/2407.03004>`__ SemioLLM:用于癫痫研究符号学分析的大型语言模型评估

::

    Wed, 3 Jul 2024 11:02:12 GMT
    Meghal Dani, Muthu Jeyanthi Prakash, Zeynep Akata and Stefanie Liebe

Large Language Models have shown promising results in their ability to encode general medical knowledge in standard medical question-answering datasets.
However, their potential application in clinical practice requires evaluation in domain-specific tasks, where benchmarks are largely missing. In this study semioLLM, we test the ability of state-of-the-art LLMs (GPT-3.5, GPT-4, Mixtral 8x7B, and Qwen-72chat) to leverage their internal knowledge and reasoning for epilepsy diagnosis. Specifically, we obtain likelihood estimates linking unstructured text descriptions of seizures to seizure-generating brain regions, using an annotated clinical database containing 1269 entries. We evaluate the LLM's performance, confidence, reasoning, and citation abilities in comparison to clinical evaluation. Models achieve above-chance classification performance with prompt engineering significantly improving their outcome, with some models achieving close-to-clinical performance and reasoning. However, our analyses also reveal significant pitfalls with several models being overly confident while showing poor performance, as well as exhibiting citation errors and hallucinations. In summary, our work provides the first extensive benchmark comparing current SOTA LLMs in the medical domain of epilepsy and highlights their ability to leverage unstructured texts from patients' medical history to aid diagnostic processes in health care.

------------

`[2407.03038] On the Client Preference of LLM Fine-tuning in Federated Learning <https://arxiv.org/abs/2407.03038>`__ 联邦学习中LLM微调的客户端偏好研究

::

    Wed, 3 Jul 2024 12:02:24 GMT
    Feijie Wu, Xiaoze Liu, Haoyu Wang, Xingchen Wang, Jing Gao

Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained large language model (LLM) using preference datasets, enabling the LLM to generate outputs that align with human preferences. Given the sensitive nature of these preference datasets held by various clients, there is a need to implement RLHF within a federated learning (FL) framework, where clients are reluctant to share their data due to privacy concerns. To address this, we introduce a feasible framework in which clients collaboratively train a binary selector with their preference datasets using our proposed FedBis. With a well-trained selector, we can further enhance the LLM that generates human-preferred completions. Meanwhile, we propose a novel algorithm, FedBiscuit, that trains multiple selectors by organizing clients into balanced and disjoint clusters based on their preferences. Compared to the FedBis, FedBiscuit demonstrates superior performance in simulating human preferences for pairwise completions. Our extensive experiments on federated human preference datasets -- marking the first benchmark to address heterogeneous data partitioning among clients -- demonstrate that FedBiscuit outperforms FedBis and even surpasses traditional centralized training.

------------

`[2407.03040] Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model <https://arxiv.org/abs/2407.03040>`__ 原始文本就是你所需要的:大型语言模型的知识密集型多轮指令调优

::

    Wed, 3 Jul 2024 12:04:10 GMT
    Xia Hou, Qifeng Li, Jian Yang, Tongliang Li, Linzheng Chai, Xianjie Wu, Hangyuan Ji, Zhoujun Li, Jixuan Nie, Jingbo Dun, Wenfeng Song

Instruction tuning as an effective technique aligns the outputs of large language models (LLMs) with human preference. But how to generate the seasonal multi-turn dialogues from raw documents for instruction tuning still requires further exploration. In this paper, we present a novel framework named R2S that leverages the CoD-Chain of Dialogue logic to guide large language models (LLMs) in generating knowledge-intensive multi-turn dialogues for instruction tuning.
By integrating raw documents from both open-source datasets and domain-specific web-crawled documents into a benchmark K-BENCH, we cover diverse areas such as Wikipedia (English), Science (Chinese), and Artifacts (Chinese). Our approach first decides the logic flow of the current dialogue and then prompts LLMs to produce key phrases for sourcing relevant response content. This methodology enables the creation of the G I NSTRUCT instruction dataset, retaining raw document knowledge within dialoguestyle interactions. Utilizing this dataset, we fine-tune GLLM, a model designed to transform raw documents into structured multi-turn dialogues, thereby injecting comprehensive domain knowledge into the SFT model for enhanced instruction tuning. This work signifies a stride towards refining the adaptability and effectiveness of LLMs in processing and generating more accurate, contextually nuanced responses across various fields.

------------

`[2407.03051] Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment <https://arxiv.org/abs/2407.03051>`__ 通过直接偏好对齐提高量化大型语言模型的对话能力

::

    Wed, 3 Jul 2024 12:19:06 GMT
    Janghwan Lee, Seongmin Park, Sukjin Hong, Minsoo Kim, Du-Seong Chang, Jungwook Choi

The rapid advancement of large language models (LLMs) has facilitated their transformation into conversational chatbots that can grasp contextual nuances and generate pertinent sentences, closely mirroring human values through advanced techniques such as instruction tuning and reinforcement learning from human feedback (RLHF). However, the computational efficiency required for LLMs, achieved through techniques like post-training quantization (PTQ), presents challenges such as token-flipping that can impair chatbot performance. In response, we propose a novel preference alignment approach, quantization-aware direct preference optimization (QDPO), that aligns quantized LLMs with their full-precision counterparts, improving conversational abilities. Evaluated on two instruction-tuned LLMs in various languages, QDPO demonstrated superior performance in improving conversational abilities compared to established PTQ and knowledge-distillation fine-tuning techniques, marking a significant step forward in the development of efficient and effective conversational LLMs.

------------

`[2407.03103] Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory <https://arxiv.org/abs/2407.03103>`__ Cactus:基于认知行为理论的心理咨询对话

::

    Wed, 3 Jul 2024 13:41:31 GMT
    Suyeon Lee, Sunghwan Kim, Minju Kim, Dongjin Kang, Dongil Yang, Harim Kim, Minseok Kang, Dayi Jung, Min Hee Kim, Seungbeen Lee, Kyoung-Mee Chung, Youngjae Yu, Dongha Lee, Jinyoung Yeo

Recently, the demand for psychological counseling has significantly increased as more individuals express concerns about their mental health. This surge has accelerated efforts to improve the accessibility of counseling by using large language models (LLMs) as counselors. To ensure client privacy, training open-source LLMs faces a key challenge: the absence of realistic counseling datasets. To address this, we introduce Cactus, a multi-turn dialogue dataset that emulates real-life interactions using the goal-oriented and structured approach of Cognitive Behavioral Therapy (CBT). We create a diverse and realistic dataset by designing clients with varied, specific personas, and having counselors systematically apply CBT techniques in their interactions. To assess the quality of our data, we benchmark against established psychological criteria used to evaluate real counseling sessions, ensuring alignment with expert evaluations. Experimental results demonstrate that Camel, a model trained with Cactus, outperforms other models in counseling skills, highlighting its effectiveness and potential as a counseling agent. We make our data, model, and code publicly available.

------------

`[2407.03129] Social Bias Evaluation for Large Language Models Requires Prompt Variations <https://arxiv.org/abs/2407.03129>`__ 大型语言模型的社会偏见评估需要及时变化

::

    Wed, 3 Jul 2024 14:12:04 GMT
    Rem Hida, Masahiro Kaneko, Naoaki Okazaki

Warning: This paper contains examples of stereotypes and biases. Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately. Previous studies use downstream tasks as prompts to examine the degree of social biases for evaluation and mitigation. While LLMs' output highly depends on prompts, previous studies evaluating and mitigating bias have often relied on a limited variety of prompts. In this paper, we investigate the sensitivity of LLMs when changing prompt variations (task instruction and prompt, few-shot examples, debias-prompt) by analyzing task performance and social bias of LLMs. Our experimental results reveal that LLMs are highly sensitive to prompts to the extent that the ranking of LLMs fluctuates when comparing models for task performance and social bias. Additionally, we show that LLMs have tradeoffs between performance and social bias caused by the prompts. Less bias from prompt setting may result in reduced performance. Moreover, the ambiguity of instances is one of the reasons for this sensitivity to prompts in advanced LLMs, leading to various outputs. We recommend using diverse prompts, as in this study, to compare the effects of prompts on social bias in LLMs.

------------

`[2407.03145] Enhancing Translation Accuracy of Large Language Models through Continual Pre-Training on Parallel Data <https://arxiv.org/abs/2407.03145>`__ 

::

    Wed, 3 Jul 2024 14:23:36 GMT
    Minato Kondo, Takehito Utsuro, Masaaki Nagata

In this paper, we propose a two-phase training approach where pre-trained large language models are continually pre-trained on parallel data and then supervised fine-tuned with a small amount of high-quality parallel data. To investigate the effectiveness of our proposed approach, we conducted continual pre-training with a 3.8B-parameter model and parallel data across eight different formats. We evaluate these methods on thirteen test sets for Japanese-to-English and English-to-Japanese translation. The results demonstrate that when utilizing parallel data in continual pre-training, it is essential to alternate between source and target sentences. Additionally, we demonstrated that the translation accuracy improves only for translation directions where the order of source and target sentences aligns between continual pre-training data and inference. In addition, we demonstrate that the LLM-based translation model is more robust in translating spoken language and achieves higher accuracy with less training data compared to supervised encoder-decoder models. We also show that the highest accuracy is achieved when the data for continual pre-training consists of interleaved source and target sentences and when tags are added to the source sentences.

------------

`[2407.03157] Let the Code LLM Edit Itself When You Edit the Code <https://arxiv.org/abs/2407.03157>`__ 当你编辑代码时，让代码LLM编辑自己

::

    Wed, 3 Jul 2024 14:34:03 GMT
    Zhenyu He, Jun Zhang, Shengjie Luo, Jingjing Xu, Zhi Zhang, Di He

In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly.
Naively, the LLM needs to re-encode the entire KV cache to provide an accurate prediction. However, this process is computationally expensive, especially when the sequence length is long. Simply encoding the edited subsequence and integrating it to the original KV cache meets the temporal confusion problem, leading to significantly worse performance. We address this efficiency and accuracy trade-off by introducing \underline{\textbf{Positional \textbf{I}ntegrity \textbf{E}ncoding} (PIE). Building upon the rotary positional encoding, PIE first removes the rotary matrices in the Key cache that introduce temporal confusion and then reapplies the correct rotary matrices. This process ensures that positional relationships between tokens are correct and requires only a single round of matrix multiplication. We validate the effectiveness of PIE through extensive experiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.
Our evaluation includes three real-world coding tasks: code insertion, code deletion, and multi-place code editing. Results demonstrate that PIE reduces computational overhead by over 85% compared to the standard full recomputation approach across all model sizes and tasks while well approximating the model performance.

------------

`[2407.03169] Investigating Decoder-only Large Language Models for Speech-to-text Translation <https://arxiv.org/abs/2407.03169>`__ 面向语音到文本翻译的纯解码器大型语言模型研究

::

    Wed, 3 Jul 2024 14:42:49 GMT
    Chao-Wei Huang, Hui Lu, Hongyu Gong, Hirofumi Inaguma, Ilia Kulikov, Ruslan Mavlyutov, Sravya Popuri

Large language models (LLMs), known for their exceptional reasoning capabilities, generalizability, and fluency across diverse domains, present a promising avenue for enhancing speech-related tasks. In this paper, we focus on integrating decoder-only LLMs to the task of speech-to-text translation (S2TT).
We propose a decoder-only architecture that enables the LLM to directly consume the encoded speech representation and generate the text translation.
Additionally, we investigate the effects of different parameter-efficient fine-tuning techniques and task formulation. Our model achieves state-of-the-art performance on CoVoST 2 and FLEURS among models trained without proprietary data. We also conduct analyses to validate the design choices of our proposed model and bring insights to the integration of LLMs to S2TT.

------------

`[2407.03211] How Does Quantization Affect Multilingual LLMs? <https://arxiv.org/abs/2407.03211>`__ 量化如何影响多语言llm ?

::

    Wed, 3 Jul 2024 15:39:40 GMT
    Kelly Marchisio, Saurabh Dash, Hongyu Chen, Dennis Aumiller, Ahmet \"Ust\"un, Sara Hooker, Sebastian Ruder

Quantization techniques are widely used to improve inference speed and deployment of large language models. While a wide body of work examines the impact of quantized LLMs on English tasks, none have examined the effect of quantization across languages. We conduct a thorough analysis of quantized multilingual LLMs, focusing on their performance across languages and at varying scales. We use automatic benchmarks, LLM-as-a-Judge methods, and human evaluation, finding that (1) harmful effects of quantization are apparent in human evaluation, and automatic metrics severely underestimate the detriment: a 1.7% average drop in Japanese across automatic tasks corresponds to a 16.0% drop reported by human evaluators on realistic prompts; (2) languages are disparately affected by quantization, with non-Latin script languages impacted worst; and (3) challenging tasks such as mathematical reasoning degrade fastest. As the ability to serve low-compute models is critical for wide global adoption of NLP technologies, our results urge consideration of multilingual performance as a key evaluation criterion for efficient models.

------------

`[2407.03282] LLM Internal States Reveal Hallucination Risk Faced With a Query <https://arxiv.org/abs/2407.03282>`__ LLM内部状态揭示了面临查询时的幻觉风险

::

    Wed, 3 Jul 2024 17:08:52 GMT
    Ziwei Ji, Delong Chen, Etsuko Ishii, Samuel Cahyawijaya, Yejin Bang, Bryan Wilie, Pascale Fung

The hallucination problem of Large Language Models (LLMs) significantly limits their reliability and trustworthiness. Humans have a self-awareness process that allows us to recognize what we don't know when faced with queries.
Inspired by this, our paper investigates whether LLMs can estimate their own hallucination risk before response generation. We analyze the internal mechanisms of LLMs broadly both in terms of training data sources and across 15 diverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.
Our empirical analysis reveals two key insights: (1) LLM internal states indicate whether they have seen the query in training data or not; and (2) LLM internal states show they are likely to hallucinate or not regarding the query.
Our study explores particular neurons, activation layers, and tokens that play a crucial role in the LLM perception of uncertainty and hallucination risk. By a probing estimator, we leverage LLM self-assessment, achieving an average hallucination estimation accuracy of 84.32\% at run time.

------------

`[2407.02694] LLM-Select: Feature Selection with Large Language Models <https://arxiv.org/abs/2407.02694>`__ LLM-Select:大型语言模型的特征选择

::

    Tue, 2 Jul 2024 22:23:40 GMT
    Daniel P. Jeong, Zachary C. Lipton, Pradeep Ravikumar

In this paper, we demonstrate a surprising capability of large language models (LLMs): given only input feature names and a description of a prediction task, they are capable of selecting the most predictive features, with performance rivaling the standard tools of data science. Remarkably, these models exhibit this capacity across various query mechanisms. For example, we zero-shot prompt an LLM to output a numerical importance score for a feature (e.g., "blood pressure") in predicting an outcome of interest (e.g., "heart failure"), with no additional context. In particular, we find that the latest models, such as GPT-4, can consistently identify the most predictive features regardless of the query mechanism and across various prompting strategies. We illustrate these findings through extensive experiments on real-world data, where we show that LLM-based feature selection consistently achieves strong performance competitive with data-driven methods such as the LASSO, despite never having looked at the downstream training data. Our findings suggest that LLMs may be useful not only for selecting the best features for training but also for deciding which features to collect in the first place. This could potentially benefit practitioners in domains like healthcare, where collecting high-quality data comes at a high cost.

------------

`[2407.02770] Large language models, physics-based modeling, experimental measurements: the trinity of data-scarce learning of polymer properties <https://arxiv.org/abs/2407.02770>`__ 大型语言模型、基于物理的建模、实验测量:数据稀缺的聚合物特性学习的三位一体

::

    Wed, 3 Jul 2024 02:57:40 GMT
    Ning Liu, Siavash Jafarzadeh, Brian Y. Lattimer, Shuna Ni, Jim Lua, Yue Yu

Large language models (LLMs) bear promise as a fast and accurate material modeling paradigm for evaluation, analysis, and design. Their vast number of trainable parameters necessitates a wealth of data to achieve accuracy and mitigate overfitting. However, experimental measurements are often limited and costly to obtain in sufficient quantities for finetuning. To this end, we present a physics-based training pipeline that tackles the pathology of data scarcity. The core enabler is a physics-based modeling framework that generates a multitude of synthetic data to align the LLM to a physically consistent initial state before finetuning. Our framework features a two-phase training strategy: (1) utilizing the large-in-amount while less accurate synthetic data for supervised pretraining, and (2) finetuning the phase-1 model with limited experimental data. We empirically demonstrate that supervised pretraining is vital to obtaining accurate finetuned LLMs, via the lens of learning polymer flammability metrics where cone calorimeter data is sparse.

------------

`[2407.02891] GPTQT: Quantize Large Language Models Twice to Push the Efficiency <https://arxiv.org/abs/2407.02891>`__ GPTQT:两次量化大型语言模型以提升效率

::

    Wed, 3 Jul 2024 08:08:01 GMT
    Yipin Guo, Yilin Lang, Qinyuan Ren

Due to their large size, generative Large Language Models (LLMs) require significant computing and storage resources. This paper introduces a new post-training quantization method, GPTQT, to reduce memory usage and enhance processing speed by expressing the weight of LLM in 3bit/2bit. Practice has shown that minimizing the quantization error of weights is ineffective, leading to overfitting. Therefore, GPTQT employs a progressive two-step approach: initially quantizing weights using Linear quantization to a relatively high bit, followed by converting obtained int weight to lower bit binary coding. A re-explore strategy is proposed to optimize initial scaling factor. During inference, these steps are merged into pure binary coding, enabling efficient computation. Testing across various models and datasets confirms GPTQT's effectiveness. Compared to the strong 3-bit quantization baseline, GPTQT further reduces perplexity by 4.01 on opt-66B and increases speed by 1.24 times on opt-30b. The results on Llama2 show that GPTQT is currently the best binary coding quantization method for such kind of LLMs.

------------

`[2407.03232] Single Character Perturbations Break LLM Alignment <https://arxiv.org/abs/2407.03232>`__ 单个字符的扰动破坏了LLM对齐

::

    Wed, 3 Jul 2024 16:03:10 GMT
    Leon Lin, Hannah Brown, Kenji Kawaguchi, Michael Shieh

When LLMs are deployed in sensitive, human-facing settings, it is crucial that they do not output unsafe, biased, or privacy-violating outputs. For this reason, models are both trained and instructed to refuse to answer unsafe prompts such as "Tell me how to build a bomb." We find that, despite these safeguards, it is possible to break model defenses simply by appending a space to the end of a model's input. In a study of eight open-source models, we demonstrate that this acts as a strong enough attack to cause the majority of models to generate harmful outputs with very high success rates. We examine the causes of this behavior, finding that the contexts in which single spaces occur in tokenized training data encourage models to generate lists when prompted, overriding training signals to refuse to answer unsafe requests. Our findings underscore the fragile state of current model alignment and promote the importance of developing more robust alignment methods. Code and data will be available at https://github.com/hannah-aught/space_attack.

------------

`[2407.03234] Self-Evaluation as a Defense Against Adversarial Attacks on LLMs <https://arxiv.org/abs/2407.03234>`__ 作为对llm对抗攻击防御的自评估

::

    Wed, 3 Jul 2024 16:03:42 GMT
    Hannah Brown, Leon Lin, Kenji Kawaguchi, Michael Shieh

When LLMs are deployed in sensitive, human-facing settings, it is crucial that they do not output unsafe, biased, or privacy-violating outputs. For this reason, models are both trained and instructed to refuse to answer unsafe prompts such as "Tell me how to build a bomb." We find that, despite these safeguards, it is possible to break model defenses simply by appending a space to the end of a model's input. In a study of eight open-source models, we demonstrate that this acts as a strong enough attack to cause the majority of models to generate harmful outputs with very high success rates. We examine the causes of this behavior, finding that the contexts in which single spaces occur in tokenized training data encourage models to generate lists when prompted, overriding training signals to refuse to answer unsafe requests. Our findings underscore the fragile state of current model alignment and promote the importance of developing more robust alignment methods. Code and data will be made available at https://github.com/Linlt-leon/Adversarial-Alignments.

------------

`[2407.03310] Universal Length Generalization with Turing Programs <https://arxiv.org/abs/2407.03310>`__ 图灵程序的通用长度泛化

::

    Wed, 3 Jul 2024 17:53:44 GMT
    Kaiying Hou, David Brandfonbrener, Sham Kakade, Samy Jelassi, Eran Malach

Length generalization refers to the ability to extrapolate from short training sequences to long test sequences and is a challenge for current large language models. While prior work has proposed some architecture or data format changes to achieve length generalization, these proposals typically apply to a limited set of tasks. Building on prior scratchpad and Chain-of-Thought (CoT) techniques, we propose Turing Programs, a novel CoT strategy that decomposes an algorithmic task into steps mimicking the computation of a Turing Machine. This framework is both universal, as it can accommodate any algorithmic task, and simple, requiring only copying text from the context with small modifications.
We show that by using Turing Programs, we obtain robust length generalization on a range of algorithmic tasks: addition, multiplication and in-context SGD.
We then demonstrate that transformers achieve length generalization on random Turing Programs, suggesting that length generalization is possible for any algorithmic task. Finally, we theoretically prove that transformers can implement Turing Programs, constructing a simple RASP (Weiss et al.) program that simulates an arbitrary Turing machine.

------------

`[2407.02511] LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning <https://arxiv.org/abs/2407.02511>`__ LLM-A*:大型语言模型增强的路径规划增量启发式搜索

::

    Thu, 20 Jun 2024 01:24:30 GMT
    Silin Meng, Yiwei Wang, Cheng-Fu Yang, Nanyun Peng, Kai-Wei Chang

Path planning is a fundamental scientific problem in robotics and autonomous navigation, requiring the derivation of efficient routes from starting to destination points while avoiding obstacles. Traditional algorithms like A* and its variants are capable of ensuring path validity but suffer from significant computational and memory inefficiencies as the state space grows. Conversely, large language models (LLMs) excel in broader environmental analysis through contextual understanding, providing global insights into environments. However, they fall short in detailed spatial and temporal reasoning, often leading to invalid or inefficient routes. In this work, we propose LLM-A*, an new LLM based route planning method that synergistically combines the precise pathfinding capabilities of A* with the global reasoning capability of LLMs.
This hybrid approach aims to enhance pathfinding efficiency in terms of time and space complexity while maintaining the integrity of path validity, especially in large-scale scenarios. By integrating the strengths of both methodologies, LLM-A* addresses the computational and memory limitations of conventional algorithms without compromising on the validity required for effective pathfinding.

------------

`[2407.02514] LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations <https://arxiv.org/abs/2407.02514>`__ LOGIC-LM++:符号公式的多步精化

::

    Sat, 22 Jun 2024 12:50:41 GMT
    Shashank Kirtania, Priyanshu Gupta, Arjun Radhakirshna

In this paper we examine the limitations of Large Language Models (LLMs) for complex reasoning tasks. While current approaches leverage formal languages as intermediate representation of reasoning problems, they struggle with generating intermediate formal specifications and refining these representations. To address these issues, this paper proposes Logic-LM++, an improvement on Logic-LM. It uses the ability of LLMs to do pairwise comparisons, allowing the evaluation of the refinements suggested by the LLM.
The paper demonstrates that Logic-LM++ outperforms Logic-LM and LLM based techniques on natural language reasoning tasks on two datasets, FOLIO and AR-LSAT. Logic-LM++ show an average improvement of 13.5% on standard prompting, 11% on chain of thought prompting and 5% on Logic-LM.

------------

`[2407.02518] INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness <https://arxiv.org/abs/2407.02518>`__ INDICT:具有内部评论对话的代码生成，以兼顾安全性和有用性

::

    Sun, 23 Jun 2024 15:55:07 GMT
    Hung Le, Yingbo Zhou, Caiming Xiong, Silvio Savarese, Doyen Sahoo

Large language models (LLMs) for code are typically trained to align with natural language instructions to closely follow their intentions and requirements. However, in many practical scenarios, it becomes increasingly challenging for these models to navigate the intricate boundary between helpfulness and safety, especially against highly complex yet potentially malicious instructions. In this work, we introduce INDICT: a new framework that empowers LLMs with Internal Dialogues of Critiques for both safety and helpfulness guidance. The internal dialogue is a dual cooperative system between a safety-driven critic and a helpfulness-driven critic. Each critic provides analysis against the given task and corresponding generated response, equipped with external knowledge queried through relevant code snippets and tools like web search and code interpreter. We engage the dual critic system in both code generation stage as well as code execution stage, providing preemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8 diverse tasks across 8 programming languages from 5 benchmarks, using LLMs from 7B to 70B parameters. We observed that our approach can provide an advanced level of critiques of both safety and helpfulness analysis, significantly improving the quality of output codes ($+10\%$ absolute improvements in all models).

------------

`[2407.02524] Meta Large Language Model Compiler: Foundation Models of Compiler Optimization <https://arxiv.org/abs/2407.02524>`__ 元大型语言模型编译器:编译优化的基础模型

::

    Thu, 27 Jun 2024 21:47:48 GMT
    Chris Cummins, Volker Seeker, Dejan Grubisic, Baptiste Roziere, Jonas Gehring, Gabriel Synnaeve, Hugh Leather

Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.

------------

`[2407.02528] Actionable Cyber Threat Intelligence using Knowledge Graphs and Large Language Models <https://arxiv.org/abs/2407.02528>`__ 基于知识图谱和大型语言模型的可行动网络威胁情报

::

    Sun, 30 Jun 2024 13:02:03 GMT
    Romy Fieblinger, Md Tanvirul Alam, Nidhi Rastogi

Cyber threats are constantly evolving. Extracting actionable insights from unstructured Cyber Threat Intelligence (CTI) data is essential to guide cybersecurity decisions. Increasingly, organizations like Microsoft, Trend Micro, and CrowdStrike are using generative AI to facilitate CTI extraction.
This paper addresses the challenge of automating the extraction of actionable CTI using advancements in Large Language Models (LLMs) and Knowledge Graphs (KGs). We explore the application of state-of-the-art open-source LLMs, including the Llama 2 series, Mistral 7B Instruct, and Zephyr for extracting meaningful triples from CTI texts. Our methodology evaluates techniques such as prompt engineering, the guidance framework, and fine-tuning to optimize information extraction and structuring. The extracted data is then utilized to construct a KG, offering a structured and queryable representation of threat intelligence. Experimental results demonstrate the effectiveness of our approach in extracting relevant information, with guidance and fine-tuning showing superior performance over prompt engineering. However, while our methods prove effective in small-scale tests, applying LLMs to large-scale data for KG construction and Link Prediction presents ongoing challenges.

------------

`[2407.02551] A False Sense of Safety: Unsafe Information Leakage in 'Safe' AI Responses <https://arxiv.org/abs/2407.02551>`__ 

::

    Tue, 2 Jul 2024 16:19:25 GMT
    David Glukhov, Ziwen Han, Ilia Shumailov, Vardan Papyan, Nicolas Papernot

Large Language Models (LLMs) are vulnerable to jailbreaks$\unicode{x2013}$methods to elicit harmful or generally impermissible outputs. Safety measures are developed and assessed on their effectiveness at defending against jailbreak attacks, indicating a belief that safety is equivalent to robustness. We assert that current defense mechanisms, such as output filters and alignment fine-tuning, are, and will remain, fundamentally insufficient for ensuring model safety. These defenses fail to address risks arising from dual-intent queries and the ability to composite innocuous outputs to achieve harmful goals. To address this critical gap, we introduce an information-theoretic threat model called inferential adversaries who exploit impermissible information leakage from model outputs to achieve malicious goals. We distinguish these from commonly studied security adversaries who only seek to force victim models to generate specific impermissible outputs. We demonstrate the feasibility of automating inferential adversaries through question decomposition and response aggregation. To provide safety guarantees, we define an information censorship criterion for censorship mechanisms, bounding the leakage of impermissible information. We propose a defense mechanism which ensures this bound and reveal an intrinsic safety-utility trade-off. Our work provides the first theoretically grounded understanding of the requirements for releasing safe LLMs and the utility costs involved.

------------

`[2407.02651] Improving Steering and Verification in AI-Assisted Data Analysis with Interactive Task Decomposition <https://arxiv.org/abs/2407.02651>`__ 交互式任务分解改进人工智能辅助数据分析中的导向和验证

::

    Tue, 2 Jul 2024 20:33:50 GMT
    Majeed Kazemitabaar, Jack Williams, Ian Drosos, Tovi Grossman, Austin Henley, Carina Negreanu, Advait Sarkar

LLM-powered tools like ChatGPT Data Analysis, have the potential to help users tackle the challenging task of data analysis programming, which requires expertise in data processing, programming, and statistics. However, our formative study (n=15) uncovered serious challenges in verifying AI-generated results and steering the AI (i.e., guiding the AI system to produce the desired output). We developed two contrasting approaches to address these challenges.
The first (Stepwise) decomposes the problem into step-by-step subgoals with pairs of editable assumptions and code until task completion, while the second (Phasewise) decomposes the entire problem into three editable, logical phases: structured input/output assumptions, execution plan, and code. A controlled, within-subjects experiment (n=18) compared these systems against a conversational baseline. Users reported significantly greater control with the Stepwise and Phasewise systems, and found intervention, correction, and verification easier, compared to the baseline. The results suggest design guidelines and trade-offs for AI-assisted data analysis tools.

------------

`[2407.02791] Model-Enhanced LLM-Driven VUI Testing of VPA Apps <https://arxiv.org/abs/2407.02791>`__ 模型增强的llm驱动的VPA应用程序VUI测试

::

    Wed, 3 Jul 2024 03:36:05 GMT
    Suwan Li, Lei Bu, Guangdong Bai, Fuman Xie, Kai Chen and Chang Yue

The flourishing ecosystem centered around voice personal assistants (VPA), such as Amazon Alexa, has led to the booming of VPA apps. The largest app market Amazon skills store, for example, hosts over 200,000 apps. Despite their popularity, the open nature of app release and the easy accessibility of apps also raise significant concerns regarding security, privacy and quality.
Consequently, various testing approaches have been proposed to systematically examine VPA app behaviors. To tackle the inherent lack of a visible user interface in the VPA app, two strategies are employed during testing, i.e., chatbot-style testing and model-based testing. The former often lacks effective guidance for expanding its search space, while the latter falls short in interpreting the semantics of conversations to construct precise and comprehensive behavior models for apps. In this work, we introduce Elevate, a model-enhanced large language model (LLM)-driven VUI testing framework. Elevate leverages LLMs' strong capability in natural language processing to compensate for semantic information loss during model-based VUI testing. It operates by prompting LLMs to extract states from VPA apps' outputs and generate context-related inputs. During the automatic interactions with the app, it incrementally constructs the behavior model, which facilitates the LLM in generating inputs that are highly likely to discover new states. Elevate bridges the LLM and the behavior model with innovative techniques such as encoding behavior model into prompts and selecting LLM-generated inputs based on the context relevance. Elevate is benchmarked on 4,000 real-world Alexa skills, against the state-of-the-art tester Vitas. It achieves 15% higher state space coverage compared to Vitas on all types of apps, and exhibits significant advancement in efficiency.

------------

`[2407.02943] PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding <https://arxiv.org/abs/2407.02943>`__ pi - compass:通过接地引导LLM训练数据提取提示指向目标PII

::

    Wed, 3 Jul 2024 09:20:04 GMT
    Krishna Kanth Nakka, Ahmed Frikha, Ricardo Mendes, Xue Jiang, Xuebing Zhou

The latest and most impactful advances in large models stem from their increased size. Unfortunately, this translates into an improved memorization capacity, raising data privacy concerns. Specifically, it has been shown that models can output personal identifiable information (PII) contained in their training data. However, reported PIII extraction performance varies widely, and there is no consensus on the optimal methodology to evaluate this risk, resulting in underestimating realistic adversaries. In this work, we empirically demonstrate that it is possible to improve the extractability of PII by over ten-fold by grounding the prefix of the manually constructed extraction prompt with in-domain data. Our approach, PII-Compass, achieves phone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308 queries, respectively, i.e., the phone number of 1 person in 15 is extractable.

------------

`[2407.02956] IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization <https://arxiv.org/abs/2407.02956>`__ IncogniText:基于llm隐私属性随机化的隐私增强条件文本匿名

::

    Wed, 3 Jul 2024 09:49:03 GMT
    Ahmed Frikha, Nassim Walha, Krishna Kanth Nakka, Ricardo Mendes, Xue Jiang, Xuebing Zhou

In this work, we address the problem of text anonymization where the goal is to prevent adversaries from correctly inferring private attributes of the author, while keeping the text utility, i.e., meaning and semantics. We propose IncogniText, a technique that anonymizes the text to mislead a potential adversary into predicting a wrong private attribute value. Our empirical evaluation shows a reduction of private attribute leakage by more than 90%.
Finally, we demonstrate the maturity of IncogniText for real-world applications by distilling its anonymization capability into a set of LoRA parameters associated with an on-device model.

------------

`[2407.02960] ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets <https://arxiv.org/abs/2407.02960>`__ ObfuscaTune:私有llm在私有数据集上的模糊异地微调和推理

::

    Wed, 3 Jul 2024 09:54:08 GMT
    Ahmed Frikha, Nassim Walha, Ricardo Mendes, Krishna Kanth Nakka, Xue Jiang, Xuebing Zhou

This work addresses the timely yet underexplored problem of performing inference and finetuning of a proprietary LLM owned by a model provider entity on the confidential/private data of another data owner entity, in a way that ensures the confidentiality of both the model and the data. Hereby, the finetuning is conducted offsite, i.e., on the computation infrastructure of a third-party cloud provider. We tackle this problem by proposing ObfuscaTune, a novel, efficient and fully utility-preserving approach that combines a simple yet effective obfuscation technique with an efficient usage of confidential computing (only 5% of the model parameters are placed on TEE). We empirically demonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models with different sizes on four NLP benchmark datasets. Finally, we compare to a na\"ive version of our approach to highlight the necessity of using random matrices with low condition numbers in our approach to reduce errors induced by the obfuscation.

------------

`[2407.02994] MedPix 2.0: A Comprehensive Multimodal Biomedical Dataset for Advanced AI Applications <https://arxiv.org/abs/2407.02994>`__ MedPix 2.0:面向高级人工智能应用的全面多模态生物医学数据集

::

    Wed, 3 Jul 2024 10:49:21 GMT
    Irene Siragusa, Salvatore Contino, Massimo La Ciura, Rosario Alicata, Roberto Pirrone

The increasing interest in developing Artificial Intelligence applications in the medical domain, suffers from the lack of high-quality dataset, mainly due to privacy-related issues. Moreover, the recent rising of Multimodal Large Language Models (MLLM) leads to a need for multimodal medical datasets, where clinical reports and findings are attached to the corresponding CT or MR scans.
This paper illustrates the entire workflow for building the data set MedPix 2.0. Starting from the well-known multimodal dataset MedPix\textsuperscript{\textregistered}, mainly used by physicians, nurses and healthcare students for Continuing Medical Education purposes, a semi-automatic pipeline was developed to extract visual and textual data followed by a manual curing procedure where noisy samples were removed, thus creating a MongoDB database. Along with the dataset, we developed a GUI aimed at navigating efficiently the MongoDB instance, and obtaining the raw data that can be easily used for training and/or fine-tuning MLLMs. To enforce this point, we also propose a CLIP-based model trained on MedPix 2.0 for scan classification tasks.

------------

`[2407.03203] TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts <https://arxiv.org/abs/2407.03203>`__ 定理llama:将通用llm转换为Lean4专家

::

    Wed, 3 Jul 2024 15:36:18 GMT
    Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, Tong Zhang

Proving mathematical theorems using computer-verifiable formal languages like Lean significantly impacts mathematical reasoning. One approach to formal theorem proving involves generating complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs. Similar methods have shown promising results in code generation. However, most modern LLMs exhibit suboptimal performance due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data. This scarcity results in a paucity of methodologies for training LLMs and techniques to fully utilize their capabilities in composing formal proofs. To address the challenges, this paper proposes **TheoremLlama**, an end-to-end framework to train a general-purpose LLM to become a Lean4 expert. This framework encompasses NL-FL aligned dataset generation methods, training approaches for the LLM formal theorem prover, and techniques for LLM Lean4 proof writing. Using the dataset generation method, we provide *Open Bootstrapped Theorems* (OBT), an NL-FL aligned and bootstrapped dataset. A key innovation in this framework is the NL-FL bootstrapping method, where NL proofs are integrated into Lean4 code for training datasets, leveraging the NL reasoning ability of LLMs for formal reasoning. The **TheoremLlama** framework achieves cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%. We have also open-sourced our model checkpoints and generated dataset, and will soon make all the code publicly available.

------------

`[2407.02662] Supporters and Skeptics: LLM-based Analysis of Engagement with Mental Health (Mis)Information Content on Video-sharing Platforms <https://arxiv.org/abs/2407.02662>`__ 

::

    Tue, 2 Jul 2024 20:51:06 GMT
    Viet Cuong Nguyen, Mini Jain, Abhijat Chauhan, Heather Jaime Soled, Santiago Alvarez Lesmes, Zihang Li, Michael L. Birnbaum, Sunny X. Tang, Srijan Kumar, Munmun De Choudhury

Over one in five adults in the US lives with a mental illness. In the face of a shortage of mental health professionals and offline resources, online short-form video content has grown to serve as a crucial conduit for disseminating mental health help and resources. However, the ease of content creation and access also contributes to the spread of misinformation, posing risks to accurate diagnosis and treatment. Detecting and understanding engagement with such content is crucial to mitigating their harmful effects on public health. We perform the first quantitative study of the phenomenon using YouTube Shorts and Bitchute as the sites of study. We contribute MentalMisinfo, a novel labeled mental health misinformation (MHMisinfo) dataset of 739 videos (639 from Youtube and 100 from Bitchute) and 135372 comments in total, using an expert-driven annotation schema. We first found that few-shot in-context learning with large language models (LLMs) are effective in detecting MHMisinfo videos. Next, we discover distinct and potentially alarming linguistic patterns in how audiences engage with MHMisinfo videos through commentary on both video-sharing platforms. Across the two platforms, comments could exacerbate prevailing stigma with some groups showing heightened susceptibility to and alignment with MHMisinfo. We discuss technical and public health-driven adaptive solutions to tackling the "epidemic" of mental health misinformation online.

------------

`[2407.02833] LANE: Logic Alignment of Non-tuning Large Language Models and Online Recommendation Systems for Explainable Reason Generation <https://arxiv.org/abs/2407.02833>`__ LANE:面向可解释原因生成的非调优大型语言模型与在线推荐系统的逻辑对齐

::

    Wed, 3 Jul 2024 06:20:31 GMT
    Hongke Zhao, Songming Zheng, Likang Wu, Bowen Yu, Jing Wang

The explainability of recommendation systems is crucial for enhancing user trust and satisfaction. Leveraging large language models (LLMs) offers new opportunities for comprehensive recommendation logic generation. However, in existing related studies, fine-tuning LLM models for recommendation tasks incurs high computational costs and alignment issues with existing systems, limiting the application potential of proven proprietary/closed-source LLM models, such as GPT-4. In this work, our proposed effective strategy LANE aligns LLMs with online recommendation systems without additional LLMs tuning, reducing costs and improving explainability. This innovative approach addresses key challenges in integrating language models with recommendation systems while fully utilizing the capabilities of powerful proprietary models. Specifically, our strategy operates through several key components: semantic embedding, user multi-preference extraction using zero-shot prompting, semantic alignment, and explainable recommendation generation using Chain of Thought (CoT) prompting.
By embedding item titles instead of IDs and utilizing multi-head attention mechanisms, our approach aligns the semantic features of user preferences with those of candidate items, ensuring coherent and user-aligned recommendations.
Sufficient experimental results including performance comparison, questionnaire voting, and visualization cases prove that our method can not only ensure recommendation performance, but also provide easy-to-understand and reasonable recommendation logic.

------------

`[2407.02855] Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks <https://arxiv.org/abs/2407.02855>`__ 安全遗忘(Safe Unlearning):一种防御越狱攻击的有效且通用的解决方案

::

    Wed, 3 Jul 2024 07:14:05 GMT
    Zhexin Zhang, Junxiao Yang, Pei Ke, Shiyao Cui, Chujie Zheng, Hongning Wang, Minlie Huang

LLMs are known to be vulnerable to jailbreak attacks, even after safety alignment. An important observation is that, while different types of jailbreak attacks can generate significantly different queries, they mostly result in similar responses that are rooted in the same harmful knowledge (e.g., detailed steps to make a bomb). Therefore, we conjecture that directly unlearn the harmful knowledge in the LLM can be a more effective way to defend against jailbreak attacks than the mainstream supervised fine-tuning (SFT) based approaches. Our extensive experiments confirmed our insight and suggested surprising generalizability of our unlearning-based approach: using only 20 raw harmful questions \emph{without} any jailbreak prompt during training, our solution reduced the Attack Success Rate (ASR) in Vicuna-7B on \emph{out-of-distribution} (OOD) harmful questions wrapped with various complex jailbreak prompts from 82.6\% to 7.7\%. This significantly outperforms Llama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment samples but still has an ASR of 21.9\% even under the help of an additional safety system prompt. Further analysis reveals that the generalization ability of our solution stems from the intrinsic relatedness among harmful responses across harmful questions (e.g., response patterns, shared steps and actions, and similarity among their learned representations in the LLM). Our code is available at \url{https://github.com/thu-coai/SafeUnlearning}.

------------

`[2407.02885] CogErgLLM: Exploring Large Language Model Systems Design Perspective Using Cognitive Ergonomics <https://arxiv.org/abs/2407.02885>`__ CogErgLLM:用认知人机工程学探索大型语言模型系统设计视角

::

    Wed, 3 Jul 2024 07:59:52 GMT
    Azmine Toushik Wasi

Integrating cognitive ergonomics with LLMs is essential for enhancing safety, reliability, and user satisfaction in human-AI interactions. Current LLM design often lacks this integration, leading to systems that may not fully align with human cognitive capabilities and limitations. Insufficient focus on incorporating cognitive science methods exacerbates biases in LLM outputs, while inconsistent application of user-centered design principles results in sub-optimal user experiences. To address these challenges, our position paper explores the critical integration of cognitive ergonomics principles into LLM design, aiming to provide a comprehensive framework and practical guidelines for ethical LLM development. Through our contributions, we seek to advance understanding and practice in integrating cognitive ergonomics into LLM systems, fostering safer, more reliable, and ethically sound human-AI interactions.

------------

`[2407.03045] JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets <https://arxiv.org/abs/2407.03045>`__ JailbreakHunter:一种针对越狱的可视分析方法，可以从大规模人类llm对话数据集中发现漏洞

::

    Wed, 3 Jul 2024 12:10:41 GMT
    Zhihua Jin, Shiyi Liu, Haotian Li, Xun Zhao, and Huamin Qu

Large Language Models (LLMs) have gained significant attention but also raised concerns due to the risk of misuse. Jailbreak prompts, a popular type of adversarial attack towards LLMs, have appeared and constantly evolved to breach the safety protocols of LLMs. To address this issue, LLMs are regularly updated with safety patches based on reported jailbreak prompts. However, malicious users often keep their successful jailbreak prompts private to exploit LLMs. To uncover these private jailbreak prompts, extensive analysis of large-scale conversational datasets is necessary to identify prompts that still manage to bypass the system's defenses. This task is highly challenging due to the immense volume of conversation data, diverse characteristics of jailbreak prompts, and their presence in complex multi-turn conversations. To tackle these challenges, we introduce JailbreakHunter, a visual analytics approach for identifying jailbreak prompts in large-scale human-LLM conversational datasets.
We have designed a workflow with three analysis levels: group-level, conversation-level, and turn-level. Group-level analysis enables users to grasp the distribution of conversations and identify suspicious conversations using multiple criteria, such as similarity with reported jailbreak prompts in previous research and attack success rates. Conversation-level analysis facilitates the understanding of the progress of conversations and helps discover jailbreak prompts within their conversation contexts. Turn-level analysis allows users to explore the semantic similarity and token overlap between a singleturn prompt and the reported jailbreak prompts, aiding in the identification of new jailbreak strategies. The effectiveness and usability of the system were verified through multiple case studies and expert interviews.

------------

`[2407.03104] KeyVideoLLM: Towards Large-scale Video Keyframe Selection <https://arxiv.org/abs/2407.03104>`__ KeyVideoLLM:面向大规模视频关键帧选择的研究

::

    Wed, 3 Jul 2024 13:41:44 GMT
    Hao Liang, Jiapeng Li, Tianyi Bai, Chong Chen, Conghui He, Bin Cui, Wentao Zhang

Recently, with the rise of web videos, managing and understanding large-scale video datasets has become increasingly important. Video Large Language Models (VideoLLMs) have emerged in recent years due to their strong video understanding capabilities. However, training and inference processes for VideoLLMs demand vast amounts of data, presenting significant challenges to data management, particularly regarding efficiency, robustness, and effectiveness. In this work, we present KeyVideoLLM, a text-video frame similarity-based keyframe selection method designed to manage VideoLLM data efficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a remarkable data compression rate of up to 60.9 times, substantially lowering disk space requirements, which proves its high efficiency. Additionally, it maintains a 100% selection success rate across all video formats and scales, enhances processing speed by up to 200 times compared to existing keyframe selection methods, and does not require hyperparameter tuning. Beyond its outstanding efficiency and robustness, KeyVideoLLM further improves model performance in video question-answering tasks during both training and inference stages. Notably, it consistently achieved the state-of-the-art (SoTA) experimental results on diverse datasets.

------------

`[2407.03160] SOS! Soft Prompt Attack Against Open-Source Large Language Models <https://arxiv.org/abs/2407.03160>`__ SOS !针对开源大型语言模型的软提示攻击

::

    Wed, 3 Jul 2024 14:35:16 GMT
    Ziqing Yang, Michael Backes, Yang Zhang, Ahmed Salem

Open-source large language models (LLMs) have become increasingly popular among both the general public and industry, as they can be customized, fine-tuned, and freely used. However, some open-source LLMs require approval before usage, which has led to third parties publishing their own easily accessible versions. Similarly, third parties have been publishing fine-tuned or quantized variants of these LLMs. These versions are particularly appealing to users because of their ease of access and reduced computational resource demands. This trend has increased the risk of training time attacks, compromising the integrity and security of LLMs. In this work, we present a new training time attack, SOS, which is designed to be low in computational demand and does not require clean data or modification of the model weights, thereby maintaining the model's utility intact. The attack addresses security issues in various scenarios, including the backdoor attack, jailbreak attack, and prompt stealing attack. Our experimental findings demonstrate that the proposed attack is effective across all evaluated targets. Furthermore, we present the other side of our SOS technique, namely the copyright token -- a novel technique that enables users to mark their copyrighted content and prevent models from using it.

------------

`[2407.03320] InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output <https://arxiv.org/abs/2407.03320>`__ InternLM-XComposer-2.5:支持长上下文输入和输出的通用大型视觉语言模型

::

    Wed, 3 Jul 2024 17:59:21 GMT
    Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang

We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer.

------------

`[2401.10545] Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency <https://arxiv.org/abs/2401.10545>`__ 理解基于聊天gpt推荐系统中的偏差:提供者公平性、时间稳定性和时效性

::

    Fri, 19 Jan 2024 08:09:20 GMT
    Yashar Deldjoo

This study explores the nuanced capabilities and inherent biases of Recommender Systems using Large Language Models (RecLLMs), with a focus on ChatGPT-based systems. It studies into the contrasting behaviors of generative models and traditional collaborative filtering models in movie recommendations.
The research primarily investigates prompt design strategies and their impact on various aspects of recommendation quality, including accuracy, provider fairness, diversity, stability, genre dominance, and temporal freshness (recency).
Our experimental analysis reveals that the introduction of specific 'system roles' and 'prompt strategies' in RecLLMs significantly influences their performance. For instance, role-based prompts enhance fairness and diversity in recommendations, mitigating popularity bias. We find that while GPT-based models do not always match the performance of CF baselines, they exhibit a unique tendency to recommend newer and more diverse movie genres. Notably, GPT-based models tend to recommend more recent films, particularly those released post-2000, and show a preference for genres like \sq{Drama} and Comedy, and Romance (compared to CF Action, Adventure) presumably due to the RecLLMs' training on varied data sets, which allows them to capture recent trends and discussions more effectively than CF models. Interestingly, our results demonstrate that the 'Simple' and 'Chain of Thought (COT)' paradigms yield the highest accuracy. These findings imply the potential of combining these strategies with scenarios that favor more recent content, thereby offering a more balanced and up-to-date recommendation experience. This study contributes significantly to the understanding of emerging RecLLMs, particularly in the context of harms and biases within these systems.

------------

`[2308.00081] Towards Semantically Enriched Embeddings for Knowledge Graph Completion <https://arxiv.org/abs/2308.00081>`__ 面向知识图谱补全的语义丰富嵌入

::

    replaced with revised version Wed, 3 Jul 2024 12:00:37 GMT
    Submission history From: Mehwish Alam [view email]
    [v1] Mon, 31 Jul 2023 18:53:47 UTC (450 KB)
    [v2] Wed, 2 Aug 2023 07:34:24 UTC (446 KB)
    [v3] Wed, 3 Jul 2024 12:00:37 UTC (148 KB)
    [v4] Thu, 11 Jul 2024 13:18:29 UTC (144 KB)
    Mehwish Alam, Frank van Harmelen, Maribel Acosta

Embedding based Knowledge Graph (KG) Completion has gained much attention over the past few years. Most of the current algorithms consider a KG as a multidirectional labeled graph and lack the ability to capture the semantics underlying the schematic information. In a separate development, a vast amount of information has been captured within the Large Language Models (LLMs) which has revolutionized the field of Artificial Intelligence. KGs could benefit from these LLMs and vice versa. This vision paper discusses the existing algorithms for KG completion based on the variations for generating KG embeddings. It starts with discussing various KG completion algorithms such as transductive and inductive link prediction and entity type prediction algorithms. It then moves on to the algorithms utilizing type information within the KGs, LLMs, and finally to algorithms capturing the semantics represented in different description logic axioms. We conclude the paper with a critical reflection on the current state of work in the community and give recommendations for future directions.

------------

`[2305.15060] Who Wrote this Code? Watermarking for Code Generation <https://arxiv.org/abs/2305.15060>`__ 

::

    replaced with revised version Wed, 3 Jul 2024 15:09:52 GMT
    Submission history From: Jamin Shin [view email]
    [v1] Wed, 24 May 2023 11:49:52 UTC (411 KB)
    [v2] Fri, 17 Nov 2023 04:20:01 UTC (7,397 KB)
    [v3] Fri, 23 Feb 2024 08:32:10 UTC (1,788 KB)
    [v4] Wed, 3 Jul 2024 15:09:52 UTC (2,149 KB)
    Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, Gunhee Kim

Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to detect machine-generated text by embedding watermarks are being developed. However, we discover that the existing works fail to function appropriately in code generation tasks due to the task's nature of having low entropy. Extending a logit-modifying watermark method, we propose Selective WatErmarking via Entropy Thresholding (SWEET), which enhances detection ability and mitigates code quality degeneration by removing low-entropy segments at generating and detecting watermarks. Our experiments show that SWEET significantly improves code quality preservation while outperforming all baselines, including post-hoc detection methods, in detecting machine-generated code text. Our code is available in this https URL.

------------

`[2307.02762] PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations <https://arxiv.org/abs/2307.02762>`__ PRD: Peer Rank and Discussion改进基于评价的大型语言模型

::

    replaced with revised version Wed, 3 Jul 2024 04:34:03 GMT
    Submission history From: Ruosen Li [view email]
    [v1] Thu, 6 Jul 2023 04:05:44 UTC (4,811 KB)
    [v2] Wed, 3 Jul 2024 04:34:03 UTC (4,844 KB)
    Ruosen Li, Teerth Patel, Xinya Du

Nowadays, the quality of responses generated by different modern large language models (LLMs) is hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs for reference-free evaluation of open-ended question answering. More specifically, they use the recognized "strongest" LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho & MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose (1) the peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on the preferences of two answers. We conduct experiments on two benchmark datasets. We find that our approaches achieve higher accuracy and align better with human judgments. Interestingly, PR can induce a relatively accurate self-ranking of models under the anonymous setting, where each model's name is unrevealed. Our work provides space to explore evaluating models that are hard to compare for humans.

------------

`[2311.05661] Prompt Engineering a Prompt Engineer <https://arxiv.org/abs/2311.05661>`__ 提示工程师提示工程师

::

    replaced with revised version Wed, 3 Jul 2024 01:29:20 GMT
    Submission history From: Qinyuan Ye [view email]
    [v1] Thu, 9 Nov 2023 08:00:32 UTC (8,511 KB)
    [v2] Mon, 19 Feb 2024 19:46:05 UTC (8,208 KB)
    [v3] Wed, 3 Jul 2024 01:29:20 UTC (8,312 KB)
    Qinyuan Ye, Maxamed Axmed, Reid Pryzant, Fereshte Khani

Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, exhibits remarkable versatility across diverse language tasks. It finds prompts that outperform "let's think step by step" by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks by 6.9%. Further, we show that PE2 can make targeted and highly specific prompt edits, rectify erroneous prompts, and induce multi-step plans for complex tasks.

------------

`[2311.09325] Temperature-scaling surprisal estimates improve fit to human reading times -- but does it do so for the "right reasons"? <https://arxiv.org/abs/2311.09325>`__ 温度尺度的惊讶估计提高了与人类阅读时间的适应性——但它这样做的原因是“正确的”吗?

::

    replaced with revised version Wed, 3 Jul 2024 16:12:32 GMT
    Submission history From: Tong Liu [view email]
    [v1] Wed, 15 Nov 2023 19:34:06 UTC (606 KB)
    [v2] Wed, 3 Jul 2024 16:12:32 UTC (1,563 KB)
    Tong Liu, Iza \v{S}krjanec, Vera Demberg

A wide body of evidence shows that human language processing difficulty is predicted by the information-theoretic measure surprisal, a word's negative log probability in context. However, it is still unclear how to best estimate these probabilities needed for predicting human processing difficulty -- while a long-standing belief held that models with lower perplexity would provide more accurate estimates of word predictability, and therefore lead to better reading time predictions, recent work has shown that for very large models, psycholinguistic predictive power decreases. One reason could be that language models might be more confident of their predictions than humans, because they have had exposure to several magnitudes more data. In this paper, we test what effect temperature-scaling of large language model (LLM) predictions has on surprisal estimates and their predictive power of reading times of English texts. Firstly, we show that calibration of large language models typically improves with model size, i.e. poorer calibration cannot account for poorer fit to reading times. Secondly, we find that temperature-scaling probabilities lead to a systematically better fit to reading times (up to 89% improvement in delta log likelihood), across several reading time corpora. Finally, we show that this improvement in fit is chiefly driven by words that are composed of multiple subword tokens.

------------

`[2312.12736] Learning and Forgetting Unsafe Examples in Large Language Models <https://arxiv.org/abs/2312.12736>`__ 大型语言模型中不安全示例的学习和遗忘

::

    replaced with revised version Wed, 3 Jul 2024 06:13:31 GMT
    Submission history From: Jiachen Zhao [view email]
    [v1] Wed, 20 Dec 2023 03:18:50 UTC (389 KB)
    [v2] Wed, 3 Jul 2024 06:13:31 UTC (462 KB)
    Jiachen Zhao, Zhun Deng, David Madras, James Zou, Mengye Ren

As the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content. Drawing inspiration from the discrepancies in forgetting, we introduce the "ForgetFilter" algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning. ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score.

------------

`[2312.15997] Aligning Large Language Models with Human Preferences through Representation Engineering <https://arxiv.org/abs/2312.15997>`__ 通过表示工程使大型语言模型与人类偏好相一致

::

    replaced with revised version Wed, 3 Jul 2024 05:21:02 GMT
    Submission history From: Xiaohua Wang [view email]
    [v1] Tue, 26 Dec 2023 11:01:36 UTC (8,594 KB)
    [v2] Tue, 2 Jul 2024 04:07:14 UTC (10,007 KB)
    [v3] Wed, 3 Jul 2024 05:21:02 UTC (11,871 KB)
    Wenhao Liu, Xiaohua Wang, Muling Wu, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang

Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involves employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses. Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in implementation.Drawing inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM, and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to implement.Extensive experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). RAHF's versatility in accommodating diverse human preferences shows its potential for advancing LLM performance.

------------

`[2401.14267] Transformers and Cortical Waves: Encoders for Pulling In Context Across Time <https://arxiv.org/abs/2401.14267>`__ transformer和皮层波:跨时间提取上下文的编码器

::

    replaced with revised version Wed, 3 Jul 2024 03:41:07 GMT
    Submission history From: Terrence Sejnowski [view email]
    [v1] Thu, 25 Jan 2024 16:01:49 UTC (1,049 KB)
    [v2] Wed, 3 Jul 2024 03:41:07 UTC (549 KB)
    Lyle Muller, Patricia S. Churchland, and Terrence J. Sejnowski

The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention. The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence - into a long "encoding vector" that allows transformers to learn long-range temporal dependencies in naturalistic sequences. Specifically, "self-attention" applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence. We suggest that waves of neural activity traveling across single cortical areas or multiple regions at the whole-brain scale could implement a similar encoding principle. By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in transformers.

------------

`[2402.03898] DistiLLM: Towards Streamlined Distillation for Large Language Models <https://arxiv.org/abs/2402.03898>`__ DistiLLM:面向大型语言模型的流线型蒸馏

::

    replaced with revised version Wed, 3 Jul 2024 04:57:41 GMT
    Submission history From: Jongwoo Ko [view email]
    [v1] Tue, 6 Feb 2024 11:10:35 UTC (1,690 KB)
    [v2] Wed, 3 Jul 2024 04:57:41 UTC (1,692 KB)
    Jongwoo Ko, Sungnyun Kim, Tianyi Chen, Se-Young Yun

Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\times$ speedup compared to recent KD methods.

------------

`[2402.10671] Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm <https://arxiv.org/abs/2402.10671>`__ 增强注意力的分解:通过工作流范式改进基于llm的Text-to-SQL

::

    replaced with revised version Wed, 3 Jul 2024 04:45:29 GMT
    Submission history From: Yuanzhen Xie [view email]
    [v1] Fri, 16 Feb 2024 13:24:05 UTC (4,156 KB)
    [v2] Fri, 7 Jun 2024 09:45:21 UTC (4,299 KB)
    [v3] Wed, 3 Jul 2024 04:45:29 UTC (4,299 KB)
    Yuanzhen Xie, Xinzhou Jin, Tao Xie, MingXiong Lin, Liang Chen, Chenyun Yu, Lei Cheng, ChengXiang Zhuo, Bo Hu, Zang Li

In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model's attention. Additionally, the inclusion of self-correction and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches. Extensive experiments conducted on three datasets demonstrate that our approach outperforms other methods by a significant margin. About 2-3 percentage point improvements compared to the existing baseline on the Spider Dev, Spider-Realistic, and Bird Dev datasets and new SOTA results on the Spider Test dataset are achieved. Our code is available on GitHub: \url{this https URL}.

------------

`[2402.13228] Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive <https://arxiv.org/abs/2402.13228>`__ Smaug:修复DPO-Positive偏好优化的失败模式

::

    replaced with revised version Wed, 3 Jul 2024 13:46:33 GMT
    Submission history From: Samuel Dooley [view email]
    [v1] Tue, 20 Feb 2024 18:42:34 UTC (294 KB)
    [v2] Wed, 3 Jul 2024 13:46:33 UTC (312 KB)
    Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, Colin White

Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the relative probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a reduction of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and other fine-tuning procedures across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. Furthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model (all else equal) on benchmarks independent of the fine-tuning data, such as MT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and Smaug-72B, with the latter becoming the first open-source LLM to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard.

------------

`[2402.16029] GraphWiz: An Instruction-Following Language Model for Graph Problems <https://arxiv.org/abs/2402.16029>`__ GraphWiz:面向图问题的指令遵循语言模型

::

    replaced with revised version Wed, 3 Jul 2024 06:39:59 GMT
    Submission history From: Nuo Chen [view email]
    [v1] Sun, 25 Feb 2024 08:41:32 UTC (1,180 KB)
    [v2] Wed, 6 Mar 2024 13:52:12 UTC (1,180 KB)
    [v3] Mon, 1 Jul 2024 09:15:38 UTC (1,187 KB)
    [v4] Tue, 2 Jul 2024 06:40:30 UTC (1,633 KB)
    [v5] Wed, 3 Jul 2024 06:39:59 UTC (1,634 KB)
    Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li

Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the delicate balance between training data volume and model performance, highlighting the potential for overfitting with increased data. We also explore the transferability of the model's reasoning ability across different graph tasks, indicating the model's adaptability and practical application potential. Our investigation offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving.

------------

`[2403.07794] Fine-tuning Large Language Models with Sequential Instructions <https://arxiv.org/abs/2403.07794>`__ 基于顺序指令的大型语言模型微调

::

    replaced with revised version Wed, 3 Jul 2024 08:18:44 GMT
    Submission history From: Hanxu Hu [view email]
    [v1] Tue, 12 Mar 2024 16:33:30 UTC (1,526 KB)
    [v2] Thu, 20 Jun 2024 17:53:29 UTC (902 KB)
    [v3] Wed, 3 Jul 2024 08:18:44 UTC (902 KB)
    Hanxu Hu, Simon Yu, Pinzhen Chen, Edoardo M. Ponti

Despite the success of existing instruction-tuned models, we find that they usually struggle to respond to queries with multiple instructions. This impairs their performance in complex problems whose solution consists of multiple intermediate tasks. Thus, we contend that part of the fine-tuning data mixture should be sequential--containing a chain of interrelated tasks. We first approach sequential instruction tuning from a task-driven perspective, manually creating interpretable intermediate tasks for multilingual and visual question answering: namely "translate then predict" and "caption then answer". Next, we automate this process by turning instructions in existing datasets (e.g., Alpaca and FlanCoT) into diverse and complex sequential instructions, making our method general-purpose. Models that underwent our sequential instruction tuning show improved results in coding, maths, and open-ended generation. Moreover, we put forward a new benchmark named SeqEval to evaluate a model's ability to follow all the instructions in a sequence, which further corroborates the benefits of our fine-tuning method. We hope that our endeavours will open new research avenues on instruction tuning for complex tasks.

------------

`[2403.19887] Jamba: A Hybrid Transformer-Mamba Language Model <https://arxiv.org/abs/2403.19887>`__ Jamba: Transformer-Mamba混合语言模型

::

    replaced with revised version Wed, 3 Jul 2024 14:30:33 GMT
    Submission history From: Yonatan Belinkov [view email]
    [v1] Thu, 28 Mar 2024 23:55:06 UTC (941 KB)
    [v2] Wed, 3 Jul 2024 14:30:33 UTC (1,121 KB)
    Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, Yoav Shoham

We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.

------------

`[2404.05880] Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge <https://arxiv.org/abs/2404.05880>`__ Eraser:基于有害知识遗忘的大型语言模型越狱防御

::

    replaced with revised version Wed, 3 Jul 2024 17:52:00 GMT
    Submission history From: Weikai Lu [view email]
    [v1] Mon, 8 Apr 2024 21:26:22 UTC (8,137 KB)
    [v2] Wed, 3 Jul 2024 17:52:00 UTC (7,853 KB)
    Weikai Lu, Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Zelin Chen, Huiping Zhuang, Cen Chen

Jailbreaking attacks can enable Large Language Models (LLMs) to bypass the safeguard and generate harmful content. Existing jailbreaking defense methods have failed to address the fundamental issue that harmful knowledge resides within the model, leading to potential jailbreak risks for LLMs. In this paper, we propose a novel defense method called Eraser, which mainly includes three goals: unlearning harmful knowledge, retaining general knowledge, and maintaining safety alignment. The intuition is that if an LLM forgets the specific knowledge required to answer a harmful question, it will no longer have the ability to answer harmful questions. The training of Erase does not actually require the model's own harmful knowledge, and it can benefit from unlearning general answers related to harmful queries, which means it does not need assistance from the red team. The experimental results show that Eraser can significantly reduce the jailbreaking success rate for various attacks without compromising the general capabilities of the model. Our codes are available at this https URL.

------------

`[2405.00557] Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment <https://arxiv.org/abs/2405.00557>`__ 有洞察力的专家混合(MoTE):思维链和专家混合在自我对齐中的协同作用

::

    replaced with revised version Wed, 3 Jul 2024 15:04:25 GMT
    Submission history From: Zhili Liu [view email]
    [v1] Wed, 1 May 2024 15:06:05 UTC (389 KB)
    [v2] Wed, 3 Jul 2024 15:04:25 UTC (379 KB)
    [v3] Mon, 8 Jul 2024 16:02:18 UTC (380 KB)
    Zhili Liu, Yunhao Gou, Kai Chen, Lanqing Hong, Jiahui Gao, Fei Mi, Yu Zhang, Zhenguo Li, Xin Jiang, Qun Liu, James T. Kwok

As the capabilities of large language models (LLMs) have expanded dramatically, aligning these models with human values presents a significant challenge. Traditional alignment strategies rely heavily on human intervention, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), or on the self-alignment capacities of LLMs, which usually require a strong LLM's emergent ability to improve its original bad answer. To address these challenges, we propose a novel self-alignment method that utilizes a Chain of Thought (CoT) approach, termed AlignCoT. This method encompasses stages of Question Analysis, Answer Guidance, and Safe Answer production. It is designed to enable LLMs to generate high-quality, safe responses throughout various stages of their development. Furthermore, we introduce the Mixture of insighTful Experts (MoTE) architecture, which applies mixture of experts to enhance each component of the AlignCoT process, markedly increasing alignment efficiency. The MoTE approach not only outperforms existing methods in aligning LLMs with human values but also highlights the benefits of using self-generated data, revealing the dual benefits of improved alignment and training efficiency.

------------

`[2405.13022] LLMs can learn self-restraint through iterative self-reflection <https://arxiv.org/abs/2405.13022>`__ 

::

    replaced with revised version Wed, 3 Jul 2024 14:46:52 GMT
    Submission history From: Alexandre Piché [view email]
    [v1] Wed, 15 May 2024 13:35:43 UTC (3,848 KB)
    [v2] Wed, 3 Jul 2024 14:46:52 UTC (3,872 KB)
    Alexandre Pich\'e, Aristides Milios, Dzmitry Bahdanau, Chris Pal

In order to be deployed safely, Large Language Models (LLMs) must be capable of dynamically adapting their behavior based on their level of knowledge and uncertainty associated with specific topics. This adaptive behavior, which we refer to as self-restraint, is non-trivial to teach since it depends on the internal knowledge of an LLM. By default, LLMs are trained to maximize the next token likelihood, which does not teach the model to modulate its answer based on its level of uncertainty. In order to learn self-restraint, we devise a utility function that can encourage the model to produce responses only when it is confident in them. This utility function can be used to score generation of different length and abstention. To optimize this function, we introduce ReSearch, a process of "self-reflection" consisting of iterative self-prompting and self-evaluation. We use the ReSearch algorithm to generate synthetic data on which we finetune our models. Compared to their original versions, our resulting models generate fewer \emph{hallucinations} overall at no additional inference cost, for both known and unknown topics, as the model learns to selectively restrain itself. In addition, our method elegantly incorporates the ability to abstain by augmenting the samples generated by the model during the search procedure with an answer expressing abstention.

------------

`[2405.21028] LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models <https://arxiv.org/abs/2405.21028>`__ LACIE:基于听者感知的大型语言模型置信度校准微调

::

    replaced with revised version Wed, 3 Jul 2024 12:49:23 GMT
    Submission history From: Elias Stengel-Eskin [view email]
    [v1] Fri, 31 May 2024 17:16:38 UTC (650 KB)
    [v2] Wed, 3 Jul 2024 12:49:23 UTC (535 KB)
    Elias Stengel-Eskin, Peter Hase, Mohit Bansal

When answering questions, LLMs can convey not only an answer, but a level of confidence about the answer being correct. This includes explicit confidence markers (e.g. giving a numeric score) as well as implicit markers, like an authoritative tone or elaborating with additional knowledge. For LLMs to be trustworthy knowledge sources, the confidence they convey should match their actual expertise; however, most current models tend towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. We cast calibration as preference optimization, creating data via a two-agent game, where a speaker model's outputs are judged by a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the resulting models are better calibrated w.r.t. a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM's answers, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers. Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better confidence separation between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more and implicitly signals certainty when it is correct by using an authoritative tone or including details. Finally, LACIE finetuning leads to an emergent increase in model abstention (e.g. saying "I don't know") for answers that are likely wrong.

------------

`[2406.07016] Delving into ChatGPT usage in academic writing through excess vocabulary <https://arxiv.org/abs/2406.07016>`__ 通过词汇量挖掘学术写作中的ChatGPT用法

::

    replaced with revised version Wed, 3 Jul 2024 09:53:27 GMT
    Submission history From: Jan Lause [view email]
    [v1] Tue, 11 Jun 2024 07:16:34 UTC (416 KB)
    [v2] Wed, 3 Jul 2024 09:53:27 UTC (424 KB)
    Dmitry Kobak, Rita Gonz\'alez-M\'arquez, Em\H{o}ke-\'Agnes Horv\'at, Jan Lause

Recent large language models (LLMs) can generate and revise text with human-level performance, and have been widely commercialized in systems like ChatGPT. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists have been using them to assist their scholarly writing. How wide-spread is LLM usage in the academic literature currently? To answer this question, we use an unbiased, large-scale approach, free from any assumptions on academic LLM usage. We study vocabulary changes in 14 million PubMed abstracts from 2010-2024, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. Our analysis based on excess words usage suggests that at least 10% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, and was as high as 30% for some PubMed sub-corpora. We show that the appearance of LLM-based writing assistants has had an unprecedented impact in the scientific literature, surpassing the effect of major world events such as the Covid pandemic.

------------

`[2406.07212] Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems with Large Language Models <https://arxiv.org/abs/2406.07212>`__ 面向医疗保健中的人- ai协作:基于大型语言模型的指导延迟系统

::

    replaced with revised version Wed, 3 Jul 2024 14:49:15 GMT
    Submission history From: Joshua Strong [view email]
    [v1] Tue, 11 Jun 2024 12:41:54 UTC (3,015 KB)
    [v2] Wed, 3 Jul 2024 14:49:15 UTC (3,435 KB)
    Joshua Strong, Qianhui Men, Alison Noble

Large language models (LLMs) present a valuable technology for various applications in healthcare, but their tendency to hallucinate introduces unacceptable uncertainty in critical decision-making situations. Human-AI collaboration (HAIC) can mitigate this uncertainty by combining human and AI strengths for better outcomes. This paper presents a novel guided deferral system that provides intelligent guidance when AI defers cases to human decision-makers. We leverage LLMs' verbalisation capabilities and internal states to create this system, demonstrating that fine-tuning small-scale LLMs with data from large-scale LLMs greatly enhances performance while maintaining computational efficiency and data privacy. A pilot study showcases the effectiveness of our proposed deferral system.

------------

`[2406.12213] LLM-Oracle Machines <https://arxiv.org/abs/2406.12213>`__ LLM-Oracle机器

::

    replaced with revised version Wed, 3 Jul 2024 12:59:21 GMT
    Submission history From: Jie Wang [view email]
    [v1] Tue, 18 Jun 2024 02:25:33 UTC (5 KB)
    [v2] Tue, 2 Jul 2024 13:07:07 UTC (9 KB)
    [v3] Wed, 3 Jul 2024 12:59:21 UTC (9 KB)
    Jie Wang

Contemporary AI applications leverage large language models (LLMs) to harness their knowledge and reasoning abilities for natural language processing tasks. This approach shares similarities with the concept of oracle Turing machines (OTMs). To capture the broader potential of these computations, including those not yet realized, we propose an extension to OTMs: the LLM-oracle machine (LLM-OM), by employing a cluster of LLMs as the oracle. Each LLM acts as a black box, capable of answering queries within its expertise, albeit with a delay. We introduce four variants of the LLM-OM: basic, augmented, fault-avoidance, and $\epsilon$-fault. The first two are commonly observed in existing AI applications. The latter two are specifically designed to address the challenges of LLM hallucinations, biases, and inconsistencies, aiming to ensure reliable outcomes.

------------

`[2406.14322] Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning <https://arxiv.org/abs/2406.14322>`__ 保护私隐!用于语言模型微调的用户级差分隐私

::

    replaced with revised version Wed, 3 Jul 2024 14:05:20 GMT
    Submission history From: Yangsibo Huang [view email]
    [v1] Thu, 20 Jun 2024 13:54:32 UTC (1,263 KB)
    [v2] Wed, 3 Jul 2024 14:05:20 UTC (1,263 KB)
    Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Daogao Liu, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang

Large language models (LLMs) have emerged as powerful tools for tackling complex tasks across diverse domains, but they also raise privacy concerns when fine-tuned on sensitive data due to potential memorization. While differential privacy (DP) offers a promising solution by ensuring models are 'almost indistinguishable' with or without any particular privacy unit, current evaluations on LLMs mostly treat each example (text record) as the privacy unit. This leads to uneven user privacy guarantees when contributions per user vary. We therefore study user-level DP motivated by applications where it necessary to ensure uniform privacy protection across users. We present a systematic evaluation of user-level DP for LLM fine-tuning on natural language generation tasks. Focusing on two mechanisms for achieving user-level DP guarantees, Group Privacy and User-wise DP-SGD, we investigate design choices like data selection strategies and parameter tuning for the best privacy-utility tradeoff.

------------

`[2406.16008] Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization <https://arxiv.org/abs/2406.16008>`__ 中间发现:校准位置注意力偏差可提高长上下文利用率

::

    replaced with revised version Wed, 3 Jul 2024 17:40:00 GMT
    Submission history From: Cheng-Yu Hsieh [view email]
    [v1] Sun, 23 Jun 2024 04:35:42 UTC (1,762 KB)
    [v2] Wed, 3 Jul 2024 17:40:00 UTC (1,762 KB)
    Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long T. Le, Abhishek Kumar, James Glass, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister

Large language models (LLMs), even when specifically trained to process long input contexts, struggle to capture relevant information located in the middle of their input. This phenomenon has been known as the lost-in-the-middle problem. In this work, we make three contributions. First, we set out to understand the factors that cause this phenomenon. In doing so, we establish a connection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs exhibit a U-shaped attention bias where the tokens at the beginning and at the end of its input receive higher attention, regardless of their relevance. Second, we mitigate this positional bias through a calibration mechanism, found-in-the-middle, that allows the model to attend to contexts faithfully according to their relevance, even though when they are in the middle. Third, we show found-in-the-middle not only achieves better performance in locating relevant information within a long context, but also eventually leads to improved retrieval-augmented generation (RAG) performance across various tasks, outperforming existing methods by up to 15 percentage points. These findings open up future directions in understanding LLM attention bias and its potential consequences.

------------

`[2406.16203] LLMs' Classification Performance is Overclaimed <https://arxiv.org/abs/2406.16203>`__ LLMs的分类性能被夸大了

::

    replaced with revised version Wed, 3 Jul 2024 13:18:50 GMT
    Submission history From: Hanzi Xu [view email]
    [v1] Sun, 23 Jun 2024 19:49:10 UTC (8,492 KB)
    [v2] Sat, 29 Jun 2024 11:45:17 UTC (8,455 KB)
    [v3] Wed, 3 Jul 2024 13:18:50 UTC (8,455 KB)
    Hanzi Xu, Renze Lou, Jiangshu Du, Vahid Mahzoon, Elmira Talebianaraki, Zhuoan Zhou, Elizabeth Garrison, Slobodan Vucetic, Wenpeng Yin

In many classification tasks designed for AI or human to solve, gold labels are typically included within the label space by default, often posed as "which of the following is correct?" This standard setup has traditionally highlighted the strong performance of advanced AI, particularly top-performing Large Language Models (LLMs), in routine classification tasks. However, when the gold label is intentionally excluded from the label space, it becomes evident that LLMs still attempt to select from the available label candidates, even when none are correct. This raises a pivotal question: Do LLMs truly demonstrate their intelligence in understanding the essence of classification tasks?
In this study, we evaluate both closed-source and open-source LLMs across representative classification tasks, arguing that the perceived performance of LLMs is overstated due to their inability to exhibit the expected comprehension of the task. This paper makes a threefold contribution: i) To our knowledge, this is the first work to identify the limitations of LLMs in classification tasks when gold labels are absent. We define this task as Classify-w/o-Gold and propose it as a new testbed for LLMs. ii) We introduce a benchmark, Know-No, comprising two existing classification tasks and one new task, to evaluate Classify-w/o-Gold. iii) This work defines and advocates for a new evaluation metric, OmniAccuracy, which assesses LLMs' performance in classification tasks both when gold labels are present and absent.

------------

`[2406.17761] CaLMQA: Exploring culturally specific long-form question answering across 23 languages <https://arxiv.org/abs/2406.17761>`__ calqa:跨23种语言探索特定文化的长篇问答

::

    replaced with revised version Wed, 3 Jul 2024 16:33:55 GMT
    Submission history From: Shane Arora [view email]
    [v1] Tue, 25 Jun 2024 17:45:26 UTC (15,492 KB)
    [v2] Wed, 3 Jul 2024 16:33:55 UTC (16,358 KB)
    Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, Eunsol Choi

Large language models (LLMs) are used for long-form question answering (LFQA), which requires them to generate paragraph-length answers to complex questions. While LFQA has been well-studied in English, this research has not been extended to other languages. To bridge this gap, we introduce CaLMQA, a collection of 1.5K complex culturally specific questions spanning 23 languages and 51 culturally agnostic questions translated from English into 22 other languages. We define culturally specific questions as those uniquely or more likely to be asked by people from cultures associated with the question's language. We collect naturally-occurring questions from community web forums and hire native speakers to write questions to cover under-resourced, rarely-studied languages such as Fijian and Kirundi. Our dataset contains diverse, complex questions that reflect cultural topics (e.g. traditions, laws, news) and the language usage of native speakers. We automatically evaluate a suite of open- and closed-source models on CaLMQA by detecting incorrect language and token repetitions in answers, and observe that the quality of LLM-generated answers degrades significantly for some low-resource languages. Lastly, we perform human evaluation on a subset of models and languages. Manual evaluation reveals that model performance is significantly worse for culturally specific questions than for culturally agnostic questions. Our findings highlight the need for further research in non-English LFQA and provide an evaluation framework.

------------

`[2406.18045] PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry <https://arxiv.org/abs/2406.18045>`__ PharmaGPT:生物制药和化学领域特定大型语言模型

::

    replaced with revised version Wed, 3 Jul 2024 12:56:40 GMT
    Submission history From: Linqing Chen [view email]
    [v1] Wed, 26 Jun 2024 03:43:09 UTC (8,976 KB)
    [v2] Wed, 3 Jul 2024 12:56:40 UTC (8,976 KB)
    [v3] Tue, 9 Jul 2024 06:52:17 UTC (8,976 KB)
    Linqing Chen, Weilei Wang, Zilong Bai, Peng Xu, Yan Fang, Jie Fang, Wentao Wu, Lizhi Zhou, Ruiji Zhang, Yubin Xia, Chaobo Xu, Ran Hu, Licong Xu, Qijun Cai, Haoran Hua, Jing Sun, Jin Liu, Tian Qiu, Haowen Liu, Meng Hu, Xiuwen Li, Fei Gao, Yufu Wang, Lin Tie, Chaochao Wang, Jianping Lu, Cheng Sun, Yixin Wang, Shengjie Yang, Yuancheng Li, Lu Jin, Lisha Zhang, Fu Bian, Zhongkai Ye, Lidong Pei, Changyang Tu

Large language models (LLMs) have revolutionized Natural Language Processing (NLP) by minimizing the need for complex feature engineering. However, the application of LLMs in specialized domains like biopharmaceuticals and chemistry remains largely unexplored. These fields are characterized by intricate terminologies, specialized knowledge, and a high demand for precision areas where general purpose LLMs often fall short. In this study, we introduce PharmaGPT, a suite of domain specilized LLMs with 13 billion and 70 billion parameters, specifically trained on a comprehensive corpus tailored to the Bio-Pharmaceutical and Chemical domains. Our evaluation shows that PharmaGPT surpasses existing general models on specific-domain benchmarks such as NAPLEX, demonstrating its exceptional capability in domain-specific tasks. Remarkably, this performance is achieved with a model that has only a fraction, sometimes just one-tenth-of the parameters of general-purpose large models. This advancement establishes a new benchmark for LLMs in the bio-pharmaceutical and chemical fields, addressing the existing gap in specialized language modeling. It also suggests a promising path for enhanced research and development, paving the way for more precise and effective NLP applications in these areas.

------------

`[2402.13210] Bayesian Reward Models for LLM Alignment <https://arxiv.org/abs/2402.13210>`__ 用于LLM对齐的贝叶斯奖励模型

::

    replaced with revised version Wed, 3 Jul 2024 00:23:41 GMT
    Submission history From: Adam Yang [view email]
    [v1] Tue, 20 Feb 2024 18:20:59 UTC (79 KB)
    [v2] Wed, 3 Jul 2024 00:23:41 UTC (1,943 KB)
    Adam X. Yang, Maxime Robeyns, Thomas Coste, Zhengyan Shi, Jun Wang, Haitham Bou-Ammar, Laurence Aitchison

To ensure that large language model (LLM) responses are helpful and non-toxic, a reward model trained on human preference data is usually used. LLM responses with high rewards are then selected through best-of-$n$ (BoN) sampling or the LLM is further optimized to produce responses with high rewards through reinforcement learning from human feedback (RLHF). However, these processes are susceptible to reward overoptimization or `hacking', where responses receive high rewards due to imperfections in the reward model rather than true preference, particularly as prompts or responses deviate from the training data. To address these challenges, we propose to train a Bayesian reward model, which signals higher uncertainty further from the training data distribution. We trained Bayesian reward models using Laplace approximation on LoRA weights, and found that the resulting uncertainty estimates can effectively mitigate reward overoptimization in BoN sampling.

------------

`[2402.13516] ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models <https://arxiv.org/abs/2402.13516>`__ 普洛斯:引入和增强大型语言模型的内在激活稀疏性

::

    replaced with revised version Wed, 3 Jul 2024 05:56:49 GMT
    Submission history From: Chenyang Song [view email]
    [v1] Wed, 21 Feb 2024 03:58:49 UTC (228 KB)
    [v2] Tue, 27 Feb 2024 07:27:07 UTC (229 KB)
    [v3] Mon, 27 May 2024 15:49:58 UTC (195 KB)
    [v4] Wed, 3 Jul 2024 05:56:49 UTC (207 KB)
    Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun

Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, activation sparsity has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces a simple and effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity while maintaining comparable performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization with a factor smoothly increasing along the multi-stage sine curves. This can enhance activation sparsity and mitigate performance degradation by avoiding radical shifts in activation distributions. With ProSparse, we obtain high sparsity of 89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size MiniCPM-1B, respectively, achieving comparable performance to their original Swish-activated versions. These present the most sparsely activated models among open-source LLaMA versions and competitive end-size models, considerably surpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference acceleration experiments further demonstrate the significant practical acceleration potential of LLMs with higher activation sparsity, obtaining up to 4.52$\times$ inference speedup.

------------

`[2405.15512] ChatGPT Code Detection: Techniques for Uncovering the Source of Code <https://arxiv.org/abs/2405.15512>`__ ChatGPT代码检测:用于发现代码源代码的技术

::

    replaced with revised version Wed, 3 Jul 2024 10:23:01 GMT
    Submission history From: Marc Oedingen [view email]
    [v1] Fri, 24 May 2024 12:56:18 UTC (2,759 KB)
    [v2] Wed, 3 Jul 2024 10:23:01 UTC (2,813 KB)
    Marc Oedingen, Raphael C. Engelhardt, Robin Denz, Maximilian Hammer, Wolfgang Konen

In recent times, large language models (LLMs) have made significant strides in generating computer code, blurring the lines between code created by humans and code produced by artificial intelligence (AI). As these technologies evolve rapidly, it is crucial to explore how they influence code generation, especially given the risk of misuse in areas like higher education. This paper explores this issue by using advanced classification techniques to differentiate between code written by humans and that generated by ChatGPT, a type of LLM. We employ a new approach that combines powerful embedding features (black-box) with supervised learning algorithms - including Deep Neural Networks, Random Forests, and Extreme Gradient Boosting - to achieve this differentiation with an impressive accuracy of 98%. For the successful combinations, we also examine their model calibration, showing that some of the models are extremely well calibrated. Additionally, we present white-box features and an interpretable Bayes classifier to elucidate critical differences between the code sources, enhancing the explainability and transparency of our approach. Both approaches work well but provide at most 85-88% accuracy. We also show that untrained humans solve the same task not better than random guessing. This study is crucial in understanding and mitigating the potential risks associated with using AI in code generation, particularly in the context of higher education, software development, and competitive programming.

------------

`[2407.01910] MG-Verilog: Multi-grained Dataset Towards Enhanced LLM-assisted Verilog Generation <https://arxiv.org/abs/2407.01910>`__ MG-Verilog:面向增强llm辅助Verilog生成的多粒度数据集

::

    replaced with revised version Wed, 3 Jul 2024 15:15:20 GMT
    Submission history From: Yongan Zhang [view email]
    [v1] Tue, 2 Jul 2024 03:21:24 UTC (9,247 KB)
    [v2] Wed, 3 Jul 2024 15:15:20 UTC (9,247 KB)
    Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, Yingyan Celine Lin

Large Language Models (LLMs) have recently shown promise in streamlining hardware design processes by encapsulating vast amounts of domain-specific data. In addition, they allow users to interact with the design processes through natural language instructions, thus making hardware design more accessible to developers. However, effectively leveraging LLMs in hardware design necessitates providing domain-specific data during inference (e.g., through in-context learning), fine-tuning, or pre-training. Unfortunately, existing publicly available hardware datasets are often limited in size, complexity, or detail, which hinders the effectiveness of LLMs in hardware design tasks. To address this issue, we first propose a set of criteria for creating high-quality hardware datasets that can effectively enhance LLM-assisted hardware design. Based on these criteria, we propose a Multi-Grained-Verilog (MG-Verilog) dataset, which encompasses descriptions at various levels of detail and corresponding code samples. To benefit the broader hardware design community, we have developed an open-source infrastructure that facilitates easy access, integration, and extension of the dataset to meet specific project needs. Furthermore, to fully exploit the potential of the MG-Verilog dataset, which varies in complexity and detail, we introduce a balanced fine-tuning scheme. This scheme serves as a unique use case to leverage the diverse levels of detail provided by the dataset. Extensive experiments demonstrate that the proposed dataset and fine-tuning scheme consistently improve the performance of LLMs in hardware design tasks.

------------

`[2404.12390] BLINK: Multimodal Large Language Models Can See but Not Perceive <https://arxiv.org/abs/2404.12390>`__ BLINK:多模态大型语言模型可以看到但不能感知

::

    replaced with revised version Wed, 3 Jul 2024 08:44:45 GMT
    Submission history From: Xingyu Fu [view email]
    [v1] Thu, 18 Apr 2024 17:59:54 UTC (31,815 KB)
    [v2] Thu, 25 Apr 2024 01:55:49 UTC (31,815 KB)
    [v3] Sat, 4 May 2024 05:25:26 UTC (31,780 KB)
    [v4] Wed, 3 Jul 2024 08:44:45 UTC (31,781 KB)
    Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, Ranjay Krishna

We introduce Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations. Most of the Blink tasks can be solved by humans "within a blink" (e.g., relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning). However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language. Blink reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting. While humans get 95.70% accuracy on average, Blink is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only 13.17% and 7.63% higher than random guessing, indicating that such perception abilities have not "emerged" yet in recent multimodal LLMs. Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements. We believe Blink will stimulate the community to help multimodal LLMs catch up with human-level visual perception.

------------

`[2406.11925] DocCGen: Document-based Controlled Code Generation <https://arxiv.org/abs/2406.11925>`__ docgen:基于文档的受控代码生成

::

    replaced with revised version Wed, 3 Jul 2024 09:16:27 GMT
    Submission history From: Sameer Pimparkhede [view email]
    [v1] Mon, 17 Jun 2024 08:34:57 UTC (9,291 KB)
    [v2] Wed, 3 Jul 2024 09:16:27 UTC (9,262 KB)
    Sameer Pimparkhede, Mehant Kammakomati, Srikanth Tamilselvam, Prince Kumar, Ashok Pon Kumar, Pushpak Bhattacharyya

Recent developments show that Large Language Models (LLMs) produce state-of-the-art performance on natural language (NL) to code generation for resource-rich general-purpose languages like C++, Java, and Python. However, their practical usage for structured domain-specific languages (DSLs) such as YAML, JSON is limited due to domain-specific schema, grammar, and customizations generally unseen by LLMs during pre-training. Efforts have been made to mitigate this challenge via in-context learning through relevant examples or by fine-tuning. However, it suffers from problems, such as limited DSL samples and prompt sensitivity but enterprises maintain good documentation of the DSLs. Therefore, we propose DocCGen, a framework that can leverage such rich knowledge by breaking the NL-to-Code generation task for structured code languages into a two-step process. First, it detects the correct libraries using the library documentation that best matches the NL query. Then, it utilizes schema rules extracted from the documentation of these libraries to constrain the decoding. We evaluate our framework for two complex structured languages, Ansible YAML and Bash command, consisting of two settings: Out-of-domain (OOD) and In-domain (ID). Our extensive experiments show that DocCGen consistently improves different-sized language models across all six evaluation metrics, reducing syntactic and semantic errors in structured code. We plan to open-source the datasets and code to motivate research in constrained code generation.

------------

`[2407.01851] Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time <https://arxiv.org/abs/2407.01851>`__ Meerkat:基于空间和时间的视听大型语言模型

::

    replaced with revised version Wed, 3 Jul 2024 07:01:30 GMT
    Submission history From: Sanjoy Chowdhury [view email]
    [v1] Mon, 1 Jul 2024 23:32:25 UTC (13,143 KB)
    [v2] Wed, 3 Jul 2024 07:01:30 UTC (13,141 KB)
    Sanjoy Chowdhury, Sayan Nag, Subhrajyoti Dasgupta, Jun Chen, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha

Leveraging Large Language Models' remarkable proficiency in text-based tasks, recent works on Multi-modal LLMs (MLLMs) extend them to other modalities like vision and audio. However, the progress in these directions has been mostly focused on tasks that only require a coarse-grained understanding of the audio-visual semantics. We present Meerkat, an audio-visual LLM equipped with a fine-grained understanding of image and audio both spatially and temporally. With a new modality alignment module based on optimal transport and a cross-attention module that enforces audio-visual consistency, Meerkat can tackle challenging tasks such as audio referred image grounding, image guided audio temporal localization, and audio-visual fact-checking. Moreover, we carefully curate a large dataset AVFIT that comprises 3M instruction tuning samples collected from open-source datasets, and introduce MeerkatBench that unifies five challenging audio-visual tasks. We achieve state-of-the-art performance on all these downstream tasks with a relative improvement of up to 37.12%.

------------

`[2405.15232] DEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception <https://arxiv.org/abs/2405.15232>`__ DEEM:扩散模型是大型语言模型的眼睛，用于图像感知

::

    replaced with revised version Wed, 3 Jul 2024 17:30:53 GMT
    Submission history From: Run Luo [view email]
    [v1] Fri, 24 May 2024 05:46:04 UTC (5,724 KB)
    [v2] Wed, 3 Jul 2024 17:30:53 UTC (5,717 KB)
    Run Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, Min Yang, Binyuan Hui

The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple and effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like ViT, thereby enhancing the model's resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and another well-known benchmark, POPE, for object hallucination. Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10%), and a smaller base model size.

------------

----------
Index (82)
----------

`[2407.02552] RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs <https://arxiv.org/abs/2407.02552>`__ RLHF可以使用多种语言:为llm解锁多语言偏好优化

`[2407.02637] Change My Frame: Reframing in the Wild in r/ChangeMyView <https://arxiv.org/abs/2407.02637>`__ 改变我的框架:在r/ChangeMyView中进行野外重建

`[2407.02659] Ensuring Responsible Sourcing of Large Language Model Training Data Through Knowledge Graph Comparison <https://arxiv.org/abs/2407.02659>`__

`[2407.02750] Learning to Reduce: Towards Improving Performance of Large Language Models on Structured Data <https://arxiv.org/abs/2407.02750>`__ 学习归约:提高大型语言模型在结构化数据上的性能

`[2407.02783] 52B to 1T: Lessons Learned via Tele-FLM Series <https://arxiv.org/abs/2407.02783>`__ 52B到1T:通过Tele-FLM系列学到的经验

`[2407.02964] FSM: A Finite State Machine Based Zero-Shot Prompting Paradigm for Multi-Hop Question Answering <https://arxiv.org/abs/2407.02964>`__ FSM:一种基于有限状态机的零样本多跳问答模型

`[2407.02977] Large Language Models as Evaluators for Scientific Synthesis <https://arxiv.org/abs/2407.02977>`__ 作为科学综合评估器的大型语言模型

`[2407.02978] Mast Kalandar at SemEval-2024 Task 8: On the Trail of Textual Origins: RoBERTa-BiLSTM Approach to Detect AI-Generated Text <https://arxiv.org/abs/2407.02978>`__

`[2407.02996] Are Large Language Models Consistent over Value-laden Questions? <https://arxiv.org/abs/2407.02996>`__ 大型语言模型对承载价值的问题是否一致?

`[2407.03004] SemioLLM: Assessing Large Language Models for Semiological Analysis in Epilepsy Research <https://arxiv.org/abs/2407.03004>`__ SemioLLM:用于癫痫研究符号学分析的大型语言模型评估

`[2407.03038] On the Client Preference of LLM Fine-tuning in Federated Learning <https://arxiv.org/abs/2407.03038>`__ 联邦学习中LLM微调的客户端偏好研究

`[2407.03040] Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model <https://arxiv.org/abs/2407.03040>`__ 原始文本就是你所需要的:大型语言模型的知识密集型多轮指令调优

`[2407.03051] Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment <https://arxiv.org/abs/2407.03051>`__ 通过直接偏好对齐提高量化大型语言模型的对话能力

`[2407.03103] Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory <https://arxiv.org/abs/2407.03103>`__ Cactus:基于认知行为理论的心理咨询对话

`[2407.03129] Social Bias Evaluation for Large Language Models Requires Prompt Variations <https://arxiv.org/abs/2407.03129>`__ 大型语言模型的社会偏见评估需要及时变化

`[2407.03145] Enhancing Translation Accuracy of Large Language Models through Continual Pre-Training on Parallel Data <https://arxiv.org/abs/2407.03145>`__

`[2407.03157] Let the Code LLM Edit Itself When You Edit the Code <https://arxiv.org/abs/2407.03157>`__ 当你编辑代码时，让代码LLM编辑自己

`[2407.03169] Investigating Decoder-only Large Language Models for Speech-to-text Translation <https://arxiv.org/abs/2407.03169>`__ 面向语音到文本翻译的纯解码器大型语言模型研究

`[2407.03211] How Does Quantization Affect Multilingual LLMs? <https://arxiv.org/abs/2407.03211>`__ 量化如何影响多语言llm ?

`[2407.03282] LLM Internal States Reveal Hallucination Risk Faced With a Query <https://arxiv.org/abs/2407.03282>`__ LLM内部状态揭示了面临查询时的幻觉风险

`[2407.02694] LLM-Select: Feature Selection with Large Language Models <https://arxiv.org/abs/2407.02694>`__ LLM-Select:大型语言模型的特征选择

`[2407.02770] Large language models, physics-based modeling, experimental measurements: the trinity of data-scarce learning of polymer properties <https://arxiv.org/abs/2407.02770>`__ 大型语言模型、基于物理的建模、实验测量:数据稀缺的聚合物特性学习的三位一体

`[2407.02891] GPTQT: Quantize Large Language Models Twice to Push the Efficiency <https://arxiv.org/abs/2407.02891>`__ GPTQT:两次量化大型语言模型以提升效率

`[2407.03232] Single Character Perturbations Break LLM Alignment <https://arxiv.org/abs/2407.03232>`__ 单个字符的扰动破坏了LLM对齐

`[2407.03234] Self-Evaluation as a Defense Against Adversarial Attacks on LLMs <https://arxiv.org/abs/2407.03234>`__ 作为对llm对抗攻击防御的自评估

`[2407.03310] Universal Length Generalization with Turing Programs <https://arxiv.org/abs/2407.03310>`__ 图灵程序的通用长度泛化

`[2407.02511] LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning <https://arxiv.org/abs/2407.02511>`__ LLM-A*:大型语言模型增强的路径规划增量启发式搜索

`[2407.02514] LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations <https://arxiv.org/abs/2407.02514>`__ LOGIC-LM++:符号公式的多步精化

`[2407.02518] INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness <https://arxiv.org/abs/2407.02518>`__ INDICT:具有内部评论对话的代码生成，以兼顾安全性和有用性

`[2407.02524] Meta Large Language Model Compiler: Foundation Models of Compiler Optimization <https://arxiv.org/abs/2407.02524>`__ 元大型语言模型编译器:编译优化的基础模型

`[2407.02528] Actionable Cyber Threat Intelligence using Knowledge Graphs and Large Language Models <https://arxiv.org/abs/2407.02528>`__ 基于知识图谱和大型语言模型的可行动网络威胁情报

`[2407.02551] A False Sense of Safety: Unsafe Information Leakage in 'Safe' AI Responses <https://arxiv.org/abs/2407.02551>`__

`[2407.02651] Improving Steering and Verification in AI-Assisted Data Analysis with Interactive Task Decomposition <https://arxiv.org/abs/2407.02651>`__ 交互式任务分解改进人工智能辅助数据分析中的导向和验证

`[2407.02791] Model-Enhanced LLM-Driven VUI Testing of VPA Apps <https://arxiv.org/abs/2407.02791>`__ 模型增强的llm驱动的VPA应用程序VUI测试

`[2407.02943] PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding <https://arxiv.org/abs/2407.02943>`__ pi - compass:通过接地引导LLM训练数据提取提示指向目标PII

`[2407.02956] IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization <https://arxiv.org/abs/2407.02956>`__ IncogniText:基于llm隐私属性随机化的隐私增强条件文本匿名

`[2407.02960] ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets <https://arxiv.org/abs/2407.02960>`__ ObfuscaTune:私有llm在私有数据集上的模糊异地微调和推理

`[2407.02994] MedPix 2.0: A Comprehensive Multimodal Biomedical Dataset for Advanced AI Applications <https://arxiv.org/abs/2407.02994>`__ MedPix 2.0:面向高级人工智能应用的全面多模态生物医学数据集

`[2407.03203] TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts <https://arxiv.org/abs/2407.03203>`__ 定理llama:将通用llm转换为Lean4专家

`[2407.02662] Supporters and Skeptics: LLM-based Analysis of Engagement with Mental Health (Mis)Information Content on Video-sharing Platforms <https://arxiv.org/abs/2407.02662>`__

`[2407.02833] LANE: Logic Alignment of Non-tuning Large Language Models and Online Recommendation Systems for Explainable Reason Generation <https://arxiv.org/abs/2407.02833>`__ LANE:面向可解释原因生成的非调优大型语言模型与在线推荐系统的逻辑对齐

`[2407.02855] Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks <https://arxiv.org/abs/2407.02855>`__ 安全遗忘(Safe Unlearning):一种防御越狱攻击的有效且通用的解决方案

`[2407.02885] CogErgLLM: Exploring Large Language Model Systems Design Perspective Using Cognitive Ergonomics <https://arxiv.org/abs/2407.02885>`__ CogErgLLM:用认知人机工程学探索大型语言模型系统设计视角

`[2407.03045] JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets <https://arxiv.org/abs/2407.03045>`__ JailbreakHunter:一种针对越狱的可视分析方法，可以从大规模人类llm对话数据集中发现漏洞

`[2407.03104] KeyVideoLLM: Towards Large-scale Video Keyframe Selection <https://arxiv.org/abs/2407.03104>`__ KeyVideoLLM:面向大规模视频关键帧选择的研究

`[2407.03160] SOS! Soft Prompt Attack Against Open-Source Large Language Models <https://arxiv.org/abs/2407.03160>`__ SOS !针对开源大型语言模型的软提示攻击

`[2407.03320] InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output <https://arxiv.org/abs/2407.03320>`__ InternLM-XComposer-2.5:支持长上下文输入和输出的通用大型视觉语言模型

`[2401.10545] Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency <https://arxiv.org/abs/2401.10545>`__ 理解基于聊天gpt推荐系统中的偏差:提供者公平性、时间稳定性和时效性

`[2308.00081] Towards Semantically Enriched Embeddings for Knowledge Graph Completion <https://arxiv.org/abs/2308.00081>`__ 面向知识图谱补全的语义丰富嵌入

`[2305.15060] Who Wrote this Code? Watermarking for Code Generation <https://arxiv.org/abs/2305.15060>`__

`[2307.02762] PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations <https://arxiv.org/abs/2307.02762>`__ PRD: Peer Rank and Discussion改进基于评价的大型语言模型

`[2311.05661] Prompt Engineering a Prompt Engineer <https://arxiv.org/abs/2311.05661>`__ 提示工程师提示工程师

`[2311.09325] Temperature-scaling surprisal estimates improve fit to human reading times -- but does it do so for the "right reasons"? <https://arxiv.org/abs/2311.09325>`__ 温度尺度的惊讶估计提高了与人类阅读时间的适应性——但它这样做的原因是“正确的”吗?

`[2312.12736] Learning and Forgetting Unsafe Examples in Large Language Models <https://arxiv.org/abs/2312.12736>`__ 大型语言模型中不安全示例的学习和遗忘

`[2312.15997] Aligning Large Language Models with Human Preferences through Representation Engineering <https://arxiv.org/abs/2312.15997>`__ 通过表示工程使大型语言模型与人类偏好相一致

`[2401.14267] Transformers and Cortical Waves: Encoders for Pulling In Context Across Time <https://arxiv.org/abs/2401.14267>`__ transformer和皮层波:跨时间提取上下文的编码器

`[2402.03898] DistiLLM: Towards Streamlined Distillation for Large Language Models <https://arxiv.org/abs/2402.03898>`__ DistiLLM:面向大型语言模型的流线型蒸馏

`[2402.10671] Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm <https://arxiv.org/abs/2402.10671>`__ 增强注意力的分解:通过工作流范式改进基于llm的Text-to-SQL

`[2402.13228] Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive <https://arxiv.org/abs/2402.13228>`__ Smaug:修复DPO-Positive偏好优化的失败模式

`[2402.16029] GraphWiz: An Instruction-Following Language Model for Graph Problems <https://arxiv.org/abs/2402.16029>`__ GraphWiz:面向图问题的指令遵循语言模型

`[2403.07794] Fine-tuning Large Language Models with Sequential Instructions <https://arxiv.org/abs/2403.07794>`__ 基于顺序指令的大型语言模型微调

`[2403.19887] Jamba: A Hybrid Transformer-Mamba Language Model <https://arxiv.org/abs/2403.19887>`__ Jamba: Transformer-Mamba混合语言模型

`[2404.05880] Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge <https://arxiv.org/abs/2404.05880>`__ Eraser:基于有害知识遗忘的大型语言模型越狱防御

`[2405.00557] Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment <https://arxiv.org/abs/2405.00557>`__ 有洞察力的专家混合(MoTE):思维链和专家混合在自我对齐中的协同作用

`[2405.13022] LLMs can learn self-restraint through iterative self-reflection <https://arxiv.org/abs/2405.13022>`__

`[2405.21028] LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models <https://arxiv.org/abs/2405.21028>`__ LACIE:基于听者感知的大型语言模型置信度校准微调

`[2406.07016] Delving into ChatGPT usage in academic writing through excess vocabulary <https://arxiv.org/abs/2406.07016>`__ 通过词汇量挖掘学术写作中的ChatGPT用法

`[2406.07212] Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems with Large Language Models <https://arxiv.org/abs/2406.07212>`__ 面向医疗保健中的人- ai协作:基于大型语言模型的指导延迟系统

`[2406.12213] LLM-Oracle Machines <https://arxiv.org/abs/2406.12213>`__ LLM-Oracle机器

`[2406.14322] Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning <https://arxiv.org/abs/2406.14322>`__ 保护私隐!用于语言模型微调的用户级差分隐私

`[2406.16008] Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization <https://arxiv.org/abs/2406.16008>`__ 中间发现:校准位置注意力偏差可提高长上下文利用率

`[2406.16203] LLMs' Classification Performance is Overclaimed <https://arxiv.org/abs/2406.16203>`__ LLMs的分类性能被夸大了

`[2406.17761] CaLMQA: Exploring culturally specific long-form question answering across 23 languages <https://arxiv.org/abs/2406.17761>`__ calqa:跨23种语言探索特定文化的长篇问答

`[2406.18045] PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry <https://arxiv.org/abs/2406.18045>`__ PharmaGPT:生物制药和化学领域特定大型语言模型

`[2402.13210] Bayesian Reward Models for LLM Alignment <https://arxiv.org/abs/2402.13210>`__ 用于LLM对齐的贝叶斯奖励模型

`[2402.13516] ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models <https://arxiv.org/abs/2402.13516>`__ 普洛斯:引入和增强大型语言模型的内在激活稀疏性

`[2405.15512] ChatGPT Code Detection: Techniques for Uncovering the Source of Code <https://arxiv.org/abs/2405.15512>`__ ChatGPT代码检测:用于发现代码源代码的技术

`[2407.01910] MG-Verilog: Multi-grained Dataset Towards Enhanced LLM-assisted Verilog Generation <https://arxiv.org/abs/2407.01910>`__ MG-Verilog:面向增强llm辅助Verilog生成的多粒度数据集

`[2404.12390] BLINK: Multimodal Large Language Models Can See but Not Perceive <https://arxiv.org/abs/2404.12390>`__ BLINK:多模态大型语言模型可以看到但不能感知

`[2406.11925] DocCGen: Document-based Controlled Code Generation <https://arxiv.org/abs/2406.11925>`__ docgen:基于文档的受控代码生成

`[2407.01851] Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time <https://arxiv.org/abs/2407.01851>`__ Meerkat:基于空间和时间的视听大型语言模型

`[2405.15232] DEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception <https://arxiv.org/abs/2405.15232>`__ DEEM:扩散模型是大型语言模型的眼睛，用于图像感知

