240708
========

----------
Survey (4)
----------

`[2407.03993] A Survey on Natural Language Counterfactual Generation <https://arxiv.org/abs/2407.03993>`__ 自然语言反事实生成综述

::

    Thu, 4 Jul 2024 15:13:59 GMT
    Yongjie Wang, Xiaoqi Qiu, Yu Yue, Xu Guo, Zhiwei Zeng, Yuhong Feng, Zhiqi Shen

Natural Language Counterfactual generation aims to minimally modify a given text such that the modified text will be classified into a different class. The generated counterfactuals provide insight into the reasoning behind a model's predictions by highlighting which words significantly influence the outcomes.
Additionally, they can be used to detect model fairness issues or augment the training data to enhance the model's robustness. A substantial amount of research has been conducted to generate counterfactuals for various NLP tasks, employing different models and methodologies. With the rapid growth of studies in this field, a systematic review is crucial to guide future researchers and developers. To bridge this gap, this survey comprehensively overview textual counterfactual generation methods, particularly including those based on Large Language Models. We propose a new taxonomy that categorizes the generation methods into four groups and systematically summarize the metrics for evaluating the generation quality. Finally, we discuss ongoing research challenges and outline promising directions for future work.

------------

`[2407.04069] A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations <https://arxiv.org/abs/2407.04069>`__ 大型语言模型评估的系统综述:挑战、限制和建议

::

    Thu, 4 Jul 2024 17:15:37 GMT
    Md Tahmid Rahman Laskar, Sawsan Alqahtani, M Saiful Bari, Mizanur Rahman, Mohammad Abdullah Matin Khan, Haidar Khan, Israt Jahan, Amran Bhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty, Jimmy Huang

Large Language Models (LLMs) have recently gained significant attention due to their remarkable capabilities in performing diverse tasks across various domains. However, a thorough evaluation of these models is crucial before deploying them in real-world applications to ensure they produce reliable performance. Despite the well-established importance of evaluating LLMs in the community, the complexity of the evaluation process has led to varied evaluation setups, causing inconsistencies in findings and interpretations. To address this, we systematically review the primary challenges and limitations causing these inconsistencies and unreliable evaluations in various steps of LLM evaluation. Based on our critical review, we present our perspectives and recommendations to ensure LLM evaluations are reproducible, reliable, and robust.

------------

`[2407.04295] Jailbreak Attacks and Defenses Against Large Language Models: A Survey <https://arxiv.org/abs/2407.04295>`__ 大型语言模型的越狱攻击与防御综述

::

    Fri, 5 Jul 2024 06:57:30 GMT
    Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, Qi Li

Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of "jailbreaking", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses.
Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.

------------

`[2401.07187] A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models <https://arxiv.org/abs/2401.07187>`__ 深度学习统计理论综述:近似、训练动力学和生成模型

::

    replaced with revised version Thu, 4 Jul 2024 04:36:06 GMT
    Submission history From: Namjoon Suh [view email]
    [v1] Sun, 14 Jan 2024 02:30:19 UTC (70 KB)
    [v2] Thu, 4 Jul 2024 04:36:06 UTC (119 KB)
    Namjoon Suh and Guang Cheng

In this article, we review the literature on statistical theories of neural networks from three perspectives. In the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression or classification. These results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks, in that tools from the approximation theory are adopted. Through these constructions, the width and depth of the networks can be expressed in terms of sample size, data dimension, and function smoothness. Nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. This motivates us to review the training dynamics of neural networks in the second part. Specifically, we review papers that attempt to answer ``how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.'' In particular, two well-known paradigms are reviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF) paradigm. In the last part, we review the most recent theoretical advancements in generative models including Generative Adversarial Networks (GANs), diffusion models, and in-context learning (ICL) in the Large Language Models (LLMs). The former two models are known to be the main pillars of the modern generative AI era, while ICL is a strong capability of LLMs in learning from a few examples in the context. Finally, we conclude the paper by suggesting several promising directions for deep learning theory.

------------

-------------
Benchmark (8)
-------------

`[2407.03791] M$\mathbf5$ -- A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks <https://arxiv.org/abs/2407.03791>`__ M$\mathbf5$——一个多样化的基准，用于评估跨多语言和多文化视觉语言任务的大型多模态模型的性能

::

    Thu, 4 Jul 2024 09:55:04 GMT
    Florian Schneider and Sunayana Sitaram

Since the release of ChatGPT, the field of Natural Language Processing has experienced rapid advancements, particularly in Large Language Models (LLMs) and their multimodal counterparts, Large Multimodal Models (LMMs). Despite their impressive capabilities, LLMs often exhibit significant performance disparities across different languages and cultural contexts, as demonstrated by various text-only benchmarks. However, current research lacks such benchmarks for multimodal visio-linguistic settings. This work fills this gap by introducing M5, the first comprehensive benchmark designed to evaluate LMMs on diverse vision-language tasks within a multilingual and multicultural context. M5 includes eight datasets covering five tasks and $41$ languages, with a focus on underrepresented languages and culturally diverse images.
Furthermore, we introduce two novel datasets, M5-VGR and M5-VLOD, including a new Visio-Linguistic Outlier Detection task, in which all evaluated open-source models fail to significantly surpass the random baseline. Through extensive evaluation and analyses, we highlight substantial task-agnostic performance disparities between high- and low-resource languages. Moreover, we show that larger models do not necessarily outperform smaller ones in a multilingual setting.

------------

`[2407.03841] On the Benchmarking of LLMs for Open-Domain Dialogue Evaluation <https://arxiv.org/abs/2407.03841>`__ llm开放域对话评估基准

::

    Thu, 4 Jul 2024 11:14:47 GMT
    John Mendon\c{c}a and Alon Lavie and Isabel Trancoso

Large Language Models (LLMs) have showcased remarkable capabilities in various Natural Language Processing tasks. For automatic open-domain dialogue evaluation in particular, LLMs have been seamlessly integrated into evaluation frameworks, and together with human evaluation, compose the backbone of most evaluations. However, existing evaluation benchmarks often rely on outdated datasets and evaluate aspects like Fluency and Relevance, which fail to adequately capture the capabilities and limitations of state-of-the-art chatbot models.
This paper critically examines current evaluation benchmarks, highlighting that the use of older response generators and quality aspects fail to accurately reflect modern chatbot capabilities. A small annotation experiment on a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as GPT-4 struggle to detect actual deficiencies in dialogues generated by current LLM chatbots.

------------

`[2407.03978] Benchmarking Complex Instruction-Following with Multiple Constraints Composition <https://arxiv.org/abs/2407.03978>`__ 基于多约束组合的复杂指令遵循基准测试

::

    Thu, 4 Jul 2024 14:50:45 GMT
    Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxin Xu, Yiming Liu, Jie Tang, Hongning Wang, Minlie Huang

Instruction following is one of the fundamental capabilities of large language models (LLMs). As the ability of LLMs is constantly improving, they have been increasingly applied to deal with complex human instructions in real-world scenarios. Therefore, how to evaluate the ability of complex instruction-following of LLMs has become a critical research problem. Existing benchmarks mainly focus on modeling different types of constraints in human instructions while neglecting the composition of different constraints, which is an indispensable constituent in complex instructions. To this end, we propose ComplexBench, a benchmark for comprehensively evaluating the ability of LLMs to follow complex instructions composed of multiple constraints. We propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types, and manually collect a high-quality dataset accordingly. To make the evaluation reliable, we augment LLM-based evaluators with rules to effectively verify whether generated texts can satisfy each constraint and composition.
Furthermore, we obtain the final evaluation score based on the dependency structure determined by different composition types. ComplexBench identifies significant deficiencies in existing LLMs when dealing with complex instructions with multiple constraints composition.

------------

`[2401.03855] PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs <https://arxiv.org/abs/2401.03855>`__ 

::

    replaced with revised version Thu, 4 Jul 2024 05:40:42 GMT
    Submission history From: Ankit Yadav [view email]
    [v1] Mon, 8 Jan 2024 12:36:43 UTC (9,603 KB)
    [v2] Fri, 23 Feb 2024 04:29:06 UTC (11,277 KB)
    [v3] Fri, 26 Apr 2024 04:53:51 UTC (11,277 KB)
    [v4] Thu, 4 Jul 2024 05:40:42 UTC (11,943 KB)
    Ankit Yadav, Himanshu Beniwal, Mayank Singh

Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of HumanEval and MBPP, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks, potentially inflating model performance estimations. To address these limitations, we propose a novel benchmark, PythonSaga, featuring 185 hand-crafted prompts on a balanced representation of 38 programming concepts across diverse difficulty levels. The robustness of our benchmark is demonstrated by the poor performance of existing Code-LLMs.

------------

`[2403.06412] CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean <https://arxiv.org/abs/2403.06412>`__ CLIcK:韩语文化和语言智能基准数据集

::

    replaced with revised version Thu, 4 Jul 2024 13:08:19 GMT
    Submission history From: Eunsu Kim [view email]
    [v1] Mon, 11 Mar 2024 03:54:33 UTC (8,945 KB)
    [v2] Tue, 12 Mar 2024 10:33:06 UTC (8,186 KB)
    [v3] Fri, 15 Mar 2024 08:53:31 UTC (8,186 KB)
    [v4] Thu, 4 Jul 2024 13:08:19 UTC (8,186 KB)
    Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, Alice Oh

Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as bias and hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in CLIcK, we provide fine-grained annotation of which cultural and linguistic knowledge is required to answer the question correctly. Using CLIcK, we test 13 language models to assess their performance. Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension. CLIcK offers the first large-scale comprehensive Korean-centric analysis of LLMs' proficiency in Korean culture and language.

------------

`[2404.13627] NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding <https://arxiv.org/abs/2404.13627>`__ Negotiation tom:谈判环境下压力测试机器心理理论的基准

::

    replaced with revised version Thu, 4 Jul 2024 14:14:38 GMT
    Submission history From: Chunkit Chan [view email]
    [v1] Sun, 21 Apr 2024 11:51:13 UTC (8,594 KB)
    [v2] Thu, 4 Jul 2024 14:14:38 UTC (8,595 KB)
    Chunkit Chan, Cheng Jiayang, Yauwai Yim, Zheye Deng, Wei Fan, Haoran Li, Xin Liu, Hongming Zhang, Weiqi Wang, Yangqiu Song

Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability. Theory of mind evaluations currently focuses on testing models using machine-generated data or game settings prone to shortcuts and spurious correlations, which lacks evaluation of machine ToM ability in real-world human interaction scenarios. This poses a pressing demand to develop new real-world scenario benchmarks. We introduce NegotiationToM, a new benchmark designed to stress-test machine ToM in real-world negotiation surrounding covered multi-dimensional mental states (i.e., desires, beliefs, and intentions). Our benchmark builds upon the Belief-Desire-Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models. Our findings demonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as they consistently perform significantly worse than humans, even when employing the chain-of-thought (CoT) method.

------------

`[2404.03027] JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks <https://arxiv.org/abs/2404.03027>`__ JailBreakV-28K:一个评估多模态大型语言模型对越狱攻击鲁棒性的基准

::

    replaced with revised version Wed, 3 Jul 2024 19:08:14 GMT
    Submission history From: Weidi Luo [view email]
    [v1] Wed, 3 Apr 2024 19:23:18 UTC (4,573 KB)
    [v2] Thu, 18 Apr 2024 17:11:53 UTC (4,575 KB)
    [v3] Wed, 3 Jul 2024 19:08:14 UTC (4,575 KB)
    Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, Chaowei Xiao

With the rapid advancements in Multimodal Large Language Models (MLLMs), securing these models against malicious inputs while aligning them with human values has emerged as a critical challenge. In this paper, we investigate an important and unexplored question of whether techniques that successfully jailbreak Large Language Models (LLMs) can be equally effective in jailbreaking MLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering benchmark designed to assess the transferability of LLM jailbreak techniques to MLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak attacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed in this paper, we generate 20, 000 text-based jailbreak prompts using advanced jailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from recent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test cases across a spectrum of adversarial scenarios. Our evaluation of 10 open-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks transferred from LLMs, highlighting a critical vulnerability in MLLMs that stems from their text-processing capabilities. Our findings underscore the urgent need for future research to address alignment vulnerabilities in MLLMs from both textual and visual inputs.

------------

`[2402.02037] EffiBench: Benchmarking the Efficiency of Automatically Generated Code <https://arxiv.org/abs/2402.02037>`__ EffiBench:自动生成代码的效率基准测试

::

    replaced with revised version Thu, 4 Jul 2024 02:55:05 GMT
    Submission history From: Huang Dong [view email]
    [v1] Sat, 3 Feb 2024 05:24:39 UTC (963 KB)
    [v2] Thu, 15 Feb 2024 15:57:06 UTC (963 KB)
    [v3] Fri, 7 Jun 2024 09:21:21 UTC (945 KB)
    [v4] Thu, 4 Jul 2024 02:55:05 UTC (946 KB)
    Dong Huang, Yuhao Qing, Weiyi Shang, Heming Cui, Jie M.Zhang

Code generation models have increasingly become integral to aiding software development. Although current research has thoroughly examined the correctness of the code produced by code generation models, a vital aspect that plays a pivotal role in green computing and sustainability efforts has often been neglected. This paper presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems to assess the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution, which obtains the SOTA efficiency on the LeetCode solution leaderboard. With EffiBench, we empirically examine the ability of 42 large language models (35 open-source and 7 closed-source) to generate efficient code. Our evaluation results demonstrate that the efficiency of the code generated by LLMs is generally worse than the efficiency of human-written canonical solutions. For example, GPT-4 generated code has an average \textbf{3.12} times execution time that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 generated code are \textbf{13.89} and \textbf{43.92} times that of the canonical solutions. The source code of EffiBench is released on this https URL. We also provide the LeaderBoard at this https URL.

------------

--------------
Accelerate (4)
--------------

`[2407.03687] STOC-TOT: Stochastic Tree-of-Thought with Constrained Decoding for Complex Reasoning in Multi-Hop Question Answering <https://arxiv.org/abs/2407.03687>`__ stock - tot:基于约束解码的多跳问答复杂推理随机思想树

::

    Thu, 4 Jul 2024 07:17:53 GMT
    Zhenyu Bi, Daniel Hajialigol, Zhongkai Sun, Jie Hao, Xuan Wang

Multi-hop question answering (MHQA) requires a model to retrieve and integrate information from multiple passages to answer a complex question.
Recent systems leverage the power of large language models and integrate evidence retrieval with reasoning prompts (e.g., chain-of-thought reasoning) for the MHQA task. However, the complexities in the question types (bridge v.s.
comparison questions) and the reasoning types (sequential v.s. parallel reasonings) require more novel and fine-grained prompting methods to enhance the performance of MHQA under the zero-shot setting. In this paper, we propose STOC-TOT, a stochastic tree-of-thought reasoning prompting method with constrained decoding for MHQA and conduct a detailed comparison with other reasoning prompts on different question types and reasoning types.
Specifically, we construct a tree-like reasoning structure by prompting the model to break down the original question into smaller sub-questions to form different reasoning paths. In addition, we prompt the model to provide a probability estimation for each reasoning path at each reasoning step. At answer time, we conduct constrained decoding on the model to generate more grounded answers and reduce hallucination. Experiments comparing STOC-TOT with two MHQA datasets and five large language models showed that our framework outperforms other reasoning prompts by a significant margin.

------------

`[2407.04528] GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning <https://arxiv.org/abs/2407.04528>`__ GPT vs RETRO:探索检索和参数高效微调的交集

::

    Fri, 5 Jul 2024 14:16:47 GMT
    Aleksander Ficek, Jiaqi Zeng, Oleksii Kuchaiev

Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT. Additionally, our study indicates that 8B parameter models strike an optimal balance between cost and performance and P-tuning lags behind other PEFT techniques. We further provide a comparative analysis of between applying PEFT to an Instruction-tuned RETRO model and base RETRO model. This work presents the first comprehensive comparison of various PEFT methods integrated with RAG, applied to both GPT and RETRO models, highlighting their relative performance.

------------

`[2407.03391] Soft Begging: Modular and Efficient Shielding of LLMs against Prompt Injection and Jailbreaking based on Prompt Tuning <https://arxiv.org/abs/2407.03391>`__ 软乞讨:模块化高效屏蔽llm对提示注入的攻击，并基于提示调优实现越狱

::

    Wed, 3 Jul 2024 14:52:09 GMT
    Simon Ostermann, Kevin Baum, Christoph Endres, Julia Masloh, Patrick Schramowski

Prompt injection (both direct and indirect) and jailbreaking are now recognized as significant issues for large language models (LLMs), particularly due to their potential for harm in application-integrated contexts. This extended abstract explores a novel approach to protecting LLMs from such attacks, termed "soft begging." This method involves training soft prompts to counteract the effects of corrupted prompts on the LLM's output. We provide an overview of prompt injections and jailbreaking, introduce the theoretical basis of the "soft begging" technique, and discuss an evaluation of its effectiveness.

------------

`[2401.18079] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization <https://arxiv.org/abs/2401.18079>`__ KVQuant:基于KV缓存量化的接近1000万上下文长度的LLM推断

::

    replaced with revised version Thu, 4 Jul 2024 08:00:01 GMT
    Submission history From: Coleman Hooper [view email]
    [v1] Wed, 31 Jan 2024 18:58:14 UTC (1,474 KB)
    [v2] Wed, 7 Feb 2024 08:39:28 UTC (1,062 KB)
    [v3] Thu, 4 Apr 2024 17:45:34 UTC (1,157 KB)
    [v4] Thu, 4 Jul 2024 08:00:01 UTC (1,783 KB)
    Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami

LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving the LLaMA-7B model with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.

------------

-----------------------
In-Context Learning (1)
-----------------------

`[2404.11225] In-Context Learning State Vector with Inner and Momentum Optimization <https://arxiv.org/abs/2404.11225>`__ 基于内部和动量优化的上下文学习状态向量

::

    replaced with revised version Thu, 4 Jul 2024 11:52:11 GMT
    Submission history From: Dongfang Li [view email]
    [v1] Wed, 17 Apr 2024 10:19:15 UTC (3,193 KB)
    [v2] Thu, 4 Jul 2024 11:52:11 UTC (4,077 KB)
    Dongfang Li, Zhenyu Liu, Xinshuo Hu, Zetian Sun, Baotian Hu, Min Zhang

Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples. Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer. However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored. In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introduce the concept of state vector. Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation. Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge. We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks. Code is available at this https URL

------------

-------------
Reasoning (9)
-------------

`[2407.03778] From Data to Commonsense Reasoning: The Use of Large Language Models for Explainable AI <https://arxiv.org/abs/2407.03778>`__ 从数据到常识推理:可解释人工智能大型语言模型的使用

::

    Thu, 4 Jul 2024 09:38:49 GMT
    Stefanie Krause, Frieder Stolzenburg

Commonsense reasoning is a difficult task for a computer, but a critical skill for an artificial intelligence (AI). It can enhance the explainability of AI models by enabling them to provide intuitive and human-like explanations for their decisions. This is necessary in many areas especially in question answering (QA), which is one of the most important tasks of natural language processing (NLP). Over time, a multitude of methods have emerged for solving commonsense reasoning problems such as knowledge-based approaches using formal logic or linguistic analysis. In this paper, we investigate the effectiveness of large language models (LLMs) on different QA tasks with a focus on their abilities in reasoning and explainability. We study three LLMs: GPT-3.5, Gemma and Llama 3. We further evaluate the LLM results by means of a questionnaire.
We demonstrate the ability of LLMs to reason with commonsense as the models outperform humans on different datasets. While GPT-3.5's accuracy ranges from 56% to 93% on various QA benchmarks, Llama 3 achieved a mean accuracy of 90% on all eleven datasets. Thereby Llama 3 is outperforming humans on all datasets with an average 21% higher accuracy over ten datasets. Furthermore, we can appraise that, in the sense of explainable artificial intelligence (XAI), GPT-3.5 provides good explanations for its decisions. Our questionnaire revealed that 66% of participants rated GPT-3.5's explanations as either "good" or "excellent". Taken together, these findings enrich our understanding of current LLMs and pave the way for future investigations of reasoning and explainability.

------------

`[2407.03624] Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks <https://arxiv.org/abs/2407.03624>`__ 问题分析提示提高了LLM在推理任务中的性能

::

    Thu, 4 Jul 2024 04:19:50 GMT
    Dharunish Yugeswardeenoo, Kevin Zhu, Sean O'Brien

Although LLMs have the potential to transform many fields, they still underperform humans in reasoning tasks. Existing methods induce the model to produce step-by-step calculations, but this research explores the question: Does making the LLM analyze the question improve its performance? We propose a novel prompting strategy called Question Analysis Prompting (QAP), in which the model is prompted to explain the question in $n$ words before solving. The value of $n$ influences the length of response generated by the model. QAP is evaluated on GPT 3.5 Turbo and GPT 4 Turbo on arithmetic datasets GSM8K, AQuA, and SAT and commonsense dataset StrategyQA. QAP is compared with other state-of-the-art prompts including Chain-of-Thought (CoT), Plan and Solve Prompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all state-of-the-art prompts on AQuA and SAT datasets on both GPT3.5 and GPT4. QAP consistently ranks among the top-2 prompts on 75\% of the tests. A key factor of QAP performance can be attributed to response length, where detailed responses are beneficial when answering harder questions, but can negatively affect easy questions.

------------

`[2407.03687] STOC-TOT: Stochastic Tree-of-Thought with Constrained Decoding for Complex Reasoning in Multi-Hop Question Answering <https://arxiv.org/abs/2407.03687>`__ stock - tot:基于约束解码的多跳问答复杂推理随机思想树

::

    Thu, 4 Jul 2024 07:17:53 GMT
    Zhenyu Bi, Daniel Hajialigol, Zhongkai Sun, Jie Hao, Xuan Wang

Multi-hop question answering (MHQA) requires a model to retrieve and integrate information from multiple passages to answer a complex question.
Recent systems leverage the power of large language models and integrate evidence retrieval with reasoning prompts (e.g., chain-of-thought reasoning) for the MHQA task. However, the complexities in the question types (bridge v.s.
comparison questions) and the reasoning types (sequential v.s. parallel reasonings) require more novel and fine-grained prompting methods to enhance the performance of MHQA under the zero-shot setting. In this paper, we propose STOC-TOT, a stochastic tree-of-thought reasoning prompting method with constrained decoding for MHQA and conduct a detailed comparison with other reasoning prompts on different question types and reasoning types.
Specifically, we construct a tree-like reasoning structure by prompting the model to break down the original question into smaller sub-questions to form different reasoning paths. In addition, we prompt the model to provide a probability estimation for each reasoning path at each reasoning step. At answer time, we conduct constrained decoding on the model to generate more grounded answers and reduce hallucination. Experiments comparing STOC-TOT with two MHQA datasets and five large language models showed that our framework outperforms other reasoning prompts by a significant margin.

------------

`[2407.04078] DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning <https://arxiv.org/abs/2407.04078>`__ DotaMath:基于代码辅助和数学推理自修正的思维分解

::

    Thu, 4 Jul 2024 17:39:16 GMT
    Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, Dayiheng Liu

Large language models (LLMs) have made impressive progress in handling simple math problems, yet they still struggle with more challenging and complex mathematical tasks. In this paper, we introduce a series of LLMs that employs the Decomposition of thought with code assistance and self-correction for mathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex mathematical tasks by decomposing them into simpler logical subtasks, leveraging code to solve these subtasks, obtaining fine-grained feedback from the code interpreter, and engaging in self-reflection and correction. By annotating diverse interactive tool-use trajectories and employing query evolution on GSM8K and MATH datasets, we generate an instruction fine-tuning dataset called DotaMathQA with 574K query-response pairs. We train a series of base LLMs using imitation learning on DotaMathQA, resulting in DotaMath models that achieve remarkable performance compared to open-source LLMs across various in-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases an outstanding performance of 64.8% on the competitive MATH dataset and 86.7% on GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a series of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward, we anticipate that the DotaMath paradigm will open new pathways for addressing intricate mathematical problems. Our code is publicly available at https://github.com/ChengpengLi1003/DotaMath.

------------

`[2401.17169] Conditional and Modal Reasoning in Large Language Models <https://arxiv.org/abs/2401.17169>`__ 大型语言模型中的条件和模态推理

::

    replaced with revised version Thu, 4 Jul 2024 18:12:25 GMT
    Submission history From: Wesley Holliday [view email]
    [v1] Tue, 30 Jan 2024 16:56:54 UTC (106 KB)
    [v2] Thu, 4 Jul 2024 18:12:25 UTC (1,077 KB)
    Wesley H. Holliday and Matthew Mandelkern and Cedegao E. Zhang

The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. In this paper, we probe the extent to which twenty-five LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental human ability to reason about distal possibilities. Assessing LLMs on these inferences is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but the GPT-4 model family often make basic mistakes with conditionals, though zero-shot chain-of-thought prompting helps them make fewer mistakes. Moreover, even the GPT-4 family displays logically inconsistent judgments across inference patterns involving epistemic modals, and almost all models give answers to certain complex conditional inferences widely discussed in the literature that do not match human judgments. These results highlight gaps in basic logical reasoning in today's LLMs.

------------

`[2402.11924] Evaluating LLMs' Inherent Multi-hop Reasoning Ability <https://arxiv.org/abs/2402.11924>`__ 评估llm固有的多跳推理能力

::

    replaced with revised version Fri, 5 Jul 2024 13:43:43 GMT
    Submission history From: Jian Wu [view email]
    [v1] Mon, 19 Feb 2024 08:12:30 UTC (1,642 KB)
    [v2] Sun, 3 Mar 2024 02:23:19 UTC (1,642 KB)
    [v3] Wed, 3 Jul 2024 15:50:48 UTC (18,721 KB)
    [v4] Fri, 5 Jul 2024 13:43:43 UTC (18,721 KB)
    Jian Wu, Linyi Yang, Zhen Wang, Manabu Okumura, Yue Zhang

While Large Language Models (LLMs) excel in question-answering (QA) tasks, their multi-step reasoning abilities on multiple evidence integration on Multi-hop QA tasks remain underexplored. LLMs sometimes generate answers that rely on internal memory rather than reasoning given context, which brings concerns about the evaluation quality of real reasoning abilities. The counterfactual QA task can separate internal memory from reasoning abilities, but focusing solely on final-QA performance without evaluating the multi-step reasoning process is insufficient for reporting LLMs' real reasoning abilities. Current Multi-hop QA (MHQA) benchmarks are factual and annotated on open-source corpora such as Wikipedia, although useful for multi-step reasoning evaluation, showing limitations due to potential data contamination in LLMs pre-training stage. To address this issue, we introduce the Inherent Reasoning Evaluation (IRE) method, a novel evaluation way that jointly evaluates the LLMs' chain-of-reasoning performance based on the first knowledge-edited counterfactual multi-hop QA data which involves editing the original Wikipedia passages, reducing data contamination risks. The IRE comprehensively assesses reasoning chains through sub-QA and final-QA evaluations. Our comparisons reveal significant performance gaps for several LLMs between Wikipedia-based benchmarks and IRE, deeming data contamination issues in existing benchmarks. We believe that the IRE benchmark will enhance and facilitate trustworthy LLM evaluations.

------------

`[2402.11997] Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models <https://arxiv.org/abs/2402.11997>`__ 还记得那年的这件事吗?评估大型语言模型中的时间信息和推理

::

    replaced with revised version Fri, 5 Jul 2024 11:26:51 GMT
    Submission history From: Himanshu Beniwal [view email]
    [v1] Mon, 19 Feb 2024 09:43:03 UTC (17,587 KB)
    [v2] Fri, 5 Jul 2024 11:26:51 UTC (22,523 KB)
    Himanshu Beniwal, Dishant Patel, Kowsik Nandagopan D, Hritik Ladia, Ankit Yadav, Mayank Singh

Large Language Models (LLMs) are increasingly ubiquitous, yet their ability to retain and reason about temporal information remains limited, hindering their application in real-world scenarios where understanding the sequential nature of events is crucial. Our study experiments with 12 state-of-the-art models (ranging from 2B to 70B+ parameters) on a novel numerical-temporal dataset, \textbf{TempUN}, spanning from 10,000 BCE to 2100 CE, to uncover significant temporal retention and comprehension limitations. We propose six metrics to assess three learning paradigms to enhance temporal knowledge acquisition. Our findings reveal that open-source models exhibit knowledge gaps more frequently, suggesting a trade-off between limited knowledge and incorrect responses. Additionally, various fine-tuning approaches significantly improved performance, reducing incorrect outputs and impacting the identification of 'information not available' in the generations. The associated dataset and code are available at (this https URL).

------------

`[2406.11012] Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game <https://arxiv.org/abs/2406.11012>`__ Connecting the Dots:使用New York Times Connections文字游戏评估llm的抽象推理能力

::

    replaced with revised version Fri, 5 Jul 2024 05:18:00 GMT
    Submission history From: Tuhin Chakrabarty Mr [view email]
    [v1] Sun, 16 Jun 2024 17:10:32 UTC (5,032 KB)
    [v2] Tue, 18 Jun 2024 15:02:28 UTC (5,032 KB)
    [v3] Sat, 22 Jun 2024 15:40:14 UTC (5,032 KB)
    [v4] Fri, 5 Jul 2024 05:18:00 UTC (5,034 KB)
    [v5] Mon, 15 Jul 2024 16:17:52 UTC (5,034 KB)
    Prisha Samadarshi, Mariam Mustafa, Anushka Kulkarni, Raven Rothkopf, Tuhin Chakrabarty, Smaranda Muresan

The New York Times Connections game has emerged as a popular and challenging pursuit for word puzzle enthusiasts. We collect 200 Connections games to evaluate the performance of state-of-the-art large language models (LLMs) against expert and novice human players. Our results show that even the best-performing LLM, GPT-4o, which has otherwise shown impressive reasoning abilities on a wide variety of benchmarks, can only fully solve 8% of the games. Compared to GPT-4o, novice and expert players perform better, with expert human players significantly outperforming GPT-4o. To deepen our understanding we create a taxonomy of the knowledge types required to successfully categorize words in the Connections game, revealing that LLMs struggle with associative, encyclopedic, and linguistic knowledge. Our findings establish the New York Times Connections game as a challenging benchmark for evaluating abstract reasoning capabilities in humans and AI systems.

------------

`[2407.01964] Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction <https://arxiv.org/abs/2407.01964>`__ 在llm中实现判别推理以进行法律判决预测

::

    replaced with revised version Thu, 4 Jul 2024 01:28:46 GMT
    Submission history From: Chenlong Deng [view email]
    [v1] Tue, 2 Jul 2024 05:43:15 UTC (722 KB)
    [v2] Wed, 3 Jul 2024 02:25:23 UTC (722 KB)
    [v3] Thu, 4 Jul 2024 01:28:46 UTC (722 KB)
    Chenlong Deng, Kelong Mao, Yuyao Zhang, Zhicheng Dou

Legal judgment prediction is essential for enhancing judicial efficiency. In this work, we identify that existing large language models (LLMs) underperform in this domain due to challenges in understanding case complexities and distinguishing between similar charges. To adapt LLMs for effective legal judgment prediction, we introduce the Ask-Discriminate-Predict (ADAPT) reasoning framework inspired by human judicial reasoning. ADAPT involves decomposing case facts, discriminating among potential charges, and predicting the final judgment. We further enhance LLMs through fine-tuning with multi-task synthetic trajectories to improve legal judgment prediction accuracy and efficiency under our ADAPT framework. Extensive experiments conducted on two widely-used datasets demonstrate the superior performance of our framework in legal judgment prediction, particularly when dealing with complex and confusing charges.

------------

-----------
ToolUse (1)
-----------

`[2407.03913] MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices <https://arxiv.org/abs/2407.03913>`__ MobileExperts:移动设备中支持工具的动态代理团队

::

    Thu, 4 Jul 2024 13:12:19 GMT
    Jiayi Zhang, Chuang Zhao, Yihan Zhao, Zhaoyang Yu, Ming He, Jianping Fan

The attainment of autonomous operations in mobile computing devices has consistently been a goal of human pursuit. With the development of Large Language Models (LLMs) and Visual Language Models (VLMs), this aspiration is progressively turning into reality. While contemporary research has explored automation of simple tasks on mobile devices via VLMs, there remains significant room for improvement in handling complex tasks and reducing high reasoning costs. In this paper, we introduce MobileExperts, which for the first time introduces tool formulation and multi-agent collaboration to address the aforementioned challenges. More specifically, MobileExperts dynamically assembles teams based on the alignment of agent portraits with the human requirements. Following this, each agent embarks on an independent exploration phase, formulating its tools to evolve into an expert. Lastly, we develop a dual-layer planning mechanism to establish coordinate collaboration among experts. To validate our effectiveness, we design a new benchmark of hierarchical intelligence levels, offering insights into algorithm's capability to address tasks across a spectrum of complexity. Experimental results demonstrate that MobileExperts performs better on all intelligence levels and achieves ~ 22% reduction in reasoning costs, thus verifying the superiority of our design.

------------

------------------------
Retrieval-Augmented (11)
------------------------

`[2407.03585] Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval <https://arxiv.org/abs/2407.03585>`__ 具有llm生成策略和信息检索的零样本说服性聊天机器人

::

    Thu, 4 Jul 2024 02:28:21 GMT
    Kazuaki Furumai, Roberto Legaspi, Julio Vizcarra, Yudai Yamazaki, Yasutaka Nishimura, Sina J. Semnani, Kazushi Ikeda, Weiyan Shi, Monica S. Lam

Persuasion plays a pivotal role in a wide range of applications from health intervention to the promotion of social good. Persuasive chatbots can accelerate the positive effects of persuasion in such applications. Existing methods rely on fine-tuning persuasive chatbots with task-specific training data which is costly, if not infeasible, to collect. To address this issue, we propose a method to leverage the generalizability and inherent persuasive abilities of large language models (LLMs) in creating effective and truthful persuasive chatbot for any given domain in a zero-shot manner. Unlike previous studies which used pre-defined persuasion strategies, our method first uses an LLM to generate responses, then extracts the strategies used on the fly, and replaces any unsubstantiated claims in the response with retrieved facts supporting the strategies. We applied our chatbot, PersuaBot, to three significantly different domains needing persuasion skills: donation solicitation, recommendations, and health intervention. Our experiments on simulated and human conversations show that our zero-shot approach is more persuasive than prior work, while achieving factual accuracy surpassing state-of-the-art knowledge-oriented chatbots. Our study demonstrated that when persuasive chatbots are employed responsibly for social good, it is an enabler of positive individual and social change.

------------

`[2407.03627] DSLR: Document Refinement with Sentence-Level Re-ranking and Reconstruction to Enhance Retrieval-Augmented Generation <https://arxiv.org/abs/2407.03627>`__ DSLR:基于句子级重排序和重构的文档精化以增强检索增强生成

::

    Thu, 4 Jul 2024 04:30:04 GMT
    Taeho Hwang, Soyeong Jeong, Sukmin Cho, SeungYoon Han, Jong C. Park

Recent advancements in Large Language Models (LLMs) have significantly improved their performance across various Natural Language Processing (NLP) tasks. However, LLMs still struggle with generating non-factual responses due to limitations in their parametric memory. Retrieval-Augmented Generation (RAG) systems address this issue by incorporating external knowledge with a retrieval module. Despite their successes, however, current RAG systems face challenges with retrieval failures and the limited ability of LLMs to filter out irrelevant information. Therefore, in this work, we propose \textit{\textbf{DSLR}} (\textbf{D}ocument Refinement with \textbf{S}entence-\textbf{L}evel \textbf{R}e-ranking and Reconstruction), an unsupervised framework that decomposes retrieved documents into sentences, filters out irrelevant sentences, and reconstructs them again into coherent passages. We experimentally validate \textit{DSLR} on multiple open-domain QA datasets and the results demonstrate that \textit{DSLR} significantly enhances the RAG performance over conventional fixed-size passage. Furthermore, our \textit{DSLR} enhances performance in specific, yet realistic scenarios without the need for additional training, providing an effective and efficient solution for refining retrieved documents in RAG systems.

------------

`[2407.03955] Meta-prompting Optimized Retrieval-augmented Generation <https://arxiv.org/abs/2407.03955>`__ 元提示优化检索增强生成

::

    Thu, 4 Jul 2024 14:20:12 GMT
    Jo\~ao Rodrigues, Ant\'onio Branco

Retrieval-augmented generation resorts to content retrieved from external sources in order to leverage the performance of large language models in downstream tasks. The excessive volume of retrieved content, the possible dispersion of its parts, or their out of focus range may happen nevertheless to eventually have a detrimental rather than an incremental effect. To mitigate this issue and improve retrieval-augmented generation, we propose a method to refine the retrieved content before it is included in the prompt by resorting to meta-prompting optimization. Put to empirical test with the demanding multi-hop question answering task from the StrategyQA dataset, the evaluation results indicate that this method outperforms a similar retrieval-augmented system but without this method by over 30%.

------------

`[2407.04485] Leveraging Graph Structures to Detect Hallucinations in Large Language Models <https://arxiv.org/abs/2407.04485>`__ 利用图结构检测大型语言模型中的幻觉

::

    Fri, 5 Jul 2024 13:08:58 GMT
    Noa Nonkes, Sergei Agaronian, Evangelos Kanoulas, Roxana Petcu

Large language models are extensively applied across a wide range of tasks, such as customer support, content creation, educational tutoring, and providing financial guidance. However, a well-known drawback is their predisposition to generate hallucinations. This damages the trustworthiness of the information these models provide, impacting decision-making and user confidence. We propose a method to detect hallucinations by looking at the structure of the latent space and finding associations within hallucinated and non-hallucinated generations. We create a graph structure that connects generations that lie closely in the embedding space. Moreover, we employ a Graph Attention Network which utilizes message passing to aggregate information from neighboring nodes and assigns varying degrees of importance to each neighbor based on their relevance. Our findings show that 1) there exists a structure in the latent space that differentiates between hallucinated and non-hallucinated generations, 2) Graph Attention Networks can learn this structure and generalize it to unseen generations, and 3) the robustness of our method is enhanced when incorporating contrastive learning. When evaluated against evidence-based benchmarks, our model performs similarly without access to search-based methods.

------------

`[2407.04528] GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning <https://arxiv.org/abs/2407.04528>`__ GPT vs RETRO:探索检索和参数高效微调的交集

::

    Fri, 5 Jul 2024 14:16:47 GMT
    Aleksander Ficek, Jiaqi Zeng, Oleksii Kuchaiev

Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT. Additionally, our study indicates that 8B parameter models strike an optimal balance between cost and performance and P-tuning lags behind other PEFT techniques. We further provide a comparative analysis of between applying PEFT to an Instruction-tuned RETRO model and base RETRO model. This work presents the first comprehensive comparison of various PEFT methods integrated with RAG, applied to both GPT and RETRO models, highlighting their relative performance.

------------

`[2407.04581] Leveraging Large Language Models for Integrated Satellite-Aerial-Terrestrial Networks: Recent Advances and Future Directions <https://arxiv.org/abs/2407.04581>`__ 利用大型语言模型实现星-空-地一体化网络:最新进展和未来方向

::

    Fri, 5 Jul 2024 15:23:43 GMT
    Shumaila Javaid, Ruhul Amin Khalil, Nasir Saeed, Bin He, and Mohamed-Slim Alouini

Integrated satellite, aerial, and terrestrial networks (ISATNs) represent a sophisticated convergence of diverse communication technologies to ensure seamless connectivity across different altitudes and platforms. This paper explores the transformative potential of integrating Large Language Models (LLMs) into ISATNs, leveraging advanced Artificial Intelligence (AI) and Machine Learning (ML) capabilities to enhance these networks. We outline the current architecture of ISATNs and highlight the significant role LLMs can play in optimizing data flow, signal processing, and network management to advance 5G/6G communication technologies through advanced predictive algorithms and real-time decision-making. A comprehensive analysis of ISATN components is conducted, assessing how LLMs can effectively address traditional data transmission and processing bottlenecks. The paper delves into the network management challenges within ISATNs, emphasizing the necessity for sophisticated resource allocation strategies, traffic routing, and security management to ensure seamless connectivity and optimal performance under varying conditions. Furthermore, we examine the technical challenges and limitations associated with integrating LLMs into ISATNs, such as data integration for LLM processing, scalability issues, latency in decision-making processes, and the design of robust, fault-tolerant systems. The study also identifies key future research directions for fully harnessing LLM capabilities in ISATNs, which is crucial for enhancing network reliability, optimizing performance, and achieving a truly interconnected and intelligent global network system.

------------

`[2407.04573] VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models <https://arxiv.org/abs/2407.04573>`__ VRSD:对大型语言模型检索的相似性和多样性的再思考

::

    Fri, 5 Jul 2024 15:08:44 GMT
    Hang Gao and Yongfeng Zhang

Vector retrieval algorithms are vital for semantic queries in the evolving landscape of Large Language Models (LLMs). Retrieving vectors that simultaneously meet criteria for both similarity and diversity significantly enhances the capabilities of LLM-based agents. Despite the widespread use of the Maximal Marginal Relevance (MMR) in retrieval scenarios with relevance and diversity requirements, fluctuations caused by variations in the parameter $ \lambda $ within the MMR complicate the determination of the optimization trajectory in vector spaces, thus obscuring the direction of enhancement.
Moreover, there is a lack of a robust theoretical analysis for the constraints of similarity and diversity in retrieval processes. This paper introduces a novel approach to characterizing both constraints through the relationship between the sum vector and the query vector. The proximity of these vectors addresses the similarity constraint, while necessitating that individual vectors within the sum vector divergently align with the query vector to satisfy the diversity constraint. We also formulate a new combinatorial optimization challenge, taking a selection of $k$ vectors from a set of candidates such that their sum vector maximally aligns with the query vector, a problem we demonstrate to be NP-complete. This establishes the profound difficulty of pursuing similarity and diversity simultaneously in vector retrieval and lays a theoretical groundwork for further research. Additionally, we present the heuristic algorithm Vectors Retrieval with Similarity and Diversity (VRSD) which not only has a definitive optimization goal and eschews the need for preset parameters but also offers a modest reduction in time complexity compared to MMR. Empirical validation further confirm that VRSD significantly surpasses MMR across various datasets.

------------

`[2406.01549] An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation <https://arxiv.org/abs/2406.01549>`__ 检索增强生成中有效噪声过滤的信息瓶颈视角

::

    replaced with revised version Thu, 4 Jul 2024 14:21:39 GMT
    Submission history From: Kun Zhu [view email]
    [v1] Mon, 3 Jun 2024 17:31:06 UTC (354 KB)
    [v2] Thu, 4 Jul 2024 14:21:39 UTC (373 KB)
    Kun Zhu, Xiaocheng Feng, Xiyuan Du, Yuxuan Gu, Weijiang Yu, Haotian Wang, Qianglong Chen, Zheng Chu, Jingchang Chen, Bing Qin

Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data. One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression. In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation. Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage. In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards. Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with $2.5\%$ compression rate.

------------

`[2406.06399] Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue <https://arxiv.org/abs/2406.06399>`__ 我们应该微调还是抹布?评估用于对话的llm的不同技术

::

    replaced with revised version Fri, 5 Jul 2024 11:47:31 GMT
    Submission history From: Massimo Rizzoli [view email]
    [v1] Mon, 10 Jun 2024 15:52:49 UTC (4,895 KB)
    [v2] Fri, 5 Jul 2024 11:47:31 UTC (4,903 KB)
    Simone Alghisi, Massimo Rizzoli, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi

We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue. Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain). However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics. In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering. We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type. We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent evaluation and explainability criteria for automatic metrics and human evaluation protocols. Our analysis shows that there is no universal best-technique for adapting large language models as the efficacy of each technique depends on both the base LLM and the specific type of dialogue. Last but not least, the assessment of the best adaptation technique should include human evaluation to avoid false expectations and outcomes derived from automatic metrics.

------------

`[2406.18064] Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need <https://arxiv.org/abs/2406.18064>`__ 评估检索增强生成的答案质量:一个强大的LLM就是你所需要的

::

    replaced with revised version Fri, 5 Jul 2024 09:46:33 GMT
    Submission history From: Yang Wang [view email]
    [v1] Wed, 26 Jun 2024 04:49:41 UTC (703 KB)
    [v2] Fri, 5 Jul 2024 09:46:33 UTC (894 KB)
    Yang Wang, Alberto Garcia Hernandez, Roman Kyslyi, Nicholas Kersting

We present a comprehensive study of answer quality evaluation in Retrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel grading system that is designed to assess correctness, completeness, and honesty. We further map the grading of quality aspects aforementioned into a binary score, indicating an accept or reject decision, mirroring the intuitive "thumbs-up" or "thumbs-down" gesture commonly used in chat applications. This approach suits factual business settings where a clear decision opinion is essential. Our assessment applies vRAG-Eval to two Large Language Models (LLMs), evaluating the quality of answers generated by a vanilla RAG application. We compare these evaluations with human expert judgments and find a substantial alignment between GPT-4's assessments and those of human experts, reaching 83% agreement on accept or reject decisions. This study highlights the potential of LLMs as reliable evaluators in closed-domain, closed-ended settings, particularly when human evaluations require significant resources.

------------

`[2407.02220] Embodied AI in Mobile Robots: Coverage Path Planning with Large Language Models <https://arxiv.org/abs/2407.02220>`__ 移动机器人中的具身人工智能:基于大型语言模型的覆盖路径规划

::

    replaced with revised version Thu, 4 Jul 2024 01:42:58 GMT
    Submission history From: Xiangrui Kong Ray [view email]
    [v1] Tue, 2 Jul 2024 12:38:46 UTC (4,774 KB)
    [v2] Thu, 4 Jul 2024 01:42:58 UTC (4,774 KB)
    Xiangrui Kong, Wenxiao Zhang, Jin Hong, Thomas Braunl

In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and solving mathematical problems, leading to advancements in various fields. We propose an LLM-embodied path planning framework for mobile agents, focusing on solving high-level coverage path planning issues and low-level control. Our proposed multi-layer architecture uses prompted LLMs in the path planning phase and integrates them with the mobile agents' low-level actuators. To evaluate the performance of various LLMs, we propose a coverage-weighted path planning metric to assess the performance of the embodied models. Our experiments show that the proposed framework improves LLMs' spatial inference abilities. We demonstrate that the proposed multi-layer framework significantly enhances the efficiency and accuracy of these tasks by leveraging the natural language understanding and generative capabilities of LLMs. Our experiments show that this framework can improve LLMs' 2D plane reasoning abilities and complete coverage path planning tasks. We also tested three LLM kernels: gpt-4o, gemini-1.5-flash, and claude-3.5-sonnet. The experimental results show that claude-3.5 can complete the coverage planning task in different scenarios, and its indicators are better than those of the other models.

------------

---------
Agent (5)
---------

`[2407.03913] MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices <https://arxiv.org/abs/2407.03913>`__ MobileExperts:移动设备中支持工具的动态代理团队

::

    Thu, 4 Jul 2024 13:12:19 GMT
    Jiayi Zhang, Chuang Zhao, Yihan Zhao, Zhaoyang Yu, Ming He, Jianping Fan

The attainment of autonomous operations in mobile computing devices has consistently been a goal of human pursuit. With the development of Large Language Models (LLMs) and Visual Language Models (VLMs), this aspiration is progressively turning into reality. While contemporary research has explored automation of simple tasks on mobile devices via VLMs, there remains significant room for improvement in handling complex tasks and reducing high reasoning costs. In this paper, we introduce MobileExperts, which for the first time introduces tool formulation and multi-agent collaboration to address the aforementioned challenges. More specifically, MobileExperts dynamically assembles teams based on the alignment of agent portraits with the human requirements. Following this, each agent embarks on an independent exploration phase, formulating its tools to evolve into an expert. Lastly, we develop a dual-layer planning mechanism to establish coordinate collaboration among experts. To validate our effectiveness, we design a new benchmark of hierarchical intelligence levels, offering insights into algorithm's capability to address tasks across a spectrum of complexity. Experimental results demonstrate that MobileExperts performs better on all intelligence levels and achieves ~ 22% reduction in reasoning costs, thus verifying the superiority of our design.

------------

`[2407.04363] AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents <https://arxiv.org/abs/2407.04363>`__ AriGraph:基于情景记忆的LLM智能体知识图谱世界模型

::

    Fri, 5 Jul 2024 09:06:47 GMT
    Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, Evgeny Burnaev

Advancements in generative AI have broadened the potential applications of Large Language Models (LLMs) in the development of autonomous agents. Achieving true autonomy requires accumulating and updating knowledge gained from interactions with the environment and effectively utilizing it. Current LLM-based approaches leverage past experiences using a full history of observations, summarization or retrieval augmentation. However, these unstructured memory representations do not facilitate the reasoning and planning essential for complex decision-making. In our study, we introduce AriGraph, a novel method wherein the agent constructs a memory graph that integrates semantic and episodic memories while exploring the environment. This graph structure facilitates efficient associative retrieval of interconnected concepts, relevant to the agent's current state and goals, thus serving as an effective environmental model that enhances the agent's exploratory and planning capabilities. We demonstrate that our Ariadne LLM agent, equipped with this proposed memory architecture augmented with planning and decision-making, effectively handles complex tasks on a zero-shot basis in the TextWorld environment. Our approach markedly outperforms established methods such as full-history, summarization, and Retrieval-Augmented Generation in various tasks, including the cooking challenge from the First TextWorld Problems competition and novel tasks like house cleaning and puzzle Treasure Hunting.

------------

`[2407.03884] Planning with Large Language Models for Conversational Agents <https://arxiv.org/abs/2407.03884>`__ 会话智能体的大型语言模型规划

::

    Thu, 4 Jul 2024 12:23:02 GMT
    Zhigen Li, Jianxiang Peng, Yanmeng Wang, Tianhao Shen, Minghui Zhang, Linxi Su, Shang Wu, Yihang Wu, Yuqian Wang, Ye Wang, Wei Hu, Jianfeng Li, Shaojun Wang, Jing Xiao and Deyi Xiong

Controllability and proactivity are crucial properties of autonomous conversational agents (CAs). Controllability requires the CAs to follow the standard operating procedures (SOPs), such as verifying identity before activating credit cards. Proactivity requires the CAs to guide the conversation towards the goal during user uncooperation, such as persuasive dialogue.
Existing research cannot be unified with controllability, proactivity, and low manual annotation. To bridge this gap, we propose a new framework for planning-based conversational agents (PCA) powered by large language models (LLMs), which only requires humans to define tasks and goals for the LLMs.
Before conversation, LLM plans the core and necessary SOP for dialogue offline.
During the conversation, LLM plans the best action path online referring to the SOP, and generates responses to achieve process controllability. Subsequently, we propose a semi-automatic dialogue data creation framework and curate a high-quality dialogue dataset (PCA-D). Meanwhile, we develop multiple variants and evaluation metrics for PCA, e.g., planning with Monte Carlo Tree Search (PCA-M), which searches for the optimal dialogue action while satisfying SOP constraints and achieving the proactive of the dialogue. Experiment results show that LLMs finetuned on PCA-D can significantly improve the performance and generalize to unseen domains. PCA-M outperforms other CoT and ToT baselines in terms of conversation controllability, proactivity, task success rate, and overall logical coherence, and is applicable in industry dialogue scenarios.
The dataset and codes are available at XXXX.

------------

`[2407.03956] Solving Zebra Puzzles Using Constraint-Guided Multi-Agent Systems <https://arxiv.org/abs/2407.03956>`__ 基于约束引导的多智能体系统求解斑马拼图

::

    Thu, 4 Jul 2024 14:22:25 GMT
    Shmuel Berman, Baishakhi Ray, Kathleen McKeown

Prior research has enhanced the ability of Large Language Models (LLMs) to solve logic puzzles using techniques such as chain-of-thought prompting or introducing a symbolic representation. These frameworks are still usually insufficient to solve complicated logical problems, such as Zebra puzzles, due to the inherent complexity of translating natural language clues into logical statements. We introduce a multi-agent system, ZPS, that integrates LLMs with an off the shelf theorem prover. This system tackles the complex puzzle-solving task by breaking down the problem into smaller, manageable parts, generating SMT (Satisfiability Modulo Theories) code to solve them with a theorem prover, and using feedback between the agents to repeatedly improve their answers. We also introduce an automated grid puzzle grader to assess the correctness of our puzzle solutions and show that the automated grader is reliable by evaluating it in a user-study. Our approach shows improvement in all three LLMs we tested, with GPT-4 showing 166% improvement in the number of fully correct solutions.

------------

`[2404.09982] Memory Sharing for Large Language Model based Agents <https://arxiv.org/abs/2404.09982>`__ 基于agent的大型语言模型的内存共享

::

    replaced with revised version Fri, 5 Jul 2024 15:22:58 GMT
    Submission history From: Hang Gao [view email]
    [v1] Mon, 15 Apr 2024 17:57:30 UTC (415 KB)
    [v2] Fri, 5 Jul 2024 15:22:58 UTC (8,526 KB)
    Hang Gao, Yongfeng Zhang

The adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning, but are constrained by the comprehensiveness and diversity of the provided examples, leading to outputs that often diverge significantly from expected results, especially when it comes to the open-ended questions. This paper introduces the Memory Sharing, a framework which integrates the real-time memory filter, storage and retrieval to enhance the In-Context Learning process. This framework allows for the sharing of memories among multiple agents, whereby the interactions and shared memories between different agents effectively enhance the diversity of the memories. The collective self-enhancement through interactive learning among multiple agents facilitates the evolution from individual intelligence to collective intelligence. Besides, the dynamically growing memory pool is utilized not only to improve the quality of responses but also to train and enhance the retriever. We evaluated our framework across three distinct domains involving specialized tasks of agents. The experimental results demonstrate that the MS framework significantly improves the agents' performance in addressing open-ended questions.

------------

-----------
Other (118)
-----------

`[2407.03652] Over the Edge of Chaos? Excess Complexity as a Roadblock to Artificial General Intelligence <https://arxiv.org/abs/2407.03652>`__ 在混乱的边缘?作为人工通用智能障碍的过度复杂性

::

    Thu, 4 Jul 2024 05:46:39 GMT
    Teo Susnjak, Timothy R. McIntosh, Andre L. C. Barczak, Napoleon H. Reyes, Tong Liu, Paul Watters, Malka N. Halgamuge

In this study, we explored the progression trajectories of artificial intelligence (AI) systems through the lens of complexity theory. We challenged the conventional linear and exponential projections of AI advancement toward Artificial General Intelligence (AGI) underpinned by transformer-based architectures, and posited the existence of critical points, akin to phase transitions in complex systems, where AI performance might plateau or regress into instability upon exceeding a critical complexity threshold. We employed agent-based modelling (ABM) to simulate hypothetical scenarios of AI systems' evolution under specific assumptions, using benchmark performance as a proxy for capability and complexity. Our simulations demonstrated how increasing the complexity of the AI system could exceed an upper criticality threshold, leading to unpredictable performance behaviours. Additionally, we developed a practical methodology for detecting these critical thresholds using simulation data and stochastic gradient descent to fine-tune detection thresholds. This research offers a novel perspective on AI advancement that has a particular relevance to Large Language Models (LLMs), emphasising the need for a tempered approach to extrapolating AI's growth potential and underscoring the importance of developing more robust and comprehensive AI performance benchmarks.

------------

`[2407.03759] Convolutional vs Large Language Models for Software Log Classification in Edge-Deployable Cellular Network Testing <https://arxiv.org/abs/2407.03759>`__ 可边部署蜂窝网络测试中软件日志分类的卷积与大型语言模型

::

    Thu, 4 Jul 2024 09:12:08 GMT
    Achintha Ihalage, Sayed M. Taheri, Faris Muhammad and Hamed Al-Raweshidy

Software logs generated by sophisticated network emulators in the telecommunications industry, such as VIAVI TM500, are extremely complex, often comprising tens of thousands of text lines with minimal resemblance to natural language. Only specialised expert engineers can decipher such logs and troubleshoot defects in test runs. While AI offers a promising solution for automating defect triage, potentially leading to massive revenue savings for companies, state-of-the-art large language models (LLMs) suffer from significant drawbacks in this specialised domain. These include a constrained context window, limited applicability to text beyond natural language, and high inference costs. To address these limitations, we propose a compact convolutional neural network (CNN) architecture that offers a context window spanning up to 200,000 characters and achieves over 96% accuracy (F1>0.9) in classifying multifaceted software logs into various layers in the telecommunications protocol stack. Specifically, the proposed model is capable of identifying defects in test runs and triaging them to the relevant department, formerly a manual engineering process that required expert knowledge. We evaluate several LLMs; LLaMA2-7B, Mixtral 8x7B, Flan-T5, BERT and BigBird, and experimentally demonstrate their shortcomings in our specialized application. Despite being lightweight, our CNN significantly outperforms LLM-based approaches in telecommunications log classification while minimizing the cost of production. Our defect triaging AI model is deployable on edge devices without dedicated hardware and widely applicable across software logs in various industries.

------------

`[2407.03942] Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data <https://arxiv.org/abs/2407.03942>`__ 基于合成数据的多样化细粒度指令遵循能力探索

::

    Thu, 4 Jul 2024 13:54:41 GMT
    Zihui Gu, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Cheng-Zhong Xu, Ju Fan

Instruction-following is particularly crucial for large language models (LLMs) to support diverse user requests. While existing work has made progress in aligning LLMs with human preferences, evaluating their capabilities on instruction following remains a challenge due to complexity and diversity of real-world user instructions. While existing evaluation methods focus on general skills, they suffer from two main shortcomings, i.e., lack of fine-grained task-level evaluation and reliance on singular instruction expression. To address these problems, this paper introduces DINGO, a fine-grained and diverse instruction-following evaluation dataset that has two main advantages: (1) DINGO is based on a manual annotated, fine-grained and multi-level category tree with 130 nodes derived from real-world user requests; (2) DINGO includes diverse instructions, generated by both GPT-4 and human experts. Through extensive experiments, we demonstrate that DINGO can not only provide more challenging and comprehensive evaluation for LLMs, but also provide task-level fine-grained directions to further improve LLMs.

------------

`[2407.04106] MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis <https://arxiv.org/abs/2407.04106>`__ 

::

    Thu, 4 Jul 2024 18:21:10 GMT
    Asma Alkhaldi, Raneem Alnajim, Layan Alabdullatef, Rawan Alyahya, Jun Chen, Deyao Zhu, Ahmed Alsinan, Mohamed Elhoseiny

Recent advancements in artificial intelligence (AI) have precipitated significant breakthroughs in healthcare, particularly in refining diagnostic procedures. However, previous studies have often been constrained to limited functionalities. This study introduces MiniGPT-Med, a vision-language model derived from large-scale language models and tailored for medical applications.
MiniGPT-Med demonstrates remarkable versatility across various imaging modalities, including X-rays, CT scans, and MRIs, enhancing its utility. The model is capable of performing tasks such as medical report generation, visual question answering (VQA), and disease identification within medical imagery.
Its integrated processing of both image and textual clinical data markedly improves diagnostic accuracy. Our empirical assessments confirm MiniGPT-Med's superior performance in disease grounding, medical report generation, and VQA benchmarks, representing a significant step towards reducing the gap in assisting radiology practice. Furthermore, it achieves state-of-the-art performance on medical report generation, higher than the previous best model by 19\% accuracy. MiniGPT-Med promises to become a general interface for radiology diagnoses, enhancing diagnostic efficiency across a wide range of medical imaging applications.

------------

`[2407.04181] Orchestrating LLMs with Different Personalizations <https://arxiv.org/abs/2407.04181>`__ 对llm进行不同的个性化编排

::

    Thu, 4 Jul 2024 22:55:02 GMT
    Jin Peng Zhou, Katie Z Luo, Jingwen Gu, Jason Yuan, Kilian Q. Weinberger, Wen Sun

This paper presents a novel approach to aligning large language models (LLMs) with individual human preferences, sometimes referred to as Reinforcement Learning from \textit{Personalized} Human Feedback (RLPHF). Given stated preferences along multiple dimensions, such as helpfulness, conciseness, or humor, the goal is to create an LLM without re-training that best adheres to this specification. Starting from specialized expert LLMs, each trained for one such particular preference dimension, we propose a black-box method that merges their outputs on a per-token level. We train a lightweight Preference Control Model (PCM) that dynamically translates the preference description and current context into next-token prediction weights. By combining the expert models' outputs at the token level, our approach dynamically generates text that optimizes the given preference. Empirical tests show that our method matches or surpasses existing preference merging techniques, providing a scalable, efficient alternative to fine-tuning LLMs for individual personalization.

------------

`[2407.04467] Are Large Language Models Strategic Decision Makers? A Study of Performance and Bias in Two-Player Non-Zero-Sum Games <https://arxiv.org/abs/2407.04467>`__ 大型语言模型是战略决策者吗?对二人非零和博弈中的表现和偏见的研究

::

    Fri, 5 Jul 2024 12:30:02 GMT
    Nathan Herr, Fernando Acero, Roberta Raileanu, Mar\'ia P\'erez-Ortiz, and Zhibin Li

Large Language Models (LLMs) have been increasingly used in real-world settings, yet their strategic abilities remain largely unexplored. Game theory provides a good framework for assessing the decision-making abilities of LLMs in interactions with other agents. Although prior studies have shown that LLMs can solve these tasks with carefully curated prompts, they fail when the problem setting or prompt changes. In this work we investigate LLMs' behaviour in strategic games, Stag Hunt and Prisoner Dilemma, analyzing performance variations under different settings and prompts. Our results show that the tested state-of-the-art LLMs exhibit at least one of the following systematic biases: (1) positional bias, (2) payoff bias, or (3) behavioural bias.
Subsequently, we observed that the LLMs' performance drops when the game configuration is misaligned with the affecting biases. Performance is assessed based on the selection of the correct action, one which agrees with the prompted preferred behaviours of both players. Alignment refers to whether the LLM's bias aligns with the correct action. For example, GPT-4o's average performance drops by 34% when misaligned. Additionally, the current trend of "bigger and newer is better" does not hold for the above, where GPT-4o (the current best-performing LLM) suffers the most substantial performance drop.
Lastly, we note that while chain-of-thought prompting does reduce the effect of the biases on most models, it is far from solving the problem at the fundamental level.

------------

`[2407.03460] Collaborative Quest Completion with LLM-driven Non-Player Characters in Minecraft <https://arxiv.org/abs/2407.03460>`__ 在《我的世界》中与llm驱动的非玩家角色合作完成任务

::

    Wed, 3 Jul 2024 19:11:21 GMT
    Sudha Rao, Weijia Xu, Michael Xu, Jorge Leandro, Ken Lobb, Gabriel DesGarennes, Chris Brockett, Bill Dolan

The use of generative AI in video game development is on the rise, and as the conversational and other capabilities of large language models continue to improve, we expect LLM-driven non-player characters (NPCs) to become widely deployed. In this paper, we seek to understand how human players collaborate with LLM-driven NPCs to accomplish in-game goals. We design a minigame within Minecraft where a player works with two GPT4-driven NPCs to complete a quest.
We perform a user study in which 28 Minecraft players play this minigame and share their feedback. On analyzing the game logs and recordings, we find that several patterns of collaborative behavior emerge from the NPCs and the human players. We also report on the current limitations of language-only models that do not have rich game-state or visual understanding. We believe that this preliminary study and analysis will inform future game developers on how to better exploit these rapidly improving generative AI models for collaborative roles in games.

------------

`[2407.03518] Improving LLM Abilities in Idiomatic Translation <https://arxiv.org/abs/2407.03518>`__ 提高LLM在习语翻译中的能力

::

    Wed, 3 Jul 2024 21:34:26 GMT
    Sundesh Donthi, Maximilian Spencer, Om Patel, Joon Doh, Eid Rodan

For large language models (LLMs) like NLLB and GPT, translating idioms remains a challenge. Our goal is to enhance translation fidelity by improving LLM processing of idiomatic language while preserving the original linguistic style. This has a significant social impact, as it preserves cultural nuances and ensures translated texts retain their intent and emotional resonance, fostering better cross-cultural communication. Previous work has utilized knowledge bases like IdiomKB by providing the LLM with the meaning of an idiom to use in translation. Although this method yielded better results than a direct translation, it is still limited in its ability to preserve idiomatic writing style across languages. In this research, we expand upon the knowledge base to find corresponding idioms in the target language. Our research performs translations using two methods: The first method employs the SentenceTransformers model to semantically generate cosine similarity scores between the meanings of the original and target language idioms, selecting the best idiom (Cosine Similarity method). The second method uses an LLM to find a corresponding idiom in the target language for use in the translation (LLM-generated idiom method). As a baseline, we performed a direct translation without providing additional information. Human evaluations on the English -> Chinese, and Chinese -> English show the Cosine Similarity Lookup method out-performed others in all GPT4o translations. To further build upon IdiomKB, we developed a low-resource Urdu dataset containing Urdu idioms and their translations. Despite dataset limitations, the Cosine Similarity Lookup method shows promise, potentially overcoming language barriers and enabling the exploration of diverse literary works in Chinese and Urdu. For access to the code and replication of our experiments, please visit (https://github.com/ANON13222/ITR).

------------

`[2407.03525] UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization <https://arxiv.org/abs/2407.03525>`__ UnSeenTimeQA:超越llm记忆的时间敏感问题回答

::

    Wed, 3 Jul 2024 22:02:07 GMT
    Md Nayem Uddin, Amir Saeidi, Divij Handa, Agastya Seth, Tran Cao Son, Eduardo Blanco, Steven R. Corman, Chitta Baral

This paper introduces UnSeenTimeQA, a novel time-sensitive question-answering (TSQA) benchmark that diverges from traditional TSQA benchmarks by avoiding factual and web-searchable queries. We present a series of time-sensitive event scenarios decoupled from real-world factual information. It requires large language models (LLMs) to engage in genuine temporal reasoning, disassociating from the knowledge acquired during the pre-training phase. Our evaluation of six open-source LLMs (ranging from 2B to 70B in size) and three closed-source LLMs reveal that the questions from the UnSeenTimeQA present substantial challenges. This indicates the models' difficulties in handling complex temporal reasoning scenarios. Additionally, we present several analyses shedding light on the models' performance in answering time-sensitive questions.

------------

`[2407.03536] Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias <https://arxiv.org/abs/2407.03536>`__ 

::

    Wed, 3 Jul 2024 22:45:36 GMT
    Jayanta Sadhu, Maneesha Rani Saha, Rifat Shahriyar

The rapid growth of Large Language Models (LLMs) has put forward the study of biases as a crucial field. It is important to assess the influence of different types of biases embedded in LLMs to ensure fair use in sensitive fields.
Although there have been extensive works on bias assessment in English, such efforts are rare and scarce for a major language like Bangla. In this work, we examine two types of social biases in LLM generated outputs for Bangla language. Our main contributions in this work are: (1) bias studies on two different social biases for Bangla (2) a curated dataset for bias measurement benchmarking (3) two different probing techniques for bias detection in the context of Bangla. This is the first work of such kind involving bias assessment of LLMs for Bangla to the best of our knowledge. All our code and resources are publicly available for the progress of bias related research in Bangla NLP.

------------

`[2407.03572] Core: Robust Factual Precision Scoring with Informative Sub-Claim Identification <https://arxiv.org/abs/2407.03572>`__ 核心:具有信息性子索赔识别的鲁棒事实精度评分

::

    Thu, 4 Jul 2024 01:51:38 GMT
    Zhengping Jiang, Jingyu Zhang, Nathaniel Weir, Seth Ebner, Miriam Wanner, Kate Sanders, Daniel Khashabi, Anqi Liu and Benjamin Van Durme

Hallucinations -- the generation of untrue claims -- pose a challenge to the application of large language models (LLMs) [1] thereby motivating the development of metrics to evaluate factual precision. We observe that popular metrics using the Decompose-Then-Verify framework, such as FActScore [2], can be manipulated by adding obvious or repetitive claims to artificially inflate scores. We expand the FActScore dataset to design and analyze factual precision metrics, demonstrating that models can be trained to achieve high scores under existing metrics through exploiting the issues we identify. This motivates our new customizable plug-and-play subclaim selection component called Core, which filters down individual subclaims according to their uniqueness and informativeness. Metrics augmented by Core are substantially more robust as shown in head-to-head comparisons. We release an evaluation framework supporting the modular use of Core (https://github.com/zipJiang/Core) and various decomposition strategies, and we suggest its adoption by the LLM community.
[1] Hong et al., "The Hallucinations Leaderboard -- An Open Effort to Measure Hallucinations in Large Language Models", arXiv:2404.05904v2 [cs.CL].
[2] Min et al., "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation", arXiv:2305.14251v2 [cs.CL].

------------

`[2407.03582] Integrating Randomness in Large Language Models: A Linear Congruential Generator Approach for Generating Clinically Relevant Content <https://arxiv.org/abs/2407.03582>`__ 在大型语言模型中整合随机性:用于生成临床相关内容的线性同余生成器方法

::

    Thu, 4 Jul 2024 02:21:47 GMT
    Andrew Bouras

Generating diverse, high-quality outputs from language models is crucial for applications in education and content creation. Achieving true randomness and avoiding repetition remains a significant challenge. This study uses the Linear Congruential Generator method for systematic fact selection, combined with AI-powered content generation. We ensured unique combinations of gastrointestinal physiology and pathology facts across multiple rounds, integrating these facts into prompts for GPT-4o to create clinically relevant, vignette-style outputs. Over 14 rounds, 98 unique outputs were generated, demonstrating LCG's effectiveness in producing diverse and high-quality content. This method addresses key issues of randomness and repetition, enhancing the quality and efficiency of language model-generated content for various applications.

------------

`[2407.03615] Visualizing Dialogues: Enhancing Image Selection through Dialogue Understanding with Large Language Models <https://arxiv.org/abs/2407.03615>`__ 可视化对话:基于大型语言模型对话理解增强图像选择

::

    Thu, 4 Jul 2024 03:50:30 GMT
    Chang-Sheng Kao, Yun-Nung Chen

Recent advancements in dialogue systems have highlighted the significance of integrating multimodal responses, which enable conveying ideas through diverse modalities rather than solely relying on text-based interactions. This enrichment not only improves overall communicative efficacy but also enhances the quality of conversational experiences. However, existing methods for dialogue-to-image retrieval face limitations due to the constraints of pre-trained vision language models (VLMs) in comprehending complex dialogues accurately. To address this, we present a novel approach leveraging the robust reasoning capabilities of large language models (LLMs) to generate precise dialogue-associated visual descriptors, facilitating seamless connection with images. Extensive experiments conducted on benchmark data validate the effectiveness of our proposed approach in deriving concise and accurate visual descriptors, leading to significant enhancements in dialogue-to-image retrieval performance. Furthermore, our findings demonstrate the method's generalizability across diverse visual cues, various LLMs, and different datasets, underscoring its practicality and potential impact in real-world applications.

------------

`[2407.03621] The Mysterious Case of Neuron 1512: Injectable Realignment Architectures Reveal Internal Characteristics of Meta's Llama 2 Model <https://arxiv.org/abs/2407.03621>`__ 神经元1512的神秘案例:可注射的重组架构揭示了Meta的Llama 2模型的内部特征

::

    Thu, 4 Jul 2024 04:05:19 GMT
    Brenden Smith, Dallin Baker, Clayton Chase, Myles Barney, Kaden Parker, Makenna Allred, Peter Hu, Alex Evans, Nancy Fulda

Large Language Models (LLMs) have an unrivaled and invaluable ability to "align" their output to a diverse range of human preferences, by mirroring them in the text they generate. The internal characteristics of such models, however, remain largely opaque. This work presents the Injectable Realignment Model (IRM) as a novel approach to language model interpretability and explainability. Inspired by earlier work on Neural Programming Interfaces, we construct and train a small network -- the IRM -- to induce emotion-based alignments within a 7B parameter LLM architecture. The IRM outputs are injected via layerwise addition at various points during the LLM's forward pass, thus modulating its behavior without changing the weights of the original model.
This isolates the alignment behavior from the complex mechanisms of the transformer model. Analysis of the trained IRM's outputs reveals a curious pattern. Across more than 24 training runs and multiple alignment datasets, patterns of IRM activations align themselves in striations associated with a neuron's index within each transformer layer, rather than being associated with the layers themselves. Further, a single neuron index (1512) is strongly correlated with all tested alignments. This result, although initially counterintuitive, is directly attributable to design choices present within almost all commercially available transformer architectures, and highlights a potential weak point in Meta's pretrained Llama 2 models. It also demonstrates the value of the IRM architecture for language model analysis and interpretability. Our code and datasets are available at https://github.com/DRAGNLabs/injectable-alignment-model

------------

`[2407.03651] Evaluating Language Model Context Windows: A "Working Memory" Test and Inference-time Correction <https://arxiv.org/abs/2407.03651>`__ 评估语言模型上下文窗口:“工作记忆”测试和推理时间校正

::

    Thu, 4 Jul 2024 05:46:20 GMT
    Amanda Dsouza, Christopher Glaze, Changho Shin, Frederic Sala

Large language models are prominently used in real-world applications, often tasked with reasoning over large volumes of documents. An exciting development in this space is models boasting extended context capabilities, with some accommodating over 2 million tokens. Such long context model capabilities remain uncertain in production systems, motivating the need to benchmark their performance on real world use cases. We address this challenge by proposing SWiM, an evaluation framework that addresses the limitations of standard tests.
Testing the framework on eight long context models, we find that even strong models such as GPT-4 and Claude 3 Opus degrade in performance when information is present in the middle of the context window (lost-in-the-middle effect).
Next, in addition to our benchmark, we propose medoid voting, a simple, but effective training-free approach that helps alleviate this effect, by generating responses a few times, each time randomly permuting documents in the context, and selecting the medoid answer. We evaluate medoid voting on single document QA tasks, achieving up to a 24% lift in accuracy.

------------

`[2407.03658] GPT-4 vs. Human Translators: A Comprehensive Evaluation of Translation Quality Across Languages, Domains, and Expertise Levels <https://arxiv.org/abs/2407.03658>`__ GPT-4 vs.人工翻译:跨语言、领域和专业水平的翻译质量综合评估

::

    Thu, 4 Jul 2024 05:58:04 GMT
    Jianhao Yan, Pingchuan Yan, Yulong Chen, Judy Li, Xianchao Zhu, Yue Zhang

This study comprehensively evaluates the translation quality of Large Language Models (LLMs), specifically GPT-4, against human translators of varying expertise levels across multiple language pairs and domains. Through carefully designed annotation rounds, we find that GPT-4 performs comparably to junior translators in terms of total errors made but lags behind medium and senior translators. We also observe the imbalanced performance across different languages and domains, with GPT-4's translation capability gradually weakening from resource-rich to resource-poor directions. In addition, we qualitatively study the translation given by GPT-4 and human translators, and find that GPT-4 translator suffers from literal translations, but human translators sometimes overthink the background information. To our knowledge, this study is the first to evaluate LLMs against human translators and analyze the systematic differences between their outputs, providing valuable insights into the current state of LLM-based translation and its potential limitations.

------------

`[2407.03678] Improving Self Consistency in LLMs through Probabilistic Tokenization <https://arxiv.org/abs/2407.03678>`__ 

::

    Thu, 4 Jul 2024 06:52:48 GMT
    Ashutosh Sathe, Divyanshu Aggarwal, Sunayana Sitaram

Prior research has demonstrated noticeable performance gains through the use of probabilistic tokenizations, an approach that involves employing multiple tokenizations of the same input string during the training phase of a language model. Despite these promising findings, modern large language models (LLMs) have yet to be trained using probabilistic tokenizations. Interestingly, while the tokenizers of these contemporary LLMs have the capability to generate multiple tokenizations, this property remains underutilized.
In this work, we propose a novel method to leverage the multiple tokenization capabilities of modern LLM tokenizers, aiming to enhance the self-consistency of LLMs in reasoning tasks. Our experiments indicate that when utilizing probabilistic tokenizations, LLMs generate logically diverse reasoning paths, moving beyond mere surface-level linguistic diversity.We carefully study probabilistic tokenization and offer insights to explain the self consistency improvements it brings through extensive experimentation on 5 LLM families and 4 reasoning benchmarks.

------------

`[2407.03689] Text2TimeSeries: Enhancing Financial Forecasting through Time Series Prediction Updates with Event-Driven Insights from Large Language Models <https://arxiv.org/abs/2407.03689>`__ Text2TimeSeries:通过来自大型语言模型的事件驱动见解的时间序列预测更新来增强金融预测

::

    Thu, 4 Jul 2024 07:21:38 GMT
    Litton Jose Kurisinkel, Pruthwik Mishra, Yue Zhang

Time series models, typically trained on numerical data, are designed to forecast future values. These models often rely on weighted averaging techniques over time intervals. However, real-world time series data is seldom isolated and is frequently influenced by non-numeric factors. For instance, stock price fluctuations are impacted by daily random events in the broader world, with each event exerting a unique influence on price signals.
Previously, forecasts in financial markets have been approached in two main ways: either as time-series problems over price sequence or sentiment analysis tasks. The sentiment analysis tasks aim to determine whether news events will have a positive or negative impact on stock prices, often categorizing them into discrete labels. Recognizing the need for a more comprehensive approach to accurately model time series prediction, we propose a collaborative modeling framework that incorporates textual information about relevant events for predictions. Specifically, we leverage the intuition of large language models about future changes to update real number time series predictions. We evaluated the effectiveness of our approach on financial market data.

------------

`[2407.03805] Cognitive Modeling with Scaffolded LLMs: A Case Study of Referential Expression Generation <https://arxiv.org/abs/2407.03805>`__ 基于脚手架llm的认知建模:参考表达生成案例研究

::

    Thu, 4 Jul 2024 10:28:48 GMT
    Polina Tsvilodub, Michael Franke, Fausto Carcassi

To what extent can LLMs be used as part of a cognitive model of language generation? In this paper, we approach this question by exploring a neuro-symbolic implementation of an algorithmic cognitive model of referential expression generation by Dale & Reiter (1995). The symbolic task analysis implements the generation as an iterative procedure that scaffolds symbolic and gpt-3.5-turbo-based modules. We compare this implementation to an ablated model and a one-shot LLM-only baseline on the A3DS dataset (Tsvilodub & Franke, 2023). We find that our hybrid approach is cognitively plausible and performs well in complex contexts, while allowing for more open-ended modeling of language generation in a larger domain.

------------

`[2407.03850] HYBRINFOX at CheckThat! 2024 -- Task 1: Enhancing Language Models with Structured Information for Check-Worthiness Estimation <https://arxiv.org/abs/2407.03850>`__ HYBRINFOX在CheckThat!2024—任务1:用结构化信息增强语言模型以进行检查价值估计

::

    Thu, 4 Jul 2024 11:33:54 GMT
    G\'eraud Faye, Morgane Casanova, Benjamin Icard, Julien Chanson, Guillaume Gadek, Guillaume Gravier, Paul \'Egr\'e

This paper summarizes the experiments and results of the HYBRINFOX team for the CheckThat! 2024 - Task 1 competition. We propose an approach enriching Language Models such as RoBERTa with embeddings produced by triples (subject ; predicate ; object) extracted from the text sentences. Our analysis of the developmental data shows that this method improves the performance of Language Models alone. On the evaluation data, its best performance was in English, where it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On the other languages (Dutch and Arabic), it obtained more mixed results. Future research tracks are identified toward adapting this processing pipeline to more recent Large Language Models.

------------

`[2407.03859] Anthropocentric bias and the possibility of artificial cognition <https://arxiv.org/abs/2407.03859>`__ 人类中心偏见和人工认知的可能性

::

    Thu, 4 Jul 2024 11:44:28 GMT
    Rapha\"el Milli\`ere, Charles Rathkopf

Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence (Type-I), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent (Type-II). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies.

------------

`[2407.03937] TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models <https://arxiv.org/abs/2407.03937>`__ 铜鼓:基于知识基础的大型语言模型掌握文言文理解

::

    Thu, 4 Jul 2024 13:52:23 GMT
    Jiahuan Cao, Dezhi Peng, Peirong Zhang, Yongxin Shi, Yang Liu, Kai Ding, Lianwen Jin

Classical Chinese is a gateway to the rich heritage and wisdom of ancient China, yet its complexities pose formidable comprehension barriers for most modern people without specialized knowledge. While Large Language Models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), they struggle with Classical Chinese Understanding (CCU), especially in data-demanding and knowledge-intensive tasks. In response to this dilemma, we propose \textbf{TongGu} (mean understanding ancient and modern), the first CCU-specific LLM, underpinned by three core contributions. First, we construct a two-stage instruction-tuning dataset ACCN-INS derived from rich classical Chinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we propose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting, enabling TongGu to acquire new capabilities while preserving its foundational knowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG) technique to reduce hallucinations based on knowledge-grounding. Extensive experiments across 24 diverse CCU tasks validate TongGu's superior ability, underscoring the effectiveness of RAT and CCU-RAG. The model and dataset will be public available.

------------

`[2407.03952] A framework for annotating and modelling intentions behind metaphor use <https://arxiv.org/abs/2407.03952>`__ 

::

    Thu, 4 Jul 2024 14:13:57 GMT
    Gianluca Michelli and Xiaoyu Tong and Ekaterina Shutova

Metaphors are part of everyday language and shape the way in which we conceptualize the world. Moreover, they play a multifaceted role in communication, making their understanding and generation a challenging task for language models (LMs). While there has been extensive work in the literature linking metaphor to the fulfilment of individual intentions, no comprehensive taxonomy of such intentions, suitable for natural language processing (NLP) applications, is available to present day. In this paper, we propose a novel taxonomy of intentions commonly attributed to metaphor, which comprises 9 categories. We also release the first dataset annotated for intentions behind metaphor use. Finally, we use this dataset to test the capability of large language models (LLMs) in inferring the intentions behind metaphor use, in zero- and in-context few-shot settings. Our experiments show that this is still a challenge for LLMs.

------------

`[2407.03963] LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs <https://arxiv.org/abs/2407.03963>`__ LLM-jp:一个研究和开发完全开放的日本llm的跨组织项目

::

    Thu, 4 Jul 2024 14:33:03 GMT
    LLM-jp: Akiko Aizawa, Eiji Aramaki, Bowen Chen, Fei Cheng, Hiroyuki Deguchi, Rintaro Enomoto, Kazuki Fujii, Kensuke Fukumoto, Takuya Fukushima, Namgi Han, Yuto Harada, Chikara Hashimoto, Tatsuya Hiraoka, Shohei Hisada, Sosuke Hosokawa, Lu Jie, Keisuke Kamata, Teruhito Kanazawa, Hiroki Kanezashi, Hiroshi Kataoka, Satoru Katsumata, Daisuke Kawahara, Seiya Kawano, Atsushi Keyaki, Keisuke Kiryu, Hirokazu Kiyomaru, Takashi Kodama, Takahiro Kubo, Yohei Kuga, Ryoma Kumon, Shuhei Kurita, Sadao Kurohashi, Conglong Li, Taiki Maekawa, Hiroshi Matsuda, Yusuke Miyao, Kentaro Mizuki, Sakae Mizuki, Yugo Murawaki, Ryo Nakamura, Taishi Nakamura, Kouta Nakayama, Tomoka Nakazato, Takuro Niitsuma, Jiro Nishitoba, Yusuke Oda, Hayato Ogawa, Takumi Okamoto, Naoaki Okazaki, Yohei Oseki, Shintaro Ozaki, Koki Ryu, Rafal Rzepka, Keisuke Sakaguchi, Shota Sasaki, Satoshi Sekine, Kohei Suda, Saku Sugawara, Issa Sugiura, Hiroaki Sugiyama, Hisami Suzuki, Jun Suzuki, Toyotaro Suzumura, Kensuke Tachibana, Yu Takagi, Kyosuke Takami, Koichi Takeda, Masashi Takeshita, Masahiro Tanaka, Kenjiro Taura, Arseny Tolmachev, Nobuhiro Ueda, Zhen Wan, Shuntaro Yada, Sakiko Yahata, Yuya Yamamoto, Yusuke Yamauchi, Hitomi Yanaka, Rio Yokota, Koichiro Yoshino

This paper introduces LLM-jp, a cross-organizational project for the research and development of Japanese large language models (LLMs). LLM-jp aims to develop open-source and strong Japanese LLMs, and as of this writing, more than 1,500 participants from academia and industry are working together for this purpose. This paper presents the background of the establishment of LLM-jp, summaries of its activities, and technical reports on the LLMs developed by LLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/.

------------

`[2407.03964] Improving Sample Efficiency of Reinforcement Learning with Background Knowledge from Large Language Models <https://arxiv.org/abs/2407.03964>`__ 利用大型语言模型背景知识提高强化学习的样本效率

::

    Thu, 4 Jul 2024 14:33:47 GMT
    Fuxiang Zhang, Junyou Li, Yi-Chen Li, Zongzhang Zhang, Yang Yu, Deheng Ye

Low sample efficiency is an enduring challenge of reinforcement learning (RL). With the advent of versatile large language models (LLMs), recent works impart common-sense knowledge to accelerate policy learning for RL processes.
However, we note that such guidance is often tailored for one specific task but loses generalizability. In this paper, we introduce a framework that harnesses LLMs to extract background knowledge of an environment, which contains general understandings of the entire environment, making various downstream RL tasks benefit from one-time knowledge representation. We ground LLMs by feeding a few pre-collected experiences and requesting them to delineate background knowledge of the environment. Afterward, we represent the output knowledge as potential functions for potential-based reward shaping, which has a good property for maintaining policy optimality from task rewards. We instantiate three variants to prompt LLMs for background knowledge, including writing code, annotating preferences, and assigning goals. Our experiments show that these methods achieve significant sample efficiency improvements in a spectrum of downstream tasks from Minigrid and Crafter domains.

------------

`[2407.03974] LLM Roleplay: Simulating Human-Chatbot Interaction <https://arxiv.org/abs/2407.03974>`__ LLM角色扮演:模拟人-聊天机器人交互

::

    Thu, 4 Jul 2024 14:49:46 GMT
    Hovhannes Tamoyan, Hendrik Schuff, Iryna Gurevych

The development of chatbots requires collecting a large number of human-chatbot dialogues to reflect the breadth of users' sociodemographic backgrounds and conversational goals. However, the resource requirements to conduct the respective user studies can be prohibitively high and often only allow for a narrow analysis of specific dialogue goals and participant demographics. In this paper, we propose LLM-Roleplay: a goal-oriented, persona-based method to automatically generate diverse multi-turn dialogues simulating human-chatbot interaction. LLM-Roleplay can be applied to generate dialogues with any type of chatbot and uses large language models (LLMs) to play the role of textually described personas. To validate our method we collect natural human-chatbot dialogues from different sociodemographic groups and conduct a human evaluation to compare real human-chatbot dialogues with our generated dialogues. We compare the abilities of state-of-the-art LLMs in embodying personas and holding a conversation and find that our method can simulate human-chatbot dialogues with a high indistinguishability rate.

------------

`[2407.03994] Unlocking the Potential of Model Merging for Low-Resource Languages <https://arxiv.org/abs/2407.03994>`__ 释放低资源语言模型合并的潜力

::

    Thu, 4 Jul 2024 15:14:17 GMT
    Mingxu Tao, Chen Zhang, Quzhe Huang, Tianyao Ma, Songfang Huang, Dongyan Zhao, Yansong Feng

Adapting large language models (LLMs) to new languages typically involves continual pre-training (CT) followed by supervised fine-tuning (SFT). However, this CT-then-SFT approach struggles with limited data in the context of low-resource languages, failing to balance language modeling and task-solving capabilities. We thus propose model merging as an alternative for low-resource languages, combining models with distinct capabilities into a single model without additional training. We use model merging to develop task-solving LLMs for low-resource languages without SFT data in the target languages. Our experiments based on Llama-2-7B demonstrate that model merging effectively endows LLMs for low-resource languages with task-solving abilities, outperforming CT-then-SFT in scenarios with extremely scarce data. Observing performance saturation in model merging with more training tokens, we further analyze the merging process and introduce a slack variable to the model merging algorithm to mitigate the loss of important parameters, thereby enhancing performance. We hope that model merging can benefit more human languages suffering from data scarcity with its higher data efficiency.

------------

`[2407.04020] LLMAEL: Large Language Models are Good Context Augmenters for Entity Linking <https://arxiv.org/abs/2407.04020>`__ LLMAEL:大型语言模型是实体链接的良好上下文增强器

::

    Thu, 4 Jul 2024 15:55:13 GMT
    Amy Xin, Yunjia Qi, Zijun Yao, Fangwei Zhu, Kaisheng Zeng, Xu Bin, Lei Hou, Juanzi Li

Entity Linking (EL) models are well-trained at mapping mentions to their corresponding entities according to a given context. However, EL models struggle to disambiguate long-tail entities due to their limited training data.
Meanwhile, large language models (LLMs) are more robust at interpreting uncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at generating correct entity IDs. Furthermore, training an LLM to perform EL is cost-intensive. Building upon these insights, we introduce LLM-Augmented Entity Linking LLMAEL, a plug-and-play approach to enhance entity linking through LLM data augmentation. We leverage LLMs as knowledgeable context augmenters, generating mention-centered descriptions as additional input, while preserving traditional EL models for task specific processing. Experiments on 6 standard datasets show that the vanilla LLMAEL outperforms baseline EL models in most cases, while the fine-tuned LLMAEL set the new state-of-the-art results across all 6 benchmarks.

------------

`[2407.04046] Systematic Task Exploration with LLMs: A Study in Citation Text Generation <https://arxiv.org/abs/2407.04046>`__ LLMs的系统任务探索:引文文本生成研究

::

    Thu, 4 Jul 2024 16:41:08 GMT
    Furkan \c{S}ahinu\c{c}, Ilia Kuznetsov, Yufang Hou, Iryna Gurevych

Large language models (LLMs) bring unprecedented flexibility in defining and executing complex, creative natural language generation (NLG) tasks. Yet, this flexibility brings new challenges, as it introduces new degrees of freedom in formulating the task inputs and instructions and in evaluating model performance. To facilitate the exploration of creative NLG tasks, we propose a three-component research framework that consists of systematic input manipulation, reference data, and output measurement. We use this framework to explore citation text generation -- a popular scholarly NLP task that lacks consensus on the task definition and evaluation metric and has not yet been tackled within the LLM paradigm. Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs, and reveal non-trivial relationships between different evaluation metrics used for citation text generation. Additional human generation and human evaluation experiments provide new qualitative insights into the task to guide future research in citation text generation. We make our code and data publicly available.

------------

`[2407.04067] Semantic Graphs for Syntactic Simplification: A Revisit from the Age of LLM <https://arxiv.org/abs/2407.04067>`__ 

::

    Thu, 4 Jul 2024 17:13:38 GMT
    Peiran Yao, Kostyantyn Guzhva, Denilson Barbosa

Symbolic sentence meaning representations, such as AMR (Abstract Meaning Representation) provide expressive and structured semantic graphs that act as intermediates that simplify downstream NLP tasks. However, the instruction-following capability of large language models (LLMs) offers a shortcut to effectively solve NLP tasks, questioning the utility of semantic graphs. Meanwhile, recent work has also shown the difficulty of using meaning representations merely as a helpful auxiliary for LLMs. We revisit the position of semantic graphs in syntactic simplification, the task of simplifying sentence structures while preserving their meaning, which requires semantic understanding, and evaluate it on a new complex and natural dataset. The AMR-based method that we propose, AMRS$^3$, demonstrates that state-of-the-art meaning representations can lead to easy-to-implement simplification methods with competitive performance and unique advantages in cost, interpretability, and generalization. With AMRS$^3$ as an anchor, we discover that syntactic simplification is a task where semantic graphs are helpful in LLM prompting. We propose AMRCoC prompting that guides LLMs to emulate graph algorithms for explicit symbolic reasoning on AMR graphs, and show its potential for improving LLM on semantic-centered tasks like syntactic simplification.

------------

`[2407.04093] Stephanie: Step-by-Step Dialogues for Mimicking Human Interactions in Social Conversations <https://arxiv.org/abs/2407.04093>`__ Stephanie:在社交对话中模仿人类互动的步骤对话

::

    Thu, 4 Jul 2024 17:59:41 GMT
    Hao Yang, Hongyuan Lu, Xinhua Zeng, Yang Liu, Xiang Zhang, Haoran Yang, Yumeng Zhang, Yiran Wei, Wai Lam

In the rapidly evolving field of natural language processing, dialogue systems primarily employ a single-step dialogue paradigm. Although this paradigm is efficient, it lacks the depth and fluidity of human interactions and does not appear natural. We introduce a novel \textbf{Step}-by-Step Dialogue Paradigm (Stephanie), designed to mimic the ongoing dynamic nature of human conversations. By employing a dual learning strategy and a further-split post-editing method, we generated and utilized a high-quality step-by-step dialogue dataset to fine-tune existing large language models, enabling them to perform step-by-step dialogues. We thoroughly present Stephanie. Tailored automatic and human evaluations are conducted to assess its effectiveness compared to the traditional single-step dialogue paradigm. We will release code, Stephanie datasets, and Stephanie LLMs to facilitate the future of chatbot eras.

------------

`[2407.04118] MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization <https://arxiv.org/abs/2407.04118>`__ MAPO:基于模型自适应提示优化的大型语言模型性能提升

::

    Thu, 4 Jul 2024 18:39:59 GMT
    Yuyan Chen, Zhihao Wen, Ge Fan, Zhengyu Chen, Wei Wu, Dayiheng Liu, Zhixu Li, Bang Liu, Yanghua Xiao

Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.

------------

`[2407.04121] Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models <https://arxiv.org/abs/2407.04121>`__ 幻觉检测:在大型语言模型中鲁棒地识别可靠答案

::

    Thu, 4 Jul 2024 18:47:42 GMT
    Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li, Yanghua Xiao

Large Language Models (LLMs) have gained widespread adoption in various natural language processing tasks, including question answering and dialogue systems. However, a major drawback of LLMs is the issue of hallucination, where they generate unfaithful or inconsistent content that deviates from the input source, leading to severe consequences. In this paper, we propose a robust discriminator named RelD to effectively detect hallucination in LLMs' generated answers. RelD is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics. Our experimental results demonstrate that the proposed RelD successfully detects hallucination in the answers generated by diverse LLMs. Moreover, it performs well in distinguishing hallucination in LLMs' generated answers from both in-distribution and out-of-distribution datasets. Additionally, we also conduct a thorough analysis of the types of hallucinations that occur and present valuable insights. This research significantly contributes to the detection of reliable answers generated by LLMs and holds noteworthy implications for mitigating hallucination in the future work.

------------

`[2407.04125] Query-Guided Self-Supervised Summarization of Nursing Notes <https://arxiv.org/abs/2407.04125>`__ 基于查询引导的自我监督护理笔记摘要

::

    Thu, 4 Jul 2024 18:54:30 GMT
    Ya Gao, Hans Moen, Saila Koivusalo, Miika Koskinen, Pekka Marttinen

Nursing notes, an important component of Electronic Health Records (EHRs), keep track of the progression of a patient's health status during a care episode. Distilling the key information in nursing notes through text summarization techniques can improve clinicians' efficiency in understanding patients' conditions when reviewing nursing notes. However, existing abstractive summarization methods in the clinical setting have often overlooked nursing notes and require the creation of reference summaries for supervision signals, which is time-consuming. In this work, we introduce QGSumm, a query-guided self-supervised domain adaptation framework for nursing note summarization. Using patient-related clinical queries as guidance, our approach generates high-quality, patient-centered summaries without relying on reference summaries for training. Through automatic and manual evaluation by an expert clinician, we demonstrate the strengths of our approach compared to the state-of-the-art Large Language Models (LLMs) in both zero-shot and few-shot settings. Ultimately, our approach provides a new perspective on conditional text summarization, tailored to the specific interests of clinical personnel.

------------

`[2407.04130] Towards Automating Text Annotation: A Case Study on Semantic Proximity Annotation using GPT-4 <https://arxiv.org/abs/2407.04130>`__ 迈向文本标注自动化:基于GPT-4的语义接近标注案例研究

::

    Thu, 4 Jul 2024 19:16:44 GMT
    Sachin Yadav, Tejaswi Choppa, Dominik Schlechtweg

This paper explores using GPT-3.5 and GPT-4 to automate the data annotation process with automatic prompting techniques. The main aim of this paper is to reuse human annotation guidelines along with some annotated data to design automatic prompts for LLMs, focusing on the semantic proximity annotation task.
Automatic prompts are compared to customized prompts. We further implement the prompting strategies into an open-source text annotation tool, enabling easy online use via the OpenAI API. Our study reveals the crucial role of accurate prompt design and suggests that prompting GPT-4 with human-like instructions is not straightforwardly possible for the semantic proximity task. We show that small modifications to the human guidelines already improve the performance, suggesting possible ways for future research.

------------

`[2407.04151] Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers <https://arxiv.org/abs/2407.04151>`__ 针对分布式后门触发器保护多轮对话语言模型

::

    Thu, 4 Jul 2024 20:57:06 GMT
    Terry Tong, Jiashu Xu, Qin Liu, Muhao Chen

The security of multi-turn conversational large language models (LLMs) is understudied despite it being one of the most popular LLM utilization.
Specifically, LLMs are vulnerable to data poisoning backdoor attacks, where an adversary manipulates the training data to cause the model to output malicious responses to predefined triggers. Specific to the multi-turn dialogue setting, LLMs are at the risk of even more harmful and stealthy backdoor attacks where the backdoor triggers may span across multiple utterances, giving lee-way to context-driven attacks. In this paper, we explore a novel distributed backdoor trigger attack that serves to be an extra tool in an adversary's toolbox that can interface with other single-turn attack strategies in a plug and play manner. Results on two representative defense mechanisms indicate that distributed backdoor triggers are robust against existing defense strategies which are designed for single-turn user-model interactions, motivating us to propose a new defense strategy for the multi-turn dialogue setting that is more challenging. To this end, we also explore a novel contrastive decoding based defense that is able to mitigate the backdoor with a low computational tradeoff.

------------

`[2407.04179] Defense Against Syntactic Textual Backdoor Attacks with Token Substitution <https://arxiv.org/abs/2407.04179>`__ 用标记替换防御语法文本后门攻击

::

    Thu, 4 Jul 2024 22:48:57 GMT
    Xinglin Li, Xianwen He, Yao Li, Minhao Cheng

Textual backdoor attacks present a substantial security risk to Large Language Models (LLM). It embeds carefully chosen triggers into a victim model at the training stage, and makes the model erroneously predict inputs containing the same triggers as a certain class. Prior backdoor defense methods primarily target special token-based triggers, leaving syntax-based triggers insufficiently addressed. To fill this gap, this paper proposes a novel online defense algorithm that effectively counters syntax-based as well as special token-based backdoor attacks. The algorithm replaces semantically meaningful words in sentences with entirely different ones but preserves the syntactic templates or special tokens, and then compares the predicted labels before and after the substitution to determine whether a sentence contains triggers.
Experimental results confirm the algorithm's performance against these two types of triggers, offering a comprehensive defense strategy for model integrity.

------------

`[2407.04183] Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality Norms <https://arxiv.org/abs/2407.04183>`__ 

::

    Thu, 4 Jul 2024 23:05:58 GMT
    Joshua Ashkinaze, Ruijia Guan, Laura Kurek, Eytan Adar, Ceren Budak, Eric Gilbert

Large language models (LLMs) are trained on broad corpora and then used in communities with specialized norms. Is providing LLMs with community rules enough for models to follow these norms? We evaluate LLMs' capacity to detect (Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's Neutral Point of View (NPOV) policy. LLMs struggled with bias detection, achieving only 64% accuracy on a balanced dataset. Models exhibited contrasting biases (some under- and others over-predicted bias), suggesting distinct priors about neutrality. LLMs performed better at generation, removing 79% of words removed by Wikipedia editors. However, LLMs made additional changes beyond Wikipedia editors' simpler neutralizations, resulting in high-recall but low-precision editing. Interestingly, crowdworkers rated AI rewrites as more neutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative analysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia editors but often made extraneous non-NPOV-related changes (such as grammar).
LLMs may apply rules in ways that resonate with the public but diverge from community experts. While potentially effective for generation, LLMs may reduce editor agency and increase moderation workload (e.g., verifying additions).
Even when rules are easy to articulate, having LLMs apply them like community members may still be difficult.

------------

`[2407.04185] HAF-RM: A Hybrid Alignment Framework for Reward Model Training <https://arxiv.org/abs/2407.04185>`__ HAF-RM:用于奖励模型训练的混合对齐框架

::

    Thu, 4 Jul 2024 23:26:56 GMT
    Shujun Liu, Xiaoyu Shen, Yuhang Lai, Siyuan Wang, Shengbin Yue, Zengfeng Huang, Xuanjing Huang, Zhongyu Wei

The reward model has become increasingly important in alignment, assessment, and data construction for large language models (LLMs). Most existing researchers focus on enhancing reward models through data improvements, following the conventional training framework for reward models that directly optimizes the predicted rewards. In this paper, we propose a hybrid alignment framework HaF-RM for reward model training by introducing an additional constraint on token-level policy probabilities in addition to the reward score.
It can simultaneously supervise the internal preference model at the token level and optimize the mapping layer of the reward model at the sequence level.
Theoretical justifications and experiment results on five datasets show the validity and effectiveness of our proposed hybrid framework for training a high-quality reward model. By decoupling the reward modeling procedure and incorporating hybrid supervision, our HaF-RM framework offers a principled and effective approach to enhancing the performance and alignment of reward models, a critical component in the responsible development of powerful language models. We release our code at https://haf-rm.github.io.

------------

`[2407.04279] BiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks <https://arxiv.org/abs/2407.04279>`__ BiosERC:集成由llm支持的ERC任务传记演讲者

::

    Fri, 5 Jul 2024 06:25:34 GMT
    Jieying Xue, Minh Phuong Nguyen, Blake Matheny, Le Minh Nguyen

In the Emotion Recognition in Conversation task, recent investigations have utilized attention mechanisms exploring relationships among utterances from intra- and inter-speakers for modeling emotional interaction between them.
However, attributes such as speaker personality traits remain unexplored and present challenges in terms of their applicability to other tasks or compatibility with diverse model architectures. Therefore, this work introduces a novel framework named BiosERC, which investigates speaker characteristics in a conversation. By employing Large Language Models (LLMs), we extract the "biographical information" of the speaker within a conversation as supplementary knowledge injected into the model to classify emotional labels for each utterance. Our proposed method achieved state-of-the-art (SOTA) results on three famous benchmark datasets: IEMOCAP, MELD, and EmoryNLP, demonstrating the effectiveness and generalization of our model and showcasing its potential for adaptation to various conversation analysis tasks. Our source code is available at https://github.com/yingjie7/BiosERC.

------------

`[2407.04307] Crafting Large Language Models for Enhanced Interpretability <https://arxiv.org/abs/2407.04307>`__ 构建增强可解释性的大型语言模型

::

    Fri, 5 Jul 2024 07:22:44 GMT
    Chung-En Sun, Tuomas Oikarinen, Tsui-Wei Weng

We introduce the Concept Bottleneck Large Language Model (CB-LLM), a pioneering approach to creating inherently interpretable Large Language Models (LLMs). Unlike traditional black-box LLMs that rely on post-hoc interpretation methods with limited neuron function insights, CB-LLM sets a new standard with its built-in interpretability, scalability, and ability to provide clear, accurate explanations. This innovation not only advances transparency in language models but also enhances their effectiveness. Our unique Automatic Concept Correction (ACC) strategy successfully narrows the performance gap with conventional black-box LLMs, positioning CB-LLM as a model that combines the high accuracy of traditional LLMs with the added benefit of clear interpretability -- a feature markedly absent in existing LLMs.

------------

`[2407.04434] From 'Showgirls' to 'Performers': Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs <https://arxiv.org/abs/2407.04434>`__ 从“歌舞女郎”到“表演者”:用性别包容性语言进行微调，以减少llm中的偏见

::

    Fri, 5 Jul 2024 11:31:30 GMT
    Marion Bartl and Susan Leavy

Gender bias is not only prevalent in Large Language Models (LLMs) and their training data, but also firmly ingrained into the structural aspects of language itself. Therefore, adapting linguistic structures within LLM training data to promote gender-inclusivity can make gender representations within the model more inclusive. The focus of our work are gender-exclusive affixes in English, such as in 'show-girl' or 'man-cave', which can perpetuate gender stereotypes and binary conceptions of gender. We use an LLM training dataset to compile a catalogue of 692 gender-exclusive terms along with gender-neutral variants and from this, develop a gender-inclusive fine-tuning dataset, the 'Tiny Heap'. Fine-tuning three different LLMs with this dataset, we observe an overall reduction in gender-stereotyping tendencies across the models. Our approach provides a practical method for enhancing gender inclusivity in LLM training data and contributes to incorporating queer-feminist linguistic activism in bias mitigation research in NLP.

------------

`[2407.04459] Generalists vs. Specialists: Evaluating Large Language Models for Urdu <https://arxiv.org/abs/2407.04459>`__ 通才vs.专家:评估乌尔都语大型语言模型

::

    Fri, 5 Jul 2024 12:09:40 GMT
    Samee Arif, Abdul Hameed Azeemi, Agha Ali Raza, Awais Athar

In this paper, we compare general-purpose pretrained models, GPT-4-Turbo and Llama-3-8b-Instruct with special-purpose models fine-tuned on specific tasks, XLM-Roberta-large, mT5-large, and Llama-3-8b-Instruct. We focus on seven classification and six generation tasks to evaluate the performance of these models on Urdu language. Urdu has 70 million native speakers, yet it remains underrepresented in Natural Language Processing (NLP). Despite the frequent advancements in Large Language Models (LLMs), their performance in low-resource languages, including Urdu, still needs to be explored. We also conduct a human evaluation for the generation tasks and compare the results with the evaluations performed by GPT-4-Turbo and Llama-3-8b-Instruct. We find that special-purpose models consistently outperform general-purpose models across various tasks. We also find that the evaluation done by GPT-4-Turbo for generation tasks aligns more closely with human evaluation compared to the evaluation by Llama-3-8b-Instruct. This paper contributes to the NLP community by providing insights into the effectiveness of general and specific-purpose LLMs for low-resource languages.

------------

`[2407.04466] Using LLMs to label medical papers according to the CIViC evidence model <https://arxiv.org/abs/2407.04466>`__ 根据公民证据模型使用llm标记医学论文

::

    Fri, 5 Jul 2024 12:30:01 GMT
    Markus Hisch, Xing David Wang

We introduce the sequence classification problem CIViC Evidence to the field of medical NLP. CIViC Evidence denotes the multi-label classification problem of assigning labels of clinical evidence to abstracts of scientific papers which have examined various combinations of genomic variants, cancer types, and treatment approaches. We approach CIViC Evidence using different language models: We fine-tune pretrained checkpoints of BERT and RoBERTa on the CIViC Evidence dataset and challenge their performance with models of the same architecture which have been pretrained on domain-specific text. In this context, we find that BiomedBERT and BioLinkBERT can outperform BERT on CIViC Evidence (+0.8% and +0.9% absolute improvement in class-support weighted F1 score). All transformer-based models show a clear performance edge when compared to a logistic regression trained on bigram tf-idf scores (+1.5 - 2.7% improved F1 score). We compare the aforementioned BERT-like models to OpenAI's GPT-4 in a few-shot setting (on a small subset of our original test dataset), demonstrating that, without additional prompt-engineering or fine-tuning, GPT-4 performs worse on CIViC Evidence than our six fine-tuned models (66.1% weighted F1 score compared to 71.8% for the best fine-tuned model). However, performance gets reasonably close to the benchmark of a logistic regression model trained on bigram tf-idf scores (67.7% weighted F1 score).

------------

`[2407.04541] PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit Posts <https://arxiv.org/abs/2407.04541>`__ PoPreRo:用于预测罗马尼亚Reddit帖子流行度的新数据集

::

    Fri, 5 Jul 2024 14:28:12 GMT
    Ana-Cristina Rogoz, Maria Ilinca Nechita, Radu Tudor Ionescu

We introduce PoPreRo, the first dataset for Popularity Prediction of Romanian posts collected from Reddit. The PoPreRo dataset includes a varied compilation of post samples from five distinct subreddits of Romania, totaling 28,107 data samples. Along with our novel dataset, we introduce a set of competitive models to be used as baselines for future research. Interestingly, the top-scoring model achieves an accuracy of 61.35% and a macro F1 score of 60.60% on the test set, indicating that the popularity prediction task on PoPreRo is very challenging. Further investigations based on few-shot prompting the Falcon-7B Large Language Model also point in the same direction. We thus believe that PoPreRo is a valuable resource that can be used to evaluate models on predicting the popularity of social media posts in Romanian. We release our dataset at https://github.com/ana-rogoz/PoPreRo.

------------

`[2407.04629] Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework <https://arxiv.org/abs/2407.04629>`__ 

::

    Fri, 5 Jul 2024 16:38:23 GMT
    Reza Averly and Xia Ning

Clinical named entity recognition (NER) aims to retrieve important entities within clinical narratives. Recent works have demonstrated that large language models (LLMs) can achieve strong performance in this task. While previous works focus on proprietary LLMs, we investigate how open NER LLMs, trained specifically for entity recognition, perform in clinical NER. In this paper, we aim to improve them through a novel framework, entity decomposition with filtering, or EDF. Our key idea is to decompose the entity recognition task into several retrievals of sub-entity types. We also introduce a filtering mechanism to remove incorrect entities. Our experimental results demonstrate the efficacy of our framework across all metrics, models, datasets, and entity types. Our analysis reveals that entity decomposition can recognize previously missed entities with substantial improvement. We further provide a comprehensive evaluation of our framework and an in-depth error analysis to pave future works.

------------

`[2407.04693] ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models <https://arxiv.org/abs/2407.04693>`__ ANAH-v2:大型语言模型的尺度分析幻觉标注

::

    Fri, 5 Jul 2024 17:56:38 GMT
    Yuzhe Gu, Ziwei Ji, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen

Large language models (LLMs) exhibit hallucinations in long-form question-answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes, which struggle to scale due to prohibitive labor costs and insufficient reliability of existing hallucination annotators. To facilitate the scalable oversight of LLM hallucinations, this paper introduces an iterative self-training framework that simultaneously and progressively scales up the hallucination annotation dataset and improves the accuracy of the hallucination annotator. Based on the Expectation Maximization (EM) algorithm, in each iteration, the framework first applies a hallucination annotation pipeline to annotate a scaled dataset and then trains a more accurate hallucination annotator on the dataset. This new hallucination annotator is adopted in the hallucination annotation pipeline used for the next iteration. Extensive experimental results demonstrate that the finally obtained hallucination annotator with only 7B parameters surpasses the performance of GPT-4 and obtains new state-of-the-art hallucination detection results on HaluEval and HalluQA by zero-shot inference. Such an annotator can not only evaluate the hallucination levels of various LLMs on the large-scale dataset but also help to mitigate the hallucination of LLMs generations, with the Natural Language Inference (NLI) metric increasing from 25% to 37% on HaluEval.

------------

`[2407.04694] Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs <https://arxiv.org/abs/2407.04694>`__ 我、我自己和AI: llm的情景感知数据集(SAD)

::

    Fri, 5 Jul 2024 17:57:02 GMT
    Rudolf Laine, Bilal Chughtai, Jan Betley, Kaivalya Hariharan, Jeremy Scheurer, Mikita Balesni, Marius Hobbhahn, Alexander Meinke, Owain Evans

AI assistants such as ChatGPT are trained to respond to users by saying, "I am a large language model". This raises questions. Do such models know that they are LLMs and reliably act on this knowledge? Are they aware of their current circumstances, such as being deployed to the public? We refer to a model's knowledge of itself and its circumstances as situational awareness. To quantify situational awareness in LLMs, we introduce a range of behavioral tests, based on question answering and instruction following. These tests form the $\textbf{Situational Awareness Dataset (SAD)}$, a benchmark comprising 7 task categories and over 13,000 questions. The benchmark tests numerous abilities, including the capacity of LLMs to (i) recognize their own generated text, (ii) predict their own behavior, (iii) determine whether a prompt is from internal evaluation or real-world deployment, and (iv) follow instructions that depend on self-knowledge.
We evaluate 16 LLMs on SAD, including both base (pretrained) and chat models.
While all models perform better than chance, even the highest-scoring model (Claude 3 Opus) is far from a human baseline on certain tasks. We also observe that performance on SAD is only partially predicted by metrics of general knowledge (e.g. MMLU). Chat models, which are finetuned to serve as AI assistants, outperform their corresponding base models on SAD but not on general knowledge tasks. The purpose of SAD is to facilitate scientific understanding of situational awareness in LLMs by breaking it down into quantitative abilities. Situational awareness is important because it enhances a model's capacity for autonomous planning and action. While this has potential benefits for automation, it also introduces novel risks related to AI safety and control. Code and latest results available at https://situational-awareness-dataset.org .

------------

`[2407.03637] HERA: High-efficiency Matrix Compression via Element Replacement <https://arxiv.org/abs/2407.03637>`__ HERA:基于元素替换的高效矩阵压缩

::

    Thu, 4 Jul 2024 05:13:58 GMT
    Yanshu Wang, Wang Li, Tong Yang

Large Language Models (LLMs) have significantly advanced natural language processing tasks such as machine translation, text generation, and sentiment analysis. However, their large size, often consisting of billions of parameters, poses challenges for storage, computation, and deployment, particularly in resource-constrained environments like mobile devices and edge computing platforms. Additionally, the key-value (k-v) cache used to speed up query processing requires substantial memory and storage, exacerbating these challenges. Vector databases have emerged as a crucial technology to efficiently manage and retrieve the high-dimensional vectors produced by LLMs, facilitating faster data access and reducing computational demands.
Effective compression and quantization techniques are essential to address these challenges, as they reduce the memory footprint and computational requirements without significantly compromising performance. Traditional methods that uniformly map parameters to compressed spaces often fail to account for the uneven distribution of parameters, leading to considerable accuracy loss. Therefore, innovative approaches are needed to achieve better compression ratios while preserving model performance.
In this work, we propose HERA, a novel algorithm that employs heuristic Element Replacement for compressing matrix. HERA systematically replaces elements within the model using heuristic methods, which simplifies the structure of the model and makes subsequent compression more effective. By hierarchically segmenting, compressing, and reorganizing the matrix dataset, our method can effectively reduce the quantization error to 12.3% of the original at the same compression ratio.

------------

`[2407.03640] Generative Technology for Human Emotion Recognition: A Scope Review <https://arxiv.org/abs/2407.03640>`__ 面向人类情感识别的生成技术研究范围综述

::

    Thu, 4 Jul 2024 05:22:55 GMT
    Fei Ma, Yucheng Yuan, Yifan Xie, Hongwei Ren, Ivan Liu, Ying He, Fuji Ren, Fei Richard Yu, Shiguang Ni

Affective computing stands at the forefront of artificial intelligence (AI), seeking to imbue machines with the ability to comprehend and respond to human emotions. Central to this field is emotion recognition, which endeavors to identify and interpret human emotional states from different modalities, such as speech, facial images, text, and physiological signals. In recent years, important progress has been made in generative models, including Autoencoder, Generative Adversarial Network, Diffusion Model, and Large Language Model.
These models, with their powerful data generation capabilities, emerge as pivotal tools in advancing emotion recognition. However, up to now, there remains a paucity of systematic efforts that review generative technology for emotion recognition. This survey aims to bridge the gaps in the existing literature by conducting a comprehensive analysis of over 320 research papers until June 2024. Specifically, this survey will firstly introduce the mathematical principles of different generative models and the commonly used datasets. Subsequently, through a taxonomy, it will provide an in-depth analysis of how generative techniques address emotion recognition based on different modalities in several aspects, including data augmentation, feature extraction, semi-supervised learning, cross-domain, etc. Finally, the review will outline future research directions, emphasizing the potential of generative models to advance the field of emotion recognition and enhance the emotional intelligence of AI systems.

------------

`[2407.03674] Short-Long Policy Evaluation with Novel Actions <https://arxiv.org/abs/2407.03674>`__ 具有新颖行动的长短政策评估

::

    Thu, 4 Jul 2024 06:42:21 GMT
    Hyunji Alex Nam, Yash Chandak and Emma Brunskill

From incorporating LLMs in education, to identifying new drugs and improving ways to charge batteries, innovators constantly try new strategies in search of better long-term outcomes for students, patients and consumers. One major bottleneck in this innovation cycle is the amount of time it takes to observe the downstream effects of a decision policy that incorporates new interventions. The key question is whether we can quickly evaluate long-term outcomes of a new decision policy without making long-term observations.
Organizations often have access to prior data about past decision policies and their outcomes, evaluated over the full horizon of interest. Motivated by this, we introduce a new setting for short-long policy evaluation for sequential decision making tasks. Our proposed methods significantly outperform prior results on simulators of HIV treatment, kidney dialysis and battery charging.
We also demonstrate that our methods can be useful for applications in AI safety by quickly identifying when a new decision policy is likely to have substantially lower performance than past policies.

------------

`[2407.03856] Q-Adapter: Training Your LLM Adapter as a Residual Q-Function <https://arxiv.org/abs/2407.03856>`__ Q-Adapter:将LLM适配器训练为残差q函数

::

    Thu, 4 Jul 2024 11:42:36 GMT
    Yi-Chen Li, Fuxiang Zhang, Wenjie Qiu, Lei Yuan, Chengxing Jia, Zongzhang Zhang, Yang Yu

We consider the problem of adapting Large Language Models (LLMs) pre-trained with Reinforcement Learning from Human Feedback (RLHF) to downstream preference data. Naive approaches to achieve this could be supervised fine-tuning on preferred responses or reinforcement learning with a learned reward model.
However, the LLM runs the risk of forgetting its initial knowledge as the fine-tuning progresses. To customize the LLM while preserving its existing capabilities, this paper proposes a novel method, named as Q-Adapter. We start by formalizing LLM adaptation as a problem of maximizing the linear combination of two rewards, one of which corresponds to the reward optimized by the pre-trained LLM and the other to the downstream preference data. Although both rewards are unknown, we show that this can be solved by directly learning a new module from the preference data that approximates the \emph{residual Q-function}. We consider this module to be an adapter because the original pre-trained LLM, together with it, can form the optimal customised LLM.
Empirically, experiments on a range of domain-specific tasks and safety alignment tasks illustrate the superiority of Q-Adapter in both anti-forgetting and learning from new preferences.

------------

`[2407.03951] Uncertainty-Guided Optimization on Large Language Model Search Trees <https://arxiv.org/abs/2407.03951>`__ 不确定性引导的大型语言模型搜索树优化

::

    Thu, 4 Jul 2024 14:08:50 GMT
    Julia Grosse, Ruotian Wu, Ahmad Rashid, Philipp Hennig, Pascal Poupart, Agustinus Kristiadi

Beam search is a standard tree search algorithm when it comes to finding sequences of maximum likelihood, for example, in the decoding processes of large language models. However, it is myopic since it does not take the whole path from the root to a leaf into account. Moreover, it is agnostic to prior knowledge available about the process: For example, it does not consider that the objective being maximized is a likelihood and thereby has specific properties, like being bound in the unit interval. Taking a probabilistic approach, we define a prior belief over the LLMs' transition probabilities and obtain a posterior belief over the most promising paths in each iteration.
These beliefs are helpful to define a non-myopic Bayesian-optimization-like acquisition function that allows for a more data-efficient exploration scheme than standard beam search. We discuss how to select the prior and demonstrate in on- and off-model experiments with recent large language models, including Llama-2-7b, that our method achieves higher efficiency than beam search: Our method achieves the same or a higher likelihood while expanding fewer nodes than beam search.

------------

`[2407.04173] Quantifying Prediction Consistency Under Model Multiplicity in Tabular LLMs <https://arxiv.org/abs/2407.04173>`__ 表式llm中模型多样性下预测一致性的量化

::

    Thu, 4 Jul 2024 22:22:09 GMT
    Faisal Hamman, Pasan Dissanayake, Saumitra Mishra, Freddy Lecue, Sanghamitra Dutta

Fine-tuning large language models (LLMs) on limited tabular data for classification tasks can lead to \textit{fine-tuning multiplicity}, where equally well-performing models make conflicting predictions on the same inputs due to variations in the training process (i.e., seed, random weight initialization, retraining on additional or deleted samples). This raises critical concerns about the robustness and reliability of Tabular LLMs, particularly when deployed for high-stakes decision-making, such as finance, hiring, education, healthcare, etc. This work formalizes the challenge of fine-tuning multiplicity in Tabular LLMs and proposes a novel metric to quantify the robustness of individual predictions without expensive model retraining. Our metric quantifies a prediction's stability by analyzing (sampling) the model's local behavior around the input in the embedding space.
Interestingly, we show that sampling in the local neighborhood can be leveraged to provide probabilistic robustness guarantees against a broad class of fine-tuned models. By leveraging Bernstein's Inequality, we show that predictions with sufficiently high robustness (as defined by our measure) will remain consistent with high probability. We also provide empirical evaluation on real-world datasets to support our theoretical results. Our work highlights the importance of addressing fine-tuning instabilities to enable trustworthy deployment of LLMs in high-stakes and safety-critical applications.

------------

`[2407.04480] LoCo: Low-Bit Communication Adaptor for Large-scale Model Training <https://arxiv.org/abs/2407.04480>`__ LoCo:面向大规模模型训练的低比特通信适配器

::

    Fri, 5 Jul 2024 13:01:36 GMT
    Xingyu Xie, Zhijie Lin, Kim-Chuan Toh, Pan Zhou

To efficiently train large-scale models, low-bit gradient communication compresses full-precision gradients on local GPU nodes into low-precision ones for higher gradient synchronization efficiency among GPU nodes. However, it often degrades training quality due to compression information loss. To address this, we propose the Low-bit Communication Adaptor (LoCo), which compensates gradients on local GPU nodes before compression, ensuring efficient synchronization without compromising training quality. Specifically, LoCo designs a moving average of historical compensation errors to stably estimate concurrent compression error and then adopts it to compensate for the concurrent gradient compression, yielding a less lossless compression. This mechanism allows it to be compatible with general optimizers like Adam and sharding strategies like FSDP. Theoretical analysis shows that integrating LoCo into full-precision optimizers like Adam and SGD does not impair their convergence speed on nonconvex problems. Experimental results show that across large-scale model training frameworks like Megatron-LM and PyTorch's FSDP, LoCo significantly improves communication efficiency, e.g., improving Adam's training speed by 14% to 40% without performance degradation on large language models like LLAMAs and MoE.

------------

`[2407.04622] On scalable oversight with weak LLMs judging strong LLMs <https://arxiv.org/abs/2407.04622>`__ 

::

    Fri, 5 Jul 2024 16:29:15 GMT
    Zachary Kenton, Noah Y. Siegel, J\'anos Kram\'ar, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian, Rishabh Agarwal, David Lindner, Yunhao Tang, Noah D. Goodman, Rohin Shah

Scalable oversight protocols aim to enable humans to accurately supervise superhuman AI. In this paper we study debate, where two AI's compete to convince a judge; consultancy, where a single AI tries to convince a judge that asks questions; and compare to a baseline of direct question-answering, where the judge just answers outright without the AI. We use large language models (LLMs) as both AI agents and as stand-ins for human judges, taking the judge models to be weaker than agent models. We benchmark on a diverse range of asymmetries between judges and agents, extending previous work on a single extractive QA task with information asymmetry, to also include mathematics, coding, logic and multimodal reasoning asymmetries. We find that debate outperforms consultancy across all tasks when the consultant is randomly assigned to argue for the correct/incorrect answer. Comparing debate to direct question answering, the results depend on the type of task: in extractive QA tasks with information asymmetry debate outperforms direct question answering, but in other tasks without information asymmetry the results are mixed.
Previous work assigned debaters/consultants an answer to argue for. When we allow them to instead choose which answer to argue for, we find judges are less frequently convinced by the wrong answer in debate than in consultancy.
Further, we find that stronger debater models increase judge accuracy, though more modestly than in previous studies.

------------

`[2407.03381] SeqMate: A Novel Large Language Model Pipeline for Automating RNA Sequencing <https://arxiv.org/abs/2407.03381>`__ SeqMate:一种用于自动化RNA测序的新型大型语言模型管道

::

    Tue, 2 Jul 2024 20:28:30 GMT
    Devam Mondal, Atharva Inamdar

RNA sequencing techniques, like bulk RNA-seq and Single Cell (sc) RNA-seq, are critical tools for the biologist looking to analyze the genetic activity/transcriptome of a tissue or cell during an experimental procedure.
Platforms like Illumina's next-generation sequencing (NGS) are used to produce the raw data for this experimental procedure. This raw FASTQ data must then be prepared via a complex series of data manipulations by bioinformaticians. This process currently takes place on an unwieldy textual user interface like a terminal/command line that requires the user to install and import multiple program packages, preventing the untrained biologist from initiating data analysis. Open-source platforms like Galaxy have produced a more user-friendly pipeline, yet the visual interface remains cluttered and highly technical, remaining uninviting for the natural scientist. To address this, SeqMate is a user-friendly tool that allows for one-click analytics by utilizing the power of a large language model (LLM) to automate both data preparation and analysis (differential expression, trajectory analysis, etc). Furthermore, by utilizing the power of generative AI, SeqMate is also capable of analyzing such findings and producing written reports of upregulated/downregulated/user-prompted genes with sources cited from known repositories like PubMed, PDB, and Uniprot.

------------

`[2407.03387] ConCodeEval: Evaluating Large Language Models for Code Constraints in Domain-Specific Languages <https://arxiv.org/abs/2407.03387>`__ ConCodeEval:面向特定领域语言代码约束的大型语言模型评估

::

    Wed, 3 Jul 2024 08:36:13 GMT
    Mehant Kammakomati, Sameer Pimparkhede, Srikanth Tamilselvam, Prince Kumar, Pushpak Bhattacharyya

Recent work shows Large Language Models (LLMs) struggle to understand natural language constraints for various text generation tasks in zero- and few-shot settings. While, in the code domain, there is wide usage of constraints in code format to maintain the integrity of code written in Domain-Specific Languages (DSLs), yet there has been no work evaluating LLMs with these constraints. We propose two novel tasks to assess the controllability of LLMs using hard and soft constraints represented as code across five representations. Our findings suggest that LLMs struggle to comprehend constraints in all representations irrespective of their portions in the pre-training data. While models are better at comprehending constraints in JSON, YAML, and natural language representations, they struggle with constraints represented in XML and the resource-rich language Python.

------------

`[2407.03469] Scaling Data-Driven Building Energy Modelling using Large Language Models <https://arxiv.org/abs/2407.03469>`__ 使用大型语言模型扩展数据驱动建筑能耗建模

::

    Wed, 3 Jul 2024 19:34:24 GMT
    Sunil Khadka, Liang Zhang

Building Management System (BMS) through a data-driven method always faces data and model scalability issues. We propose a methodology to tackle the scalability challenges associated with the development of data-driven models for BMS by using Large Language Models (LLMs). LLMs' code generation adaptability can enable broader adoption of BMS by "automating the automation," particularly the data handling and data-driven modeling processes. In this paper, we use LLMs to generate code that processes structured data from BMS and build data-driven models for BMS's specific requirements. This eliminates the need for manual data and model development, reducing the time, effort, and cost associated with this process. Our hypothesis is that LLMs can incorporate domain knowledge about data science and BMS into data processing and modeling, ensuring that the data-driven modeling is automated for specific requirements of different building types and control objectives, which also improves accuracy and scalability. We generate a prompt template following the framework of Machine Learning Operations so that the prompts are designed to systematically generate Python code for data-driven modeling. Our case study indicates that bi-sequential prompting under the prompt template can achieve a high success rate of code generation and code accuracy, and significantly reduce human labor costs.

------------

`[2407.03611] An Empirical Study on Capability of Large Language Models in Understanding Code Semantics <https://arxiv.org/abs/2407.03611>`__ 大型语言模型理解代码语义能力的实证研究

::

    Thu, 4 Jul 2024 03:40:58 GMT
    Thu-Trang Nguyen, Thanh Trong Vu, Hieu Dinh Vo and Son Nguyen

Large Language Models for Code (code LLMs) have demonstrated remarkable performance across various software engineering (SE) tasks, increasing the application of code LLMs in software development. Despite the success of code LLMs, there remain significant concerns about the actual capabilities and reliability of these models, "whether these models really learn the semantics of code from the training data and leverage the learned knowledge to perform the SE tasks". In this paper, we introduce EMPICA, a comprehensive framework designed to systematically and empirically evaluate the capabilities of code LLMs in understanding code semantics. Specifically, EMPICA systematically introduces controlled modifications/transformations into the input code and examines the models' responses. Generally, code LLMs must be robust to semantically equivalent code inputs and be sensitive to non-equivalent ones for all SE tasks. Specifically, for every SE task, given an input code snippet c and its semantic equivalent variants, code LLMs must robustly produce consistent/equivalent outputs while they are expected to generate different outputs for c and its semantic non-equivalent variants. Our experimental results on three representative code understanding tasks, including code summarization, method name prediction, and output prediction, reveal that the robustness and sensitivity of the state-of-the-art code LLMs to code transformations vary significantly across tasks and transformation operators.
In addition, the code LLMs exhibit better robustness to the semantic preserving transformations than their sensitivity to the semantic non-preserving transformations. These results highlight a need to enhance the model's capabilities of understanding code semantics, especially the sensitivity property.

------------

`[2407.04051] FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs <https://arxiv.org/abs/2407.04051>`__ FunAudioLLM:人类与llm自然交互的语音理解和生成基础模型

::

    Thu, 4 Jul 2024 16:49:02 GMT
    Tongyi SpeechTeam

This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs). At its core are two innovative models: SenseVoice, which handles multilingual speech recognition, emotion recognition, and audio event detection; and CosyVoice, which facilitates natural speech generation with control over multiple languages, timbre, speaking style, and speaker identity.
SenseVoice-Small delivers exceptionally low-latency ASR for 5 languages, and SenseVoice-Large supports high-precision ASR for over 50 languages, while CosyVoice excels in multi-lingual voice generation, zero-shot in-context learning, cross-lingual voice cloning, and instruction-following capabilities.
The models related to SenseVoice and CosyVoice have been open-sourced on Modelscope and Huggingface, along with the corresponding training, inference, and fine-tuning codes released on GitHub. By integrating these models with LLMs, FunAudioLLM enables applications such as speech-to-speech translation, emotional voice chat, interactive podcasts, and expressive audiobook narration, thereby pushing the boundaries of voice interaction technology. Demos are available at https://fun-audio-llm.github.io, and the code can be accessed at https://github.com/FunAudioLLM.

------------

`[2407.04411] Waterfall: Framework for Robust and Scalable Text Watermarking <https://arxiv.org/abs/2407.04411>`__ 

::

    Fri, 5 Jul 2024 10:51:33 GMT
    Gregory Kang Ruey Lau, Xinyuan Niu, Hieu Dao, Jiangwei Chen, Chuan-Sheng Foo, Bryan Kian Hsiang Low

Protecting intellectual property (IP) of text such as articles and code is increasingly important, especially as sophisticated attacks become possible, such as paraphrasing by large language models (LLMs) or even unauthorized training of LLMs on copyrighted text to infringe such IP. However, existing text watermarking methods are not robust enough against such attacks nor scalable to millions of users for practical implementation. In this paper, we propose Waterfall, the first training-free framework for robust and scalable text watermarking applicable across multiple text types (e.g., articles, code) and languages supportable by LLMs, for general text and LLM data provenance.
Waterfall comprises several key innovations, such as being the first to use LLM as paraphrasers for watermarking along with a novel combination of techniques that are surprisingly effective in achieving robust verifiability and scalability. We empirically demonstrate that Waterfall achieves significantly better scalability, robust verifiability, and computational efficiency compared to SOTA article-text watermarking methods, and also showed how it could be directly applied to the watermarking of code.

------------

`[2407.04418] Enabling On-Device LLMs Personalization with Smartphone Sensing <https://arxiv.org/abs/2407.04418>`__ 通过智能手机感知实现设备上的llm个性化

::

    Fri, 5 Jul 2024 11:09:05 GMT
    Shiquan Zhang, Ying Ma, Le Fang, Hong Jia, Simon D'Alfonso, Vassilis Kostakos

This demo presents a novel end-to-end framework that combines on-device large language models (LLMs) with smartphone sensing technologies to achieve context-aware and personalized services. The framework addresses critical limitations of current personalization solutions via cloud-based LLMs, such as privacy concerns, latency and cost, and limited personal sensor data. To achieve this, we innovatively proposed deploying LLMs on smartphones with multimodal sensor data and customized prompt engineering, ensuring privacy and enhancing personalization performance through context-aware sensing. A case study involving a university student demonstrated the proposed framework's capability to provide tailored recommendations. In addition, we show that the proposed framework achieves the best trade-off in privacy, performance, latency, cost, battery and energy consumption between on-device and cloud LLMs.
Future work aims to integrate more diverse sensor data and conduct large-scale user studies to further refine the personalization. We envision the proposed framework could significantly improve user experiences in various domains such as healthcare, productivity, and entertainment by providing secure, context-aware, and efficient interactions directly on users' devices.

------------

`[2407.04472] EventChat: Implementation and user-centric evaluation of a large language model-driven conversational recommender system for exploring leisure events in an SME context <https://arxiv.org/abs/2407.04472>`__ EventChat:一个大型语言模型驱动的会话推荐系统的实现和以用户为中心的评估，用于探索中小企业环境中的休闲事件

::

    Fri, 5 Jul 2024 12:42:31 GMT
    Hannes Kunstmann, Joseph Ollier, Joel Persson, Florian von Wangenheim

Large language models (LLMs) present an enormous evolution in the strategic potential of conversational recommender systems (CRS). Yet to date, research has predominantly focused upon technical frameworks to implement LLM-driven CRS, rather than end-user evaluations or strategic implications for firms, particularly from the perspective of a small to medium enterprises (SME) that makeup the bedrock of the global economy. In the current paper, we detail the design of an LLM-driven CRS in an SME setting, and its subsequent performance in the field using both objective system metrics and subjective user evaluations. While doing so, we additionally outline a short-form revised ResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly evolving field. Our results reveal good system performance from a user experience perspective (85.5% recommendation accuracy) but underscore latency, cost, and quality issues challenging business viability. Notably, with a median cost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and response time emerge as crucial areas for achieving a more user-friendly and economically viable LLM-driven CRS for SME settings. One major driver of these costs is the use of an advanced LLM as a ranker within the retrieval-augmented generation (RAG) technique. Our results additionally indicate that relying solely on approaches such as Prompt-based learning with ChatGPT as the underlying LLM makes it challenging to achieve satisfying quality in a production environment. Strategic considerations for SMEs deploying an LLM-driven CRS are outlined, particularly considering trade-offs in the current technical landscape.

------------

`[2407.04503] When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions <https://arxiv.org/abs/2407.04503>`__ llm玩电话游戏时:迭代文化传播中的累积变化和吸引者

::

    Fri, 5 Jul 2024 13:44:09 GMT
    J\'er\'emy Perez, Corentin L\'eger, Grgur Kova\v{c}, C\'edric Colas, Gaia Molinaro, Maxime Derex, Pierre-Yves Oudeyer, Cl\'ement Moulin-Frier

As large language models (LLMs) start interacting with each other and generating an increasing amount of text online, it becomes crucial to better understand how information is transformed as it passes from one LLM to the next. While significant research has examined individual LLM behaviors, existing studies have largely overlooked the collective behaviors and information distortions arising from iterated LLM interactions. Small biases, negligible at the single output level, risk being amplified in iterated interactions, potentially leading the content to evolve towards attractor states. In a series of telephone game experiments, we apply a transmission chain design borrowed from the human cultural evolution literature: LLM agents iteratively receive, produce, and transmit texts from the previous to the next agent in the chain. By tracking the evolution of text toxicity, positivity, difficulty, and length across transmission chains, we uncover the existence of biases and attractors, and study their dependence on the initial text, the instructions, language model, and model size. For instance, we find that more open-ended instructions lead to stronger attraction effects compared to more constrained tasks. We also find that different text properties display different sensitivity to attraction effects, with toxicity leading to stronger attractors than length. These findings highlight the importance of accounting for multi-step transmission dynamics and represent a first step towards a more comprehensive understanding of LLM cultural dynamics.

------------

`[2407.04681] Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge <https://arxiv.org/abs/2407.04681>`__ 基于外部知识的多模态大型语言模型视觉提示的再思考

::

    Fri, 5 Jul 2024 17:43:30 GMT
    Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr, Lu Yuan

In recent years, multimodal large language models (MLLMs) have made significant strides by training on vast high-quality image-text datasets, enabling them to generally understand images well. However, the inherent difficulty in explicitly conveying fine-grained or spatially dense information in text, such as masks, poses a challenge for MLLMs, limiting their ability to answer questions requiring an understanding of detailed or localized visual elements. Drawing inspiration from the Retrieval-Augmented Generation (RAG) concept, this paper proposes a new visual prompt approach to integrate fine-grained external knowledge, gleaned from specialized vision models (e.g., instance segmentation/OCR models), into MLLMs. This is a promising yet underexplored direction for enhancing MLLMs' performance. Our approach diverges from concurrent works, which transform external knowledge into additional text prompts, necessitating the model to indirectly learn the correspondence between visual content and text coordinates. Instead, we propose embedding fine-grained knowledge information directly into a spatial embedding map as a visual prompt.
This design can be effortlessly incorporated into various MLLMs, such as LLaVA and Mipha, considerably improving their visual understanding performance.
Through rigorous experiments, we demonstrate that our method can enhance MLLM performance across nine benchmarks, amplifying their fine-grained context-aware capabilities.

------------

`[2407.03876] DART: Deep Adversarial Automated Red Teaming for LLM Safety <https://arxiv.org/abs/2407.03876>`__ DART:面向LLM安全的深度对抗性自动化红色团队

::

    Thu, 4 Jul 2024 12:14:27 GMT
    Bojian Jiang, Yi Jing, Tianhao Shen, Qing Yang, Deyi Xiong

Manual Red teaming is a commonly-used method to identify vulnerabilities in large language models (LLMs), which, is costly and unscalable. In contrast, automated red teaming uses a Red LLM to automatically generate adversarial prompts to the Target LLM, offering a scalable way for safety vulnerability detection. However, the difficulty of building a powerful automated Red LLM lies in the fact that the safety vulnerabilities of the Target LLM are dynamically changing with the evolution of the Target LLM. To mitigate this issue, we propose a Deep Adversarial Automated Red Teaming (DART) framework in which the Red LLM and Target LLM are deeply and dynamically interacting with each other in an iterative manner. In each iteration, in order to generate successful attacks as many as possible, the Red LLM not only takes into account the responses from the Target LLM, but also adversarially adjust its attacking directions by monitoring the global diversity of generated attacks across multiple iterations. Simultaneously, to explore dynamically changing safety vulnerabilities of the Target LLM, we allow the Target LLM to enhance its safety via an active learning based data selection mechanism. Experimential results demonstrate that DART significantly reduces the safety risk of the target LLM. For human evaluation on Anthropic Harmless dataset, compared to the instruction-tuning target LLM, DART eliminates the violation risks by 53.4\%.
We will release the datasets and codes of DART soon.

------------

`[2407.04482] Controlling Whisper: Universal Acoustic Adversarial Attacks to Control Speech Foundation Models <https://arxiv.org/abs/2407.04482>`__ 控制耳语:控制语音基础模型的通用声学对抗攻击

::

    Fri, 5 Jul 2024 13:04:31 GMT
    Vyas Raina and Mark Gales

Speech enabled foundation models, either in the form of flexible speech recognition based systems or audio-prompted large language models (LLMs), are becoming increasingly popular. One of the interesting aspects of these models is their ability to perform tasks other than automatic speech recognition (ASR) using an appropriate prompt. For example, the OpenAI Whisper model can perform both speech transcription and speech translation. With the development of audio-prompted LLMs there is the potential for even greater control options. In this work we demonstrate that with this greater flexibility the systems can be susceptible to model-control adversarial attacks. Without any access to the model prompt it is possible to modify the behaviour of the system by appropriately changing the audio input. To illustrate this risk, we demonstrate that it is possible to prepend a short universal adversarial acoustic segment to any input speech signal to override the prompt setting of an ASR foundation model. Specifically, we successfully use a universal adversarial acoustic segment to control Whisper to always perform speech translation, despite being set to perform speech transcription. Overall, this work demonstrates a new form of adversarial attack on multi-tasking speech enabled foundation models that needs to be considered prior to the deployment of this form of model.

------------

`[2407.03453] On Large Language Models in National Security Applications <https://arxiv.org/abs/2407.03453>`__ 

::

    Wed, 3 Jul 2024 18:53:22 GMT
    William N. Caballero and Phillip R. Jenkins

The overwhelming success of GPT-4 in early 2023 highlighted the transformative potential of large language models (LLMs) across various sectors, including national security. This article explores the implications of LLM integration within national security contexts, analyzing their potential to revolutionize information processing, decision-making, and operational efficiency. Whereas LLMs offer substantial benefits, such as automating tasks and enhancing data analysis, they also pose significant risks, including hallucinations, data privacy concerns, and vulnerability to adversarial attacks. Through their coupling with decision-theoretic principles and Bayesian reasoning, LLMs can significantly improve decision-making processes within national security organizations. Namely, LLMs can facilitate the transition from data to actionable decisions, enabling decision-makers to quickly receive and distill available information with less manpower. Current applications within the US Department of Defense and beyond are explored, e.g., the USAF's use of LLMs for wargaming and automatic summarization, that illustrate their potential to streamline operations and support decision-making. However, these applications necessitate rigorous safeguards to ensure accuracy and reliability. The broader implications of LLM integration extend to strategic planning, international relations, and the broader geopolitical landscape, with adversarial nations leveraging LLMs for disinformation and cyber operations, emphasizing the need for robust countermeasures. Despite exhibiting "sparks" of artificial general intelligence, LLMs are best suited for supporting roles rather than leading strategic decisions. Their use in training and wargaming can provide valuable insights and personalized learning experiences for military personnel, thereby improving operational readiness.

------------

`[2407.04065] On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards <https://arxiv.org/abs/2407.04065>`__ 基于LBOps的排行榜操作(workflow and smell of Leaderboard Operations):基础模型排行榜的探索性研究

::

    Thu, 4 Jul 2024 17:12:00 GMT
    Zhimin Zhao, Abdul Ali Bangash, Filipe Roseiro C\^ogo, Bram Adams, Ahmed E. Hassan

Foundation models (FM), such as large language models (LLMs), which are large-scale machine learning (ML) models, have demonstrated remarkable adaptability in various downstream software engineering (SE) tasks, such as code completion, code understanding, and software development. As a result, FM leaderboards, especially those hosted on cloud platforms, have become essential tools for SE teams to compare and select the best third-party FMs for their specific products and purposes. However, the lack of standardized guidelines for FM evaluation and comparison threatens the transparency of FM leaderboards and limits stakeholders' ability to perform effective FM selection. As a first step towards addressing this challenge, our research focuses on understanding how these FM leaderboards operate in real-world scenarios ("leaderboard operations") and identifying potential leaderboard pitfalls and areas for improvement ("leaderboard smells"). In this regard, we perform a multivocal literature review to collect up to 721 FM leaderboards, after which we examine their documentation and engage in direct communication with leaderboard operators to understand their workflow patterns. Using card sorting and negotiated agreement, we identify 5 unique workflow patterns and develop a domain model that outlines the essential components and their interaction within FM leaderboards. We then identify 8 unique types of leaderboard smells in LBOps. By mitigating these smells, SE teams can improve transparency, accountability, and collaboration in current LBOps practices, fostering a more robust and responsible ecosystem for FM comparison and selection.

------------

`[2407.04108] Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs <https://arxiv.org/abs/2407.04108>`__ 作为后门触发器的未来事件:研究llm中的时间漏洞

::

    Thu, 4 Jul 2024 18:24:09 GMT
    Sara Price, Arjun Panickssery, Sam Bowman, Asa Cooper Stickland

Backdoors are hidden behaviors that are only triggered once an AI system has been deployed. Bad actors looking to create successful backdoors must design them to avoid activation during training and evaluation. Since data used in these stages often only contains information about events that have already occurred, a component of a simple backdoor trigger could be a model recognizing data that is in the future relative to when it was trained. Through prompting experiments and by probing internal activations, we show that current large language models (LLMs) can distinguish past from future events, with probes on model activations achieving $90\%$ accuracy. We train models with backdoors triggered by a temporal distributional shift; they activate when the model is exposed to news headlines beyond their training cut-off dates. Fine-tuning on helpful, harmless and honest (HHH) data does not work well for removing simpler backdoor triggers but is effective on our backdoored models, although this distinction is smaller for the larger-scale model we tested. We also find that an activation-steering vector representing a model's internal representation of the date influences the rate of backdoor activation. We take these results as initial evidence that, at least for models at the modest scale we test, standard safety measures are enough to remove these backdoors. We publicly release all relevant code (https://github.com/sbp354/Future_triggered_backdoors), datasets (https://tinyurl.com/future-backdoor-datasets), and models (https://huggingface.co/saraprice).

------------

`[2407.04656] Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models with Adaptive Expert Placement <https://arxiv.org/abs/2407.04656>`__ Lazarus:基于自适应专家布局的弹性混合专家模型训练

::

    Fri, 5 Jul 2024 17:13:41 GMT
    Yongji Wu, Wenjie Qu, Tianyang Tao, Zhuang Wang, Wei Bai, Zhuohao Li, Yuan Tian, Jiaheng Zhang, Matthew Lentz, Danyang Zhuo

Sparsely-activated Mixture-of-Experts (MoE) architecture has increasingly been adopted to further scale large language models (LLMs) due to its sub-linear scaling for computation costs. However, frequent failures still pose significant challenges as training scales. The cost of even a single failure is significant, as all GPUs need to wait idle until the failure is resolved, potentially losing considerable training progress as training has to restart from checkpoints. Existing solutions for efficient fault-tolerant training either lack elasticity or rely on building resiliency into pipeline parallelism, which cannot be applied to MoE models due to the expert parallelism strategy adopted by the MoE architecture.
We present Lazarus, a system for resilient and elastic training of MoE models. Lazarus adaptively allocates expert replicas to address the inherent imbalance in expert workload and speeds-up training, while a provably optimal expert placement algorithm is developed to maximize the probability of recovery upon failures. Through adaptive expert placement and a flexible token dispatcher, Lazarus can also fully utilize all available nodes after failures, leaving no GPU idle. Our evaluation shows that Lazarus outperforms existing MoE training systems by up to 5.7x under frequent node failures and 3.4x on a real spot instance trace.

------------

`[2403.04449] Feedback-Generation for Programming Exercises With GPT-4 <https://arxiv.org/abs/2403.04449>`__ 使用GPT-4编程练习的反馈生成

::

    replaced with revised version Thu, 4 Jul 2024 07:30:22 GMT
    Submission history From: Sven Strickroth [view email]
    [v1] Thu, 7 Mar 2024 12:37:52 UTC (82 KB)
    [v2] Thu, 4 Jul 2024 07:30:22 UTC (82 KB)
    Imen Azaiz, Natalie Kiesler, Sven Strickroth

Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For example, the output is more structured and consistent. GPT-4 Turbo can also accurately identify invalid casing in student programs' output. In some cases, the feedback also includes the output of the student program. At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. The present work increases our understanding of LLMs' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on GPT-4.

------------

`[2406.14343] IWISDM: Assessing instruction following in multimodal models at scale <https://arxiv.org/abs/2406.14343>`__ IWISDM:评估大规模多模态模型的指令跟随

::

    replaced with revised version Wed, 3 Jul 2024 21:44:23 GMT
    Submission history From: Xiaoxuan Lei [view email]
    [v1] Thu, 20 Jun 2024 14:09:54 UTC (14,117 KB)
    [v2] Sun, 23 Jun 2024 01:51:53 UTC (14,118 KB)
    [v3] Tue, 25 Jun 2024 15:12:01 UTC (14,118 KB)
    [v4] Wed, 3 Jul 2024 21:44:23 UTC (14,118 KB)
    Xiaoxuan Lei and Lucas Gomez and Hao Yuan Bai and Pouya Bashivan

The ability to perform complex tasks from detailed instructions is a key to many remarkable achievements of our species. As humans, we are not only capable of performing a wide variety of tasks but also very complex ones that may entail hundreds or thousands of steps to complete. Large language models and their more recent multimodal counterparts that integrate textual and visual inputs have achieved unprecedented success in performing complex tasks. Yet, most existing benchmarks are largely confined to single-modality inputs (either text or vision), narrowing the scope of multimodal assessments, particularly for instruction-following in multimodal contexts. To bridge this gap, we introduce the instructed-Virtual VISual Decision Making (iWISDM) environment engineered to generate a limitless array of vision-language tasks of varying complexity. Using iWISDM, we compiled three distinct benchmarks of instruction following visual tasks across varying complexity levels and evaluated several newly developed multimodal models on these benchmarks. Our findings establish iWISDM as a robust benchmark for assessing the instructional adherence of both existing and emergent multimodal models and highlight a large gap between these models' ability to precisely follow instructions with that of humans.The code of iWISDM is available on GitHub at this https URL.

------------

`[2406.19859] MetaDesigner: Advancing Artistic Typography through AI-Driven, User-Centric, and Multilingual WordArt Synthesis <https://arxiv.org/abs/2406.19859>`__ MetaDesigner:通过人工智能驱动、以用户为中心和多语言文字艺术合成来推进艺术排版

::

    replaced with revised version Thu, 4 Jul 2024 15:47:40 GMT
    Submission history From: Zhi-Qi Cheng [view email]
    [v1] Fri, 28 Jun 2024 11:58:26 UTC (18,619 KB)
    [v2] Thu, 4 Jul 2024 15:47:40 UTC (18,620 KB)
    Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Qi He, Wangmeng Xiang, Hanyuan Chen, Jin-Peng Lan, Xianhui Lin, Kang Zhu, Bin Luo, Yifeng Geng, Xuansong Xie, Alexander G. Hauptmann

MetaDesigner revolutionizes artistic typography synthesis by leveraging the strengths of Large Language Models (LLMs) to drive a design paradigm centered around user engagement. At the core of this framework lies a multi-agent system comprising the Pipeline, Glyph, and Texture agents, which collectively enable the creation of customized WordArt, ranging from semantic enhancements to the imposition of complex textures. MetaDesigner incorporates a comprehensive feedback mechanism that harnesses insights from multimodal models and user evaluations to refine and enhance the design process iteratively. Through this feedback loop, the system adeptly tunes hyperparameters to align with user-defined stylistic and thematic preferences, generating WordArt that not only meets but exceeds user expectations of visual appeal and contextual relevance. Empirical validations highlight MetaDesigner's capability to effectively serve diverse WordArt applications, consistently producing aesthetically appealing and context-sensitive results.

------------

`[2308.13782] Planning with Logical Graph-based Language Model for Instruction Generation <https://arxiv.org/abs/2308.13782>`__ 基于逻辑图语言模型的指令生成规划

::

    replaced with revised version Fri, 5 Jul 2024 15:24:03 GMT
    Submission history From: Hankz Hankui Zhuo [view email]
    [v1] Sat, 26 Aug 2023 06:28:14 UTC (3,507 KB)
    [v2] Fri, 5 Jul 2024 15:24:03 UTC (3,800 KB)
    Fan Zhang, Kebing Jin, and Hankz Hankui Zhuo

Despite the superior performance of large language models to generate natural language texts, it is hard to generate texts with correct logic according to a given task, due to the difficulties for neural models to capture implied rules from free-form texts. In this paper, we propose a novel graph-based language model, Logical-GLM, to infuse logic into language models for more valid text generation and interpretability. Specifically, we first capture information from natural language instructions and construct logical bayes graphs that generally describe domains. Next, we generate logical skeletons to guide language model training, infusing domain knowledge into language models. Finally, we alternately optimize the searching policy of graphs and language models until convergence. The experimental results show that Logical-GLM is both effective and efficient compared with traditional language models, despite using smaller-scale training data and fewer parameters. Our approach can generate instructional texts with more correct logic owing to the internalized domain knowledge. Moreover, the usage of logical graphs reflects the inner mechanism of the language models, which improves the interpretability of black-box models.

------------

`[2310.04444] What's the Magic Word? A Control Theory of LLM Prompting <https://arxiv.org/abs/2310.04444>`__ 最神奇的词是什么?LLM激励的控制理论

::

    replaced with revised version Wed, 3 Jul 2024 22:23:50 GMT
    Submission history From: Aman Bhargava [view email]
    [v1] Mon, 2 Oct 2023 22:35:40 UTC (318 KB)
    [v2] Tue, 10 Oct 2023 10:15:14 UTC (318 KB)
    [v3] Wed, 3 Jan 2024 06:38:36 UTC (1,206 KB)
    [v4] Wed, 3 Jul 2024 22:23:50 UTC (1,398 KB)
    Aman Bhargava, Cameron Witkowski, Shi-Zhuo Looi, Matt Thomson

Prompt engineering is crucial for deploying LLMs but is poorly understood mathematically. We formalize LLM systems as a class of discrete stochastic dynamical systems to explore prompt engineering through the lens of control theory. We offer a mathematical analysis of the limitations on the controllability of self-attention as a function of the singular values of the parameter matrices. We present complementary empirical results on the controllability of a panel of LLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Given initial state $\mathbf x_0$ from Wikitext and prompts of length $k \leq 10$ tokens, we find that the "correct" next token is reachable at least 97% of the time, and that the top 75 most likely next tokens are reachable at least 85% of the time. Intriguingly, short prompt sequences can dramatically alter the likelihood of specific outputs, even making the least likely tokens become the most likely ones. This control-theoretic analysis of LLMs demonstrates the significant and poorly understood role of input sequences in steering output probabilities, offering a foundational perspective for enhancing language model system capabilities.

------------

`[2311.09132] Aligning Neural Machine Translation Models: Human Feedback in Training and Inference <https://arxiv.org/abs/2311.09132>`__ 神经机器翻译模型对齐:训练和推理中的人工反馈

::

    replaced with revised version Thu, 4 Jul 2024 10:16:35 GMT
    Submission history From: Miguel Moura Ramos [view email]
    [v1] Wed, 15 Nov 2023 17:21:58 UTC (236 KB)
    [v2] Thu, 4 Jul 2024 10:16:35 UTC (689 KB)
    Miguel Moura Ramos, Patrick Fernandes, Ant\'onio Farinhas, Andr\'e F. T. Martins

Reinforcement learning from human feedback (RLHF) is a recent technique to improve the quality of the text generated by a language model, making it closer to what humans would generate. A core ingredient in RLHF's success in aligning and improving large language models (LLMs) is its reward model, trained using human feedback on model outputs. In machine translation (MT), where metrics trained from human annotations can readily be used as reward models, recent methods using minimum Bayes risk decoding and reranking have succeeded in improving the final quality of translation. In this study, we comprehensively explore and compare techniques for integrating quality metrics as reward models into the MT pipeline. This includes using the reward model for data filtering, during the training phase through RL, and at inference time by employing reranking techniques, and we assess the effects of combining these in a unified approach. Our experimental results, conducted across multiple translation tasks, underscore the crucial role of effective data filtering, based on estimated quality, in harnessing the full potential of RL in enhancing MT quality. Furthermore, our findings demonstrate the effectiveness of combining RL training with reranking techniques, showcasing substantial improvements in translation quality.

------------

`[2311.09684] Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation <https://arxiv.org/abs/2311.09684>`__ 医生知道如何提示吗?对自动提示优化的需求有助于临床病历的生成

::

    replaced with revised version Fri, 5 Jul 2024 09:14:11 GMT
    Submission history From: Zonghai Yao [view email]
    [v1] Thu, 16 Nov 2023 08:54:52 UTC (7,930 KB)
    [v2] Tue, 19 Mar 2024 16:27:37 UTC (8,108 KB)
    [v3] Fri, 5 Jul 2024 09:14:11 UTC (8,109 KB)
    Zonghai Yao, Ahmed Jaafar, Beining Wang, Zhichao Yang, Hong Yu

This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in standardizing prompt quality across clinical note sections. A human-in-the-loop approach shows that experts maintain content quality post-APO, with a preference for their own modifications, suggesting the value of expert customization. We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization.

------------

`[2312.04691] Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models <https://arxiv.org/abs/2312.04691>`__ simulm - llm:基于大型语言模型探索高质量同传的框架

::

    replaced with revised version Thu, 4 Jul 2024 07:05:48 GMT
    Submission history From: Lizhong Chen [view email]
    [v1] Thu, 7 Dec 2023 20:42:05 UTC (9,361 KB)
    [v2] Tue, 12 Dec 2023 03:17:29 UTC (9,361 KB)
    [v3] Wed, 5 Jun 2024 03:12:13 UTC (9,549 KB)
    [v4] Thu, 4 Jul 2024 07:05:48 UTC (9,549 KB)
    Victor Agostinelli, Max Wild, Matthew Raffel, Kazi Ahmed Asif Fuad, Lizhong Chen

Large language models (LLMs) with billions of parameters and pretrained on massive amounts of data are now capable of near or better than state-of-the-art performance in a variety of downstream natural language processing tasks. Neural machine translation (NMT) is one such task that LLMs have been applied to with great success. However, little research has focused on applying LLMs to the more difficult subset of NMT called simultaneous translation (SimulMT), where translation begins before the entire source context is available to the model. In this paper, we address key challenges facing LLMs fine-tuned for SimulMT, validate classical SimulMT concepts and practices in the context of LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT, and introduce Simul-LLM, the first open-source fine-tuning and evaluation pipeline development framework for LLMs focused on SimulMT.

------------

`[2312.06681] Steering Llama 2 via Contrastive Activation Addition <https://arxiv.org/abs/2312.06681>`__ 通过对比激活添加转向美洲驼2

::

    replaced with revised version Fri, 5 Jul 2024 15:30:45 GMT
    Submission history From: Nina Panickssery [view email]
    [v1] Sat, 9 Dec 2023 04:40:46 UTC (9,269 KB)
    [v2] Wed, 27 Dec 2023 05:02:35 UTC (8,671 KB)
    [v3] Thu, 7 Mar 2024 02:06:00 UTC (10,693 KB)
    [v4] Fri, 5 Jul 2024 15:30:45 UTC (10,738 KB)
    Nina Panickssery, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, Alexander Matt Turner

We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).

------------

`[2402.00371] What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection <https://arxiv.org/abs/2402.00371>`__ 机器人会说什么?大型语言模型在社交媒体机器人检测中的机会和风险

::

    replaced with revised version Thu, 4 Jul 2024 23:37:40 GMT
    Submission history From: Shangbin Feng [view email]
    [v1] Thu, 1 Feb 2024 06:21:19 UTC (595 KB)
    [v2] Thu, 4 Jul 2024 23:37:40 UTC (604 KB)
    Shangbin Feng, Herun Wan, Ningnan Wang, Zhaoxuan Tan, Minnan Luo, Yulia Tsvetkov

Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems.

------------

`[2402.06341] RareBench: Can LLMs Serve as Rare Diseases Specialists? <https://arxiv.org/abs/2402.06341>`__ RareBench: llm可以成为罕见疾病专家吗?

::

    replaced with revised version Thu, 4 Jul 2024 09:10:17 GMT
    Submission history From: Xuanzhong Chen [view email]
    [v1] Fri, 9 Feb 2024 11:34:16 UTC (10,973 KB)
    [v2] Thu, 4 Jul 2024 09:10:17 UTC (5,742 KB)
    Xuanzhong Chen, Xiaohao Mao, Qihan Guo, Lun Wang, Shuyang Zhang, Ting Chen

Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's rare disease after 17 doctors failed" underscore LLMs' potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic few-shot prompt methodology, leveraging a comprehensive rare disease knowledge graph synthesized from multiple knowledge bases, significantly enhancing LLMs' diagnostic performance. Moreover, we present an exhaustive comparative study of GPT-4's diagnostic capabilities against those of specialist physicians. Our experimental findings underscore the promising potential of integrating LLMs into the clinical diagnostic process for rare diseases. This paves the way for exciting possibilities in future advancements in this field.

------------

`[2402.10426] DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection <https://arxiv.org/abs/2402.10426>`__ DELL:对基于llm的错误信息检测的反应和解释

::

    replaced with revised version Fri, 5 Jul 2024 00:59:45 GMT
    Submission history From: Herun Wan [view email]
    [v1] Fri, 16 Feb 2024 03:24:56 UTC (631 KB)
    [v2] Fri, 5 Jul 2024 00:59:45 UTC (639 KB)
    Herun Wan, Shangbin Feng, Zhaoxuan Tan, Heng Wang, Yulia Tsvetkov, Minnan Luo

Large language models are limited by challenges in factuality and hallucinations to be directly employed off-the-shelf for judging the veracity of news articles, where factual accuracy is paramount. In this work, we propose DELL that identifies three key stages in misinformation detection where LLMs could be incorporated as part of the pipeline: 1) LLMs could \emph{generate news reactions} to represent diverse perspectives and simulate user-news interaction networks; 2) LLMs could \emph{generate explanations} for proxy tasks (e.g., sentiment, stance) to enrich the contexts of news articles and produce experts specializing in various aspects of news understanding; 3) LLMs could \emph{merge task-specific experts} and provide an overall prediction by incorporating the predictions and confidence scores of varying experts. Extensive experiments on seven datasets with three LLMs demonstrate that DELL outperforms state-of-the-art baselines by up to 16.8\% in macro f1-score. Further analysis reveals that the generated reactions and explanations are greatly helpful in misinformation detection, while our proposed LLM-guided expert merging helps produce better-calibrated predictions.

------------

`[2402.14016] Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment <https://arxiv.org/abs/2402.14016>`__ llm -as- judge稳健吗?调查零样本LLM评估上的通用对抗性攻击

::

    replaced with revised version Thu, 4 Jul 2024 12:34:44 GMT
    Submission history From: Vyas Raina [view email]
    [v1] Wed, 21 Feb 2024 18:55:20 UTC (837 KB)
    [v2] Thu, 4 Jul 2024 12:34:44 UTC (1,121 KB)
    Vyas Raina, Adian Liusie, Mark Gales

Large Language Models (LLMs) are powerful zero-shot assessors used in real-world situations such as assessing written exams and benchmarking systems. Despite these critical applications, no existing work has analyzed the vulnerability of judge-LLMs to adversarial manipulation. This work presents the first study on the adversarial robustness of assessment LLMs, where we demonstrate that short universal adversarial phrases can be concatenated to deceive judge LLMs to predict inflated scores. Since adversaries may not know or have access to the judge-LLMs, we propose a simple surrogate attack where a surrogate model is first attacked, and the learned attack phrase then transferred to unknown judge-LLMs. We propose a practical algorithm to determine the short universal attack phrases and demonstrate that when transferred to unseen models, scores can be drastically inflated such that irrespective of the assessed text, maximum scores are predicted. It is found that judge-LLMs are significantly more susceptible to these adversarial attacks when used for absolute scoring, as opposed to comparative assessment. Our findings raise concerns on the reliability of LLM-as-a-judge methods, and emphasize the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios.

------------

`[2402.14499] "My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models <https://arxiv.org/abs/2402.14499>`__ "My Answer is C":在指令调优的语言模型中，第一个token的概率与文本答案不匹配

::

    replaced with revised version Thu, 4 Jul 2024 12:51:29 GMT
    Submission history From: Xinpeng Wang [view email]
    [v1] Thu, 22 Feb 2024 12:47:33 UTC (7,355 KB)
    [v2] Thu, 4 Jul 2024 12:51:29 UTC (7,439 KB)
    Xinpeng Wang, Bolei Ma, Chengzhi Hu, Leon Weber-Genzel, Paul R\"ottger, Frauke Kreuter, Dirk Hovy, Barbara Plank

The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with "Sure" or refusing to answer. Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output as well and ii) caution against relying solely on first-token evaluation.

------------

`[2403.02130] Using LLMs for the Extraction and Normalization of Product Attribute Values <https://arxiv.org/abs/2403.02130>`__ 使用LLMs进行产品属性值的提取和归一化

::

    replaced with revised version Thu, 4 Jul 2024 10:12:38 GMT
    Submission history From: Alexander Brinkmann [view email]
    [v1] Mon, 4 Mar 2024 15:39:59 UTC (634 KB)
    [v2] Tue, 5 Mar 2024 08:12:18 UTC (633 KB)
    [v3] Thu, 4 Jul 2024 10:12:38 UTC (648 KB)
    [v4] Mon, 15 Jul 2024 10:16:14 UTC (648 KB)
    Alexander Brinkmann, Nick Baumann, Christian Bizer

Product offers on e-commerce websites often consist of a product title and a textual product description. In order to enable features such as faceted product search or to generate product comparison tables, it is necessary to extract structured attribute-value pairs from the unstructured product titles and descriptions and to normalize the extracted values to a single, unified scale for each attribute. This paper explores the potential of using large language models (LLMs), such as GPT-3.5 and GPT-4, to extract and normalize attribute values from product titles and descriptions. We experiment with different zero-shot and few-shot prompt templates for instructing LLMs to extract and normalize attribute-value pairs. We introduce the Web Data Commons - Product Attribute Value Extraction (WDC-PAVE) benchmark dataset for our experiments. WDC-PAVE consists of product offers from 59 different websites which provide this http URL annotations. The offers belong to five different product categories, each with a specific set of attributes. The dataset provides manually verified attribute-value pairs in two forms: (i) directly extracted values and (ii) normalized attribute values. The normalization of the attribute values requires systems to perform the following types of operations: name expansion, generalization, unit of measurement conversion, and string wrangling. Our experiments demonstrate that GPT-4 outperforms the PLM-based extraction methods SU-OpenTag, AVEQA, and MAVEQA by 10%, achieving an F1-score of 91%. For the extraction and normalization of product attribute values, GPT-4 achieves a similar performance to the extraction scenario, while being particularly strong at string wrangling and name expansion.

------------

`[2403.05881] KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques <https://arxiv.org/abs/2403.05881>`__ KG-Rank:基于知识图谱和排名技术的医学问答大型语言模型增强

::

    replaced with revised version Thu, 4 Jul 2024 07:45:07 GMT
    Submission history From: Rui Yang [view email]
    [v1] Sat, 9 Mar 2024 11:23:38 UTC (401 KB)
    [v2] Tue, 19 Mar 2024 03:48:11 UTC (401 KB)
    [v3] Thu, 4 Jul 2024 07:45:07 UTC (396 KB)
    Rui Yang, Haoran Liu, Edison Marrese-Taylor, Qingcheng Zeng, Yu He Ke, Wanxin Li, Lechao Cheng, Qingyu Chen, James Caverlee, Yutaka Matsuo, Irene Li

Large language models (LLMs) have demonstrated impressive generative capabilities with the potential to innovate in medicine. However, the application of LLMs in real clinical settings remains challenging due to the lack of factual consistency in the generated content. In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) along with ranking and re-ranking techniques, to improve the factuality of long-form question answering (QA) in the medical domain. Specifically, when receiving a question, KG-Rank automatically identifies medical entities within the question and retrieves the related triples from the medical KG to gather factual information. Subsequently, KG-Rank innovatively applies multiple ranking techniques to refine the ordering of these triples, providing more relevant and precise information for LLM inference. To the best of our knowledge, KG-Rank is the first application of KG combined with ranking models in medical QA specifically for generating long answers. Evaluation on four selected medical QA datasets demonstrates that KG-Rank achieves an improvement of over 18% in ROUGE-L score. Additionally, we extend KG-Rank to open domains, including law, business, music, and history, where it realizes a 14% improvement in ROUGE-L score, indicating the effectiveness and great potential of KG-Rank.

------------

`[2403.17104] Attribute First, then Generate: Locally-attributable Grounded Text Generation <https://arxiv.org/abs/2403.17104>`__ 先属性，再生成:局部可归因的基础文本生成

::

    replaced with revised version Thu, 4 Jul 2024 08:07:58 GMT
    Submission history From: Aviv Slobodkin [view email]
    [v1] Mon, 25 Mar 2024 18:41:47 UTC (8,448 KB)
    [v2] Mon, 1 Apr 2024 17:57:40 UTC (8,450 KB)
    [v3] Thu, 4 Jul 2024 08:07:58 UTC (8,457 KB)
    Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, Ido Dagan

Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable text generation approach, prioritizing concise attributions. Our method, named "Attribute First, then Generate", breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments ("select first") and then conditioning the generation process on them ("then generate"), we ensure these segments also act as the output's fine-grained attributions ("select" becomes "attribute"). Tested on Multi-document Summarization and Long-form Question-answering, our method not only yields more concise citations than the baselines but also maintains - and in some cases enhances - both generation quality and attribution accuracy. Furthermore, it significantly reduces the time required for fact verification by human assessors.

------------

`[2403.20041] Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs <https://arxiv.org/abs/2403.20041>`__ Transformer-Lite:手机gpu上大型语言模型的高效部署

::

    replaced with revised version Fri, 5 Jul 2024 07:18:42 GMT
    Submission history From: Luchang Li [view email]
    [v1] Fri, 29 Mar 2024 08:26:53 UTC (654 KB)
    [v2] Tue, 21 May 2024 01:21:19 UTC (654 KB)
    [v3] Fri, 5 Jul 2024 07:18:42 UTC (654 KB)
    Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, Qin Xie

The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones. However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference. Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We evaluated Transformer-Lite's performance using LLMs with varied architectures and parameters ranging from 2B to 14B. Specifically, we achieved prefill and decoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s and 30 token/s for smaller Gemma 2B, respectively. Compared with CPU-based FastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the prefill speed and 2~3x speedup for the decoding speed.

------------

`[2404.06709] CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers <https://arxiv.org/abs/2404.06709>`__ CQIL:准独立层并发计算的推理延迟优化

::

    replaced with revised version Thu, 4 Jul 2024 09:17:15 GMT
    Submission history From: Longwei Zou [view email]
    [v1] Wed, 10 Apr 2024 03:30:01 UTC (173 KB)
    [v2] Thu, 4 Jul 2024 09:17:15 UTC (180 KB)
    Longwei Zou, Qingyang Wang, Han Zhao, Jiangang Kong, Yi Yang, Yangdong Deng

The fast-growing large scale language models are delivering unprecedented performance on almost all natural language processing tasks. However, the effectiveness of large language models are reliant on an exponentially increasing number of parameters. The overwhelming computation complexity incurs a high inference latency that negatively affects user experience. Existing methods to improve inference efficiency, such as tensor parallelism and quantization, target to reduce per-layer computing latency, yet overlook the cumulative latency due to the number of layers. Recent works on reducing the cumulative latency through layer removing, however, lead to significant performance drop. Motivated by the similarity of inputs among adjacent layers, we propose to identify quasi-independent layers, which can be concurrently computed to significantly decrease inference latency. We also introduce a bypassing technique to mitigate the effect of information loss. Empirical experiments of the proposed approach on the LLaMA models confirm that Concurrent Computation of Quasi-Independent Layers (CQIL) can reduce latency by up to 48.3% on LLaMA-33B, while maintaining a close level of performance.

------------

`[2406.00656] Presence or Absence: Are Unknown Word Usages in Dictionaries? <https://arxiv.org/abs/2406.00656>`__ 存在还是缺席:词典中未登录词的用法?

::

    replaced with revised version Thu, 4 Jul 2024 16:23:35 GMT
    Submission history From: Wei Zhao [view email]
    [v1] Sun, 2 Jun 2024 07:57:45 UTC (129 KB)
    [v2] Thu, 4 Jul 2024 16:23:35 UTC (130 KB)
    Xianghe Ma, Dominik Schlechtweg, Wei Zhao

There has been a surge of interest in computational modeling of semantic change. The foci of previous works are on detecting and interpreting word senses gained over time; however, it remains unclear whether the gained senses are covered by dictionaries. In this work, we aim to fill this research gap by comparing detected word senses with dictionary sense inventories in order to bridge between the communities of lexical semantic change detection and lexicography. We evaluate our system in the AXOLOTL-24 shared task for Finnish, Russian and German languages \cite{fedorova-etal-2024-axolotl}. Our system is fully unsupervised. It leverages a graph-based clustering approach to predict mappings between unknown word usages and dictionary entries for Subtask 1, and generates dictionary-like definitions for those novel word usages through the state-of-the-art Large Language Models such as GPT-4 and LLaMA-3 for Subtask 2. In Subtask 1, our system outperforms the baseline system by a large margin, and it offers interpretability for the mapping results by distinguishing between matched and unmatched (novel) word usages through our graph-based clustering approach. Our system ranks first in Finnish and German, and ranks second in Russian on the Subtask 2 test-phase leaderboard. These results show the potential of our system in managing dictionary entries, particularly for updating dictionaries to include novel sense entries. Our code and data are made publicly available\footnote{\url{this https URL}}.

------------

`[2406.14657] OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset <https://arxiv.org/abs/2406.14657>`__ OpenDebateEvidence:大规模论据挖掘和摘要数据集

::

    replaced with revised version Fri, 5 Jul 2024 16:51:15 GMT
    Submission history From: Allen Roush [view email]
    [v1] Thu, 20 Jun 2024 18:22:59 UTC (7,526 KB)
    [v2] Fri, 5 Jul 2024 16:51:15 UTC (7,526 KB)
    Allen Roush, Yusuf Shabazz, Arvind Balaji, Peter Zhang, Stefano Mezza, Markus Zhang, Sanjay Basu, Sriram Vishwanath, Mehdi Fatemi, Ravid Shwartz-Ziv

We introduce OpenDebateEvidence, a comprehensive dataset for argument mining and summarization sourced from the American Competitive Debate community. This dataset includes over 3.5 million documents with rich metadata, making it one of the most extensive collections of debate evidence. OpenDebateEvidence captures the complexity of arguments in high school and college debates, providing valuable resources for training and evaluation. Our extensive experiments demonstrate the efficacy of fine-tuning state-of-the-art large language models for argumentative abstractive summarization across various methods, models, and datasets. By providing this comprehensive resource, we aim to advance computational argumentation and support practical applications for debaters, educators, and researchers. OpenDebateEvidence is publicly available to support further research and innovation in computational argumentation. Access it here: this https URL

------------

`[2406.17807] Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary <https://arxiv.org/abs/2406.17807>`__ 非完全信息牌游戏解说策略的增强——关丹解说大型语言模型研究

::

    replaced with revised version Thu, 4 Jul 2024 04:47:22 GMT
    Submission history From: Meiling Tao [view email]
    [v1] Sun, 23 Jun 2024 11:58:26 UTC (18,623 KB)
    [v2] Thu, 4 Jul 2024 04:47:22 UTC (18,623 KB)
    Meiling Tao, Xuechen Liang, Yiling Tao, Tianyu Shi

Recent advancements in large language models (LLMs) have unlocked the potential for generating high-quality game commentary. However, producing insightful and engaging commentary for complex games with incomplete information remains a significant challenge. In this paper, we introduce a novel commentary method that combine Reinforcement Learning (RL) and LLMs, tailored specifically for the Chinese card game \textit{Guandan}. Our system leverages RL to generate intricate card-playing scenarios and employs LLMs to generate corresponding commentary text, effectively emulating the strategic analysis and narrative prowess of professional commentators. The framework comprises a state commentary guide, a Theory of Mind (ToM)-based strategy analyzer, and a style retrieval module, which seamlessly collaborate to deliver detailed and context-relevant game commentary in the Chinese language environment. We empower LLMs with ToM capabilities and refine both retrieval and information filtering mechanisms. This facilitates the generation of personalized commentary content. Our experimental results showcase the substantial enhancement in performance achieved by the proposed commentary framework when applied to open-source LLMs, surpassing the performance of GPT-4 across multiple evaluation metrics.

------------

`[2407.01906] Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models <https://arxiv.org/abs/2407.01906>`__ 让专家坚持他的最后一个:专家专门的稀疏架构大型语言模型的微调

::

    replaced with revised version Fri, 5 Jul 2024 03:23:59 GMT
    Submission history From: Deli Chen [view email]
    [v1] Tue, 2 Jul 2024 03:11:13 UTC (8,250 KB)
    [v2] Fri, 5 Jul 2024 03:23:59 UTC (8,250 KB)
    Zihan Wang, Deli Chen, Damai Dai, Runxin Xu, Zhuoshu Li, Y. Wu

Parameter-efficient fine-tuning (PEFT) is crucial for customizing Large Language Models (LLMs) with constrained resources. Although there have been various PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture LLMs is still underexplored. In this work, we study the PEFT method for LLMs with the Mixture-of-Experts (MoE) architecture and the contents of this work are mainly threefold: (1) We investigate the dispersion degree of the activated experts in customized tasks, and found that the routing distribution for a specific task tends to be highly concentrated, while the distribution of activated experts varies significantly across different tasks. (2) We propose Expert-Specialized Fine-Tuning, or ESFT, which tunes the experts most relevant to downstream tasks while freezing the other experts and modules; experimental results demonstrate that our method not only improves the tuning efficiency, but also matches or even surpasses the performance of full-parameter fine-tuning. (3) We further analyze the impact of the MoE architecture on expert-specialized fine-tuning. We find that MoE models with finer-grained experts are more advantageous in selecting the combination of experts that are most relevant to downstream tasks, thereby enhancing both the training efficiency and effectiveness. Our code is available at this https URL.

------------

`[2308.08268] It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models <https://arxiv.org/abs/2308.08268>`__ 

::

    replaced with revised version Thu, 4 Jul 2024 06:32:57 GMT
    Submission history From: Xingcheng Xu [view email]
    [v1] Wed, 16 Aug 2023 10:09:42 UTC (2,647 KB)
    [v2] Thu, 4 Jul 2024 06:32:57 UTC (3,442 KB)
    Xingcheng Xu, Zihao Pan, Haipeng Zhang, Yanqing Yang

Large language models (LLMs) have achieved remarkable proficiency on solving diverse problems. However, their generalization ability is not always satisfying and the generalization problem is common for generative transformer models in general. Researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. It is observed that when training models on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably on longer, unseen cases (out-of-distribution (OOD) generalization). We bring this unexplained performance drop into attention and ask whether there is systematic OOD generalization. Towards understanding LLMs, we train various smaller language models which may share the same underlying mechanism. We discover that the strong ID generalization stems from structured representations, while behind the unsatisfying OOD performance, the models still exhibit clear learned algebraic structures. Specifically, these models map unseen OOD inputs to outputs with learned equivalence relations in the ID domain, which we call the equivalence generalization. These findings deepen our knowledge regarding the generalizability of generative models including LLMs, and provide insights into potential avenues for improvement.

------------

`[2310.08419] Jailbreaking Black Box Large Language Models in Twenty Queries <https://arxiv.org/abs/2310.08419>`__ 用20个查询破解黑盒大型语言模型

::

    replaced with revised version Wed, 3 Jul 2024 19:50:34 GMT
    Submission history From: Alexander Robey [view email]
    [v1] Thu, 12 Oct 2023 15:38:28 UTC (2,411 KB)
    [v2] Fri, 13 Oct 2023 21:50:59 UTC (2,244 KB)
    [v3] Wed, 3 Jul 2024 19:50:34 UTC (8,075 KB)
    Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong

There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.

------------

`[2402.13148] Defending Jailbreak Prompts via In-Context Adversarial Game <https://arxiv.org/abs/2402.13148>`__ 基于情境对抗游戏的越狱提示防御

::

    replaced with revised version Fri, 5 Jul 2024 00:03:24 GMT
    Submission history From: Yujun Zhou [view email]
    [v1] Tue, 20 Feb 2024 17:04:06 UTC (386 KB)
    [v2] Fri, 5 Jul 2024 00:03:24 UTC (325 KB)
    Yujun Zhou, Yufei Han, Haomin Zhuang, Kehan Guo, Zhenwen Liang, Hongyan Bao and Xiangliang Zhang

Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover, ICAG demonstrates remarkable transferability to other LLMs, indicating its potential as a versatile defense mechanism.

------------

`[2403.13213] From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards <https://arxiv.org/abs/2403.13213>`__ 从代表性危害到服务质量危害:Llama 2安全保障案例研究

::

    replaced with revised version Fri, 5 Jul 2024 15:40:13 GMT
    Submission history From: Khaoula Chehbouni [view email]
    [v1] Wed, 20 Mar 2024 00:22:38 UTC (1,069 KB)
    [v2] Thu, 21 Mar 2024 02:27:57 UTC (1,069 KB)
    [v3] Sat, 8 Jun 2024 01:58:20 UTC (7,934 KB)
    [v4] Fri, 5 Jul 2024 15:40:13 UTC (7,934 KB)
    Khaoula Chehbouni, Megha Roshan, Emmanuel Ma, Futian Andrew Wei, Afaf Taik, Jackie CK Cheung, Golnoosh Farnadi

Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluating models on already mitigated biases. Using the case of Llama 2 as an example, we illustrate how LLMs' safety responses can still encode harmful assumptions. To do so, we create a set of non-toxic prompts, which we then use to evaluate Llama models. Through our new taxonomy of LLMs responses to users, we observe that the safety/helpfulness trade-offs are more pronounced for certain demographic groups which can lead to quality-of-service harms for marginalized populations.

------------

`[2404.17625] Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land <https://arxiv.org/abs/2404.17625>`__ 《爱丽丝漫游奇境记》第一卷，大地之旅

::

    replaced with revised version Thu, 4 Jul 2024 14:52:11 GMT
    Submission history From: Simone Scardapane [view email]
    [v1] Fri, 26 Apr 2024 15:19:58 UTC (13,152 KB)
    [v2] Thu, 4 Jul 2024 14:52:11 UTC (29,301 KB)
    Simone Scardapane

Neural networks surround us, in the form of large language models, speech transcription systems, molecular discovery algorithms, robotics, and much more. Stripped of anything else, neural networks are compositions of differentiable primitives, and studying them means learning how to program and how to interact with these models, a particular example of what is called differentiable programming.
This primer is an introduction to this fascinating field imagined for someone, like Alice, who has just ventured into this strange differentiable wonderland. I overview the basics of optimizing a function via automatic differentiation, and a selection of the most common designs for handling sequences, graphs, texts, and audios. The focus is on a intuitive, self-contained introduction to the most important design techniques, including convolutional, attentional, and recurrent blocks, hoping to bridge the gap between theory and code (PyTorch and JAX) and leaving the reader capable of understanding some of the most advanced models out there, such as large language models (LLMs) and multimodal architectures.

------------

`[2405.19320] Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF <https://arxiv.org/abs/2405.19320>`__ 价值激励偏好优化:线上线下RLHF的统一方法

::

    replaced with revised version Fri, 5 Jul 2024 04:59:42 GMT
    Submission history From: Shicong Cen [view email]
    [v1] Wed, 29 May 2024 17:51:42 UTC (789 KB)
    [v2] Tue, 4 Jun 2024 18:49:36 UTC (1,639 KB)
    [v3] Fri, 5 Jul 2024 04:59:42 UTC (1,637 KB)
    Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi, Bo Dai

Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.
In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\textit{sign}$ to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO.

------------

`[2305.13338] Gene Set Summarization using Large Language Models <https://arxiv.org/abs/2305.13338>`__ 基于大型语言模型的基因集摘要

::

    replaced with revised version Thu, 4 Jul 2024 02:16:11 GMT
    Submission history From: Marcin Joachimiak [view email]
    [v1] Sun, 21 May 2023 02:06:33 UTC (916 KB)
    [v2] Thu, 25 May 2023 19:10:13 UTC (917 KB)
    [v3] Thu, 4 Jul 2024 02:16:11 UTC (1,670 KB)
    Marcin P. Joachimiak, J. Harry Caufield, Nomi L. Harris, Hyeongsik Kim, Christopher J. Mungall

Molecular biologists frequently interpret gene lists derived from high-throughput experiments and computational analysis. This is typically done as a statistical enrichment analysis that measures the over- or under-representation of biological function terms associated with genes or their properties, based on curated assertions from a knowledge base (KB) such as the Gene Ontology (GO). Interpreting gene lists can also be framed as a textual summarization task, enabling the use of Large Language Models (LLMs), potentially utilizing scientific texts directly and avoiding reliance on a KB.
We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language Descriptions of Controlled Terms for Ontology Reporting), a method that uses GPT models to perform gene set function summarization as a complement to standard enrichment analysis. This method can use different sources of gene functional information: (1) structured text derived from curated ontological KB annotations, (2) ontology-free narrative gene summaries, or (3) direct model retrieval.
We demonstrate that these methods are able to generate plausible and biologically valid summary GO term lists for gene sets. However, GPT-based approaches are unable to deliver reliable scores or p-values and often return terms that are not statistically significant. Crucially, these methods were rarely able to recapitulate the most precise and informative term from standard enrichment, likely due to an inability to generalize and reason using an ontology. Results are highly nondeterministic, with minor variations in prompt resulting in radically different term lists. Our results show that at this point, LLM-based methods are unsuitable as a replacement for standard term enrichment analysis and that manual curation of ontological assertions remains necessary.

------------

`[2306.05036] Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Mining Insights at Scale <https://arxiv.org/abs/2306.05036>`__ 绘制人机交互的挑战:ChatGPT和GPT-4在大规模数据挖掘中的应用与评估

::

    replaced with revised version Thu, 4 Jul 2024 11:47:23 GMT
    Submission history From: Jonas Oppenlaender [view email]
    [v1] Thu, 8 Jun 2023 08:41:30 UTC (3,614 KB)
    [v2] Sat, 7 Oct 2023 14:56:40 UTC (3,722 KB)
    [v3] Tue, 12 Dec 2023 11:57:22 UTC (6,716 KB)
    [v4] Thu, 4 Jul 2024 11:47:23 UTC (6,558 KB)
    Jonas Oppenlaender, Joonas H\"am\"al\"ainen

Large language models (LLMs), such as ChatGPT and GPT-4, are gaining wide-spread real world use. Yet, these LLMs are closed source, and little is known about their performance in real-world use cases. In this paper, we apply and evaluate the combination of ChatGPT and GPT-4 for the real-world task of mining insights from a text corpus in order to identify research challenges in the field of HCI. We extract 4,392 research challenges in over 100 topics from the 2023~CHI conference proceedings and visualize the research challenges for interactive exploration. We critically evaluate the LLMs on this practical task and conclude that the combination of ChatGPT and GPT-4 makes an excellent cost-efficient means for analyzing a text corpus at scale. Cost-efficiency is key for flexibly prototyping research ideas and analyzing text corpora from different perspectives, with implications for applying LLMs for mining insights in academia and practice.

------------

`[2310.14804] Large Language Models can Share Images, Too! <https://arxiv.org/abs/2310.14804>`__ 大型语言模型也可以共享图像!

::

    replaced with revised version Thu, 4 Jul 2024 13:55:33 GMT
    Submission history From: Young-Jun Lee [view email]
    [v1] Mon, 23 Oct 2023 10:59:21 UTC (1,308 KB)
    [v2] Thu, 4 Jul 2024 13:55:33 UTC (1,109 KB)
    Young-Jun Lee, Dokyong Lee, Joo Won Sung, Jonghwan Hyeon, Ho-Jin Choi

This paper explores the image-sharing capability of Large Language Models (LLMs), such as GPT-4 and LLaMA 2, in a zero-shot setting. To facilitate a comprehensive evaluation of LLMs, we introduce the PhotoChat++ dataset, which includes enriched annotations (i.e., intent, triggering sentence, image description, and salient information). Furthermore, we present the gradient-free and extensible Decide, Describe, and Retrieve (DribeR) framework. With extensive experiments, we unlock the image-sharing capability of DribeR equipped with LLMs in zero-shot prompting, with ChatGPT achieving the best performance. Our findings also reveal the emergent image-sharing ability in LLMs under zero-shot conditions, validating the effectiveness of DribeR. We use this framework to demonstrate its practicality and effectiveness in two real-world scenarios: (1) human-bot interaction and (2) dataset augmentation. To the best of our knowledge, this is the first study to assess the image-sharing ability of various LLMs in a zero-shot setting. We make our source code and dataset publicly available at this https URL.

------------

`[2401.03315] Malla: Demystifying Real-world Large Language Model Integrated Malicious Services <https://arxiv.org/abs/2401.03315>`__ Malla:揭示集成恶意服务的真实大型语言模型

::

    replaced with revised version Thu, 4 Jul 2024 19:25:47 GMT
    Submission history From: Zilong Lin [view email]
    [v1] Sat, 6 Jan 2024 22:25:42 UTC (1,648 KB)
    [v2] Thu, 4 Jul 2024 19:25:47 UTC (1,353 KB)
    Zilong Lin, Jian Cui, Xiaojing Liao, XiaoFeng Wang

The underground exploitation of large language models (LLMs) for malicious services (i.e., Malla) is witnessing an uptick, amplifying the cyber threat landscape and posing questions about the trustworthiness of LLM technologies. However, there has been little effort to understand this new cybercrime, in terms of its magnitude, impact, and techniques. In this paper, we conduct the first systematic study on 212 real-world Mallas, uncovering their proliferation in underground marketplaces and exposing their operational modalities. Our study discloses the Malla ecosystem, revealing its significant growth and impact on today's public LLM services. Through examining 212 Mallas, we uncovered eight backend LLMs used by Mallas, along with 182 prompts that circumvent the protective measures of public LLM APIs. We further demystify the tactics employed by Mallas, including the abuse of uncensored LLMs and the exploitation of public LLM APIs through jailbreak prompts. Our findings enable a better understanding of the real-world exploitation of LLMs by cybercriminals, offering insights into strategies to counteract this cybercrime.

------------

`[2401.11061] PhotoBot: Reference-Guided Interactive Photography via Natural Language <https://arxiv.org/abs/2401.11061>`__ PhotoBot:基于自然语言的参考引导交互式摄影

::

    replaced with revised version Thu, 4 Jul 2024 16:08:38 GMT
    Submission history From: Oliver Limoyo [view email]
    [v1] Fri, 19 Jan 2024 23:34:48 UTC (23,835 KB)
    [v2] Wed, 20 Mar 2024 19:44:07 UTC (23,863 KB)
    [v3] Thu, 4 Jul 2024 16:08:38 UTC (23,748 KB)
    Oliver Limoyo, Jimmy Li, Dmitriy Rivkin, Jonathan Kelly, and Gregory Dudek

We introduce PhotoBot, a framework for fully automated photo acquisition based on an interplay between high-level human language guidance and a robot photographer. We propose to communicate photography suggestions to the user via reference images that are selected from a curated gallery. We leverage a visual language model (VLM) and an object detector to characterize the reference images via textual descriptions and then use a large language model (LLM) to retrieve relevant reference images based on a user's language query through text-based reasoning. To correspond the reference image and the observed scene, we exploit pre-trained features from a vision transformer capable of capturing semantic similarity across marked appearance variations. Using these features, we compute suggested pose adjustments for an RGB-D camera by solving a perspective-n-point (PnP) problem. We demonstrate our approach using a manipulator equipped with a wrist camera. Our user studies show that photos taken by PhotoBot are often more aesthetically pleasing than those taken by users themselves, as measured by human feedback. We also show that PhotoBot can generalize to other reference sources such as paintings.

------------

`[2401.13481] How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment <https://arxiv.org/abs/2401.13481>`__ 人工智能思想如何影响人类思想的创造力、多样性和进化:来自大型动态实验的证据

::

    replaced with revised version Thu, 4 Jul 2024 17:14:24 GMT
    Submission history From: Joshua Ashkinaze [view email]
    [v1] Wed, 24 Jan 2024 14:29:39 UTC (12,538 KB)
    [v2] Thu, 4 Jul 2024 17:14:24 UTC (9,085 KB)
    Joshua Ashkinaze, Julia Mendelsohn, Li Qiwei, Ceren Budak, Eric Gilbert

Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- speaks to the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made ideas different, not better. There were no main effects of disclosure. We also found that self-reported creative people were less influenced by knowing an idea was from AI and that participants may knowingly adopt AI ideas when the task is difficult. Our findings suggest that introducing AI ideas may increase collective diversity but not individual creativity.

------------

`[2401.16807] Detecting LLM-Assisted Writing in Scientific Communication: Are We There Yet? <https://arxiv.org/abs/2401.16807>`__ 探索llm辅助写作在科学交流中的应用:我们已经做到了吗?

::

    replaced with revised version Fri, 5 Jul 2024 14:19:36 GMT
    Submission history From: Teddy Lazebnik Dr. [view email]
    [v1] Tue, 30 Jan 2024 08:07:28 UTC (448 KB)
    [v2] Fri, 5 Jul 2024 14:19:36 UTC (468 KB)
    Teddy Lazebnik, Ariel Rosenfeld

Large Language Models (LLMs), exemplified by ChatGPT, have significantly reshaped text generation, particularly in the realm of writing assistance. While ethical considerations underscore the importance of transparently acknowledging LLM use, especially in scientific communication, genuine acknowledgment remains infrequent. A potential avenue to encourage accurate acknowledging of LLM-assisted writing involves employing automated detectors. Our evaluation of four cutting-edge LLM-generated text detectors reveals their suboptimal performance compared to a simple ad-hoc detector designed to identify abrupt writing style changes around the time of LLM proliferation. We contend that the development of specialized detectors exclusively dedicated to LLM-assisted writing detection is necessary. Such detectors could play a crucial role in fostering more authentic recognition of LLM involvement in scientific communication, addressing the current challenges in acknowledgment practices.

------------

`[2402.04615] ScreenAI: A Vision-Language Model for UI and Infographics Understanding <https://arxiv.org/abs/2402.04615>`__ ScreenAI:用于理解UI和信息图的视觉-语言模型

::

    replaced with revised version Thu, 4 Jul 2024 07:08:15 GMT
    Submission history From: Jindong Chen [view email]
    [v1] Wed, 7 Feb 2024 06:42:33 UTC (6,360 KB)
    [v2] Mon, 19 Feb 2024 17:03:36 UTC (6,345 KB)
    [v3] Thu, 4 Jul 2024 07:08:15 UTC (4,130 KB)
    Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor C\u{a}rbune, Jason Lin, Jindong Chen, Abhanshu Sharma

Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in-class performance on others (Chart QA, DocVQA, and InfographicVQA) compared to models of similar size. Finally, we release three new datasets: one focused on the screen annotation task and two others focused on question answering.

------------

`[2404.00579] A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys) <https://arxiv.org/abs/2404.00579>`__ 基于生成模型的现代推荐系统综述(Gen-RecSys)

::

    replaced with revised version Thu, 4 Jul 2024 15:06:42 GMT
    Submission history From: Zhankui He [view email]
    [v1] Sun, 31 Mar 2024 06:57:57 UTC (1,288 KB)
    [v2] Thu, 4 Jul 2024 15:06:42 UTC (213 KB)
    Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, Ren\'e Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, Silvia Milano

Traditional recommender systems (RS) typically use user-item rating histories as their main data source. However, deep generative models now have the capability to model and sample from complex data distributions, including user-item interactions, text, images, and videos, enabling novel recommendation tasks. This comprehensive, multidisciplinary survey connects key advancements in RS using Generative Models (Gen-RecSys), covering: interaction-driven generative models; the use of large language models (LLM) and textual data for natural language recommendation; and the integration of multimodal models for generating and processing images/videos in RS. Our work highlights necessary paradigms for evaluating the impact and harm of Gen-RecSys and identifies open challenges. This survey accompanies a tutorial presented at ACM KDD'24, with supporting materials provided at: this https URL.

------------

`[2404.07396] Can Base ChatGPT be Used for Forecasting without Additional Optimization? <https://arxiv.org/abs/2404.07396>`__ ChatGPT可以用于预测而不需要额外的优化吗?

::

    replaced with revised version Thu, 4 Jul 2024 23:03:45 GMT
    Submission history From: Van Pham [view email]
    [v1] Thu, 11 Apr 2024 00:03:03 UTC (3,365 KB)
    [v2] Sat, 13 Apr 2024 01:58:13 UTC (3,365 KB)
    [v3] Thu, 4 Jul 2024 23:03:45 UTC (2,771 KB)
    Van Pham and Scott Cunningham

This study investigates whether OpenAI's ChatGPT-3.5 and ChatGPT-4 can forecast future events. To evaluate the accuracy of the predictions, we take advantage of the fact that the training data at the time of our experiments (mid 2023) stopped at September 2021, and ask about events that happened in 2022. We employed two prompting strategies: direct prediction and what we call future narratives which ask ChatGPT to tell fictional stories set in the future with characters retelling events that happened in the past, but after ChatGPT's training data had been collected. We prompted ChatGPT to engage in storytelling, particularly within economic contexts. After analyzing 100 trials, we find that future narrative prompts significantly enhanced ChatGPT-4's forecasting accuracy. This was especially evident in its predictions of major Academy Award winners as well as economic trends, the latter inferred from scenarios where the model impersonated public figures like the Federal Reserve Chair, Jerome Powell. As a falsification exercise, we repeated our experiments in May 2024 at which time the models included more recent training data. ChatGPT-4's accuracy significantly improved when the training window included the events being prompted for, achieving 100% accuracy in many instances. The poorer accuracy for events outside of the training window suggests that in the 2023 prediction experiments, ChatGPT-4 was forming predictions based solely on its training data. Narrative prompting also consistently outperformed direct prompting. These findings indicate that narrative prompts leverage the models' capacity for hallucinatory narrative construction, facilitating more effective data synthesis and extrapolation than straightforward predictions. Our research reveals new aspects of LLMs' predictive capabilities and suggests potential future applications in analytical contexts.

------------

`[2406.18069] Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals <https://arxiv.org/abs/2406.18069>`__ 基于可穿戴生物信号的无袖血压测量大型语言模型

::

    replaced with revised version Fri, 5 Jul 2024 01:25:13 GMT
    Submission history From: Zengding Liu [view email]
    [v1] Wed, 26 Jun 2024 04:54:45 UTC (3,386 KB)
    [v2] Thu, 27 Jun 2024 03:58:25 UTC (3,386 KB)
    [v3] Fri, 5 Jul 2024 01:25:13 UTC (3,386 KB)
    Zengding Liu, Chen Chen, Jiannong Cao, Minglei Pan, Jikui Liu, Nan Li, Fen Miao, and Ye Li

Large language models (LLMs) have captured significant interest from both academia and industry due to their impressive performance across various textual tasks. However, the potential of LLMs to analyze physiological time-series data remains an emerging research field. Particularly, there is a notable gap in the utilization of LLMs for analyzing wearable biosignals to achieve cuffless blood pressure (BP) measurement, which is critical for the management of cardiovascular diseases. This paper presents the first work to explore the capacity of LLMs to perform cuffless BP estimation based on wearable biosignals. We extracted physiological features from electrocardiogram (ECG) and photoplethysmogram (PPG) signals and designed context-enhanced prompts by combining these features with BP domain knowledge and user information. Subsequently, we adapted LLMs to BP estimation tasks through fine-tuning. To evaluate the proposed approach, we conducted assessments of ten advanced LLMs using a comprehensive public dataset of wearable biosignals from 1,272 participants. The experimental results demonstrate that the optimally fine-tuned LLM significantly surpasses conventional task-specific baselines, achieving an estimation error of 0.00 $\pm$ 9.25 mmHg for systolic BP and 1.29 $\pm$ 6.37 mmHg for diastolic BP. Notably, the ablation studies highlight the benefits of our context enhancement strategy, leading to an 8.9% reduction in mean absolute error for systolic BP estimation. This paper pioneers the exploration of LLMs for cuffless BP measurement, providing a potential solution to enhance the accuracy of cuffless BP measurement.

------------

`[2406.19528] Using Large Language Models to Assist Video Content Analysis: An Exploratory Study of Short Videos on Depression <https://arxiv.org/abs/2406.19528>`__ 用大型语言模型辅助视频内容分析:抑郁症短视频的探索性研究

::

    replaced with revised version Thu, 4 Jul 2024 13:47:41 GMT
    Submission history From: Jiaying Liu [view email]
    [v1] Thu, 27 Jun 2024 21:03:56 UTC (10,855 KB)
    [v2] Thu, 4 Jul 2024 13:47:41 UTC (10,855 KB)
    Jiaying Liu, Yunlong Wang, Yao Lyu, Yiheng Su, Shuo Niu, Xuhai Orson Xu, Yan Zhang

Despite the growing interest in leveraging Large Language Models (LLMs) for content analysis, current studies have primarily focused on text-based content. In the present work, we explored the potential of LLMs in assisting video content analysis by conducting a case study that followed a new workflow of LLM-assisted multimodal content analysis. The workflow encompasses codebook design, prompt engineering, LLM processing, and human evaluation. We strategically crafted annotation prompts to get LLM Annotations in structured form and explanation prompts to generate LLM Explanations for a better understanding of LLM reasoning and transparency. To test LLM's video annotation capabilities, we analyzed 203 keyframes extracted from 25 YouTube short videos about depression. We compared the LLM Annotations with those of two human coders and found that LLM has higher accuracy in object and activity Annotations than emotion and genre Annotations. Moreover, we identified the potential and limitations of LLM's capabilities in annotating videos. Based on the findings, we explore opportunities and challenges for future research and improvements to the workflow. We also discuss ethical concerns surrounding future studies based on LLM-assisted video analysis.

------------

`[2407.02514] LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations <https://arxiv.org/abs/2407.02514>`__ LOGIC-LM++:符号公式的多步精化

::

    replaced with revised version Thu, 4 Jul 2024 21:49:07 GMT
    Submission history From: Shashank Kirtania [view email]
    [v1] Sat, 22 Jun 2024 12:50:41 UTC (10,543 KB)
    [v2] Thu, 4 Jul 2024 21:49:07 UTC (9,997 KB)
    Shashank Kirtania, Priyanshu Gupta, Arjun Radhakirshna

In this paper we examine the limitations of Large Language Models (LLMs) for complex reasoning tasks. Although recent works have started to employ formal languages as an intermediate representation for reasoning tasks, they often face challenges in accurately generating and refining these formal specifications to ensure correctness. To address these issues, this paper proposes Logic-LM++, an improvement on Logic-LM . It uses the ability of LLMs to do pairwise comparisons, allowing the evaluation of the refinements suggested by the LLM. The paper demonstrates that Logic-LM++ outperforms Logic-LM and other contemporary techniques across natural language reasoning tasks on three datasets, FOLIO, ProofWriter and AR-LSAT, with an average improvement of 18.5% on standard prompting, 12.3% on chain of thought prompting and 5% on Logic-LM.

------------

`[2407.01509] MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs <https://arxiv.org/abs/2407.01509>`__ MIA-Bench:多模态llm评估后的更好指导

::

    replaced with revised version Wed, 3 Jul 2024 18:11:45 GMT
    Submission history From: Zhe Gan [view email]
    [v1] Mon, 1 Jul 2024 17:53:35 UTC (27,031 KB)
    [v2] Wed, 3 Jul 2024 18:11:45 UTC (27,031 KB)
    Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, Zhe Gan

We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large language models (MLLMs) on their ability to strictly adhere to complex instructions. Our benchmark comprises a diverse set of 400 image-prompt pairs, each crafted to challenge the models' compliance with layered instructions in generating accurate responses that satisfy specific requested patterns. Evaluation results from a wide array of state-of-the-art MLLMs reveal significant variations in performance, highlighting areas for improvement in instruction fidelity. Additionally, we create extra training data and explore supervised fine-tuning to enhance the models' ability to strictly follow instructions without compromising performance on other tasks. We hope this benchmark not only serves as a tool for measuring MLLM adherence to instructions, but also guides future developments in MLLM training methods.

------------

`[2407.02395] Is Your AI-Generated Code Really Safe? Evaluating Large Language Models on Secure Code Generation with CodeSecEval <https://arxiv.org/abs/2407.02395>`__ 

::

    replaced with revised version Thu, 4 Jul 2024 08:59:31 GMT
    Submission history From: Jiexin Wang [view email]
    [v1] Tue, 2 Jul 2024 16:13:21 UTC (5,675 KB)
    [v2] Thu, 4 Jul 2024 08:59:31 UTC (5,675 KB)
    Jiexin Wang, Xitong Luo, Liuwen Cao, Hongkui He, Hailin Huang, Jiayuan Xie, Adam Jatowt, Yi Cai

Large language models (LLMs) have brought significant advancements to code generation and code repair, benefiting both novice and experienced developers. However, their training using unsanitized data from open-source repositories, like GitHub, raises the risk of inadvertently propagating security vulnerabilities. Despite numerous studies investigating the safety of code LLMs, there remains a gap in comprehensively addressing their security features. In this work, we aim to present a comprehensive study aimed at precisely evaluating and enhancing the security aspects of code LLMs. To support our research, we introduce CodeSecEval, a meticulously curated dataset designed to address 44 critical vulnerability types with 180 distinct samples. CodeSecEval serves as the foundation for the automatic evaluation of code models in two crucial tasks: code generation and code repair, with a strong emphasis on security. Our experimental results reveal that current models frequently overlook security issues during both code generation and repair processes, resulting in the creation of vulnerable code. In response, we propose different strategies that leverage vulnerability-aware information and insecure code explanations to mitigate these security vulnerabilities. Furthermore, our findings highlight that certain vulnerability types particularly challenge model performance, influencing their effectiveness in real-world applications. Based on these findings, we believe our study will have a positive impact on the software engineering community, inspiring the development of improved methods for training and utilizing LLMs, thereby leading to safer and more trustworthy model deployment.

------------

`[2307.07604] Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes <https://arxiv.org/abs/2307.07604>`__ 基于填充和置换指纹码的差分隐私算法下界平滑

::

    replaced with revised version Wed, 3 Jul 2024 19:22:10 GMT
    Submission history From: Eliad Tsfadia [view email]
    [v1] Fri, 14 Jul 2023 19:58:02 UTC (54 KB)
    [v2] Tue, 19 Sep 2023 19:06:02 UTC (54 KB)
    [v3] Sun, 4 Feb 2024 04:36:42 UTC (61 KB)
    [v4] Wed, 3 Jul 2024 19:22:10 UTC (48 KB)
    Naty Peter, Eliad Tsfadia, Jonathan Ullman

Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.
We present a new framework and tools to generate smooth lower bounds on the sample complexity of differentially private algorithms satisfying very weak accuracy. We illustrate the applicability of our method by providing new lower bounds in various settings:
1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).
2. A lower bound on the additive error of DP algorithms for approximate k-means clustering and general (k,z)-clustering, as a function of the multiplicative error, which is tight for a constant multiplication error.
3. A lower bound for estimating the top singular vector of a matrix under DP in low-accuracy regimes, which is a special case of DP subspace estimation studied by Singhal and Steinke (NeurIPS 2021).
Our main technique is to apply a padding-and-permuting transformation to a fingerprinting code. However, rather than proving our results using a black-box access to an existing fingerprinting code (e.g., Tardos' code), we develop a new fingerprinting lemma that is stronger than those of Dwork et al. (FOCS 2015) and Bun et al. (SODA 2017), and prove our lower bounds directly from the lemma. Our lemma, in particular, gives a simpler fingerprinting code construction with optimal rate (up to polylogarithmic factors) that is of independent interest.

------------

`[2401.10545] Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency <https://arxiv.org/abs/2401.10545>`__ 理解基于聊天gpt推荐系统中的偏差:提供者公平性、时间稳定性和时效性

::

    replaced with revised version Thu, 4 Jul 2024 12:59:01 GMT
    Submission history From: Yashar Deldjoo [view email]
    [v1] Fri, 19 Jan 2024 08:09:20 UTC (3,649 KB)
    [v2] Tue, 30 Jan 2024 09:00:16 UTC (3,649 KB)
    [v3] Thu, 4 Jul 2024 12:59:01 UTC (3,875 KB)
    Yashar Deldjoo

This paper explores the biases in ChatGPT-based recommender systems, focusing on provider fairness (item-side fairness). Through extensive experiments and over a thousand API calls, we investigate the impact of prompt design strategies-including structure, system role, and intent-on evaluation metrics such as provider fairness, catalog coverage, temporal stability, and recency. The first experiment examines these strategies in classical top-K recommendations, while the second evaluates sequential in-context learning (ICL).
In the first experiment, we assess seven distinct prompt scenarios on top-K recommendation accuracy and fairness. Accuracy-oriented prompts, like Simple and Chain-of-Thought (COT), outperform diversification prompts, which, despite enhancing temporal freshness, reduce accuracy by up to 50%. Embedding fairness into system roles, such as "act as a fair recommender," proved more effective than fairness directives within prompts. Diversification prompts led to recommending newer movies, offering broader genre distribution compared to traditional collaborative filtering (CF) models.
The second experiment explores sequential ICL, comparing zero-shot and few-shot ICL. Results indicate that including user demographic information in prompts affects model biases and stereotypes. However, ICL did not consistently improve item fairness and catalog coverage over zero-shot learning. Zero-shot learning achieved higher NDCG and coverage, while ICL-2 showed slight improvements in hit rate (HR) when age-group context was included. Our study provides insights into biases of RecLLMs, particularly in provider fairness and catalog coverage. By examining prompt design, learning strategies, and system roles, we highlight the potential and challenges of integrating LLMs into recommendation systems. Further details can be found at this https URL.

------------

-----------
Index (118)
-----------

`[2407.03652] Over the Edge of Chaos? Excess Complexity as a Roadblock to Artificial General Intelligence <https://arxiv.org/abs/2407.03652>`__ 在混乱的边缘?作为人工通用智能障碍的过度复杂性

`[2407.03759] Convolutional vs Large Language Models for Software Log Classification in Edge-Deployable Cellular Network Testing <https://arxiv.org/abs/2407.03759>`__ 可边部署蜂窝网络测试中软件日志分类的卷积与大型语言模型

`[2407.03942] Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data <https://arxiv.org/abs/2407.03942>`__ 基于合成数据的多样化细粒度指令遵循能力探索

`[2407.04106] MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis <https://arxiv.org/abs/2407.04106>`__

`[2407.04181] Orchestrating LLMs with Different Personalizations <https://arxiv.org/abs/2407.04181>`__ 对llm进行不同的个性化编排

`[2407.04467] Are Large Language Models Strategic Decision Makers? A Study of Performance and Bias in Two-Player Non-Zero-Sum Games <https://arxiv.org/abs/2407.04467>`__ 大型语言模型是战略决策者吗?对二人非零和博弈中的表现和偏见的研究

`[2407.03460] Collaborative Quest Completion with LLM-driven Non-Player Characters in Minecraft <https://arxiv.org/abs/2407.03460>`__ 在《我的世界》中与llm驱动的非玩家角色合作完成任务

`[2407.03518] Improving LLM Abilities in Idiomatic Translation <https://arxiv.org/abs/2407.03518>`__ 提高LLM在习语翻译中的能力

`[2407.03525] UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization <https://arxiv.org/abs/2407.03525>`__ UnSeenTimeQA:超越llm记忆的时间敏感问题回答

`[2407.03536] Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias <https://arxiv.org/abs/2407.03536>`__

`[2407.03572] Core: Robust Factual Precision Scoring with Informative Sub-Claim Identification <https://arxiv.org/abs/2407.03572>`__ 核心:具有信息性子索赔识别的鲁棒事实精度评分

`[2407.03582] Integrating Randomness in Large Language Models: A Linear Congruential Generator Approach for Generating Clinically Relevant Content <https://arxiv.org/abs/2407.03582>`__ 在大型语言模型中整合随机性:用于生成临床相关内容的线性同余生成器方法

`[2407.03615] Visualizing Dialogues: Enhancing Image Selection through Dialogue Understanding with Large Language Models <https://arxiv.org/abs/2407.03615>`__ 可视化对话:基于大型语言模型对话理解增强图像选择

`[2407.03621] The Mysterious Case of Neuron 1512: Injectable Realignment Architectures Reveal Internal Characteristics of Meta's Llama 2 Model <https://arxiv.org/abs/2407.03621>`__ 神经元1512的神秘案例:可注射的重组架构揭示了Meta的Llama 2模型的内部特征

`[2407.03651] Evaluating Language Model Context Windows: A "Working Memory" Test and Inference-time Correction <https://arxiv.org/abs/2407.03651>`__ 评估语言模型上下文窗口:“工作记忆”测试和推理时间校正

`[2407.03658] GPT-4 vs. Human Translators: A Comprehensive Evaluation of Translation Quality Across Languages, Domains, and Expertise Levels <https://arxiv.org/abs/2407.03658>`__ GPT-4 vs.人工翻译:跨语言、领域和专业水平的翻译质量综合评估

`[2407.03678] Improving Self Consistency in LLMs through Probabilistic Tokenization <https://arxiv.org/abs/2407.03678>`__

`[2407.03689] Text2TimeSeries: Enhancing Financial Forecasting through Time Series Prediction Updates with Event-Driven Insights from Large Language Models <https://arxiv.org/abs/2407.03689>`__ Text2TimeSeries:通过来自大型语言模型的事件驱动见解的时间序列预测更新来增强金融预测

`[2407.03805] Cognitive Modeling with Scaffolded LLMs: A Case Study of Referential Expression Generation <https://arxiv.org/abs/2407.03805>`__ 基于脚手架llm的认知建模:参考表达生成案例研究

`[2407.03850] HYBRINFOX at CheckThat! 2024 -- Task 1: Enhancing Language Models with Structured Information for Check-Worthiness Estimation <https://arxiv.org/abs/2407.03850>`__ HYBRINFOX在CheckThat!2024—任务1:用结构化信息增强语言模型以进行检查价值估计

`[2407.03859] Anthropocentric bias and the possibility of artificial cognition <https://arxiv.org/abs/2407.03859>`__ 人类中心偏见和人工认知的可能性

`[2407.03937] TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models <https://arxiv.org/abs/2407.03937>`__ 铜鼓:基于知识基础的大型语言模型掌握文言文理解

`[2407.03952] A framework for annotating and modelling intentions behind metaphor use <https://arxiv.org/abs/2407.03952>`__

`[2407.03963] LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs <https://arxiv.org/abs/2407.03963>`__ LLM-jp:一个研究和开发完全开放的日本llm的跨组织项目

`[2407.03964] Improving Sample Efficiency of Reinforcement Learning with Background Knowledge from Large Language Models <https://arxiv.org/abs/2407.03964>`__ 利用大型语言模型背景知识提高强化学习的样本效率

`[2407.03974] LLM Roleplay: Simulating Human-Chatbot Interaction <https://arxiv.org/abs/2407.03974>`__ LLM角色扮演:模拟人-聊天机器人交互

`[2407.03994] Unlocking the Potential of Model Merging for Low-Resource Languages <https://arxiv.org/abs/2407.03994>`__ 释放低资源语言模型合并的潜力

`[2407.04020] LLMAEL: Large Language Models are Good Context Augmenters for Entity Linking <https://arxiv.org/abs/2407.04020>`__ LLMAEL:大型语言模型是实体链接的良好上下文增强器

`[2407.04046] Systematic Task Exploration with LLMs: A Study in Citation Text Generation <https://arxiv.org/abs/2407.04046>`__ LLMs的系统任务探索:引文文本生成研究

`[2407.04067] Semantic Graphs for Syntactic Simplification: A Revisit from the Age of LLM <https://arxiv.org/abs/2407.04067>`__

`[2407.04093] Stephanie: Step-by-Step Dialogues for Mimicking Human Interactions in Social Conversations <https://arxiv.org/abs/2407.04093>`__ Stephanie:在社交对话中模仿人类互动的步骤对话

`[2407.04118] MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization <https://arxiv.org/abs/2407.04118>`__ MAPO:基于模型自适应提示优化的大型语言模型性能提升

`[2407.04121] Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models <https://arxiv.org/abs/2407.04121>`__ 幻觉检测:在大型语言模型中鲁棒地识别可靠答案

`[2407.04125] Query-Guided Self-Supervised Summarization of Nursing Notes <https://arxiv.org/abs/2407.04125>`__ 基于查询引导的自我监督护理笔记摘要

`[2407.04130] Towards Automating Text Annotation: A Case Study on Semantic Proximity Annotation using GPT-4 <https://arxiv.org/abs/2407.04130>`__ 迈向文本标注自动化:基于GPT-4的语义接近标注案例研究

`[2407.04151] Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers <https://arxiv.org/abs/2407.04151>`__ 针对分布式后门触发器保护多轮对话语言模型

`[2407.04179] Defense Against Syntactic Textual Backdoor Attacks with Token Substitution <https://arxiv.org/abs/2407.04179>`__ 用标记替换防御语法文本后门攻击

`[2407.04183] Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality Norms <https://arxiv.org/abs/2407.04183>`__

`[2407.04185] HAF-RM: A Hybrid Alignment Framework for Reward Model Training <https://arxiv.org/abs/2407.04185>`__ HAF-RM:用于奖励模型训练的混合对齐框架

`[2407.04279] BiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks <https://arxiv.org/abs/2407.04279>`__ BiosERC:集成由llm支持的ERC任务传记演讲者

`[2407.04307] Crafting Large Language Models for Enhanced Interpretability <https://arxiv.org/abs/2407.04307>`__ 构建增强可解释性的大型语言模型

`[2407.04434] From 'Showgirls' to 'Performers': Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs <https://arxiv.org/abs/2407.04434>`__ 从“歌舞女郎”到“表演者”:用性别包容性语言进行微调，以减少llm中的偏见

`[2407.04459] Generalists vs. Specialists: Evaluating Large Language Models for Urdu <https://arxiv.org/abs/2407.04459>`__ 通才vs.专家:评估乌尔都语大型语言模型

`[2407.04466] Using LLMs to label medical papers according to the CIViC evidence model <https://arxiv.org/abs/2407.04466>`__ 根据公民证据模型使用llm标记医学论文

`[2407.04541] PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit Posts <https://arxiv.org/abs/2407.04541>`__ PoPreRo:用于预测罗马尼亚Reddit帖子流行度的新数据集

`[2407.04629] Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework <https://arxiv.org/abs/2407.04629>`__

`[2407.04693] ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models <https://arxiv.org/abs/2407.04693>`__ ANAH-v2:大型语言模型的尺度分析幻觉标注

`[2407.04694] Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs <https://arxiv.org/abs/2407.04694>`__ 我、我自己和AI: llm的情景感知数据集(SAD)

`[2407.03637] HERA: High-efficiency Matrix Compression via Element Replacement <https://arxiv.org/abs/2407.03637>`__ HERA:基于元素替换的高效矩阵压缩

`[2407.03640] Generative Technology for Human Emotion Recognition: A Scope Review <https://arxiv.org/abs/2407.03640>`__ 面向人类情感识别的生成技术研究范围综述

`[2407.03674] Short-Long Policy Evaluation with Novel Actions <https://arxiv.org/abs/2407.03674>`__ 具有新颖行动的长短政策评估

`[2407.03856] Q-Adapter: Training Your LLM Adapter as a Residual Q-Function <https://arxiv.org/abs/2407.03856>`__ Q-Adapter:将LLM适配器训练为残差q函数

`[2407.03951] Uncertainty-Guided Optimization on Large Language Model Search Trees <https://arxiv.org/abs/2407.03951>`__ 不确定性引导的大型语言模型搜索树优化

`[2407.04173] Quantifying Prediction Consistency Under Model Multiplicity in Tabular LLMs <https://arxiv.org/abs/2407.04173>`__ 表式llm中模型多样性下预测一致性的量化

`[2407.04480] LoCo: Low-Bit Communication Adaptor for Large-scale Model Training <https://arxiv.org/abs/2407.04480>`__ LoCo:面向大规模模型训练的低比特通信适配器

`[2407.04622] On scalable oversight with weak LLMs judging strong LLMs <https://arxiv.org/abs/2407.04622>`__

`[2407.03381] SeqMate: A Novel Large Language Model Pipeline for Automating RNA Sequencing <https://arxiv.org/abs/2407.03381>`__ SeqMate:一种用于自动化RNA测序的新型大型语言模型管道

`[2407.03387] ConCodeEval: Evaluating Large Language Models for Code Constraints in Domain-Specific Languages <https://arxiv.org/abs/2407.03387>`__ ConCodeEval:面向特定领域语言代码约束的大型语言模型评估

`[2407.03469] Scaling Data-Driven Building Energy Modelling using Large Language Models <https://arxiv.org/abs/2407.03469>`__ 使用大型语言模型扩展数据驱动建筑能耗建模

`[2407.03611] An Empirical Study on Capability of Large Language Models in Understanding Code Semantics <https://arxiv.org/abs/2407.03611>`__ 大型语言模型理解代码语义能力的实证研究

`[2407.04051] FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs <https://arxiv.org/abs/2407.04051>`__ FunAudioLLM:人类与llm自然交互的语音理解和生成基础模型

`[2407.04411] Waterfall: Framework for Robust and Scalable Text Watermarking <https://arxiv.org/abs/2407.04411>`__

`[2407.04418] Enabling On-Device LLMs Personalization with Smartphone Sensing <https://arxiv.org/abs/2407.04418>`__ 通过智能手机感知实现设备上的llm个性化

`[2407.04472] EventChat: Implementation and user-centric evaluation of a large language model-driven conversational recommender system for exploring leisure events in an SME context <https://arxiv.org/abs/2407.04472>`__ EventChat:一个大型语言模型驱动的会话推荐系统的实现和以用户为中心的评估，用于探索中小企业环境中的休闲事件

`[2407.04503] When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions <https://arxiv.org/abs/2407.04503>`__ llm玩电话游戏时:迭代文化传播中的累积变化和吸引者

`[2407.04681] Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge <https://arxiv.org/abs/2407.04681>`__ 基于外部知识的多模态大型语言模型视觉提示的再思考

`[2407.03876] DART: Deep Adversarial Automated Red Teaming for LLM Safety <https://arxiv.org/abs/2407.03876>`__ DART:面向LLM安全的深度对抗性自动化红色团队

`[2407.04482] Controlling Whisper: Universal Acoustic Adversarial Attacks to Control Speech Foundation Models <https://arxiv.org/abs/2407.04482>`__ 控制耳语:控制语音基础模型的通用声学对抗攻击

`[2407.03453] On Large Language Models in National Security Applications <https://arxiv.org/abs/2407.03453>`__

`[2407.04065] On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards <https://arxiv.org/abs/2407.04065>`__ 基于LBOps的排行榜操作(workflow and smell of Leaderboard Operations):基础模型排行榜的探索性研究

`[2407.04108] Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs <https://arxiv.org/abs/2407.04108>`__ 作为后门触发器的未来事件:研究llm中的时间漏洞

`[2407.04656] Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models with Adaptive Expert Placement <https://arxiv.org/abs/2407.04656>`__ Lazarus:基于自适应专家布局的弹性混合专家模型训练

`[2403.04449] Feedback-Generation for Programming Exercises With GPT-4 <https://arxiv.org/abs/2403.04449>`__ 使用GPT-4编程练习的反馈生成

`[2406.14343] IWISDM: Assessing instruction following in multimodal models at scale <https://arxiv.org/abs/2406.14343>`__ IWISDM:评估大规模多模态模型的指令跟随

`[2406.19859] MetaDesigner: Advancing Artistic Typography through AI-Driven, User-Centric, and Multilingual WordArt Synthesis <https://arxiv.org/abs/2406.19859>`__ MetaDesigner:通过人工智能驱动、以用户为中心和多语言文字艺术合成来推进艺术排版

`[2308.13782] Planning with Logical Graph-based Language Model for Instruction Generation <https://arxiv.org/abs/2308.13782>`__ 基于逻辑图语言模型的指令生成规划

`[2310.04444] What's the Magic Word? A Control Theory of LLM Prompting <https://arxiv.org/abs/2310.04444>`__ 最神奇的词是什么?LLM激励的控制理论

`[2311.09132] Aligning Neural Machine Translation Models: Human Feedback in Training and Inference <https://arxiv.org/abs/2311.09132>`__ 神经机器翻译模型对齐:训练和推理中的人工反馈

`[2311.09684] Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation <https://arxiv.org/abs/2311.09684>`__ 医生知道如何提示吗?对自动提示优化的需求有助于临床病历的生成

`[2312.04691] Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models <https://arxiv.org/abs/2312.04691>`__ simulm - llm:基于大型语言模型探索高质量同传的框架

`[2312.06681] Steering Llama 2 via Contrastive Activation Addition <https://arxiv.org/abs/2312.06681>`__ 通过对比激活添加转向美洲驼2

`[2402.00371] What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection <https://arxiv.org/abs/2402.00371>`__ 机器人会说什么?大型语言模型在社交媒体机器人检测中的机会和风险

`[2402.06341] RareBench: Can LLMs Serve as Rare Diseases Specialists? <https://arxiv.org/abs/2402.06341>`__ RareBench: llm可以成为罕见疾病专家吗?

`[2402.10426] DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection <https://arxiv.org/abs/2402.10426>`__ DELL:对基于llm的错误信息检测的反应和解释

`[2402.14016] Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment <https://arxiv.org/abs/2402.14016>`__ llm -as- judge稳健吗?调查零样本LLM评估上的通用对抗性攻击

`[2402.14499] "My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models <https://arxiv.org/abs/2402.14499>`__ "My Answer is C":在指令调优的语言模型中，第一个token的概率与文本答案不匹配

`[2403.02130] Using LLMs for the Extraction and Normalization of Product Attribute Values <https://arxiv.org/abs/2403.02130>`__ 使用LLMs进行产品属性值的提取和归一化

`[2403.05881] KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques <https://arxiv.org/abs/2403.05881>`__ KG-Rank:基于知识图谱和排名技术的医学问答大型语言模型增强

`[2403.17104] Attribute First, then Generate: Locally-attributable Grounded Text Generation <https://arxiv.org/abs/2403.17104>`__ 先属性，再生成:局部可归因的基础文本生成

`[2403.20041] Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs <https://arxiv.org/abs/2403.20041>`__ Transformer-Lite:手机gpu上大型语言模型的高效部署

`[2404.06709] CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers <https://arxiv.org/abs/2404.06709>`__ CQIL:准独立层并发计算的推理延迟优化

`[2406.00656] Presence or Absence: Are Unknown Word Usages in Dictionaries? <https://arxiv.org/abs/2406.00656>`__ 存在还是缺席:词典中未登录词的用法?

`[2406.14657] OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset <https://arxiv.org/abs/2406.14657>`__ OpenDebateEvidence:大规模论据挖掘和摘要数据集

`[2406.17807] Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary <https://arxiv.org/abs/2406.17807>`__ 非完全信息牌游戏解说策略的增强——关丹解说大型语言模型研究

`[2407.01906] Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models <https://arxiv.org/abs/2407.01906>`__ 让专家坚持他的最后一个:专家专门的稀疏架构大型语言模型的微调

`[2308.08268] It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models <https://arxiv.org/abs/2308.08268>`__

`[2310.08419] Jailbreaking Black Box Large Language Models in Twenty Queries <https://arxiv.org/abs/2310.08419>`__ 用20个查询破解黑盒大型语言模型

`[2402.13148] Defending Jailbreak Prompts via In-Context Adversarial Game <https://arxiv.org/abs/2402.13148>`__ 基于情境对抗游戏的越狱提示防御

`[2403.13213] From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards <https://arxiv.org/abs/2403.13213>`__ 从代表性危害到服务质量危害:Llama 2安全保障案例研究

`[2404.17625] Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land <https://arxiv.org/abs/2404.17625>`__ 《爱丽丝漫游奇境记》第一卷，大地之旅

`[2405.19320] Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF <https://arxiv.org/abs/2405.19320>`__ 价值激励偏好优化:线上线下RLHF的统一方法

`[2305.13338] Gene Set Summarization using Large Language Models <https://arxiv.org/abs/2305.13338>`__ 基于大型语言模型的基因集摘要

`[2306.05036] Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Mining Insights at Scale <https://arxiv.org/abs/2306.05036>`__ 绘制人机交互的挑战:ChatGPT和GPT-4在大规模数据挖掘中的应用与评估

`[2310.14804] Large Language Models can Share Images, Too! <https://arxiv.org/abs/2310.14804>`__ 大型语言模型也可以共享图像!

`[2401.03315] Malla: Demystifying Real-world Large Language Model Integrated Malicious Services <https://arxiv.org/abs/2401.03315>`__ Malla:揭示集成恶意服务的真实大型语言模型

`[2401.11061] PhotoBot: Reference-Guided Interactive Photography via Natural Language <https://arxiv.org/abs/2401.11061>`__ PhotoBot:基于自然语言的参考引导交互式摄影

`[2401.13481] How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment <https://arxiv.org/abs/2401.13481>`__ 人工智能思想如何影响人类思想的创造力、多样性和进化:来自大型动态实验的证据

`[2401.16807] Detecting LLM-Assisted Writing in Scientific Communication: Are We There Yet? <https://arxiv.org/abs/2401.16807>`__ 探索llm辅助写作在科学交流中的应用:我们已经做到了吗?

`[2402.04615] ScreenAI: A Vision-Language Model for UI and Infographics Understanding <https://arxiv.org/abs/2402.04615>`__ ScreenAI:用于理解UI和信息图的视觉-语言模型

`[2404.00579] A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys) <https://arxiv.org/abs/2404.00579>`__ 基于生成模型的现代推荐系统综述(Gen-RecSys)

`[2404.07396] Can Base ChatGPT be Used for Forecasting without Additional Optimization? <https://arxiv.org/abs/2404.07396>`__ ChatGPT可以用于预测而不需要额外的优化吗?

`[2406.18069] Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals <https://arxiv.org/abs/2406.18069>`__ 基于可穿戴生物信号的无袖血压测量大型语言模型

`[2406.19528] Using Large Language Models to Assist Video Content Analysis: An Exploratory Study of Short Videos on Depression <https://arxiv.org/abs/2406.19528>`__ 用大型语言模型辅助视频内容分析:抑郁症短视频的探索性研究

`[2407.02514] LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations <https://arxiv.org/abs/2407.02514>`__ LOGIC-LM++:符号公式的多步精化

`[2407.01509] MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs <https://arxiv.org/abs/2407.01509>`__ MIA-Bench:多模态llm评估后的更好指导

`[2407.02395] Is Your AI-Generated Code Really Safe? Evaluating Large Language Models on Secure Code Generation with CodeSecEval <https://arxiv.org/abs/2407.02395>`__

`[2307.07604] Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes <https://arxiv.org/abs/2307.07604>`__ 基于填充和置换指纹码的差分隐私算法下界平滑

`[2401.10545] Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency <https://arxiv.org/abs/2401.10545>`__ 理解基于聊天gpt推荐系统中的偏差:提供者公平性、时间稳定性和时效性

