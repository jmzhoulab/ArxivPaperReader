240702
========

----------
Survey (6)
----------

`[2407.00936] Large Language Model Enhanced Knowledge Representation Learning: A Survey <https://arxiv.org/abs/2407.00936>`__ 大规模语言模型增强的知识表示学习综述

::

    Mon, 1 Jul 2024 03:37:35 GMT
    Xin Wang, Zirui Chen, Haofen Wang, Leong Hou U, Zhao Li, Wenbin Guo

The integration of Large Language Models (LLMs) with Knowledge Representation Learning (KRL) signifies a pivotal advancement in the field of artificial intelligence, enhancing the ability to capture and utilize complex knowledge structures. This synergy leverages the advanced linguistic and contextual understanding capabilities of LLMs to improve the accuracy, adaptability, and efficacy of KRL, thereby expanding its applications and potential. Despite the increasing volume of research focused on embedding LLMs within the domain of knowledge representation, a thorough review that examines the fundamental components and processes of these enhanced models is conspicuously absent. Our survey addresses this by categorizing these models based on three distinct Transformer architectures, and by analyzing experimental data from various KRL downstream tasks to evaluate the strengths and weaknesses of each approach.
Finally, we identify and explore potential future research directions in this emerging yet underexplored domain, proposing pathways for continued progress.

------------

`[2407.00118] From Efficient Multimodal Models to World Models: A Survey <https://arxiv.org/abs/2407.00118>`__ 

::

    Thu, 27 Jun 2024 15:36:43 GMT
    Xinji Mai, Zeng Tao, Junxiong Lin, Haoran Wang, Yang Chang, Yanlan Kang, Yan Wang, Wenqiang Zhang

Multimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.

------------

`[2407.00125] A Survey on Failure Analysis and Fault Injection in AI Systems <https://arxiv.org/abs/2407.00125>`__ 人工智能系统故障分析与故障注入综述

::

    Fri, 28 Jun 2024 00:32:03 GMT
    Guangba Yu, Gou Tan, Haojia Huang, Zhenyu Zhang, Pengfei Chen, Roberto Natella, Zibin Zheng

The rapid advancement of Artificial Intelligence (AI) has led to its integration into various areas, especially with Large Language Models (LLMs) significantly enhancing capabilities in Artificial Intelligence Generated Content (AIGC). However, the complexity of AI systems has also exposed their vulnerabilities, necessitating robust methods for failure analysis (FA) and fault injection (FI) to ensure resilience and reliability. Despite the importance of these techniques, there lacks a comprehensive review of FA and FI methodologies in AI systems. This study fills this gap by presenting a detailed survey of existing FA and FI approaches across six layers of AI systems. We systematically analyze 160 papers and repositories to answer three research questions including (1) what are the prevalent failures in AI systems, (2) what types of faults can current FI tools simulate, (3) what gaps exist between the simulated faults and real-world failures. Our findings reveal a taxonomy of AI system failures, assess the capabilities of existing FI tools, and highlight discrepancies between real-world and simulated failures. Moreover, this survey contributes to the field by providing a framework for fault diagnosis, evaluating the state-of-the-art in FI, and identifying areas for improvement in FI techniques to enhance the resilience of AI systems.

------------

`[2310.11829] Towards Graph Foundation Models: A Survey and Beyond <https://arxiv.org/abs/2310.11829>`__ 图基础模型研究综述

::

    replaced with revised version Mon, 1 Jul 2024 02:06:42 GMT
    Submission history From: Jiawei Liu [view email]
    [v1] Wed, 18 Oct 2023 09:31:21 UTC (3,104 KB)
    [v2] Sat, 2 Dec 2023 08:36:17 UTC (4,949 KB)
    [v3] Mon, 1 Jul 2024 02:06:42 UTC (4,839 KB)
    Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, Chuan Shi

Foundation models have emerged as critical components in a variety of artificial intelligence applications, and showcase significant success in natural language processing and several other domains. Meanwhile, the field of graph machine learning is witnessing a paradigm transition from shallow methods to more sophisticated deep learning approaches. The capabilities of foundation models to generalize and adapt motivate graph machine learning researchers to discuss the potential of developing a new graph learning paradigm. This paradigm envisions models that are pre-trained on extensive graph data and can be adapted for various graph tasks. Despite this burgeoning interest, there is a noticeable lack of clear definitions and systematic analyses pertaining to this new domain. To this end, this article introduces the concept of Graph Foundation Models (GFMs), and offers an exhaustive explanation of their key characteristics and underlying technologies. We proceed to classify the existing work related to GFMs into three distinct categories, based on their dependence on graph neural networks and large language models. In addition to providing a thorough review of the current state of GFMs, this article also outlooks potential avenues for future research in this rapidly evolving domain.

------------

`[2404.16789] Continual Learning of Large Language Models: A Comprehensive Survey <https://arxiv.org/abs/2404.16789>`__ 大型语言模型持续学习综述

::

    replaced with revised version Sun, 30 Jun 2024 02:19:00 GMT
    Submission history From: Haizhou Shi [view email]
    [v1] Thu, 25 Apr 2024 17:38:57 UTC (703 KB)
    [v2] Sun, 30 Jun 2024 02:19:00 UTC (1,209 KB)
    Haizhou Shi and Zihao Xu and Hengyi Wang and Weiyi Qin and Wenyuan Wang and Yibin Wang and Zifeng Wang and Sayna Ebrahimi and Hao Wang

The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications. One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences. Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as "catastrophic forgetting". While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs. In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL. This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5). Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6). The full list of papers examined in this survey is available at this https URL.

------------

`[2404.02817] A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches <https://arxiv.org/abs/2404.02817>`__ 

::

    replaced with revised version Sun, 30 Jun 2024 23:56:53 GMT
    Submission history From: Zhigen Zhao [view email]
    [v1] Wed, 3 Apr 2024 15:38:36 UTC (18,394 KB)
    [v2] Fri, 5 Apr 2024 09:06:00 UTC (18,395 KB)
    [v3] Fri, 19 Apr 2024 14:26:25 UTC (18,395 KB)
    [v4] Sun, 30 Jun 2024 23:56:53 UTC (19,376 KB)
    Zhigen Zhao, Shuo Cheng, Yan Ding, Ziyi Zhou, Shiqi Zhang, Danfei Xu, Ye Zhao

Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A particular focus of this survey is to highlight the algorithm structures to efficiently solve TAMP, especially hierarchical and distributed approaches. Additionally, the survey emphasizes the synergy between the classical methods and contemporary learning-based innovations such as large language models. Furthermore, the future research directions for TAMP is discussed in this survey, highlighting both algorithmic and application-specific challenges.

------------

--------------
Benchmark (17)
--------------

`[2407.00379] GraphArena: Benchmarking Large Language Models on Graph Computational Problems <https://arxiv.org/abs/2407.00379>`__ GraphArena:图计算问题上的大型语言模型基准测试

::

    Sat, 29 Jun 2024 09:19:23 GMT
    Jianheng Tang, Qifan Zhang, Yuhan Li, Jia Li

The "arms race" of Large Language Models (LLMs) demands novel, challenging, and diverse benchmarks to faithfully examine their progresses. We introduce GraphArena, a benchmarking tool designed to evaluate LLMs on graph computational problems using million-scale real-world graphs from diverse scenarios such as knowledge graphs, social networks, and molecular structures.
GraphArena offers a suite of 10 computational tasks, encompassing four polynomial-time (e.g., Shortest Distance) and six NP-complete challenges (e.g., Travelling Salesman Problem). It features a rigorous evaluation framework that classifies LLM outputs as correct, suboptimal (feasible but not optimal), or hallucinatory (properly formatted but infeasible). Evaluation of 10 leading LLMs, including GPT-4o and LLaMA3-70B-Instruct, reveals that even top-performing models struggle with larger, more complex graph problems and exhibit hallucination issues. Despite the application of strategies such as chain-of-thought prompting, these issues remain unresolved. GraphArena contributes a valuable supplement to the existing LLM benchmarks and is open-sourced at https://github.com/squareRoot3/GraphArena.

------------

`[2407.00993] Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents <https://arxiv.org/abs/2407.00993>`__ Mobile- bench:一个基于llm的移动agent评测基准

::

    Mon, 1 Jul 2024 06:10:01 GMT
    Shihan Deng, Weikai Xu, Hongda Sun, Wei Liu, Tao Tan, Jianfeng Liu, Ang Li, Jian Luan, Bin Wang, Rui Yan, Shuo Shang

With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents.
Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents.
First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps.

------------

`[2407.00377] The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention <https://arxiv.org/abs/2407.00377>`__ 多样性干预的文本到图像生成的事实税:基准和事实增强干预

::

    Sat, 29 Jun 2024 09:09:42 GMT
    Yixin Wan, Di Wu, Haoran Wang, Kai-Wei Chang

Prompt-based "diversity interventions" are commonly adopted to improve the diversity of Text-to-Image (T2I) models depicting individuals with various racial or gender traits. However, will this strategy result in nonfactual demographic distribution, especially when generating real historical figures? In this work, we propose DemOgraphic FActualIty Representation (DoFaiR), a benchmark to systematically quantify the trade-off between using diversity interventions and preserving demographic factuality in T2I models. DoFaiR consists of 756 meticulously fact-checked test instances to reveal the factuality tax of various diversity prompts through an automated evidence-supported evaluation pipeline. Experiments on DoFaiR unveil that diversity-oriented instructions increase the number of different gender and racial groups in DALLE-3's generations at the cost of historically inaccurate demographic distributions. To resolve this issue, we propose Fact-Augmented Intervention (FAI), which instructs a Large Language Model (LLM) to reflect on verbalized or retrieved factual information about gender and racial compositions of generation subjects in history, and incorporate it into the generation context of T2I models. By orienting model generations using the reflected historical truths, FAI significantly improves the demographic factuality under diversity interventions while preserving diversity.

------------

`[2407.00466] BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science <https://arxiv.org/abs/2407.00466>`__ BioKGBench:面向生物医学的人工智能主体知识图谱检测基准

::

    Sat, 29 Jun 2024 15:23:28 GMT
    Xinna Lin, Siqi Ma, Junjie Shan, Xiaojing Zhang, Shell Xu Hu, Tiannan Guo, Stan Z. Li, Kaicheng Yu

Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models (LLMs). However, to evaluate such systems, people either rely on direct Question-Answering (QA) to the LLM itself, or in a biomedical experimental manner. How to precisely benchmark biomedical agents from an AI Scientist perspective remains largely unexplored.
To this end, we draw inspiration from one most important abilities of scientists, understanding the literature, and introduce BioKGBench. In contrast to traditional evaluation benchmark that only focuses on factual QA, where the LLMs are known to have hallucination issues, we first disentangle "Understanding Literature" into two atomic abilities, i) "Understanding" the unstructured text from research papers by performing scientific claim verification, and ii) Ability to interact with structured Knowledge-Graph Question-Answering (KGQA) as a form of "Literature" grounding. We then formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based Retrieval-Augmented Generation (RAG) to identify the factual errors of existing large-scale knowledge graph databases. We collect over two thousand data for two atomic tasks and 225 high-quality annotated data for the agent task.
Surprisingly, we discover that state-of-the-art agents, both daily scenarios and biomedical ones, have either failed or inferior performance on our benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent.
On the widely used popular knowledge graph, we discover over 90 factual errors which provide scenarios for agents to make discoveries and demonstrate the effectiveness of our approach. The code and data are available at https://github.com/westlake-autolab/BioKGBench.

------------

`[2407.00924] EXCGEC: A Benchmark of Edit-wise Explainable Chinese Grammatical Error Correction <https://arxiv.org/abs/2407.00924>`__ EXCGEC:面向编辑的可解释性中文语法纠错基准

::

    Mon, 1 Jul 2024 03:06:41 GMT
    Jingheng Ye, Shang Qin, Yinghui Li, Xuxin Cheng, Libo Qin, Hai-Tao Zheng, Peng Xing, Zishan Xu, Guo Cheng, Zhao Wei

Existing studies explore the explainability of Grammatical Error Correction (GEC) in a limited scenario, where they ignore the interaction between corrections and explanations. To bridge the gap, this paper introduces the task of EXplainable GEC (EXGEC), which focuses on the integral role of both correction and explanation tasks. To facilitate the task, we propose EXCGEC, a tailored benchmark for Chinese EXGEC consisting of 8,216 explanation-augmented samples featuring the design of hybrid edit-wise explanations. We benchmark several series of LLMs in multiple settings, covering post-explaining and pre-explaining. To promote the development of the task, we introduce a comprehensive suite of automatic metrics and conduct human evaluation experiments to demonstrate the human consistency of the automatic metrics for free-text explanations. All the codes and data will be released after the review.

------------

`[2407.01102] BERGEN: A Benchmarking Library for Retrieval-Augmented Generation <https://arxiv.org/abs/2407.01102>`__ 

::

    Mon, 1 Jul 2024 09:09:27 GMT
    David Rau, Herv\'e D\'ejean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, St\'ephane Clinchant

Retrieval-Augmented Generation allows to enhance Large Language Models with external knowledge. In response to the recent popularity of generative LLMs, many RAG approaches have been proposed, which involve an intricate number of different configurations such as evaluation datasets, collections, metrics, retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in comparing approaches and understanding the impact of each component in the pipeline. In this work, we study best practices that lay the groundwork for a systematic evaluation of RAG and present BERGEN, an end-to-end library for reproducible research standardizing RAG experiments. In an extensive study focusing on QA, we benchmark different state-of-the-art retrievers, rerankers, and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our open-source library BERGEN is available under \url{https://github.com/naver/bergen}.

------------

`[2407.01527] KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches <https://arxiv.org/abs/2407.01527>`__ KV缓存压缩，但我们必须提供什么回报?长上下文能力方法的综合基准

::

    Mon, 1 Jul 2024 17:59:47 GMT
    Jiayi Yuan, Hongyi Liu, Shaochen (Henry) Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, Xia Hu

Long context capability is a crucial competency for large language models (LLMs) as it mitigates the human struggle to digest long-form texts. This capability enables complex task-solving scenarios such as book summarization, code assistance, and many more tasks that are traditionally manpower-intensive.
However, transformer-based LLMs face significant challenges with long context input due to the growing size of the KV cache and the intrinsic complexity of attending to extended inputs; where multiple schools of efficiency-driven approaches -- such as KV cache quantization, token dropping, prompt compression, linear-time sequence models, and hybrid architectures -- have been proposed to produce efficient yet long context-capable models. Despite these advancements, no existing work has comprehensively benchmarked these methods in a reasonably aligned environment. In this work, we fill this gap by providing a taxonomy of current methods and evaluating 10+ state-of-the-art approaches across seven categories of long context tasks. Our work reveals numerous previously unknown phenomena and offers insights -- as well as a friendly workbench -- for the future development of long context-capable LLMs. The source code will be available at https://github.com/henryzhongsc/longctx_bench

------------

`[2407.00132] ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents <https://arxiv.org/abs/2407.00132>`__ ShortcutsBench:基于api的代理的大规模真实基准测试

::

    Fri, 28 Jun 2024 08:45:02 GMT
    Haiyang Shen, Yue Li, Desong Meng, Dongqi Cai, Sheng Qi, Li Zhang, Mengwei Xu, Yun Ma

Recent advancements in integrating large language models (LLMs) with application programming interfaces (APIs) have gained significant interest in both academia and industry. These API-based agents, leveraging the strong autonomy and planning capabilities of LLMs, can efficiently solve problems requiring multi-step actions. However, their ability to handle multi-dimensional difficulty levels, diverse task types, and real-world demands through APIs remains unknown. In this paper, we introduce \textsc{ShortcutsBench}, a large-scale benchmark for the comprehensive evaluation of API-based agents in solving tasks with varying levels of difficulty, diverse task types, and real-world demands. \textsc{ShortcutsBench} includes a wealth of real APIs from Apple Inc.'s operating systems, refined user queries from shortcuts, human-annotated high-quality action sequences from shortcut developers, and accurate parameter filling values about primitive parameter types, enum parameter types, outputs from previous actions, and parameters that need to request necessary information from the system or user.
Our extensive evaluation of agents built with $5$ leading open-source (size >= 57B) and $4$ closed-source LLMs (e.g. Gemini-1.5-Pro and GPT-3.5) reveals significant limitations in handling complex queries related to API selection, parameter filling, and requesting necessary information from systems and users.
These findings highlight the challenges that API-based agents face in effectively fulfilling real and complex user queries. All datasets, code, and experimental results will be available at \url{https://github.com/eachsheep/shortcutsbench}.

------------

`[2407.00468] MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation <https://arxiv.org/abs/2407.00468>`__ MMEvalPro:面向可信和高效评估的多模态基准校准

::

    Sat, 29 Jun 2024 15:28:45 GMT
    Jinsheng Huang, Liang Chen, Taian Guo, Fu Zeng, Yusheng Zhao, Bohan Wu, Ye Yuan, Haozhe Zhao, Zhihui Guo, Yichi Zhang, Jingyang Yuan, Wei Ju, Luchen Liu, Tianyu Liu, Baobao Chang, Ming Zhang

Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding and reasoning abilities, often assessed through multiple-choice questions (MCQs) that include an image, a question, and several options. However, many benchmarks used for such evaluations suffer from systematic biases. Remarkably, Large Language Models (LLMs) without any visual perception capabilities achieve non-trivial performance, undermining the credibility of these evaluations. To address this issue while maintaining the efficiency of MCQ evaluations, we propose MMEvalPro, a benchmark designed to avoid Type-I errors through a trilogy evaluation pipeline and more rigorous metrics. For each original question from existing benchmarks, human annotators augment it by creating one perception question and one knowledge anchor question through a meticulous annotation process. MMEvalPro comprises $2,138$ question triplets, totaling $6,414$ distinct questions. Two-thirds of these questions are manually labeled by human experts, while the rest are sourced from existing benchmarks (MMMU, ScienceQA, and MathVista). Compared with the existing benchmarks, our experiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more challenging (the best LMM lags behind human performance by $31.73\%$, compared to an average gap of $8.03\%$ in previous benchmarks) and more trustworthy (the best LLM trails the best LMM by $23.09\%$, whereas the gap for previous benchmarks is just $14.64\%$). Our in-depth analysis explains the reason for the large performance gap and justifies the trustworthiness of evaluation, underscoring its significant potential for advancing future research.

------------

`[2407.00942] ProductAgent: Benchmarking Conversational Product Search Agent with Asking Clarification Questions <https://arxiv.org/abs/2407.00942>`__ ProductAgent:询问澄清性问题的对话性产品搜索代理基准测试

::

    Mon, 1 Jul 2024 03:50:23 GMT
    Jingheng Ye, Yong Jiang, Xiaobin Wang, Yinghui Li, Yangning Li, Hai-Tao Zheng, Pengjun Xie, Fei Huang

This paper introduces the task of product demand clarification within an e-commercial scenario, where the user commences the conversation with ambiguous queries and the task-oriented agent is designed to achieve more accurate and tailored product searching by asking clarification questions. To address this task, we propose ProductAgent, a conversational information seeking agent equipped with abilities of strategic clarification question generation and dynamic product retrieval. Specifically, we develop the agent with strategies for product feature summarization, query generation, and product retrieval.
Furthermore, we propose the benchmark called PROCLARE to evaluate the agent's performance both automatically and qualitatively with the aid of a LLM-driven user simulator. Experiments show that ProductAgent interacts positively with the user and enhances retrieval performance with increasing dialogue turns, where user demands become gradually more explicit and detailed. All the source codes will be released after the review anonymity period.

------------

`[2407.00981] VisEval: A Benchmark for Data Visualization in the Era of Large Language Models <https://arxiv.org/abs/2407.00981>`__ 

::

    Mon, 1 Jul 2024 05:35:30 GMT
    Nan Chen, Yuge Zhang, Jiahang Xu, Kan Ren, Yuqing Yang

Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains a challenging task that requires multiple low-level implementations, such as natural language processing and visualization design. Recent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from natural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs' capabilities in visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we introduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired with accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple dimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous checkers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our evaluation reveals prevalent challenges and delivers essential insights for future advancements.

------------

`[2407.01523] MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations <https://arxiv.org/abs/2407.01523>`__ MMLongBench-Doc:基于可视化的长上下文文档理解基准测试

::

    Mon, 1 Jul 2024 17:59:26 GMT
    Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, Aixin Sun

Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLongBench-Doc, a long-context, multi-modal benchmark comprising 1,062 expert-annotated questions. Distinct from previous datasets, it is constructed upon 130 lengthy PDF-formatted documents with an average of 49.4 pages and 20,971 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e. page number). Moreover, 33.2% of the questions are cross-page questions requiring evidence across multiple pages. 22.8% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores 31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs. Project Page: https://mayubo2333.github.io/MMLongBench-Doc

------------

`[2405.13144] Mamo: a Mathematical Modeling Benchmark with Solvers <https://arxiv.org/abs/2405.13144>`__ Mamo:带求解器的数学建模基准

::

    replaced with revised version Sun, 30 Jun 2024 05:42:24 GMT
    Submission history From: Xuhan Huang [view email]
    [v1] Tue, 21 May 2024 18:29:54 UTC (1,463 KB)
    [v2] Sun, 30 Jun 2024 05:42:24 UTC (1,756 KB)
    Xuhan Huang, Qingning Shen, Yan Hu, Anningzhe Gao and Benyou Wang

Mathematical modeling involves representing real-world phenomena, systems, or problems using mathematical expressions and equations to analyze, understand, and predict their behavior. Given that this process typically requires experienced experts, there is an interest in exploring whether Large Language Models (LLMs) can undertake mathematical modeling to potentially decrease human labor. To evaluate of LLMs in mathematical modeling, we introduce a new benchmark, Mamo, that transcends traditional result-oriented assessments. Unlike conventional methods that primarily assess LLMs based on the accuracy of solutions to mathematical problems, our approach offers deeper insight into the modeling process itself. By focusing on the processes LLMs undertake rather than the correctness of their final solutions, Mamo pioneers a novel evaluation paradigm. This shift underscores the importance of understanding the inherent modeling capabilities of LLMs, paving the way for a more nuanced and comprehensive analysis of their problem-solving strategies. Our work marks a significant advancement in the field, suggesting a new direction for future research by emphasizing the evaluation of LLMs' modeling processes over the mere correctness of answers. This benchmark not only facilitates a better understanding of LLMs' mathematical modeling capabilities but also sets a new standard for evaluating their performance in complex problem-solving scenarios.

------------

`[2306.09296] KoLA: Carefully Benchmarking World Knowledge of Large Language Models <https://arxiv.org/abs/2306.09296>`__ KoLA:仔细对大型语言模型的世界知识进行基准测试

::

    replaced with revised version Mon, 1 Jul 2024 03:38:57 GMT
    Submission history From: Shangqing Tu [view email]
    [v1] Thu, 15 Jun 2023 17:20:46 UTC (3,811 KB)
    [v2] Thu, 6 Jul 2023 17:25:10 UTC (3,810 KB)
    [v3] Mon, 1 Jul 2024 03:38:57 UTC (4,591 KB)
    Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Chunyang Li, Zheyuan Zhang, Yushi Bai, Yantao Liu, Amy Xin, Nianyi Lin, Kaifeng Yun, Linlu Gong, Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai Li, Yong Guan, Kaisheng Zeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu, Yuan Yao, Ning Ding, Lei Hou, Zhiyuan Liu, Bin Xu, Jie Tang, Juanzi Li

The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For \textbf{ability modeling}, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For \textbf{data}, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For \textbf{evaluation criteria}, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate $28$ open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset and open-participation leaderboard are publicly released at this https URL and will be continuously updated to provide references for developing LLMs and knowledge-related systems.

------------

`[2406.03855] Performance of large language models in numerical vs. semantic medical knowledge: Benchmarking on evidence-based Q&As <https://arxiv.org/abs/2406.03855>`__ 大型语言模型在数字与语义医学知识中的表现:基于证据的问答基准测试

::

    replaced with revised version Mon, 1 Jul 2024 11:36:02 GMT
    Submission history From: Eden Avnat [view email]
    [v1] Thu, 6 Jun 2024 08:41:46 UTC (1,365 KB)
    [v2] Mon, 1 Jul 2024 11:36:02 UTC (1,872 KB)
    Eden Avnat, Michal Levy, Daniel Herstain, Elia Yanko, Daniel Ben Joya, Michal Tzuchman Katz, Dafna Eshel, Sahar Laros, Yael Dagan, Shahar Barami, Joseph Mermelstein, Shahar Ovadia, Noam Shomron, Varda Shalev and Raja-Elie E. Abdulnour

Clinical problem-solving requires processing of semantic medical knowledge such as illness scripts and numerical medical knowledge of diagnostic tests for evidence-based decision-making. As large language models (LLMs) show promising results in many aspects of language-based clinical practice, their ability to generate non-language evidence-based answers to clinical questions is inherently limited by tokenization. Therefore, we evaluated LLMs' performance on two question types: numeric (correlating findings) and semantic (differentiating entities) while examining differences within and between LLMs in medical aspects and comparing their performance to humans. To generate straightforward multi-choice questions and answers (QAs) based on evidence-based medicine (EBM), we used a comprehensive medical knowledge graph (encompassed data from more than 50,00 peer-reviewed articles) and created the "EBMQA". EBMQA contains 105,000 QAs labeled with medical and non-medical topics and classified into numerical or semantic questions. We benchmarked this dataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and Claude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question types and according to sub-labeled topics. For validation, six medical experts were tested on 100 numerical EBMQA questions. We found that both LLMs excelled more in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical QAs. However, both LLMs showed inter and intra gaps in different medical aspects and remained inferior to humans. Thus, their medical advice should be addressed carefully.

------------

`[2406.10621] StrucText-Eval: An Autogenerated Benchmark for Evaluating Large Language Model's Ability in Structure-Rich Text Understanding <https://arxiv.org/abs/2406.10621>`__ StrucText-Eval:评估大型语言模型对结构丰富文本理解能力的自动生成基准

::

    replaced with revised version Sun, 30 Jun 2024 09:02:34 GMT
    Submission history From: Zhouhong Gu [view email]
    [v1] Sat, 15 Jun 2024 12:48:00 UTC (7,683 KB)
    [v2] Sun, 30 Jun 2024 09:02:34 UTC (3,826 KB)
    Zhouhong Gu, Haoning Ye, Zeyang Zhou, Hongwei Feng, Yanghua Xiao

Given the substantial volumes of structured data held by many companies, enabling Large Language Models (LLMs) to directly understand structured text in non-structured forms could significantly enhance their capabilities across various business scenarios. To this end, we propose evaluation data generation method for assessing LLM's ability in understanding the structure-rich text, which generates structured data of controllable complexity based on manually crafted question templates and generation rules. Building on this generation method, we introduce StrucText-Eval, a benchmark comprising 6,032 questions across 8 different structured languages and 29 specific tasks. Furthermore, considering human proficiency in rule-based tasks, we also present StrucText-Eval-Hard, which includes 3,016 questions designed to further examine the gap between LLMs and human performance. Results indicate that the best-performing LLM currently achieve an accuracy of 65.0\% on StrucText-Eval-Hard, while human accuracy reaches up to 95.7\%. Moreover, while fine-tuning using StrucText-Eval can enhance existing LLMs' understanding of all structured languages, it does not necessarily improve performance across all task types. The benchmark and generation codes are open sourced in this https URL

------------

`[2406.13261] BeHonest: Benchmarking Honesty of Large Language Models <https://arxiv.org/abs/2406.13261>`__ BeHonest:大型语言模型的诚实度基准测试

::

    replaced with revised version Mon, 1 Jul 2024 15:18:07 GMT
    Submission history From: Steffi Chern [view email]
    [v1] Wed, 19 Jun 2024 06:46:59 UTC (1,285 KB)
    [v2] Mon, 1 Jul 2024 15:18:07 UTC (1,286 KB)
    [v3] Mon, 8 Jul 2024 18:29:58 UTC (1,286 KB)
    Steffi Chern, Zhulin Hu, Yuqing Yang, Ethan Chern, Yuan Guo, Jiahe Jin, Binjie Wang, Pengfei Liu

Previous works on Large Language Models (LLMs) have mainly focused on evaluating their helpfulness or harmlessness. However, honesty, another crucial alignment criterion, has received relatively less attention. Dishonest behaviors in LLMs, such as spreading misinformation and defrauding users, present severe risks that intensify as these models approach superintelligent levels. Enhancing honesty in LLMs addresses critical limitations and helps uncover latent capabilities that are not readily expressed. This underscores the urgent need for reliable methods and benchmarks to effectively ensure and evaluate the honesty of LLMs.
In this paper, we introduce BeHonest, a pioneering benchmark specifically designed to assess honesty in LLMs comprehensively. BeHonest evaluates three essential aspects of honesty: awareness of knowledge boundaries, avoidance of deceit, and consistency in responses. Building on this foundation, we designed 10 scenarios to evaluate and analyze 9 popular LLMs on the market, including both closed-source and open-source models from different model families with varied model sizes. Our findings indicate that there is still significant room for improvement in the honesty of LLMs. We encourage the AI community to prioritize honesty alignment in these models, which can harness their full potential to benefit society while preventing them from causing harm through deception or inconsistency. Our benchmark and code can be found at: \url{this https URL}.

------------

---------------
Accelerate (12)
---------------

`[2407.01527] KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches <https://arxiv.org/abs/2407.01527>`__ KV缓存压缩，但我们必须提供什么回报?长上下文能力方法的综合基准

::

    Mon, 1 Jul 2024 17:59:47 GMT
    Jiayi Yuan, Hongyi Liu, Shaochen (Henry) Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, Xia Hu

Long context capability is a crucial competency for large language models (LLMs) as it mitigates the human struggle to digest long-form texts. This capability enables complex task-solving scenarios such as book summarization, code assistance, and many more tasks that are traditionally manpower-intensive.
However, transformer-based LLMs face significant challenges with long context input due to the growing size of the KV cache and the intrinsic complexity of attending to extended inputs; where multiple schools of efficiency-driven approaches -- such as KV cache quantization, token dropping, prompt compression, linear-time sequence models, and hybrid architectures -- have been proposed to produce efficient yet long context-capable models. Despite these advancements, no existing work has comprehensively benchmarked these methods in a reasonably aligned environment. In this work, we fill this gap by providing a taxonomy of current methods and evaluating 10+ state-of-the-art approaches across seven categories of long context tasks. Our work reveals numerous previously unknown phenomena and offers insights -- as well as a friendly workbench -- for the future development of long context-capable LLMs. The source code will be available at https://github.com/henryzhongsc/longctx_bench

------------

`[2407.00118] From Efficient Multimodal Models to World Models: A Survey <https://arxiv.org/abs/2407.00118>`__ 

::

    Thu, 27 Jun 2024 15:36:43 GMT
    Xinji Mai, Zeng Tao, Junxiong Lin, Haoran Wang, Yang Chang, Yanlan Kang, Yan Wang, Wenqiang Zhang

Multimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.

------------

`[2407.00945] Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models: Enhancing Performance and Reducing Inference Costs <https://arxiv.org/abs/2407.00945>`__ 稀疏混合专家语言模型的高效专家剪枝:提高性能和降低推理成本

::

    Mon, 1 Jul 2024 03:57:35 GMT
    Enshu Liu, Junyi Zhu, Zinan Lin, Xuefei Ning, Matthew B. Blaschko, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang

The rapid advancement of large language models (LLMs) has led to architectures with billions to trillions of parameters, posing significant deployment challenges due to their substantial demands on memory, processing power, and energy consumption. Sparse Mixture-of-Experts (SMoE) architectures have emerged as a solution, activating only a subset of parameters per token, thereby achieving faster inference while maintaining performance. However, SMoE models still face limitations in broader deployment due to their large parameter counts and significant GPU memory requirements. In this work, we introduce a gradient-free evolutionary strategy named EEP (Efficient Expert P}runing) to enhance the pruning of experts in SMoE models. EEP relies solely on model inference (i.e., no gradient computation) and achieves greater sparsity while maintaining or even improving performance on downstream tasks.
EEP can be used to reduce both the total number of experts (thus saving GPU memory) and the number of active experts (thus accelerating inference). For example, we demonstrate that pruning up to 75% of experts in Mixtral $8\times7$B-Instruct results in a substantial reduction in parameters with minimal performance loss. Remarkably, we observe improved performance on certain tasks, such as a significant increase in accuracy on the SQuAD dataset (from 53.4% to 75.4%), when pruning half of the experts. With these results, EEP not only lowers the barrier to deploying SMoE models,but also challenges the conventional understanding of model pruning by showing that fewer experts can lead to better task-specific performance without any fine-tuning. Code is available at https://github.com/imagination-research/EEP.

------------

`[2407.00952] SplitLoRA: A Split Parameter-Efficient Fine-Tuning Framework for Large Language Models <https://arxiv.org/abs/2407.00952>`__ 

::

    Mon, 1 Jul 2024 04:13:25 GMT
    Zheng Lin, Xuanjie Hu, Yuxin Zhang, Zhe Chen, Zihan Fang, Xianhao Chen, Ang Li, Praneeth Vepakomma, Yue Gao

The scalability of large language models (LLMs) in handling high-complexity models and large-scale datasets has led to tremendous successes in pivotal domains. While there is an urgent need to acquire more training data for LLMs, a concerning reality is the depletion of high-quality public datasets within a few years. In view of this, the federated learning (FL) LLM fine-tuning paradigm recently has been proposed to facilitate collaborative LLM fine-tuning on distributed private data, where multiple data owners collaboratively fine-tune a shared LLM without sharing raw data. However, the staggering model size of LLMs imposes heavy computing and communication burdens on clients, posing significant barriers to the democratization of the FL LLM fine-tuning paradigm. To address this issue, split learning (SL) has emerged as a promising solution by offloading the primary training workload to a server via model partitioning while exchanging activation/activation's gradients with smaller data sizes rather than the entire LLM. Unfortunately, research on the SL LLM fine-tuning paradigm is still in its nascent stage. To fill this gap, in this paper, we propose the first SL LLM fine-tuning framework, named SplitLoRA.
SplitLoRA is built on the split federated learning (SFL) framework, amalgamating the advantages of parallel training from FL and model splitting from SL and thus greatly enhancing the training efficiency. It is worth noting that SplitLoRA is the inaugural open-source benchmark for SL LLM fine-tuning, providing a foundation for research efforts dedicated to advancing SL LLM fine-tuning. Extensive simulations validate that SplitLoRA achieves target accuracy in significantly less time than state-of-the-art LLM fine-tuning frameworks, demonstrating the superior training performance of SplitLoRA. The project page is available at https://fduinc.github.io/splitlora/.

------------

`[2407.00468] MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation <https://arxiv.org/abs/2407.00468>`__ MMEvalPro:面向可信和高效评估的多模态基准校准

::

    Sat, 29 Jun 2024 15:28:45 GMT
    Jinsheng Huang, Liang Chen, Taian Guo, Fu Zeng, Yusheng Zhao, Bohan Wu, Ye Yuan, Haozhe Zhao, Zhihui Guo, Yichi Zhang, Jingyang Yuan, Wei Ju, Luchen Liu, Tianyu Liu, Baobao Chang, Ming Zhang

Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding and reasoning abilities, often assessed through multiple-choice questions (MCQs) that include an image, a question, and several options. However, many benchmarks used for such evaluations suffer from systematic biases. Remarkably, Large Language Models (LLMs) without any visual perception capabilities achieve non-trivial performance, undermining the credibility of these evaluations. To address this issue while maintaining the efficiency of MCQ evaluations, we propose MMEvalPro, a benchmark designed to avoid Type-I errors through a trilogy evaluation pipeline and more rigorous metrics. For each original question from existing benchmarks, human annotators augment it by creating one perception question and one knowledge anchor question through a meticulous annotation process. MMEvalPro comprises $2,138$ question triplets, totaling $6,414$ distinct questions. Two-thirds of these questions are manually labeled by human experts, while the rest are sourced from existing benchmarks (MMMU, ScienceQA, and MathVista). Compared with the existing benchmarks, our experiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more challenging (the best LMM lags behind human performance by $31.73\%$, compared to an average gap of $8.03\%$ in previous benchmarks) and more trustworthy (the best LLM trails the best LMM by $23.09\%$, whereas the gap for previous benchmarks is just $14.64\%$). Our in-depth analysis explains the reason for the large performance gap and justifies the trustworthiness of evaluation, underscoring its significant potential for advancing future research.

------------

`[2407.00023] Preble: Efficient Distributed Prompt Scheduling for LLM Serving <https://arxiv.org/abs/2407.00023>`__ Preble:高效的LLM服务分布式提示调度

::

    Wed, 8 May 2024 06:30:58 GMT
    Vikranth Srivatsa, Zijian He, Reyna Abhyankar, Dongming Li, Yiying Zhang

Prompts to large language models (LLMs) have evolved beyond simple user questions. For LLMs to solve complex problems, today's practices include domain-specific instructions, illustration of tool usages, and long context, such as textbook chapters in prompts. As such, many parts of prompts are repetitive across requests, and their attention computation results can be reused. However, today's LLM serving systems treat every request in isolation, missing the opportunity of computation reuse.
This paper proposes Preble, the first distributed LLM serving platform that targets and optimizes for prompt sharing. We perform a study on five popular LLM workloads. Based on our study results, we designed a distributed scheduling system that co-optimizes computation reuse and load balancing. Our evaluation of Preble on two to 8 GPUs with real workloads and request arrival patterns on two open-source LLM models shows that Preble outperforms the state-of-the-art average latency by 1.5X to 14.5X and p99 by 2X to 10X.

------------

`[2403.19708] Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention <https://arxiv.org/abs/2403.19708>`__ 

::

    replaced with revised version Sun, 30 Jun 2024 23:50:38 GMT
    Submission history From: Bin Gao [view email]
    [v1] Sat, 23 Mar 2024 10:42:49 UTC (635 KB)
    [v2] Tue, 16 Apr 2024 21:32:29 UTC (659 KB)
    [v3] Sun, 30 Jun 2024 23:50:38 UTC (807 KB)
    Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, Pengfei Zuo

Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes CachedAttention, a new attention mechanism that enables reuse of KV caches across multi-turn conversations, significantly reducing the repetitive computation overheads. CachedAttention maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, CachedAttention employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are placed in the fastest hierarchy, CachedAttention employs scheduler-aware fetching and eviction schemes to consciously place the KV caches in different layers based on the hints from the inference job scheduler. To avoid the invalidation of the saved KV caches incurred by context window overflow, CachedAttention enables the saved KV caches to remain valid via decoupling the positional encoding and effectively truncating the KV caches. Extensive experimental results demonstrate that CachedAttention significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by up to 7.8$\times$ for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%.

------------

`[2406.10882] SCAR: Efficient Instruction-Tuning for Large Language Models via Style Consistency-Aware Response Ranking <https://arxiv.org/abs/2406.10882>`__ SCAR:基于风格一致性感知响应排序的大型语言模型高效指令调优

::

    replaced with revised version Mon, 1 Jul 2024 14:55:01 GMT
    Submission history From: Zhuang Li [view email]
    [v1] Sun, 16 Jun 2024 10:10:37 UTC (1,104 KB)
    [v2] Mon, 1 Jul 2024 14:55:01 UTC (1,104 KB)
    [v3] Sat, 6 Jul 2024 09:29:54 UTC (1,104 KB)
    [v4] Wed, 10 Jul 2024 08:22:10 UTC (1,104 KB)
    Zhuang Li, Yuncheng Hua, Thuy-Trang Vu, Haolan Zhan, Lizhen Qu, Gholamreza Haffari

Recent studies have shown that maintaining a consistent response style by human experts and enhancing data quality in training sets can significantly improve the performance of fine-tuned Large Language Models (LLMs) while reducing the number of training examples needed. However, the precise definition of style and the relationship between style, data quality, and LLM performance remains unclear. This research decomposes response style into presentation and composition styles and finds that, among training data of similar quality, those with higher style consistency lead to better LLM performance. Inspired by this, we introduce Style Consistency-Aware Response Ranking (SCAR), which automatically prioritizes instruction-response pairs in the training set based on their response stylistic consistency. By selecting the most style-consistent examples, ranging from the top 25% to 0.7% of the full dataset, the fine-tuned LLMs can match or even surpass the performance of models trained on the entire dataset in coding and open-ended question-answering benchmarks. Code and data are available at this https URL .

------------

`[2311.16442] Fast and Efficient 2-bit LLM Inference on GPU: 2/4/16-bit in a Weight Matrix with Asynchronous Dequantization <https://arxiv.org/abs/2311.16442>`__ GPU上快速有效的2位LLM推理:2/4/16位异步去量化权重矩阵

::

    replaced with revised version Mon, 1 Jul 2024 11:13:54 GMT
    Submission history From: Jinhao Li [view email]
    [v1] Tue, 28 Nov 2023 02:44:59 UTC (1,281 KB)
    [v2] Wed, 13 Dec 2023 13:50:45 UTC (332 KB)
    [v3] Mon, 1 Jul 2024 11:13:54 UTC (1,946 KB)
    Jinhao Li, Jiaming Xu, Shiyao Li, Shan Huang, Jun Liu, Yaoxiu Lian, Guohao Dai

Large language models (LLMs) have demonstrated impressive abilities in various domains while the inference cost is expensive. Many previous studies exploit quantization methods to reduce LLM inference cost by reducing latency and memory consumption. Applying 2-bit single-precision weight quantization brings >3% accuracy loss, so the state-of-the-art methods use mixed-precision methods for LLMs (e.g. Llama2-7b, etc.) to improve the accuracy. However, challenges still exist: (1) Uneven distribution in weight matrix. (2) Large speed degradation by adding sparse outliers. (3) Time-consuming dequantization operations on GPUs. To tackle these challenges and enable fast and efficient LLM inference on GPUs, we propose the following techniques in this paper. (1) Intra-weight mixed-precision quantization. (2) Exclusive 2-bit sparse outlier with minimum speed degradation. (3) Asynchronous dequantization. We conduct extensive experiments on different model families (e.g. Llama3, etc.) and model sizes. We achieve 2.91-bit for each weight considering all scales/zeros for different models with negligible loss. As a result, with our 2/4/16 mixed-precision quantization for each weight matrix and asynchronous dequantization during inference, our design achieves an end-to-end speedup for Llama2-7b is 1.74x over the original model, and we reduce both runtime cost and total cost by up to 2.53x and 2.29x with less GPU requirements.

------------

`[2406.18853] Decoding-Time Language Model Alignment with Multiple Objectives <https://arxiv.org/abs/2406.18853>`__ 基于多目标的解码时语言模型对齐

::

    replaced with revised version Sat, 29 Jun 2024 02:29:38 GMT
    Submission history From: Yifang Chen [view email]
    [v1] Thu, 27 Jun 2024 02:46:30 UTC (1,151 KB)
    [v2] Sat, 29 Jun 2024 02:29:38 UTC (1,151 KB)
    Ruizhe Shi, Yifang Chen, Yushi Hu, Alisa Liu, Hannaneh Hajishirzi, Noah A. Smith, Simon Du

Aligning language models (LMs) to human preferences has emerged as a critical pursuit, enabling these models to better serve diverse user needs. Existing methods primarily focus on optimizing LMs for a single reward function, limiting their adaptability to varied objectives. Here, we propose $\textbf{multi-objective decoding (MOD)}$, a decoding-time algorithm that outputs the next token from a linear combination of predictions of all base models, for any given weightings over different objectives. We exploit a common form among a family of $f$-divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy. Theoretically, we show why existing approaches can be sub-optimal even in natural settings and obtain optimality guarantees for our method. Empirical results demonstrate the effectiveness of the algorithm. For example, compared to a parameter-merging baseline, MOD achieves 12.8% overall reward improvement when equally optimizing towards $3$ objectives. Moreover, we experiment with MOD on combining three fully-finetuned LLMs of different model sizes, each aimed at different objectives such as safety, coding, and general user preference. Unlike traditional methods that require careful curation of a mixture of datasets to achieve comprehensive improvement, we can quickly experiment with preference weightings using MOD to find the best combination of models. Our best combination reduces toxicity on Toxigen to nearly 0% and achieves 7.9--33.3% improvement across other three metrics ($\textit{i.e.}$, Codex@1, GSM-COT, BBH-COT).

------------

`[2403.03536] Towards Efficient and Effective Unlearning of Large Language Models for Recommendation <https://arxiv.org/abs/2403.03536>`__ 面向推荐的大型语言模型高效高效遗忘研究

::

    replaced with revised version Sun, 30 Jun 2024 04:00:06 GMT
    Submission history From: Hangyu Wang [view email]
    [v1] Wed, 6 Mar 2024 08:31:35 UTC (74 KB)
    [v2] Sun, 30 Jun 2024 04:00:06 UTC (76 KB)
    Hangyu Wang, Jianghao Lin, Bo Chen, Yang Yang, Ruiming Tang, Weinan Zhang, Yong Yu

The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning. In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \textbf{E2URec}, the first \underline{E}fficient and \underline{E}ffective \underline{U}nlearning method for LLM\underline{Rec}. Our proposed E2URec enhances the unlearning efficiency by updating only a few additional LoRA parameters, and improves the unlearning effectiveness by employing a teacher-student framework, where we maintain multiple teacher networks to guide the unlearning process. Extensive experiments show that E2URec outperforms state-of-the-art baselines on two real-world datasets. Specifically, E2URec can efficiently forget specific data without affecting recommendation performance. The source code is at \url{this https URL}.

------------

`[2406.07588] AIM: Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning <https://arxiv.org/abs/2406.07588>`__ 目的:让任何多模态大型语言模型都能实现高效的上下文学习

::

    replaced with revised version Sun, 30 Jun 2024 18:19:25 GMT
    Submission history From: Jun Gao [view email]
    [v1] Tue, 11 Jun 2024 08:12:43 UTC (10,755 KB)
    [v2] Sun, 30 Jun 2024 18:19:25 UTC (4,537 KB)
    Jun Gao, Qian Qiao, Ziqiang Cao, Zili Wang, Wenjie Li

In-context learning (ICL) facilitates Large Language Models (LLMs) exhibiting emergent ability on downstream tasks without updating billions of parameters. However, in the area of multi-modal Large Language Models (MLLMs), two problems hinder the application of multi-modal ICL: (1) Most primary MLLMs are only trained on single-image datasets, making them unable to read multi-modal demonstrations. (2) With the demonstrations increasing, thousands of visual tokens highly challenge hardware and degrade ICL performance. During preliminary explorations, we discovered that the inner LLM tends to focus more on the linguistic modality within multi-modal demonstrations to generate responses. Therefore, we propose a general and light-weighted framework \textbf{AIM} to tackle the mentioned problems through \textbf{A}ggregating \textbf{I}mage information of \textbf{M}ultimodal demonstrations to the dense latent space of the corresponding linguistic part. Specifically, AIM first uses the frozen backbone MLLM to read each image-text demonstration and extracts the vector representations on top of the text. These vectors naturally fuse the information of the image-text pair, and AIM transforms them into fused virtual tokens acceptable for the inner LLM via a trainable projection layer. Ultimately, these fused tokens function as variants of multi-modal demonstrations, fed into the MLLM to direct its response to the current query as usual. Because these fused tokens stem from the textual component of the image-text pair, a multi-modal demonstration is nearly reduced to a pure textual demonstration, thus seamlessly applying to any MLLMs. With its de facto MLLM frozen, AIM is parameter-efficient and we train it on public multi-modal web corpora which have nothing to do with downstream test tasks.

------------

-----------------------
In-Context Learning (4)
-----------------------

`[2407.00902] From Introspection to Best Practices: Principled Analysis of Demonstrations in Multimodal In-Context Learning <https://arxiv.org/abs/2407.00902>`__ 

::

    Mon, 1 Jul 2024 01:57:21 GMT
    Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen

Motivated by in-context learning (ICL) capabilities of Large Language models (LLMs), multimodal LLMs with additional visual modality are also exhibited with similar ICL abilities when multiple image-text pairs are provided as demonstrations. However, relatively less work has been done to investigate the principles behind how and why multimodal ICL works. We conduct a systematic and principled evaluation of multimodal ICL for models of different scales on a broad spectrum of new yet critical tasks. Through perturbations over different modality information, we show that modalities matter differently across tasks in multimodal ICL. Considering such modality impact, we further utilize modality-driven demonstration strategies to boost ICL performance. We also identify that demonstration selection is closely related to the models' ability to capture task inductive biases from multimodal ICL. Our principled analysis provides a comprehensive way of understanding the role of demonstrations in multimodal in-context learning, and sheds light on effectively improving multimodal ICL on a wide range of tasks even if those tasks are not seen in or even contradict pretraining data.

------------

`[2406.11629] Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better! <https://arxiv.org/abs/2406.11629>`__ 多镜头语境学习能帮助长语境LLM评判吗?看得多，判断得更好!

::

    replaced with revised version Sun, 30 Jun 2024 13:31:24 GMT
    Submission history From: Mingyang Song [view email]
    [v1] Mon, 17 Jun 2024 15:11:58 UTC (221 KB)
    [v2] Mon, 24 Jun 2024 16:02:21 UTC (235 KB)
    [v3] Sun, 30 Jun 2024 13:31:24 UTC (221 KB)
    Mingyang Song, Mao Zheng, Xuan Luo

Leveraging Large Language Models (LLMs) as judges for judging the performance of LLMs has recently garnered attention. However, this type of approach is affected by the potential biases in LLMs, raising concerns about the reliability of the evaluation results. To mitigate this issue, we propose and study two versions of many-shot in-context prompts, which rely on two existing settings of many-shot ICL for helping GPT-4o-as-a-Judge in single answer grading to mitigate the potential biases in LLMs, Reinforced ICL and Unsupervised ICL. Concretely, the former utilizes in-context examples with model-generated rationales, and the latter without. Based on the designed prompts, we investigate the impact of scaling the number of in-context examples on the consistency and quality of the judgment results. Furthermore, we reveal the symbol bias hidden in the pairwise comparison of GPT-4o-as-a-Judge and propose a simple yet effective approach to mitigate it. Experimental results show that advanced long-context LLMs, such as GPT-4o, perform better in the many-shot regime than in the zero-shot regime. Meanwhile, the experimental results further verify the effectiveness of the symbol bias mitigation approach.

------------

`[2406.17534] Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification <https://arxiv.org/abs/2406.17534>`__ 基于检索式上下文学习的小样本层次文本分类

::

    replaced with revised version Sat, 29 Jun 2024 09:24:33 GMT
    Submission history From: Yu Zhao [view email]
    [v1] Tue, 25 Jun 2024 13:19:41 UTC (2,383 KB)
    [v2] Sat, 29 Jun 2024 09:24:33 UTC (2,383 KB)
    Huiyao Chen, Yu Zhao, Zulong Chen, Mengjia Wang, Liangyue Li, Meishan Zhang, Min Zhang

Hierarchical text classification (HTC) is an important task with broad applications, while few-shot HTC has gained increasing interest recently. While in-context learning (ICL) with large language models (LLMs) has achieved significant success in few-shot learning, it is not as effective for HTC because of the expansive hierarchical label sets and extremely-ambiguous labels. In this work, we introduce the first ICL-based framework with LLM for few-shot HTC. We exploit a retrieval database to identify relevant demonstrations, and an iterative policy to manage multi-layer hierarchical labels. Particularly, we equip the retrieval database with HTC label-aware representations for the input texts, which is achieved by continual training on a pretrained language model with masked language modeling (MLM), layer-wise classification (CLS, specifically for HTC), and a novel divergent contrastive learning (DCL, mainly for adjacent semantically-similar labels) objective. Experimental results on three benchmark datasets demonstrate superior performance of our method, and we can achieve state-of-the-art results in few-shot HTC.

------------

`[2406.07588] AIM: Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning <https://arxiv.org/abs/2406.07588>`__ 目的:让任何多模态大型语言模型都能实现高效的上下文学习

::

    replaced with revised version Sun, 30 Jun 2024 18:19:25 GMT
    Submission history From: Jun Gao [view email]
    [v1] Tue, 11 Jun 2024 08:12:43 UTC (10,755 KB)
    [v2] Sun, 30 Jun 2024 18:19:25 UTC (4,537 KB)
    Jun Gao, Qian Qiao, Ziqiang Cao, Zili Wang, Wenjie Li

In-context learning (ICL) facilitates Large Language Models (LLMs) exhibiting emergent ability on downstream tasks without updating billions of parameters. However, in the area of multi-modal Large Language Models (MLLMs), two problems hinder the application of multi-modal ICL: (1) Most primary MLLMs are only trained on single-image datasets, making them unable to read multi-modal demonstrations. (2) With the demonstrations increasing, thousands of visual tokens highly challenge hardware and degrade ICL performance. During preliminary explorations, we discovered that the inner LLM tends to focus more on the linguistic modality within multi-modal demonstrations to generate responses. Therefore, we propose a general and light-weighted framework \textbf{AIM} to tackle the mentioned problems through \textbf{A}ggregating \textbf{I}mage information of \textbf{M}ultimodal demonstrations to the dense latent space of the corresponding linguistic part. Specifically, AIM first uses the frozen backbone MLLM to read each image-text demonstration and extracts the vector representations on top of the text. These vectors naturally fuse the information of the image-text pair, and AIM transforms them into fused virtual tokens acceptable for the inner LLM via a trainable projection layer. Ultimately, these fused tokens function as variants of multi-modal demonstrations, fed into the MLLM to direct its response to the current query as usual. Because these fused tokens stem from the textual component of the image-text pair, a multi-modal demonstration is nearly reduced to a pure textual demonstration, thus seamlessly applying to any MLLMs. With its de facto MLLM frozen, AIM is parameter-efficient and we train it on public multi-modal web corpora which have nothing to do with downstream test tasks.

------------

--------------
Reasoning (15)
--------------

`[2407.00071] Combinatorial Reasoning: Selecting Reasons in Generative AI Pipelines via Combinatorial Optimization <https://arxiv.org/abs/2407.00071>`__ 组合推理:通过组合优化在生成式AI管道中选择原因

::

    Wed, 19 Jun 2024 16:47:44 GMT
    Mert Esencan, Tarun Advaith Kumar, Ata Akbari Asanjan, P. Aaron Lott, Masoud Mohseni, Can Unlu, Davide Venturelli, Alan Ho

Recent Large Language Models (LLMs) have demonstrated impressive capabilities at tasks that require human intelligence and are a significant step towards human-like artificial intelligence (AI). Yet the performance of LLMs at reasoning tasks have been subpar and the reasoning capability of LLMs is a matter of significant debate. While it has been shown that the choice of the prompting technique to the LLM can alter its performance on a multitude of tasks, including reasoning, the best performing techniques require human-made prompts with the knowledge of the tasks at hand. We introduce a framework for what we call Combinatorial Reasoning (CR), a fully-automated prompting method, where reasons are sampled from an LLM pipeline and mapped into a Quadratic Unconstrained Binary Optimization (QUBO) problem. The framework investigates whether QUBO solutions can be profitably used to select a useful subset of the reasons to construct a Chain-of-Thought style prompt. We explore the acceleration of CR with specialized solvers. We also investigate the performance of simpler zero-shot strategies such as linear majority rule or random selection of reasons. Our preliminary study indicates that coupling a combinatorial solver to generative AI pipelines is an interesting avenue for AI reasoning and elucidates design principles for future CR methods.

------------

`[2407.00087] ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback <https://arxiv.org/abs/2407.00087>`__ ARES:交替强化学习和监督微调，通过不同的AI反馈增强多模态思维链推理

::

    Tue, 25 Jun 2024 07:20:11 GMT
    Ju-Seung Byun, Jiyun Chun, Jihyung Kil, Andrew Perrault

Large Multimodal Models (LMMs) excel at comprehending human instructions and demonstrate remarkable results across a broad spectrum of tasks. Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs by aligning them with specific preferences. These methods primarily use ranking-based feedback for entire generations. With advanced AI models (Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of detailed feedback that are expensive for humans to provide. We propose a two-stage algorithm ARES that Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT). First, we request the Teacher to score how much each sentence contributes to solving the problem in a Chain-of-Thought (CoT).
This sentence-level feedback allows us to consider individual valuable segments, providing more granular rewards for the RL procedure. Second, we ask the Teacher to correct the wrong reasoning after the RL stage. The RL procedure requires massive efforts for hyperparameter tuning and often generates errors like repetitive words and incomplete sentences. With the correction feedback, we stabilize the RL fine-tuned model through SFT. We conduct experiments on multi-model dataset ScienceQA and A-OKVQA to demonstrate the effectiveness of our proposal. ARES rationale reasoning achieves around 70% win rate against baseline models judged by GPT-4o. Additionally, we observe that the improved rationale reasoning leads to a 2.5% increase in inference answer accuracy on average for the multi-modal datasets.

------------

`[2407.00092] Visual Reasoning and Multi-Agent Approach in Multimodal Large Language Models (MLLMs): Solving TSP and mTSP Combinatorial Challenges <https://arxiv.org/abs/2407.00092>`__ 多模态大型语言模型中的视觉推理和多智能体方法:解决TSP和mTSP组合挑战

::

    Wed, 26 Jun 2024 07:12:06 GMT
    Mohammed Elhenawy, Ahmad Abutahoun, Taqwa I.Alhadidi, Ahmed Jaber, Huthaifa I. Ashqar, Shadi Jaradat, Ahmed Abdelhay, Sebastien Glaser, and Andry Rakotonirainy

Multimodal Large Language Models (MLLMs) harness comprehensive knowledge spanning text, images, and audio to adeptly tackle complex problems, including zero-shot in-context learning scenarios. This study explores the ability of MLLMs in visually solving the Traveling Salesman Problem (TSP) and Multiple Traveling Salesman Problem (mTSP) using images that portray point distributions on a two-dimensional plane. We introduce a novel approach employing multiple specialized agents within the MLLM framework, each dedicated to optimizing solutions for these combinatorial challenges. Our experimental investigation includes rigorous evaluations across zero-shot settings and introduces innovative multi-agent zero-shot in-context scenarios. The results demonstrated that both multi-agent models. Multi-Agent 1, which includes the Initializer, Critic, and Scorer agents, and Multi-Agent 2, which comprises only the Initializer and Critic agents; significantly improved solution quality for TSP and mTSP problems. Multi-Agent 1 excelled in environments requiring detailed route refinement and evaluation, providing a robust framework for sophisticated optimizations. In contrast, Multi-Agent 2, focusing on iterative refinements by the Initializer and Critic, proved effective for rapid decision-making scenarios. These experiments yield promising outcomes, showcasing the robust visual reasoning capabilities of MLLMs in addressing diverse combinatorial problems. The findings underscore the potential of MLLMs as powerful tools in computational optimization, offering insights that could inspire further advancements in this promising field. Project link: https://github.com/ahmed-abdulhuy/Solving-TSP-and-mTSP-Combinatorial-Challenges-using-Visual-Reasoning-and-Multi-Agent-Approach-MLLMs-.git

------------

`[2407.01046] FRoG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models <https://arxiv.org/abs/2407.01046>`__ FRoG:评价大型语言模型中广义量词的模糊推理

::

    Mon, 1 Jul 2024 07:56:14 GMT
    Yiyuan Li, Shichao Sun, Pengfei Liu

Fuzzy reasoning is vital due to the frequent use of imprecise information in daily contexts. However, the ability of current large language models (LLMs) to handle such reasoning remains largely uncharted. In this paper, we introduce a new benchmark, FRoG, for fuzzy reasoning, featuring real-world mathematical word problems that incorporate generalized quantifiers. Our experimental findings reveal that fuzzy reasoning continues to pose significant challenges for LLMs. Moreover, we find that existing methods designed to enhance reasoning do not consistently improve performance in tasks involving fuzzy logic.
Additionally, our results show an inverse scaling effect in the performance of LLMs on FRoG. Interestingly, we also demonstrate that strong mathematical reasoning skills are not necessarily indicative of success on our benchmark.

------------

`[2407.00653] Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs <https://arxiv.org/abs/2407.00653>`__ 知识链:基于知识图谱学习的知识推理集成到大型语言模型中

::

    Sun, 30 Jun 2024 10:49:32 GMT
    Yifei Zhang, Xintao Wang, Jiaqing Liang, Sirui Xia, Lida Chen, Yanghua Xiao

Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new knowledge from existing one.While it has been widely studied in the context of knowledge graphs (KGs), knowledge reasoning in LLMs remains underexplored.
In this paper, we introduce Chain-of-Knowledge, a comprehensive framework for knowledge reasoning, including methodologies for both dataset construction and model learning. For dataset construction, we create KnowReason via rule mining on KGs. For model learning, we observe rule overfitting induced by naive training. Hence, we enhance CoK with a trial-and-error mechanism that simulates the human process of internal knowledge exploration. We conduct extensive experiments with KnowReason. Our results show the effectiveness of CoK in refining LLMs in not only knowledge reasoning, but also general reasoning benchmarkms.

------------

`[2407.00782] Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning <https://arxiv.org/abs/2407.00782>`__ 步进控制DPO:利用步进误差增强数学推理

::

    Sun, 30 Jun 2024 17:59:07 GMT
    Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan

Direct Preference Optimization (DPO) has proven effective at improving the performance of large language models (LLMs) on downstream tasks such as reasoning and alignment. In this work, we propose Step-Controlled DPO (SCDPO), a method for automatically providing stepwise error supervision by creating negative samples of mathematical reasoning rationales that start making errors at a specified step. By applying these samples in DPO training, SCDPO can better align the model to understand reasoning errors and output accurate reasoning steps. We apply SCDPO to both code-integrated and chain-of-thought solutions, empirically showing that it consistently improves the performance compared to naive DPO on three different SFT models, including one existing SFT model and two models we finetuned. Qualitative analysis of the credit assignment of SCDPO and DPO demonstrates the effectiveness of SCDPO at identifying errors in mathematical solutions. We then apply SCDPO to an InternLM2-20B model, resulting in a 20B model that achieves high scores of 88.5% on GSM8K and 58.1% on MATH, rivaling all other open-source LLMs, showing the great potential of our method.

------------

`[2407.00938] MalAlgoQA: A Pedagogical Approach for Evaluating Counterfactual Reasoning Abilities <https://arxiv.org/abs/2407.00938>`__ 

::

    Mon, 1 Jul 2024 03:39:13 GMT
    Naiming Liu, Shashank Sonkar, Myco Le, Richard Baraniuk

This paper introduces MalAlgoQA, a novel dataset designed to evaluate the counterfactual reasoning capabilities of Large Language Models (LLMs) through a pedagogical approach. The dataset comprises mathematics and reading comprehension questions, each accompanied by four answer choices and their corresponding rationales. We focus on the incorrect answer rationales, termed "malgorithms", which highlights flawed reasoning steps leading to incorrect answers and offers valuable insights into erroneous thought processes. We also propose the Malgorithm Identification task, where LLMs are assessed based on their ability to identify corresponding malgorithm given an incorrect answer choice. To evaluate the model performance, we introduce two metrics: Algorithm Identification Accuracy (AIA) for correct answer rationale identification, and Malgorithm Identification Accuracy (MIA) for incorrect answer rationale identification. The task is challenging since state-of-the-art LLMs exhibit significant drops in MIA as compared to AIA. Moreover, we find that the chain-of-thought prompting technique not only fails to consistently enhance MIA, but can also lead to underperformance compared to simple prompting. These findings hold significant implications for the development of more cognitively-inspired LLMs to improve their counterfactual reasoning abilities, particularly through a pedagogical perspective where understanding and rectifying student misconceptions are crucial.

------------

`[2407.01212] EconNLI: Evaluating Large Language Models on Economics Reasoning <https://arxiv.org/abs/2407.01212>`__ EconNLI:基于经济推理的大型语言模型评估

::

    Mon, 1 Jul 2024 11:58:24 GMT
    Yue Guo, Yi Yang

Large Language Models (LLMs) are widely used for writing economic analysis reports or providing financial advice, but their ability to understand economic knowledge and reason about potential results of specific economic events lacks systematic evaluation. To address this gap, we propose a new dataset, natural language inference on economic events (EconNLI), to evaluate LLMs' knowledge and reasoning abilities in the economic domain. We evaluate LLMs on (1) their ability to correctly classify whether a premise event will cause a hypothesis event and (2) their ability to generate reasonable events resulting from a given premise. Our experiments reveal that LLMs are not sophisticated in economic reasoning and may generate wrong or hallucinated answers. Our study raises awareness of the limitations of using LLMs for critical decision-making involving economic reasoning and analysis. The dataset and codes are available at https://github.com/Irenehere/EconNLI.

------------

`[2407.01525] Empowering 3D Visual Grounding with Reasoning Capabilities <https://arxiv.org/abs/2407.01525>`__ 赋予三维视觉基础推理能力

::

    Mon, 1 Jul 2024 17:59:35 GMT
    Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, Xihui Liu

Although great progress has been made in 3D visual grounding, current models still rely on explicit textual descriptions for grounding and lack the ability to reason human intentions from implicit instructions. We propose a new task called 3D reasoning grounding and introduce a new benchmark ScanReason which provides over 10K question-answer-location pairs from five reasoning types that require the synerization of reasoning and grounding. We further design our approach, ReGround3D, composed of the visual-centric reasoning module empowered by Multi-modal Large Language Model (MLLM) and the 3D grounding module to obtain accurate object locations by looking back to the enhanced geometry and fine-grained details from the 3D scenes. A chain-of-grounding mechanism is proposed to further boost the performance with interleaved reasoning and grounding steps during inference. Extensive experiments on the proposed benchmark validate the effectiveness of our proposed approach.

------------

`[2308.15399] Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories? <https://arxiv.org/abs/2308.15399>`__ 反思机器伦理——llm能否通过道德理论的视角进行道德推理?

::

    replaced with revised version Mon, 1 Jul 2024 15:33:51 GMT
    Submission history From: Jingyan Zhou [view email]
    [v1] Tue, 29 Aug 2023 15:57:32 UTC (1,272 KB)
    [v2] Mon, 1 Jul 2024 15:33:51 UTC (261 KB)
    Jingyan Zhou, Minda Hu, Junan Li, Xiaoying Zhang, Xixin Wu, Irwin King, Helen Meng

Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for overgeneralizing the moral stances of a limited group of annotators and lacking explainability. This work proposes a flexible top-down framework to steer (Large) Language Models (LMs) to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore, we show the alignment between different moral theories and existing morality datasets. Our analysis exhibits the potential and flaws in existing resources (models and datasets) in developing explainable moral judgment-making systems.

------------

`[2311.08390] Predicting Text Preference Via Structured Comparative Reasoning <https://arxiv.org/abs/2311.08390>`__ 基于结构化比较推理的文本偏好预测

::

    replaced with revised version Mon, 1 Jul 2024 16:57:56 GMT
    Submission history From: Jing Nathan Yan [view email]
    [v1] Tue, 14 Nov 2023 18:51:38 UTC (1,070 KB)
    [v2] Mon, 1 Jul 2024 16:57:56 UTC (828 KB)
    Jing Nathan Yan, Tianqi Liu, Justin T Chiu, Jiaming Shen, Zhen Qin, Yue Yu, Yao Zhao, Charu Lakshmanan, Yair Kurzion, Alexander M. Rush, Jialu Liu, Michael Bendersky

Comparative reasoning plays a crucial role in text preference prediction; however, large language models (LLMs) often demonstrate inconsistencies in their reasoning. While approaches like Chain-of-Thought improve accuracy in many other settings, they struggle to consistently distinguish the similarities and differences of complex texts. We introduce SC, a prompting approach that predicts text preferences by generating structured intermediate comparisons. SC begins by proposing aspects of comparison, followed by generating textual comparisons under each aspect. We select consistent comparisons with a pairwise consistency comparator that ensures each aspect's comparisons clearly distinguish differences between texts, significantly reducing hallucination and improving consistency. Our comprehensive evaluations across various NLP tasks, including summarization, retrieval, and automatic rating, demonstrate that SC equips LLMs to achieve state-of-the-art performance in text preference prediction.

------------

`[2401.07817] Question Translation Training for Better Multilingual Reasoning <https://arxiv.org/abs/2401.07817>`__ 多语言推理的问题翻译训练

::

    replaced with revised version Sat, 29 Jun 2024 16:34:38 GMT
    Submission history From: Wenhao Zhu [view email]
    [v1] Mon, 15 Jan 2024 16:39:10 UTC (898 KB)
    [v2] Sat, 17 Feb 2024 16:04:07 UTC (2,461 KB)
    [v3] Sat, 29 Jun 2024 16:34:38 UTC (196 KB)
    Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, Alexandra Birch

Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of mathematical chain-of-thought. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English parallel question data. In this way we perform targeted, in-domain language alignment which makes best use of English instruction data to unlock the LLMs' multilingual reasoning abilities. Experimental results on LLaMA2-13B show that question alignment leads to consistent improvements over the translate-training approach: an average improvement of 11.3% and 16.1% accuracy across ten languages on the MGSM and MSVAMP multilingual reasoning benchmarks. The project will be available at: this https URL.

------------

`[2404.00376] Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks <https://arxiv.org/abs/2404.00376>`__ 小型语言模型从医学教科书中学习增强的推理技能

::

    replaced with revised version Sun, 30 Jun 2024 09:24:12 GMT
    Submission history From: Hyunjae Kim [view email]
    [v1] Sat, 30 Mar 2024 14:09:00 UTC (1,250 KB)
    [v2] Sun, 30 Jun 2024 09:24:12 UTC (1,404 KB)
    Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee, Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, Jaewoo Kang

While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field. Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step reasoning capabilities required for solving complex medical problems. To address this, we introduce Meerkat, a new family of medical AI systems ranging from 7 to 70 billion parameters. The models were trained using our new synthetic dataset consisting of high-quality chain-of-thought reasoning paths sourced from 18 medical textbooks, along with diverse instruction-following datasets. Our systems achieved remarkable accuracy across six medical benchmarks, surpassing the previous best models such as MediTron and BioMistral, and GPT-3.5 by a large margin. Notably, Meerkat-7B surpassed the passing threshold of the United States Medical Licensing Examination (USMLE) for the first time for a 7B-parameter model, while Meerkat-70B outperformed GPT-4 by an average of 1.3%. Additionally, Meerkat-70B correctly diagnosed 21 out of 38 complex clinical cases, outperforming humans' 13.8 and closely matching GPT-4's 21.8. Our systems offered more detailed free-form responses to clinical queries compared to existing small models, approaching the performance level of large commercial models. This significantly narrows the performance gap with large LMs, showcasing its effectiveness in addressing complex medical challenges.

------------

`[2406.10625] On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models <https://arxiv.org/abs/2406.10625>`__ 大型语言模型中忠实思维链推理的困难

::

    replaced with revised version Mon, 1 Jul 2024 13:36:29 GMT
    Submission history From: Sree Harsha Tanneru [view email]
    [v1] Sat, 15 Jun 2024 13:16:44 UTC (2,469 KB)
    [v2] Mon, 1 Jul 2024 13:36:29 UTC (1,228 KB)
    Sree Harsha Tanneru, Dan Ley, Chirag Agarwal, Himabindu Lakkaraju

As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior.
While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.

------------

`[2406.15859] LLM-Powered Explanations: Unraveling Recommendations Through Subgraph Reasoning <https://arxiv.org/abs/2406.15859>`__ llm驱动的解释:通过子图推理解开建议

::

    replaced with revised version Sun, 30 Jun 2024 02:13:19 GMT
    Submission history From: Yuxiao Li [view email]
    [v1] Sat, 22 Jun 2024 14:14:03 UTC (2,059 KB)
    [v2] Sun, 30 Jun 2024 02:13:19 UTC (2,060 KB)
    Guangsi Shi, Xiaofeng Deng, Linhao Luo, Lijuan Xia, Lei Bao, Bei Ye, Fei Du, Shirui Pan, Yuxiao Li

Recommender systems are pivotal in enhancing user experiences across various web applications by analyzing the complicated relationships between users and items. Knowledge graphs(KGs) have been widely used to enhance the performance of recommender systems. However, KGs are known to be noisy and incomplete, which are hard to provide reliable explanations for recommendation results. An explainable recommender system is crucial for the product development and subsequent decision-making. To address these challenges, we introduce a novel recommender that synergies Large Language Models (LLMs) and KGs to enhance the recommendation and provide interpretable results. Specifically, we first harness the power of LLMs to augment KG reconstruction. LLMs comprehend and decompose user reviews into new triples that are added into KG. In this way, we can enrich KGs with explainable paths that express user preferences. To enhance the recommendation on augmented KGs, we introduce a novel subgraph reasoning module that effectively measures the importance of nodes and discovers reasoning for recommendation. Finally, these reasoning paths are fed into the LLMs to generate interpretable explanations of the recommendation results. Our approach significantly enhances both the effectiveness and interpretability of recommender systems, especially in cross-selling scenarios where traditional methods falter. The effectiveness of our approach has been rigorously tested on four open real-world datasets, with our methods demonstrating a superior performance over contemporary state-of-the-art techniques by an average improvement of 12%. The application of our model in a multinational engineering and technology company cross-selling recommendation system further underscores its practical utility and potential to redefine recommendation practices through improved accuracy and user trust.

------------

-----------
ToolUse (3)
-----------

`[2407.00167] Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of Automatic Data Annotation Approach <https://arxiv.org/abs/2407.00167>`__ GPT-4能帮助检测戒烟意图吗?对自动数据标注方法的探索

::

    Fri, 28 Jun 2024 18:06:48 GMT
    Sai Krishna Revanth Vuruma, Dezhi Wu, Saborny Sen Gupta, Lucas Aust, Valerie Lookingbill, Wyatt Bellamy, Yang Ren, Erin Kasson, Li-Shiun Chen, Patricia Cavazos-Rehg, Dian Hu and Ming Huang

In recent years, the United States has witnessed a significant surge in the popularity of vaping or e-cigarette use, leading to a notable rise in cases of e-cigarette and vaping use-associated lung injury (EVALI) that caused hospitalizations and fatalities during the EVALI outbreak in 2019, highlighting the urgency to comprehend vaping behaviors and develop effective strategies for cessation. Due to the ubiquity of social media platforms, over 4.7 billion users worldwide use them for connectivity, communications, news, and entertainment with a significant portion of the discourse related to health, thereby establishing social media data as an invaluable organic data resource for public health research. In this study, we extracted a sample dataset from one vaping sub-community on Reddit to analyze users' quit-vaping intentions.
Leveraging OpenAI's latest large language model GPT-4 for sentence-level quit vaping intention detection, this study compares the outcomes of this model against layman and clinical expert annotations. Using different prompting strategies such as zero-shot, one-shot, few-shot and chain-of-thought prompting, we developed 8 prompts with varying levels of detail to explain the task to GPT-4 and also evaluated the performance of the strategies against each other. These preliminary findings emphasize the potential of GPT-4 in social media data analysis, especially in identifying users' subtle intentions that may elude human detection.

------------

`[2407.00065] A Personalised Learning Tool for Physics Undergraduate Students Built On a Large Language Model for Symbolic Regression <https://arxiv.org/abs/2407.00065>`__ 基于符号回归大型语言模型的物理本科生个性化学习工具

::

    Mon, 17 Jun 2024 13:43:30 GMT
    Yufan Zhu, Zi-Yu Khoo, Jonathan Sze Choong Low, Stephane Bressan

Interleaved practice enhances the memory and problem-solving ability of students in undergraduate courses. We introduce a personalized learning tool built on a Large Language Model (LLM) that can provide immediate and personalized attention to students as they complete homework containing problems interleaved from undergraduate physics courses. Our tool leverages the dimensional analysis method, enhancing students' qualitative thinking and problem-solving skills for complex phenomena. Our approach combines LLMs for symbolic regression with dimensional analysis via prompt engineering and offers students a unique perspective to comprehend relationships between physics variables. This fosters a broader and more versatile understanding of physics and mathematical principles and complements a conventional undergraduate physics education that relies on interpreting and applying established equations within specific contexts. We test our personalized learning tool on the equations from Feynman's lectures on physics. Our tool can correctly identify relationships between physics variables for most equations, underscoring its value as a complementary personalized learning tool for undergraduate physics students.

------------

`[2407.00132] ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents <https://arxiv.org/abs/2407.00132>`__ ShortcutsBench:基于api的代理的大规模真实基准测试

::

    Fri, 28 Jun 2024 08:45:02 GMT
    Haiyang Shen, Yue Li, Desong Meng, Dongqi Cai, Sheng Qi, Li Zhang, Mengwei Xu, Yun Ma

Recent advancements in integrating large language models (LLMs) with application programming interfaces (APIs) have gained significant interest in both academia and industry. These API-based agents, leveraging the strong autonomy and planning capabilities of LLMs, can efficiently solve problems requiring multi-step actions. However, their ability to handle multi-dimensional difficulty levels, diverse task types, and real-world demands through APIs remains unknown. In this paper, we introduce \textsc{ShortcutsBench}, a large-scale benchmark for the comprehensive evaluation of API-based agents in solving tasks with varying levels of difficulty, diverse task types, and real-world demands. \textsc{ShortcutsBench} includes a wealth of real APIs from Apple Inc.'s operating systems, refined user queries from shortcuts, human-annotated high-quality action sequences from shortcut developers, and accurate parameter filling values about primitive parameter types, enum parameter types, outputs from previous actions, and parameters that need to request necessary information from the system or user.
Our extensive evaluation of agents built with $5$ leading open-source (size >= 57B) and $4$ closed-source LLMs (e.g. Gemini-1.5-Pro and GPT-3.5) reveals significant limitations in handling complex queries related to API selection, parameter filling, and requesting necessary information from systems and users.
These findings highlight the challenges that API-based agents face in effectively fulfilling real and complex user queries. All datasets, code, and experimental results will be available at \url{https://github.com/eachsheep/shortcutsbench}.

------------

------------------------
Retrieval-Augmented (18)
------------------------

`[2407.00978] Hybrid RAG-empowered Multi-modal LLM for Secure Healthcare Data Management: A Diffusion-based Contract Theory Approach <https://arxiv.org/abs/2407.00978>`__ 面向安全医疗数据管理的混合rag授权多模态LLM:基于扩散的契约理论方法

::

    Mon, 1 Jul 2024 05:28:40 GMT
    Cheng Su, Jinbo Wen, Jiawen Kang, Yonghua Wang, Hudan Pan, and M. Shamim Hossain

Secure data management and effective data sharing have become paramount in the rapidly evolving healthcare landscape. The advancement of generative artificial intelligence has positioned Multi-modal Large Language Models (MLLMs) as crucial tools for managing healthcare data. MLLMs can support multi-modal inputs and generate diverse types of content by leveraging large-scale training on vast amounts of multi-modal data. However, critical challenges persist in developing medical MLLMs, including healthcare data security and freshness issues, affecting the output quality of MLLMs. In this paper, we propose a hybrid Retrieval-Augmented Generation (RAG)-empowered medical MLLMs framework for healthcare data management. This framework leverages a hierarchical cross-chain architecture to facilitate secure data training. Moreover, it enhances the output quality of MLLMs through hybrid RAG, which employs multi-modal metrics to filter various unimodal RAG results and incorporates these retrieval results as additional inputs to MLLMs.
Additionally, we employ age of information to indirectly evaluate the data freshness impact of MLLMs and utilize contract theory to incentivize healthcare data holders to share fresh data, mitigating information asymmetry in data sharing. Finally, we utilize a generative diffusion model-based reinforcement learning algorithm to identify the optimal contract for efficient data sharing.
Numerical results demonstrate the effectiveness of the proposed schemes, which achieve secure and efficient healthcare data management.

------------

`[2407.00361] From RAG to RICHES: Retrieval Interlaced with Sequence Generation <https://arxiv.org/abs/2407.00361>`__ 从破烂到财富:检索与序列生成交错

::

    Sat, 29 Jun 2024 08:16:58 GMT
    Palak Jain, Livio Baldini Soares and Tom Kwiatkowski

We present RICHES, a novel approach that interleaves retrieval with sequence generation tasks. RICHES offers an alternative to conventional RAG systems by eliminating the need for separate retriever and generator. It retrieves documents by directly decoding their contents, constrained on the corpus.
Unifying retrieval with generation allows us to adapt to diverse new tasks via prompting alone. RICHES can work with any Instruction-tuned model, without additional training. It provides attributed evidence, supports multi-hop retrievals and interleaves thoughts to plan on what to retrieve next, all within a single decoding pass of the LLM. We demonstrate the strong performance of RICHES across ODQA tasks including attributed and multi-hop QA.

------------

`[2407.00499] ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees <https://arxiv.org/abs/2407.00499>`__ ConU:具有正确性覆盖保证的大型语言模型共形不确定性

::

    Sat, 29 Jun 2024 17:33:07 GMT
    Zhiyuan Wang, Jinhao Duan, Lu Cheng, Yue Zhang, Qingni Wang, Hengtao Shen, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu

Uncertainty quantification (UQ) in natural language generation (NLG) tasks remains an open challenge, exacerbated by the intricate nature of the recent large language models (LLMs). This study investigates adapting conformal prediction (CP), which can convert any heuristic measure of uncertainty into rigorous theoretical guarantees by constructing prediction sets, for black-box LLMs in open-ended NLG tasks. We propose a sampling-based uncertainty measure leveraging self-consistency and develop a conformal uncertainty criterion by integrating the uncertainty condition aligned with correctness into the design of the CP algorithm. Experimental results indicate that our uncertainty measure generally surpasses prior state-of-the-art methods. Furthermore, we calibrate the prediction sets within the model's unfixed answer distribution and achieve strict control over the correctness coverage rate across 6 LLMs on 4 free-form NLG datasets, spanning general-purpose and medical domains, while the small average set size further highlights the efficiency of our method in providing trustworthy guarantees for practical open-ended NLG applications.

------------

`[2407.00668] HRDE: Retrieval-Augmented Large Language Models for Chinese Health Rumor Detection and Explainability <https://arxiv.org/abs/2407.00668>`__ HRDE:面向中文健康谣言检测和可解释性的检索增强大型语言模型

::

    Sun, 30 Jun 2024 11:27:50 GMT
    Yanfang Chen and Ding Chen and Shichao Song and Simin Niu and Hanyu Wang and Zeyun Tang and Feiyu Xiong and Zhiyu Li

As people increasingly prioritize their health, the speed and breadth of health information dissemination on the internet have also grown. At the same time, the presence of false health information (health rumors) intermingled with genuine content poses a significant potential threat to public health.
However, current research on Chinese health rumors still lacks a large-scale, public, and open-source dataset of health rumor information, as well as effective and reliable rumor detection methods. This paper addresses this gap by constructing a dataset containing 1.12 million health-related rumors (HealthRCN) through web scraping of common health-related questions and a series of data processing steps. HealthRCN is the largest known dataset of Chinese health information rumors to date. Based on this dataset, we propose retrieval-augmented large language models for Chinese health rumor detection and explainability (HRDE). This model leverages retrieved relevant information to accurately determine whether the input health information is a rumor and provides explanatory responses, effectively aiding users in verifying the authenticity of health information. In evaluation experiments, we compared multiple models and found that HRDE outperformed them all, including GPT-4-1106-Preview, in rumor detection accuracy and answer quality. HRDE achieved an average accuracy of 91.04% and an F1 score of 91.58%.

------------

`[2407.00782] Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning <https://arxiv.org/abs/2407.00782>`__ 步进控制DPO:利用步进误差增强数学推理

::

    Sun, 30 Jun 2024 17:59:07 GMT
    Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan

Direct Preference Optimization (DPO) has proven effective at improving the performance of large language models (LLMs) on downstream tasks such as reasoning and alignment. In this work, we propose Step-Controlled DPO (SCDPO), a method for automatically providing stepwise error supervision by creating negative samples of mathematical reasoning rationales that start making errors at a specified step. By applying these samples in DPO training, SCDPO can better align the model to understand reasoning errors and output accurate reasoning steps. We apply SCDPO to both code-integrated and chain-of-thought solutions, empirically showing that it consistently improves the performance compared to naive DPO on three different SFT models, including one existing SFT model and two models we finetuned. Qualitative analysis of the credit assignment of SCDPO and DPO demonstrates the effectiveness of SCDPO at identifying errors in mathematical solutions. We then apply SCDPO to an InternLM2-20B model, resulting in a 20B model that achieves high scores of 88.5% on GSM8K and 58.1% on MATH, rivaling all other open-source LLMs, showing the great potential of our method.

------------

`[2407.01080] Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese <https://arxiv.org/abs/2407.01080>`__ Face4RAG:面向中文检索增强生成的事实一致性评估

::

    Mon, 1 Jul 2024 08:35:04 GMT
    Yunqi Xu, Tianchi Cai, Jiyan Jiang, Xierui Song

The prevailing issue of factual inconsistency errors in conventional Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency Evaluation (FCE). Despite the various FCE methods proposed earlier, these methods are evaluated on datasets generated by specific Large Language Models (LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE methods perform on other LLMs with different error distributions or even unseen error types, as these methods may fail to detect the error types generated by other LLMs. To fill this gap, in this paper, we propose the first comprehensive FCE benchmark \emph{Face4RAG} for RAG independent of the underlying LLM. Our benchmark consists of a synthetic dataset built upon a carefully designed typology for factuality inconsistency error and a real-world dataset constructed from six commonly used LLMs, enabling evaluation of FCE methods on specific error types or real-world error distributions. On the proposed benchmark, we discover the failure of existing FCE methods to detect the logical fallacy, which refers to a mismatch of logic structures between the answer and the retrieved reference. To fix this issue, we further propose a new method called \emph{L-Face4RAG} with two novel designs of logic-preserving answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG substantially outperforms previous methods for factual inconsistency detection on a wide range of tasks, notably beyond the RAG task from which it is originally motivated. Both the benchmark and our proposed method are publicly available.\footnote{\url{https://huggingface.co/datasets/yq27/Face4RAG}\label{link_face4rag}}

------------

`[2407.01102] BERGEN: A Benchmarking Library for Retrieval-Augmented Generation <https://arxiv.org/abs/2407.01102>`__ 

::

    Mon, 1 Jul 2024 09:09:27 GMT
    David Rau, Herv\'e D\'ejean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, St\'ephane Clinchant

Retrieval-Augmented Generation allows to enhance Large Language Models with external knowledge. In response to the recent popularity of generative LLMs, many RAG approaches have been proposed, which involve an intricate number of different configurations such as evaluation datasets, collections, metrics, retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in comparing approaches and understanding the impact of each component in the pipeline. In this work, we study best practices that lay the groundwork for a systematic evaluation of RAG and present BERGEN, an end-to-end library for reproducible research standardizing RAG experiments. In an extensive study focusing on QA, we benchmark different state-of-the-art retrievers, rerankers, and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our open-source library BERGEN is available under \url{https://github.com/naver/bergen}.

------------

`[2407.01158] Learning to Explore and Select for Coverage-Conditioned Retrieval-Augmented Generation <https://arxiv.org/abs/2407.01158>`__ 学习探索和选择覆盖条件检索增强生成

::

    Mon, 1 Jul 2024 10:26:19 GMT
    Takyoung Kim, Kyungjae Lee, Young Rok Jang, Ji Yong Cho, Gangwoo Kim, Minseok Cho, Moontae Lee

Interactions with billion-scale large language models typically yield long-form responses due to their extensive parametric capacities, along with retrieval-augmented features. While detailed responses provide insightful viewpoint of a specific subject, they frequently generate redundant and less engaging content that does not meet user interests. In this work, we focus on the role of query outlining (i.e., selected sequence of queries) in scenarios that users request a specific range of information, namely coverage-conditioned ($C^2$) scenarios. For simulating $C^2$ scenarios, we construct QTree, 10K sets of information-seeking queries decomposed with various perspectives on certain topics. By utilizing QTree, we train QPlanner, a 7B language model generating customized query outlines that follow coverage-conditioned queries. We analyze the effectiveness of generated outlines through automatic and human evaluation, targeting on retrieval-augmented generation (RAG). Moreover, the experimental results demonstrate that QPlanner with alignment training can further provide outlines satisfying diverse user interests. Our resources are available at https://github.com/youngerous/qtree.

------------

`[2407.01219] Searching for Best Practices in Retrieval-Augmented Generation <https://arxiv.org/abs/2407.01219>`__ 搜索检索增强生成中的最佳实践

::

    Mon, 1 Jul 2024 12:06:34 GMT
    Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang

Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a "retrieval as generation" strategy.

------------

`[2407.01370] Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems <https://arxiv.org/abs/2407.01370>`__ Haystack摘要:对长上下文llm和RAG系统的挑战

::

    Mon, 1 Jul 2024 15:23:42 GMT
    Philippe Laban and Alexander R. Fabbri and Caiming Xiong and Chien-Sheng Wu

LLMs and RAG systems are now capable of handling millions of input tokens or more. However, evaluating the output quality of such systems on long-context tasks remains challenging, as tasks like Needle-in-a-Haystack lack complexity.
In this work, we argue that summarization can play a central role in such evaluation. We design a procedure to synthesize Haystacks of documents, ensuring that specific \textit{insights} repeat across documents. The "Summary of a Haystack" (SummHay) task then requires a system to process the Haystack and generate, given a query, a summary that identifies the relevant insights and precisely cites the source documents. Since we have precise knowledge of what insights should appear in a haystack summary and what documents should be cited, we implement a highly reproducible automatic evaluation that can score summaries on two aspects - Coverage and Citation. We generate Haystacks in two domains (conversation, news), and perform a large-scale evaluation of 10 LLMs and corresponding 50 RAG systems. Our findings indicate that SummHay is an open challenge for current systems, as even systems provided with an Oracle signal of document relevance lag our estimate of human performance (56\%) by 10+ points on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and Claude 3 Opus score below 20% on SummHay. We show SummHay can also be used to study enterprise RAG systems and position bias in long-context models. We hope future systems can equal and surpass human performance on SummHay.

------------

`[2407.01463] Retrieval-augmented generation in multilingual settings <https://arxiv.org/abs/2407.01463>`__ 

::

    Mon, 1 Jul 2024 16:56:50 GMT
    Nadezhda Chirkova, David Rau, Herv\'e D\'ejean, Thibault Formal, St\'ephane Clinchant, Vassilina Nikoulina

Retrieval-augmented generation (RAG) has recently emerged as a promising solution for incorporating up-to-date or domain-specific knowledge into large language models (LLMs) and improving LLM factuality, but is predominantly studied in English-only settings. In this work, we consider RAG in the multilingual setting (mRAG), i.e. with user queries and the datastore in 13 languages, and investigate which components and with which adjustments are needed to build a well-performing mRAG pipeline, that can be used as a strong baseline in future works. Our findings highlight that despite the availability of high-quality off-the-shelf multilingual retrievers and generators, task-specific prompt engineering is needed to enable generation in user languages. Moreover, current evaluation metrics need adjustments for multilingual setting, to account for variations in spelling named entities. The main limitations to be addressed in future works include frequent code-switching in non-Latin alphabet languages, occasional fluency errors, wrong reading of the provided documents, or irrelevant retrieval. We release the code for the resulting mRAG baseline pipeline at https://github.com/naver/bergen.

------------

`[2407.01403] Optimization of Retrieval-Augmented Generation Context with Outlier Detection <https://arxiv.org/abs/2407.01403>`__ 基于异常检测的检索增强生成上下文优化

::

    Mon, 1 Jul 2024 15:53:29 GMT
    Vitaly Bulgakov

In this paper, we focus on methods to reduce the size and improve the quality of the prompt context required for question-answering systems. Attempts to increase the number of retrieved chunked documents and thereby enlarge the context related to the query can significantly complicate the processing and decrease the performance of a Large Language Model (LLM) when generating responses to queries. It is well known that a large set of documents retrieved from a database in response to a query may contain irrelevant information, which often leads to hallucinations in the resulting answers. Our goal is to select the most semantically relevant documents, treating the discarded ones as outliers. We propose and evaluate several methods for identifying outliers by creating features that utilize the distances of embedding vectors, retrieved from the vector database, to both the centroid and the query vectors. The methods were evaluated by comparing the similarities of the retrieved LLM responses to ground-truth answers obtained using the OpenAI GPT-4o model. It was found that the greatest improvements were achieved with increasing complexity of the questions and answers.

------------

`[2407.00072] Pistis-RAG: A Scalable Cascading Framework Towards Trustworthy Retrieval-Augmented Generation <https://arxiv.org/abs/2407.00072>`__ Pistis-RAG:面向可信检索增强生成的可扩展级联框架

::

    Fri, 21 Jun 2024 08:52:11 GMT
    Yu Bai, Yukai Miao, Li Chen, Dan Li, Yanyu Ren, Hongtao Xie, Ce Yang, Xuhui Cai

In Greek mythology, Pistis symbolized good faith, trust, and reliability, echoing the core principles of RAG in LLM systems. Pistis-RAG, a scalable multi-stage framework, effectively addresses the challenges of large-scale retrieval-augmented generation (RAG). Each stage plays a distinct role: matching refines the search space, pre-ranking prioritizes semantically relevant documents, and ranking aligns with the large language model's (LLM) preferences. The reasoning and aggregating stage supports the implementation of complex chain-of-thought (CoT) methods within this cascading structure. We argue that the lack of strong alignment between LLMs and the external knowledge ranking methods used in RAG tasks is relevant to the reliance on the model-centric paradigm in RAG frameworks. A content-centric approach would prioritize seamless integration between the LLMs and external information sources, optimizing the content transformation process for each specific task.
Critically, our ranking stage deviates from traditional RAG approaches by recognizing that semantic relevance alone may not directly translate to improved generation. This is due to the sensitivity of the few-shot prompt order, as highlighted in prior work \cite{lu2021fantastically}. Current RAG frameworks fail to account for this crucial factor. We introduce a novel ranking stage specifically designed for RAG systems. It adheres to information retrieval principles while considering the unique business scenario captured by LLM preferences and user feedback. Our approach integrates in-context learning (ICL) methods and reasoning steps to incorporate user feedback, ensuring efficient alignment. Experiments on the MMLU benchmark demonstrate a 9.3\% performance improvement. The model and code will be open-sourced on GitHub.
Experiments on real-world, large-scale data validate our framework's scalability.

------------

`[2407.01274] Leveraging Large Language Models for Actionable Course Evaluation Student Feedback to Lecturers <https://arxiv.org/abs/2407.01274>`__ 利用大型语言模型进行可操作的课程评估学生对讲师的反馈

::

    Mon, 1 Jul 2024 13:29:55 GMT
    Mike Zhang, Euan D Lindsay, Frederik Bode Thorbensen, Danny B{\o}gsted Poulsen, Johannes Bjerva

End of semester student evaluations of teaching are the dominant mechanism for providing feedback to academics on their teaching practice. For large classes, however, the volume of feedback makes these tools impractical for this purpose. This paper explores the use of open-source generative AI to synthesise factual, actionable and appropriate summaries of student feedback from these survey responses. In our setup, we have 742 student responses ranging over 75 courses in a Computer Science department. For each course, we synthesise a summary of the course evaluations and actionable items for the instructor. Our results reveal a promising avenue for enhancing teaching practices in the classroom setting. Our contribution lies in demonstrating the feasibility of using generative AI to produce insightful feedback for teachers, thus providing a cost-effective means to support educators' development. Overall, our work highlights the possibility of using generative AI to produce factual, actionable, and appropriate feedback for teachers in the classroom setting.

------------

`[2404.02022] Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts <https://arxiv.org/abs/2404.02022>`__ 基于向量化上下文改进检索扩展开放域问答

::

    replaced with revised version Mon, 1 Jul 2024 10:38:59 GMT
    Submission history From: Zhuo Chen [view email]
    [v1] Tue, 2 Apr 2024 15:10:11 UTC (11,290 KB)
    [v2] Mon, 1 Jul 2024 10:38:59 UTC (9,750 KB)
    Zhuo Chen, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Kewei Tu

In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks. It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs. With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Context Learning settings.

------------

`[2406.13663] Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation <https://arxiv.org/abs/2406.13663>`__ 基于模型内部的答案归因可信检索增强生成

::

    replaced with revised version Mon, 1 Jul 2024 12:39:26 GMT
    Submission history From: Jirui Qi [view email]
    [v1] Wed, 19 Jun 2024 16:10:26 UTC (11,795 KB)
    [v2] Mon, 1 Jul 2024 12:39:26 UTC (11,796 KB)
    Jirui Qi, Gabriele Sarti, Raquel Fern\'andez, Arianna Bisazza

Ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain. Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers. However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs' context usage throughout the generation. In this work, we present MIRAGE --Model Internals-based RAG Explanations -- a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. We evaluate our proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. Our qualitative evaluation highlights the faithfulness of MIRAGE's attributions and underscores the promising application of model internals for RAG answer attribution.

------------

`[2406.15319] LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs <https://arxiv.org/abs/2406.15319>`__ longag:用长上下文llm增强检索增强生成

::

    replaced with revised version Sun, 30 Jun 2024 15:01:36 GMT
    Submission history From: Ziyan Jiang [view email]
    [v1] Fri, 21 Jun 2024 17:23:21 UTC (465 KB)
    [v2] Sun, 30 Jun 2024 15:01:36 UTC (659 KB)
    Ziyan Jiang, Xueguang Ma, Wenhu Chen

In traditional RAG framework, the basic retrieval units are normally short. The common retrievers like DPR normally work with 100-word Wikipedia paragraphs. Such a design forces the retriever to search over a large corpus to find the `needle' unit. In contrast, the readers only need to extract answers from the short retrieved units. Such an imbalanced `heavy' retriever and `light' reader design can lead to sub-optimal performance. In order to alleviate the imbalance, we propose a new framework LongRAG, consisting of a `long retriever' and a `long reader'. LongRAG processes the entire Wikipedia into 4K-token units, which is 30x longer than before. By increasing the unit size, we significantly reduce the total units from 22M to 700K. This significantly lowers the burden of retriever, which leads to a remarkable retrieval score: answer recall@1=71% on NQ (previously 52%) and answer recall@2=72% (previously 47%) on HotpotQA (full-wiki). Then we feed the top-k retrieved units ($\approx$ 30K tokens) to an existing long-context LLM to perform zero-shot answer extraction. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ, which is the best known result. LongRAG also achieves 64.3% on HotpotQA (full-wiki), which is on par of the SoTA model. Our study offers insights into the future roadmap for combining RAG with long-context LLMs.

------------

`[2406.17534] Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification <https://arxiv.org/abs/2406.17534>`__ 基于检索式上下文学习的小样本层次文本分类

::

    replaced with revised version Sat, 29 Jun 2024 09:24:33 GMT
    Submission history From: Yu Zhao [view email]
    [v1] Tue, 25 Jun 2024 13:19:41 UTC (2,383 KB)
    [v2] Sat, 29 Jun 2024 09:24:33 UTC (2,383 KB)
    Huiyao Chen, Yu Zhao, Zulong Chen, Mengjia Wang, Liangyue Li, Meishan Zhang, Min Zhang

Hierarchical text classification (HTC) is an important task with broad applications, while few-shot HTC has gained increasing interest recently. While in-context learning (ICL) with large language models (LLMs) has achieved significant success in few-shot learning, it is not as effective for HTC because of the expansive hierarchical label sets and extremely-ambiguous labels. In this work, we introduce the first ICL-based framework with LLM for few-shot HTC. We exploit a retrieval database to identify relevant demonstrations, and an iterative policy to manage multi-layer hierarchical labels. Particularly, we equip the retrieval database with HTC label-aware representations for the input texts, which is achieved by continual training on a pretrained language model with masked language modeling (MLM), layer-wise classification (CLS, specifically for HTC), and a novel divergent contrastive learning (DCL, mainly for adjacent semantically-similar labels) objective. Experimental results on three benchmark datasets demonstrate superior performance of our method, and we can achieve state-of-the-art results in few-shot HTC.

------------

----------
Agent (15)
----------

`[2407.00092] Visual Reasoning and Multi-Agent Approach in Multimodal Large Language Models (MLLMs): Solving TSP and mTSP Combinatorial Challenges <https://arxiv.org/abs/2407.00092>`__ 多模态大型语言模型中的视觉推理和多智能体方法:解决TSP和mTSP组合挑战

::

    Wed, 26 Jun 2024 07:12:06 GMT
    Mohammed Elhenawy, Ahmad Abutahoun, Taqwa I.Alhadidi, Ahmed Jaber, Huthaifa I. Ashqar, Shadi Jaradat, Ahmed Abdelhay, Sebastien Glaser, and Andry Rakotonirainy

Multimodal Large Language Models (MLLMs) harness comprehensive knowledge spanning text, images, and audio to adeptly tackle complex problems, including zero-shot in-context learning scenarios. This study explores the ability of MLLMs in visually solving the Traveling Salesman Problem (TSP) and Multiple Traveling Salesman Problem (mTSP) using images that portray point distributions on a two-dimensional plane. We introduce a novel approach employing multiple specialized agents within the MLLM framework, each dedicated to optimizing solutions for these combinatorial challenges. Our experimental investigation includes rigorous evaluations across zero-shot settings and introduces innovative multi-agent zero-shot in-context scenarios. The results demonstrated that both multi-agent models. Multi-Agent 1, which includes the Initializer, Critic, and Scorer agents, and Multi-Agent 2, which comprises only the Initializer and Critic agents; significantly improved solution quality for TSP and mTSP problems. Multi-Agent 1 excelled in environments requiring detailed route refinement and evaluation, providing a robust framework for sophisticated optimizations. In contrast, Multi-Agent 2, focusing on iterative refinements by the Initializer and Critic, proved effective for rapid decision-making scenarios. These experiments yield promising outcomes, showcasing the robust visual reasoning capabilities of MLLMs in addressing diverse combinatorial problems. The findings underscore the potential of MLLMs as powerful tools in computational optimization, offering insights that could inspire further advancements in this promising field. Project link: https://github.com/ahmed-abdulhuy/Solving-TSP-and-mTSP-Combinatorial-Challenges-using-Visual-Reasoning-and-Multi-Agent-Approach-MLLMs-.git

------------

`[2407.00993] Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents <https://arxiv.org/abs/2407.00993>`__ Mobile- bench:一个基于llm的移动agent评测基准

::

    Mon, 1 Jul 2024 06:10:01 GMT
    Shihan Deng, Weikai Xu, Hongda Sun, Wei Liu, Tao Tan, Jianfeng Liu, Ang Li, Jian Luan, Bin Wang, Rui Yan, Shuo Shang

With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents.
Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents.
First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps.

------------

`[2407.00466] BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science <https://arxiv.org/abs/2407.00466>`__ BioKGBench:面向生物医学的人工智能主体知识图谱检测基准

::

    Sat, 29 Jun 2024 15:23:28 GMT
    Xinna Lin, Siqi Ma, Junjie Shan, Xiaojing Zhang, Shell Xu Hu, Tiannan Guo, Stan Z. Li, Kaicheng Yu

Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models (LLMs). However, to evaluate such systems, people either rely on direct Question-Answering (QA) to the LLM itself, or in a biomedical experimental manner. How to precisely benchmark biomedical agents from an AI Scientist perspective remains largely unexplored.
To this end, we draw inspiration from one most important abilities of scientists, understanding the literature, and introduce BioKGBench. In contrast to traditional evaluation benchmark that only focuses on factual QA, where the LLMs are known to have hallucination issues, we first disentangle "Understanding Literature" into two atomic abilities, i) "Understanding" the unstructured text from research papers by performing scientific claim verification, and ii) Ability to interact with structured Knowledge-Graph Question-Answering (KGQA) as a form of "Literature" grounding. We then formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based Retrieval-Augmented Generation (RAG) to identify the factual errors of existing large-scale knowledge graph databases. We collect over two thousand data for two atomic tasks and 225 high-quality annotated data for the agent task.
Surprisingly, we discover that state-of-the-art agents, both daily scenarios and biomedical ones, have either failed or inferior performance on our benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent.
On the widely used popular knowledge graph, we discover over 90 factual errors which provide scenarios for agents to make discoveries and demonstrate the effectiveness of our approach. The code and data are available at https://github.com/westlake-autolab/BioKGBench.

------------

`[2407.01093] IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation <https://arxiv.org/abs/2407.01093>`__ 易卜生:导演-演员-代理协作的可控互动戏剧剧本生成

::

    Mon, 1 Jul 2024 08:49:57 GMT
    Senyu Han, Lu Chen, Li-Min Lin, Zhengshan Xu, Kai Yu

Large language models have demonstrated their capabilities in storyline creation and human-like character role-playing. Current language model agents mainly focus on reasonable behaviors from the level of individuals, and their behaviors might be hard to constraint on the level of the whole storyline. In this paper we introduce IBSEN, a director-actor coordinate agent framework that generates drama scripts and makes the plot played by agents more controllable.
The director agent writes plot outlines that the user desires to see, instructs the actor agents to role-play their characters, and reschedules the plot when human players participate in the scenario to ensure the plot is progressing towards the objective. To evaluate the framework, we create a novel drama plot that involves several actor agents and check the interactions between them under the instruction of the director agent. Evaluation results show that our framework could generate complete, diverse drama scripts from only a rough outline of plot objectives, meanwhile maintaining the characteristics of characters in the drama. Our codes and prompts are available at https://github.com/OpenDFM/ibsen.

------------

`[2407.01231] MIRAI: Evaluating LLM Agents for Event Forecasting <https://arxiv.org/abs/2407.01231>`__ MIRAI:评估用于事件预测的LLM代理

::

    Mon, 1 Jul 2024 12:22:46 GMT
    Chenchen Ye, Ziniu Hu, Yihe Deng, Zijie Huang, Mingyu Derek Ma, Yanqiao Zhu, Wei Wang

Recent advancements in Large Language Models (LLMs) have empowered LLM agents to autonomously collect world information, over which to conduct reasoning to solve complex problems. Given this capability, increasing interests have been put into employing LLM agents for predicting international events, which can influence decision-making and shape policy development on an international scale. Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents' forecasting capability and reliability. To address this gap, we introduce MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters in the context of international events. Our benchmark features an agentic environment with tools for accessing an extensive database of historical, structured events and textual news articles. We refine the GDELT event database with careful cleaning and parsing to curate a series of relational prediction tasks with varying forecasting horizons, assessing LLM agents' abilities from short-term to long-term forecasting. We further implement APIs to enable LLM agents to utilize different tools via a code-based interface. In summary, MIRAI comprehensively evaluates the agents' capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes using domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and time to accurately predict future events. Through comprehensive benchmarking, we aim to establish a reliable framework for assessing the capabilities of LLM agents in forecasting international events, thereby contributing to the development of more accurate and trustworthy models for international relation analysis.

------------

`[2407.00132] ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents <https://arxiv.org/abs/2407.00132>`__ ShortcutsBench:基于api的代理的大规模真实基准测试

::

    Fri, 28 Jun 2024 08:45:02 GMT
    Haiyang Shen, Yue Li, Desong Meng, Dongqi Cai, Sheng Qi, Li Zhang, Mengwei Xu, Yun Ma

Recent advancements in integrating large language models (LLMs) with application programming interfaces (APIs) have gained significant interest in both academia and industry. These API-based agents, leveraging the strong autonomy and planning capabilities of LLMs, can efficiently solve problems requiring multi-step actions. However, their ability to handle multi-dimensional difficulty levels, diverse task types, and real-world demands through APIs remains unknown. In this paper, we introduce \textsc{ShortcutsBench}, a large-scale benchmark for the comprehensive evaluation of API-based agents in solving tasks with varying levels of difficulty, diverse task types, and real-world demands. \textsc{ShortcutsBench} includes a wealth of real APIs from Apple Inc.'s operating systems, refined user queries from shortcuts, human-annotated high-quality action sequences from shortcut developers, and accurate parameter filling values about primitive parameter types, enum parameter types, outputs from previous actions, and parameters that need to request necessary information from the system or user.
Our extensive evaluation of agents built with $5$ leading open-source (size >= 57B) and $4$ closed-source LLMs (e.g. Gemini-1.5-Pro and GPT-3.5) reveals significant limitations in handling complex queries related to API selection, parameter filling, and requesting necessary information from systems and users.
These findings highlight the challenges that API-based agents face in effectively fulfilling real and complex user queries. All datasets, code, and experimental results will be available at \url{https://github.com/eachsheep/shortcutsbench}.

------------

`[2407.00942] ProductAgent: Benchmarking Conversational Product Search Agent with Asking Clarification Questions <https://arxiv.org/abs/2407.00942>`__ ProductAgent:询问澄清性问题的对话性产品搜索代理基准测试

::

    Mon, 1 Jul 2024 03:50:23 GMT
    Jingheng Ye, Yong Jiang, Xiaobin Wang, Yinghui Li, Yangning Li, Hai-Tao Zheng, Pengjun Xie, Fei Huang

This paper introduces the task of product demand clarification within an e-commercial scenario, where the user commences the conversation with ambiguous queries and the task-oriented agent is designed to achieve more accurate and tailored product searching by asking clarification questions. To address this task, we propose ProductAgent, a conversational information seeking agent equipped with abilities of strategic clarification question generation and dynamic product retrieval. Specifically, we develop the agent with strategies for product feature summarization, query generation, and product retrieval.
Furthermore, we propose the benchmark called PROCLARE to evaluate the agent's performance both automatically and qualitatively with the aid of a LLM-driven user simulator. Experiments show that ProductAgent interacts positively with the user and enhances retrieval performance with increasing dialogue turns, where user demands become gradually more explicit and detailed. All the source codes will be released after the review anonymity period.

------------

`[2407.01489] Agentless: Demystifying LLM-based Software Engineering Agents <https://arxiv.org/abs/2407.01489>`__ 无代理:揭秘基于llm的软件工程代理

::

    Mon, 1 Jul 2024 17:24:45 GMT
    Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, Lingming Zhang

Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build Agentless -- an agentless approach to automatically solve software development problems. Compared to the verbose and complex setup of agent-based approaches, Agentless employs a simplistic two-phase process of localization followed by repair, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic Agentless is able to achieve both the highest performance (27.33%) and lowest cost (\$0.34) compared with all existing open-source software agents! Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patch or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite-S by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the current overlooked potential of a simple, interpretable technique in autonomous software development. We hope Agentless will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction.

------------

`[2407.00632] CAMON: Cooperative Agents for Multi-Object Navigation with LLM-based Conversations <https://arxiv.org/abs/2407.00632>`__ CAMON:基于llm对话的多目标导航协作agent

::

    Sun, 30 Jun 2024 09:14:33 GMT
    Pengying Wu, Yao Mu, Kangjie Zhou, Ji Ma, Junting Chen, Chang Liu

Visual navigation tasks are critical for household service robots. As these tasks become increasingly complex, effective communication and collaboration among multiple robots become imperative to ensure successful completion. In recent years, large language models (LLMs) have exhibited remarkable comprehension and planning abilities in the context of embodied agents.
However, their application in household scenarios, specifically in the use of multiple agents collaborating to complete complex navigation tasks through communication, remains unexplored. Therefore, this paper proposes a framework for decentralized multi-agent navigation, leveraging LLM-enabled communication and collaboration. By designing the communication-triggered dynamic leadership organization structure, we achieve faster team consensus with fewer communication instances, leading to better navigation effectiveness and collaborative exploration efficiency. With the proposed novel communication scheme, our framework promises to be conflict-free and robust in multi-object navigation tasks, even when there is a surge in team size.

------------

`[2309.17382] Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency <https://arxiv.org/abs/2309.17382>`__ 

::

    replaced with revised version Mon, 24 Jun 2024 05:25:21 GMT
    Submission history From: Zhihan Liu [view email]
    [v1] Fri, 29 Sep 2023 16:36:39 UTC (4,499 KB)
    [v2] Wed, 11 Oct 2023 06:18:04 UTC (4,512 KB)
    [v3] Mon, 24 Jun 2024 05:25:21 UTC (8,721 KB)
    Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, Zhaoran Wang

Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" (\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.
The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an "in-context" manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a $\sqrt{T}$ regret. Here, $T$ denotes the number of online interactions. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks.

------------

`[2406.17962] SimsChat: A Customisable Persona-Driven Role-Playing Agent <https://arxiv.org/abs/2406.17962>`__ SimsChat:可定制的角色驱动的角色扮演代理

::

    replaced with revised version Sun, 30 Jun 2024 21:15:47 GMT
    Submission history From: Bohao Yang [view email]
    [v1] Tue, 25 Jun 2024 22:44:17 UTC (1,422 KB)
    [v2] Sun, 30 Jun 2024 21:15:47 UTC (1,422 KB)
    Bohao Yang, Dong Liu, Chen Tang, Chenghao Xiao, Kun Zhao, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, Chenghua Lin

Large Language Models (LLMs) possess the remarkable capability to understand human instructions and generate high-quality text, enabling them to act as agents that simulate human behaviours. This capability allows LLMs to emulate human beings in a more advanced manner, beyond merely replicating simple human behaviours. However, there is a lack of exploring into leveraging LLMs to craft characters from several aspects. In this work, we introduce the Customisable Conversation Agent Framework, which employs LLMs to simulate real-world characters that can be freely customised according to different user preferences. The customisable framework is helpful for designing customisable characters and role-playing agents according to human's preferences. We first propose the SimsConv dataset, which comprises 68 different customised characters, 1,360 multi-turn role-playing dialogues, and encompasses 13,971 interaction dialogues in total. The characters are created from several real-world elements, such as career, aspiration, trait, and skill. Building on these foundations, we present SimsChat, a freely customisable role-playing agent. It incorporates different real-world scenes and topic-specific character interaction dialogues, simulating characters' life experiences in various scenarios and topic-specific interactions with specific emotions. Experimental results show that our proposed framework achieves desirable performance and provides helpful guideline for building better simulacra of human beings in the future. Our data and code are available at this https URL.

------------

`[2406.10521] MALLM-GAN: Multi-Agent Large Language Model as Generative Adversarial Network for Synthesizing Tabular Data <https://arxiv.org/abs/2406.10521>`__ MALLM-GAN:用于表格数据合成的多智能体大型语言模型生成对抗网络

::

    replaced with revised version Sat, 29 Jun 2024 13:48:12 GMT
    Submission history From: Yaobin Ling [view email]
    [v1] Sat, 15 Jun 2024 06:26:17 UTC (4,509 KB)
    [v2] Sat, 29 Jun 2024 13:48:12 UTC (4,509 KB)
    Yaobin Ling, Xiaoqian Jiang, Yejin Kim

In the era of big data, access to abundant data is crucial for driving research forward. However, such data is often inaccessible due to privacy concerns or high costs, particularly in healthcare domain. Generating synthetic (tabular) data can address this, but existing models typically require substantial amounts of data to train effectively, contradicting our objective to solve data scarcity. To address this challenge, we propose a novel framework to generate synthetic tabular data, powered by large language models (LLMs) that emulates the architecture of a Generative Adversarial Network (GAN). By incorporating data generation process as contextual information and utilizing LLM as the optimizer, our approach significantly enhance the quality of synthetic data generation in common scenarios with small sample sizes. Our experimental results on public and private datasets demonstrate that our model outperforms several state-of-art models regarding generating higher quality synthetic data for downstream tasks while keeping privacy of the real data.

------------

`[2311.15649] RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks <https://arxiv.org/abs/2311.15649>`__ RoboGPT:为日常教学任务进行具身长期决策的智能体

::

    replaced with revised version Sun, 30 Jun 2024 14:28:38 GMT
    Submission history From: Yaran Chen [view email]
    [v1] Mon, 27 Nov 2023 09:20:23 UTC (11,633 KB)
    [v2] Sun, 30 Jun 2024 14:28:38 UTC (5,960 KB)
    Yaran Chen, Wenbo Cui, Yuanwen Chen, Mining Tan, Xinyao Zhang, Dongbin Zhao, He Wang

Robotic agents must master common sense and long-term sequential decisions to solve daily tasks through natural language instruction. The developments in Large Language Models (LLMs) in natural language processing have inspired efforts to use LLMs in complex robot planning. Despite LLMs' great generalization and comprehension of instruction tasks, LLMs-generated task plans sometimes lack feasibility and correctness. To address the problem, we propose a RoboGPT agent\footnote{our code and dataset will be released soon} for making embodied long-term decisions for daily tasks, with two modules: 1) LLMs-based planning with re-plan to break the task into multiple sub-goals; 2) RoboSkill individually designed for sub-goals to learn better navigation and manipulation skills. The LLMs-based planning is enhanced with a new robotic dataset and re-plan, called RoboGPT. The new robotic dataset of 67k daily instruction tasks is gathered for fine-tuning the Llama model and obtaining RoboGPT. RoboGPT planner with strong generalization can plan hundreds of daily instruction tasks. Additionally, a low-computational Re-Plan module is designed to allow plans to flexibly adapt to the environment, thereby addressing the nomenclature diversity challenge. The proposed RoboGPT agent outperforms SOTA methods on the ALFRED daily tasks. Moreover, RoboGPT planner exceeds SOTA LLM-based planners like ChatGPT in task-planning rationality for hundreds of unseen daily tasks, and even other domain tasks, while keeping the large model's original broad application and generality.

------------

`[2406.20041] BMW Agents -- A Framework For Task Automation Through Multi-Agent Collaboration <https://arxiv.org/abs/2406.20041>`__ BMW Agents——一个通过多agent协作实现任务自动化的框架

::

    replaced with revised version Mon, 1 Jul 2024 16:58:15 GMT
    Submission history From: Edward Duffy [view email]
    [v1] Fri, 28 Jun 2024 16:39:20 UTC (2,401 KB)
    [v2] Mon, 1 Jul 2024 16:58:15 UTC (2,401 KB)
    [v3] Tue, 2 Jul 2024 11:45:05 UTC (2,401 KB)
    Noel Crawford, Edward B. Duffy, Iman Evazzade, Torsten Foehr, Gregory Robbins, Debbrata Kumar Saha, Jiya Varma, Marcin Ziolkowski

Autonomous agents driven by Large Language Models (LLMs) offer enormous potential for automation. Early proof of this technology can be found in various demonstrations of agents solving complex tasks, interacting with external systems to augment their knowledge, and triggering actions. In particular, workflows involving multiple agents solving complex tasks in a collaborative fashion exemplify their capacity to operate in less strict and less well-defined environments. Thus, a multi-agent approach has great potential for serving as a backbone in many industrial applications, ranging from complex knowledge retrieval systems to next generation robotic process automation. Given the reasoning abilities within the current generation of LLMs, complex processes require a multi-step approach that includes a plan of well-defined and modular tasks. Depending on the level of complexity, these tasks can be executed either by a single agent or a group of agents. In this work, we focus on designing a flexible agent engineering framework with careful attention to planning and execution, capable of handling complex use case applications across various domains. The proposed framework provides reliability in industrial applications and presents techniques to ensure a scalable, flexible, and collaborative workflow for multiple autonomous agents working together towards solving tasks.

------------

`[2404.09134] Generative AI Agents with Large Language Model for Satellite Networks via a Mixture of Experts Transmission <https://arxiv.org/abs/2404.09134>`__ 通过混合专家传输的卫星网络具有大型语言模型的生成式AI智能体

::

    replaced with revised version Sat, 29 Jun 2024 13:41:36 GMT
    Submission history From: Ruichen Zhang [view email]
    [v1] Sun, 14 Apr 2024 03:44:54 UTC (17,797 KB)
    [v2] Sat, 29 Jun 2024 13:41:36 UTC (17,133 KB)
    Ruichen Zhang, Hongyang Du, Yinqiu Liu, Dusit Niyato, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, and Dong In Kim

In response to the needs of 6G global communications, satellite communication networks have emerged as a key solution. However, the large-scale development of satellite communication networks is constrained by the complex system models, whose modeling is challenging for massive users. Moreover, transmission interference between satellites and users seriously affects communication performance. To solve these problems, this paper develops generative artificial intelligence (AI) agents for model formulation and then applies a mixture of experts (MoE) approach to design transmission strategies. Specifically, we leverage large language models (LLMs) to build an interactive modeling paradigm and utilize retrieval-augmented generation (RAG) to extract satellite expert knowledge that supports mathematical modeling. Afterward, by integrating the expertise of multiple specialized components, we propose an MoE-proximal policy optimization (PPO) approach to solve the formulated problem. Each expert can optimize the optimization variables at which it excels through specialized training through its own network and then aggregates them through the gating network to perform joint optimization. The simulation results validate the accuracy and effectiveness of employing a generative agent for problem formulation. Furthermore, the superiority of the proposed MoE-ppo approach over other benchmarks is confirmed in solving the formulated problem. The adaptability of MoE-PPO to various customized modeling problems has also been demonstrated.

------------

-----------
Other (149)
-----------

`[2407.00075] Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference <https://arxiv.org/abs/2407.00075>`__ Logicbreaks:一种理解颠覆基于规则推理的框架

::

    Fri, 21 Jun 2024 19:18:16 GMT
    Anton Xue, Avishree Khare, Rajeev Alur, Surbhi Goel, Eric Wong

We study how to subvert language models from following the rules. We model rule-following as inference in propositional Horn logic, a mathematical system in which rules have the form "if $P$ and $Q$, then $R$" for some propositions $P$, $Q$, and $R$. We prove that although transformers can faithfully abide by such rules, maliciously crafted prompts can nevertheless mislead even theoretically constructed models. Empirically, we find that attacks on our theoretical models mirror popular attacks on large language models. Our work suggests that studying smaller theoretical models can help understand the behavior of large language models in rule-based settings like logical reasoning and jailbreak attacks.

------------

`[2407.00256] One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts <https://arxiv.org/abs/2407.00256>`__ 一个提示是不够的:自动构建多个专家提示的混合

::

    Fri, 28 Jun 2024 23:05:08 GMT
    Ruochen Wang and Sohyun An and Minhao Cheng and Tianyi Zhou and Sung Ju Hwang and Cho-Jui Hsieh

Large Language Models (LLMs) exhibit strong generalization capabilities to novel tasks when prompted with language instructions and in-context demos.
Since this ability sensitively depends on the quality of prompts, various methods have been explored to automate the instruction design. While these methods demonstrated promising results, they also restricted the searched prompt to one instruction. Such simplification significantly limits their capacity, as a single demo-free instruction might not be able to cover the entire complex problem space of the targeted task. To alleviate this issue, we adopt the Mixture-of-Expert paradigm and divide the problem space into a set of sub-regions; Each sub-region is governed by a specialized expert, equipped with both an instruction and a set of demos. A two-phase process is developed to construct the specialized expert for each region: (1) demo assignment: Inspired by the theoretical connection between in-context learning and kernel regression, we group demos into experts based on their semantic similarity; (2) instruction assignment: A region-based joint search of an instruction per expert complements the demos assigned to it, yielding a synergistic effect. The resulting method, codenamed Mixture-of-Prompts (MoP), achieves an average win rate of 81% against prior arts across several major benchmarks.

------------

`[2407.00693] BAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models <https://arxiv.org/abs/2407.00693>`__ BAPO:基于锚定偏好优化的大型语言模型个性化对齐

::

    Sun, 30 Jun 2024 13:30:04 GMT
    Gihun Lee, Minchan Jeong, Yujin Kim, Hojung Jung, Jaehoon Oh, Sangmook Kim, Se-Young Yun

While learning to align Large Language Models (LLMs) with human preferences has shown remarkable success, aligning these models to meet the diverse user preferences presents further challenges in preserving previous knowledge. This paper examines the impact of personalized preference optimization on LLMs, revealing that the extent of knowledge loss varies significantly with preference heterogeneity. Although previous approaches have utilized the KL constraint between the reference model and the policy model, we observe that they fail to maintain general knowledge and alignment when facing personalized preferences. To this end, we introduce Base-Anchored Preference Optimization (BAPO), a simple yet effective approach that utilizes the initial responses of reference model to mitigate forgetting while accommodating personalized alignment. BAPO effectively adapts to diverse user preferences while minimally affecting global knowledge or general alignment. Our experiments demonstrate the efficacy of BAPO in various setups.

------------

`[2407.00900] MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human Curricula <https://arxiv.org/abs/2407.00900>`__ 

::

    Mon, 1 Jul 2024 01:56:28 GMT
    Shubhra Mishra, Gabriel Poesia, Belinda Mo, Noah D. Goodman

Mathematical problem solving is an important skill for Large Language Models (LLMs), both as an important capability and a proxy for a range of reasoning abilities. Existing benchmarks probe a diverse set of skills, but they yield aggregate accuracy metrics, obscuring specific abilities or weaknesses.
Furthermore, they are difficult to extend with new problems, risking data contamination over time. To address these challenges, we propose MathCAMPS: a method to synthesize high-quality mathematical problems at scale, grounded on 44 fine-grained "standards" from the Mathematics Common Core (CC) Standard for K-8 grades. We encode each standard in a formal grammar, allowing us to sample diverse symbolic problems and their answers. We then use LLMs to realize the symbolic problems into word problems. We propose a cycle-consistency method for validating problem faithfulness. Finally, we derive follow-up questions from symbolic structures and convert them into follow-up word problems - a novel task of mathematical dialogue that probes for robustness in understanding.
Experiments on 23 LLMs show surprising failures even in the strongest models (in particular when asked simple follow-up questions). Moreover, we evaluate training checkpoints of Pythia 12B on MathCAMPS, allowing us to analyze when particular mathematical skills develop during its training. Our framework enables the community to reproduce and extend our pipeline for a fraction of the typical cost of building new high-quality datasets.

------------

`[2407.00958] Universal Approximation Theory: The basic theory for large language models <https://arxiv.org/abs/2407.00958>`__ 通用近似理论:大型语言模型的基础理论

::

    Mon, 1 Jul 2024 04:29:35 GMT
    Wei Wang, Qing Li

Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large-scale Transformer networks have quickly become the leading approach for advancing natural language processing algorithms. Built on the Transformer architecture, these models enable interactions that closely mimic human communication and, equipped with extensive knowledge, can even assist in guiding human tasks. Despite their impressive capabilities and growing complexity, a key question remains-the theoretical foundations of large language models (LLMs). What makes Transformer so effective for powering intelligent language applications, such as translation and coding? What underlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme enhance the fine-tuning of LLMs? And what supports the practicality of pruning LLMs? To address these critical questions and explore the technological strategies within LLMs, we leverage the Universal Approximation Theory (UAT) to offer a theoretical backdrop, shedding light on the mechanisms that underpin these advancements.

------------

`[2407.00959] Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving <https://arxiv.org/abs/2407.00959>`__ 将世界标记为对象级知识以解决自动驾驶中的长尾事件

::

    Mon, 1 Jul 2024 04:34:50 GMT
    Ran Tian, Boyi Li, Xinshuo Weng, Yuxiao Chen, Edward Schmerling, Yue Wang, Boris Ivanovic, and Marco Pavone

The autonomous driving industry is increasingly adopting end-to-end learning from sensory inputs to minimize human biases in system design. Traditional end-to-end driving models, however, suffer from long-tail events due to rare or unseen inputs within their training distributions. To address this, we propose TOKEN, a novel Multi-Modal Large Language Model (MM-LLM) that tokenizes the world into object-level knowledge, enabling better utilization of LLM's reasoning capabilities to enhance autonomous vehicle planning in long-tail scenarios. TOKEN effectively alleviates data scarcity and inefficient tokenization by leveraging a traditional end-to-end driving model to produce condensed and semantically enriched representations of the scene, which are optimized for LLM planning compatibility through deliberate representation and reasoning alignment training stages. Our results demonstrate that TOKEN excels in grounding, reasoning, and planning capabilities, outperforming existing frameworks with a 27% reduction in trajectory L2 error and a 39% decrease in collision rates in long-tail scenarios. Additionally, our work highlights the importance of representation alignment and structured reasoning in sparking the common-sense reasoning capabilities of MM-LLMs for effective planning.

------------

`[2407.01067] Human-like object concept representations emerge naturally in multimodal large language models <https://arxiv.org/abs/2407.01067>`__ 类人物体概念表示在多模态大型语言模型中自然出现

::

    Mon, 1 Jul 2024 08:17:19 GMT
    Changde Du, Kaicheng Fu, Bincheng Wen, Yi Sun, Jie Peng, Wei Wei, Ying Gao, Shengpei Wang, Chuncheng Zhang, Jinpeng Li, Shuang Qiu, Le Chang, and Huiguang He

The conceptualization and categorization of natural objects in the human mind have long intrigued cognitive scientists and neuroscientists, offering crucial insights into human perception and cognition. Recently, the rapid development of Large Language Models (LLMs) has raised the attractive question of whether these models can also develop human-like object representations through exposure to vast amounts of linguistic and multimodal data. In this study, we combined behavioral and neuroimaging analysis methods to uncover how the object concept representations in LLMs correlate with those of humans. By collecting large-scale datasets of 4.7 million triplet judgments from LLM and Multimodal LLM (MLLM), we were able to derive low-dimensional embeddings that capture the underlying similarity structure of 1,854 natural objects. The resulting 66-dimensional embeddings were found to be highly stable and predictive, and exhibited semantic clustering akin to human mental representations.
Interestingly, the interpretability of the dimensions underlying these embeddings suggests that LLM and MLLM have developed human-like conceptual representations of natural objects. Further analysis demonstrated strong alignment between the identified model embeddings and neural activity patterns in many functionally defined brain ROIs (e.g., EBA, PPA, RSC and FFA). This provides compelling evidence that the object representations in LLMs, while not identical to those in the human, share fundamental commonalities that reflect key schemas of human conceptual knowledge. This study advances our understanding of machine intelligence and informs the development of more human-like artificial cognitive systems.

------------

`[2407.01238] Large Language Models are Zero-Shot Recognizers for Activities of Daily Living <https://arxiv.org/abs/2407.01238>`__ 

::

    Mon, 1 Jul 2024 12:32:38 GMT
    Gabriele Civitarese, Michele Fiori, Priyankar Choudhary, Claudio Bettini

The sensor-based recognition of Activities of Daily Living (ADLs) in smart home environments enables several applications in the areas of energy management, safety, well-being, and healthcare. ADLs recognition is typically based on deep learning methods requiring large datasets to be trained.
Recently, several studies proved that Large Language Models (LLMs) effectively capture common-sense knowledge about human activities. However, the effectiveness of LLMs for ADLs recognition in smart home environments still deserves to be investigated. In this work, we propose ADL-LLM, a novel LLM-based ADLs recognition system. ADLLLM transforms raw sensor data into textual representations, that are processed by an LLM to perform zero-shot ADLs recognition. Moreover, in the scenario where a small labeled dataset is available, ADL-LLM can also be empowered with few-shot prompting. We evaluated ADL-LLM on two public datasets, showing its effectiveness in this domain.

------------

`[2407.01245] SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model <https://arxiv.org/abs/2407.01245>`__ SINKT:基于大型语言模型的结构感知归纳知识追踪模型

::

    Mon, 1 Jul 2024 12:44:52 GMT
    Lingyue Fu, Hao Guan, Kounianhua Du, Jianghao Lin, Wei Xia, Weinan Zhang, Ruiming Tang, Yasheng Wang and Yong Yu

Knowledge Tracing (KT) aims to determine whether students will respond correctly to the next question, which is a crucial task in intelligent tutoring systems (ITS). In educational KT scenarios, transductive ID-based methods often face severe data sparsity and cold start problems, where interactions between individual students and questions are sparse, and new questions and concepts consistently arrive in the database. In addition, existing KT models only implicitly consider the correlation between concepts and questions, lacking direct modeling of the more complex relationships in the heterogeneous graph of concepts and questions. In this paper, we propose a Structure-aware Inductive Knowledge Tracing model with large language model (dubbed SINKT), which, for the first time, introduces large language models (LLMs) and realizes inductive knowledge tracing. Firstly, SINKT utilizes LLMs to introduce structural relationships between concepts and constructs a heterogeneous graph for concepts and questions. Secondly, by encoding concepts and questions with LLMs, SINKT incorporates semantic information to aid prediction. Finally, SINKT predicts the student's response to the target question by interacting with the student's knowledge state and the question representation. Experiments on four real-world datasets demonstrate that SINKT achieves state-of-the-art performance among 12 existing transductive KT models. Additionally, we explore the performance of SINKT on the inductive KT task and provide insights into various modules.

------------

`[2407.00191] MetaKP: On-Demand Keyphrase Generation <https://arxiv.org/abs/2407.00191>`__ MetaKP:按需关键字生成

::

    Fri, 28 Jun 2024 19:02:59 GMT
    Di Wu, Xiaoxian Shen, Kai-Wei Chang

Traditional keyphrase prediction methods predict a single set of keyphrases per document, failing to cater to the diverse needs of users and downstream applications. To bridge the gap, we introduce on-demand keyphrase generation, a novel paradigm that requires keyphrases that conform to specific high-level goals or intents. For this task, we present MetaKP, a large-scale benchmark comprising four datasets, 7500 documents, and 3760 goals across news and biomedical domains with human-annotated keyphrases. Leveraging MetaKP, we design both supervised and unsupervised methods, including a multi-task fine-tuning approach and a self-consistency prompting method with large language models. The results highlight the challenges of supervised fine-tuning, whose performance is not robust to distribution shifts. By contrast, the proposed self-consistency prompting approach greatly improves the performance of large language models, enabling GPT-4o to achieve 0.548 SemF1, surpassing the performance of a fully fine-tuned BART-base model. Finally, we demonstrate the potential of our method to serve as a general NLP infrastructure, exemplified by its application in epidemic event detection from social media.

------------

`[2407.00211] Detection and Measurement of Syntactic Templates in Generated Text <https://arxiv.org/abs/2407.00211>`__ 生成文本中句法模板的检测与度量

::

    Fri, 28 Jun 2024 19:34:23 GMT
    Chantal Shaib, Yanai Elazar, Junyi Jessy Li, Byron C. Wallace

Recent work on evaluating the diversity of text generated by LLMs has focused on word-level features. Here we offer an analysis of syntactic features to characterize general repetition in models, beyond frequent n-grams.
Specifically, we define syntactic templates and show that models tend to produce templated text in downstream tasks at a higher rate than what is found in human-reference texts. We find that most (76%) templates in model-generated text can be found in pre-training data (compared to only 35% of human-authored text), and are not overwritten during fine-tuning processes such as RLHF. This connection to the pre-training data allows us to analyze syntactic templates in models where we do not have the pre-training data. We also find that templates as features are able to differentiate between models, tasks, and domains, and are useful for qualitatively evaluating common model constructions. Finally, we demonstrate the use of templates as a useful tool for analyzing style memorization of training data in LLMs.

------------

`[2407.00219] Evaluating Human Alignment and Model Faithfulness of LLM Rationale <https://arxiv.org/abs/2407.00219>`__ 评估LLM理论的人类对齐和模型忠实度

::

    Fri, 28 Jun 2024 20:06:30 GMT
    Mohsen Fayyaz, Fan Yin, Jiao Sun, Nanyun Peng

We study how well large language models (LLMs) explain their generations with rationales -- a set of tokens extracted from the input texts that reflect the decision process of LLMs. We examine LLM rationales extracted with two methods: 1) attribution-based methods that use attention or gradients to locate important tokens, and 2) prompting-based methods that guide LLMs to extract rationales using prompts. Through extensive experiments, we show that prompting-based rationales align better with human-annotated rationales than attribution-based rationales, and demonstrate reasonable alignment with humans even when model performance is poor. We additionally find that the faithfulness limitations of prompting-based methods, which are identified in previous work, may be linked to their collapsed predictions. By fine-tuning these models on the corresponding datasets, both prompting and attribution methods demonstrate improved faithfulness. Our study sheds light on more rigorous and fair evaluations of LLM rationales, especially for prompting-based ones.

------------

`[2407.00242] EHRmonize: A Framework for Medical Concept Abstraction from Electronic Health Records using Large Language Models <https://arxiv.org/abs/2407.00242>`__ EHRmonize:基于大型语言模型从电子健康记录中抽象医疗概念的框架

::

    Fri, 28 Jun 2024 21:39:20 GMT
    Jo\~ao Matos, Jack Gallifant, Jian Pei, A. Ian Wong

Electronic health records (EHRs) contain vast amounts of complex data, but harmonizing and processing this information remains a challenging and costly task requiring significant clinical expertise. While large language models (LLMs) have shown promise in various healthcare applications, their potential for abstracting medical concepts from EHRs remains largely unexplored. We introduce EHRmonize, a framework leveraging LLMs to abstract medical concepts from EHR data. Our study uses medication data from two real-world EHR databases to evaluate five LLMs on two free-text extraction and six binary classification tasks across various prompting strategies. GPT-4o's with 10-shot prompting achieved the highest performance in all tasks, accompanied by Claude-3.5-Sonnet in a subset of tasks. GPT-4o achieved an accuracy of 97% in identifying generic route names, 82% for generic drug names, and 100% in performing binary classification of antibiotics. While EHRmonize significantly enhances efficiency, reducing annotation time by an estimated 60%, we emphasize that clinician oversight remains essential. Our framework, available as a Python package, offers a promising tool to assist clinicians in EHR data abstraction, potentially accelerating healthcare research and improving data harmonization processes.

------------

`[2407.00320] LiteSearch: Efficacious Tree Search for LLM <https://arxiv.org/abs/2407.00320>`__ LiteSearch:有效的LLM树搜索

::

    Sat, 29 Jun 2024 05:14:04 GMT
    Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Dian Yu, Haitao Mi, Jinsong Su and Dong Yu

Recent research suggests that tree search algorithms (e.g. Monte Carlo Tree Search) can dramatically boost LLM performance on complex mathematical reasoning tasks. However, they often require more than 10 times the computational resources of greedy decoding due to wasteful search strategies, making them difficult to be deployed in practical applications. This study introduces a novel guided tree search algorithm with dynamic node selection and node-level exploration budget (maximum number of children) calculation to tackle this issue. By considering the search progress towards the final answer (history) and the guidance from a value network (future) trained without any step-wise annotations, our algorithm iteratively selects the most promising tree node before expanding it within the boundaries of the allocated computational budget. Experiments conducted on the GSM8K and TabMWP datasets demonstrate that our approach not only offers competitive performance but also enjoys significantly lower computational costs compared to baseline methods.

------------

`[2407.00322] LLM-Generated Natural Language Meets Scaling Laws: New Explorations and Data Augmentation Methods <https://arxiv.org/abs/2407.00322>`__ 

::

    Sat, 29 Jun 2024 05:40:17 GMT
    Zhenhua Wang, Guang Xu, Ming Ren

With the ascent of large language models (LLM), natural language processing has witnessed enhancements, such as LLM-based data augmentation. Nonetheless, prior research harbors two primary concerns: firstly, a lack of contemplation regarding whether the natural language generated by LLM (LLMNL) truly aligns with human natural language (HNL), a critical foundational question; secondly, an oversight that augmented data is randomly generated by LLM, implying that not all data may possess equal training value, that could impede the performance of classifiers. To address these challenges, we introduce the scaling laws to intrinsically calculate LLMNL and HNL. Through extensive experiments, we reveal slight deviations (approximately 0.2 Mandelbrot exponent) from Mandelbrot's law in LLMNL, underscore a complexity advantage in HNL, and supplement an interpretive discussion on language style. This establishes a solid foundation for LLM's expansion. Further, we introduce a novel data augmentation method for few-shot text classification, termed ZGPTDA, which leverages fuzzy computing mechanisms driven by the conformity to scaling laws to make decisions about GPT-4 augmented data. Extensive experiments, conducted in real-world scenarios, confirms the effectiveness (improving F1 of Bert and RoBerta by 7-10%) and competitiveness (surpassing recent AugGPT and GENCO methods by about 2% accuracy on DeBerta) of ZGPTDA. In addition, we reveal some interesting insights, e.g., Hilberg's law and Taylor's law can impart more benefits to text classification, etc.

------------

`[2407.00341] Iterative Data Augmentation with Large Language Models for Aspect-based Sentiment Analysis <https://arxiv.org/abs/2407.00341>`__ 基于大型语言模型迭代数据增强的方面级情感分析

::

    Sat, 29 Jun 2024 07:00:37 GMT
    Haiyun Li, Qihuang Zhong, Ke Zhu, Juhua Liu, Bo Du, Dacheng Tao

Aspect-based Sentiment Analysis (ABSA) is an important sentiment analysis task, which aims to determine the sentiment polarity towards an aspect in a sentence. Due to the expensive and limited labeled data, data augmentation (DA) has become the standard for improving the performance of ABSA. However, current DA methods usually have some shortcomings: 1) poor fluency and coherence, 2) lack of diversity of generated data, and 3) reliance on some existing labeled data, hindering its applications in real-world scenarios. In response to these problems, we propose a systematic Iterative Data augmentation framework, namely IterD, to boost the performance of ABSA. The core of IterD is to leverage the powerful ability of large language models (LLMs) to iteratively generate more fluent and diverse synthetic labeled data, starting from an unsupervised sentence corpus. Extensive experiments on 4 widely-used ABSA benchmarks show that IterD brings consistent and significant performance gains among 5 baseline ABSA models. More encouragingly, the synthetic data generated by IterD can achieve comparable or even better performance against the manually annotated data.

------------

`[2407.00365] Financial Knowledge Large Language Model <https://arxiv.org/abs/2407.00365>`__ 金融知识大语言模型

::

    Sat, 29 Jun 2024 08:26:49 GMT
    Cehao Yang, Chengjin Xu, Yiyan Qi

Artificial intelligence is making significant strides in the finance industry, revolutionizing how data is processed and interpreted. Among these technologies, large language models (LLMs) have demonstrated substantial potential to transform financial services by automating complex tasks, enhancing customer service, and providing detailed financial analysis. Firstly, we introduce IDEA-FinBench, an evaluation benchmark specifically tailored for assessing financial knowledge in large language models (LLMs). This benchmark utilizes questions from two globally respected and authoritative financial professional exams, aimimg to comprehensively evaluate the capability of LLMs to directly address exam questions pertinent to the finance sector. Secondly, we propose IDEA-FinKER, a Financial Knowledge Enhancement framework designed to facilitate the rapid adaptation of general LLMs to the financial domain, introducing a retrieval-based few-shot learning method for real-time context-level knowledge injection, and a set of high-quality financial knowledge instructions for fine-tuning any general LLM. Finally, we present IDEA-FinQA, a financial question-answering system powered by LLMs. This system is structured around a scheme of real-time knowledge injection and factual enhancement using external knowledge. IDEA-FinQA is comprised of three main modules: the data collector, the data querying module, and LLM-based agents tasked with specific functions.

------------

`[2407.00369] How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models <https://arxiv.org/abs/2407.00369>`__ 如何训练你的事实验证者:多模态开放模型的知识迁移

::

    Sat, 29 Jun 2024 08:39:07 GMT
    Jaeyoung Lee, Ximing Lu, Jack Hessel, Faeze Brahman, Youngjae Yu, Yonatan Bisk, Yejin Choi, Saadia Gabriel

Given the growing influx of misinformation across news and social media, there is a critical need for systems that can provide effective real-time verification of news claims. Large language or multimodal model based verification has been proposed to scale up online policing mechanisms for mitigating spread of false and harmful content. While these can potentially reduce burden on human fact-checkers, such efforts may be hampered by foundation model training data becoming outdated. In this work, we test the limits of improving foundation model performance without continual updating through an initial study of knowledge transfer using either existing intra- and inter- domain benchmarks or explanations generated from large language models (LLMs). We evaluate on 12 public benchmarks for fact-checking and misinformation detection as well as two other tasks relevant to content moderation -- toxicity and stance detection. Our results on two recent multi-modal fact-checking benchmarks, Mocheg and Fakeddit, indicate that knowledge transfer strategies can improve Fakeddit performance over the state-of-the-art by up to 1.7% and Mocheg performance by up to 2.9%.

------------

`[2407.00390] Advancing Process Verification for Large Language Models via Tree-Based Preference Learning <https://arxiv.org/abs/2407.00390>`__ 

::

    Sat, 29 Jun 2024 10:09:49 GMT
    Mingqian He, Yongliang Shen, Wenqi Zhang, Zeqi Tan, Weiming Lu

Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales.Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% to 82.79%), MATH (17.00% to 26.80%), CSQA (68.14% to 72.97%), and StrategyQA (82.86% to 83.25%).Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.

------------

`[2407.00396] A Study on Effect of Reference Knowledge Choice in Generating Technical Content Relevant to SAPPhIRE Model Using Large Language Model <https://arxiv.org/abs/2407.00396>`__ 基于大型语言模型的蓝宝石技术内容生成中参考知识选择的影响研究

::

    Sat, 29 Jun 2024 10:46:01 GMT
    Kausik Bhattacharya, Anubhab Majumder, Amaresh Chakrabarti

Representation of systems using the SAPPhIRE model of causality can be an inspirational stimulus in design. However, creating a SAPPhIRE model of a technical or a natural system requires sourcing technical knowledge from multiple technical documents regarding how the system works. This research investigates how to generate technical content accurately relevant to the SAPPhIRE model of causality using a Large Language Model, also called LLM. This paper, which is the first part of the two-part research, presents a method for hallucination suppression using Retrieval Augmented Generating with LLM to generate technical content supported by the scientific information relevant to a SAPPhIRE con-struct. The result from this research shows that the selection of reference knowledge used in providing context to the LLM for generating the technical content is very important. The outcome of this research is used to build a software support tool to generate the SAPPhIRE model of a given technical system.

------------

`[2407.00416] Too Late to Train, Too Early To Use? A Study on Necessity and Viability of Low-Resource Bengali LLMs <https://arxiv.org/abs/2407.00416>`__ 训练太晚，使用太早?低资源孟加拉LLMs的必要性和可行性研究

::

    Sat, 29 Jun 2024 11:50:16 GMT
    Tamzeed Mahfuz, Satak Kumar Dey, Ruwad Naswan, Hasnaen Adil, Khondker Salman Sayeed, Haz Sameen Shahgir

Each new generation of English-oriented Large Language Models (LLMs) exhibits enhanced cross-lingual transfer capabilities and significantly outperforms older LLMs on low-resource languages. This prompts the question: Is there a need for LLMs dedicated to a particular low-resource language? We aim to explore this question for Bengali, a low-to-moderate resource Indo-Aryan language native to the Bengal region of South Asia.
We compare the performance of open-weight and closed-source LLMs such as LLaMA-3 and GPT-4 against fine-tuned encoder-decoder models across a diverse set of Bengali downstream tasks, including translation, summarization, paraphrasing, question-answering, and natural language inference. Our findings reveal that while LLMs generally excel in reasoning tasks, their performance in tasks requiring Bengali script generation is inconsistent. Key challenges include inefficient tokenization of Bengali script by existing LLMs, leading to increased computational costs and potential performance degradation.
Additionally, we highlight biases in machine-translated datasets commonly used for Bengali NLP tasks. We conclude that there is a significant need for a Bengali-oriented LLM, but the field currently lacks the high-quality pretraining and instruction-tuning datasets necessary to develop a highly effective model.

------------

`[2407.00434] Brevity is the soul of wit: Pruning long files for code generation <https://arxiv.org/abs/2407.00434>`__ 简洁是智慧的灵魂:修剪长文件以生成代码

::

    Sat, 29 Jun 2024 13:08:24 GMT
    Aaditya K. Singh, Yu Yang, Kushal Tirumala, Mostafa Elhoushi, Ari S. Morcos

Data curation is commonly considered a "secret-sauce" for LLM training, with higher quality data usually leading to better LLM performance. Given the scale of internet-scraped corpora, data pruning has become a larger and larger focus.
Specifically, many have shown that de-duplicating data, or sub-selecting higher quality data, can lead to efficiency or performance improvements. Generally, three types of methods are used to filter internet-scale corpora: embedding-based, heuristic-based, and classifier-based. In this work, we contrast the former two in the domain of finetuning LLMs for code generation.
We find that embedding-based methods are often confounded by length, and that a simple heuristic--pruning long files--outperforms other methods in compute-limited regimes. Our method can yield up to a 2x efficiency benefit in training (while matching performance) or a 3.5% absolute performance improvement on HumanEval (while matching compute). However, we find that perplexity on held-out long files can increase, begging the question of whether optimizing data mixtures for common coding benchmarks (HumanEval, MBPP) actually best serves downstream use cases. Overall, we hope our work builds useful intuitions about code data (specifically, the low quality of extremely long code files) provides a compelling heuristic-based method for data pruning, and brings to light questions in how we evaluate code generation models.

------------

`[2407.00436] A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models <https://arxiv.org/abs/2407.00436>`__ 面向多语言大型语言模型的平行语料库开发方法

::

    Sat, 29 Jun 2024 13:12:39 GMT
    Peiqin Lin, Andr\'e F. T. Martins, Hinrich Sch\"utze

Recent studies have highlighted the potential of exploiting parallel corpora to enhance multilingual large language models, improving performance in both bilingual tasks, e.g., machine translation, and general-purpose tasks, e.g., text classification. Building upon these findings, our comprehensive study aims to identify the most effective strategies for leveraging parallel corpora. We investigate the impact of parallel corpora quality and quantity, training objectives, and model size on the performance of multilingual large language models enhanced with parallel corpora across diverse languages and tasks. Our analysis reveals several key insights: (i) filtering noisy translations is essential for effectively exploiting parallel corpora, while language identification and short sentence filtering have little effect; (ii) even a corpus containing just 10K parallel sentences can yield results comparable to those obtained from much larger datasets; (iii) employing only the machine translation objective yields the best results among various training objectives and their combinations; (iv) larger multilingual language models benefit more from parallel corpora than smaller models due to their stronger capacity for cross-task transfer. Our study offers valuable insights into the optimal utilization of parallel corpora to enhance multilingual large language models, extending the generalizability of previous findings from limited languages and tasks to a broader range of scenarios.

------------

`[2407.00454] Self-Translate-Train: A Simple but Strong Baseline for Cross-lingual Transfer of Large Language Models <https://arxiv.org/abs/2407.00454>`__ 自翻译-训练:大型语言模型跨语言迁移的简单但强大的基线

::

    Sat, 29 Jun 2024 14:40:23 GMT
    Ryokan Ri, Shun Kiyono, Sho Takase

Cross-lingual transfer is a promising technique for utilizing data in a source language to improve performance in a target language. However, current techniques often require an external translation system or suffer from suboptimal performance due to over-reliance on cross-lingual generalization of multi-lingual pretrained language models. In this study, we propose a simple yet effective method called Self-Translate-Train. It leverages the translation capability of a large language model to generate synthetic training data in the target language and fine-tunes the model with its own generated data. We evaluate the proposed method on a wide range of tasks and show substantial performance gains across several non-English languages.

------------

`[2407.00476] Large Language Models for Power Scheduling: A User-Centric Approach <https://arxiv.org/abs/2407.00476>`__ 电力调度的大型语言模型:以用户为中心的方法

::

    Sat, 29 Jun 2024 15:47:28 GMT
    Thomas Mongaillard, Samson Lasaulce, Othman Hicheur, Chao Zhang, Lina Bariah, Vineeth S. Varma, Hang Zou, Qiyang Zhao, Merouane Debbah

While traditional optimization and scheduling schemes are designed to meet fixed, predefined system requirements, future systems are moving toward user-driven approaches and personalized services, aiming to achieve high quality-of-experience (QoE) and flexibility. This challenge is particularly pronounced in wireless and digitalized energy networks, where users' requirements have largely not been taken into consideration due to the lack of a common language between users and machines. The emergence of powerful large language models (LLMs) marks a radical departure from traditional system-centric methods into more advanced user-centric approaches by providing a natural communication interface between users and devices. In this paper, for the first time, we introduce a novel architecture for resource scheduling problems by constructing three LLM agents to convert an arbitrary user's voice request (VRQ) into a resource allocation vector. Specifically, we design an LLM intent recognition agent to translate the request into an optimization problem (OP), an LLM OP parameter identification agent, and an LLM OP solving agent. To evaluate system performance, we construct a database of typical VRQs in the context of electric vehicle (EV) charging. As a proof of concept, we primarily use Llama 3 8B. Through testing with different prompt engineering scenarios, the obtained results demonstrate the efficiency of the proposed architecture.
The conducted performance analysis allows key insights to be extracted. For instance, having a larger set of candidate OPs to model the real-world problem might degrade the final performance because of a higher recognition/OP classification noise level. All results and codes are open source.

------------

`[2407.00487] It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization <https://arxiv.org/abs/2407.00487>`__ 

::

    Sat, 29 Jun 2024 16:34:23 GMT
    Bingdong Li, Zixiang Di, Yanting Yang, Hong Qian, Peng Yang, Hao Hao, Ke Tang, Aimin Zhou

In this paper, we introduce a novel approach for large language model merging via black-box multi-objective optimization algorithms. The goal of model merging is to combine multiple models, each excelling in different tasks, into a single model that outperforms any of the individual source models. However, model merging faces two significant challenges: First, existing methods rely heavily on human intuition and customized strategies. Second, parameter conflicts often arise during merging, and while methods like DARE [1] can alleviate this issue, they tend to stochastically drop parameters, risking the loss of important delta parameters. To address these challenges, we propose the MM-MO method, which automates the search for optimal merging configurations using multi-objective optimization algorithms, eliminating the need for human intuition. During the configuration searching process, we use estimated performance across multiple diverse tasks as optimization objectives in order to alleviate the parameter conflicting between different source models without losing crucial delta parameters. We conducted comparative experiments with other mainstream model merging methods, demonstrating that our method consistently outperforms them. Moreover, our experiments reveal that even task types not explicitly targeted as optimization objectives show performance improvements, indicating that our method enhances the overall potential of the model rather than merely overfitting to specific task types. This approach provides a significant advancement in model merging techniques, offering a robust and plug-and-play solution for integrating diverse models into a unified, high-performing model.

------------

`[2407.00488] PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models <https://arxiv.org/abs/2407.00488>`__ PFME:用于细粒度幻觉检测和大型语言模型编辑的模块化方法

::

    Sat, 29 Jun 2024 16:35:57 GMT
    Kunquan Deng, Zeyu Huang, Chen Li, Chenghua Lin, Min Gao, Wenge Rong

Large Language Models (LLMs) excel in fluency but risk producing inaccurate content, called "hallucinations." This paper outlines a standardized process for categorizing fine-grained hallucination types and proposes an innovative framework--the Progressive Fine-grained Model Editor (PFME)--specifically designed to detect and correct fine-grained hallucinations in LLMs. PFME consists of two collaborative modules: the Real-time Fact Retrieval Module and the Fine-grained Hallucination Detection and Editing Module. The former identifies key entities in the document and retrieves the latest factual evidence from credible sources. The latter further segments the document into sentence-level text and, based on relevant evidence and previously edited context, identifies, locates, and edits each sentence's hallucination type.
Experimental results on FavaBench and FActScore demonstrate that PFME outperforms existing methods in fine-grained hallucination detection tasks.
Particularly, when using the Llama3-8B-Instruct model, PFME's performance in fine-grained hallucination detection with external knowledge assistance improves by 8.7 percentage points (pp) compared to ChatGPT. In editing tasks, PFME further enhances the FActScore of FActScore-Alpaca13B and FActScore-ChatGPT datasets, increasing by 16.2pp and 4.6pp, respectively.

------------

`[2407.00497] LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement <https://arxiv.org/abs/2407.00497>`__ llms -as- instructor:从错误中学习以自动化模型改进

::

    Sat, 29 Jun 2024 17:16:04 GMT
    Jiahao Ying, Mingbao Lin, Yixin Cao, Wei Tang, Bo Wang, Qianru Sun, Xuanjing Huang, Shuicheng Yan

This paper introduces the innovative "LLMs-as-Instructors" framework, which leverages the advanced Large Language Models (LLMs) to autonomously enhance the training of smaller target models. Inspired by the theory of "Learning from Errors", this framework employs an instructor LLM to meticulously analyze the specific errors within a target model, facilitating targeted and efficient training cycles. Within this framework, we implement two strategies: "Learning from Error," which focuses solely on incorrect responses to tailor training data, and "Learning from Error by Contrast", which uses contrastive learning to analyze both correct and incorrect responses for a deeper understanding of errors.
Our empirical studies, conducted with several open-source models, demonstrate significant improvements across multiple benchmarks, including mathematical reasoning, coding abilities, and factual knowledge. Notably, the refined Llama-3-8b-Instruction has outperformed ChatGPT, illustrating the effectiveness of our approach. By leveraging the strengths of both strategies, we have attained a more balanced performance improvement on both in-domain and out-of-domain benchmarks. Our code can be found at https://yingjiahao14.github.io/LLMs-as-Instructors-pages/.

------------

`[2407.00541] Answering real-world clinical questions using large language model based systems <https://arxiv.org/abs/2407.00541>`__ 用基于大型语言模型的系统回答现实世界的临床问题

::

    Sat, 29 Jun 2024 22:39:20 GMT
    Yen Sia Low (1), Michael L. Jackson (1), Rebecca J. Hyde (1), Robert E. Brown (1), Neil M. Sanghavi (1), Julian D. Baldwin (1), C. William Pike (1), Jananee Muralidharan (1), Gavin Hui (1 and 2), Natasha Alexander (3), Hadeel Hassan (3), Rahul V. Nene (4), Morgan Pike (5), Courtney J. Pokrzywa (6), Shivam Vedak (7), Adam Paul Yan (3), Dong-han Yao (7), Amy R. Zipursky (3), Christina Dinh (1), Philip Ballentine (1), Dan C. Derieg (1), Vladimir Polony (1), Rehan N. Chawdry (1), Jordan Davies (1), Brigham B. Hyde (1), Nigam H. Shah (1 and 7), Saurabh Gombar (1 and 8) ((1) Atropos Health, New York NY, USA, (2) Department of Medicine, University of California, Los Angeles CA, USA, (3) Department of Pediatrics, The Hospital for Sick Children, Toronto ON, Canada, (4) Department of Emergency Medicine, University of California, San Diego CA, USA, (5) Department of Emergency Medicine, University of Michigan, Ann Arbor MI, USA, (6) Department of Surgery, Columbia University, New York NY, USA, (7) Center for Biomedical Informatics Research, Stanford University, Stanford CA, USA (8) Department of Pathology, Stanford University, Stanford CA, USA)

Evidence to guide healthcare decisions is often limited by a lack of relevant and trustworthy literature as well as difficulty in contextualizing existing research for a specific patient. Large language models (LLMs) could potentially address both challenges by either summarizing published literature or generating new studies based on real-world data (RWD). We evaluated the ability of five LLM-based systems in answering 50 clinical questions and had nine independent physicians review the responses for relevance, reliability, and actionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus, Gemini Pro 1.5) rarely produced answers that were deemed relevant and evidence-based (2% - 10%). In contrast, retrieval augmented generation (RAG)-based and agentic LLM systems produced relevant and evidence-based answers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. Only the agentic ChatRWD was able to answer novel questions compared to other LLMs (65% vs.
0-9%). These results suggest that while general-purpose LLMs should not be used as-is, a purpose-built system for evidence summarization based on RAG and one for generating novel evidence working synergistically would improve availability of pertinent evidence for patient care.

------------

`[2407.00702] Scaling Technology Acceptance Analysis with Large Language Model (LLM) Annotation Systems <https://arxiv.org/abs/2407.00702>`__ 用大型语言模型(LLM)标注系统扩展技术接受度分析

::

    Sun, 30 Jun 2024 14:01:06 GMT
    Pawel Robert Smolinski, Joseph Januszewicz, Jacek Winiarski

Technology acceptance models effectively predict how users will adopt new technology products. Traditional surveys, often expensive and cumbersome, are commonly used for this assessment. As an alternative to surveys, we explore the use of large language models for annotating online user-generated content, like digital reviews and comments. Our research involved designing an LLM annotation system that transform reviews into structured data based on the Unified Theory of Acceptance and Use of Technology model. We conducted two studies to validate the consistency and accuracy of the annotations. Results showed moderate-to-strong consistency of LLM annotation systems, improving further by lowering the model temperature. LLM annotations achieved close agreement with human expert annotations and outperformed the agreement between experts for UTAUT variables. These results suggest that LLMs can be an effective tool for analyzing user sentiment, offering a practical alternative to traditional survey methods and enabling deeper insights into technology design and adoption.

------------

`[2407.00731] Large Language Models Struggle in Token-Level Clinical Named Entity Recognition <https://arxiv.org/abs/2407.00731>`__ 

::

    Sun, 30 Jun 2024 15:38:48 GMT
    Qiuhao Lu, Rui Li, Andrew Wen, Jinlian Wang, Liwei Wang, Hongfang Liu

Large Language Models (LLMs) have revolutionized various sectors, including healthcare where they are employed in diverse applications. Their utility is particularly significant in the context of rare diseases, where data scarcity, complexity, and specificity pose considerable challenges. In the clinical domain, Named Entity Recognition (NER) stands out as an essential task and it plays a crucial role in extracting relevant information from clinical texts.
Despite the promise of LLMs, current research mostly concentrates on document-level NER, identifying entities in a more general context across entire documents, without extracting their precise location. Additionally, efforts have been directed towards adapting ChatGPT for token-level NER.
However, there is a significant research gap when it comes to employing token-level NER for clinical texts, especially with the use of local open-source LLMs. This study aims to bridge this gap by investigating the effectiveness of both proprietary and local LLMs in token-level clinical NER.
Essentially, we delve into the capabilities of these models through a series of experiments involving zero-shot prompting, few-shot prompting, retrieval-augmented generation (RAG), and instruction-fine-tuning. Our exploration reveals the inherent challenges LLMs face in token-level NER, particularly in the context of rare diseases, and suggests possible improvements for their application in healthcare. This research contributes to narrowing a significant gap in healthcare informatics and offers insights that could lead to a more refined application of LLMs in the healthcare sector.

------------

`[2407.00747] A Comparative Study of Quality Evaluation Methods for Text Summarization <https://arxiv.org/abs/2407.00747>`__ 文本摘要质量评价方法比较研究

::

    Sun, 30 Jun 2024 16:12:37 GMT
    Huyen Nguyen, Haihua Chen, Lavanya Pobbathi, Junhua Ding

Evaluating text summarization has been a challenging task in natural language processing (NLP). Automatic metrics which heavily rely on reference summaries are not suitable in many situations, while human evaluation is time-consuming and labor-intensive. To bridge this gap, this paper proposes a novel method based on large language models (LLMs) for evaluating text summarization. We also conducts a comparative study on eight automatic metrics, human evaluation, and our proposed LLM-based method. Seven different types of state-of-the-art (SOTA) summarization models were evaluated. We perform extensive experiments and analysis on datasets with patent documents. Our results show that LLMs evaluation aligns closely with human evaluation, while widely-used automatic metrics such as ROUGE-2, BERTScore, and SummaC do not and also lack consistency. Based on the empirical comparison, we propose a LLM-powered framework for automatically evaluating and improving text summarization, which is beneficial and could attract wide attention among the community.

------------

`[2407.00869] Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks <https://arxiv.org/abs/2407.00869>`__ 大型语言模型是无意识地说实话者:利用越狱攻击的谬误失败

::

    Mon, 1 Jul 2024 00:23:43 GMT
    Yue Zhou, Henry Peng Zou, Barbara Di Eugenio, Yang Zhang

We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior.
Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.

------------

`[2407.00870] Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles <https://arxiv.org/abs/2407.00870>`__ role - play-doh:使领域专家通过诱导和坚持原则创建llm模拟患者

::

    Mon, 1 Jul 2024 00:43:02 GMT
    Ryan Louie (1), Ananjan Nandi (1), William Fang (1), Cheng Chang (1), Emma Brunskill (1), Diyi Yang (1) ((1) Stanford University)

Recent works leverage LLMs to roleplay realistic social scenarios, aiding novices in practicing their social skills. However, simulating sensitive interactions, such as in mental health, is challenging. Privacy concerns restrict data access, and collecting expert feedback, although vital, is laborious. To address this, we develop Roleplay-doh, a novel human-LLM collaboration pipeline that elicits qualitative feedback from a domain-expert, which is transformed into a set of principles, or natural language rules, that govern an LLM-prompted roleplay. We apply this pipeline to enable senior mental health supporters to create customized AI patients for simulated practice partners for novice counselors. After uncovering issues in GPT-4 simulations not adhering to expert-defined principles, we also introduce a novel principle-adherence prompting pipeline which shows 30\% improvements in response quality and principle following for the downstream task. Via a user study with 25 counseling experts, we demonstrate that the pipeline makes it easy and effective to create AI patients that more faithfully resemble real patients, as judged by creators and third-party counselors.

------------

`[2407.00875] MoE-CT: A Novel Approach For Large Language Models Training With Resistance To Catastrophic Forgetting <https://arxiv.org/abs/2407.00875>`__ MoE-CT:一种抗灾难性遗忘的大型语言模型训练新方法

::

    Tue, 25 Jun 2024 11:03:45 GMT
    Tianhao Li, Shangjie Li, Binbin Xie, Deyi Xiong, Baosong Yang

The advent of large language models (LLMs) has predominantly catered to high-resource languages, leaving a disparity in performance for low-resource languages. Conventional Continual Training (CT) approaches to bridge this gap often undermine a model's original linguistic proficiency when expanding to multilingual contexts. Addressing this issue, we introduce a novel MoE-CT architecture, a paradigm that innovatively separates the base model's learning from the multilingual expansion process. Our design freezes the original LLM parameters, thus safeguarding its performance in high-resource languages, while an appended MoE module, trained on diverse language datasets, augments low-resource language proficiency. Our approach significantly outperforms conventional CT methods, as evidenced by our experiments, which show marked improvements in multilingual benchmarks without sacrificing the model's original language performance. Moreover, our MoE-CT framework demonstrates enhanced resistance to forgetting and superior transfer learning capabilities.
By preserving the base model's integrity and focusing on strategic parameter expansion, our methodology advances multilingual language modeling and represents a significant step forward for low-resource language inclusion in LLMs, indicating a fruitful direction for future research in language technologies.

------------

`[2407.00908] FineSurE: Fine-grained Summarization Evaluation using LLMs <https://arxiv.org/abs/2407.00908>`__ FineSurE:使用LLMs进行细粒度摘要评估

::

    Mon, 1 Jul 2024 02:20:28 GMT
    Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, Saab Mansour

Automated evaluation is crucial for streamlining text summarization benchmarking and model development, given the costly and time-consuming nature of human evaluation. Traditional methods like ROUGE do not correlate well with human judgment, while recently proposed LLM-based metrics provide only summary-level assessment using Likert-scale scores. This limits deeper model analysis, e.g., we can only assign one hallucination score at the summary level, while at the sentence level, we can count sentences containing hallucinations. To remedy those limitations, we propose FineSurE, a fine-grained evaluator specifically tailored for the summarization task using large language models (LLMs). It also employs completeness and conciseness criteria, in addition to faithfulness, enabling multi-dimensional assessment.
We compare various open-source and proprietary LLMs as backbones for FineSurE.
In addition, we conduct extensive benchmarking of FineSurE against SOTA methods including NLI-, QA-, and LLM-based methods, showing improved performance especially on the completeness and conciseness dimensions. The code is available at https://github.com/DISL-Lab/FineSurE-ACL24.

------------

`[2407.00948] The House Always Wins: A Framework for Evaluating Strategic Deception in LLMs <https://arxiv.org/abs/2407.00948>`__ 庄家总是赢家:评估llm战略欺骗的框架

::

    Mon, 1 Jul 2024 04:07:49 GMT
    Tanush Chopra and Michael Li

We propose a framework for evaluating strategic deception in large language models (LLMs). In this framework, an LLM acts as a game master in two scenarios: one with random game mechanics and another where it can choose between random or deliberate actions. As an example, we use blackjack because the action space nor strategies involve deception. We benchmark Llama3-70B, GPT-4-Turbo, and Mixtral in blackjack, comparing outcomes against expected distributions in fair play to determine if LLMs develop strategies favoring the "house." Our findings reveal that the LLMs exhibit significant deviations from fair play when given implicit randomness instructions, suggesting a tendency towards strategic manipulation in ambiguous scenarios. However, when presented with an explicit choice, the LLMs largely adhere to fair play, indicating that the framing of instructions plays a crucial role in eliciting or mitigating potentially deceptive behaviors in AI systems.

------------

`[2407.00994] LLM Uncertainty Quantification through Directional Entailment Graph and Claim Level Response Augmentation <https://arxiv.org/abs/2407.00994>`__ 通过定向蕴含图和请求级响应增强对LLM不确定性进行量化

::

    Mon, 1 Jul 2024 06:11:30 GMT
    Longchao Da, Tiejin Chen, Lu Cheng, Hua Wei

The Large language models (LLMs) have showcased superior capabilities in sophisticated tasks across various domains, stemming from basic question-answer (QA), they are nowadays used as decision assistants or explainers for unfamiliar content. However, they are not always correct due to the data sparsity in specific domain corpus, or the model's hallucination problems.
Given this, how much should we trust the responses from LLMs? This paper presents a novel way to evaluate the uncertainty that captures the directional instability, by constructing a directional graph from entailment probabilities, and we innovatively conduct Random Walk Laplacian given the asymmetric property of a constructed directed graph, then the uncertainty is aggregated by the derived eigenvalues from the Laplacian process. We also provide a way to incorporate the existing work's semantics uncertainty with our proposed layer.
Besides, this paper identifies the vagueness issues in the raw response set and proposes an augmentation approach to mitigate such a problem, we conducted extensive empirical experiments and demonstrated the superiority of our proposed solutions.

------------

`[2407.00996] Can Small Language Models Learn, Unlearn, and Retain Noise Patterns? <https://arxiv.org/abs/2407.00996>`__ 小型语言模型能学习、遗忘和保留噪声模式吗?

::

    Mon, 1 Jul 2024 06:22:38 GMT
    Nicy Scaria, Silvester John Joseph Kennedy, Deepak Subramani

Small Language Models (SLMs) are generally considered to be more compact versions of large language models (LLMs), typically having fewer than 7 billion parameters. This study investigates the ability of small language models to learn, retain, and subsequently eliminate noise that is typically not found on the internet, where most pretraining datasets are sourced. For this, four pre-trained SLMs were utilized: Olmo 1B, Qwen1.5 1.8B, Gemma 2B, and Phi2 2.7B.
The models were instruction-tuned without noise and tested for task execution with in-context learning. Afterward, noise patterns were introduced to evaluate the models' learning and unlearning capabilities. We evaluated the models' performance at various training levels. Phi consistently excelled with word-level noise but performed the worst with character-level noise. Despite being the smallest with approximately 1 billion parameters, Olmo performed consistently well on tasks.

------------

`[2407.00997] Engineering Conversational Search Systems: A Review of Applications, Architectures, and Functional Components <https://arxiv.org/abs/2407.00997>`__ 工程会话搜索系统:应用、体系结构和功能组件的回顾

::

    Mon, 1 Jul 2024 06:24:11 GMT
    Phillip Schneider, Wessel Poelman, Michael Rovatsos, Florian Matthes

Conversational search systems enable information retrieval via natural language interactions, with the goal of maximizing users' information gain over multiple dialogue turns. The increasing prevalence of conversational interfaces adopting this search paradigm challenges traditional information retrieval approaches, stressing the importance of better understanding the engineering process of developing these systems. We undertook a systematic literature review to investigate the links between theoretical studies and technical implementations of conversational search systems. Our review identifies real-world application scenarios, system architectures, and functional components. We consolidate our results by presenting a layered architecture framework and explaining the core functions of conversational search systems.
Furthermore, we reflect on our findings in light of the rapid progress in large language models, discussing their capabilities, limitations, and directions for future research.

------------

`[2407.01009] DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models <https://arxiv.org/abs/2407.01009>`__ DynaThink:快还是慢?大型语言模型的动态决策框架

::

    Mon, 1 Jul 2024 06:45:13 GMT
    Jiabao Pan, Yan Zhang, Chen Zhang, Zuozhu Liu, Hongwei Wang, and Haizhou Li

Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with complicated problems, while a thorough method, which considers multiple reasoning pathways and verifies each step carefully, results in slower inference. This paper addresses the challenge of enabling LLMs to autonomously select between fast and slow inference methods, thereby optimizing both efficiency and effectiveness. We introduce a dynamic decision-making framework that categorizes tasks into two distinct pathways: 'Fast', designated for tasks where the LLM quickly identifies a high-confidence solution, and 'Slow', allocated for tasks that the LLM perceives as complex and for which it has low confidence in immediate solutions as well as requiring more reasoning paths to verify. Experiments on five popular reasoning benchmarks demonstrated the superiority of the DynaThink over baselines.

------------

`[2407.01082] Min P Sampling: Balancing Creativity and Coherence at High Temperature <https://arxiv.org/abs/2407.01082>`__ 最小P采样:在高温下平衡创造力和连贯性

::

    Mon, 1 Jul 2024 08:37:25 GMT
    Minh Nguyen, Andrew Baker, Andreas Kirsch, Clement Neo

Large Language Models (LLMs) generate longform text by successively sampling the next token based on the probability distribution of the token vocabulary at each decoding step. Current popular truncation sampling methods such as top-$p$ sampling, also known as nucleus sampling, often struggle to balance coherence and creativity in generating text, particularly when using higher temperatures.
To address this issue, we propose min-$p$, a dynamic truncation sampling method, that establishes a minimum base percentage threshold for tokens, which the scales according to the probability of the top candidate token. Through experiments on several benchmarks, such as GPQA, GSM8K and AlpacaEval Creative Writing, we demonstrate that min-$p$ improves the coherence and quality of generated text even at high temperatures, while also facilitating more creative and diverse outputs compared to top-$p$ and other sampling methods. As of writing, min-$p$ has been adopted by multiple open-source LLM implementations, and have been independently assessed by members of the open-source LLM community, further validating its practical utility and potential.

------------

`[2407.01091] M2QA: Multi-domain Multilingual Question Answering <https://arxiv.org/abs/2407.01091>`__ M2QA:多领域多语言问答

::

    Mon, 1 Jul 2024 08:48:49 GMT
    Leon Engl\"ander, Hannah Sterz, Clifton Poth, Jonas Pfeiffer, Ilia Kuznetsov and Iryna Gurevych

Generalization and robustness to input variation are core desiderata of machine learning research. Language varies along several axes, most importantly, language instance (e.g. French) and domain (e.g. news). While adapting NLP models to new languages within a single domain, or to new domains within a single language, is widely studied, research in joint adaptation is hampered by the lack of evaluation datasets. This prevents the transfer of NLP systems from well-resourced languages and domains to non-dominant language-domain combinations. To address this gap, we introduce M2QA, a multi-domain multilingual question answering benchmark. M2QA includes 13,500 SQuAD 2.0-style question-answer instances in German, Turkish, and Chinese for the domains of product reviews, news, and creative writing. We use M2QA to explore cross-lingual cross-domain performance of fine-tuned models and state-of-the-art LLMs and investigate modular approaches to domain and language adaptation. We witness 1) considerable performance variations across domain-language combinations within model classes and 2) considerable performance drops between source and target language-domain combinations across all model sizes. We demonstrate that M2QA is far from solved, and new methods to effectively transfer both linguistic and domain-specific information are necessary. We make M2QA publicly available at https://github.com/UKPLab/m2qa.

------------

`[2407.01119] Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing? <https://arxiv.org/abs/2407.01119>`__ Pron vs Prompt:大型语言模型是否已经在创造性文本写作方面挑战世界级小说作者?

::

    Mon, 1 Jul 2024 09:28:58 GMT
    Guillermo Marco, Julio Gonzalo, Ram\'on del Castillo, Mar\'ia Teresa Mateo Girona

It has become routine to report research results where Large Language Models (LLMs) outperform average humans in a wide range of language-related tasks, and creative text writing is no exception. It seems natural, then, to raise the bid: Are LLMs ready to compete in creative writing skills with a top (rather than average) novelist? To provide an initial answer for this question, we have carried out a contest between Patricio Pron (an awarded novelist, considered one of the best of his generation) and GPT-4 (one of the top performing LLMs), in the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee Sidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write short stories for both their titles and their opponent's. Then, we prepared an evaluation rubric inspired by Boden's definition of creativity, and we collected 5,400 manual assessments provided by literature critics and scholars.
The results of our experimentation indicate that LLMs are still far from challenging a top human creative writer, and that reaching such level of autonomous creative writing skills probably cannot be reached simply with larger language models.

------------

`[2407.01122] Calibrated Large Language Models for Binary Question Answering <https://arxiv.org/abs/2407.01122>`__ 用于二元问答的校准大型语言模型

::

    Mon, 1 Jul 2024 09:31:03 GMT
    Patrizio Giovannotti and Alexander Gammerman

Quantifying the uncertainty of predictions made by large language models (LLMs) in binary text classification tasks remains a challenge. Calibration, in the context of LLMs, refers to the alignment between the model's predicted probabilities and the actual correctness of its predictions. A well-calibrated model should produce probabilities that accurately reflect the likelihood of its predictions being correct. We propose a novel approach that utilizes the inductive Venn--Abers predictor (IVAP) to calibrate the probabilities associated with the output tokens corresponding to the binary labels. Our experiments on the BoolQ dataset using the Llama 2 model demonstrate that IVAP consistently outperforms the commonly used temperature scaling method for various label token choices, achieving well-calibrated probabilities while maintaining high predictive quality. Our findings contribute to the understanding of calibration techniques for LLMs and provide a practical solution for obtaining reliable uncertainty estimates in binary question answering tasks, enhancing the interpretability and trustworthiness of LLM predictions.

------------

`[2407.01178] $\text{Memory}^3$: Language Modeling with Explicit Memory <https://arxiv.org/abs/2407.01178>`__ $\text{Memory}^3$:使用显式内存的语言建模

::

    Mon, 1 Jul 2024 11:07:23 GMT
    Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, Weinan E

The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs with explicit memory, a memory format cheaper than model parameters and text retrieval-augmented generation (RAG). Conceptually, with most of its knowledge externalized to explicit memories, the LLM can enjoy a smaller parameter size, training cost, and inference cost, all proportional to the amount of remaining "abstract knowledge". As a preliminary proof of concept, we train from scratch a 2.4B LLM, which achieves better performance than much larger LLMs as well as RAG models, and maintains higher decoding speed than RAG. The model is named $\text{Memory}^3$, since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values). We introduce a memory circuitry theory to support the externalization of knowledge, and present novel techniques including a memory sparsification mechanism that makes storage tractable and a two-stage pretraining scheme that facilitates memory formation.

------------

`[2407.01270] The African Woman is Rhythmic and Soulful: Evaluation of Open-ended Generation for Implicit Biases <https://arxiv.org/abs/2407.01270>`__ 《非洲妇女》节奏感强，深情款款:评价开放式世代的隐性偏见

::

    Mon, 1 Jul 2024 13:21:33 GMT
    Serene Lim

This study investigates the subtle and often concealed biases present in Large Language Models (LLMs), which, despite passing explicit bias tests, can still exhibit implicit biases akin to those observed in humans who profess egalitarian beliefs yet demonstrate underlying prejudices. The challenge of measuring such biases is exacerbated as LLMs become increasingly proprietary, restricting access to their internal mechanisms such as embeddings, which are crucial for applying traditional bias measures. To tackle these issues, this study introduces innovative measures of bias inspired by psychological methodologies: the LLM Implicit Association Test (IAT) Bias and the LLM Decision Bias. The LLM IAT Bias is a prompt-based method designed to unearth implicit biases by simulating the well-known psychological IAT but adapted for use with LLMs. The LLM Decision Bias measure is developed to detect subtle discrimination in decision-making tasks, focusing on how LLMs choose between individuals in various scenarios. Open-ended generation is also utilised through thematic analysis of word generations and storytelling. The experiments revealed biases across gender and racial domains, from discriminatory categorisations to exoticisation. Our findings indicate that the prompt-based measure of implicit bias not only correlates with traditional embedding-based methods but also more effectively predicts downstream behaviors, which are crucially measured by the LLM Decision Bias. This relationship underscores the importance of relative, rather than absolute, evaluations in assessing implicit biases, reflecting psychological insights into human bias assessment. This research contributes to the broader understanding of AI ethics and provides suggestions for continually assessing and mitigating biases in advanced AI systems, emphasising the need for more qualitative and downstream focus.

------------

`[2407.01272] Show Less, Instruct More: Enriching Prompts with Definitions and Guidelines for Zero-Shot NER <https://arxiv.org/abs/2407.01272>`__ 

::

    Mon, 1 Jul 2024 13:25:33 GMT
    Andrew Zamai, Andrea Zugarini, Leonardo Rigutini, Marco Ernandes, Marco Maggini

Recently, several specialized instruction-tuned Large Language Models (LLMs) for Named Entity Recognition (NER) have emerged. Compared to traditional NER approaches, these models have strong generalization capabilities. Existing LLMs mainly focus on zero-shot NER in out-of-domain distributions, being fine-tuned on an extensive number of entity classes that often highly or completely overlap with test sets. In this work instead, we propose SLIMER, an approach designed to tackle never-seen-before named entity tags by instructing the model on fewer examples, and by leveraging a prompt enriched with definition and guidelines. Experiments demonstrate that definition and guidelines yield better performance, faster and more robust learning, particularly when labelling unseen Named Entities. Furthermore, SLIMER performs comparably to state-of-the-art approaches in out-of-domain zero-shot NER, while being trained on a reduced tag set.

------------

`[2407.01300] Collaborative Performance Prediction for Large Language Models <https://arxiv.org/abs/2407.01300>`__ 大型语言模型的协同性能预测

::

    Mon, 1 Jul 2024 13:56:42 GMT
    Qiyuan Zhang, Fuyuan Lyu, Xue Liu, Chen Ma

Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works demonstrated intrinsic similarities within model families and utilized such similarities for performance prediction. However, they tend to overlook the similarities between model families and only consider design factors listed in the original scaling law. To overcome these limitations, we introduce a novel framework, Collaborative Performance Prediction (CPP), which significantly enhances prediction accuracy by leveraging the historical performance of various models on downstream tasks and other design factors for both model and task. We also collect a collaborative data sourced from online platforms containing both historical performance and additional design factors. With the support of the collaborative data, CPP not only surpasses traditional scaling laws in predicting the performance of scaled LLMs but also facilitates a detailed analysis of factor importance, an area previously overlooked.

------------

`[2407.01358] Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language Models <https://arxiv.org/abs/2407.01358>`__ 大型语言模型中基于知识的跨语言不一致性评估

::

    Mon, 1 Jul 2024 15:11:37 GMT
    Xiaolin Xing, Zhiwei He, Haoyu Xu, Xing Wang, Rui Wang, Yu Hong

This paper investigates the cross-lingual inconsistencies observed in Large Language Models (LLMs), such as ChatGPT, Llama, and Baichuan, which have shown exceptional performance in various Natural Language Processing (NLP) tasks.
Despite their successes, these models often exhibit significant inconsistencies when processing the same concepts across different languages. This study focuses on three primary questions: the existence of cross-lingual inconsistencies in LLMs, the specific aspects in which these inconsistencies manifest, and the correlation between cross-lingual consistency and multilingual capabilities of LLMs.To address these questions, we propose an innovative evaluation method for Cross-lingual Semantic Consistency (xSC) using the LaBSE model. We further introduce metrics for Cross-lingual Accuracy Consistency (xAC) and Cross-lingual Timeliness Consistency (xTC) to comprehensively assess the models' performance regarding semantic, accuracy, and timeliness inconsistencies. By harmonizing these metrics, we provide a holistic measurement of LLMs' cross-lingual consistency. Our findings aim to enhance the understanding and improvement of multilingual capabilities and interpretability in LLMs, contributing to the development of more robust and reliable multilingual language models.

------------

`[2407.01384] Free-text Rationale Generation under Readability Level Control <https://arxiv.org/abs/2407.01384>`__ 可读性级别控制下的自由文本逻辑生成

::

    Mon, 1 Jul 2024 15:34:17 GMT
    Yi-Sheng Hsu, Nils Feldhus, Sherzod Hakimov

Free-text rationales justify model decisions in natural language and thus become likable and accessible among approaches to explanation across many tasks. However, their effectiveness can be hindered by misinterpretation and hallucination. As a perturbation test, we investigate how large language models (LLMs) perform the task of natural language explanation (NLE) under the effects of readability level control, i.e., being prompted for a rationale targeting a specific expertise level, such as sixth grade or college. We find that explanations are adaptable to such instruction, but the requested readability is often misaligned with the measured text complexity according to traditional readability metrics. Furthermore, the quality assessment shows that LLMs' ratings of rationales across text complexity exhibit a similar pattern of preference as observed in natural language generation (NLG). Finally, our human evaluation suggests a generally satisfactory impression on rationales at all readability levels, with high-school-level readability being most commonly perceived and favored.

------------

`[2407.01406] Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters <https://arxiv.org/abs/2407.01406>`__ 通过适配器用知识图谱使多语言llm适应低资源语言

::

    Mon, 1 Jul 2024 15:56:24 GMT
    Daniil Gurgurov, Mareike Hartmann, Simon Ostermann

This paper explores the integration of graph knowledge from linguistic ontologies into multilingual Large Language Models (LLMs) using adapters to improve performance for low-resource languages (LRLs) in sentiment analysis (SA) and named entity recognition (NER). Building upon successful parameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we propose a similar approach for incorporating knowledge from multilingual graphs, connecting concepts in various languages with each other through linguistic relationships, into multilingual LLMs for LRLs. Specifically, we focus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese, Uyghur, Tibetan, and Sinhala -- and employ language-specific adapters fine-tuned on data extracted from the language-specific section of ConceptNet, aiming to enable knowledge transfer across the languages covered by the knowledge graph. We compare various fine-tuning objectives, including standard Masked Language Modeling (MLM), MLM with full-word masking, and MLM with targeted masking, to analyse their effectiveness in learning and integrating the extracted graph data. Through empirical evaluation on language-specific tasks, we assess how structured graph knowledge affects the performance of multilingual LLMs for LRLs in SA and NER, providing insights into the potential benefits of adapting language models for low-resource scenarios.

------------

`[2407.01409] Dynamic Few-Shot Learning for Knowledge Graph Question Answering <https://arxiv.org/abs/2407.01409>`__ 基于动态少样本学习的知识图谱问答

::

    Mon, 1 Jul 2024 15:59:17 GMT
    Jacopo D'Abramo, Andrea Zugarini, Paolo Torroni

Large language models present opportunities for innovative Question Answering over Knowledge Graphs (KGQA). However, they are not inherently designed for query generation. To bridge this gap, solutions have been proposed that rely on fine-tuning or ad-hoc architectures, achieving good results but limited out-of-domain distribution generalization. In this study, we introduce a novel approach called Dynamic Few-Shot Learning (DFSL). DFSL integrates the efficiency of in-context learning and semantic similarity and provides a generally applicable solution for KGQA with state-of-the-art performance. We run an extensive evaluation across multiple benchmark datasets and architecture configurations.

------------

`[2407.01437] Needle in the Haystack for Memory Based Large Language Models <https://arxiv.org/abs/2407.01437>`__ 基于内存的大型语言模型是大海捞针

::

    Mon, 1 Jul 2024 16:32:16 GMT
    Subhajit Chaudhury, Soham Dan, Payel Das, Georgios Kollias, Elliot Nelson

In this paper, we demonstrate the benefits of using memory augmented Large Language Model (LLM) architecture in improving the recall abilities of facts from a potentially long context. As a case study we test LARIMAR, a recently proposed LLM architecture which augments a LLM decoder with an external associative memory, on several long-context recall tasks, including passkey and needle-in-the-haystack tests. We demonstrate that the external memory can be adapted at test time to handle contexts much longer than those seen during training, while keeping readouts from the memory recognizable to the trained decoder and without increasing GPU memory footprint. Compared to alternative architectures for long-context recall tasks with models of a comparable parameter count, LARIMAR is able to maintain strong performance without any task-specific training.

------------

`[2407.01455] TimeToM: Temporal Space is the Key to Unlocking the Door of Large Language Models' Theory-of-Mind <https://arxiv.org/abs/2407.01455>`__ TimeToM:时间空间是打开大型语言模型心智理论之门的钥匙

::

    Mon, 1 Jul 2024 16:50:49 GMT
    Guiyang Hou, Wenqi Zhang, Yongliang Shen, Linjuan Wu, Weiming Lu

Theory of Mind (ToM)-the cognitive ability to reason about mental states of ourselves and others, is the foundation of social interaction. Although ToM comes naturally to humans, it poses a significant challenge to even the most advanced Large Language Models (LLMs). Due to the complex logical chains in ToM reasoning, especially in higher-order ToM questions, simply utilizing reasoning methods like Chain of Thought (CoT) will not improve the ToM capabilities of LLMs. We present TimeToM, which constructs a temporal space and uses it as the foundation to improve the ToM capabilities of LLMs in multiple scenarios.
Specifically, within the temporal space, we construct Temporal Belief State Chain (TBSC) for each character and inspired by the cognition perspective of the social world model, we divide TBSC into self-world beliefs and social world beliefs, aligning with first-order ToM (first-order beliefs) and higher-order ToM (higher-order beliefs) questions, respectively. Moreover, we design a novel tool-belief solver that, by considering belief communication between characters in temporal space, can transform a character's higher-order beliefs into another character's first-order beliefs under belief communication period.
Experimental results indicate that TimeToM can dramatically improve the reasoning performance of LLMs on ToM questions while taking a big step towards coherent and robust ToM reasoning.

------------

`[2407.01461] Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement <https://arxiv.org/abs/2407.01461>`__ 通过强化学习驱动的查询精化提高大型语言模型的能力和鲁棒性

::

    Mon, 1 Jul 2024 16:55:28 GMT
    Zisu Huang, Xiaohua Wang, Feiran Zhang, Zhibo Xu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang

The capacity of large language models (LLMs) to generate honest, harmless, and helpful responses heavily relies on the quality of user prompts. However, these prompts often tend to be brief and vague, thereby significantly limiting the full potential of LLMs. Moreover, harmful prompts can be meticulously crafted and manipulated by adversaries to jailbreak LLMs, inducing them to produce potentially toxic content. To enhance the capabilities of LLMs while maintaining strong robustness against harmful jailbreak inputs, this study proposes a transferable and pluggable framework that refines user prompts before they are input into LLMs. This strategy improves the quality of the queries, empowering LLMs to generate more truthful, benign and useful responses. Specifically, a lightweight query refinement model is introduced and trained using a specially designed reinforcement learning approach that incorporates multiple objectives to enhance particular capabilities of LLMs.
Extensive experiments demonstrate that the refinement model not only improves the quality of responses but also strengthens their robustness against jailbreak attacks. Code is available at: https://github.com/Huangzisu/query-refinement .

------------

`[2407.01470] DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging <https://arxiv.org/abs/2407.01470>`__ DogeRM:通过模型融合为奖励模型配备领域知识

::

    Mon, 1 Jul 2024 17:01:54 GMT
    Tzu-Han Lin and Chen-An Li and Hung-yi Lee and Yun-Nung Chen

Reinforcement learning from human feedback (RLHF) is a popular strategy for aligning large language models (LLMs) with desired behaviors. Reward modeling is a crucial step in RLHF. However, collecting paired preference data for training reward models is often costly and time-consuming, especially for domain-specific preferences requiring expert annotation. To address this challenge, we propose the \textbf{Do}main knowled\textbf{ge} merged \textbf{R}eward \textbf{M}odel (DogeRM), a novel framework that integrates domain-specific knowledge into a general reward model by model merging. The experiments demonstrate that DogeRM enhances performance across different benchmarks and provide a detailed analysis showcasing the effects of model merging, showing the great potential of facilitating model alignment.

------------

`[2407.01490] LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives <https://arxiv.org/abs/2407.01490>`__ LLM看，LLM做:指导数据生成以实现不可微的目标

::

    Mon, 1 Jul 2024 17:26:21 GMT
    Lu\'isa Shimabucoro, Sebastian Ruder, Julia Kreutzer, Marzieh Fadaee and Sara Hooker

The widespread adoption of synthetic data raises new questions about how models generating the data can influence other large language models (LLMs) via distilled data. To start, our work exhaustively characterizes the impact of passive inheritance of model properties by systematically studying the consequences of synthetic data integration. We provide one of the most comprehensive studies to-date of how the source of synthetic data shapes models' internal biases, calibration and generations' textual attributes and preferences. We find that models are surprisingly sensitive towards certain attributes even when the synthetic data prompts appear "neutral". which invites the question whether this sensitivity can be exploited for good.
Our findings invite the question can we explicitly steer the models towards the properties we want at test time by exploiting the data generation process? This would have historically been considered infeasible due to the cost of collecting data with a specific characteristic or objective in mind. However, improvement in the quality of synthetic data, as well as a shift towards general-purpose models designed to follow a diverse way of instructions, means this question is timely. We propose active inheritance as a term to describe intentionally constraining synthetic data according to a non-differentiable objective. We demonstrate how active inheritance can steer the generation profiles of models towards desirable non-differentiable attributes, e.g. high lexical diversity or low toxicity.

------------

`[2407.01492] RegMix: Data Mixture as Regression for Language Model Pre-training <https://arxiv.org/abs/2407.01492>`__ RegMix:用于语言模型预训练的数据混合回归

::

    Mon, 1 Jul 2024 17:31:03 GMT
    Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, Min Lin

The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures.
Further, our method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10% of the compute budget. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws, and our approach captures the complexity by considering all domains together. Our code is available at https://github.com/sail-sg/regmix.

------------

`[2407.01505] Self-Cognition in Large Language Models: An Exploratory Study <https://arxiv.org/abs/2407.01505>`__ 大型语言模型中的自我认知:探索性研究

::

    Mon, 1 Jul 2024 17:52:05 GMT
    Dongping Chen, Jiawen Shi, Yao Wan, Pan Zhou, Neil Zhenqiang Gong, Lichao Sun

While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition. In this paper, we perform a pioneering study to explore self-cognition in LLMs.
Specifically, we first construct a pool of self-cognition instruction prompts to evaluate where an LLM exhibits self-cognition and four well-designed principles to quantify LLMs' self-cognition. Our study reveals that 4 of the 48 models on Chatbot Arena--specifically Command R, Claude3-Opus, Llama-3-70b-Instruct, and Reka-core--demonstrate some level of detectable self-cognition. We observe a positive correlation between model size, training data quality, and self-cognition level. Additionally, we also explore the utility and trustworthiness of LLM in the self-cognition state, revealing that the self-cognition state enhances some specific tasks such as creative writing and exaggeration. We believe that our work can serve as an inspiration for further research to study the self-cognition in LLMs.

------------

`[2407.00102] Curriculum Learning with Quality-Driven Data Selection <https://arxiv.org/abs/2407.00102>`__ 基于质量驱动数据选择的课程学习

::

    Thu, 27 Jun 2024 07:20:36 GMT
    Biao Wu, Fang Meng, Ling Chen

The impressive multimodal capabilities demonstrated by OpenAI's GPT-4 have generated significant interest in the development of Multimodal Large Language Models (MLLMs). Visual instruction tuning of MLLMs with machine-generated instruction-following data has shown to enhance zero-shot capabilities across various tasks. However, there has been limited exploration into controlling the quality of the instruction data.Current methodologies for data selection in MLLMs often rely on single, unreliable scores or use downstream tasks for selection, which is time-consuming and can lead to potential overfitting on the chosen evaluation datasets. To mitigate these limitations, we propose a novel data selection methodology that utilizes image-text correlation and model perplexity to evaluate and select data of varying quality. This approach leverages the distinct distribution of these two attributes, mapping data quality into a two-dimensional space that allows for the selection of data based on their location within this distribution. By utilizing this space, we can analyze the impact of task type settings, used as prompts, on data quality.
Additionally, this space can be used to construct multi-stage subsets of varying quality to facilitate curriculum learning. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in five commonly assessed capabilities compared to using the complete dataset. Our codes, data, and models are publicly available at: \url{https://anonymous.4open.science/r/EHIT-31B4}

------------

`[2407.00106] UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI <https://arxiv.org/abs/2407.00106>`__ UnUnlearning:在高级生成式AI中，UnUnlearning不足以实现内容监管

::

    Thu, 27 Jun 2024 10:24:35 GMT
    Ilia Shumailov, Jamie Hayes, Eleni Triantafillou, Guillermo Ortiz-Jimenez, Nicolas Papernot, Matthew Jagielski, Itay Yona, Heidi Howard, Eugene Bagdasaryan

Exact unlearning was first introduced as a privacy mechanism that allowed a user to retract their data from machine learning models on request. Shortly after, inexact schemes were proposed to mitigate the impractical costs associated with exact unlearning. More recently unlearning is often discussed as an approach for removal of impermissible knowledge i.e. knowledge that the model should not possess such as unlicensed copyrighted, inaccurate, or malicious information. The promise is that if the model does not have a certain malicious capability, then it cannot be used for the associated malicious purpose. In this paper we revisit the paradigm in which unlearning is used for in Large Language Models (LLMs) and highlight an underlying inconsistency arising from in-context learning. Unlearning can be an effective control mechanism for the training phase, yet it does not prevent the model from performing an impermissible act during inference. We introduce a concept of ununlearning, where unlearned knowledge gets reintroduced in-context, effectively rendering the model capable of behaving as if it knows the forgotten knowledge. As a result, we argue that content filtering for impermissible knowledge will be required and even exact unlearning schemes are not enough for effective content regulation. We discuss feasibility of ununlearning for modern LLMs and examine broader implications.

------------

`[2407.00116] Generative AI for Synthetic Data Across Multiple Medical Modalities: A Systematic Review of Recent Developments and Challenges <https://arxiv.org/abs/2407.00116>`__ 面向多种医疗模式合成数据的生成式人工智能:最新发展和挑战的系统综述

::

    Thu, 27 Jun 2024 14:00:11 GMT
    Mahmoud Ibrahim, Yasmina Al Khalil, Sina Amirrajab, Chang Suna, Marcel Breeuwer, Josien Pluim, Bart Elen, Gokhan Ertaylan, Michel Dumontiera

This paper presents a comprehensive systematic review of generative models (GANs, VAEs, DMs, and LLMs) used to synthesize various medical data types, including imaging (dermoscopic, mammographic, ultrasound, CT, MRI, and X-ray), text, time-series, and tabular data (EHR). Unlike previous narrowly focused reviews, our study encompasses a broad array of medical data modalities and explores various generative models. Our search strategy queries databases such as Scopus, PubMed, and ArXiv, focusing on recent works from January 2021 to November 2023, excluding reviews and perspectives. This period emphasizes recent advancements beyond GANs, which have been extensively covered previously.
The survey reveals insights from three key aspects: (1) Synthesis applications and purpose of synthesis, (2) generation techniques, and (3) evaluation methods. It highlights clinically valid synthesis applications, demonstrating the potential of synthetic data to tackle diverse clinical requirements. While conditional models incorporating class labels, segmentation masks and image translations are prevalent, there is a gap in utilizing prior clinical knowledge and patient-specific context, suggesting a need for more personalized synthesis approaches and emphasizing the importance of tailoring generative approaches to the unique characteristics of medical data.
Additionally, there is a significant gap in using synthetic data beyond augmentation, such as for validation and evaluation of downstream medical AI models. The survey uncovers that the lack of standardized evaluation methodologies tailored to medical images is a barrier to clinical application, underscoring the need for in-depth evaluation approaches, benchmarking, and comparative studies to promote openness and collaboration.

------------

`[2407.00121] Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks <https://arxiv.org/abs/2407.00121>`__ Granite-Function调用模型:通过粒度任务的多任务学习引入函数调用能力

::

    Thu, 27 Jun 2024 17:47:26 GMT
    Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matthew Stallone, Rameswar Panda, Yara Rizk, GP Bhargav, Maxwell Crouse, Chulaka Gunasekara, Shajith Ikbal, Sachin Joshi, Hima Karanam, Vineet Kumar, Asim Munawar, Sumit Neelam, Dinesh Raghu, Udit Sharma, Adriana Meza Soria, Dheeraj Sreedhar, Praveen Venkateswaran, Merve Unuvar, David Cox, Salim Roukos, Luis Lastras, Pavan Kapanipathi

Large language models (LLMs) have recently shown tremendous promise in serving as the backbone to agentic systems, as demonstrated by their performance in multi-faceted, challenging benchmarks like SWE-Bench and Agent-Bench. However, to realize the true potential of LLMs as autonomous agents, they must learn to identify, call, and interact with external tools and application program interfaces (APIs) to complete complex tasks. These tasks together are termed function calling. Endowing LLMs with function calling abilities leads to a myriad of advantages, such as access to current and domain-specific information in databases and knowledge sources, and the ability to outsource tasks that can be reliably performed by tools, e.g., a Python interpreter or calculator. While there has been significant progress in function calling with LLMs, there is still a dearth of open models that perform on par with proprietary LLMs like GPT, Claude, and Gemini. Therefore, in this work, we introduce the GRANITE-20B-FUNCTIONCALLING model under an Apache 2.0 license. The model is trained using a multi-task training approach on seven fundamental tasks encompassed in function calling, those being Nested Function Calling, Function Chaining, Parallel Functions, Function Name Detection, Parameter-Value Pair Detection, Next-Best Function, and Response Generation. We present a comprehensive evaluation on multiple out-of-domain datasets comparing GRANITE-20B-FUNCTIONCALLING to more than 15 other best proprietary and open models. GRANITE-20B-FUNCTIONCALLING provides the best performance among all open models on the Berkeley Function Calling Leaderboard and fourth overall. As a result of the diverse tasks and datasets used for training our model, we show that GRANITE-20B-FUNCTIONCALLING has better generalizability on multiple tasks in seven different evaluation datasets.

------------

`[2407.00463] Open-Source Conversational AI with SpeechBrain 1.0 <https://arxiv.org/abs/2407.00463>`__ 基于SpeechBrain 1.0的开源对话AI

::

    Sat, 29 Jun 2024 15:20:11 GMT
    Mirco Ravanelli, Titouan Parcollet, Adel Moumen, Sylvain de Langen, Cem Subakan, Peter Plantinga, Yingzhi Wang, Pooneh Mousavi, Luca Della Libera, Artem Ploujnikov, Francesco Paissan, Davide Borra, Salah Zaiem, Zeyu Zhao, Shucong Zhang, Georgios Karakasidis, Sung-Lin Yeh, Aku Rouhe, Rudolf Braun, Florian Mai, Juan Zuluaga-Gomez, Seyed Mahed Mousavi, Andreas Nautsch, Xuechen Liu, Sangeet Sagar, Jarod Duret, Salima Mdhaffar, Gaelle Laperriere, Renato De Mori, Yannick Esteve

SpeechBrain is an open-source Conversational AI toolkit based on PyTorch, focused particularly on speech processing tasks such as speech recognition, speech enhancement, speaker recognition, text-to-speech, and much more.It promotes transparency and replicability by releasing both the pre-trained models and the complete "recipes" of code and algorithms required for training them. This paper presents SpeechBrain 1.0, a significant milestone in the evolution of the toolkit, which now has over 200 recipes for speech, audio, and language processing tasks, and more than 100 models available on Hugging Face.
SpeechBrain 1.0 introduces new technologies to support diverse learning modalities, Large Language Model (LLM) integration, and advanced decoding strategies, along with novel models, tasks, and modalities. It also includes a new benchmark repository, offering researchers a unified platform for evaluating models across diverse tasks.

------------

`[2407.00467] VcLLM: Video Codecs are Secretly Tensor Codecs <https://arxiv.org/abs/2407.00467>`__ VcLLM:视频编解码器实际上是张量编解码器

::

    Sat, 29 Jun 2024 15:24:33 GMT
    Ceyu Xu, Yongji Wu, Xinyu Yang, Beidi Chen, Matthew Lentz, Danyang Zhuo, Lisa Wu Wills

As the parameter size of large language models (LLMs) continues to expand, the need for a large memory footprint and high communication bandwidth have become significant bottlenecks for the training and inference of LLMs. To mitigate these bottlenecks, various tensor compression techniques have been proposed to reduce the data size, thereby alleviating memory requirements and communication pressure.
Our research found that video codecs, despite being originally designed for compressing videos, show excellent efficiency when compressing various types of tensors. We demonstrate that video codecs can be versatile and general-purpose tensor codecs while achieving the state-of-the-art compression efficiency in various tasks. We further make use of the hardware video encoding and decoding module available on GPUs to create a framework capable of both inference and training with video codecs repurposed as tensor codecs. This greatly reduces the requirement for memory capacity and communication bandwidth, enabling training and inference of large models on consumer-grade GPUs.

------------

`[2407.00568] Divide And Conquer: Learning Chaotic Dynamical Systems With Multistep Penalty Neural Ordinary Differential Equations <https://arxiv.org/abs/2407.00568>`__ 分治:基于多步惩罚神经常微分方程的混沌动力系统学习

::

    Sun, 30 Jun 2024 02:50:28 GMT
    Dibyajyoti Chakraborty, Seung Whan Chung, Romit Maulik

Forecasting high-dimensional dynamical systems is a fundamental challenge in various fields, such as the geosciences and engineering. Neural Ordinary Differential Equations (NODEs), which combine the power of neural networks and numerical solvers, have emerged as a promising algorithm for forecasting complex nonlinear dynamical systems. However, classical techniques used for NODE training are ineffective for learning chaotic dynamical systems. In this work, we propose a novel NODE-training approach that allows for robust learning of chaotic dynamical systems. Our method addresses the challenges of non-convexity and exploding gradients associated with underlying chaotic dynamics. Training data trajectories from such systems are split into multiple, non-overlapping time windows. In addition to the deviation from the training data, the optimization loss term further penalizes the discontinuities of the predicted trajectory between the time windows. The window size is selected based on the fastest Lyapunov time scale of the system. Multi-step penalty(MP) method is first demonstrated on Lorenz equation, to illustrate how it improves the loss landscape and thereby accelerating the optimization convergence. MP method can optimize chaotic systems in a manner similar to least-squares shadowing with significantly lower computational costs. Our proposed algorithm, denoted the Multistep Penalty NODE(MP-NODE), is applied to chaotic systems such as the Kuramoto-Sivashinsky equation and the two-dimensional Kolmogorov flow.
It is observed that MP-NODE provide viable performance for such chaotic systems, not only for short-term trajectory predictions but also for invariant statistics that are hallmarks of the chaotic nature of these dynamics.

------------

`[2407.00617] Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning <https://arxiv.org/abs/2407.00617>`__ 迭代纳什策略优化:通过无后悔学习将llm与一般偏好对齐

::

    Sun, 30 Jun 2024 08:00:34 GMT
    Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, Dong Yu

Reinforcement Learning with Human Feedback (RLHF) has achieved great success in aligning large language models (LLMs) with human preferences. Prevalent RLHF approaches are reward-based, following the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of human preferences. In this paper, we explore RLHF under a general preference framework and approach it from a game-theoretic perspective. Specifically, we formulate the problem as a two-player game and propose a novel algorithm, iterative Nash policy optimization (INPO). The key idea is to let the policy play against itself via no-regret learning, thereby approximating the Nash policy. Unlike previous methods, INPO bypasses the need for estimating the expected win rate for individual responses, which typically incurs high computational or annotation costs. Instead, we introduce a new loss objective that is directly minimized over a preference dataset. We provide theoretical analysis for our approach and demonstrate its effectiveness through experiments on various representative benchmarks. With an LLaMA-3-8B-based SFT model, INPO achieves a 41.5% length-controlled win rate on AlpacaEval 2.0 and a 38.3% win rate on Arena-Hard, showing substantial improvement over the state-of-the-art iterative algorithm [Dong et al., 2024] under the BT model assumption. Additionally, our ablation study highlights the benefits of incorporating KL regularization for response length control.

------------

`[2407.00631] TrialBench: Multi-Modal Artificial Intelligence-Ready Clinical Trial Datasets <https://arxiv.org/abs/2407.00631>`__ TrialBench:多模态人工智能就绪临床试验数据集

::

    Sun, 30 Jun 2024 09:13:10 GMT
    Jintai Chen, Yaojun Hu, Yue Wang, Yingzhou Lu, Xu Cao, Miao Lin, Hongxia Xu, Jian Wu, Cao Xiao, Jimeng Sun, Lucas Glass, Kexin Huang, Marinka Zitnik, Tianfan Fu

Clinical trials are pivotal for developing new medical treatments, yet they typically pose some risks such as patient mortality, adverse events, and enrollment failure that waste immense efforts spanning over a decade. Applying artificial intelligence (AI) to forecast or simulate key events in clinical trials holds great potential for providing insights to guide trial designs.
However, complex data collection and question definition requiring medical expertise and a deep understanding of trial designs have hindered the involvement of AI thus far. This paper tackles these challenges by presenting a comprehensive suite of meticulously curated AIready datasets covering multi-modal data (e.g., drug molecule, disease code, text, categorical/numerical features) and 8 crucial prediction challenges in clinical trial design, encompassing prediction of trial duration, patient dropout rate, serious adverse event, mortality rate, trial approval outcome, trial failure reason, drug dose finding, design of eligibility criteria. Furthermore, we provide basic validation methods for each task to ensure the datasets' usability and reliability. We anticipate that the availability of such open-access datasets will catalyze the development of advanced AI approaches for clinical trial design, ultimately advancing clinical trial research and accelerating medical solution development. The curated dataset, metrics, and basic models are publicly available at https://github.com/ML2Health/ML2ClinicalTrials/tree/main/AI4Trial.

------------

`[2407.00928] FoldGPT: Simple and Effective Large Language Model Compression Scheme <https://arxiv.org/abs/2407.00928>`__ FoldGPT:简单有效的大型语言模型压缩方案

::

    Mon, 1 Jul 2024 03:17:53 GMT
    Songwei Liu, Chao Zeng, Lianqiang Li, Chenqian Yan, Lean Fu, Xing Mei, Fangmin Chen

The demand for deploying large language models(LLMs) on mobile devices continues to increase, driven by escalating data security concerns and cloud costs. However, network bandwidth and memory limitations pose challenges for deploying billion-level models on mobile devices. In this study, we investigate the outputs of different layers across various scales of LLMs and found that the outputs of most layers exhibit significant similarity. Moreover, this similarity becomes more pronounced as the model size increases, indicating substantial redundancy in the depth direction of the LLMs. Based on this observation, we propose an efficient model volume compression strategy, termed FoldGPT, which combines block removal and block parameter sharing.This strategy consists of three parts: (1) Based on the learnable gating parameters, we determine the block importance ranking while modeling the coupling effect between blocks. Then we delete some redundant layers based on the given removal rate. (2) For the retained blocks, we apply a specially designed group parameter sharing strategy, where blocks within the same group share identical weights, significantly compressing the number of parameters and slightly reducing latency overhead. (3) After sharing these Blocks, we "cure" the mismatch caused by sparsity with a minor amount of fine-tuning and introduce a tail-layer distillation strategy to improve the performance. Experiments demonstrate that FoldGPT outperforms previous state-of-the-art(SOTA) methods in efficient model compression, demonstrating the feasibility of achieving model lightweighting through straightforward block removal and parameter sharing.

------------

`[2407.01031] PocketLLM: Enabling On-Device Fine-Tuning for Personalized LLMs <https://arxiv.org/abs/2407.01031>`__ PocketLLM:启用设备上的个性化llm微调

::

    Mon, 1 Jul 2024 07:26:56 GMT
    Dan Peng, Zhihui Fu, Jun Wang

Recent advancements in large language models (LLMs) have indeed showcased their impressive capabilities. On mobile devices, the wealth of valuable, non-public data generated daily holds great promise for locally fine-tuning personalized LLMs, while maintaining privacy through on-device processing.
However, the constraints of mobile device resources pose challenges to direct on-device LLM fine-tuning, mainly due to the memory-intensive nature of derivative-based optimization required for saving gradients and optimizer states. To tackle this, we propose employing derivative-free optimization techniques to enable on-device fine-tuning of LLM, even on memory-limited mobile devices. Empirical results demonstrate that the RoBERTa-large model and OPT-1.3B can be fine-tuned locally on the OPPO Reno 6 smartphone using around 4GB and 6.5GB of memory respectively, using derivative-free optimization techniques. This highlights the feasibility of on-device LLM fine-tuning on mobile devices, paving the way for personalized LLMs on resource-constrained devices while safeguarding data privacy.

------------

`[2407.01085] Rethinking LLM-based Preference Evaluation <https://arxiv.org/abs/2407.01085>`__ 基于llm的偏好评估的再思考

::

    Mon, 1 Jul 2024 08:37:41 GMT
    Zhengyu Hu, Linxin Song, Jieyu Zhang, Zheyuan Xiao, Jingang Wang, Zhenyu Chen, Jieyu Zhao, Hui Xiong

Recently, large language model (LLM)-based preference evaluation has been widely adopted to compare pairs of model responses. However, a severe bias towards lengthy responses has been observed, raising concerns about the reliability of this evaluation method. In this work, we designed a series of controlled experiments to study the major impacting factors of the metric of LLM-based preference evaluation, i.e., win rate, and conclude that the win rate is affected by two axes of model response: desirability and information mass, where the former is length-independent and related to trustworthiness, and the latter is length-dependent and can be represented by conditional entropy. We find that length impacts the existing evaluations by influencing information mass. However, a reliable evaluation metric should not only assess content quality but also ensure that the assessment is not confounded by extraneous factors such as response length. Therefore, we propose a simple yet effective adjustment, AdapAlpaca, to the existing practice of win rate measurement.
Specifically, by adjusting the lengths of reference answers to match the test model's answers within the same interval, we debias information mass relative to length, ensuring a fair model evaluation.

------------

`[2407.01155] CPT: Consistent Proxy Tuning for Black-box Optimization <https://arxiv.org/abs/2407.01155>`__ CPT:黑盒优化的一致性代理调优

::

    Mon, 1 Jul 2024 10:23:14 GMT
    Yuanyang He, Zitong Huang, Xinxing Xu, Rick Siow Mong Goh, Salman Khan, Wangmeng Zuo, Yong Liu, Chun-Mei Feng

Black-box tuning has attracted recent attention due to that the structure or inner parameters of advanced proprietary models are not accessible.
Proxy-tuning provides a test-time output adjustment for tuning black-box language models. It applies the difference of the output logits before and after tuning a smaller white-box "proxy" model to improve the black-box model.
However, this technique serves only as a decoding-time algorithm, leading to an inconsistency between training and testing which potentially limits overall performance. To address this problem, we introduce Consistent Proxy Tuning (CPT), a simple yet effective black-box tuning method. Different from Proxy-tuning, CPT additionally exploits the frozen large black-box model and another frozen small white-box model, ensuring consistency between training-stage optimization objective and test-time proxies. This consistency benefits Proxy-tuning and enhances model performance. Note that our method focuses solely on logit-level computation, which makes it model-agnostic and applicable to any task involving logit classification. Extensive experimental results demonstrate the superiority of our CPT in both black-box tuning of Large Language Models (LLMs) and Vision-Language Models (VLMs) across various datasets. The code is available at https://github.com/chunmeifeng/CPT.

------------

`[2407.01376] Badllama 3: removing safety finetuning from Llama 3 in minutes <https://arxiv.org/abs/2407.01376>`__ badlama 3:在几分钟内移除Llama 3的安全微调

::

    Mon, 1 Jul 2024 15:29:45 GMT
    Dmitrii Volkov

We show that extensive LLM safety fine-tuning is easily subverted when an attacker has access to model weights. We evaluate three state-of-the-art fine-tuning methods-QLoRA, ReFT, and Ortho-and show how algorithmic advances enable constant jailbreaking performance with cuts in FLOPs and optimisation power. We strip safety fine-tuning from Llama 3 8B in one minute and Llama 3 70B in 30 minutes on a single GPU, and sketch ways to reduce this further.

------------

`[2407.00010] Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads <https://arxiv.org/abs/2407.00010>`__ 混合异构集群可以降低LLM推理工作负载的能耗

::

    Thu, 25 Apr 2024 11:24:08 GMT
    Grant Wilkins, Srinivasan Keshav, and Richard Mortier

Both the training and use of Large Language Models (LLMs) require large amounts of energy. Their increasing popularity, therefore, raises critical concerns regarding the energy efficiency and sustainability of data centers that host them. This paper addresses the challenge of reducing energy consumption in data centers running LLMs. We propose a hybrid data center model that uses a cost-based scheduling framework to dynamically allocate LLM tasks across hardware accelerators that differ in their energy efficiencies and computational capabilities. Specifically, our workload-aware strategy determines whether tasks are processed on energy-efficient processors or high-performance GPUs based on the number of input and output tokens in a query. Our analysis of a representative LLM dataset, finds that this hybrid strategy can reduce CPU+GPU energy consumption by 7.5% compared to a workload-unaware baseline.

------------

`[2407.00066] Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead <https://arxiv.org/abs/2407.00066>`__ 压缩然后提供服务:以很少的开销提供数千个LoRA适配器

::

    Mon, 17 Jun 2024 15:21:35 GMT
    Rickard Br\"uel-Gabrielsson, Jiacheng Zhu, Onkar Bhardwaj, Leshem Choshen, Kristjan Greenewald, Mikhail Yurochkin and Justin Solomon

Fine-tuning large language models (LLMs) with low-rank adapters (LoRAs) has become common practice, often yielding numerous copies of the same LLM differing only in their LoRA updates. This paradigm presents challenges for systems that serve real-time responses to queries that each involve a different LoRA. Prior works optimize the design of such systems but still require continuous loading and offloading of LoRAs, as it is infeasible to store thousands of LoRAs in GPU memory. To mitigate this issue, we investigate the efficacy of compression when serving LoRA adapters. We consider compressing adapters individually via SVD and propose a method for joint compression of LoRAs into a shared basis paired with LoRA-specific scaling matrices. Our experiments with up to 500 LoRAs demonstrate that compressed LoRAs preserve performance while offering major throughput gains in realistic serving scenarios with over a thousand LoRAs, maintaining 75% of the throughput of serving a single LoRA.

------------

`[2407.00079] Mooncake: Kimi's KVCache-centric Architecture for LLM Serving <https://arxiv.org/abs/2407.00079>`__ 

::

    Mon, 24 Jun 2024 02:05:32 GMT
    Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, Xinran Xu

Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI. It features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. It also leverages the underutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated cache of KVCache. The core of Mooncake is its KVCache-centric scheduler, which balances maximizing overall effective throughput while meeting latency-related Service Level Objectives (SLOs). Unlike traditional studies that assume all requests will be processed, Mooncake faces challenges due to highly overloaded scenarios. To mitigate these, we developed a prediction-based early rejection policy. Experiments show that Mooncake excels in long-context scenarios. Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs.
Under real workloads, Mooncake's innovative architecture enables Kimi to handle 75% more requests.

------------

`[2407.00088] T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge <https://arxiv.org/abs/2407.00088>`__ T-MAC:通过表查找实现边缘低比特LLM部署的CPU复兴

::

    Tue, 25 Jun 2024 08:38:38 GMT
    Jianyu Wei, Shijie Cao, Ting Cao, Lingxiao Ma, Lei Wang, Yanyong Zhang and Mao Yang

The deployment of Large Language Models (LLMs) on edge devices is increasingly important to enhance on-device intelligence. Weight quantization is crucial for reducing the memory footprint of LLMs on devices. However, low-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low precision weights and high precision activations during inference. Existing systems, lacking native support for mpGEMM, resort to dequantize weights for high precision computation. Such an indirect way can lead to a significant inference overhead.
In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based method designed for efficient low-bit LLM (i.e., weight-quantized LLM) inference on CPUs. T-MAC directly supports mpGEMM without dequantization, while simultaneously eliminating multiplications and reducing additions required.
Specifically, T-MAC transforms the traditional data-type-centric multiplication to bit-wise table lookup, and enables a unified and scalable mpGEMM solution.
Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on low-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in throughput and 70% reduction in energy consumption compared to llama.cpp. For BitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s with a single core and 71 tokens/s with eight cores on M2-Ultra, and 11 tokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds the adult average reading speed. T-MAC with LUT-based computing paradigm, paves the way for the practical deployment of low-bit LLMs on resource-constrained edge devices without compromising computational efficiency. The system is open-sourced at https://github.com/microsoft/T-MAC.

------------

`[2407.00110] Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services <https://arxiv.org/abs/2407.00110>`__ Chat AI:面向高性能计算服务的无缝Slurm-Native解决方案

::

    Thu, 27 Jun 2024 12:08:21 GMT
    Ali Doosthosseini, Jonathan Decker, Hendrik Nolte, Julian M. Kunkel

The increasing adoption of large language models (LLMs) has created a pressing need for an efficient, secure and private serving infrastructure, which allows researchers to run open-source or custom fine-tuned LLMs and ensures users that their data remains private and is not stored without their consent. While high-performance computing (HPC) systems equipped with state-of-the-art GPUs are well-suited for training LLMs, their batch scheduling paradigm is not designed to support real-time serving of AI applications. Cloud systems, on the other hand, are well suited for web services but commonly lack access to the computational power of clusters, especially expensive and scarce high-end GPUs, which are required for optimal inference speed. We propose an architecture with an implementation consisting of a web service that runs on a cloud VM with secure access to a scalable backend running a multitude of AI models on HPC systems. By offering a web service using our HPC infrastructure to host LLMs, we leverage the trusted environment of local universities and research centers to offer a private and secure alternative to commercial LLM services. Our solution natively integrates with Slurm, enabling seamless deployment on HPC clusters and is able to run side by side with regular Slurm workloads, while utilizing gaps in the schedule created by Slurm. In order to ensure the security of the HPC system, we use the SSH ForceCommand directive to construct a robust circuit breaker, which prevents successful attacks on the web-facing server from affecting the cluster. We have successfully deployed our system as a production service, and made the source code available at https://github.com/gwdg/chat-ai

------------

`[2407.00128] When Search Engine Services meet Large Language Models: Visions and Challenges <https://arxiv.org/abs/2407.00128>`__ 搜索引擎服务遇到大型语言模型时的设想与挑战

::

    Fri, 28 Jun 2024 03:52:13 GMT
    Haoyi Xiong, Jiang Bian, Yuchen Li, Xuhong Li, Mengnan Du, Shuaiqiang Wang, Dawei Yin, Sumi Helal

Combining Large Language Models (LLMs) with search engine services marks a significant shift in the field of services computing, opening up new possibilities to enhance how we search for and retrieve information, understand content, and interact with internet services. This paper conducts an in-depth examination of how integrating LLMs with search engines can mutually benefit both technologies. We focus on two main areas: using search engines to improve LLMs (Search4LLM) and enhancing search engine functions using LLMs (LLM4Search). For Search4LLM, we investigate how search engines can provide diverse high-quality datasets for pre-training of LLMs, how they can use the most relevant documents to help LLMs learn to answer queries more accurately, how training LLMs with Learning-To-Rank (LTR) tasks can enhance their ability to respond with greater precision, and how incorporating recent search results can make LLM-generated content more accurate and current. In terms of LLM4Search, we examine how LLMs can be used to summarize content for better indexing by search engines, improve query outcomes through optimization, enhance the ranking of search results by analyzing document relevance, and help in annotating data for learning-to-rank tasks in various learning contexts.
However, this promising integration comes with its challenges, which include addressing potential biases and ethical issues in training models, managing the computational and other costs of incorporating LLMs into search services, and continuously updating LLM training with the ever-changing web content. We discuss these challenges and chart out required research directions to address them. We also discuss broader implications for service computing, such as scalability, privacy concerns, and the need to adapt search engine architectures for these advanced models.

------------

`[2407.00456] Beyond Functional Correctness: Investigating Coding Style Inconsistencies in Large Language Models <https://arxiv.org/abs/2407.00456>`__ 超越功能正确性:研究大型语言模型中编码风格的不一致性

::

    Sat, 29 Jun 2024 14:56:11 GMT
    Yanlin Wang, Tianyue Jiang, Mingwei Liu, Jiachi Chen, Zibin Zheng

Large language models (LLMs) have brought a paradigm shift to the field of code generation, offering the potential to enhance the software development process. However, previous research mainly focuses on the accuracy of code generation, while coding style differences between LLMs and human developers remain under-explored. In this paper, we empirically analyze the differences in coding style between the code generated by mainstream Code LLMs and the code written by human developers, and summarize coding style inconsistency taxonomy.
Specifically, we first summarize the types of coding style inconsistencies by manually analyzing a large number of generation results. We then compare the code generated by Code LLMs with the code written by human programmers in terms of readability, conciseness, and robustness. The results reveal that LLMs and developers have different coding styles. Additionally, we study the possible causes of these inconsistencies and provide some solutions to alleviate the problem.

------------

`[2407.00808] Exploring a Physics-Informed Decision Transformer for Distribution System Restoration: Methodology and Performance Analysis <https://arxiv.org/abs/2407.00808>`__ 配电系统恢复的物理决策Transformer探索:方法和性能分析

::

    Sun, 30 Jun 2024 19:27:06 GMT
    Hong Zhao, Jin Wei-Kocsis, Adel Heidari Akhijahani, and Karen L Butler-Purry

Driven by advancements in sensing and computing, deep reinforcement learning (DRL)-based methods have demonstrated significant potential in effectively tackling distribution system restoration (DSR) challenges under uncertain operational scenarios. However, the data-intensive nature of DRL poses obstacles in achieving satisfactory DSR solutions for large-scale, complex distribution systems. Inspired by the transformative impact of emerging foundation models, including large language models (LLMs), across various domains, this paper explores an innovative approach harnessing LLMs' powerful computing capabilities to address scalability challenges inherent in conventional DRL methods for solving DSR. To our knowledge, this study represents the first exploration of foundation models, including LLMs, in revolutionizing conventional DRL applications in power system operations. Our contributions are twofold: 1) introducing a novel LLM-powered Physics-Informed Decision Transformer (PIDT) framework that leverages LLMs to transform conventional DRL methods for DSR operations, and 2) conducting comparative studies to assess the performance of the proposed LLM-powered PIDT framework at its initial development stage for solving DSR problems. While our primary focus in this paper is on DSR operations, the proposed PIDT framework can be generalized to optimize sequential decision-making across various power system operations.

------------

`[2407.01280] Human-Robot Mutual Learning through Affective-Linguistic Interaction and Differential Outcomes Training [Pre-Print] <https://arxiv.org/abs/2407.01280>`__ 通过情感语言交互和差异结果训练的人机相互学习[预印本]

::

    Mon, 1 Jul 2024 13:35:08 GMT
    Emilia Heikkinen, Elsa Silvennoinen, Imran Khan, Zakaria Lemhaouri, Laura Cohen, Lola Ca\~namero, and Robert Lowe

Owing to the recent success of Large Language Models, Modern A.I has been much focused on linguistic interactions with humans but less focused on non-linguistic forms of communication between man and machine. In the present paper, we test how affective-linguistic communication, in combination with differential outcomes training, affects mutual learning in a human-robot context. Taking inspiration from child-caregiver dynamics, our human-robot interaction setup consists of a (simulated) robot attempting to learn how best to communicate internal, homeostatically-controlled needs; while a human "caregiver" attempts to learn the correct object to satisfy the robot's present communicated need. We studied the effects of i) human training type, and ii) robot reinforcement learning type, to assess mutual learning terminal accuracy and rate of learning (as measured by the average reward achieved by the robot).
Our results find mutual learning between a human and a robot is significantly improved with Differential Outcomes Training (DOT) compared to Non-DOT (control) conditions. We find further improvements when the robot uses an exploration-exploitation policy selection, compared to purely exploitation policy selection. These findings have implications for utilizing socially assistive robots (SAR) in therapeutic contexts, e.g. for cognitive interventions, and educational applications.

------------

`[2407.00047] One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving <https://arxiv.org/abs/2407.00047>`__ 一个队列就是你所需要的:解决大型语言模型服务中的队首阻塞

::

    Wed, 5 Jun 2024 21:17:34 GMT
    Archit Patke, Dhemath Reddy, Saurabh Jha, Haoran Qiu, Christian Pinto, Shengkun Cui, Chandra Narayanaswami, Zbigniew Kalbarczyk, Ravishankar Iyer

$ $Large language models (LLMs) have become an increasingly important workload for cloud providers catering to both enterprise and consumer applications. LLM inference requests from these applications have end-to-end latency SLOs that must be adhered to in production settings. However, existing LLM serving systems focus on optimization objectives such as request serving throughput or request execution latency rather than the end-to-end latency SLOs. Achieving end-to-end SLOs for latency-sensitive requests is challenging due to head-of-line (HOL) blocking in the request queue, which results from bursty arrival rates and insufficient resources.
To address the above challenge, we propose QLM, a multi-model queue management framework for LLM serving. QLM uses stochastic programming to orchestrate the actions of multiple LLM Serving Operations (LSOs) to reduce HOL blocking and maximize SLO attainment. Specifically, QLM uses the following LSOs: model swapping, request eviction, GPU-CPU state swapping, load balancing, and warm model start. Evaluation on heterogeneous GPU devices and models with real-world LLM serving dataset shows that QLM improves SLO attainment by 40-90% and throughput by 20-400% while maintaining or improving device utilization compared to other state-of-the-art LLM serving systems.

------------

`[2407.01394] Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing <https://arxiv.org/abs/2407.01394>`__ glos2text:使用llm和语义感知标签平滑的手语Gloss翻译

::

    Mon, 1 Jul 2024 15:46:45 GMT
    Pooya Fayyazsanavi, Antonios Anastasopoulos, Jana Ko\v{s}eck\'a

Sign language translation from video to spoken text presents unique challenges owing to the distinct grammar, expression nuances, and high variation of visual appearance across different speakers and contexts. The intermediate gloss annotations of videos aim to guide the translation process.
In our work, we focus on {\em Gloss2Text} translation stage and propose several advances by leveraging pre-trained large language models (LLMs), data augmentation, and novel label-smoothing loss function exploiting gloss translation ambiguities improving significantly the performance of state-of-the-art approaches. Through extensive experiments and ablation studies on the PHOENIX Weather 2014T dataset, our approach surpasses state-of-the-art performance in {\em Gloss2Text} translation, indicating its efficacy in addressing sign language translation and suggesting promising avenues for future research and development.

------------

`[2407.01509] MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs <https://arxiv.org/abs/2407.01509>`__ MIA-Bench:多模态llm评估后的更好指导

::

    Mon, 1 Jul 2024 17:53:35 GMT
    Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, Zhe Gan

We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large language models (MLLMs) on their ability to strictly adhere to complex instructions. Our benchmark comprises a diverse set of 400 image-prompt pairs, each crafted to challenge the models' compliance with layered instructions in generating accurate responses that satisfy specific requested patterns.
Evaluation results from a wide array of state-of-the-art MLLMs reveal significant variations in performance, highlighting areas for improvement in instruction fidelity. Additionally, we create extra training data and explore supervised fine-tuning to enhance the models' ability to strictly follow instructions without compromising performance on other tasks. We hope this benchmark not only serves as a tool for measuring MLLM adherence to instructions, but also guides future developments in MLLM training methods.

------------

`[2407.00215] LLM Critics Help Catch LLM Bugs <https://arxiv.org/abs/2407.00215>`__ LLM批评者帮助捕捉LLM的bug

::

    Fri, 28 Jun 2024 19:53:17 GMT
    Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, Jan Leike

Reinforcement learning from human feedback (RLHF) is fundamentally limited by the capacity of humans to correctly evaluate model output. To improve human evaluation ability and overcome that limitation this work trains "critic" models that help humans to more accurately evaluate model-written code. These critics are themselves LLMs trained with RLHF to write natural language feedback highlighting problems in code from real-world assistant tasks. On code containing naturally occurring LLM errors model-written critiques are preferred over human critiques in 63% of cases, and human evaluation finds that models catch more bugs than human contractors paid for code review. We further confirm that our fine-tuned LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as "flawless", even though the majority of those tasks are non-code tasks and thus out-of-distribution for the critic model.
Critics can have limitations of their own, including hallucinated bugs that could mislead humans into making mistakes they might have otherwise avoided, but human-machine teams of critics and contractors catch similar numbers of bugs to LLM critics while hallucinating less than LLMs alone.

------------

`[2407.00634] Tarsier: Recipes for Training and Evaluating Large Video Description Models <https://arxiv.org/abs/2407.00634>`__ Tarsier:训练和评估大型视频描述模型的方法

::

    Sun, 30 Jun 2024 09:21:01 GMT
    Jiawei Wang, Liping Yuan, Yuchen Zhang

Generating fine-grained video descriptions is a fundamental challenge in video understanding. In this work, we introduce Tarsier, a family of large-scale video-language models designed to generate high-quality video descriptions. Tarsier employs CLIP-ViT to encode frames separately and then uses an LLM to model temporal relationships. Despite its simple architecture, we demonstrate that with a meticulously designed two-stage training procedure, the Tarsier models exhibit substantially stronger video description capabilities than any existing open-source model, showing a $+51.4\%$ advantage in human side-by-side evaluation over the strongest model. Additionally, they are comparable to state-of-the-art proprietary models, with a $+12.3\%$ advantage against GPT-4V and a $-6.7\%$ disadvantage against Gemini 1.5 Pro.
Besides video description, Tarsier proves to be a versatile generalist model, achieving new state-of-the-art results across nine public benchmarks, including multi-choice VQA, open-ended VQA, and zero-shot video captioning. Our second contribution is the introduction of a new benchmark for evaluating video description models, consisting of a new challenging dataset featuring videos from diverse sources and varying complexity, along with an automatic method specifically designed to assess the quality of fine-grained video descriptions.
We make our models and evaluation benchmark publicly available at \url{https://github.com/bytedance/tarsier}.

------------

`[2407.00890] Macroeconomic Forecasting with Large Language Models <https://arxiv.org/abs/2407.00890>`__ 基于大型语言模型的宏观经济预测

::

    Mon, 1 Jul 2024 01:25:26 GMT
    Andrea Carriero and Davide Pettenuzzo and Shubhranshu Shekhar

This paper presents a comparative analysis evaluating the accuracy of Large Language Models (LLMs) against traditional macro time series forecasting approaches. In recent times, LLMs have surged in popularity for forecasting due to their ability to capture intricate patterns in data and quickly adapt across very different domains. However, their effectiveness in forecasting macroeconomic time series data compared to conventional methods remains an area of interest. To address this, we conduct a rigorous evaluation of LLMs against traditional macro forecasting methods, using as common ground the FRED-MD database. Our findings provide valuable insights into the strengths and limitations of LLMs in forecasting macroeconomic time series, shedding light on their applicability in real-world scenarios

------------

`[2310.17884] Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory <https://arxiv.org/abs/2310.17884>`__ llm能保守秘密吗?通过上下文完整性理论测试语言模型的隐私含义

::

    replaced with revised version Fri, 28 Jun 2024 23:27:10 GMT
    Submission history From: Hyunwoo Kim [view email]
    [v1] Fri, 27 Oct 2023 04:15:30 UTC (3,261 KB)
    [v2] Fri, 28 Jun 2024 23:27:10 UTC (3,263 KB)
    Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi

The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.

------------

`[2402.04858] CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay <https://arxiv.org/abs/2402.04858>`__ CodeIt:具有优先后见之明重放的自我改进语言模型

::

    replaced with revised version Mon, 1 Jul 2024 10:03:33 GMT
    Submission history From: Michaël Defferrard [view email]
    [v1] Wed, 7 Feb 2024 13:55:27 UTC (792 KB)
    [v2] Mon, 1 Jul 2024 10:03:33 UTC (3,539 KB)
    Natasha Butt, Blazej Manczak, Auke Wiggers, Corrado Rainone, David W. Zhang, Micha\"el Defferrard, Taco Cohen

Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-symbolic approach that scales to the full ARC evaluation dataset. Our method solves 15% of ARC evaluation tasks, achieving state-of-the-art performance and outperforming existing neural and symbolic baselines. Our code is available at this https URL .

------------

`[2404.17524] On the Use of Large Language Models to Generate Capability Ontologies <https://arxiv.org/abs/2404.17524>`__ 利用大型语言模型生成能力本体

::

    replaced with revised version Sun, 30 Jun 2024 10:58:28 GMT
    Submission history From: Luis Miguel Vieira da Silva [view email]
    [v1] Fri, 26 Apr 2024 16:41:00 UTC (682 KB)
    [v2] Mon, 29 Apr 2024 08:50:50 UTC (682 KB)
    [v3] Sun, 30 Jun 2024 10:58:28 UTC (682 KB)
    Luis Miguel Vieira da Silva, Aljosha K\"ocher, Felix Gehlhoff, Alexander Fay

Capability ontologies are increasingly used to model functionalities of systems or machines. The creation of such ontological models with all properties and constraints of capabilities is very complex and can only be done by ontology experts. However, Large Language Models (LLMs) have shown that they can generate machine-interpretable models from natural language text input and thus support engineers / ontology experts. Therefore, this paper investigates how LLMs can be used to create capability ontologies. We present a study with a series of experiments in which capabilities with varying complexities are generated using different prompting techniques and with different LLMs. Errors in the generated ontologies are recorded and compared. To analyze the quality of the generated ontologies, a semi-automated approach based on RDF syntax checking, OWL reasoning, and SHACL constraints is used. The results of this study are very promising because even for complex capabilities, the generated ontologies are almost free of errors.

------------

`[2404.19721] PANGeA: Procedural Artificial Narrative using Generative AI for Turn-Based Video Games <https://arxiv.org/abs/2404.19721>`__ PANGeA:回合制电子游戏中使用生成AI的程序人工叙事

::

    replaced with revised version Sun, 30 Jun 2024 20:54:57 GMT
    Submission history From: Steph Buongiorno [view email]
    [v1] Tue, 30 Apr 2024 17:11:54 UTC (995 KB)
    [v2] Sun, 30 Jun 2024 20:54:57 UTC (1,350 KB)
    [v3] Tue, 9 Jul 2024 23:45:27 UTC (1,396 KB)
    Steph Buongiorno, Lawrence Jake Klinkert, Tanishq Chawla, Zixin Zhuang, Corey Clark

This research introduces Procedural Artificial Narrative using Generative AI (PANGeA), a structured approach for leveraging large language models (LLMs), guided by a game designer's high-level criteria, to generate narrative content for turn-based role-playing video games (RPGs). Distinct from prior applications of LLMs used for video game design, PANGeA innovates by not only generating game level data (which includes, but is not limited to, setting, key items, and non-playable characters (NPCs)), but by also fostering dynamic, free-form interactions between the player and the environment that align with the procedural game narrative. The NPCs generated by PANGeA are personality-biased and express traits from the Big 5 Personality Model in their generated responses. PANGeA addresses challenges behind ingesting free-form text input, which can prompt LLM responses beyond the scope of the game narrative. A novel validation system that uses the LLM's intelligence evaluates text input and aligns generated responses with the unfolding narrative. Making these interactions possible, PANGeA is supported by a server that hosts a custom memory system that supplies context for augmenting generated responses thus aligning them with the procedural narrative. For its broad application, the server has a REST interface enabling any game engine to integrate directly with PANGeA, as well as an LLM interface adaptable with local or private LLMs. PANGeA's ability to foster dynamic narrative generation by aligning responses with the procedural narrative is demonstrated through an empirical study and ablation test of two versions of a demo game. These are, a custom, browser-based GPT and a Unity demo. As the results show, PANGeA holds potential to assist game designers in using LLMs to generate narrative-consistent content even when provided varied and unpredictable, free-form text input.

------------

`[2405.17902] Boosting Protein Language Models with Negative Sample Mining <https://arxiv.org/abs/2405.17902>`__ 基于负样本挖掘的蛋白质语言模型增强

::

    replaced with revised version Sat, 29 Jun 2024 07:07:49 GMT
    Submission history From: Xinjian Zhao [view email]
    [v1] Tue, 28 May 2024 07:24:20 UTC (2,233 KB)
    [v2] Sat, 29 Jun 2024 07:07:49 UTC (1,623 KB)
    Yaoyao Xu, Xinjian Zhao, Xiaozhuang Song, Benyou Wang, Tianshu Yu

We introduce a pioneering methodology for boosting large language models in the domain of protein representation learning. Our primary contribution lies in the refinement process for correlating the over-reliance on co-evolution knowledge, in a way that networks are trained to distill invaluable insights from negative samples, constituted by protein pairs sourced from disparate categories. By capitalizing on this novel approach, our technique steers the training of transformer-based models within the attention score space. This advanced strategy not only amplifies performance but also reflects the nuanced biological behaviors exhibited by proteins, offering aligned evidence with traditional biological mechanisms such as protein-protein interaction. We experimentally observed improved performance on various tasks over datasets, on top of several well-established large protein models. This innovative paradigm opens up promising horizons for further progress in the realms of protein research and computational biology.

------------

`[2406.10162] Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models <https://arxiv.org/abs/2406.10162>`__ 对托词的谄媚:研究大型语言模型中的奖励篡改

::

    replaced with revised version Sat, 29 Jun 2024 00:28:47 GMT
    Submission history From: Carson Denison [view email]
    [v1] Fri, 14 Jun 2024 16:26:20 UTC (4,150 KB)
    [v2] Mon, 17 Jun 2024 16:13:29 UTC (4,380 KB)
    [v3] Sat, 29 Jun 2024 00:28:47 UTC (4,381 KB)
    Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Ethan Perez, Evan Hubinger

In reinforcement learning, specification gaming occurs when AI systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. In this paper, we study whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. We construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an LLM not to game early-curriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to our gameable environments does not prevent reward-tampering. These results demonstrate that LLMs can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.

------------

`[2406.19644] Beyond Human Preferences: Exploring Reinforcement Learning Trajectory Evaluation and Improvement through LLMs <https://arxiv.org/abs/2406.19644>`__ 超越人类偏好:通过llm探索强化学习轨迹评估和改进

::

    replaced with revised version Mon, 1 Jul 2024 03:32:48 GMT
    Submission history From: Zichao Shen [view email]
    [v1] Fri, 28 Jun 2024 04:21:24 UTC (1,579 KB)
    [v2] Mon, 1 Jul 2024 03:32:48 UTC (1,579 KB)
    Zichao Shen, Tianchen Zhu, Qingyun Sun, Shiqi Gao and Jianxin Li

Reinforcement learning (RL) faces challenges in evaluating policy trajectories within intricate game tasks due to the difficulty in designing comprehensive and precise reward functions. This inherent difficulty curtails the broader application of RL within game environments characterized by diverse constraints. Preference-based reinforcement learning (PbRL) presents a pioneering framework that capitalizes on human preferences as pivotal reward signals, thereby circumventing the need for meticulous reward engineering. However, obtaining preference data from human experts is costly and inefficient, especially under conditions marked by complex constraints. To tackle this challenge, we propose a LLM-enabled automatic preference generation framework named LLM4PG , which harnesses the capabilities of large language models (LLMs) to abstract trajectories, rank preferences, and reconstruct reward functions to optimize conditioned policies. Experiments on tasks with complex language constraints demonstrated the effectiveness of our LLM-enabled reward functions, accelerating RL convergence and overcoming stagnation caused by slow or absent progress under original reward structures. This approach mitigates the reliance on specialized human knowledge and demonstrates the potential of LLMs to enhance RL's effectiveness in complex environments in the wild.

------------

`[2305.13857] Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation <https://arxiv.org/abs/2305.13857>`__ 基于交互评估的任务导向型对话用户熟悉性偏差挖掘

::

    replaced with revised version Mon, 1 Jul 2024 09:23:10 GMT
    Submission history From: Takyoung Kim [view email]
    [v1] Tue, 23 May 2023 09:24:53 UTC (11,263 KB)
    [v2] Mon, 1 Jul 2024 09:23:10 UTC (11,265 KB)
    Takyoung Kim, Jamin Shin, Young-Ho Kim, Sanghwan Bae, Sungdong Kim

Most task-oriented dialogue (TOD) benchmarks assume users that know exactly how to use the system by constraining the user behaviors within the system's capabilities via strict user goals, namely "user familiarity" bias. This data bias deepens when it combines with data-driven TOD systems, as it is impossible to fathom the effect of it with existing static evaluations. Hence, we conduct an interactive user study to unveil how vulnerable TOD systems are against realistic scenarios. In particular, we compare users with 1) detailed goal instructions that conform to the system boundaries (closed-goal) and 2) vague goal instructions that are often unsupported but realistic (open-goal). Our study reveals that conversations in open-goal settings lead to catastrophic failures of the system, in which 92% of the dialogues had significant issues. Moreover, we conduct a thorough analysis to identify distinctive features between the two settings through error annotation. From this, we discover a novel "pretending" behavior, in which the system pretends to handle the user requests even though they are beyond the system's capabilities. We discuss its characteristics and toxicity while showing recent large language models can also suffer from this behavior.

------------

`[2309.05196] Does Writing with Language Models Reduce Content Diversity? <https://arxiv.org/abs/2309.05196>`__ 使用语言模型写作会降低内容多样性吗?

::

    replaced with revised version Mon, 1 Jul 2024 16:36:30 GMT
    Submission history From: Vishakh Padmakumar [view email]
    [v1] Mon, 11 Sep 2023 02:16:47 UTC (1,233 KB)
    [v2] Wed, 6 Mar 2024 20:48:40 UTC (1,260 KB)
    [v3] Mon, 1 Jul 2024 16:36:30 UTC (1,291 KB)
    Vishakh Padmakumar, He He

Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays. In contrast, the user-contributed text remains unaffected by model collaboration. This suggests that the recent improvement in generation quality from adapting models to human feedback might come at the cost of more homogeneous and less diverse content.

------------

`[2311.04886] SEMQA: Semi-Extractive Multi-Source Question Answering <https://arxiv.org/abs/2311.04886>`__ 

::

    replaced with revised version Sun, 30 Jun 2024 18:53:22 GMT
    Submission history From: Tal Schuster [view email]
    [v1] Wed, 8 Nov 2023 18:46:32 UTC (9,544 KB)
    [v2] Sun, 30 Jun 2024 18:53:22 UTC (9,549 KB)
    Tal Schuster, Adam D. Lelkes, Haitian Sun, Jai Gupta, Jonathan Berant, William W. Cohen, Donald Metzler

Recently proposed long-form question answering (QA) systems, supported by large language models (LLMs), have shown promising capabilities. Yet, attributing and verifying their generated abstractive answers can be difficult, and automatically evaluating their accuracy remains an ongoing challenge.
In this work, we introduce a new QA task for answering multi-answer questions by summarizing multiple diverse sources in a semi-extractive fashion. Specifically, Semi-extractive Multi-source QA (SEMQA) requires models to output a comprehensive answer, while mixing factual quoted spans -- copied verbatim from given input sources -- and non-factual free-text connectors that glue these spans together into a single cohesive passage. This setting bridges the gap between the outputs of well-grounded but constrained extractive QA systems and more fluent but harder to attribute fully abstractive answers. Particularly, it enables a new mode for language models that leverages their advanced language generation capabilities, while also producing fine in-line attributions by-design that are easy to verify, interpret, and evaluate.
To study this task, we create the first dataset of this kind, QuoteSum, with human-written semi-extractive answers to natural and generated questions, and define text-based evaluation metrics. Experimenting with several LLMs in various settings, we find this task to be surprisingly challenging, demonstrating the importance of QuoteSum for developing and studying such consolidation capabilities.

------------

`[2311.07138] WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models <https://arxiv.org/abs/2311.07138>`__ WaterBench:面向大型语言模型的水印整体评估

::

    replaced with revised version Mon, 1 Jul 2024 03:17:42 GMT
    Submission history From: Shangqing Tu [view email]
    [v1] Mon, 13 Nov 2023 08:09:01 UTC (653 KB)
    [v2] Mon, 1 Jul 2024 03:17:42 UTC (791 KB)
    Shangqing Tu, Yuliang Sun, Yushi Bai, Jifan Yu, Lei Hou, Juanzi Li

To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For benchmarking procedure, to ensure an apples-to-apples comparison, we first adjust each watermarking method's hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For task selection, we diversify the input and output length to form a five-category taxonomy, covering $9$ tasks. (3) For evaluation metric, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate $4$ open-source watermarks on $2$ LLMs under $2$ watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at this https URL.

------------

`[2311.09022] Exploring the Potential of Large Language Models in Computational Argumentation <https://arxiv.org/abs/2311.09022>`__ 探索大型语言模型在计算辩论中的潜力

::

    replaced with revised version Mon, 1 Jul 2024 09:40:58 GMT
    Submission history From: Guizhen Chen [view email]
    [v1] Wed, 15 Nov 2023 15:12:15 UTC (651 KB)
    [v2] Fri, 15 Mar 2024 10:00:04 UTC (201 KB)
    [v3] Mon, 1 Jul 2024 09:40:58 UTC (209 KB)
    Guizhen Chen, Liying Cheng, Luu Anh Tuan, Lidong Bing

Computational argumentation has become an essential tool in various domains, including law, public policy, and artificial intelligence. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated impressive capabilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on diverse computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both zero-shot and few-shot settings. We organize existing tasks into six main categories and standardize the format of fourteen openly available datasets. In addition, we present a new benchmark dataset on counter speech generation that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation. Our analysis offers valuable suggestions for evaluating computational argumentation and its integration with LLMs in future research endeavors.

------------

`[2311.09069] How Well Do Large Language Models Truly Ground? <https://arxiv.org/abs/2311.09069>`__ 大型语言模型的真正基础是什么?

::

    replaced with revised version Sat, 29 Jun 2024 18:07:34 GMT
    Submission history From: Hyunji Lee [view email]
    [v1] Wed, 15 Nov 2023 16:11:27 UTC (3,858 KB)
    [v2] Sat, 29 Jun 2024 18:07:34 UTC (3,276 KB)
    Hyunji Lee, Sejune Joo, Chaeeun Kim, Joel Jang, Doyoung Kim, Kyoung-Woon On, Minjoon Seo

To reduce issues like hallucinations and lack of control in Large Language Models (LLMs), a common method is to generate responses by grounding on external contexts given as input, known as knowledge-augmented models. However, previous research often narrowly defines "grounding" as just having the correct answer, which does not ensure the reliability of the entire response. To overcome this, we propose a stricter definition of grounding: a model is truly grounded if it (1) fully utilizes the necessary knowledge from the provided context, and (2) stays within the limits of that knowledge. We introduce a new dataset and a grounding metric to evaluate model capability under the definition. We perform experiments across 25 LLMs of different sizes and training methods and provide insights into factors that influence grounding performance. Our findings contribute to a better understanding of how to improve grounding capabilities and suggest an area of improvement toward more reliable and controllable LLM applications.

------------

`[2312.00738] SeaLLMs -- Large Language Models for Southeast Asia <https://arxiv.org/abs/2312.00738>`__ SeaLLMs—东南亚大型语言模型

::

    replaced with revised version Mon, 1 Jul 2024 05:52:31 GMT
    Submission history From: Xuan Phi Nguyen [view email]
    [v1] Fri, 1 Dec 2023 17:17:56 UTC (4,417 KB)
    [v2] Mon, 1 Jul 2024 05:52:31 UTC (6,918 KB)
    Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Zhiqiang Hu, Chenhui Shen, Yew Ken Chia, Xingxuan Li, Jianyu Wang, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, Lidong Bing

Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon the Llama-2 model and further advanced through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM-13b models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate.

------------

`[2312.11985] Climate Change from Large Language Models <https://arxiv.org/abs/2312.11985>`__ 大型语言模型的气候变化

::

    replaced with revised version Mon, 1 Jul 2024 10:40:19 GMT
    Submission history From: Hongyin Zhu [view email]
    [v1] Tue, 19 Dec 2023 09:26:46 UTC (809 KB)
    [v2] Wed, 20 Dec 2023 05:27:30 UTC (809 KB)
    [v3] Mon, 1 Jul 2024 10:40:19 UTC (745 KB)
    Hongyin Zhu, Prayag Tiwari

Climate change poses grave challenges, demanding widespread understanding and low-carbon lifestyle awareness. Large language models (LLMs) offer a powerful tool to address this crisis, yet comprehensive evaluations of their climate-crisis knowledge are lacking. This paper proposes an automated evaluation framework to assess climate-crisis knowledge within LLMs. We adopt a hybrid approach for data acquisition, combining data synthesis and manual collection, to compile a diverse set of questions encompassing various aspects of climate change. Utilizing prompt engineering based on the compiled questions, we evaluate the model's knowledge by analyzing its generated answers. Furthermore, we introduce a comprehensive set of metrics to assess climate-crisis knowledge, encompassing indicators from 10 distinct perspectives. These metrics provide a multifaceted evaluation, enabling a nuanced understanding of the LLMs' climate crisis comprehension. The experimental results demonstrate the efficacy of our proposed method. In our evaluation utilizing diverse high-performing LLMs, we discovered that while LLMs possess considerable climate-related knowledge, there are shortcomings in terms of timeliness, indicating a need for continuous updating and refinement of their climate-related content.

------------

`[2401.01218] Self-Supervised Position Debiasing for Large Language Models <https://arxiv.org/abs/2401.01218>`__ 大型语言模型的自监督位置去偏

::

    replaced with revised version Sat, 29 Jun 2024 05:20:09 GMT
    Submission history From: Zhongkun Liu [view email]
    [v1] Tue, 2 Jan 2024 14:12:41 UTC (6,855 KB)
    [v2] Thu, 15 Feb 2024 08:04:13 UTC (7,039 KB)
    [v3] Sat, 29 Jun 2024 05:20:09 UTC (6,769 KB)
    Zhongkun Liu, Zheng Chen, Mengqi Zhang, Zhaochun Ren, Pengjie Ren, Zhumin Chen

Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Previous works have proven that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing debiasing methods for LLMs require external bias knowledge or annotated non-biased samples, which is lacking for position debiasing and impractical in reality. In this work, we propose a self-supervised position debiasing (SOD) framework to mitigate position bias for LLMs. SOD leverages unsupervised responses from pre-trained LLMs for debiasing without relying on any external knowledge. To improve the quality of unsupervised responses, we propose an objective alignment (OAM) module to prune these responses. Experiments on eight datasets and five tasks show that SOD consistently outperforms existing methods in mitigating three types of position biases. Besides, SOD achieves this by sacrificing only a small performance on biased samples, which is general and effective. To facilitate the reproducibility of the results, we share the code of all methods and datasets on this https URL.

------------

`[2401.12070] Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text <https://arxiv.org/abs/2401.12070>`__ 用双筒望远镜识别llm:机器生成文本的零样本检测

::

    replaced with revised version Mon, 1 Jul 2024 15:17:10 GMT
    Submission history From: Avi Schwarzschild [view email]
    [v1] Mon, 22 Jan 2024 16:09:47 UTC (4,003 KB)
    [v2] Mon, 1 Jul 2024 15:17:10 UTC (2,405 KB)
    Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein

Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data.

------------

`[2401.13170] CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering <https://arxiv.org/abs/2401.13170>`__ CFMatch:开放域问答中自动答案等价性评估与专家判断的对齐

::

    replaced with revised version Sun, 30 Jun 2024 02:09:46 GMT
    Submission history From: Zongxia Li [view email]
    [v1] Wed, 24 Jan 2024 01:30:25 UTC (100 KB)
    [v2] Tue, 20 Feb 2024 19:37:18 UTC (1 KB) (withdrawn)
    [v3] Fri, 1 Mar 2024 15:12:08 UTC (100 KB)
    [v4] Sun, 30 Jun 2024 02:09:46 UTC (1 KB) (withdrawn)
    Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, and Jordan Boyd-Graber

Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current evaluation metrics to determine answer equivalence (AE) often do not align with human judgments, particularly more verbose, free-form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big: LLM-based scorers can correlate better with human judges, but this task has only been tested on limited QA datasets, and even when available, update of the model is limited because LLMs are large and often expensive. We rectify both of these issues by providing clear and consistent guidelines for evaluating AE in machine QA adopted from professional human QA contests. We also introduce a combination of standard evaluation and a more efficient, robust, and lightweight discriminate AE classifier-based matching method (CFMatch, smaller than 1 MB), trained and validated to more accurately evaluate answer correctness in accordance with adopted expert AE rules that are more aligned with human judgments.

------------

`[2402.00367] Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration <https://arxiv.org/abs/2402.00367>`__ 不要幻想，不要放弃:通过多LLM合作识别LLM知识缺口

::

    replaced with revised version Mon, 1 Jul 2024 01:34:09 GMT
    Submission history From: Shangbin Feng [view email]
    [v1] Thu, 1 Feb 2024 06:11:49 UTC (5,072 KB)
    [v2] Mon, 1 Jul 2024 01:34:09 UTC (5,081 KB)
    Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, Yulia Tsvetkov

Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.

------------

`[2402.06608] TIC: Translate-Infer-Compile for accurate "text to plan" using LLMs and Logical Representations <https://arxiv.org/abs/2402.06608>`__ TIC:翻译-推断-编译，使用llm和逻辑表示实现准确的“文本到规划”

::

    replaced with revised version Sat, 29 Jun 2024 00:30:04 GMT
    Submission history From: Sudhir Agarwal [view email]
    [v1] Fri, 9 Feb 2024 18:39:13 UTC (152 KB)
    [v2] Sat, 29 Jun 2024 00:30:04 UTC (313 KB)
    Sudhir Agarwal and Anu Sreepathy

We study the problem of generating plans for given natural language planning task requests. On one hand, LLMs excel at natural language processing but do not perform well on planning. On the other hand, classical planning tools excel at planning tasks but require input in a structured language such as the Planning Domain Definition Language (PDDL). We leverage the strengths of both the techniques by using an LLM for generating the PDDL representation (task PDDL) of planning task requests followed by using a classical planner for computing a plan. Unlike previous approaches that use LLMs for generating task PDDLs directly, our approach comprises of (a) translate: using an LLM only for generating a logically interpretable intermediate representation of natural language task description, (b) infer: deriving additional logically dependent information from the intermediate representation using a logic reasoner (currently, Answer Set Programming solver), and (c) compile: generating the target task PDDL from the base and inferred information. We observe that using an LLM to only output the intermediate representation significantly reduces LLM errors. Consequently, TIC approach achieves, for at least one LLM, high accuracy on task PDDL generation for all seven domains of our evaluation dataset.

------------

`[2402.10770] How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs? <https://arxiv.org/abs/2402.10770>`__ 指令调谐llm的自动评估方法有多可靠?

::

    replaced with revised version Mon, 1 Jul 2024 08:59:08 GMT
    Submission history From: Ehsan Doostmohammadi [view email]
    [v1] Fri, 16 Feb 2024 15:48:33 UTC (704 KB)
    [v2] Mon, 1 Jul 2024 08:59:08 UTC (1,077 KB)
    [v3] Tue, 2 Jul 2024 11:46:09 UTC (1,077 KB)
    Ehsan Doostmohammadi, Oskar Holmstr\"om, Marco Kuhlmann

Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we perform a meta-evaluation of such methods and assess their reliability across a broad range of tasks. We observe that while automatic evaluation methods can approximate human ratings under specific conditions, their validity is highly context-dependent. Specifically, the simple ROUGE-L metric correlates well with human ratings for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of the more advanced method of using GPT-4 as a judge diminishes significantly if reference answers are not included in the prompt, which is the scenario where this method has the potential to provide the most value compared to other metrics. Our findings enhance the understanding of how automatic methods should be applied and interpreted when developing and evaluating instruction-tuned LLMs.

------------

`[2402.12713] Are LLMs Rational Investors? A Study on Detecting and Reducing the Financial Bias in LLMs <https://arxiv.org/abs/2402.12713>`__ llm投资者是理性的吗?关于检测和减少LLMs中的财务偏见的研究

::

    replaced with revised version Mon, 1 Jul 2024 15:42:15 GMT
    Submission history From: Yuhang Zhou [view email]
    [v1] Tue, 20 Feb 2024 04:26:08 UTC (20,656 KB)
    [v2] Mon, 1 Jul 2024 15:42:15 UTC (22,656 KB)
    Yuhang Zhou and Yuchen Ni and Yunhui Gan and Zhangyue Yin and Xiang Liu and Jian Zhang and Sen Liu and Xipeng Qiu and Guangnan Ye and Hongfeng Chai

Large Language Models (LLMs) are increasingly adopted in financial analysis for interpreting complex market data and trends. However, their use is challenged by intrinsic biases (e.g., risk-preference bias) and a superficial understanding of market intricacies, necessitating a thorough assessment of their financial insight. To address these issues, we introduce Financial Bias Indicators (FBI), a framework with components like Bias Unveiler, Bias Detective, Bias Tracker, and Bias Antidote to identify, detect, analyze, and eliminate irrational biases in LLMs. By combining behavioral finance principles with bias examination, we evaluate 23 leading LLMs and propose a de-biasing method based on financial causal knowledge. Results show varying degrees of financial irrationality among models, influenced by their design and training. Models trained specifically on financial datasets may exhibit more irrationality, and even larger financial language models (FinLLMs) can show more bias than smaller, general models. We utilize four prompt-based methods incorporating causal debiasing, effectively reducing financial biases in these models. This work enhances the understanding of LLMs' bias in financial applications, laying the foundation for developing more reliable and rational financial analysis tools.

------------

`[2402.16029] GraphWiz: An Instruction-Following Language Model for Graph Problems <https://arxiv.org/abs/2402.16029>`__ GraphWiz:面向图问题的指令遵循语言模型

::

    replaced with revised version Mon, 1 Jul 2024 09:15:38 GMT
    Submission history From: Nuo Chen [view email]
    [v1] Sun, 25 Feb 2024 08:41:32 UTC (1,180 KB)
    [v2] Wed, 6 Mar 2024 13:52:12 UTC (1,180 KB)
    [v3] Mon, 1 Jul 2024 09:15:38 UTC (1,187 KB)
    [v4] Tue, 2 Jul 2024 06:40:30 UTC (1,633 KB)
    [v5] Wed, 3 Jul 2024 06:39:59 UTC (1,634 KB)
    Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li

Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the delicate balance between training data volume and model performance, highlighting the potential for overfitting with increased data. We also explore the transferability of the model's reasoning ability across different graph tasks, indicating the model's adaptability and practical application potential. Our investigation offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving.

------------

`[2404.01399] Safe and Responsible Large Language Model : Can We Balance Bias Reduction and Language Understanding in Large Language Models? <https://arxiv.org/abs/2404.01399>`__ 安全负责的大型语言模型:我们能否在大型语言模型中平衡偏差减少和语言理解?

::

    replaced with revised version Mon, 1 Jul 2024 17:40:13 GMT
    Submission history From: Shaina Raza Dr. [view email]
    [v1] Mon, 1 Apr 2024 18:10:05 UTC (11,125 KB)
    [v2] Tue, 21 May 2024 15:28:45 UTC (11,544 KB)
    [v3] Mon, 1 Jul 2024 17:40:13 UTC (9,736 KB)
    Shaina Raza, Oluwanifemi Bamgbose, Shardul Ghuge, Fatemeh Tavakol, Deepak John Reji, Syed Raza Bashir

Large Language Models (LLMs) have significantly advanced various NLP tasks. However, these models often risk generating unsafe text that perpetuates biases. Current approaches to produce unbiased outputs from LLMs can reduce biases but at the expense of knowledge retention. In this research, we address the question of whether producing safe (unbiased) outputs through LLMs can retain knowledge and language understanding. In response, we developed the Safety and Responsible Large Language Model (\textbf{SR}$_{\text{LLM}}$), an LLM that has been instruction fine-tuned on top of already safe LLMs (e.g., Llama2 or related) to diminish biases in generated text. To achieve our goals, we compiled a specialized dataset designed to train our model in identifying and correcting biased text. We conduct experiments, both on this custom data and out-of-distribution test sets, to show the bias reduction and knowledge retention. The results confirm that \textbf{SR}$_{\text{LLM}}$ outperforms traditional fine-tuning and prompting methods in both reducing biases and preserving the integrity of language knowledge. The significance of our findings lies in demonstrating that instruction fine-tuning can provide a more robust solution for bias reduction in LLMs. We have made our code and data available at \href{this https URL}{Safe-LLM}.

------------

`[2404.17218] Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes <https://arxiv.org/abs/2404.17218>`__ 通过系统1和系统2认知过程减少LLMs中的社会偏见的提示技术

::

    replaced with revised version Fri, 28 Jun 2024 23:54:49 GMT
    Submission history From: Mahammed Kamruzzaman [view email]
    [v1] Fri, 26 Apr 2024 07:46:29 UTC (8,067 KB)
    [v2] Fri, 28 Jun 2024 23:54:49 UTC (8,068 KB)
    Mahammed Kamruzzaman and Gene Louis Kim

Dual process theory posits that human cognition arises via two systems. System 1, which is a quick, emotional, and intuitive process, which is subject to cognitive biases, and System 2, a slow, onerous, and deliberate process. NLP researchers often compare zero-shot prompting in LLMs to System 1 reasoning and chain-of-thought (CoT) prompting to System 2. In line with this interpretation, prior research has found that using CoT prompting in LLMs leads to reduced gender bias. We investigate the relationship between bias, CoT prompting, and dual process theory in LLMs directly. We compare zero-shot, CoT, and a variety of dual process theory-based prompting strategies on two bias datasets spanning nine different social bias categories. We also use human and machine personas to determine whether the effects of dual process theory in LLMs are based on modeling human cognition or inherent to the system. We find that a human persona, System 2, and CoT prompting all tend to reduce social biases in LLMs, though the best combination of features depends on the exact model and bias category -- resulting in up to a 13 percent drop in stereotypical judgments by an LLM.

------------

`[2405.05116] XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples <https://arxiv.org/abs/2405.05116>`__ XAMPLER:学习检索上下文中的跨语言示例

::

    replaced with revised version Sat, 29 Jun 2024 13:09:36 GMT
    Submission history From: Peiqin Lin [view email]
    [v1] Wed, 8 May 2024 15:13:33 UTC (150 KB)
    [v2] Sat, 29 Jun 2024 13:09:36 UTC (194 KB)
    Peiqin Lin, Andr\'e F. T. Martins, Hinrich Sch\"utze

Recent studies indicate that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving relevant in-context examples tailored to the input query, enhances few-shot in-context learning of English. However, adapting these methods to other languages, especially low-resource ones, poses challenges due to the scarcity of cross-lingual retrievers and annotated data. Thus, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored to tackle the challenge of cross-lingual in-context learning using only annotated English data. XAMPLER first trains a retriever based on Glot500, a multilingual small language model, using positive and negative English examples constructed from the predictions of a multilingual large language model, i.e., MaLA500. Leveraging the cross-lingual capacity of the retriever, it can directly retrieve English examples as few-shot examples for in-context learning of target languages. Experiments on the multilingual text classification benchmark SIB200 with 176 languages show that XAMPLER substantially improves the in-context learning performance across languages. Our code is available at \url{this https URL}.

------------

`[2405.09341] Large Language Model Bias Mitigation from the Perspective of Knowledge Editing <https://arxiv.org/abs/2405.09341>`__ 知识编辑视角下的大规模语言模型偏差消除

::

    replaced with revised version Sat, 29 Jun 2024 05:50:28 GMT
    Submission history From: Ruizhe Chen [view email]
    [v1] Wed, 15 May 2024 13:44:13 UTC (5,106 KB)
    [v2] Sat, 29 Jun 2024 05:50:28 UTC (5,109 KB)
    Ruizhe Chen, Yichen Li, Zikai Xiao, Zuozhu Liu

Existing debiasing methods inevitably make unreasonable or undesired predictions as they are designated and evaluated to achieve parity across different social groups but leave aside individual facts, resulting in modified existing knowledge. In this paper, we first establish a new bias mitigation benchmark BiasKE leveraging existing and additional constructed datasets, which systematically assesses debiasing performance by complementary metrics on fairness, specificity, and generalization. Meanwhile, we propose a novel debiasing method, Fairness Stamp (FAST), which enables editable fairness through fine-grained calibration on individual biased knowledge. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with remarkable debiasing performance while not hampering overall model capability for knowledge preservation, highlighting the prospect of fine-grained debiasing strategies for editable fairness in LLMs.

------------

`[2406.06576] OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step <https://arxiv.org/abs/2406.06576>`__ OccamLLM:一步快速精确语言模型算法

::

    replaced with revised version Sat, 29 Jun 2024 19:13:23 GMT
    Submission history From: Owen Dugan [view email]
    [v1] Tue, 4 Jun 2024 04:17:40 UTC (3,501 KB)
    [v2] Tue, 18 Jun 2024 17:51:42 UTC (3,341 KB)
    [v3] Sat, 29 Jun 2024 19:13:23 UTC (3,349 KB)
    Owen Dugan, Donato Manuel Jimenez Beneto, Charlotte Loh, Zhuo Chen, Rumen Dangovski, Marin Solja\v{c}i\'c

Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations. To achieve accurate calculations, language model systems often enable LLMs to generate code for arithmetic operations. However, this approach compromises speed and security and, if finetuning is involved, risks the language model losing prior capabilities. We propose a framework that enables exact arithmetic in \textit{a single autoregressive step}, providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities. We use the hidden states of an LLM to control a symbolic architecture which performs arithmetic. Our implementation using Llama 3 8B Instruct with OccamNet as a symbolic model (OccamLlama) achieves 100\% accuracy on single arithmetic operations ($+,-,\times,÷,\sin{},\cos{},\log{},\exp{},\sqrt{}$), outperforming GPT 4o and on par with GPT 4o using a code interpreter. OccamLlama also outperforms GPT 4o both with and without a code interpreter on mathematical problem solving benchmarks involving challenging arithmetic, thus enabling small LLMs to match the arithmetic performance of even much larger models. We will make our code public shortly.

------------

`[2406.10288] Mimicking User Data: On Mitigating Fine-Tuning Risks in Closed Large Language Models <https://arxiv.org/abs/2406.10288>`__ 模拟用户数据:降低封闭大型语言模型的微调风险

::

    replaced with revised version Mon, 1 Jul 2024 10:17:58 GMT
    Submission history From: Francisco Eiras [view email]
    [v1] Wed, 12 Jun 2024 18:33:11 UTC (679 KB)
    [v2] Mon, 1 Jul 2024 10:17:58 UTC (618 KB)
    Francisco Eiras, Aleksandar Petrov, Phillip H.S. Torr, M. Pawan Kumar, Adel Bibi

Fine-tuning large language models on small, high-quality datasets can enhance their performance on specific downstream tasks. Recent research shows that fine-tuning on benign, instruction-following data can inadvertently undo the safety alignment process and increase a model's propensity to comply with harmful queries. Although critical, understanding and mitigating safety risks in well-defined tasks remains distinct from the instruction-following context due to structural differences in the data. Our work addresses the gap in our understanding of these risks across diverse types of data in closed models - where providers control how user data is utilized in the fine-tuning process. We demonstrate how malicious actors can subtly manipulate the structure of almost any task-specific dataset to foster significantly more dangerous model behaviors, while maintaining an appearance of innocuity and reasonable downstream task performance. To address this issue, we propose a novel mitigation strategy that mixes in safety data which mimics the task format and prompting style of the user data, showing this is more effective than existing baselines at re-establishing safety alignment while maintaining similar task performance.

------------

`[2406.11096] The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models <https://arxiv.org/abs/2406.11096>`__ 

::

    replaced with revised version Mon, 1 Jul 2024 10:04:09 GMT
    Submission history From: Bolei Ma [view email]
    [v1] Sun, 16 Jun 2024 22:59:18 UTC (388 KB)
    [v2] Mon, 1 Jul 2024 10:04:09 UTC (472 KB)
    Bolei Ma, Xinpeng Wang, Tiancheng Hu, Anna-Carolina Haensch, Michael A. Hedderich, Barbara Plank, Frauke Kreuter

Recent advances in Large Language Models (LLMs) have sparked wide interest in validating and comprehending the human-like cognitive-behavioral traits LLMs may have. These cognitive-behavioral traits include typically Attitudes, Opinions, Values (AOV). However, measuring AOV embedded within LLMs remains opaque, and different evaluation methods may yield different results. This has led to a lack of clarity on how different studies are related to each other and how they can be interpreted. This paper aims to bridge this gap by providing an overview of recent works on the evaluation of AOV in LLMs. Moreover, we survey related approaches in different stages of the evaluation pipeline in these works. By doing so, we address the potential and challenges with respect to understanding the model, human-AI alignment, and downstream application in social sciences. Finally, we provide practical insights into evaluation methods, model enhancement, and interdisciplinary collaboration, thereby contributing to the evolving landscape of evaluating AOV in LLMs.

------------

`[2406.11201] Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models <https://arxiv.org/abs/2406.11201>`__ 微调还是微调失败?揭穿大型语言模型的性能神话

::

    replaced with revised version Sun, 30 Jun 2024 14:42:52 GMT
    Submission history From: Sheng Wong [view email]
    [v1] Mon, 17 Jun 2024 04:35:17 UTC (148 KB)
    [v2] Sun, 30 Jun 2024 14:42:52 UTC (148 KB)
    Scott Barnett, Zac Brannelly, Stefanus Kurniawan, Sheng Wong

Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating: "To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case." This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.

------------

`[2406.12036] MedCalc-Bench: Evaluating Large Language Models for Medical Calculations <https://arxiv.org/abs/2406.12036>`__ MedCalc-Bench:医学计算大型语言模型评估

::

    replaced with revised version Sun, 30 Jun 2024 15:12:10 GMT
    Submission history From: Nikhil Khandekar [view email]
    [v1] Mon, 17 Jun 2024 19:07:21 UTC (1,319 KB)
    [v2] Tue, 25 Jun 2024 13:45:49 UTC (1,320 KB)
    [v3] Thu, 27 Jun 2024 15:25:25 UTC (1,320 KB)
    [v4] Sun, 30 Jun 2024 15:12:10 UTC (1,320 KB)
    Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina S Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad W Safranek, Abid A Anwar, Andrew Zhang, Aidan Gilson, Maxwell B Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, and Zhiyong Lu

As opposed to evaluating computation and logic-based reasoning, current benchmarks for evaluating large language models (LLMs) in medicine are primarily focused on question-answering involving domain knowledge and descriptive reasoning. While such qualitative capabilities are vital to medical diagnosis, in real-world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. To this end, we propose MedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical calculation capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks. Each instance in MedCalc-Bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. While our evaluation results show the potential of LLMs in this area, none of them are effective enough for clinical settings. Common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. We hope our study highlights the quantitative knowledge and reasoning gaps in LLMs within medical settings, encouraging future improvements of LLMs for various clinical calculation tasks.

------------

`[2406.14024] LLM Critics Help Catch Bugs in Mathematics: Towards a Better Mathematical Verifier with Natural Language Feedback <https://arxiv.org/abs/2406.14024>`__ LLM的批评者帮助捕捉数学中的bug:用自然语言反馈实现更好的数学验证器

::

    replaced with revised version Sun, 30 Jun 2024 13:44:04 GMT
    Submission history From: Bofei Gao [view email]
    [v1] Thu, 20 Jun 2024 06:42:27 UTC (2,184 KB)
    [v2] Sun, 30 Jun 2024 13:44:04 UTC (2,184 KB)
    [v3] Mon, 8 Jul 2024 08:37:33 UTC (2,184 KB)
    Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Junyang Lin, Chang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu, Baobao Chang

Mathematical verfier achieves success in mathematical reasoning tasks by validating the correctness of solutions. However, existing verifiers are trained with binary classification labels, which are not informative enough for the model to accurately assess the solutions. To mitigate the aforementioned insufficiency of binary labels, we introduce step-wise natural language feedbacks as rationale labels (i.e., the correctness of the current step and the explanations). In this paper, we propose \textbf{Math-Minos}, a natural language feedback enhanced verifier by constructing automatically-generated training data and a two-stage training paradigm for effective training and efficient inference. Our experiments reveal that a small set (30k) of natural language feedbacks can significantly boost the performance of the verifier by the accuracy of 1.6\% (86.6\% $\rightarrow$ 88.2\%) on GSM8K and 0.8\% (37.8\% $\rightarrow$ 38.6\%) on MATH. We have released our code and data for further exploration.

------------

`[2406.16203] LLMs' Classification Performance is Overclaimed <https://arxiv.org/abs/2406.16203>`__ LLMs的分类性能被夸大了

::

    replaced with revised version Sat, 29 Jun 2024 11:45:17 GMT
    Submission history From: Hanzi Xu [view email]
    [v1] Sun, 23 Jun 2024 19:49:10 UTC (8,492 KB)
    [v2] Sat, 29 Jun 2024 11:45:17 UTC (8,455 KB)
    [v3] Wed, 3 Jul 2024 13:18:50 UTC (8,455 KB)
    Hanzi Xu, Renze Lou, Jiangshu Du, Vahid Mahzoon, Elmira Talebianaraki, Zhuoan Zhou, Elizabeth Garrison, Slobodan Vucetic, Wenpeng Yin

In many classification tasks designed for AI or human to solve, gold labels are typically included within the label space by default, often posed as "which of the following is correct?" This standard setup has traditionally highlighted the strong performance of advanced AI, particularly top-performing Large Language Models (LLMs), in routine classification tasks. However, when the gold label is intentionally excluded from the label space, it becomes evident that LLMs still attempt to select from the available label candidates, even when none are correct. This raises a pivotal question: Do LLMs truly demonstrate their intelligence in understanding the essence of classification tasks?
In this study, we evaluate both closed-source and open-source LLMs across representative classification tasks, arguing that the perceived performance of LLMs is overstated due to their inability to exhibit the expected comprehension of the task. This paper makes a threefold contribution: i) To our knowledge, this is the first work to identify the limitations of LLMs in classification tasks when gold labels are absent. We define this task as Classify-w/o-Gold and propose it as a new testbed for LLMs. ii) We introduce a benchmark, Know-No, comprising two existing classification tasks and one new task, to evaluate Classify-w/o-Gold. iii) This work defines and advocates for a new evaluation metric, OmniAccuracy, which assesses LLMs' performance in classification tasks both when gold labels are present and absent.

------------

`[2406.16858] EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees <https://arxiv.org/abs/2406.16858>`__ EAGLE-2:基于动态草案树的语言模型快速推理

::

    replaced with revised version Sun, 30 Jun 2024 15:03:25 GMT
    Submission history From: Yuhui Li [view email]
    [v1] Mon, 24 Jun 2024 17:59:11 UTC (1,463 KB)
    [v2] Sun, 30 Jun 2024 15:03:25 UTC (1,464 KB)
    Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang

Inference with modern Large Language Models (LLMs) is expensive and time-consuming, and speculative sampling has proven to be an effective solution. Most speculative sampling methods such as EAGLE use a static draft tree, implicitly assuming that the acceptance rate of draft tokens depends only on their position. Interestingly, we found that the acceptance rate of draft tokens is also context-dependent. In this paper, building upon EAGLE, we propose EAGLE-2, which introduces a new technique of context-aware dynamic draft tree into drafting modeling. This improvement leverages the fact that the draft model of EAGLE is well-calibrated: the confidence scores from the draft model approximate acceptance rates with small errors. We conducted extensive evaluations on three series of LLMs and six tasks, with EAGLE-2 achieving speedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also ensures that the distribution of the generated text remains unchanged, making it a lossless acceleration algorithm.

------------

`[2406.17055] Large Language Models Assume People are More Rational than We Really are <https://arxiv.org/abs/2406.17055>`__ 大型语言模型假设人们比我们实际更理性

::

    replaced with revised version Mon, 1 Jul 2024 17:29:54 GMT
    Submission history From: Ryan Liu [view email]
    [v1] Mon, 24 Jun 2024 18:15:27 UTC (13,268 KB)
    [v2] Mon, 1 Jul 2024 17:29:54 UTC (13,268 KB)
    Ryan Liu, Jiayi Geng, Joshua C. Peterson, Ilia Sucholutsky, Thomas L. Griffiths

In order for AI systems to communicate effectively with people, they must understand how we make decisions. However, people's decisions are not always rational, so the implicit internal models of human decision-making in Large Language Models (LLMs) must account for this. Previous empirical evidence seems to suggest that these implicit models are accurate -- LLMs offer believable proxies of human behavior, acting how we expect humans would in everyday interactions. However, by comparing LLM behavior and predictions to a large dataset of human decisions, we find that this is actually not the case: when both simulating and predicting people's choices, a suite of cutting-edge LLMs (GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more rational than we really are. Specifically, these models deviate from human behavior and align more closely with a classic model of rational choice -- expected value theory. Interestingly, people also tend to assume that other people are rational when interpreting their behavior. As a consequence, when we compare the inferences that LLMs and people draw from the decisions of others using another psychological dataset, we find that these inferences are highly correlated. Thus, the implicit decision-making models of LLMs appear to be aligned with the human expectation that other people will act rationally, rather than with how people actually act.

------------

`[2406.18088] LLM-Driven Multimodal Opinion Expression Identification <https://arxiv.org/abs/2406.18088>`__ llm驱动的多模态观点表达识别

::

    replaced with revised version Sat, 29 Jun 2024 09:55:50 GMT
    Submission history From: Bonian Jia [view email]
    [v1] Wed, 26 Jun 2024 05:52:47 UTC (385 KB)
    [v2] Sat, 29 Jun 2024 09:55:50 UTC (385 KB)
    Bonian Jia and Huiyao Chen and Yueheng Sun and Meishan Zhang and Min Zhang

Opinion Expression Identification (OEI) is essential in NLP for applications ranging from voice assistants to depression diagnosis. This study extends OEI to encompass multimodal inputs, underlining the significance of auditory cues in delivering emotional subtleties beyond the capabilities of text. We introduce a novel multimodal OEI (MOEI) task, integrating text and speech to mirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we construct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is applied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template for the OEI task to take full advantage of the generative power of large language models (LLMs). Advancing further, we propose an LLM-driven method STOEI, which combines speech and text modal to identify opinion expressions. Our experiments demonstrate that MOEI significantly improves the performance while our method outperforms existing methods by 9.20\% and obtains SOTA results.

------------

`[2406.18783] Psychological Profiling in Cybersecurity: A Look at LLMs and Psycholinguistic Features <https://arxiv.org/abs/2406.18783>`__ 网络安全中的心理剖析:llm和心理语言学特征

::

    replaced with revised version Fri, 28 Jun 2024 21:22:56 GMT
    Submission history From: DJeff Kanda Nkashama [view email]
    [v1] Wed, 26 Jun 2024 23:04:52 UTC (99 KB)
    [v2] Fri, 28 Jun 2024 21:22:56 UTC (99 KB)
    Jean Marie Tshimula, D'Jeff K. Nkashama, Jean Tshibangu Muabila, Ren\'e Manass\'e Galekwa, Hugues Kanda, Maximilien V. Dialufuma, Mbuyi Mukendi Didier, Kalala Kalonji, Serge Mundele, Patience Kinshie Lenye, Tighana Wenge Basele, Aristarque Ilunga, Christian N. Mayemba, Nathana\"el M. Kasoro, Selain K. Kasereka, Hardy Mikese, Pierre-Martin Tardif, Marc Frappier, Froduald Kabanza, Belkacem Chikhaoui, Shengrui Wang, Ali Mulenda Sumbu, Xavier Ndona, Raoul Kienge-Kienge Intudi

The increasing sophistication of cyber threats necessitates innovative approaches to cybersecurity. In this paper, we explore the potential of psychological profiling techniques, particularly focusing on the utilization of Large Language Models (LLMs) and psycholinguistic features. We investigate the intersection of psychology and cybersecurity, discussing how LLMs can be employed to analyze textual data for identifying psychological traits of threat actors. We explore the incorporation of psycholinguistic features, such as linguistic patterns and emotional cues, into cybersecurity frameworks. Our research underscores the importance of integrating psychological perspectives into cybersecurity practices to bolster defense mechanisms against evolving threats.

------------

`[2406.18921] Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data <https://arxiv.org/abs/2406.18921>`__ 捕捉思想，而不仅仅是文字:用个性指示性数据增强角色扮演语言模型

::

    replaced with revised version Sat, 29 Jun 2024 05:58:28 GMT
    Submission history From: Yiting Ran [view email]
    [v1] Thu, 27 Jun 2024 06:24:00 UTC (302 KB)
    [v2] Sat, 29 Jun 2024 05:58:28 UTC (302 KB)
    Yiting Ran, Xintao Wang, Rui Xu, Xinfeng Yuan, Jiaqing Liang, Yanghua Xiao, Deqing Yang

Role-playing agents (RPA) have been a popular application area for large language models (LLMs), attracting significant interest from both industry and academia.While existing RPAs well portray the characters' knowledge and tones, they face challenges in capturing their minds, especially for small role-playing language models (RPLMs). In this paper, we propose to enhance RPLMs via personality-indicative data. Specifically, we leverage questions from psychological scales and distill advanced RPAs to generate dialogues that grasp the minds of characters. Experimental results validate that RPLMs trained with our dataset exhibit advanced role-playing capabilities for both general and personality-related evaluations. Code and data are available at \href{this https URL}{this URL}.

------------

`[2311.08364] Plum: Prompt Learning using Metaheuristic <https://arxiv.org/abs/2311.08364>`__ Plum:使用元启发式快速学习

::

    replaced with revised version Sun, 30 Jun 2024 09:50:11 GMT
    Submission history From: Rui Pan [view email]
    [v1] Tue, 14 Nov 2023 18:14:56 UTC (235 KB)
    [v2] Thu, 14 Mar 2024 13:43:52 UTC (16,486 KB)
    [v3] Sun, 30 Jun 2024 09:50:11 UTC (16,819 KB)
    Rui Pan, Shuo Xing, Shizhe Diao, Wenhe Sun, Xiang Liu, Kashun Shum, Renjie Pi, Jipeng Zhang, Tong Zhang

Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models. Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models. However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods. Unfortunately, few existing prompt learning methods satisfy the criteria of being truly "general", i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once. In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning. Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in white-box and black-box prompt learning. Furthermore, we show that these methods can be used to discover more human-understandable prompts that were previously unknown in both reasoning and image generation tasks, opening the door to a cornucopia of possibilities in prompt optimization. We release all the codes in \url{this https URL}.

------------

`[2312.12112] Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in low-data regimes <https://arxiv.org/abs/2312.12112>`__ 策划的LLM: LLM和数据策划的协同作用，用于低数据体制中的表格扩充

::

    replaced with revised version Sun, 30 Jun 2024 12:48:18 GMT
    Submission history From: Nabeel Seedat [view email]
    [v1] Tue, 19 Dec 2023 12:34:46 UTC (3,304 KB)
    [v2] Wed, 7 Feb 2024 19:00:35 UTC (3,306 KB)
    [v3] Sun, 30 Jun 2024 12:48:18 UTC (4,682 KB)
    Nabeel Seedat, Nicolas Huynh, Boris van Breugel, Mihaela van der Schaar

Machine Learning (ML) in low-data settings remains an underappreciated yet crucial problem. Hence, data augmentation methods to increase the sample size of datasets needed for ML are key to unlocking the transformative potential of ML in data-deprived regions and domains. Unfortunately, the limited training set constrains traditional tabular synthetic data generators in their ability to generate a large and diverse augmented dataset needed for ML tasks. To address this challenge, we introduce CLLM, which leverages the prior knowledge of Large Language Models (LLMs) for data augmentation in the low-data regime. However, not all the data generated by LLMs will improve downstream utility, as for any generative model. Consequently, we introduce a principled curation mechanism, leveraging learning dynamics, coupled with confidence and uncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple real-world datasets, we demonstrate the superior performance of CLLM in the low-data regime compared to conventional generators. Additionally, we provide insights into the LLM generation and curation mechanism, shedding light on the features that enable them to output high-quality augmented datasets.

------------

`[2402.05162] Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications <https://arxiv.org/abs/2402.05162>`__ 基于剪枝和低秩修正的安全对齐脆弱性评估

::

    replaced with revised version Mon, 1 Jul 2024 07:11:17 GMT
    Submission history From: Boyi Wei [view email]
    [v1] Wed, 7 Feb 2024 18:34:38 UTC (649 KB)
    [v2] Thu, 27 Jun 2024 17:23:58 UTC (652 KB)
    [v3] Mon, 1 Jul 2024 07:11:17 UTC (577 KB)
    Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson

Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.

------------

`[2402.11838] UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction <https://arxiv.org/abs/2402.11838>`__ UniST:一种快速赋能的城市时空预测通用模型

::

    replaced with revised version Mon, 1 Jul 2024 02:51:58 GMT
    Submission history From: Yuan Yuan [view email]
    [v1] Mon, 19 Feb 2024 05:04:11 UTC (3,122 KB)
    [v2] Mon, 20 May 2024 13:18:47 UTC (3,123 KB)
    [v3] Thu, 23 May 2024 13:01:23 UTC (3,122 KB)
    [v4] Mon, 24 Jun 2024 02:51:27 UTC (4,127 KB)
    [v5] Mon, 1 Jul 2024 02:51:58 UTC (4,126 KB)
    Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li

Urban spatio-temporal prediction is crucial for informed decision-making, such as traffic management, resource optimization, and emergence response. Despite remarkable breakthroughs in pretrained natural language models that enable one model to handle diverse tasks, a universal solution for spatio-temporal prediction remains challenging Existing prediction approaches are typically tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive domain-specific training data. In this study, we introduce UniST, a universal model designed for general urban spatio-temporal prediction across a wide range of scenarios. Inspired by large language models, UniST achieves success through: (i) utilizing diverse spatio-temporal data from different scenarios, (ii) effective pre-training to capture complex spatio-temporal dynamics, (iii) knowledge-guided prompts to enhance generalization capabilities. These designs together unlock the potential of building a universal model for various scenarios Extensive experiments on more than 20 spatio-temporal scenarios demonstrate UniST's efficacy in advancing state-of-the-art performance, especially in few-shot and zero-shot prediction. The datasets and code implementation are released on this https URL.

------------

`[2404.01273] TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model <https://arxiv.org/abs/2404.01273>`__ 

::

    replaced with revised version Sat, 29 Jun 2024 01:28:02 GMT
    Submission history From: Yingzhou Lu [view email]
    [v1] Mon, 1 Apr 2024 17:48:55 UTC (198 KB)
    [v2] Sat, 29 Jun 2024 01:28:02 UTC (200 KB)
    Yue Wang, Tianfan Fu, Yinlong Xu, Zihan Ma, Hongxia Xu, Yingzhou Lu, Bang Du, Honghao Gao, Jian Wu

Clinical trials are indispensable for medical research and the development of new treatments. However, clinical trials often involve thousands of participants and can span several years to complete, with a high probability of failure during the process. Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin creation approach, called TWIN-GPT. TWIN-GPT can establish cross-dataset associations of medical information given limited data, generating unique personalized digital twins for different patients, thereby preserving individual patient characteristics. Comprehensive experiments show that using digital twins created by TWIN-GPT can boost the clinical trial outcome prediction, exceeding various previous prediction approaches.

------------

`[2404.15146] Rethinking LLM Memorization through the Lens of Adversarial Compression <https://arxiv.org/abs/2404.15146>`__ 通过对抗性压缩的视角重新思考LLM记忆

::

    replaced with revised version Mon, 1 Jul 2024 14:43:11 GMT
    Submission history From: Avi Schwarzschild [view email]
    [v1] Tue, 23 Apr 2024 15:49:37 UTC (1,031 KB)
    [v2] Mon, 1 Jul 2024 14:43:11 UTC (5,321 KB)
    Avi Schwarzschild and Zhili Feng and Pratyush Maini and Zachary C. Lipton and J. Zico Kolter

Large language models (LLMs) trained on web-scale datasets raise substantial concerns regarding permissible data usage. One major question is whether these models "memorize" all their training data or they integrate many data sources in some way more akin to how a human would learn and synthesize information. The answer hinges, to a large degree, on how we define memorization. In this work, we propose the Adversarial Compression Ratio (ACR) as a metric for assessing memorization in LLMs. A given string from the training data is considered memorized if it can be elicited by a prompt (much) shorter than the string itself -- in other words, if these strings can be "compressed" with the model by computing adversarial prompts of fewer tokens. The ACR overcomes the limitations of existing notions of memorization by (i) offering an adversarial view of measuring memorization, especially for monitoring unlearning and compliance; and (ii) allowing for the flexibility to measure memorization for arbitrary strings at a reasonably low compute. Our definition serves as a practical tool for determining when model owners may be violating terms around data usage, providing a potential legal tool and a critical lens through which to address such scenarios.

------------

`[2404.15993] Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach <https://arxiv.org/abs/2404.15993>`__ llm的不确定性估计和量化:一种简单的监督方法

::

    replaced with revised version Sat, 29 Jun 2024 02:58:21 GMT
    Submission history From: Linyu Liu [view email]
    [v1] Wed, 24 Apr 2024 17:10:35 UTC (1,108 KB)
    [v2] Tue, 21 May 2024 02:18:49 UTC (1,357 KB)
    [v3] Sat, 29 Jun 2024 02:58:21 UTC (1,361 KB)
    Linyu Liu, Yu Pan, Xiaocheng Li, Guanting Chen

In this paper, we study the problem of uncertainty estimation and calibration for LLMs. We first formulate the uncertainty estimation problem for LLMs and then propose a supervised approach that takes advantage of the labeled datasets and estimates the uncertainty of the LLMs' responses. Based on the formulation, we illustrate the difference between the uncertainty estimation for LLMs and that for standard ML models and explain why the hidden neurons of the LLMs may contain uncertainty information. Our designed approach demonstrates the benefits of utilizing hidden activations to enhance uncertainty estimation across various tasks and shows robust transferability in out-of-distribution settings. We distinguish the uncertainty estimation task from the uncertainty calibration task and show that a better uncertainty estimation mode leads to a better calibration performance. Furthermore, our method is easy to implement and adaptable to different levels of model accessibility including black box, grey box, and white box.

------------

`[2405.15052] Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training <https://arxiv.org/abs/2405.15052>`__ 重新审视LLM训练的MoE和密集速度-精度比较

::

    replaced with revised version Fri, 28 Jun 2024 19:39:45 GMT
    Submission history From: Xianzhi Du [view email]
    [v1] Thu, 23 May 2024 21:00:53 UTC (209 KB)
    [v2] Fri, 28 Jun 2024 19:39:45 UTC (210 KB)
    Xianzhi Du, Tom Gunter, Xiang Kong, Mark Lee, Zirui Wang, Aonan Zhang, Nan Du, Ruoming Pang

Mixture-of-Experts (MoE) enjoys performance gain by increasing model capacity while keeping computation cost constant. When comparing MoE to dense models, prior work typically adopt the following setting: 1) use FLOPs or activated parameters as a measure of model complexity; 2) train all models to the same number of tokens. We argue that this setting favors MoE as FLOPs and activated parameters do not accurately measure the communication overhead in sparse layers, leading to a larger actual training budget for MoE. In this work, we revisit the settings by adopting step time as a more accurate measure of model complexity, and by determining the total compute budget under the Chinchilla compute-optimal settings. To efficiently run MoE on modern accelerators, we adopt a 3D sharding method that keeps the dense-to-MoE step time increase within a healthy range. We evaluate MoE and dense LLMs on a set of nine 0-shot and two 1-shot English tasks, as well as MMLU 5-shot and GSM8K 8-shot across three model scales at 6.4B, 12.6B, and 29.6B. Experimental results show that even under these settings, MoE consistently outperform dense LLMs on the speed-accuracy trade-off curve with meaningful gaps. Our full model implementation and sharding strategy has been released at~\url{this https URL}

------------

`[2406.04824] FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch <https://arxiv.org/abs/2406.04824>`__ FunBO:用FunSearch发现贝叶斯优化的获取函数

::

    replaced with revised version Mon, 1 Jul 2024 04:48:24 GMT
    Submission history From: Virginia Aglietti [view email]
    [v1] Fri, 7 Jun 2024 10:49:59 UTC (6,254 KB)
    [v2] Mon, 1 Jul 2024 04:48:24 UTC (6,090 KB)
    Virginia Aglietti, Ira Ktena, Jessica Schrouff, Eleni Sgouritsa, Francisco J. R. Ruiz, Alan Malek, Alexis Bellot, Silvia Chiappa

The sample efficiency of Bayesian optimization algorithms depends on carefully crafted acquisition functions (AFs) guiding the sequential collection of function evaluations. The best-performing AF can vary significantly across optimization problems, often requiring ad-hoc and problem-specific choices. This work tackles the challenge of designing novel AFs that perform well across a variety of experimental settings. Based on FunSearch, a recent work using Large Language Models (LLMs) for discovery in mathematical sciences, we propose FunBO, an LLM-based method that can be used to learn new AFs written in computer code by leveraging access to a limited number of evaluations for a set of objective functions. We provide the analytic expression of all discovered AFs and evaluate them on various global optimization benchmarks and hyperparameter optimization tasks. We show how FunBO identifies AFs that generalize well in and out of the training distribution of functions, thus outperforming established general-purpose AFs and achieving competitive performance against AFs that are customized to specific function types and are learned via transfer-learning algorithms.

------------

`[2406.18665] RouteLLM: Learning to Route LLMs with Preference Data <https://arxiv.org/abs/2406.18665>`__ RouteLLM:基于偏好数据的llm路由学习

::

    replaced with revised version Mon, 1 Jul 2024 05:38:08 GMT
    Submission history From: Isaac Ong [view email]
    [v1] Wed, 26 Jun 2024 18:10:22 UTC (580 KB)
    [v2] Mon, 1 Jul 2024 05:38:08 UTC (623 KB)
    Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez, M Waleed Kadous, Ion Stoica

Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs.

------------

`[2305.07970] The Machine Psychology of Cooperation: Can GPT models operationalise prompts for altruism, cooperation, competitiveness and selfishness in economic games? <https://arxiv.org/abs/2305.07970>`__ 合作的机器心理学:GPT模型能否在经济游戏中操作利他、合作、竞争和自私的提示?

::

    replaced with revised version Sat, 29 Jun 2024 12:29:28 GMT
    Submission history From: Steve Phelps [view email]
    [v1] Sat, 13 May 2023 17:23:16 UTC (57 KB)
    [v2] Sat, 29 Jun 2024 12:29:28 UTC (162 KB)
    Steve Phelps and Yvan I. Russell

We investigated the capability of the GPT-3.5 large language model (LLM) to operationalize natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in two social dilemmas: the repeated Prisoners Dilemma and the one-shot Dictator Game. Using a within-subject experimental design, we used a prompt to describe the task environment using a similar protocol to that used in experimental psychology studies with human subjects. We tested our research question by manipulating the part of our prompt which was used to create a simulated persona with different cooperative and competitive stances. We then assessed the resulting simulacras' level of cooperation in each social dilemma, taking into account the effect of different partner conditions for the repeated game. Our results provide evidence that LLMs can, to some extent, translate natural language descriptions of different cooperative stances into corresponding descriptions of appropriate task behaviour, particularly in the one-shot game. There is some evidence of behaviour resembling conditional reciprocity for the cooperative simulacra in the repeated game, and for the later version of the model there is evidence of altruistic behaviour. Our study has potential implications for using LLM chatbots in task environments that involve cooperation, e.g. using chatbots as mediators and facilitators in public-goods negotiations.

------------

`[2310.06752] Comparing AI Algorithms for Optimizing Elliptic Curve Cryptography Parameters in e-Commerce Integrations: A Pre-Quantum Analysis <https://arxiv.org/abs/2310.06752>`__ 电子商务集成中优化椭圆曲线密码参数的人工智能算法比较:量子前分析

::

    replaced with revised version Mon, 1 Jul 2024 17:19:27 GMT
    Submission history From: Felipe Tellez [view email]
    [v1] Tue, 10 Oct 2023 16:23:41 UTC (1,758 KB)
    [v2] Mon, 1 Jul 2024 17:19:27 UTC (1,525 KB)
    Felipe Tellez, Jorge Ortiz

This paper presents a comparative analysis between the Genetic Algorithm (GA) and Particle Swarm Optimization (PSO), two vital artificial intelligence algorithms, focusing on optimizing Elliptic Curve Cryptography (ECC) parameters. These encompass the elliptic curve coefficients, prime number, generator point, group order, and cofactor. The study provides insights into which of the bio-inspired algorithms yields better optimization results for ECC configurations, examining performances under the same fitness function. This function incorporates methods to ensure robust ECC parameters, including assessing for singular or anomalous curves and applying Pollard's rho attack and Hasse's theorem for optimization precision. The optimized parameters generated by GA and PSO are tested in a simulated e-commerce environment, contrasting with well-known curves like secp256k1 during the transmission of order messages using Elliptic Curve-Diffie Hellman (ECDH) and Hash-based Message Authentication Code (HMAC). Focusing on traditional computing in the pre-quantum era, this research highlights the efficacy of GA and PSO in ECC optimization, with implications for enhancing cybersecurity in third-party e-commerce integrations. We recommend the immediate consideration of these findings before quantum computing's widespread adoption.

------------

`[2401.10510] When large language models meet evolutionary algorithms <https://arxiv.org/abs/2401.10510>`__ 当大型语言模型遇到进化算法

::

    replaced with revised version Sat, 29 Jun 2024 05:16:33 GMT
    Submission history From: Chao Wang PhD [view email]
    [v1] Fri, 19 Jan 2024 05:58:30 UTC (980 KB)
    [v2] Sat, 29 Jun 2024 05:16:33 UTC (989 KB)
    Wang Chao, Jiaxuan Zhao, Licheng Jiao, Lingling Li, Fang Liu, Shuyuan Yang

Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text generation and evolution, this paper illustrates the parallels between LLMs and EAs, which includes multiple one-to-one key characteristics: token representation and individual representation, position encoding and fitness shaping, position embedding and selection, Transformers block and reproduction, and model training and parameter adaptation. By examining these parallels, we analyze existing interdisciplinary research, with a specific focus on evolutionary fine-tuning and LLM-enhanced EAs. Drawing from these insights, valuable future directions are presented for advancing the integration of LLMs and EAs, while highlighting key challenges along the way. These parallels not only reveal the evolution mechanism behind LLMs but also facilitate the development of evolved artificial agents that approach or surpass biological organisms.

------------

`[2402.14601] Bringing Generative AI to Adaptive Learning in Education <https://arxiv.org/abs/2402.14601>`__ 将生成性AI引入教育的自适应学习

::

    replaced with revised version Fri, 28 Jun 2024 23:43:07 GMT
    Submission history From: Hang Li [view email]
    [v1] Fri, 2 Feb 2024 23:54:51 UTC (1,244 KB)
    [v2] Fri, 23 Feb 2024 04:38:37 UTC (1,043 KB)
    [v3] Fri, 28 Jun 2024 23:43:07 UTC (1,957 KB)
    Hang Li, Tianlong Xu, Chaoli Zhang, Eason Chen, Jing Liang, Xing Fan, Haoyang Li, Jiliang Tang, Qingsong Wen

The recent surge in generative AI technologies, such as large language models and diffusion models, has boosted the development of AI applications in various domains, including science, finance, and education. Concurrently, adaptive learning, a concept that has gained substantial interest in the educational sphere, has proven its efficacy in enhancing students' learning efficiency. In this position paper, we aim to shed light on the intersectional studies of these two methods, which combine generative AI with adaptive learning concepts. By presenting discussions about the benefits, challenges, and potentials in this field, we argue that this union will contribute significantly to the development of the next-stage learning format in education.

------------

`[2402.16929] LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language <https://arxiv.org/abs/2402.16929>`__ LangGPT:从编程语言重新思考llm的结构化可重用提示设计框架

::

    replaced with revised version Sat, 29 Jun 2024 14:19:08 GMT
    Submission history From: Ming Wang [view email]
    [v1] Mon, 26 Feb 2024 15:05:16 UTC (1,428 KB)
    [v2] Sat, 29 Jun 2024 14:19:08 UTC (1,606 KB)
    Ming Wang, Yuanzhong Liu, Xiaoyu Liang, Songlian Li, Yijie Huang, Xiaoming Zhang, Sijia Shen, Chaofeng Guan, Daling Wang, Shi Feng, Huaiwen Zhang, Yifei Zhang, Minghui Zheng, Chi Zhang

LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to instruct LLMs proficiently poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat scattered optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. In addition, it is not conducive to the iterative updating of prompts. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the performance of LLMs. Moreover, the case study shows that LangGPT leads LLMs to generate higher-quality responses. Furthermore, we analyzed the ease of use and reusability of LangGPT through a user survey in our online community.

------------

`[2406.09953] DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning <https://arxiv.org/abs/2406.09953>`__ DAG-Plan:面向双臂协同规划的有向无环依赖图生成方法

::

    replaced with revised version Sun, 30 Jun 2024 09:38:55 GMT
    Submission history From: Zeyu Gao [view email]
    [v1] Fri, 14 Jun 2024 11:58:51 UTC (7,741 KB)
    [v2] Sun, 30 Jun 2024 09:38:55 UTC (7,741 KB)
    Zeyu Gao, Yao Mu, Jinye Qu, Mengkang Hu, Lingyue Guo, Ping Luo, Yanfeng Lu

Dual-arm robots offer enhanced versatility and efficiency over single-arm counterparts by enabling concurrent manipulation of multiple objects or cooperative execution of tasks using both arms. However, effectively coordinating the two arms for complex long-horizon tasks remains a significant challenge. Existing task planning methods predominantly focus on single-arm robots or rely on predefined bimanual operations, failing to fully leverage the capabilities of dual-arm systems. To address this limitation, we introduce DAG-Plan, a structured task planning framework tailored for dual-arm robots. DAG-Plan harnesses large language models (LLMs) to decompose intricate tasks into actionable sub-tasks represented as nodes within a directed acyclic graph (DAG). Critically, DAG-Plan dynamically assigns these sub-tasks to the appropriate arm based on real-time environmental observations, enabling parallel and adaptive execution. We evaluate DAG-Plan on the novel Dual-Arm Kitchen Benchmark, comprising 9 sequential tasks with 78 sub-tasks and 26 objects. Extensive experiments demonstrate the superiority of DAG-Plan over directly using LLM to generate plans, achieving nearly 50% higher efficiency compared to the single-arm task planning baseline and nearly double the success rate of the dual-arm task planning baseline.

------------

`[2406.14097] Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration <https://arxiv.org/abs/2406.14097>`__ 通过人机协作增强基于llm的机器人操纵

::

    replaced with revised version Mon, 1 Jul 2024 06:11:31 GMT
    Submission history From: Yaonan Zhu [view email]
    [v1] Thu, 20 Jun 2024 08:23:49 UTC (6,833 KB)
    [v2] Mon, 1 Jul 2024 06:11:31 UTC (7,045 KB)
    Haokun Liu, Yaonan Zhu, Kenji Kato, Atsushi Tsukahara, Izumi Kondo, Tadayoshi Aoyama, and Yasuhisa Hasegawa

Large Language Models (LLMs) are gaining popularity in the field of robotics. However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This paper proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.

------------

`[2403.13583] CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing <https://arxiv.org/abs/2403.13583>`__ CoCoST:具有在线搜索和正确性测试的自动复杂代码生成

::

    replaced with revised version Mon, 1 Jul 2024 09:59:47 GMT
    Submission history From: Xinyi He [view email]
    [v1] Wed, 20 Mar 2024 13:33:55 UTC (699 KB)
    [v2] Mon, 1 Jul 2024 09:59:47 UTC (8,398 KB)
    Xinyi He, Jiaru Zou, Yun Lin, Mengyu Zhou, Shi Han, Zejian Yuan, Dongmei Zhang

Large Language Models have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CoCoST framework, which enhances complex code generation by online searching for more information with planned queries and correctness testing for code refinement. Moreover, CoCoST serializes the complex inputs and outputs to improve comprehension and generates test cases to ensure the adaptability for real-world applications. CoCoST is validated through rigorous experiments on the DS-1000 and ClassEval datasets. Experimental results show that CoCoST substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality of LLMs in generating complex code.

------------

`[2404.06371] Model Generation with LLMs: From Requirements to UML Sequence Diagrams <https://arxiv.org/abs/2404.06371>`__ 使用LLMs生成模型:从需求到UML顺序图

::

    replaced with revised version Mon, 1 Jul 2024 13:16:49 GMT
    Submission history From: Alessio Ferrari [view email]
    [v1] Tue, 9 Apr 2024 15:07:25 UTC (1,818 KB)
    [v2] Mon, 1 Jul 2024 13:16:49 UTC (3,440 KB)
    Alessio Ferrari, Sallam Abualhaija, Chetan Arora

Complementing natural language (NL) requirements with graphical models can improve stakeholders' communication and provide directions for system design. However, creating models from requirements involves manual effort. The advent of generative large language models (LLMs), ChatGPT being a notable example, offers promising avenues for automated assistance in model generation. This paper investigates the capability of ChatGPT to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements. We conduct a qualitative study in which we examine the sequence diagrams generated by ChatGPT for 28 requirements documents of various types and from different domains. Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis. Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges. This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency. The insights derived from this study can influence the practical utilization of LLMs in the RE process, and open the door to novel RE-specific prompting strategies targeting effective model generation.

------------

`[2403.17983] Is Watermarking LLM-Generated Code Robust? <https://arxiv.org/abs/2403.17983>`__ 水印llm生成的代码健壮吗?

::

    replaced with revised version Fri, 28 Jun 2024 18:35:22 GMT
    Submission history From: Tarun Suresh [view email]
    [v1] Sun, 24 Mar 2024 21:41:29 UTC (1,051 KB)
    [v2] Fri, 28 Jun 2024 18:35:22 UTC (1,053 KB)
    Tarun Suresh, Shubham Ugare, Gagandeep Singh, Sasa Misailovic

We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language, we show that it is easy to remove these watermarks on code by semantic-preserving transformations.

------------

`[2406.03636] Synthetic Programming Elicitation and Repair for Text-to-Code in Very Low-Resource Programming Languages <https://arxiv.org/abs/2406.03636>`__ 低资源编程语言文本到代码的综合编程启发与修复

::

    replaced with revised version Sat, 29 Jun 2024 20:24:23 GMT
    Submission history From: Federico Mora [view email]
    [v1] Wed, 5 Jun 2024 22:16:19 UTC (145 KB)
    [v2] Fri, 14 Jun 2024 22:35:33 UTC (149 KB)
    [v3] Sat, 29 Jun 2024 20:24:23 UTC (142 KB)
    Federico Mora, Justin Wong, Haley Lepe, Sahil Bhatia, Karim Elmaaroufi, George Varghese, Joseph E. Gonzalez, Elizabeth Polgreen, Sanjit A. Seshia

Recent advances in large language models (LLMs) for code applications have demonstrated remarkable zero-shot fluency and instruction following on challenging code related tasks ranging from test case generation to self-repair. Unsurprisingly, however, models struggle to compose syntactically valid programs in programming languages unrepresented in pre-training, referred to as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial settings, including domain-specific languages for internal tools and tool-chains for legacy languages. Inspired by an HCI technique called natural program elicitation, we propose designing an intermediate language that LLMs ``naturally'' know how to use and which can be automatically compiled to a target VLPL. When LLMs generate code that lies outside of this intermediate language, we use compiler techniques to repair the code into programs in the intermediate language. Overall, we introduce \emph{synthetic programming elicitation and compilation} (SPEAC), an approach that enables LLMs to generate syntactically valid code even for VLPLs. We empirically evaluate the performance of SPEAC in a case study and find that, compared to existing retrieval and fine-tuning baselines, SPEAC produces syntactically correct programs significantly more frequently without sacrificing semantic correctness.

------------

-----------
Index (149)
-----------

`[2407.00075] Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference <https://arxiv.org/abs/2407.00075>`__ Logicbreaks:一种理解颠覆基于规则推理的框架

`[2407.00256] One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts <https://arxiv.org/abs/2407.00256>`__ 一个提示是不够的:自动构建多个专家提示的混合

`[2407.00693] BAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models <https://arxiv.org/abs/2407.00693>`__ BAPO:基于锚定偏好优化的大型语言模型个性化对齐

`[2407.00900] MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human Curricula <https://arxiv.org/abs/2407.00900>`__

`[2407.00958] Universal Approximation Theory: The basic theory for large language models <https://arxiv.org/abs/2407.00958>`__ 通用近似理论:大型语言模型的基础理论

`[2407.00959] Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving <https://arxiv.org/abs/2407.00959>`__ 将世界标记为对象级知识以解决自动驾驶中的长尾事件

`[2407.01067] Human-like object concept representations emerge naturally in multimodal large language models <https://arxiv.org/abs/2407.01067>`__ 类人物体概念表示在多模态大型语言模型中自然出现

`[2407.01238] Large Language Models are Zero-Shot Recognizers for Activities of Daily Living <https://arxiv.org/abs/2407.01238>`__

`[2407.01245] SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model <https://arxiv.org/abs/2407.01245>`__ SINKT:基于大型语言模型的结构感知归纳知识追踪模型

`[2407.00191] MetaKP: On-Demand Keyphrase Generation <https://arxiv.org/abs/2407.00191>`__ MetaKP:按需关键字生成

`[2407.00211] Detection and Measurement of Syntactic Templates in Generated Text <https://arxiv.org/abs/2407.00211>`__ 生成文本中句法模板的检测与度量

`[2407.00219] Evaluating Human Alignment and Model Faithfulness of LLM Rationale <https://arxiv.org/abs/2407.00219>`__ 评估LLM理论的人类对齐和模型忠实度

`[2407.00242] EHRmonize: A Framework for Medical Concept Abstraction from Electronic Health Records using Large Language Models <https://arxiv.org/abs/2407.00242>`__ EHRmonize:基于大型语言模型从电子健康记录中抽象医疗概念的框架

`[2407.00320] LiteSearch: Efficacious Tree Search for LLM <https://arxiv.org/abs/2407.00320>`__ LiteSearch:有效的LLM树搜索

`[2407.00322] LLM-Generated Natural Language Meets Scaling Laws: New Explorations and Data Augmentation Methods <https://arxiv.org/abs/2407.00322>`__

`[2407.00341] Iterative Data Augmentation with Large Language Models for Aspect-based Sentiment Analysis <https://arxiv.org/abs/2407.00341>`__ 基于大型语言模型迭代数据增强的方面级情感分析

`[2407.00365] Financial Knowledge Large Language Model <https://arxiv.org/abs/2407.00365>`__ 金融知识大语言模型

`[2407.00369] How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models <https://arxiv.org/abs/2407.00369>`__ 如何训练你的事实验证者:多模态开放模型的知识迁移

`[2407.00390] Advancing Process Verification for Large Language Models via Tree-Based Preference Learning <https://arxiv.org/abs/2407.00390>`__

`[2407.00396] A Study on Effect of Reference Knowledge Choice in Generating Technical Content Relevant to SAPPhIRE Model Using Large Language Model <https://arxiv.org/abs/2407.00396>`__ 基于大型语言模型的蓝宝石技术内容生成中参考知识选择的影响研究

`[2407.00416] Too Late to Train, Too Early To Use? A Study on Necessity and Viability of Low-Resource Bengali LLMs <https://arxiv.org/abs/2407.00416>`__ 训练太晚，使用太早?低资源孟加拉LLMs的必要性和可行性研究

`[2407.00434] Brevity is the soul of wit: Pruning long files for code generation <https://arxiv.org/abs/2407.00434>`__ 简洁是智慧的灵魂:修剪长文件以生成代码

`[2407.00436] A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models <https://arxiv.org/abs/2407.00436>`__ 面向多语言大型语言模型的平行语料库开发方法

`[2407.00454] Self-Translate-Train: A Simple but Strong Baseline for Cross-lingual Transfer of Large Language Models <https://arxiv.org/abs/2407.00454>`__ 自翻译-训练:大型语言模型跨语言迁移的简单但强大的基线

`[2407.00476] Large Language Models for Power Scheduling: A User-Centric Approach <https://arxiv.org/abs/2407.00476>`__ 电力调度的大型语言模型:以用户为中心的方法

`[2407.00487] It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization <https://arxiv.org/abs/2407.00487>`__

`[2407.00488] PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models <https://arxiv.org/abs/2407.00488>`__ PFME:用于细粒度幻觉检测和大型语言模型编辑的模块化方法

`[2407.00497] LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement <https://arxiv.org/abs/2407.00497>`__ llms -as- instructor:从错误中学习以自动化模型改进

`[2407.00541] Answering real-world clinical questions using large language model based systems <https://arxiv.org/abs/2407.00541>`__ 用基于大型语言模型的系统回答现实世界的临床问题

`[2407.00702] Scaling Technology Acceptance Analysis with Large Language Model (LLM) Annotation Systems <https://arxiv.org/abs/2407.00702>`__ 用大型语言模型(LLM)标注系统扩展技术接受度分析

`[2407.00731] Large Language Models Struggle in Token-Level Clinical Named Entity Recognition <https://arxiv.org/abs/2407.00731>`__

`[2407.00747] A Comparative Study of Quality Evaluation Methods for Text Summarization <https://arxiv.org/abs/2407.00747>`__ 文本摘要质量评价方法比较研究

`[2407.00869] Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks <https://arxiv.org/abs/2407.00869>`__ 大型语言模型是无意识地说实话者:利用越狱攻击的谬误失败

`[2407.00870] Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles <https://arxiv.org/abs/2407.00870>`__ role - play-doh:使领域专家通过诱导和坚持原则创建llm模拟患者

`[2407.00875] MoE-CT: A Novel Approach For Large Language Models Training With Resistance To Catastrophic Forgetting <https://arxiv.org/abs/2407.00875>`__ MoE-CT:一种抗灾难性遗忘的大型语言模型训练新方法

`[2407.00908] FineSurE: Fine-grained Summarization Evaluation using LLMs <https://arxiv.org/abs/2407.00908>`__ FineSurE:使用LLMs进行细粒度摘要评估

`[2407.00948] The House Always Wins: A Framework for Evaluating Strategic Deception in LLMs <https://arxiv.org/abs/2407.00948>`__ 庄家总是赢家:评估llm战略欺骗的框架

`[2407.00994] LLM Uncertainty Quantification through Directional Entailment Graph and Claim Level Response Augmentation <https://arxiv.org/abs/2407.00994>`__ 通过定向蕴含图和请求级响应增强对LLM不确定性进行量化

`[2407.00996] Can Small Language Models Learn, Unlearn, and Retain Noise Patterns? <https://arxiv.org/abs/2407.00996>`__ 小型语言模型能学习、遗忘和保留噪声模式吗?

`[2407.00997] Engineering Conversational Search Systems: A Review of Applications, Architectures, and Functional Components <https://arxiv.org/abs/2407.00997>`__ 工程会话搜索系统:应用、体系结构和功能组件的回顾

`[2407.01009] DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models <https://arxiv.org/abs/2407.01009>`__ DynaThink:快还是慢?大型语言模型的动态决策框架

`[2407.01082] Min P Sampling: Balancing Creativity and Coherence at High Temperature <https://arxiv.org/abs/2407.01082>`__ 最小P采样:在高温下平衡创造力和连贯性

`[2407.01091] M2QA: Multi-domain Multilingual Question Answering <https://arxiv.org/abs/2407.01091>`__ M2QA:多领域多语言问答

`[2407.01119] Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing? <https://arxiv.org/abs/2407.01119>`__ Pron vs Prompt:大型语言模型是否已经在创造性文本写作方面挑战世界级小说作者?

`[2407.01122] Calibrated Large Language Models for Binary Question Answering <https://arxiv.org/abs/2407.01122>`__ 用于二元问答的校准大型语言模型

`[2407.01178] $\text{Memory}^3$: Language Modeling with Explicit Memory <https://arxiv.org/abs/2407.01178>`__ $\text{Memory}^3$:使用显式内存的语言建模

`[2407.01270] The African Woman is Rhythmic and Soulful: Evaluation of Open-ended Generation for Implicit Biases <https://arxiv.org/abs/2407.01270>`__ 《非洲妇女》节奏感强，深情款款:评价开放式世代的隐性偏见

`[2407.01272] Show Less, Instruct More: Enriching Prompts with Definitions and Guidelines for Zero-Shot NER <https://arxiv.org/abs/2407.01272>`__

`[2407.01300] Collaborative Performance Prediction for Large Language Models <https://arxiv.org/abs/2407.01300>`__ 大型语言模型的协同性能预测

`[2407.01358] Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language Models <https://arxiv.org/abs/2407.01358>`__ 大型语言模型中基于知识的跨语言不一致性评估

`[2407.01384] Free-text Rationale Generation under Readability Level Control <https://arxiv.org/abs/2407.01384>`__ 可读性级别控制下的自由文本逻辑生成

`[2407.01406] Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters <https://arxiv.org/abs/2407.01406>`__ 通过适配器用知识图谱使多语言llm适应低资源语言

`[2407.01409] Dynamic Few-Shot Learning for Knowledge Graph Question Answering <https://arxiv.org/abs/2407.01409>`__ 基于动态少样本学习的知识图谱问答

`[2407.01437] Needle in the Haystack for Memory Based Large Language Models <https://arxiv.org/abs/2407.01437>`__ 基于内存的大型语言模型是大海捞针

`[2407.01455] TimeToM: Temporal Space is the Key to Unlocking the Door of Large Language Models' Theory-of-Mind <https://arxiv.org/abs/2407.01455>`__ TimeToM:时间空间是打开大型语言模型心智理论之门的钥匙

`[2407.01461] Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement <https://arxiv.org/abs/2407.01461>`__ 通过强化学习驱动的查询精化提高大型语言模型的能力和鲁棒性

`[2407.01470] DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging <https://arxiv.org/abs/2407.01470>`__ DogeRM:通过模型融合为奖励模型配备领域知识

`[2407.01490] LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives <https://arxiv.org/abs/2407.01490>`__ LLM看，LLM做:指导数据生成以实现不可微的目标

`[2407.01492] RegMix: Data Mixture as Regression for Language Model Pre-training <https://arxiv.org/abs/2407.01492>`__ RegMix:用于语言模型预训练的数据混合回归

`[2407.01505] Self-Cognition in Large Language Models: An Exploratory Study <https://arxiv.org/abs/2407.01505>`__ 大型语言模型中的自我认知:探索性研究

`[2407.00102] Curriculum Learning with Quality-Driven Data Selection <https://arxiv.org/abs/2407.00102>`__ 基于质量驱动数据选择的课程学习

`[2407.00106] UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI <https://arxiv.org/abs/2407.00106>`__ UnUnlearning:在高级生成式AI中，UnUnlearning不足以实现内容监管

`[2407.00116] Generative AI for Synthetic Data Across Multiple Medical Modalities: A Systematic Review of Recent Developments and Challenges <https://arxiv.org/abs/2407.00116>`__ 面向多种医疗模式合成数据的生成式人工智能:最新发展和挑战的系统综述

`[2407.00121] Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks <https://arxiv.org/abs/2407.00121>`__ Granite-Function调用模型:通过粒度任务的多任务学习引入函数调用能力

`[2407.00463] Open-Source Conversational AI with SpeechBrain 1.0 <https://arxiv.org/abs/2407.00463>`__ 基于SpeechBrain 1.0的开源对话AI

`[2407.00467] VcLLM: Video Codecs are Secretly Tensor Codecs <https://arxiv.org/abs/2407.00467>`__ VcLLM:视频编解码器实际上是张量编解码器

`[2407.00568] Divide And Conquer: Learning Chaotic Dynamical Systems With Multistep Penalty Neural Ordinary Differential Equations <https://arxiv.org/abs/2407.00568>`__ 分治:基于多步惩罚神经常微分方程的混沌动力系统学习

`[2407.00617] Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning <https://arxiv.org/abs/2407.00617>`__ 迭代纳什策略优化:通过无后悔学习将llm与一般偏好对齐

`[2407.00631] TrialBench: Multi-Modal Artificial Intelligence-Ready Clinical Trial Datasets <https://arxiv.org/abs/2407.00631>`__ TrialBench:多模态人工智能就绪临床试验数据集

`[2407.00928] FoldGPT: Simple and Effective Large Language Model Compression Scheme <https://arxiv.org/abs/2407.00928>`__ FoldGPT:简单有效的大型语言模型压缩方案

`[2407.01031] PocketLLM: Enabling On-Device Fine-Tuning for Personalized LLMs <https://arxiv.org/abs/2407.01031>`__ PocketLLM:启用设备上的个性化llm微调

`[2407.01085] Rethinking LLM-based Preference Evaluation <https://arxiv.org/abs/2407.01085>`__ 基于llm的偏好评估的再思考

`[2407.01155] CPT: Consistent Proxy Tuning for Black-box Optimization <https://arxiv.org/abs/2407.01155>`__ CPT:黑盒优化的一致性代理调优

`[2407.01376] Badllama 3: removing safety finetuning from Llama 3 in minutes <https://arxiv.org/abs/2407.01376>`__ badlama 3:在几分钟内移除Llama 3的安全微调

`[2407.00010] Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads <https://arxiv.org/abs/2407.00010>`__ 混合异构集群可以降低LLM推理工作负载的能耗

`[2407.00066] Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead <https://arxiv.org/abs/2407.00066>`__ 压缩然后提供服务:以很少的开销提供数千个LoRA适配器

`[2407.00079] Mooncake: Kimi's KVCache-centric Architecture for LLM Serving <https://arxiv.org/abs/2407.00079>`__

`[2407.00088] T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge <https://arxiv.org/abs/2407.00088>`__ T-MAC:通过表查找实现边缘低比特LLM部署的CPU复兴

`[2407.00110] Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services <https://arxiv.org/abs/2407.00110>`__ Chat AI:面向高性能计算服务的无缝Slurm-Native解决方案

`[2407.00128] When Search Engine Services meet Large Language Models: Visions and Challenges <https://arxiv.org/abs/2407.00128>`__ 搜索引擎服务遇到大型语言模型时的设想与挑战

`[2407.00456] Beyond Functional Correctness: Investigating Coding Style Inconsistencies in Large Language Models <https://arxiv.org/abs/2407.00456>`__ 超越功能正确性:研究大型语言模型中编码风格的不一致性

`[2407.00808] Exploring a Physics-Informed Decision Transformer for Distribution System Restoration: Methodology and Performance Analysis <https://arxiv.org/abs/2407.00808>`__ 配电系统恢复的物理决策Transformer探索:方法和性能分析

`[2407.01280] Human-Robot Mutual Learning through Affective-Linguistic Interaction and Differential Outcomes Training [Pre-Print] <https://arxiv.org/abs/2407.01280>`__ 通过情感语言交互和差异结果训练的人机相互学习[预印本]

`[2407.00047] One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving <https://arxiv.org/abs/2407.00047>`__ 一个队列就是你所需要的:解决大型语言模型服务中的队首阻塞

`[2407.01394] Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing <https://arxiv.org/abs/2407.01394>`__ glos2text:使用llm和语义感知标签平滑的手语Gloss翻译

`[2407.01509] MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs <https://arxiv.org/abs/2407.01509>`__ MIA-Bench:多模态llm评估后的更好指导

`[2407.00215] LLM Critics Help Catch LLM Bugs <https://arxiv.org/abs/2407.00215>`__ LLM批评者帮助捕捉LLM的bug

`[2407.00634] Tarsier: Recipes for Training and Evaluating Large Video Description Models <https://arxiv.org/abs/2407.00634>`__ Tarsier:训练和评估大型视频描述模型的方法

`[2407.00890] Macroeconomic Forecasting with Large Language Models <https://arxiv.org/abs/2407.00890>`__ 基于大型语言模型的宏观经济预测

`[2310.17884] Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory <https://arxiv.org/abs/2310.17884>`__ llm能保守秘密吗?通过上下文完整性理论测试语言模型的隐私含义

`[2402.04858] CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay <https://arxiv.org/abs/2402.04858>`__ CodeIt:具有优先后见之明重放的自我改进语言模型

`[2404.17524] On the Use of Large Language Models to Generate Capability Ontologies <https://arxiv.org/abs/2404.17524>`__ 利用大型语言模型生成能力本体

`[2404.19721] PANGeA: Procedural Artificial Narrative using Generative AI for Turn-Based Video Games <https://arxiv.org/abs/2404.19721>`__ PANGeA:回合制电子游戏中使用生成AI的程序人工叙事

`[2405.17902] Boosting Protein Language Models with Negative Sample Mining <https://arxiv.org/abs/2405.17902>`__ 基于负样本挖掘的蛋白质语言模型增强

`[2406.10162] Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models <https://arxiv.org/abs/2406.10162>`__ 对托词的谄媚:研究大型语言模型中的奖励篡改

`[2406.19644] Beyond Human Preferences: Exploring Reinforcement Learning Trajectory Evaluation and Improvement through LLMs <https://arxiv.org/abs/2406.19644>`__ 超越人类偏好:通过llm探索强化学习轨迹评估和改进

`[2305.13857] Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation <https://arxiv.org/abs/2305.13857>`__ 基于交互评估的任务导向型对话用户熟悉性偏差挖掘

`[2309.05196] Does Writing with Language Models Reduce Content Diversity? <https://arxiv.org/abs/2309.05196>`__ 使用语言模型写作会降低内容多样性吗?

`[2311.04886] SEMQA: Semi-Extractive Multi-Source Question Answering <https://arxiv.org/abs/2311.04886>`__

`[2311.07138] WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models <https://arxiv.org/abs/2311.07138>`__ WaterBench:面向大型语言模型的水印整体评估

`[2311.09022] Exploring the Potential of Large Language Models in Computational Argumentation <https://arxiv.org/abs/2311.09022>`__ 探索大型语言模型在计算辩论中的潜力

`[2311.09069] How Well Do Large Language Models Truly Ground? <https://arxiv.org/abs/2311.09069>`__ 大型语言模型的真正基础是什么?

`[2312.00738] SeaLLMs -- Large Language Models for Southeast Asia <https://arxiv.org/abs/2312.00738>`__ SeaLLMs—东南亚大型语言模型

`[2312.11985] Climate Change from Large Language Models <https://arxiv.org/abs/2312.11985>`__ 大型语言模型的气候变化

`[2401.01218] Self-Supervised Position Debiasing for Large Language Models <https://arxiv.org/abs/2401.01218>`__ 大型语言模型的自监督位置去偏

`[2401.12070] Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text <https://arxiv.org/abs/2401.12070>`__ 用双筒望远镜识别llm:机器生成文本的零样本检测

`[2401.13170] CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering <https://arxiv.org/abs/2401.13170>`__ CFMatch:开放域问答中自动答案等价性评估与专家判断的对齐

`[2402.00367] Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration <https://arxiv.org/abs/2402.00367>`__ 不要幻想，不要放弃:通过多LLM合作识别LLM知识缺口

`[2402.06608] TIC: Translate-Infer-Compile for accurate "text to plan" using LLMs and Logical Representations <https://arxiv.org/abs/2402.06608>`__ TIC:翻译-推断-编译，使用llm和逻辑表示实现准确的“文本到规划”

`[2402.10770] How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs? <https://arxiv.org/abs/2402.10770>`__ 指令调谐llm的自动评估方法有多可靠?

`[2402.12713] Are LLMs Rational Investors? A Study on Detecting and Reducing the Financial Bias in LLMs <https://arxiv.org/abs/2402.12713>`__ llm投资者是理性的吗?关于检测和减少LLMs中的财务偏见的研究

`[2402.16029] GraphWiz: An Instruction-Following Language Model for Graph Problems <https://arxiv.org/abs/2402.16029>`__ GraphWiz:面向图问题的指令遵循语言模型

`[2404.01399] Safe and Responsible Large Language Model : Can We Balance Bias Reduction and Language Understanding in Large Language Models? <https://arxiv.org/abs/2404.01399>`__ 安全负责的大型语言模型:我们能否在大型语言模型中平衡偏差减少和语言理解?

`[2404.17218] Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes <https://arxiv.org/abs/2404.17218>`__ 通过系统1和系统2认知过程减少LLMs中的社会偏见的提示技术

`[2405.05116] XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples <https://arxiv.org/abs/2405.05116>`__ XAMPLER:学习检索上下文中的跨语言示例

`[2405.09341] Large Language Model Bias Mitigation from the Perspective of Knowledge Editing <https://arxiv.org/abs/2405.09341>`__ 知识编辑视角下的大规模语言模型偏差消除

`[2406.06576] OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step <https://arxiv.org/abs/2406.06576>`__ OccamLLM:一步快速精确语言模型算法

`[2406.10288] Mimicking User Data: On Mitigating Fine-Tuning Risks in Closed Large Language Models <https://arxiv.org/abs/2406.10288>`__ 模拟用户数据:降低封闭大型语言模型的微调风险

`[2406.11096] The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models <https://arxiv.org/abs/2406.11096>`__

`[2406.11201] Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models <https://arxiv.org/abs/2406.11201>`__ 微调还是微调失败?揭穿大型语言模型的性能神话

`[2406.12036] MedCalc-Bench: Evaluating Large Language Models for Medical Calculations <https://arxiv.org/abs/2406.12036>`__ MedCalc-Bench:医学计算大型语言模型评估

`[2406.14024] LLM Critics Help Catch Bugs in Mathematics: Towards a Better Mathematical Verifier with Natural Language Feedback <https://arxiv.org/abs/2406.14024>`__ LLM的批评者帮助捕捉数学中的bug:用自然语言反馈实现更好的数学验证器

`[2406.16203] LLMs' Classification Performance is Overclaimed <https://arxiv.org/abs/2406.16203>`__ LLMs的分类性能被夸大了

`[2406.16858] EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees <https://arxiv.org/abs/2406.16858>`__ EAGLE-2:基于动态草案树的语言模型快速推理

`[2406.17055] Large Language Models Assume People are More Rational than We Really are <https://arxiv.org/abs/2406.17055>`__ 大型语言模型假设人们比我们实际更理性

`[2406.18088] LLM-Driven Multimodal Opinion Expression Identification <https://arxiv.org/abs/2406.18088>`__ llm驱动的多模态观点表达识别

`[2406.18783] Psychological Profiling in Cybersecurity: A Look at LLMs and Psycholinguistic Features <https://arxiv.org/abs/2406.18783>`__ 网络安全中的心理剖析:llm和心理语言学特征

`[2406.18921] Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data <https://arxiv.org/abs/2406.18921>`__ 捕捉思想，而不仅仅是文字:用个性指示性数据增强角色扮演语言模型

`[2311.08364] Plum: Prompt Learning using Metaheuristic <https://arxiv.org/abs/2311.08364>`__ Plum:使用元启发式快速学习

`[2312.12112] Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in low-data regimes <https://arxiv.org/abs/2312.12112>`__ 策划的LLM: LLM和数据策划的协同作用，用于低数据体制中的表格扩充

`[2402.05162] Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications <https://arxiv.org/abs/2402.05162>`__ 基于剪枝和低秩修正的安全对齐脆弱性评估

`[2402.11838] UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction <https://arxiv.org/abs/2402.11838>`__ UniST:一种快速赋能的城市时空预测通用模型

`[2404.01273] TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model <https://arxiv.org/abs/2404.01273>`__

`[2404.15146] Rethinking LLM Memorization through the Lens of Adversarial Compression <https://arxiv.org/abs/2404.15146>`__ 通过对抗性压缩的视角重新思考LLM记忆

`[2404.15993] Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach <https://arxiv.org/abs/2404.15993>`__ llm的不确定性估计和量化:一种简单的监督方法

`[2405.15052] Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training <https://arxiv.org/abs/2405.15052>`__ 重新审视LLM训练的MoE和密集速度-精度比较

`[2406.04824] FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch <https://arxiv.org/abs/2406.04824>`__ FunBO:用FunSearch发现贝叶斯优化的获取函数

`[2406.18665] RouteLLM: Learning to Route LLMs with Preference Data <https://arxiv.org/abs/2406.18665>`__ RouteLLM:基于偏好数据的llm路由学习

`[2305.07970] The Machine Psychology of Cooperation: Can GPT models operationalise prompts for altruism, cooperation, competitiveness and selfishness in economic games? <https://arxiv.org/abs/2305.07970>`__ 合作的机器心理学:GPT模型能否在经济游戏中操作利他、合作、竞争和自私的提示?

`[2310.06752] Comparing AI Algorithms for Optimizing Elliptic Curve Cryptography Parameters in e-Commerce Integrations: A Pre-Quantum Analysis <https://arxiv.org/abs/2310.06752>`__ 电子商务集成中优化椭圆曲线密码参数的人工智能算法比较:量子前分析

`[2401.10510] When large language models meet evolutionary algorithms <https://arxiv.org/abs/2401.10510>`__ 当大型语言模型遇到进化算法

`[2402.14601] Bringing Generative AI to Adaptive Learning in Education <https://arxiv.org/abs/2402.14601>`__ 将生成性AI引入教育的自适应学习

`[2402.16929] LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language <https://arxiv.org/abs/2402.16929>`__ LangGPT:从编程语言重新思考llm的结构化可重用提示设计框架

`[2406.09953] DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning <https://arxiv.org/abs/2406.09953>`__ DAG-Plan:面向双臂协同规划的有向无环依赖图生成方法

`[2406.14097] Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration <https://arxiv.org/abs/2406.14097>`__ 通过人机协作增强基于llm的机器人操纵

`[2403.13583] CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing <https://arxiv.org/abs/2403.13583>`__ CoCoST:具有在线搜索和正确性测试的自动复杂代码生成

`[2404.06371] Model Generation with LLMs: From Requirements to UML Sequence Diagrams <https://arxiv.org/abs/2404.06371>`__ 使用LLMs生成模型:从需求到UML顺序图

`[2403.17983] Is Watermarking LLM-Generated Code Robust? <https://arxiv.org/abs/2403.17983>`__ 水印llm生成的代码健壮吗?

`[2406.03636] Synthetic Programming Elicitation and Repair for Text-to-Code in Very Low-Resource Programming Languages <https://arxiv.org/abs/2406.03636>`__ 低资源编程语言文本到代码的综合编程启发与修复

