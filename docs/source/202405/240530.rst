240530
========

----------
Survey (5)
----------

`[2405.19334] LLMs Meet Multimodal Generation and Editing: A Survey <https://arxiv.org/abs/2405.19334>`__ llm满足多模态生成和编辑:综述

::

    Wed, 29 May 2024 17:59:20 GMT
    Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, Jifeng Dai, Yong Zhang, Wei Xue, Qifeng Liu, Yike Guo, Qifeng Chen

With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on understanding. This survey elaborates on multimodal generation across different domains, including image, video, 3D, and audio, where we highlight the notable advancements with milestone works in these fields. Specifically, we exhaustively investigate the key technical components behind methods and multimodal datasets utilized in these studies. Moreover, we dig into tool-augmented multimodal agents that can use existing generative models for human-computer interaction. Lastly, we also comprehensively discuss the advancement in AI safety and investigate emerging applications as well as future prospects. Our work provides a systematic and insightful overview of multimodal generation, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation

------------

`[2405.18653] Recent Advances of Foundation Language Models-based Continual Learning: A Survey <https://arxiv.org/abs/2405.18653>`__ 基于基础语言模型的持续学习研究进展综述

::

    Tue, 28 May 2024 23:32:46 GMT
    Yutao Yang, Jie Zhou, Xuanwen Ding, Tianyu Huai, Shunyu Liu, Qin Chen, Liang He, Yuan Xie

Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing (NLP) and computer vision (CV). Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich commonsense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters. However, they still can not emulate human-like continuous learning due to catastrophic forgetting. Consequently, various continual learning (CL)-based methodologies have been developed to refine LMs, enabling them to adapt to new tasks without forgetting previous knowledge. However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking, which is the gap that our survey aims to fill. We delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models (PLMs), large language models (LLMs) and vision-language models (VLMs). We divide these studies into offline CL and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods.
Offline CL encompasses domain-incremental learning, task-incremental learning, and class-incremental learning, while online CL is subdivided into hard task boundary and blurry task boundary settings. Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning.

------------

`[2402.02244] Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models <https://arxiv.org/abs/2402.02244>`__ 超越极限:大型语言模型中上下文长度扩展技术综述

::

    replaced with revised version Wed, 29 May 2024 13:38:25 GMT
    Submission history From: Xindi Wang [view email]
    [v1] Sat, 3 Feb 2024 19:20:02 UTC (232 KB)
    [v2] Wed, 22 May 2024 10:06:29 UTC (805 KB)
    [v3] Wed, 29 May 2024 13:38:25 UTC (806 KB)
    Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, Armaghan Eshaghi

Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.

------------

`[2404.15777] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry <https://arxiv.org/abs/2404.15777>`__ 大型语言模型在医疗行业中的应用评估综述

::

    replaced with revised version Wed, 29 May 2024 15:50:43 GMT
    Submission history From: Yining Huang [view email]
    [v1] Wed, 24 Apr 2024 09:55:24 UTC (36 KB)
    [v2] Sun, 5 May 2024 16:44:58 UTC (55 KB)
    [v3] Wed, 22 May 2024 08:57:19 UTC (77 KB)
    [v4] Wed, 29 May 2024 15:50:43 UTC (477 KB)
    Yining Huang, Keke Tang, Meilian Chen, Boyuan Wang

Since the inception of the Transformer architecture in 2017, Large Language Models (LLMs) such as GPT and BERT have evolved significantly, impacting various industries with their advanced capabilities in language understanding and generation. These models have shown potential to transform the medical field, highlighting the necessity for specialized evaluation frameworks to ensure their effective and ethical deployment. This comprehensive survey delineates the extensive application and requisite evaluation of LLMs within healthcare, emphasizing the critical need for empirical validation to fully exploit their capabilities in enhancing healthcare outcomes. Our survey is structured to provide an in-depth analysis of LLM applications across clinical settings, medical text data processing, research, education, and public health awareness. We begin by exploring the roles of LLMs in various medical applications, detailing their evaluation based on performance in tasks such as clinical diagnosis, medical text data processing, information retrieval, data analysis, and educational content generation. The subsequent sections offer a comprehensive discussion on the evaluation methods and metrics employed, including models, evaluators, and comparative experiments. We further examine the benchmarks and datasets utilized in these evaluations, providing a categorized description of benchmarks for tasks like question answering, summarization, information extraction, bioinformatics, information retrieval and general comprehensive benchmarks. This structure ensures a thorough understanding of how LLMs are assessed for their effectiveness, accuracy, usability, and ethical alignment in the medical domain. ...

------------

`[2401.10034] Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap <https://arxiv.org/abs/2401.10034>`__ 大规模语言模型时代的进化计算:综述与路线图

::

    replaced with revised version Wed, 29 May 2024 09:00:25 GMT
    Submission history From: Xingyu Wu [view email]
    [v1] Thu, 18 Jan 2024 14:58:17 UTC (222 KB)
    [v2] Wed, 7 Feb 2024 07:37:34 UTC (242 KB)
    [v3] Wed, 29 May 2024 09:00:25 UTC (256 KB)
    Xingyu Wu, Sheng-hao Wu, Jibin Wu, Liang Feng, Kay Chen Tan

Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM's further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this paper provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: LLM-enhanced EA and EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: this https URL.

------------

-------------
Benchmark (1)
-------------

`[2402.11493] Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation <https://arxiv.org/abs/2402.11493>`__ 大型语言模型知识边界基准测试:模型评估的不同视角

::

    replaced with revised version Wed, 29 May 2024 17:39:47 GMT
    Submission history From: Xunjian Yin [view email]
    [v1] Sun, 18 Feb 2024 07:48:15 UTC (8,827 KB)
    [v2] Wed, 29 May 2024 17:39:47 UTC (8,828 KB)
    Xunjian Yin and Xu Zhang and Jie Ruan and Xiaojun Wan

In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs. We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt. Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models. Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge. Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods. Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.

------------

--------------
Accelerate (4)
--------------

`[2405.18718] Efficient Model-agnostic Alignment via Bayesian Persuasion <https://arxiv.org/abs/2405.18718>`__ 基于贝叶斯说服的高效模型无关对齐

::

    Wed, 29 May 2024 02:57:07 GMT
    Fengshuo Bai, Mingzhi Wang, Zhaowei Zhang, Boyuan Chen, Yinda Xu, Ying Wen, Yaodong Yang

With recent advancements in large language models (LLMs), alignment has emerged as an effective technique for keeping LLMs consensus with human intent.
Current methods primarily involve direct training through Supervised Fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), both of which require substantial computational resources and extensive ground truth data. This paper explores an efficient method for aligning black-box large models using smaller models, introducing a model-agnostic and lightweight Bayesian Persuasion Alignment framework. We formalize this problem as an optimization of the signaling strategy from the small model's perspective. In the persuasion process, the small model (Advisor) observes the information item (i.e., state) and persuades large models (Receiver) to elicit improved responses. The Receiver then generates a response based on the input, the signal from the Advisor, and its updated belief about the information item.
Through training using our framework, we demonstrate that the Advisor can significantly enhance the performance of various Receivers across a range of tasks. We theoretically analyze our persuasion framework and provide an upper bound on the Advisor's regret, confirming its effectiveness in learning the optimal signaling strategy. Our Empirical results demonstrates that GPT-2 can significantly improve the performance of various models, achieving an average enhancement of 16.1% in mathematical reasoning ability and 13.7% in code generation. We hope our work can provide an initial step toward rethinking the alignment framework from the Bayesian Persuasion perspective.

------------

`[2405.19325] Nearest Neighbor Speculative Decoding for LLM Generation and Attribution <https://arxiv.org/abs/2405.19325>`__ 用于LLM生成和归因的最近邻推测解码

::

    Wed, 29 May 2024 17:55:03 GMT
    Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, Xi Victoria Lin

Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts.
In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B.

------------

`[2405.18628] Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference <https://arxiv.org/abs/2405.18628>`__ 基于硬件感知并行提示解码的高效内存加速LLM推理

::

    Tue, 28 May 2024 22:19:30 GMT
    Hao (Mark) Chen, Wayne Luk, Ka Fai Cedric Yiu, Rui Li, Konstantin Mishchenko, Stylianos I. Venieris, Hongxiang Fan

The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.

------------

`[2405.12523] Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models <https://arxiv.org/abs/2405.12523>`__ 单图像遗忘:多模态大型语言模型中的高效机器遗忘

::

    replaced with revised version Wed, 29 May 2024 05:27:38 GMT
    Submission history From: JiaQi Li [view email]
    [v1] Tue, 21 May 2024 06:27:12 UTC (3,929 KB)
    [v2] Wed, 29 May 2024 05:27:38 UTC (4,307 KB)
    Jiaqi Li, Qianshan Wei, Chuanyi Zhang, Guilin Qi, Miaozeng Du, Yongrui Chen, Sheng Bi

Machine unlearning empowers individuals with the `right to be forgotten' by removing their private or sensitive information encoded in machine learning models. However, it remains uncertain whether MU can be effectively applied to Multimodal Large Language Models (MLLMs), particularly in scenarios of forgetting the leaked visual data of concepts. To overcome the challenge, we propose an efficient method, Single Image Unlearning (SIU), to unlearn the visual recognition of a concept by fine-tuning a single associated image for few steps. SIU consists of two key aspects: (i) Constructing Multifaceted fine-tuning data. We introduce four targets, based on which we construct fine-tuning data for the concepts to be forgotten; (ii) Jointly training loss. To synchronously forget the visual recognition of concepts and preserve the utility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergence Loss combined with Cross Entropy loss. Alongside our method, we establish MMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics for its evaluation. Experimental results on MMUBench show that SIU completely surpasses the performance of existing methods. Furthermore, we surprisingly find that SIU can avoid invasive membership inference attacks and jailbreak attacks. To the best of our knowledge, we are the first to explore MU in MLLMs. We will release the code and benchmark in the near future.

------------

-----------------------
In-Context Learning (2)
-----------------------

`[2404.02054] Deconstructing In-Context Learning: Understanding Prompts via Corruption <https://arxiv.org/abs/2404.02054>`__ 解构语境学习:通过腐败理解提示

::

    replaced with revised version Wed, 29 May 2024 16:12:37 GMT
    Submission history From: Namrata Shivagunde [view email]
    [v1] Tue, 2 Apr 2024 15:50:55 UTC (959 KB)
    [v2] Wed, 29 May 2024 16:12:37 UTC (964 KB)
    Namrata Shivagunde, Vladislav Lialin, Sherin Muckatira, Anna Rumshisky

The ability of large language models (LLMs) to $``$learn in context$"$ based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt attributes and often produced contradictory results. Additionally, previous research either focused on models with fewer than 15 billion parameters or exclusively examined black-box models like GPT-3 or PaLM, making replication challenging. In the present study, we decompose the entire prompt into four components: task description, demonstration inputs, labels, and inline instructions provided for each demonstration. We investigate the effects of structural and semantic corruptions of these elements on model performance. We study models ranging from 1.5B to 70B in size, using ten datasets covering classification and generation tasks. We find that repeating text within the prompt boosts model performance, and bigger models ($\geq$30B) are more sensitive to the semantics of the prompt. Finally, we observe that adding task and inline instructions to the demonstrations enhances model performance even when the instructions are semantically corrupted.

------------

`[2402.10828] RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model <https://arxiv.org/abs/2402.10828>`__ RAG-Driver:基于检索增强上下文学习的多模态大型语言模型通用驾驶解释

::

    replaced with revised version Wed, 29 May 2024 14:44:20 GMT
    Submission history From: Jianhao Yuan [view email]
    [v1] Fri, 16 Feb 2024 16:57:18 UTC (14,101 KB)
    [v2] Wed, 29 May 2024 14:44:20 UTC (15,600 KB)
    Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, Matthew Gadd

We need to trust robots that use often opaque AI methods. They need to explain themselves to us, and we need to trust their explanation. In this regard, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment. To address these challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving. By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours.

------------

-------------
Reasoning (6)
-------------

`[2405.18711] Calibrating Reasoning in Language Models with Internal Consistency <https://arxiv.org/abs/2405.18711>`__ 基于内部一致性的语言模型推理校准

::

    Wed, 29 May 2024 02:44:12 GMT
    Zhihui Xie, Jizhou Guo, Tong Yu, Shuai Li

Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious mistakes and contradictions, raising doubts about their ability to robustly process and utilize generated rationales. In this work, we investigate CoT reasoning in LLMs through the lens of internal representations, focusing on how these representations are influenced by generated rationales. Our preliminary analysis reveals that while generated rationales improve answer accuracy, inconsistencies emerge between the model's internal representations in middle layers and those in final layers, potentially undermining the reliability of their reasoning processes. To address this, we propose internal consistency as a measure of the model's confidence by examining the agreement of latent predictions decoded from intermediate layers. Extensive empirical studies across different models and datasets demonstrate that internal consistency effectively distinguishes between correct and incorrect reasoning paths. Motivated by this, we propose a new approach to calibrate CoT reasoning by up-weighting reasoning paths with high internal consistency, resulting in a significant boost in reasoning performance. Further analysis uncovers distinct patterns in attention and feed-forward modules across layers, providing insights into the emergence of internal inconsistency. In summary, our results demonstrate the potential of using internal representations for self-evaluation of LLMs.

------------

`[2405.19164] Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in eDiscovery <https://arxiv.org/abs/2405.19164>`__ 从诉讼中学习:电子发现中检索和推理的图和llm

::

    Wed, 29 May 2024 15:08:55 GMT
    Sounak Lahiri, Sumit Pai, Tim Weninger, Sanmitra Bhattacharya

Electronic Discovery (eDiscovery) involves identifying relevant documents from a vast collection based on legal production requests. The integration of artificial intelligence (AI) and natural language processing (NLP) has transformed this process, helping document review and enhance efficiency and cost-effectiveness. Although traditional approaches like BM25 or fine-tuned pre-trained models are common in eDiscovery, they face performance, computational, and interpretability challenges. In contrast, Large Language Model (LLM)-based methods prioritize interpretability but sacrifice performance and throughput. This paper introduces DISCOvery Graph (DISCOG), a hybrid approach that combines the strengths of two worlds: a heterogeneous graph-based method for accurate document relevance prediction and subsequent LLM-driven approach for reasoning. Graph representational learning generates embeddings and predicts links, ranking the corpus for a given request, and the LLMs provide reasoning for document relevance. Our approach handles datasets with balanced and imbalanced distributions, outperforming baselines in F1-score, precision, and recall by an average of 12%, 3%, and 16%, respectively. In an enterprise context, our approach drastically reduces document review costs by 99.9% compared to manual processes and by 95% compared to LLM-based classification methods

------------

`[2405.19109] PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering <https://arxiv.org/abs/2405.19109>`__ PathReasoner:逻辑问答中等价扩展的推理路径建模

::

    Wed, 29 May 2024 14:14:05 GMT
    Fangzhi Xu, Qika Lin, Tianzhe Zhao, Jiawei Han, Jun Liu

Logical reasoning task has attracted great interest since it was proposed.
Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMs struggle in logical consistency modeling and logical structure perception. To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture \textbf{PathReasoner}.
It addresses the task from the views of both data and model. To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths. From the model perspective, we design a stack of transformer-style blocks. In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy. Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities.

------------

`[2405.19209] VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos <https://arxiv.org/abs/2405.19209>`__ VideoTree:基于自适应树的长视频LLM推理视频表示

::

    Wed, 29 May 2024 15:49:09 GMT
    Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, Mohit Bansal

Video-language understanding tasks have focused on short video clips, often struggling with long-form video understanding tasks. Recently, many long video-language understanding approaches have leveraged the reasoning capabilities of Large Language Models (LLMs) to perform long video QA, transforming videos into densely sampled frame captions, and asking LLMs to respond to text queries over captions. However, the frames used for captioning are often redundant and contain irrelevant information, making dense sampling inefficient, and ignoring the fact that video QA requires varying levels of granularity, with some video segments being highly relevant to the question (needing more fine-grained detail) while others being less relevant. Thus, these LLM-based approaches are prone to missing information and operate on large numbers of irrelevant captions, lowering both performance and efficiency.
To address these issues, we introduce VideoTree, a query-adaptive and hierarchical framework for long-video understanding with LLMs. VideoTree dynamically extracts query-related information from a video and builds a tree-based representation for LLM reasoning. First, VideoTree adaptively selects frames for captioning by iteratively clustering frames based on their visual features and scoring clusters using their relevance to the query.
Second, it organizes visual clusters into a query-adaptive and hierarchical tree structure; the tree encodes varying levels of granularity, with higher resolution on relevant segments. Finally, VideoTree produces an answer by traversing the tree's keyframes and passing their captions to an LLM answerer.
Our method improves both reasoning accuracy and efficiency compared to existing methods: VideoTree achieves a 7.0%, 2.2%, and 2.7% accuracy gain over baselines on the EgoSchema, NExT-QA, and IntentQA benchmarks, respectively, while reducing inference time by 40%.

------------

`[2405.13872] Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models <https://arxiv.org/abs/2405.13872>`__ 多模态大型语言模型中思维图像提示的视觉推理精化

::

    replaced with revised version Wed, 29 May 2024 02:24:36 GMT
    Submission history From: Qiji Zhou [view email]
    [v1] Wed, 22 May 2024 17:56:51 UTC (6,529 KB)
    [v2] Wed, 29 May 2024 02:24:36 UTC (6,530 KB)
    Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, Yue Zhang

Recent advancements in Chain-of-Thought (CoT) and related rationale-based works have significantly improved the performance of Large Language Models (LLMs) in complex reasoning tasks. With the evolution of Multimodal Large Language Models (MLLMs), enhancing their capability to tackle complex multimodal reasoning problems is a crucial frontier. However, incorporating multimodal rationales in CoT has yet to be thoroughly investigated. We propose the Image-of-Thought (IoT) prompting method, which helps MLLMs to extract visual rationales step-by-step. Specifically, IoT prompting can automatically design critical visual information extraction operations based on the input images and questions. Each step of visual information refinement identifies specific visual rationales that support answers to complex visual reasoning questions. Beyond the textual CoT, IoT simultaneously utilizes visual and textual rationales to help MLLMs understand complex multimodal information. IoT prompting has improved zero-shot visual reasoning performance across various visual understanding tasks in different MLLMs. Moreover, the step-by-step visual feature explanations generated by IoT prompting elucidate the visual reasoning process, aiding in analyzing the cognitive processes of large multimodal models

------------

`[2405.16802] AutoCV: Empowering Reasoning with Automated Process Labeling via Confidence Variation <https://arxiv.org/abs/2405.16802>`__ AutoCV:基于置信度变化的自动过程标记授权推理

::

    replaced with revised version Wed, 29 May 2024 01:47:35 GMT
    Submission history From: Jianqiao Lu [view email]
    [v1] Mon, 27 May 2024 03:44:24 UTC (203 KB)
    [v2] Tue, 28 May 2024 09:35:27 UTC (351 KB)
    [v3] Wed, 29 May 2024 01:47:35 UTC (345 KB)
    Jianqiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yingjia Wan, Yinya Huang, Zhijiang Guo

In this work, we propose a novel method named \textbf{Auto}mated Process Labeling via \textbf{C}onfidence \textbf{V}ariation (\textbf{\textsc{AutoCV}}) to enhance the reasoning capabilities of large language models (LLMs) by automatically annotating the reasoning steps. Our approach begins by training a verification model on the correctness of final answers, enabling it to generate automatic process annotations. This verification model assigns a confidence score to each reasoning step, indicating the probability of arriving at the correct final answer from that point onward. We detect relative changes in the verification's confidence scores across reasoning steps to automatically annotate the reasoning process. This alleviates the need for numerous manual annotations or the high computational costs associated with model-induced annotation approaches. We experimentally validate that the confidence variations learned by the verification model trained on the final answer correctness can effectively identify errors in the reasoning steps. Subsequently, we demonstrate that the process annotations generated by \textsc{AutoCV} can improve the accuracy of the verification model in selecting the correct answer from multiple outputs generated by LLMs. Notably, we achieve substantial improvements across five datasets in mathematics and commonsense reasoning. The source code of \textsc{AutoCV} is available at \url{this https URL}.

------------

-----------------------
Retrieval-Augmented (8)
-----------------------

`[2405.19164] Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in eDiscovery <https://arxiv.org/abs/2405.19164>`__ 从诉讼中学习:电子发现中检索和推理的图和llm

::

    Wed, 29 May 2024 15:08:55 GMT
    Sounak Lahiri, Sumit Pai, Tim Weninger, Sanmitra Bhattacharya

Electronic Discovery (eDiscovery) involves identifying relevant documents from a vast collection based on legal production requests. The integration of artificial intelligence (AI) and natural language processing (NLP) has transformed this process, helping document review and enhance efficiency and cost-effectiveness. Although traditional approaches like BM25 or fine-tuned pre-trained models are common in eDiscovery, they face performance, computational, and interpretability challenges. In contrast, Large Language Model (LLM)-based methods prioritize interpretability but sacrifice performance and throughput. This paper introduces DISCOvery Graph (DISCOG), a hybrid approach that combines the strengths of two worlds: a heterogeneous graph-based method for accurate document relevance prediction and subsequent LLM-driven approach for reasoning. Graph representational learning generates embeddings and predicts links, ranking the corpus for a given request, and the LLMs provide reasoning for document relevance. Our approach handles datasets with balanced and imbalanced distributions, outperforming baselines in F1-score, precision, and recall by an average of 12%, 3%, and 16%, respectively. In an enterprise context, our approach drastically reduces document review costs by 99.9% compared to manual processes and by 95% compared to LLM-based classification methods

------------

`[2405.18727] CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control <https://arxiv.org/abs/2405.18727>`__ CtrlA:探针引导控制的自适应检索增强生成

::

    Wed, 29 May 2024 03:17:16 GMT
    Huanshuo Liu and Hao Zhang and Zhijiang Guo and Kuicai Dong and Xiangyang Li and Yi Quan Lee and Cong Zhang and Yong Liu

Retrieval-augmented generation (RAG) has emerged as a promising solution for mitigating hallucinations of large language models (LLMs) with retrieved external knowledge. Adaptive RAG enhances this approach by dynamically assessing the retrieval necessity, aiming to balance external and internal knowledge usage. However, existing adaptive RAG methods primarily realize retrieval on demand by relying on superficially verbalize-based or probability-based feedback of LLMs, or directly fine-tuning LLMs via carefully crafted datasets, resulting in unreliable retrieval necessity decisions, heavy extra costs, and sub-optimal response generation. We present the first attempts to delve into the internal states of LLMs to mitigate such issues by introducing an effective probe-guided adaptive RAG framework, termed CtrlA.
Specifically, CtrlA employs an honesty probe to regulate the LLM's behavior by manipulating its representations for increased honesty, and a confidence probe to monitor the internal states of LLM and assess confidence levels, determining the retrieval necessity during generation. Experiments show that CtrlA is superior to existing adaptive RAG methods on a diverse set of tasks, the honesty control can effectively make LLMs more honest and confidence monitoring is proven to be a promising indicator of retrieval trigger. Our codes are available at https://github.com/HSLiu-Initial/CtrlA.git.

------------

`[2405.18740] Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs <https://arxiv.org/abs/2405.18740>`__ 反向图像检索提示多模态llm的参数记忆

::

    Wed, 29 May 2024 04:00:41 GMT
    Jialiang Xu, Michael Moor, Jure Leskovec

Despite impressive advances in recent multimodal large language models (MLLMs), state-of-the-art models such as from the GPT-4 suite still struggle with knowledge-intensive tasks. To address this, we consider Reverse Image Retrieval (RIR) augmented generation, a simple yet effective strategy to augment MLLMs with web-scale reverse image search results. RIR robustly improves knowledge-intensive visual question answering (VQA) of GPT-4V by 37-43%, GPT-4 Turbo by 25-27%, and GPT-4o by 18-20% in terms of open-ended VQA evaluation metrics. To our surprise, we discover that RIR helps the model to better access its own world knowledge. Concretely, our experiments suggest that RIR augmentation helps by providing further visual and textual cues without necessarily containing the direct answer to a query. In addition, we elucidate cases in which RIR can hurt performance and conduct a human evaluation.
Finally, we find that the overall advantage of using RIR makes it difficult for an agent that can choose to use RIR to perform better than an approach where RIR is the default setting.

------------

`[2310.07713] InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining <https://arxiv.org/abs/2310.07713>`__ InstructRetro:检索后增强预训练的指令调优

::

    replaced with revised version Wed, 29 May 2024 04:15:39 GMT
    Submission history From: Wei Ping [view email]
    [v1] Wed, 11 Oct 2023 17:59:05 UTC (1,795 KB)
    [v2] Wed, 31 Jan 2024 23:27:26 UTC (2,097 KB)
    [v3] Wed, 29 May 2024 04:15:39 UTC (2,100 KB)
    Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, Bryan Catanzaro

Pretraining auto-regressive large language models~(LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA and reading comprehension tasks, 10% over GPT across 4 challenging long-form QA tasks, and 16% over GPT across 3 summarization tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. Our results highlight the promising direction to obtain a better GPT decoder through continued pretraining with retrieval before instruction tuning. Our code and checkpoints are publicly available at: this https URL.

------------

`[2404.14760] Retrieval Augmented Generation for Domain-specific Question Answering <https://arxiv.org/abs/2404.14760>`__ 面向领域问答的检索增强生成

::

    replaced with revised version Wed, 29 May 2024 16:18:02 GMT
    Submission history From: Sanat Sharma [view email]
    [v1] Tue, 23 Apr 2024 05:51:45 UTC (468 KB)
    [v2] Wed, 29 May 2024 16:18:02 UTC (535 KB)
    Sanat Sharma, David Seunghyun Yoon, Franck Dernoncourt, Dewang Sultania, Karishma Bagga, Mengjiao Zhang, Trung Bui, Varun Kotte

Question answering (QA) has become an important application in the advanced development of large language models. General pre-trained large language models for question-answering are not trained to properly understand the knowledge or terminology for a specific domain, such as finance, healthcare, education, and customer service for a product. To better cater to domain-specific understanding, we build an in-house question-answering system for Adobe products. We propose a novel framework to compile a large question-answer database and develop the approach for retrieval-aware finetuning of a Large Language model. We showcase that fine-tuning the retriever leads to major improvements in the final generation. Our overall approach reduces hallucinations during generation while keeping in context the latest retrieval information for contextual grounding.

------------

`[2404.19232] GRAMMAR: Grounded and Modular Methodology for Assessment of Domain-Specific Retrieval-Augmented Language Model <https://arxiv.org/abs/2404.19232>`__ 语法:特定领域检索增强语言模型评估的基础和模块化方法

::

    replaced with revised version Wed, 29 May 2024 11:12:21 GMT
    Submission history From: Xinzhe Li [view email]
    [v1] Tue, 30 Apr 2024 03:29:30 UTC (8,420 KB)
    [v2] Thu, 2 May 2024 05:32:23 UTC (8,420 KB)
    [v3] Thu, 9 May 2024 01:46:48 UTC (8,420 KB)
    [v4] Wed, 29 May 2024 11:12:21 UTC (8,414 KB)
    Xinzhe Li, Ming Liu and Shang Gao

Retrieval-augmented Generation (RAG) systems have been actively studied and deployed across various industries to query on domain-specific knowledge base. However, evaluating these systems presents unique challenges due to the scarcity of domain-specific queries and corresponding ground truths, as well as a lack of systematic approaches to diagnosing the cause of failure cases -- whether they stem from knowledge deficits or issues related to system robustness. To address these challenges, we introduce GRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation framework comprising two key elements: 1) a data generation process that leverages relational databases and LLMs to efficiently produce scalable query-answer pairs. This method facilitates the separation of query logic from linguistic variations for enhanced debugging capabilities; and 2) an evaluation framework that differentiates knowledge gaps from robustness and enables the identification of defective modules. Our empirical results underscore the limitations of current reference-free evaluation approaches and the reliability of GRAMMAR to accurately identify model vulnerabilities.

------------

`[2405.18035] Instruction Tuning with Retrieval-based Examples Ranking for Aspect-based Sentiment Analysis <https://arxiv.org/abs/2405.18035>`__ 面向方面情感分析的基于检索示例排序的指令调优

::

    replaced with revised version Wed, 29 May 2024 06:35:46 GMT
    Submission history From: Guangmin Zheng [view email]
    [v1] Tue, 28 May 2024 10:39:10 UTC (7,138 KB)
    [v2] Wed, 29 May 2024 06:35:46 UTC (7,138 KB)
    Guangmin Zheng, Jin Wang, Liang-Chih Yu, Xuejie Zhang

Aspect-based sentiment analysis (ABSA) identifies sentiment information related to specific aspects and provides deeper market insights to businesses and organizations. With the emergence of large language models (LMs), recent studies have proposed using fixed examples for instruction tuning to reformulate ABSA as a generation task. However, the performance is sensitive to the selection of in-context examples; several retrieval methods are based on surface similarity and are independent of the LM generative objective. This study proposes an instruction learning method with retrieval-based example ranking for ABSA tasks. For each target sample, an LM was applied as a scorer to estimate the likelihood of the output given the input and a candidate example as the prompt, and training examples were labeled as positive or negative by ranking the scores. An alternating training schema is proposed to train both the retriever and LM. Instructional prompts can be constructed using high-quality examples. The LM is used for both scoring and inference, improving the generation efficiency without incurring additional computational costs or training difficulties. Extensive experiments on three ABSA subtasks verified the effectiveness of the proposed method, demonstrating its superiority over various strong baseline models. Code and data are released at this https URL.

------------

`[2402.10828] RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model <https://arxiv.org/abs/2402.10828>`__ RAG-Driver:基于检索增强上下文学习的多模态大型语言模型通用驾驶解释

::

    replaced with revised version Wed, 29 May 2024 14:44:20 GMT
    Submission history From: Jianhao Yuan [view email]
    [v1] Fri, 16 Feb 2024 16:57:18 UTC (14,101 KB)
    [v2] Wed, 29 May 2024 14:44:20 UTC (15,600 KB)
    Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, Matthew Gadd

We need to trust robots that use often opaque AI methods. They need to explain themselves to us, and we need to trust their explanation. In this regard, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment. To address these challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving. By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours.

------------

---------
Agent (3)
---------

`[2405.16334] Devil's Advocate: Anticipatory Reflection for LLM Agents <https://arxiv.org/abs/2405.16334>`__ 魔鬼代言人:LLM代理的预期反思

::

    replaced with revised version Wed, 29 May 2024 14:12:53 GMT
    Submission history From: Haoyu Wang [view email]
    [v1] Sat, 25 May 2024 19:20:15 UTC (1,067 KB)
    [v2] Tue, 28 May 2024 03:22:44 UTC (1,083 KB)
    [v3] Wed, 29 May 2024 14:12:53 UTC (1,067 KB)
    Haoyu Wang and Tao Li and Zhiwei Deng and Dan Roth and Yang Li

In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks. Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions. We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement. By deploying and experimenting with this methodology - a zero-shot approach - within WebArena for practical tasks in web environments, our agent demonstrates superior performance over existing zero-shot methods. The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions needed to achieve a task.

------------

`[2405.17009] Position: Foundation Agents as the Paradigm Shift for Decision Making <https://arxiv.org/abs/2405.17009>`__ 职位:作为决策范式转换的基础代理

::

    replaced with revised version Wed, 29 May 2024 14:15:09 GMT
    Submission history From: Xiaoqian Liu [view email]
    [v1] Mon, 27 May 2024 09:54:50 UTC (1,335 KB)
    [v2] Tue, 28 May 2024 13:00:14 UTC (1,332 KB)
    [v3] Wed, 29 May 2024 14:15:09 UTC (1,332 KB)
    Xiaoqian Liu, Xingzhou Lou, Jianbin Jiao, Junge Zhang

Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies. Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization. In contrast, foundation models in language and vision have showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents. This proposal is underpinned by the formulation of foundation agents with their fundamental characteristics and challenges motivated by the success of large language models (LLMs). Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs. Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future.

------------

`[2402.06700] Entropy-Regularized Token-Level Policy Optimization for Language Agent Reinforcement <https://arxiv.org/abs/2402.06700>`__ 熵正则化令牌级语言智能体强化策略优化

::

    replaced with revised version Wed, 29 May 2024 12:15:46 GMT
    Submission history From: Muning Wen [view email]
    [v1] Fri, 9 Feb 2024 07:45:26 UTC (504 KB)
    [v2] Tue, 5 Mar 2024 05:17:21 UTC (504 KB)
    [v3] Wed, 29 May 2024 12:15:46 UTC (500 KB)
    Muning Wen, Cheng Deng, Jun Wang, Weinan Zhang and Ying Wen

Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed to harmonize the RL process with the principles of language modeling. This methodology decomposes the Q-function update from a coarse action-level view to a more granular token-level perspective, backed by theoretical proof of optimization consistency. Crucially, this decomposition renders linear time complexity in action exploration. We assess the effectiveness of ETPO within a simulated environment that models data science code generation as a series of multi-step interactive tasks; results underline ETPO's potential as a robust method for refining the interactive decision-making capabilities of language agents. For a more detailed preliminary work describing our motivation for token-level decomposition and applying it in PPO methods, please refer to arXiv:2405.15821.

------------

----------
Other (92)
----------

`[2405.18581] Unleashing the Potential of Text-attributed Graphs: Automatic Relation Decomposition via Large Language Models <https://arxiv.org/abs/2405.18581>`__ 释放文本属性图的潜力:基于大型语言模型的自动关系分解

::

    Tue, 28 May 2024 20:54:47 GMT
    Hyunjin Seo, Taewon Kim, June Yong Yang, Eunho Yang

Recent advancements in text-attributed graphs (TAGs) have significantly improved the quality of node features by using the textual modeling capabilities of language models. Despite this success, utilizing text attributes to enhance the predefined graph structure remains largely unexplored. Our extensive analysis reveals that conventional edges on TAGs, treated as a single relation (e.g., hyperlinks) in previous literature, actually encompass mixed semantics (e.g., "advised by" and "participates in").
This simplification hinders the representation learning process of Graph Neural Networks (GNNs) on downstream tasks, even when integrated with advanced node features. In contrast, we discover that decomposing these edges into distinct semantic relations significantly enhances the performance of GNNs. Despite this, manually identifying and labeling of edges to corresponding semantic relations is labor-intensive, often requiring domain expertise. To this end, we introduce RoSE (Relation-oriented Semantic Edge-decomposition), a novel framework that leverages the capability of Large Language Models (LLMs) to decompose the graph structure by analyzing raw text attributes - in a fully automated manner. RoSE operates in two stages: (1) identifying meaningful relations using an LLM-based generator and discriminator, and (2) categorizing each edge into corresponding relations by analyzing textual contents associated with connected nodes via an LLM-based decomposer. Extensive experiments demonstrate that our model-agnostic framework significantly enhances node classification performance across various datasets, with improvements of up to 16% on the Wisconsin dataset.

------------

`[2405.18636] ChatGPT as the Marketplace of Ideas: Should Truth-Seeking Be the Goal of AI Content Governance? <https://arxiv.org/abs/2405.18636>`__ ChatGPT作为思想的市场:寻求真理应该是AI内容治理的目标吗?

::

    Tue, 28 May 2024 22:38:24 GMT
    Jiawei Zhang

As one of the most enduring metaphors within legal discourse, the marketplace of ideas has wielded considerable influence over the jurisprudential landscape for decades. A century after the inception of this theory, ChatGPT emerged as a revolutionary technological advancement in the twenty-first century. This research finds that ChatGPT effectively manifests the marketplace metaphor. It not only instantiates the promises envisaged by generations of legal scholars but also lays bare the perils discerned through sustained academic critique.
Specifically, the workings of ChatGPT and the marketplace of ideas theory exhibit at least four common features: arena, means, objectives, and flaws.
These shared attributes are sufficient to render ChatGPT historically the most qualified engine for actualizing the marketplace of ideas theory.
The comparison of the marketplace theory and ChatGPT merely marks a starting point. A more meaningful undertaking entails reevaluating and reframing both internal and external AI policies by referring to the accumulated experience, insights, and suggestions researchers have raised to fix the marketplace theory. Here, a pivotal issue is: should truth-seeking be set as the goal of AI content governance? Given the unattainability of the absolute truth-seeking goal, I argue against adopting zero-risk policies. Instead, a more judicious approach would be to embrace a knowledge-based alternative wherein large language models (LLMs) are trained to generate competing and divergent viewpoints based on sufficient justifications. This research also argues that so-called AI content risks are not created by AI companies but are inherent in the entire information ecosystem. Thus, the burden of managing these risks should be distributed among different social actors, rather than being solely shouldered by chatbot companies.

------------

`[2405.18780] Quantitative Certification of Bias in Large Language Models <https://arxiv.org/abs/2405.18780>`__ 大型语言模型偏差的定量证明

::

    Wed, 29 May 2024 05:39:37 GMT
    Isha Chaudhary, Qian Hu, Manoj Kumar, Morteza Ziyadi, Rahul Gupta, Gagandeep Singh

Large Language Models (LLMs) can produce responses that exhibit social biases and support stereotypes. However, conventional benchmarking is insufficient to thoroughly evaluate LLM bias, as it can not scale to large sets of prompts and provides no guarantees. Therefore, we propose a novel certification framework QuaCer-B (Quantitative Certification of Bias) that provides formal guarantees on obtaining unbiased responses from target LLMs under large sets of prompts. A certificate consists of high-confidence bounds on the probability of obtaining biased responses from the LLM for any set of prompts containing sensitive attributes, sampled from a distribution. We illustrate the bias certification in LLMs for prompts with various prefixes drawn from given distributions. We consider distributions of random token sequences, mixtures of manual jailbreaks, and jailbreaks in the LLM's embedding space to certify its bias. We certify popular LLMs with QuaCer-B and present novel insights into their biases.

------------

`[2405.18870] LLMs achieve adult human performance on higher-order theory of mind tasks <https://arxiv.org/abs/2405.18870>`__ llm在高阶心智理论任务上实现成人的表现

::

    Wed, 29 May 2024 08:31:16 GMT
    Winnie Street, John Oliver Siy, Geoff Keeling, Adrien Baranes, Benjamin Barnett, Michael McKibben, Tatenda Kanyere, Alison Lentz, Blaise Aguera y Arcas, Robin I. M. Dunbar

This paper examines the extent to which large language models (LLMs) have developed higher-order theory of mind (ToM); the human ability to reason about multiple mental and emotional states in a recursive manner (e.g. I think that you believe that she knows). This paper builds on prior work by introducing a handwritten test suite -- Multi-Order Theory of Mind Q&A -- and using it to compare the performance of five LLMs to a newly gathered adult human benchmark.
We find that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences. Our results suggest that there is an interplay between model size and finetuning for the realisation of ToM abilities, and that the best-performing LLMs have developed a generalised capacity for ToM. Given the role that higher-order ToM plays in a wide range of cooperative and competitive human behaviours, these findings have significant implications for user-facing LLM applications.

------------

`[2405.19032] Large Language Models for Code Summarization <https://arxiv.org/abs/2405.19032>`__ 用于代码摘要的大型语言模型

::

    Wed, 29 May 2024 12:18:51 GMT
    Bal\'azs Szalontai, Gerg\H{o} Szalay, Tam\'as M\'arton, Anna Sike, Bal\'azs Pint\'er, Tibor Gregorics

Recently, there has been increasing activity in using deep learning for software engineering, including tasks like code generation and summarization.
In particular, the most recent coding Large Language Models seem to perform well on these problems. In this technical report, we aim to review how these models perform in code explanation/summarization, while also investigating their code generation capabilities (based on natural language descriptions).

------------

`[2405.19132] Analyzing Chat Protocols of Novice Programmers Solving Introductory Programming Tasks with ChatGPT <https://arxiv.org/abs/2405.19132>`__ 用ChatGPT分析新手程序员解决编程入门任务时的聊天协议

::

    Wed, 29 May 2024 14:38:32 GMT
    Andreas Scholl, Daniel Schiffner and Natalie Kiesler

Large Language Models (LLMs) have taken the world by storm, and students are assumed to use related tools at a great scale. In this research paper we aim to gain an understanding of how introductory programming students chat with LLMs and related tools, e.g., ChatGPT-3.5. To address this goal, computing students at a large German university were motivated to solve programming exercises with the assistance of ChatGPT as part of their weekly introductory course exercises. Then students (n=213) submitted their chat protocols (with 2335 prompts in sum) as data basis for this analysis. The data was analyzed w.r.t.
the prompts, frequencies, the chats' progress, contents, and other use pattern, which revealed a great variety of interactions, both potentially supportive and concerning. Learning about students' interactions with ChatGPT will help inform and align teaching practices and instructions for future introductory programming courses in higher education.

------------

`[2405.19255] Towards Next-Generation Urban Decision Support Systems through AI-Powered Generation of Scientific Ontology using Large Language Models -- A Case in Optimizing Intermodal Freight Transportation <https://arxiv.org/abs/2405.19255>`__ 通过使用大型语言模型人工智能驱动的科学本体生成走向下一代城市决策支持系统——以优化联运货物运输为例

::

    Wed, 29 May 2024 16:40:31 GMT
    Jose Tupayachi, Haowen Xu, Olufemi A. Omitaomu, Mustafa Can Camur, Aliza Sharmin, Xueping Li

The incorporation of Artificial Intelligence (AI) models into various optimization systems is on the rise. Yet, addressing complex urban and environmental management problems normally requires in-depth domain science and informatics expertise. This expertise is essential for deriving data and simulation-driven for informed decision support. In this context, we investigate the potential of leveraging the pre-trained Large Language Models (LLMs). By adopting ChatGPT API as the reasoning core, we outline an integrated workflow that encompasses natural language processing, methontology-based prompt tuning, and transformers. This workflow automates the creation of scenario-based ontology using existing research articles and technical manuals of urban datasets and simulations. The outcomes of our methodology are knowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL).
These facilitate the development of urban decision support systems by enhancing the data and metadata modeling, the integration of complex datasets, the coupling of multi-domain simulation models, and the formulation of decision-making metrics and workflow. The feasibility of our methodology is evaluated through a comparative analysis that juxtaposes our AI-generated ontology with the well-known Pizza Ontology employed in tutorials for popular ontology software (e.g., prot\'eg\'e). We close with a real-world case study of optimizing the complex urban system of multi-modal freight transportation by generating anthologies of various domain data and simulations to support informed decision-making.

------------

`[2405.19313] Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice <https://arxiv.org/abs/2405.19313>`__ 经过算术训练的语言模型可以预测人类的风险和跨期选择

::

    Wed, 29 May 2024 17:37:14 GMT
    Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths

The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models. For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences. Consequently, the origins of these behavioral similarities are not well understood. In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models. This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors. We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations. We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models. Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making. Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data.

------------

`[2405.18492] LLMs and Memorization: On Quality and Specificity of Copyright Compliance <https://arxiv.org/abs/2405.18492>`__ LLMs和记忆:关于版权遵守的质量和特异性

::

    Tue, 28 May 2024 18:01:52 GMT
    Felix B Mueller, Rebekka G\"orge, Anna K Bernzen, Janna C Pirk, Maximilian Poretschkin

Memorization in large language models (LLMs) is a growing concern. LLMs have been shown to easily reproduce parts of their training data, including copyrighted work. This is an important problem to solve, as it may violate existing copyright laws as well as the European AI Act. In this work, we propose a systematic analysis to quantify the extent of potential copyright infringements in LLMs using European law as an example. Unlike previous work, we evaluate instruction-finetuned models in a realistic end-user scenario. Our analysis builds on a proposed threshold of 160 characters, which we borrow from the German Copyright Service Provider Act and a fuzzy text matching algorithm to identify potentially copyright-infringing textual reproductions. The specificity of countermeasures against copyright infringement is analyzed by comparing model behavior on copyrighted and public domain data. We investigate what behaviors models show instead of producing protected text (such as refusal or hallucination) and provide a first legal assessment of these behaviors. We find that there are huge differences in copyright compliance, specificity, and appropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous perform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing a particularly low absolute number of potential copyright violations. Code will be published soon.

------------

`[2405.18540] Learning diverse attacks on large language models for robust red-teaming and safety tuning <https://arxiv.org/abs/2405.18540>`__ 在大型语言模型上学习多种攻击以进行鲁棒红队和安全调优

::

    Tue, 28 May 2024 19:16:17 GMT
    Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, Moksh Jain

Red-teaming, or identifying prompts that elicit harmful responses, is a critical step in ensuring the safe and responsible deployment of large language models (LLMs). Developing effective protection against many modes of attack prompts requires discovering diverse attacks. Automated red-teaming typically uses reinforcement learning to fine-tune an attacker language model to generate prompts that elicit undesirable responses from a target LLM, as measured, for example, by an auxiliary toxicity classifier. We show that even with explicit regularization to favor novelty and diversity, existing approaches suffer from mode collapse or fail to generate effective attacks. As a flexible and probabilistically principled alternative, we propose to use GFlowNet fine-tuning, followed by a secondary smoothing phase, to train the attacker model to generate diverse and effective attack prompts. We find that the attacks generated by our method are effective against a wide range of target LLMs, both with and without safety tuning, and transfer well between target LLMs. Finally, we demonstrate that models safety-tuned using a dataset of red-teaming prompts generated by our method are robust to attacks from other RL-based red-teaming approaches.

------------

`[2405.18638] ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models <https://arxiv.org/abs/2405.18638>`__ 考虑人工评估框架:重新思考人工对生成式大型语言模型的评估

::

    Tue, 28 May 2024 22:45:28 GMT
    Aparna Elangovan, Ling Liu, Lei Xu, Sravan Bodapati, Dan Roth

In this position paper, we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable. The conclusions from these evaluations, thus, must consider factors such as usability, aesthetics, and cognitive biases. We highlight how cognitive biases can conflate fluent information and truthfulness, and how cognitive uncertainty affects the reliability of rating scores such as Likert.
Furthermore, the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models -- which requires effective test sets. The scalability of human evaluation is also crucial to wider adoption. Hence, to design an effective human evaluation system in the age of generative NLP, we propose the ConSiDERS-The-Human evaluation framework consisting of 6 pillars --Consistency, Scoring Critera, Differentiating, User Experience, Responsible, and Scalability.

------------

`[2405.18649] Training LLMs to Better Self-Debug and Explain Code <https://arxiv.org/abs/2405.18649>`__ 培训llm，使其更好地自我调试和解释代码

::

    Tue, 28 May 2024 23:20:24 GMT
    Nan Jiang, Xiaopeng Li, Shiqi Wang, Qiang Zhou, Soneya Binta Hossain, Baishakhi Ray, Varun Kumar, Xiaofei Ma, Anoop Deoras

In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks. Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose a training framework that significantly improves self-debugging capability of LLMs.
Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement.
We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by 9.30% over four benchmarks. RL training brings additional up to 3.54% improvement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show iterative refinement ability, and can keep refining code continuously.
Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.

------------

`[2405.18662] Understanding Intrinsic Socioeconomic Biases in Large Language Models <https://arxiv.org/abs/2405.18662>`__ 理解大型语言模型中的内在社会经济偏见

::

    Tue, 28 May 2024 23:54:44 GMT
    Mina Arzaghi, Florian Carichon, Golnoosh Farnadi

Large Language Models (LLMs) are increasingly integrated into critical decision-making processes, such as loan approvals and visa applications, where inherent biases can lead to discriminatory outcomes. In this paper, we examine the nuanced relationship between demographic attributes and socioeconomic biases in LLMs, a crucial yet understudied area of fairness in LLMs. We introduce a novel dataset of one million English sentences to systematically quantify socioeconomic biases across various demographic groups. Our findings reveal pervasive socioeconomic biases in both established models such as GPT-2 and state-of-the-art models like Llama 2 and Falcon. We demonstrate that these biases are significantly amplified when considering intersectionality, with LLMs exhibiting a remarkable capacity to extract multiple demographic attributes from names and then correlate them with specific socioeconomic biases. This research highlights the urgent necessity for proactive and robust bias mitigation techniques to safeguard against discriminatory outcomes when deploying these powerful models in critical real-world applications.

------------

`[2405.18682] Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine Reading Comprehension <https://arxiv.org/abs/2405.18682>`__ GPT能重新定义医学理解吗?评估GPT在生物医学机器阅读理解上的表现

::

    Wed, 29 May 2024 01:12:53 GMT
    Shubham Vatsal, Ayush Singh

Large language models (LLMs) have shown remarkable performance on many tasks in different domains. However, their performance in closed-book biomedical machine reading comprehension (MRC) has not been evaluated in depth. In this work, we evaluate GPT on four closed-book biomedical MRC benchmarks. We experiment with different conventional prompting techniques as well as introduce our own novel prompting method. To solve some of the retrieval problems inherent to LLMs, we propose a prompting strategy named Implicit Retrieval Augmented Generation (RAG) that alleviates the need for using vector databases to retrieve important chunks in traditional RAG setups. Moreover, we report qualitative assessments on the natural language generation outputs from our approach. The results show that our new prompting technique is able to get the best performance in two out of four datasets and ranks second in rest of them. Experiments show that modern-day LLMs like GPT even in a zero-shot setting can outperform supervised models, leading to new state-of-the-art (SoTA) results on two of the benchmarks.

------------

`[2405.18719] Contextual Position Encoding: Learning to Count What's Important <https://arxiv.org/abs/2405.18719>`__ 上下文位置编码:学习计算什么是重要的

::

    Wed, 29 May 2024 02:57:15 GMT
    Olga Golovneva, Tianlu Wang, Jason Weston, Sainbayar Sukhbaatar

The attention mechanism is a critical component of Large Language Models (LLMs) that allows tokens in a sequence to interact with each other, but is order-invariant. Incorporating position encoding (PE) makes it possible to address by position, such as attending to the i-th token. However, current PE methods use token counts to derive position, and thus cannot generalize to higher levels of abstraction, such as attending to the i-th sentence. In this paper, we propose a new position encoding method, Contextual Position Encoding (CoPE), that allows positions to be conditioned on context by incrementing position only on certain tokens determined by the model. This allows more general position addressing such as attending to the $i$-th particular word, noun, or sentence. We show that CoPE can solve the selective copy, counting and Flip-Flop tasks where popular position embeddings fail, and improves perplexity on language modeling and coding tasks.

------------

`[2405.18741] Genshin: General Shield for Natural Language Processing with Large Language Models <https://arxiv.org/abs/2405.18741>`__ Genshin:基于大型语言模型的自然语言处理通用盾

::

    Wed, 29 May 2024 04:04:05 GMT
    Xiao Peng, Tao Liu, Ying Wang

Large language models (LLMs) like ChatGPT, Gemini, or LLaMA have been trending recently, demonstrating considerable advancement and generalizability power in countless domains. However, LLMs create an even bigger black box exacerbating opacity, with interpretability limited to few approaches. The uncertainty and opacity embedded in LLMs' nature restrict their application in high-stakes domains like financial fraud, phishing, etc. Current approaches mainly rely on traditional textual classification with posterior interpretable algorithms, suffering from attackers who may create versatile adversarial samples to break the system's defense, forcing users to make trade-offs between efficiency and robustness. To address this issue, we propose a novel cascading framework called Genshin (General Shield for Natural Language Processing with Large Language Models), utilizing LLMs as defensive one-time plug-ins. Unlike most applications of LLMs that try to transform text into something new or structural, Genshin uses LLMs to recover text to its original state. Genshin aims to combine the generalizability of the LLM, the discrimination of the median model, and the interpretability of the simple model. Our experiments on the task of sentimental analysis and spam detection have shown fatal flaws of the current median models and exhilarating results on LLMs' recovery ability, demonstrating that Genshin is both effective and efficient. In our ablation study, we unearth several intriguing observations. Utilizing the LLM defender, a tool derived from the 4th paradigm, we have reproduced BERT's 15% optimal mask rate results in the 3rd paradigm of NLP. Additionally, when employing the LLM as a potential adversarial tool, attackers are capable of executing effective attacks that are nearly semantically lossless.

------------

`[2405.18822] Toxicity Detection for Free <https://arxiv.org/abs/2405.18822>`__ 免费进行毒性检测

::

    Wed, 29 May 2024 07:03:31 GMT
    Zhanhao Hu, Julien Piet, Geng Zhao, Jiantao Jiao, David Wagner

Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples. In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare. In this paper, we explore Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves. We found significant gaps between benign and toxic prompts in the distribution of alternative refusal responses and in the distribution of the first response token's logits.
These gaps can be used to detect toxicities: We show that a toy model based on the logits of specific starting tokens gets reliable performance, while requiring no training or additional computational cost. We build a more robust detector using a sparse logistic regression model on the first response token logits, which greatly exceeds SOTA detectors under multiple metrics.

------------

`[2405.18906] Language Generation with Strictly Proper Scoring Rules <https://arxiv.org/abs/2405.18906>`__ 严格适当的评分规则的语言生成

::

    Wed, 29 May 2024 09:09:00 GMT
    Chenze Shao, Fandong Meng, Yijin Liu, Jie Zhou

Language generation based on maximum likelihood estimation (MLE) has become the fundamental approach for text generation. Maximum likelihood estimation is typically performed by minimizing the log-likelihood loss, also known as the logarithmic score in statistical decision theory. The logarithmic score is strictly proper in the sense that it encourages honest forecasts, where the expected score is maximized only when the model reports true probabilities.
Although many strictly proper scoring rules exist, the logarithmic score is the only local scoring rule among them that depends exclusively on the probability of the observed sample, making it capable of handling the exponentially large sample space of natural text. In this work, we propose a straightforward strategy for adapting scoring rules to language generation, allowing for language modeling with any non-local scoring rules. Leveraging this strategy, we train language generation models using two classic strictly proper scoring rules, the Brier score and the Spherical score, as alternatives to the logarithmic score. Experimental results indicate that simply substituting the loss function, without adjusting other hyperparameters, can yield substantial improvements in model's generation capabilities. Moreover, these improvements can scale up to large language models (LLMs) such as LLaMA-7B and LLaMA-13B.
Source code: \url{https://github.com/shaochenze/ScoringRulesLM}.

------------

`[2405.18915] Towards Faithful Chain-of-Thought: Large Language Models are Bridging Reasoners <https://arxiv.org/abs/2405.18915>`__ 走向忠实的思维链:大型语言模型是推理机的桥梁

::

    Wed, 29 May 2024 09:17:46 GMT
    Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao

Large language models (LLMs) suffer from serious unfaithful chain-of-thought (CoT) issues. Previous work attempts to measure and explain it but lacks in-depth analysis within CoTs and does not consider the interactions among all reasoning components jointly. In this paper, we first study the CoT faithfulness issue at the granularity of CoT steps, identify two reasoning paradigms: centralized reasoning and distributed reasoning, and find their relationship with faithfulness. Subsequently, we conduct a joint analysis of the causal relevance among the context, CoT, and answer during reasoning. The result proves that, when the LLM predicts answers, it can recall correct information missing in the CoT from the context, leading to unfaithfulness issues. Finally, we propose the inferential bridging method to mitigate this issue, in which we use the attribution method to recall information as hints for CoT generation and filter out noisy CoTs based on their semantic consistency and attribution scores. Extensive experiments demonstrate that our approach effectively alleviates the unfaithful CoT problem.

------------

`[2405.18952] Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets <https://arxiv.org/abs/2405.18952>`__ 你确定吗?再次排序:为更好的偏好数据集进行重复排序

::

    Wed, 29 May 2024 10:08:31 GMT
    Peter Devine

Training Large Language Models (LLMs) with Reinforcement Learning from AI Feedback (RLAIF) aligns model outputs more closely with human preferences. This involves an evaluator model ranking multiple candidate responses to user prompts. However, the rankings from popular evaluator models such as GPT-4 can be inconsistent. We propose the Repeat Ranking method - where we evaluate the same responses multiple times and train only on those responses which are consistently ranked. Using 2,714 prompts in 62 languages, we generated responses from 7 top multilingual LLMs and had GPT-4 rank them five times each.
Evaluating on MT-Bench chat benchmarks in six languages, our method outperformed the standard practice of training on all available prompts. Our work highlights the quality versus quantity trade-off in RLAIF dataset generation and offers a stackable strategy for enhancing dataset and thus model quality.

------------

`[2405.19010] Evaluating the External and Parametric Knowledge Fusion of Large Language Models <https://arxiv.org/abs/2405.19010>`__ 大型语言模型的外部和参数化知识融合评估

::

    Wed, 29 May 2024 11:48:27 GMT
    Hao Zhang, Yuyang Zhang, Xiaoguang Li, Wenxuan Shi, Haonan Xu, Huanshuo Liu, Yasheng Wang, Lifeng Shang, Qun Liu, Yong Liu, Ruiming Tang

Integrating external knowledge into large language models (LLMs) presents a promising solution to overcome the limitations imposed by their antiquated and static parametric memory. Prior studies, however, have tended to over-reliance on external knowledge, underestimating the valuable contributions of an LLMs' intrinsic parametric knowledge. The efficacy of LLMs in blending external and parametric knowledge remains largely unexplored, especially in cases where external knowledge is incomplete and necessitates supplementation by their parametric knowledge. We propose to deconstruct knowledge fusion into four distinct scenarios, offering the first thorough investigation of LLM behavior across each. We develop a systematic pipeline for data construction and knowledge infusion to simulate these fusion scenarios, facilitating a series of controlled experiments. Our investigation reveals that enhancing parametric knowledge within LLMs can significantly bolster their capability for knowledge integration. Nonetheless, we identify persistent challenges in memorizing and eliciting parametric knowledge, and determining parametric knowledge boundaries. Our findings aim to steer future explorations on harmonizing external and parametric knowledge within LLMs.

------------

`[2405.19041] BLSP-KD: Bootstrapping Language-Speech Pre-training via Knowledge Distillation <https://arxiv.org/abs/2405.19041>`__ BLSP-KD:基于知识蒸馏的自助语言-语音预训练

::

    Wed, 29 May 2024 12:32:08 GMT
    Chen Wang, Minpeng Liao, Zhongqiang Huang, Jiajun Zhang

Recent end-to-end approaches have shown promise in extending large language models (LLMs) to speech inputs, but face limitations in directly assessing and optimizing alignment quality and fail to achieve fine-grained alignment due to speech-text length mismatch. We introduce BLSP-KD, a novel approach for Bootstrapping Language-Speech Pretraining via Knowledge Distillation, which addresses these limitations through two key techniques. First, it optimizes speech-text alignment by minimizing the divergence between the LLM's next-token prediction distributions for speech and text inputs using knowledge distillation. Second, it employs a continuous-integrate-andfire strategy to segment speech into tokens that correspond one-to-one with text tokens, enabling fine-grained alignment. We also introduce Partial LoRA (PLoRA), a new adaptation method supporting LLM finetuning for speech inputs under knowledge distillation. Quantitative evaluation shows that BLSP-KD outperforms previous end-to-end baselines and cascaded systems with comparable scale of parameters, facilitating general instruction-following capabilities for LLMs with speech inputs. This approach provides new possibilities for extending LLMs to spoken language interactions.

------------

`[2405.19086] MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors <https://arxiv.org/abs/2405.19086>`__ MEMoE:基于混合专家适配器的模型编辑增强

::

    Wed, 29 May 2024 13:49:44 GMT
    Renzhi Wang, Piji Li

Model editing aims to efficiently alter the behavior of Large Language Models (LLMs) within a desired scope, while ensuring no adverse impact on other inputs. Recent years have witnessed various model editing methods been proposed. However, these methods either exhibit poor overall performance or struggle to strike a balance between generalization and locality. We propose MOMoE, a model editing adapter utilizing a Mixture of Experts (MoE) architecture with a knowledge anchor routing strategy. MOMoE updates knowledge using a bypass MoE structure, keeping the original parameters unchanged to preserve the general ability of LLMs. And, the knowledge anchor routing ensures that inputs requiring similar knowledge are routed to the same expert, thereby enhancing the generalization of the updated knowledge. Experimental results show the superiority of our approach over both batch editing and sequential batch editing tasks, exhibiting exceptional overall performance alongside outstanding balance between generalization and locality. Our code will be available.

------------

`[2405.19262] Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models <https://arxiv.org/abs/2405.19262>`__ 弱到强搜索:通过搜索小语言模型来对齐大型语言模型

::

    Wed, 29 May 2024 16:55:32 GMT
    Zhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, Yu Qiao

Large language models are usually fine-tuned to align with human preferences.
However, fine-tuning a large language model can be challenging. In this work, we introduce $\textit{weak-to-strong search}$, framing the alignment of a large language model as a test-time greedy search to maximize the log-likelihood difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (i) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (ii) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance. Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned $\texttt{gpt2}$s to effectively improve the alignment of large models without additional training.
Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small model pairs (e.g., $\texttt{zephyr-7b-beta}$ and its untuned version) can significantly improve the length-controlled win rates of both white-box and black-box large models against $\texttt{gpt-4-turbo}$ (e.g., $34.4 \rightarrow 37.9$ for $\texttt{Llama-3-70B-Instruct}$ and $16.0 \rightarrow 20.1$ for $\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates $\approx 10.0$.

------------

`[2405.19265] AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data <https://arxiv.org/abs/2405.19265>`__ AlchemistCoder:通过对多源数据的后见之明调整来协调和诱导代码能力

::

    Wed, 29 May 2024 16:57:33 GMT
    Zifan Song, Yudong Wang, Wenwei Zhang, Kuikun Liu, Chengqi Lyu, Demin Song, Qipeng Guo, Hang Yan, Dahua Lin, Kai Chen, Cairong Zhao

Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance.
However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed AlchemistPrompts, to harmonize different data sources and instruction-response pairs. Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence.

------------

`[2405.19266] PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications <https://arxiv.org/abs/2405.19266>`__ 儿科sgpt:用于儿科应用的大型语言模型作为中文医疗助理

::

    Wed, 29 May 2024 16:59:38 GMT
    Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang

Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. Our model and dataset will be open-source for community development.

------------

`[2405.19285] MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection <https://arxiv.org/abs/2405.19285>`__ 大规模多语言抽象语义表示:幻觉检测的数据集和基线

::

    Wed, 29 May 2024 17:17:22 GMT
    Michael Regan and Shira Wein and George Baker and Emilio Monti

Abstract Meaning Representation (AMR) is a semantic formalism that captures the core meaning of an utterance. There has been substantial work developing AMR corpora in English and more recently across languages, though the limited size of existing datasets and the cost of collecting more annotations are prohibitive. With both engineering and scientific questions in mind, we introduce MASSIVE-AMR, a dataset with more than 84,000 text-to-graph annotations, currently the largest and most diverse of its kind: AMR graphs for 1,685 information-seeking utterances mapped to 50+ typologically diverse languages. We describe how we built our resource and its unique features before reporting on experiments using large language models for multilingual AMR and SPARQL parsing as well as applying AMRs for hallucination detection in the context of knowledge base question answering, with results shedding light on persistent issues using LLMs for structured parsing.

------------

`[2405.19299] Expert-Guided Extinction of Toxic Tokens for Debiased Generation <https://arxiv.org/abs/2405.19299>`__ 专家指导的去偏代有毒代币灭绝

::

    Wed, 29 May 2024 17:26:52 GMT
    Xueyao Sun, Kaize Shi, Haoran Tang, Guandong Xu, Qing Li

Large language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts. Controlling the sensitive attributes in generation encounters challenges in data distribution, generalizability, and efficiency. Specifically, fine-tuning and retrieval demand extensive unbiased corpus, while direct prompting requires meticulously curated instructions for correcting the output in multiple rounds of thoughts but poses challenges on memory and inference latency. In this work, we propose the Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED) to eliminate the undesired harmful outputs for LLMs without the aforementioned requirements. EXPOSED constructs a debiasing expert based on the abundant toxic corpus to expose and elicit the potentially dangerous tokens. It then processes the output to the LLMs and constructs a fair distribution by suppressing and attenuating the toxic tokens. EXPOSED is evaluated on fairness benchmarks over three LLM families. Extensive experiments demonstrate that compared with other baselines, the proposed EXPOSED significantly reduces the potential social bias while balancing fairness and generation performance.

------------

`[2405.19323] Are Large Language Models Chameleons? <https://arxiv.org/abs/2405.19323>`__ 大型语言模型是变色龙吗?

::

    Wed, 29 May 2024 17:54:22 GMT
    Mingmeng Geng, Sihong He, Roberto Trotta

Do large language models (LLMs) have their own worldviews and personality tendencies? Simulations in which an LLM was asked to answer subjective questions were conducted more than 1 million times. Comparison of the responses from different LLMs with real data from the European Social Survey (ESS) suggests that the effect of prompts on bias and variability is fundamental, highlighting major cultural, age, and gender biases. Methods for measuring the difference between LLMs and survey data are discussed, such as calculating weighted means and a new proposed measure inspired by Jaccard similarity. We conclude that it is important to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior, as their imitation abilities are approximate at best.

------------

`[2405.19327] MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series <https://arxiv.org/abs/2405.19327>`__ MAP-Neo:高性能透明双语大型语言模型系列

::

    Wed, 29 May 2024 17:57:16 GMT
    Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen

Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.

------------

`[2405.18572] Low-rank finetuning for LLMs: A fairness perspective <https://arxiv.org/abs/2405.18572>`__ llm的低秩微调:公平视角

::

    Tue, 28 May 2024 20:43:53 GMT
    Saswat Das, Marco Romanelli, Cuong Tran, Zarreen Reza, Bhavya Kailkhura, Ferdinando Fioretto

Low-rank approximation techniques have become the de facto standard for fine-tuning Large Language Models (LLMs) due to their reduced computational and memory requirements. This paper investigates the effectiveness of these methods in capturing the shift of fine-tuning datasets from the initial pre-trained data distribution. Our findings reveal that there are cases in which low-rank fine-tuning falls short in learning such shifts. This, in turn, produces non-negligible side effects, especially when fine-tuning is adopted for toxicity mitigation in pre-trained models, or in scenarios where it is important to provide fair models. Through comprehensive empirical evidence on several models, datasets, and tasks, we show that low-rank fine-tuning inadvertently preserves undesirable biases and toxic behaviors. We also show that this extends to sequential decision-making tasks, emphasizing the need for careful evaluation to promote responsible LLMs development.

------------

`[2405.18634] A Theoretical Understanding of Self-Correction through In-context Alignment <https://arxiv.org/abs/2405.18634>`__ 通过上下文对齐对自我矫正的理论理解

::

    Tue, 28 May 2024 22:33:02 GMT
    Yifei Wang, Yuyang Wu, Zeming Wei, Stefanie Jegelka, Yisen Wang

Going beyond mimicking limited human experiences, recent studies show initial evidence that, like humans, large language models (LLMs) are capable of improving their abilities purely by self-correction, i.e., correcting previous responses through self-examination, in certain circumstances. Nevertheless, little is known about how such capabilities arise. In this work, based on a simplified setup akin to an alignment task, we theoretically analyze self-correction from an in-context learning perspective, showing that when LLMs give relatively accurate self-examinations as rewards, they are capable of refining responses in an in-context way. Notably, going beyond previous theories on over-simplified linear transformers, our theoretical construction underpins the roles of several key designs of realistic transformers for self-correction: softmax attention, multi-head attention, and the MLP block. We validate these findings extensively on synthetic datasets. Inspired by these findings, we also illustrate novel applications of self-correction, such as defending against LLM jailbreaks, where a simple self-correction step does make a large difference. We believe that these findings will inspire further research on understanding, exploiting, and enhancing self-correction for building better foundation models.

------------

`[2405.18641] Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning <https://arxiv.org/abs/2405.18641>`__ 针对有害微调的大型语言模型的惰性安全对齐

::

    Tue, 28 May 2024 22:53:43 GMT
    Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu

Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data. First time in the literature, we show that the jail-broken effect can be mitigated by separating states in the finetuning stage to optimize the alignment and user datasets. Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance. By statistical analysis, we show that the \textit{excess drift} towards consensus could be a probable reason for the instability. To remedy this issue, we propose \textbf{L}azy(\textbf{i}) \textbf{s}afety \textbf{a}lignment (\textbf{Lisa}), which introduces a proximal term to constraint the drift of each state. Theoretically, the benefit of the proximal term is supported by the convergence analysis, wherein we show that a sufficient large proximal factor is necessary to guarantee Lisa's convergence.
Empirically, our results on four downstream finetuning tasks show that Lisa with a proximal term can significantly increase alignment performance while maintaining the LLM's accuracy on the user tasks. Code is available at \url{https://github.com/git-disl/Lisa}.

------------

`[2405.18710] To FP8 and Back Again: Quantifying the Effects of Reducing Precision on LLM Training Stability <https://arxiv.org/abs/2405.18710>`__ 量化降低精度对LLM训练稳定性的影响

::

    Wed, 29 May 2024 02:42:23 GMT
    Joonhyung Lee, Jeongin Bae, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee

The massive computational costs associated with large language model (LLM) pretraining have spurred great interest in reduced-precision floating-point representations to accelerate the process. As a result, the BrainFloat16 (BF16) precision has become the de facto standard for LLM training, with hardware support included in recent accelerators. This trend has gone even further in the latest processors, where FP8 has recently been introduced. However, prior experience with FP16, which was found to be less stable than BF16, raises concerns as to whether FP8, with even fewer bits than FP16, can be a cost-effective option for LLM training. We argue that reduced-precision training schemes must have similar training stability and hyperparameter sensitivities to their higher-precision counterparts in order to be cost-effective. However, we find that currently available methods for FP8 training are not robust enough to allow their use as economical replacements.
This prompts us to investigate the stability of reduced-precision LLM training in terms of robustness across random seeds and learning rates. To this end, we propose new evaluation techniques and a new metric for quantifying loss landscape sharpness in autoregressive language models. By simulating incremental bit reductions in floating-point representations, we analyze the relationship between representational power and training stability with the intent of aiding future research into the field.

------------

`[2405.18765] Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI <https://arxiv.org/abs/2405.18765>`__ 脑机接口中基于大量脑电图数据学习通用表示的大型脑模型

::

    Wed, 29 May 2024 05:08:16 GMT
    Wei-Bang Jiang, Li-Ming Zhao, Bao-Liang Lu

The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks.
However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches.
Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks.
Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM.

------------

`[2405.18832] MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models <https://arxiv.org/abs/2405.18832>`__ MoNDE:面向大规模稀疏模型的近数据专家混合

::

    Wed, 29 May 2024 07:23:29 GMT
    Taehyun Kim, Kwanseok Choi, Youngmock Cho, Jaehoon Cho, Hyuk-Jae Lee and Jaewoong Sim

Mixture-of-Experts (MoE) large language models (LLM) have memory requirements that often exceed the GPU memory capacity, requiring costly parameter movement from secondary memories to the GPU for expert computation. In this work, we present Mixture of Near-Data Experts (MoNDE), a near-data computing solution that efficiently enables MoE LLM inference. MoNDE reduces the volume of MoE parameter movement by transferring only the $\textit{hot}$ experts to the GPU, while computing the remaining $\textit{cold}$ experts inside the host memory device. By replacing the transfers of massive expert parameters with the ones of small activations, MoNDE enables far more communication-efficient MoE inference, thereby resulting in substantial speedups over the existing parameter offloading frameworks for both encoder and decoder operations.

------------

`[2405.18886] Compressing Large Language Models using Low Rank and Low Precision Decomposition <https://arxiv.org/abs/2405.18886>`__ 基于低秩低精度分解的大型语言模型压缩

::

    Wed, 29 May 2024 08:42:30 GMT
    Rajarshi Saha, Naomi Sagan, Varun Srivastava, Andrea J. Goldsmith, Mert Pilanci

The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices. This work introduces $\rm CALDERA$ -- a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix $\mathbf{W}$ by approximating it via a low-rank, low-precision decomposition as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$. Here, $\mathbf{L}$ and $\mathbf{R}$ are low rank factors, and the entries of $\mathbf{Q}$, $\mathbf{L}$ and $\mathbf{R}$ are quantized. The model is compressed by substituting each layer with its $\mathbf{Q} + \mathbf{L}\mathbf{R}$ decomposition, and the zero-shot performance of the compressed model is evaluated. Additionally, $\mathbf{L}$ and $\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance. $\rm CALDERA$ obtains this decomposition by formulating it as an optimization problem $\min_{\mathbf{Q},\mathbf{L},\mathbf{R}}\lVert(\mathbf{Q} + \mathbf{L}\mathbf{R} - \mathbf{W})\mathbf{X}^\top\rVert_{\rm F}^2$, where $\mathbf{X}$ is the calibration data, and $\mathbf{Q}, \mathbf{L}, \mathbf{R}$ are constrained to be representable using low-precision formats. Theoretical upper bounds on the approximation error of $\rm CALDERA$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget. Results illustrate that compressing LlaMa-$2$ $7$B/$70$B and LlaMa-$3$ $8$B models obtained using $\rm CALDERA$ outperforms existing post-training LLM compression techniques in the regime of less than $2.5$ bits per parameter. The implementation is available at: \href{https://github.com/pilancilab/caldera}{https://github.com/pilancilab/caldera}.

------------

`[2405.19024] Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory <https://arxiv.org/abs/2405.19024>`__ 逆凹效用强化学习是逆博弈论

::

    Wed, 29 May 2024 12:07:17 GMT
    Mustafa Mert \c{C}elikok, Frans A. Oliehoek, Jan-Willem van de Meent

We consider inverse reinforcement learning problems with concave utilities.
Concave Utility Reinforcement Learning (CURL) is a generalisation of the standard RL objective, which employs a concave function of the state occupancy measure, rather than a linear function. CURL has garnered recent attention for its ability to represent instances of many important applications including the standard RL such as imitation learning, pure exploration, constrained MDPs, offline RL, human-regularized RL, and others. Inverse reinforcement learning is a powerful paradigm that focuses on recovering an unknown reward function that can rationalize the observed behaviour of an agent. There has been recent theoretical advances in inverse RL where the problem is formulated as identifying the set of feasible reward functions. However, inverse RL for CURL problems has not been considered previously. In this paper we show that most of the standard IRL results do not apply to CURL in general, since CURL invalidates the classical Bellman equations. This calls for a new theoretical framework for the inverse CURL problem. Using a recent equivalence result between CURL and Mean-field Games, we propose a new definition for the feasible rewards for I-CURL by proving that this problem is equivalent to an inverse game theory problem in a subclass of mean-field games. We present initial query and sample complexity results for the I-CURL problem under assumptions such as Lipschitz-continuity. Finally, we outline future directions and applications in human--AI collaboration enabled by our results.

------------

`[2405.19026] DiveR-CT: Diversity-enhanced Red Teaming with Relaxing Constraints <https://arxiv.org/abs/2405.19026>`__ 潜水员- ct:放宽约束的多样性增强红色团队

::

    Wed, 29 May 2024 12:12:09 GMT
    Andrew Zhao, Quentin Xu, Matthieu Lin, Shenzhi Wang, Yong-jin Liu, Zilong Zheng, Gao Huang

Recent advances in large language models (LLMs) have made them indispensable, raising significant concerns over managing their safety. Automated red teaming offers a promising alternative to the labor-intensive and error-prone manual probing for vulnerabilities, providing more consistent and scalable safety evaluations. However, existing approaches often compromise diversity by focusing on maximizing attack success rate. Additionally, methods that decrease the cosine similarity from historical embeddings with semantic diversity rewards lead to novelty stagnation as history grows. To address these issues, we introduce DiveR-CT, which relaxes conventional constraints on the objective and semantic reward, granting greater freedom for the policy to enhance diversity. Our experiments demonstrate DiveR-CT's marked superiority over baselines by 1) generating data that perform better in various diversity metrics across different attack success rate levels, 2) better-enhancing resiliency in blue team models through safety tuning based on collected data, 3) allowing dynamic control of objective weights for reliable and controllable attack success rates, and 4) reducing susceptibility to reward overoptimization. Project details and code can be found at https://andrewzh112.github.io/#diverct.

------------

`[2405.19107] Offline Regularised Reinforcement Learning for Large Language Models Alignment <https://arxiv.org/abs/2405.19107>`__ 面向大型语言模型对齐的离线正则化强化学习

::

    Wed, 29 May 2024 14:11:29 GMT
    Pierre Harvey Richemond, Yunhao Tang, Daniel Guo, Daniele Calandriello, Mohammad Gheshlaghi Azar, Rafael Rafailov, Bernardo Avila Pires, Eugene Tarassov, Lucas Spangher, Will Ellsworth, Aliaksei Severyn, Jonathan Mallinson, Lior Shani, Gil Shamir, Rishabh Joshi, Tianqi Liu, Remi Munos, Bilal Piot

The dominant framework for alignment of large language models (LLM), whether through reinforcement learning from human feedback or direct preference optimisation, is to learn from preference data. This involves building datasets where each element is a quadruplet composed of a prompt, two independent responses (completions of the prompt) and a human preference between the two independent responses, yielding a preferred and a dis-preferred response. Such data is typically scarce and expensive to collect. On the other hand, \emph{single-trajectory} datasets where each element is a triplet composed of a prompt, a response and a human feedback is naturally more abundant. The canonical element of such datasets is for instance an LLM's response to a user's prompt followed by a user's feedback such as a thumbs-up/down.
Consequently, in this work, we propose DRO, or \emph{Direct Reward Optimisation}, as a framework and associated algorithms that do not require pairwise preferences. DRO uses a simple mean-squared objective that can be implemented in various ways. We validate our findings empirically, using T5 encoder-decoder language models, and show DRO's performance over selected baselines such as Kahneman-Tversky Optimization (KTO). Thus, we confirm that DRO is a simple and empirically compelling method for single-trajectory policy optimisation.

------------

`[2405.19119] Can Graph Learning Improve Task Planning? <https://arxiv.org/abs/2405.19119>`__ 图学习能改善任务规划吗?

::

    Wed, 29 May 2024 14:26:24 GMT
    Xixi Wu, Yifei Shen, Caihua Shan, Kaitao Song, Siwei Wang, Bohang Zhang, Jiarui Feng, Hong Cheng, Wei Chen, Yun Xiong, Dongsheng Li

Task planning is emerging as an important research topic alongside the development of large language models (LLMs). It aims to break down complex user requests into solvable sub-tasks, thereby fulfilling the original requests. In this context, the sub-tasks can be naturally viewed as a graph, where the nodes represent the sub-tasks, and the edges denote the dependencies among them.
Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it. In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design. Our interest in graph learning stems from a theoretical discovery: the biases of attention and auto-regressive loss impede LLMs' ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs). This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance. Extensive experiments demonstrate that GNN-based methods surpass existing solutions even without training, and minimal training can further enhance their performance. Additionally, our approach complements prompt engineering and fine-tuning techniques, with performance further enhanced by improved prompts or a fine-tuned model.

------------

`[2405.19320] Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF <https://arxiv.org/abs/2405.19320>`__ 价值激励偏好优化:线上线下RLHF的统一方法

::

    Wed, 29 May 2024 17:51:42 GMT
    Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi, Bo Dai

Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference.
Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected.
While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.
In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\textit{sign}$ to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO.

------------

`[2405.19332] Self-Exploring Language Models: Active Preference Elicitation for Online Alignment <https://arxiv.org/abs/2405.19332>`__ 自探索语言模型:在线对齐的主动偏好诱导

::

    Wed, 29 May 2024 17:59:07 GMT
    Shenao Zhang, Donghan Yu, Hiteshi Sharma, Ziyi Yang, Shuohang Wang, Hany Hassan, Zhaoran Wang

Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed dataset, online feedback collection from humans or AI on model generations typically leads to more capable reward models and better-aligned LLMs through an iterative process. However, achieving a globally accurate reward model requires systematic exploration to generate diverse responses that span the vast space of natural language. Random sampling from standard reward-maximizing LLMs alone is insufficient to fulfill this requirement. To address this issue, we propose a bilevel objective optimistically biased towards potentially high-reward responses to actively explore out-of-distribution regions. By solving the inner-level problem with the reparameterized reward function, the resulting algorithm, named Self-Exploring Language Models (SELM), eliminates the need for a separate RM and iteratively updates the LLM with a straightforward objective. Compared to Direct Preference Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen extrapolations and enhances exploration efficiency. Our experimental results demonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instruct models, SELM significantly boosts the performance on instruction-following benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard academic benchmarks in different settings. Our code and models are available at https://github.com/shenao-zhang/SELM.

------------

`[2405.11591] Generative Students: Using LLM-Simulated Student Profiles to Support Question Item Evaluation <https://arxiv.org/abs/2405.11591>`__ 生成型学生:使用llm模拟的学生档案来支持问题项目评估

::

    Sun, 19 May 2024 15:53:18 GMT
    Xinyi Lu, Xu Wang

Evaluating the quality of automatically generated question items has been a long standing challenge. In this paper, we leverage LLMs to simulate student profiles and generate responses to multiple-choice questions (MCQs). The generative students' responses to MCQs can further support question item evaluation. We propose Generative Students, a prompt architecture designed based on the KLI framework. A generative student profile is a function of the list of knowledge components the student has mastered, has confusion about or has no evidence of knowledge of. We instantiate the Generative Students concept on the subject domain of heuristic evaluation. We created 45 generative students using GPT-4 and had them respond to 20 MCQs. We found that the generative students produced logical and believable responses that were aligned with their profiles. We then compared the generative students' responses to real students' responses on the same set of MCQs and found a high correlation.
Moreover, there was considerable overlap in the difficult questions identified by generative students and real students. A subsequent case study demonstrated that an instructor could improve question quality based on the signals provided by Generative Students.

------------

`[2405.18620] RealitySummary: On-Demand Mixed Reality Document Enhancement using Large Language Models <https://arxiv.org/abs/2405.18620>`__ RealitySummary:基于大型语言模型的按需混合现实文档增强

::

    Tue, 28 May 2024 21:59:56 GMT
    Aditya Gunturu, Shivesh Jadon, Nandi Zhang, Jarin Thundathil, Wesley Willett, Ryo Suzuki

We introduce RealitySummary, a mixed reality reading assistant that can enhance any printed or digital document using on-demand text extraction, summarization, and augmentation. While augmented reading tools promise to enhance physical reading experiences with overlaid digital content, prior systems have typically required pre-processed documents, which limits their generalizability and real-world use cases. In this paper, we explore on-demand document augmentation by leveraging large language models. To understand generalizable techniques for diverse documents, we first conducted an exploratory design study which identified five categories of document enhancements (summarization, augmentation, navigation, comparison, and extraction). Based on this, we developed a proof-of-concept system that can automatically extract and summarize text using Google Cloud OCR and GPT-4, then embed information around documents using a Microsoft Hololens 2 and Apple Vision Pro. We demonstrate real-time examples of six specific document augmentations: 1) summaries, 2) comparison tables, 3) timelines, 4) keyword lists, 5) summary highlighting, and 6) information cards. Results from a usability study (N=12) and in-the-wild study (N=11) highlight the potential benefits of on-demand MR document enhancement and opportunities for future research.

------------

`[2405.18632] Large Language Models as Partners in Student Essay Evaluation <https://arxiv.org/abs/2405.18632>`__ 大型语言模型在学生论文评估中的合作伙伴

::

    Tue, 28 May 2024 22:28:50 GMT
    Toru Ishida, Tongxi Liu, Hailong Wang, and William K. Cheung

As the importance of comprehensive evaluation in workshop courses increases, there is a growing demand for efficient and fair assessment methods that reduce the workload for faculty members. This paper presents an evaluation conducted with Large Language Models (LLMs) using actual student essays in three scenarios: 1) without providing guidance such as rubrics, 2) with pre-specified rubrics, and 3) through pairwise comparison of essays. Quantitative analysis of the results revealed a strong correlation between LLM and faculty member assessments in the pairwise comparison scenario with pre-specified rubrics, although concerns about the quality and stability of evaluations remained.
Therefore, we conducted a qualitative analysis of LLM assessment comments, showing that: 1) LLMs can match the assessment capabilities of faculty members, 2) variations in LLM assessments should be interpreted as diversity rather than confusion, and 3) assessments by humans and LLMs can differ and complement each other. In conclusion, this paper suggests that LLMs should not be seen merely as assistants to faculty members but as partners in evaluation committees and outlines directions for further research.

------------

`[2405.18732] Gemini & Physical World: Large Language Models Can Estimate the Intensity of Earthquake Shaking from Multi-Modal Social Media Posts <https://arxiv.org/abs/2405.18732>`__ Gemini & Physical World:大型语言模型可以从多模态社交媒体帖子中估计地震震动的强度

::

    Wed, 29 May 2024 03:23:34 GMT
    S. Mostafa Mousavi, Marc Stogaitis, Tajinder Gadh, Richard M Allen, Alexei Barski, Robert Bosch, Patrick Robertson, Nivetha Thiruverahan, Youngmin Cho

This paper presents a novel approach for estimating the ground shaking intensity using social media data and CCTV footage. Employing the Gemini Pro (Reid et al. 2024) model, a multi-modal language model, we demonstrate the ability to extract relevant information from unstructured data utilizing generative AI and natural language processing. The model output, in the form of Modified Mercalli Intensity (MMI) values, align well with independent observational data. Furthermore, our results suggest that beyond its advanced visual and auditory understanding abilities, Gemini appears to utilize additional sources of knowledge, including a simplified understanding of the general relationship between earthquake magnitude, distance, and MMI intensity, which it presumably acquired during its training, in its reasoning and decision-making processes. These findings raise intriguing questions about the extent of Gemini's general understanding of the physical world and its phenomena. The ability of Gemini to generate results consistent with established scientific knowledge highlights the potential of LLMs like Gemini in augmenting our understanding of complex physical phenomena such as earthquakes. More specifically, the results of this study highlight the potential of LLMs like Gemini to revolutionize citizen seismology by enabling rapid, effective, and flexible analysis of crowdsourced data from eyewitness accounts for assessing earthquake impact and providing crisis situational awareness. This approach holds great promise for improving early warning systems, disaster response, and overall resilience in earthquake-prone regions.
This study provides a significant step toward harnessing the power of social media and AI for earthquake disaster mitigation.

------------

`[2405.18672] LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained Image Classification <https://arxiv.org/abs/2405.18672>`__ 基于llm的可解释细粒度图像分类的层次概念分解

::

    Wed, 29 May 2024 00:36:56 GMT
    Renyi Qu, Mark Yatskar

Recent advancements in interpretable models for vision-language tasks have achieved competitive performance; however, their interpretability often suffers due to the reliance on unstructured text outputs from large language models (LLMs). This introduces randomness and compromises both transparency and reliability, which are essential for addressing safety issues in AI systems. We introduce \texttt{Hi-CoDe} (Hierarchical Concept Decomposition), a novel framework designed to enhance model interpretability through structured concept analysis. Our approach consists of two main components: (1) We use GPT-4 to decompose an input image into a structured hierarchy of visual concepts, thereby forming a visual concept tree. (2) We then employ an ensemble of simple linear classifiers that operate on concept-specific features derived from CLIP to perform classification. Our approach not only aligns with the performance of state-of-the-art models but also advances transparency by providing clear insights into the decision-making process and highlighting the importance of various concepts. This allows for a detailed analysis of potential failure modes and improves model compactness, therefore setting a new benchmark in interpretability without compromising the accuracy.

------------

`[2405.18937] Kestrel: Point Grounding Multimodal LLM for Part-Aware 3D Vision-Language Understanding <https://arxiv.org/abs/2405.18937>`__ Kestrel:基于点接地多模态LLM的部分感知3D视觉-语言理解

::

    Wed, 29 May 2024 09:43:48 GMT
    Junjie Fei, Mahmoud Ahmed, Jian Ding, Eslam Mohamed Bakr, Mohamed Elhoseiny

While 3D MLLMs have achieved significant progress, they are restricted to object and scene understanding and struggle to understand 3D spatial structures at the part level. In this paper, we introduce Kestrel, representing a novel approach that empowers 3D MLLMs with part-aware understanding, enabling better interpretation and segmentation grounding of 3D objects at the part level.
Despite its significance, the current landscape lacks tasks and datasets that endow and assess this capability. Therefore, we propose two novel tasks: (1) Part-Aware Point Grounding, the model is tasked with directly predicting a part-level segmentation mask based on user instructions, and (2) Part-Aware Point Grounded Captioning, the model provides a detailed caption that includes part-level descriptions and their corresponding masks. To support learning and evaluating for these tasks, we introduce 3DCoMPaT Grounded Instructions Dataset (3DCoMPaT-GRIN). 3DCoMPaT-GRIN Vanilla, comprising 789k part-aware point cloud-instruction-segmentation mask triplets, is used to evaluate MLLMs' ability of part-aware segmentation grounding. 3DCoMPaT-GRIN Grounded Caption, containing 107k part-aware point cloud-instruction-grounded caption triplets, assesses both MLLMs' part-aware language comprehension and segmentation grounding capabilities. Our introduced tasks, dataset, and Kestrel represent a preliminary effort to bridge the gap between human cognition and 3D MLLMs, i.e., the ability to perceive and engage with the environment at both global and part levels. Extensive experiments on the 3DCoMPaT-GRIN show that Kestrel can generate user-specified segmentation masks, a capability not present in any existing 3D MLLM. Kestrel thus established a benchmark for evaluating the part-aware language comprehension and segmentation grounding of 3D objects.
Project page at https://feielysia.github.io/Kestrel.github.io/

------------

`[2405.19076] Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials Analysis and Design <https://arxiv.org/abs/2405.19076>`__ cepalo:面向仿生材料分析与设计的多模态视觉-语言模型

::

    Wed, 29 May 2024 13:34:32 GMT
    Markus J. Buehler

We present Cephalo, a series of multimodal vision large language models (V-LLMs) designed for materials science applications, integrating visual and linguistic data for enhanced understanding and interaction within human-AI and multi-agent AI frameworks. A key innovation of Cephalo is its advanced dataset generation method, which employs a sophisticated algorithm to accurately detect and separate images and their corresponding textual descriptions from PDF documents, such as scientific papers. The method includes a careful refinement of image-text pairs through integrated vision and language processing, ensuring high-quality, contextually relevant, and well reasoned training data. Cephalo is trained on integrated image and text data extracted from thousands of scientific papers and science-focused Wikipedia pages demonstrates can interpret complex visual scenes, generate precise language descriptions, and answer queries about images effectively. The combination of a vision encoder with an autoregressive transformer supports complex natural language understanding in an integrated model, which can be coupled with other generative methods to create an image-to-text-to-image or image-to-text-to-3D pipeline. To explore the development of larger models from smaller ones, we merge sets of layers that originate from different pre-trained source models.
This hybrid approach allows us to leverage the domain-specific expertise and general conversational capabilities to harness the strengths of multiple models. We examine the models in diverse use cases that incorporate biological materials, fracture and engineering analysis, protein biophysics, and bio-inspired design based on insect behavior. Generative applications include bio-inspired designs, including pollen-inspired architected materials, as well as the synthesis of bio-inspired material microstructures from a photograph of a solar eclipse.

------------

`[2405.19335] X-VILA: Cross-Modality Alignment for Large Language Model <https://arxiv.org/abs/2405.19335>`__ X-VILA:大型语言模型的跨模态对齐

::

    Wed, 29 May 2024 17:59:58 GMT
    Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo Molchanov, Hongxu Yin

We introduce X-VILA, an omni-modality model designed to extend the capabilities of large language models (LLMs) by incorporating image, video, and audio modalities. By aligning modality-specific encoders with LLM inputs and diffusion decoders with LLM outputs, X-VILA achieves cross-modality understanding, reasoning, and generation. To facilitate this cross-modality alignment, we curate an effective interleaved any-to-any modality instruction-following dataset. Furthermore, we identify a significant problem with the current cross-modality alignment method, which results in visual information loss. To address the issue, we propose a visual alignment mechanism with a visual embedding highway module. We then introduce a resource-efficient recipe for training X-VILA, that exhibits proficiency in any-to-any modality conversation, surpassing previous approaches by large margins. X-VILA also showcases emergent properties across modalities even in the absence of similar training data. The project will be made open-source.

------------

`[2405.19103] Voice Jailbreak Attacks Against GPT-4o <https://arxiv.org/abs/2405.19103>`__ 针对GPT-4o的语音越狱攻击

::

    Wed, 29 May 2024 14:07:44 GMT
    Xinyue Shen and Yixin Wu and Michael Backes and Yang Zhang

Recently, the concept of artificial assistants has evolved from science fiction into real-world applications. GPT-4o, the newest multimodal large language model (MLLM) across audio, vision, and text, has further blurred the line between fiction and reality by enabling more natural human-computer interactions. However, the advent of GPT-4o's voice mode may also introduce a new attack surface. In this paper, we present the first systematic measurement of jailbreak attacks against the voice mode of GPT-4o. We show that GPT-4o demonstrates good resistance to forbidden questions and text jailbreak prompts when directly transferring them to voice mode. This resistance is primarily due to GPT-4o's internal safeguards and the difficulty of adapting text jailbreak prompts to voice mode. Inspired by GPT-4o's human-like behaviors, we propose VoiceJailbreak, a novel voice jailbreak attack that humanizes GPT-4o and attempts to persuade it through fictional storytelling (setting, character, and plot). VoiceJailbreak is capable of generating simple, audible, yet effective jailbreak prompts, which significantly increases the average attack success rate (ASR) from 0.033 to 0.778 in six forbidden scenarios. We also conduct extensive experiments to explore the impacts of interaction steps, key elements of fictional writing, and different languages on VoiceJailbreak's effectiveness and further enhance the attack performance with advanced fictional writing techniques. We hope our study can assist the research community in building more secure and well-regulated MLLMs.

------------

`[2308.15030] SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget <https://arxiv.org/abs/2308.15030>`__ SwapMoE:提供可调内存预算的基于moe的现成大型语言模型

::

    replaced with revised version Wed, 29 May 2024 08:25:03 GMT
    Submission history From: Rui Kong [view email]
    [v1] Tue, 29 Aug 2023 05:25:21 UTC (4,047 KB)
    [v2] Thu, 28 Dec 2023 02:53:41 UTC (4,437 KB)
    [v3] Tue, 28 May 2024 02:08:30 UTC (4,719 KB)
    [v4] Wed, 29 May 2024 08:25:03 UTC (4,719 KB)
    Rui Kong, Yuanchun Li, Qingtian Feng, Weijun Wang, Xiaozhou Ye, Ye Ouyang, Linghe Kong, Yunxin Liu

Mixture of experts (MoE) is a popular technique to improve capacity of Large Language Models (LLMs) with conditionally-activated parallel experts. However, serving MoE models on memory-constrained devices is challenging due to the large parameter size. Typical solutions such as memory swapping or expert pruning may lead to significantly higher latency or severe accuracy loss. In this paper, we introduce SwapMoE, a framework for efficient serving of MoE-based large language models with tunable memory budgets. The main idea of SwapMoE is to keep a small dynamic set of important experts, namely Virtual Experts, in the main memory for inference, while seamlessly maintaining how the Virtual Experts map to the actual experts. Experiments have shown that SwapMoE can reduce the memory footprint while maintaining reasonable accuracy. For example, on text summarization tasks with Switch Transformer, SwapMoE can reduce the memory consumption from 14.2 GiB to 4.7 GiB, together with 50\% latency reduction and a slight Rouge-2 score drop of 0.041.

------------

`[2405.17888] Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment <https://arxiv.org/abs/2405.17888>`__ 从SFT数据中获得更多的能量:来自人类演示的奖励学习改进了SFT对LLM的对齐

::

    replaced with revised version Wed, 29 May 2024 13:33:33 GMT
    Submission history From: Jiaxiang Li [view email]
    [v1] Tue, 28 May 2024 07:11:05 UTC (402 KB)
    [v2] Wed, 29 May 2024 13:33:33 UTC (402 KB)
    Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garcia, Mingyi Hong

Aligning human preference and value is an important requirement for contemporary foundation models. State-of-the-art techniques such as Reinforcement Learning from Human Feedback (RLHF) often consist of two stages: 1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from human demonstration data; 2) Preference learning, where preference data is used to learn a reward model, which is in turn used by a reinforcement learning (RL) step to fine-tune the model. Such reward model serves as a proxy to human preference, and it is critical to guide the RL step towards improving the model quality. In this work, we argue that the SFT stage significantly benefits from learning a reward model as well. Instead of using the human demonstration data directly via supervised learning, we propose to leverage an Inverse Reinforcement Learning (IRL) technique to (explicitly or implicitly) build an reward model, while learning the policy model. This approach leads to new SFT algorithms that are not only efficient to implement, but also promote the ability to distinguish between the preferred and non-preferred continuations. Moreover, we identify a connection between the proposed IRL based approach, and certain self-play approach proposed recently, and showed that self-play is a special case of modeling a reward-learning agent. Theoretically, we show that the proposed algorithms converge to the stationary solutions of the IRL problem. Empirically, we align 1B and 7B models using proposed methods and evaluate them on a reward benchmark model and the HuggingFace Open LLM Leaderboard. The proposed methods show significant performance improvement over existing SFT approaches. Our results indicate that it is beneficial to explicitly or implicitly leverage reward learning throughout the entire alignment process.

------------

`[2307.01379] Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models <https://arxiv.org/abs/2307.01379>`__ 将注意力转移到相关性:自由形式大型语言模型的预测不确定性量化

::

    replaced with revised version Tue, 28 May 2024 20:01:04 GMT
    Submission history From: Jinhao Duan [view email]
    [v1] Mon, 3 Jul 2023 22:17:16 UTC (513 KB)
    [v2] Mon, 9 Oct 2023 14:26:59 UTC (451 KB)
    [v3] Tue, 28 May 2024 20:01:04 UTC (621 KB)
    Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, Kaidi Xu

Large Language Models (LLMs) show promising results in language generation and instruction following but frequently "hallucinate", making their outputs less reliable. Despite Uncertainty Quantification's (UQ) potential solutions, implementing it accurately within LLMs is challenging. Our research introduces a simple heuristic: not all tokens in auto-regressive LLM text equally represent the underlying meaning, as "linguistic redundancy" often allows a few keywords to convey the essence of long sentences. However, current methods underestimate this inequality when assessing uncertainty, causing tokens with limited semantics to be equally or excessively weighted in UQ. To correct this, we propose Shifting Attention to more Relevant (SAR) components at both token- and sentence-levels for better UQ. We conduct extensive experiments involving a range of popular "off-the-shelf" LLMs, such as Vicuna, WizardLM, and LLaMA-2-chat, with model sizes extending up to 33B parameters. We evaluate various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results, coupled with a comprehensive demographic analysis, demonstrate the superior performance of SAR. The code is available at this https URL.

------------

`[2307.02179] Open-Source LLMs for Text Annotation: A Practical Guide for Model Setting and Fine-Tuning <https://arxiv.org/abs/2307.02179>`__ 用于文本注释的开源llm:模型设置和微调的实用指南

::

    replaced with revised version Wed, 29 May 2024 12:29:08 GMT
    Submission history From: Fabrizio Gilardi [view email]
    [v1] Wed, 5 Jul 2023 10:15:07 UTC (71 KB)
    [v2] Wed, 29 May 2024 12:29:08 UTC (2,603 KB)
    Meysam Alizadeh, Ma\"el Kubli, Zeynab Samei, Shirin Dehghani, Mohammadmasiha Zahedivafa, Juan Diego Bermeo, Maria Korobeynikova, Fabrizio Gilardi

This paper studies the performance of open-source Large Language Models (LLMs) in text classification tasks typical for political science research. By examining tasks like stance, topic, and relevance classification, we aim to guide scholars in making informed decisions about their use of LLMs for text analysis. Specifically, we conduct an assessment of both zero-shot and fine-tuned LLMs across a range of text annotation tasks using news articles and tweets datasets. Our analysis shows that fine-tuning improves the performance of open-source LLMs, allowing them to match or even surpass zero-shot GPT-3.5 and GPT-4, though still lagging behind fine-tuned GPT-3.5. We further establish that fine-tuning is preferable to few-shot training with a relatively modest quantity of annotated text. Our findings show that fine-tuned open-source LLMs can be effectively deployed in a broad spectrum of text annotation applications. We provide a Python notebook facilitating the application of LLMs in text annotation for other researchers.

------------

`[2310.13571] Why Can Large Language Models Generate Correct Chain-of-Thoughts? <https://arxiv.org/abs/2310.13571>`__ 为什么大型语言模型可以生成正确的思维链?

::

    replaced with revised version Wed, 29 May 2024 09:23:59 GMT
    Submission history From: Haitham Bou Ammar PhD [view email]
    [v1] Fri, 20 Oct 2023 15:09:46 UTC (62 KB)
    [v2] Mon, 30 Oct 2023 09:47:39 UTC (63 KB)
    [v3] Wed, 29 May 2024 09:23:59 UTC (63 KB)
    Rasul Tutunov, Antoine Grosnit, Juliusz Ziomek, Jun Wang, Haitham Bou-Ammar

This paper delves into the capabilities of large language models (LLMs), specifically focusing on advancing the theoretical comprehension of chain-of-thought prompting. We investigate how LLMs can be effectively induced to generate a coherent chain of thoughts. To achieve this, we introduce a two-level hierarchical graphical model tailored for natural language generation. Within this framework, we establish a compelling geometrical convergence rate that gauges the likelihood of an LLM-generated chain of thoughts compared to those originating from the true language. Our findings provide a theoretical justification for the ability of LLMs to produce the correct sequence of thoughts (potentially) explaining performance gains in tasks demanding reasoning skills.

------------

`[2310.18913] Debiasing Algorithm through Model Adaptation <https://arxiv.org/abs/2310.18913>`__ 基于模型自适应的去偏算法

::

    replaced with revised version Wed, 29 May 2024 10:22:52 GMT
    Submission history From: Tomasz Limisiewicz [view email]
    [v1] Sun, 29 Oct 2023 05:50:03 UTC (2,662 KB)
    [v2] Thu, 18 Jan 2024 14:23:07 UTC (2,662 KB)
    [v3] Fri, 15 Mar 2024 16:39:26 UTC (5,293 KB)
    [v4] Wed, 29 May 2024 10:22:52 UTC (5,293 KB)
    Tomasz Limisiewicz and David Mare\v{c}ek and Tom\'a\v{s} Musil

Large language models are becoming the go-to solution for the ever-growing number of tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey bias. Based on the analysis results, we intervene in the model by applying a linear projection to the weight matrices of these layers. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.

------------

`[2311.01041] Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism <https://arxiv.org/abs/2311.01041>`__ 学会拒绝:通过知识范围限制和拒绝机制使大型语言模型更加可控和可靠

::

    replaced with revised version Wed, 29 May 2024 09:19:35 GMT
    Submission history From: Lang Cao [view email]
    [v1] Thu, 2 Nov 2023 07:20:49 UTC (8,375 KB)
    [v2] Tue, 16 Apr 2024 06:24:38 UTC (8,356 KB)
    [v3] Wed, 29 May 2024 09:19:35 UTC (8,172 KB)
    Lang Cao

Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM's understanding of the world, enabling it to provide traceable gold knowledge. This knowledge base is separate from the LLM and initially empty. It can be filled with validated knowledge and progressively expanded. When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently. Additionally, we introduce a method for automatically and efficiently expanding the knowledge base of LLMs. Through qualitative and quantitative analysis, we demonstrate that our approach enhances the controllability and reliability of LLMs.

------------

`[2402.01822] Building Guardrails for Large Language Models <https://arxiv.org/abs/2402.01822>`__ 为大型语言模型建立护栏

::

    replaced with revised version Wed, 29 May 2024 12:57:01 GMT
    Submission history From: Yi Dong [view email]
    [v1] Fri, 2 Feb 2024 16:35:00 UTC (7,682 KB)
    [v2] Wed, 29 May 2024 12:57:01 UTC (8,179 KB)
    Yi Dong, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu Zhao, Jie Meng, Wenjie Ruan, Xiaowei Huang

As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and testing to ensure the utmost quality of the final product.

------------

`[2402.09025] SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks <https://arxiv.org/abs/2402.09025>`__ SLEB:通过冗余验证和消除变压器块精简llm

::

    replaced with revised version Wed, 29 May 2024 02:53:16 GMT
    Submission history From: Jiwon Song [view email]
    [v1] Wed, 14 Feb 2024 09:01:13 UTC (8,750 KB)
    [v2] Wed, 29 May 2024 02:53:16 UTC (8,759 KB)
    Jiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon Kim

Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB outperforms previous LLM pruning methods in accelerating LLM inference while also maintaining superior perplexity and accuracy, making SLEB as a promising technique for enhancing the efficiency of LLMs. The code is available at: this https URL.

------------

`[2402.13963] Towards Building Multilingual Language Model for Medicine <https://arxiv.org/abs/2402.13963>`__ 面向医学的多语言语言模型构建

::

    replaced with revised version Wed, 29 May 2024 06:15:38 GMT
    Submission history From: Pengcheng Qiu [view email]
    [v1] Wed, 21 Feb 2024 17:47:20 UTC (5,297 KB)
    [v2] Mon, 26 Feb 2024 11:01:25 UTC (5,424 KB)
    [v3] Wed, 29 May 2024 06:15:38 UTC (4,516 KB)
    Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, Weidi Xie

The development of open-source, multilingual medical language models can benefit a wide, linguistically diverse audience from different regions. To promote this domain, we present contributions from the following: First, we construct a multilingual medical corpus, containing approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, enabling auto-regressive domain adaptation for general LLMs; Second, to monitor the development of multilingual medical LLMs, we propose a multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; Third, we have assessed a number of open-source large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC. Our final model, MMed-Llama 3, with only 8B parameters, achieves superior performance compared to all other open-source models on both MMedBench and English benchmarks, even rivaling GPT-4. In conclusion, in this work, we present a large-scale corpus, a benchmark and a series of models to support the development of multilingual medical LLMs.

------------

`[2402.17463] Training-Free Long-Context Scaling of Large Language Models <https://arxiv.org/abs/2402.17463>`__ 大型语言模型的免训练长上下文扩展

::

    replaced with revised version Wed, 29 May 2024 05:44:58 GMT
    Submission history From: Chenxin An [view email]
    [v1] Tue, 27 Feb 2024 12:39:23 UTC (508 KB)
    [v2] Wed, 29 May 2024 05:44:58 UTC (1,288 KB)
    Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, Lingpeng Kong

The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative. All code and data used in this work are released at \url{this https URL}.

------------

`[2403.01774] WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations <https://arxiv.org/abs/2403.01774>`__ WebCiteS:中文网页搜索结果的带引用属性查询摘要

::

    replaced with revised version Wed, 29 May 2024 02:45:11 GMT
    Submission history From: Haolin Deng [view email]
    [v1] Mon, 4 Mar 2024 07:06:41 UTC (833 KB)
    [v2] Wed, 29 May 2024 02:45:11 UTC (835 KB)
    Haolin Deng, Chang Wang, Xin Li, Dezhang Yuan, Junlang Zhan, Tianhua Zhou, Jin Ma, Jun Gao, Ruifeng Xu

Enhancing the attribution in large language models (LLMs) is a crucial task. One feasible approach is to enable LLMs to cite external sources that support their generations. However, existing datasets and evaluation methods in this domain still exhibit notable limitations. In this work, we formulate the task of attributed query-focused summarization (AQFS) and present WebCiteS, a Chinese dataset featuring 7k human-annotated summaries with citations. WebCiteS derives from real-world user queries and web search results, offering a valuable resource for model training and evaluation. Prior works in attribution evaluation do not differentiate between groundedness errors and citation errors. They also fall short in automatically verifying sentences that draw partial support from multiple sources. We tackle these issues by developing detailed metrics and enabling the automatic evaluator to decompose the sentences into sub-claims for fine-grained verification. Our comprehensive evaluation of both open-source and proprietary models on WebCiteS highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement. The dataset and code will be open-sourced to facilitate further research in this crucial field.

------------

`[2403.15112] Text clustering with LLM embeddings <https://arxiv.org/abs/2403.15112>`__ 基于LLM嵌入的文本聚类

::

    replaced with revised version Wed, 29 May 2024 10:16:13 GMT
    Submission history From: Alina Petukhova [view email]
    [v1] Fri, 22 Mar 2024 11:08:48 UTC (83 KB)
    [v2] Wed, 29 May 2024 10:16:13 UTC (87 KB)
    Alina Petukhova, Joao P. Matos-Carvalho, Nuno Fachada

Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a complex balance between the need for nuanced text representation and computational feasibility in text clustering applications. This study extends traditional text clustering frameworks by incorporating embeddings from LLMs, thereby paving the way for improved methodologies and opening new avenues for future research in various types of textual analysis.

------------

`[2403.19285] Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation <https://arxiv.org/abs/2403.19285>`__ 超越单词匹配:语法提高了机器翻译中的上下文示例选择

::

    replaced with revised version Wed, 29 May 2024 00:51:24 GMT
    Submission history From: Chenming Tang [view email]
    [v1] Thu, 28 Mar 2024 10:13:34 UTC (249 KB)
    [v2] Wed, 29 May 2024 00:51:24 UTC (1 KB) (withdrawn)
    Chenming Tang, Zhixiang Wang and Yunfang Wu

In-context learning (ICL) is the trending prompting strategy in the era of large language models (LLMs), where a few examples are demonstrated to evoke LLMs' power for a given task. How to select informative examples remains an open issue. Previous works on in-context example selection for machine translation (MT) focus on superficial word-level features while ignoring deep syntax-level knowledge. In this paper, we propose a syntax-based in-context example selection method for MT, by computing the syntactic similarity between dependency trees using Polynomial Distance. In addition, we propose an ensemble strategy combining examples selected by both word-level and syntax-level criteria. Experimental results between English and 6 common languages indicate that syntax can effectively enhancing ICL for MT, obtaining the highest COMET scores on 11 out of 12 translation directions.

------------

`[2404.14963] Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems <https://arxiv.org/abs/2404.14963>`__ 在GSM8K上实现>97%:对问题的深入理解使llm更好地解决数学应用题

::

    replaced with revised version Wed, 29 May 2024 15:27:53 GMT
    Submission history From: Kang Wang [view email]
    [v1] Tue, 23 Apr 2024 12:16:05 UTC (1,214 KB)
    [v2] Sun, 28 Apr 2024 15:55:52 UTC (1,231 KB)
    [v3] Wed, 29 May 2024 15:27:53 UTC (1,689 KB)
    Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, Dacheng Tao

Chain-of-Thought (CoT) prompting has enhanced the performance of Large Language Models (LLMs) across various reasoning tasks. However, CoT still falls short in dealing with complex math word problems, as it usually suffers from three pitfalls: semantic misunderstanding errors, calculation errors and step-missing errors. Prior studies involve addressing the calculation errors and step-missing errors, but neglect the semantic misunderstanding errors, which is the major factor limiting the LLMs' performance. To this end, we propose a simple-yet-effective method, namely Deeply Understanding the Problems (DUP), to improve the LLMs' math problem-solving ability by addressing semantic misunderstanding errors. The core of our method is to encourage the LLMs to deeply understand the problems and extract the key problem-solving information used for better reasoning. Extensive experiments on 10 diverse reasoning benchmarks show that our DUP method consistently outperforms the other counterparts by a large margin. More encouragingly, DUP achieves a new SOTA result on the GSM8K benchmark, with an accuracy of 97.1% under zero-shot setting.

------------

`[2405.06800] LLM-Generated Black-box Explanations Can Be Adversarially Helpful <https://arxiv.org/abs/2405.06800>`__ llm生成的黑箱解释可能有不利的帮助

::

    replaced with revised version Wed, 29 May 2024 15:18:59 GMT
    Submission history From: Zining Zhu [view email]
    [v1] Fri, 10 May 2024 20:23:46 UTC (705 KB)
    [v2] Wed, 29 May 2024 15:18:59 UTC (709 KB)
    Rohan Ajwani, Shashidhar Reddy Javaji, Frank Rudzicz, Zining Zhu

Large Language Models (LLMs) are becoming vital tools that help us solve and understand complex problems by acting as digital assistants. LLMs can generate convincing explanations, even when only given the inputs and outputs of these problems, i.e., in a ``black-box'' approach. However, our research uncovers a hidden risk tied to this approach, which we call *adversarial helpfulness*. This happens when an LLM's explanations make a wrong answer look right, potentially leading people to trust incorrect solutions. In this paper, we show that this issue affects not just humans, but also LLM evaluators. Digging deeper, we identify and examine key persuasive strategies employed by LLMs. Our findings reveal that these models employ strategies such as reframing the questions, expressing an elevated level of confidence, and cherry-picking evidence to paint misleading answers in a credible light. To examine if LLMs are able to navigate complex-structured knowledge when generating adversarially helpful explanations, we create a special task based on navigating through graphs. Most LLMs are not able to find alternative paths along simple graphs, indicating that their misleading explanations aren't produced by only logical deductions using complex knowledge. These findings shed light on the limitations of the black-box explanation setting and allow us to provide advice on the safe usage of LLMs.

------------

`[2405.11039] CC-GPX: Extracting High-Quality Annotated Geospatial Data from Common Crawl <https://arxiv.org/abs/2405.11039>`__ CC-GPX:从普通爬行器中提取高质量标注地理空间数据

::

    replaced with revised version Wed, 29 May 2024 09:16:28 GMT
    Submission history From: Ilya Ilyankou [view email]
    [v1] Fri, 17 May 2024 18:31:26 UTC (946 KB)
    [v2] Wed, 29 May 2024 09:16:28 UTC (946 KB)
    Ilya Ilyankou, Meihui Wang, James Haworth, Stefano Cavazzi

The Common Crawl (CC) corpus is the largest open web crawl dataset containing 9.5+ petabytes of data captured since 2008. The dataset is instrumental in training large language models, and as such it has been studied for (un)desirable content, and distilled for smaller, domain-specific datasets. However, to our knowledge, no research has been dedicated to using CC as a source of annotated geospatial data. In this paper, we introduce an efficient pipeline to extract annotated user-generated tracks from GPX files found in CC, and the resulting multimodal dataset with 1,416 pairings of human-written descriptions and MultiLineString vector data from the 6 most recent CC releases. The dataset can be used to study people's outdoor activity patterns, the way people talk about their outdoor experiences, and for developing trajectory generation or track annotation models. Our reproducible code is available on GitHub: this https URL

------------

`[2405.12689] Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text <https://arxiv.org/abs/2405.12689>`__ 识别人工智能的触觉:识别文本中由llm转述的span

::

    replaced with revised version Wed, 29 May 2024 07:09:59 GMT
    Submission history From: Yafu Li [view email]
    [v1] Tue, 21 May 2024 11:22:27 UTC (5,278 KB)
    [v2] Wed, 29 May 2024 07:09:59 UTC (5,285 KB)
    Yafu Li, Zhilin Wang, Leyang Cui, Wei Bi, Shuming Shi and Yue Zhang

AI-generated text detection has attracted increasing attention as powerful language models approach human-level generation. Limited work is devoted to detecting (partially) AI-paraphrased texts. However, AI paraphrasing is commonly employed in various application scenarios for text refinement and diversity. To this end, we propose a novel detection framework, paraphrased text span detection (PTD), aiming to identify paraphrased text spans within a text. Different from text-level detection, PTD takes in the full text and assigns each of the sentences with a score indicating the paraphrasing degree. We construct a dedicated dataset, PASTED, for paraphrased text span detection. Both in-distribution and out-of-distribution results demonstrate the effectiveness of PTD models in identifying AI-paraphrased text spans. Statistical and model analysis explains the crucial role of the surrounding context of the paraphrased text spans. Extensive experiments show that PTD models can generalize to versatile paraphrasing prompts and multiple paraphrased text spans. We release our resources at this https URL.

------------

`[2405.13967] DeTox: Toxic Subspace Projection for Model Editing <https://arxiv.org/abs/2405.13967>`__ DeTox:用于模型编辑的有毒子空间投影

::

    replaced with revised version Tue, 28 May 2024 19:55:16 GMT
    Submission history From: Rheeya Uppaal [view email]
    [v1] Wed, 22 May 2024 20:08:48 UTC (1,106 KB)
    [v2] Sun, 26 May 2024 03:46:22 UTC (1,114 KB)
    [v3] Tue, 28 May 2024 19:55:16 UTC (1,114 KB)
    Rheeya Uppaal, Apratim Dey, Yiting He, Yiqiao Zhong, Junjie Hu

Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data. However, these methods are both computationally intensive and lacking in controllability and transparency, making them prone to jailbreaking and inhibiting their widespread use. Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data. In this paper, we introduce a tuning-free alignment alternative (DeTox) and demonstrate its effectiveness under the use case of toxicity reduction. Grounded on theory from factor analysis, DeTox is a sample-efficient model editing approach that identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The toxic sub-space is identified by extracting preference data embeddings from the language model, and removing non-toxic information from these embeddings. We show that DeTox is more sample-efficient than DPO, further showcasing greater robustness to noisy data. Finally, we establish both theoretical and empirical connections between DeTox and DPO, showing that DeTox can be interpreted as a denoised version of a single DPO step.

------------

`[2405.14782] Lessons from the Trenches on Reproducible Evaluation of Language Models <https://arxiv.org/abs/2405.14782>`__ 语言模型可重复性评估的经验教训

::

    replaced with revised version Wed, 29 May 2024 17:15:53 GMT
    Submission history From: Hailey Schoelkopf [view email]
    [v1] Thu, 23 May 2024 16:50:49 UTC (1,650 KB)
    [v2] Wed, 29 May 2024 17:15:53 UTC (1,650 KB)
    Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Jeffrey Hsu, Mimansa Jaiswal, Wilson Y. Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A. Wang, Genta Indra Winata, Fran\c{c}ois Yvon, Andy Zou

Effective evaluation of language models remains an open challenge in NLP. Researchers and engineers face methodological issues such as the sensitivity of models to evaluation setup, difficulty of proper comparisons across methods, and the lack of reproducibility and transparency. In this paper we draw on three years of experience in evaluating large language models to provide guidance and lessons for researchers. First, we provide an overview of common challenges faced in language model evaluation. Second, we delineate best practices for addressing or lessening the impact of these challenges on research. Third, we present the Language Model Evaluation Harness (lm-eval): an open source library for independent, reproducible, and extensible evaluation of language models that seeks to address these issues. We describe the features of the library as well as case studies in which the library has been used to alleviate these methodological concerns.

------------

`[2405.15924] SLIDE: A Framework Integrating Small and Large Language Models for Open-Domain Dialogues Evaluation <https://arxiv.org/abs/2405.15924>`__ SLIDE:用于开放域对话评估的小型和大型语言模型集成框架

::

    replaced with revised version Wed, 29 May 2024 15:10:27 GMT
    Submission history From: Kun Zhao [view email]
    [v1] Fri, 24 May 2024 20:32:49 UTC (352 KB)
    [v2] Wed, 29 May 2024 15:10:27 UTC (445 KB)
    Kun Zhao, Bohao Yang, Chen Tang, Chenghua Lin, Liang Zhan

The long-standing one-to-many problem of gold standard responses in open-domain dialogue systems presents challenges for automatic evaluation metrics. Though prior works have demonstrated some success by applying powerful Large Language Models (LLMs), existing approaches still struggle with the one-to-many problem, and exhibit subpar performance in domain-specific scenarios. We assume the commonsense reasoning biases within LLMs may hinder their performance in domainspecific evaluations. To address both issues, we propose a novel framework SLIDE (Small and Large Integrated for Dialogue Evaluation), that leverages both a small, specialised model (SLM), and LLMs for the evaluation of open domain dialogues. Our approach introduces several techniques: (1) Contrastive learning to differentiate between robust and non-robust response embeddings; (2) A novel metric for semantic sensitivity that combines embedding cosine distances with similarity learned through neural networks, and (3) a strategy for incorporating the evaluation results from both the SLM and LLMs. Our empirical results demonstrate that our approach achieves state-of-the-art performance in both the classification and evaluation tasks, and additionally the SLIDE evaluator exhibits better correlation with human judgements. Our code is available at https:// this http URL.

------------

`[2405.16282] Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models <https://arxiv.org/abs/2405.16282>`__ 幕后的信心:大型语言模型的信心概率对齐研究

::

    replaced with revised version Wed, 29 May 2024 13:05:16 GMT
    Submission history From: Ali Emami Dr. [view email]
    [v1] Sat, 25 May 2024 15:42:04 UTC (9,154 KB)
    [v2] Wed, 29 May 2024 13:05:16 UTC (9,152 KB)
    Abhishek Kumar, Robert Morabito, Sanzhar Umbet, Jad Kabbara, and Ali Emami

As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM's internal confidence, quantified by token probabilities, to the confidence conveyed in the model's response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model's confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI's GPT-4 showed the strongest confidence-probability alignment, with an average Spearman's $\hat{\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness.

------------

`[2305.16338] Think Before You Act: Decision Transformers with Working Memory <https://arxiv.org/abs/2305.16338>`__ 三思而后行:基于工作记忆的决策transformer

::

    replaced with revised version Tue, 28 May 2024 19:03:33 GMT
    Submission history From: Jikun Kang [view email]
    [v1] Wed, 24 May 2023 01:20:22 UTC (3,022 KB)
    [v2] Mon, 27 May 2024 16:00:31 UTC (3,725 KB)
    [v3] Tue, 28 May 2024 19:03:33 UTC (3,725 KB)
    Jikun Kang, Romain Laroche, Xingdi Yuan, Adam Trischler, Xue Liu, Jie Fu

Decision Transformer-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and computation. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Inspired by this, we propose a working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in Atari games and Meta-World object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture.

------------

`[2310.10692] ACES: Generating Diverse Programming Puzzles with with Autotelic Generative Models <https://arxiv.org/abs/2310.10692>`__ ACES:使用Autotelic生成模型生成多样化编程难题

::

    replaced with revised version Wed, 29 May 2024 08:56:23 GMT
    Submission history From: Laetitia Teodorescu [view email]
    [v1] Sun, 15 Oct 2023 14:57:14 UTC (8,771 KB)
    [v2] Wed, 18 Oct 2023 22:19:14 UTC (8,771 KB)
    [v3] Wed, 25 Oct 2023 12:01:38 UTC (8,771 KB)
    [v4] Wed, 29 May 2024 08:56:23 UTC (1,728 KB)
    Julien Pourcel, C\'edric Colas, Gaia Molinaro, Pierre-Yves Oudeyer, Laetitia Teodorescu

The ability to invent novel and interesting problems is a remarkable feature of human intelligence that drives innovation, art, and science. We propose a method that aims to automate this process by harnessing the power of state-of-the-art generative models to produce a diversity of challenging yet solvable problems, here in the context of Python programming puzzles. Inspired by the intrinsically motivated literature, Autotelic CodE Search (ACES) jointly optimizes for the diversity and difficulty of generated problems. We represent problems in a space of LLM-generated semantic descriptors describing the programming skills required to solve them (e.g. string manipulation, dynamic programming, etc.) and measure their difficulty empirically as a linearly decreasing function of the success rate of Llama-3-70B, a state-of-the-art LLM problem solver. ACES iteratively prompts a large language model to generate difficult problems achieving a diversity of target semantic descriptors (goal-directed exploration) using previously generated problems as in-context examples. ACES generates problems that are more diverse and more challenging than problems produced by baseline methods and three times more challenging than problems found in existing Python programming benchmarks on average across 11 state-of-the-art code LLMs.

------------

`[2311.09735] GEO: Generative Engine Optimization <https://arxiv.org/abs/2311.09735>`__ GEO:生成引擎优化

::

    replaced with revised version Tue, 28 May 2024 17:40:31 GMT
    Submission history From: Pranjal Aggarwal [view email]
    [v1] Thu, 16 Nov 2023 10:06:09 UTC (2,081 KB)
    [v2] Tue, 28 May 2024 17:40:31 UTC (1,153 KB)
    Pranjal Aggarwal and Vishvak Murahari and Tanmay Rajpurohit and Ashwin Kalyan and Karthik R Narasimhan and Ameet Deshpande

The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines (GEs), can generate accurate and personalized responses, rapidly replacing traditional search engines like Google and Bing. Generative Engines typically satisfy queries by synthesizing information from multiple sources and summarizing them using LLMs. While this shift significantly improves \textit{user} utility and \textit{generative search engine} traffic, it poses a huge challenge for the third stakeholder - website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over \textit{when} and \textit{how} their content is displayed. With generative engines here to stay, we must ensure the creator economy is not disadvantaged. To address this, we introduce Generative Engine Optimization (GEO), the first novel paradigm to aid content creators in improving their content visibility in GE responses through a flexible black-box optimization framework for optimizing and defining visibility metrics. We facilitate systematic evaluation by introducing GEO-bench, a large-scale benchmark of diverse user queries across multiple domains, along with relevant web sources to answer these queries. Through rigorous evaluation, we demonstrate that GEO can boost visibility by up to 40\% in GE responses. Moreover, we show the efficacy of these strategies varies across domains, underscoring the need for domain-specific optimization methods. Our work opens a new frontier in information discovery systems, with profound implications for both developers of GEs and content creators.

------------

`[2401.09074] Code Simulation Challenges for Large Language Models <https://arxiv.org/abs/2401.09074>`__ 大型语言模型的代码模拟挑战

::

    replaced with revised version Wed, 29 May 2024 17:56:58 GMT
    Submission history From: Emanuele La Malfa [view email]
    [v1] Wed, 17 Jan 2024 09:23:59 UTC (8,355 KB)
    [v2] Sun, 21 Jan 2024 15:15:30 UTC (8,472 KB)
    [v3] Wed, 29 May 2024 17:56:58 UTC (20,052 KB)
    Emanuele La Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin, Samuele Marro, Anthony Cohn, Nigel Shadbolt, Michael Wooldridge

Many reasoning, planning, and problem-solving tasks share an intrinsic algorithmic nature: correctly simulating each step is a sufficient condition to solve them correctly. This work studies to what extent Large Language Models (LLMs) can simulate coding and algorithmic tasks to provide insights into general capabilities in such algorithmic reasoning tasks. We introduce benchmarks for straight-line programs, code that contains critical paths, and approximate and redundant instructions. We further assess the simulation capabilities of LLMs with sorting algorithms and nested loops and show that a routine's computational complexity directly affects an LLM's ability to simulate its execution. While the most powerful LLMs exhibit relatively strong simulation capabilities, the process is fragile, seems to rely heavily on pattern recognition, and is affected by memorisation. We propose a novel off-the-shelf prompting method, Chain of Simulation (CoSm), which instructs LLMs to simulate code execution line by line/follow the computation pattern of compilers. CoSm efficiently helps LLMs reduce memorisation and shallow pattern recognition while improving simulation performance. We consider the success of CoSm in code simulation to be inspirational for other general routine simulation reasoning tasks.

------------

`[2402.05015] A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules? <https://arxiv.org/abs/2402.05015>`__ 冷静看待用于材料发现的llm:它们实际上有利于分子的贝叶斯优化吗?

::

    replaced with revised version Tue, 28 May 2024 18:59:01 GMT
    Submission history From: Agustinus Kristiadi [view email]
    [v1] Wed, 7 Feb 2024 16:32:58 UTC (2,536 KB)
    [v2] Tue, 28 May 2024 18:59:01 UTC (2,788 KB)
    Agustinus Kristiadi, Felix Strieth-Kalthoff, Marta Skreta, Pascal Poupart, Al\'an Aspuru-Guzik, Geoff Pleiss

Automation is one of the cornerstones of contemporary material discovery. Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space. While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs). However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs. In this work, we study the question of whether LLMs are actually useful to accelerate principled Bayesian optimization in the molecular space. We take a sober, dispassionate stance in answering this question. This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-efficient finetuning methods and Bayesian neural networks to obtain the posterior of the LLM surrogate. Our extensive experiments with real-world chemistry problems show that LLMs can be useful for BO over molecules, but only if they have been pretrained or finetuned with domain-specific data.

------------

`[2403.05612] Unfamiliar Finetuning Examples Control How Language Models Hallucinate <https://arxiv.org/abs/2403.05612>`__ 不熟悉的微调示例控制了语言模型的幻觉

::

    replaced with revised version Tue, 28 May 2024 23:56:14 GMT
    Submission history From: Katie Kang [view email]
    [v1] Fri, 8 Mar 2024 18:28:13 UTC (1,515 KB)
    [v2] Tue, 28 May 2024 23:56:14 UTC (10,591 KB)
    Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, Sergey Levine

Large language models are known to hallucinate when faced with unfamiliar queries, but the underlying mechanism that govern how models hallucinate are not yet fully understood. In this work, we find that unfamiliar examples in the models' finetuning data -- those that introduce concepts beyond the base model's scope of knowledge -- are crucial in shaping these errors. In particular, we find that an LLM's hallucinated predictions tend to mirror the responses associated with its unfamiliar finetuning examples. This suggests that by modifying how unfamiliar finetuning examples are supervised, we can influence a model's responses to unfamiliar queries (e.g., say ``I don't know''). We empirically validate this observation in a series of controlled experiments involving SFT, RL, and reward model finetuning on TriviaQA and MMLU. Our work further investigates RL finetuning strategies for improving the factuality of long-form model generations. We find that, while hallucinations from the reward model can significantly undermine the effectiveness of RL factuality finetuning, strategically controlling how reward models hallucinate can minimize these negative effects. Leveraging our previous observations on controlling hallucinations, we propose an approach for learning more reliable reward models, and show that they improve the efficacy of RL factuality finetuning in long-form biography and book/movie plot generation tasks.

------------

`[2404.07774] Sketch-Plan-Generalize: Continual Few-Shot Learning of Inductively Generalizable Spatial Concepts <https://arxiv.org/abs/2404.07774>`__ 草图-规划-概化:归纳概化空间概念的连续少次学习

::

    replaced with revised version Wed, 29 May 2024 09:46:39 GMT
    Submission history From: Namasivayam Kalithasan K [view email]
    [v1] Thu, 11 Apr 2024 14:09:41 UTC (26,226 KB)
    [v2] Wed, 29 May 2024 09:46:39 UTC (37,666 KB)
    Namasivayam Kalithasan, Sachit Sachdeva, Himanshu Gaurav Singh, Vishal Bindal, Arnav Tuli, Gurarmaan Singh Panjeta, Divyanshu Aggarwal, Rohan Paul, Parag Singla

Our goal is to enable embodied agents to learn inductively generalizable spatial concepts, e.g., learning staircase as an inductive composition of towers of increasing height. Given a human demonstration, we seek a learning architecture that infers a succinct ${program}$ representation that explains the observed instance. Additionally, the approach should generalize inductively to novel structures of different sizes or complex structures expressed as a hierarchical composition of previously learned concepts. Existing approaches that use code generation capabilities of pre-trained large (visual) language models, as well as purely neural models, show poor generalization to a-priori unseen complex concepts. Our key insight is to factor inductive concept learning as (i) ${\it Sketch:}$ detecting and inferring a coarse signature of a new concept (ii) ${\it Plan:}$ performing MCTS search over grounded action sequences (iii) ${\it Generalize:}$ abstracting out grounded plans as inductive programs. Our pipeline facilitates generalization and modular reuse, enabling continual concept learning. Our approach combines the benefits of the code generation ability of large language models (LLM) along with grounded neural representations, resulting in neuro-symbolic programs that show stronger inductive generalization on the task of constructing complex structures in relation to LLM-only and neural-only approaches. Furthermore, we demonstrate reasoning and planning capabilities with learned concepts for embodied instruction following.

------------

`[2404.16767] REBEL: Reinforcement Learning via Regressing Relative Rewards <https://arxiv.org/abs/2404.16767>`__ REBEL:基于相对奖励回归的强化学习

::

    replaced with revised version Wed, 29 May 2024 15:52:20 GMT
    Submission history From: Zhaolin Gao [view email]
    [v1] Thu, 25 Apr 2024 17:20:45 UTC (2,618 KB)
    [v2] Wed, 29 May 2024 15:52:20 UTC (7,813 KB)
    Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kiant\'e Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, Wen Sun

While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping), and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative reward between two completions to a prompt in terms of the policy, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and be extended to handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally efficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong performance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard.

------------

`[2405.16406] SpinQuant: LLM quantization with learned rotations <https://arxiv.org/abs/2405.16406>`__ SpinQuant:具有学习旋转的LLM量化

::

    replaced with revised version Tue, 28 May 2024 18:14:15 GMT
    Submission history From: Zechun Liu [view email]
    [v1] Sun, 26 May 2024 02:15:49 UTC (9,632 KB)
    [v2] Tue, 28 May 2024 18:14:15 UTC (9,632 KB)
    Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, Tijmen Blankevoort

Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Recent findings suggest that rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures, and find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zero-shot reasoning performance. As a result, we propose SpinQuant that optimizes (or learns) the rotation matrices with Cayley optimization on a small validation set. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points. SpinQuant also outperforms concurrent work QuaRot, which applies random rotations to remove outliers. In particular, for LLaMA-2 7B/LLaMA-3 8B models that are hard to quantize, SpinQuant reduces the gap to full precision by 30.2%/34.1% relative to QuaRot.

------------

`[2405.17618] Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales <https://arxiv.org/abs/2405.17618>`__ 对称强化学习损失用于不同任务和模型规模的鲁棒学习

::

    replaced with revised version Wed, 29 May 2024 04:19:00 GMT
    Submission history From: Ju-Seung Byun [view email]
    [v1] Mon, 27 May 2024 19:28:33 UTC (245 KB)
    [v2] Wed, 29 May 2024 04:19:00 UTC (245 KB)
    Ju-Seung Byun, Andrew Perrault

Reinforcement learning (RL) training is inherently unstable due to factors such as moving targets and high gradient variance. Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) can introduce additional difficulty. Differing preferences can complicate the alignment process, and prediction errors in a trained reward model can become more severe as the LLM generates unseen outputs. To enhance training robustness, RL has adopted techniques from supervised learning, such as ensembles and layer normalization. In this work, we improve the stability of RL training by adapting the reverse cross entropy (RCE) from supervised learning for noisy data to define a symmetric RL loss. We demonstrate performance improvements across various tasks and scales. We conduct experiments in discrete action tasks (Atari games) and continuous action space tasks (MuJoCo benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO), with and without added noise with especially notable performance in SPPO across different hyperparameters. Furthermore, we validate the benefits of the symmetric RL loss when using SPPO for large language models through improved performance in RLHF tasks, such as IMDB positive sentiment sentiment and TL;DR summarization tasks.

------------

`[2305.12090] UP5: Unbiased Foundation Model for Fairness-aware Recommendation <https://arxiv.org/abs/2305.12090>`__ UP5:公平感知推荐的无偏基础模型

::

    replaced with revised version Wed, 29 May 2024 16:46:47 GMT
    Submission history From: Yongfeng Zhang [view email]
    [v1] Sat, 20 May 2023 04:32:59 UTC (4,657 KB)
    [v2] Wed, 29 May 2024 16:46:47 UTC (4,281 KB)
    Wenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji, Yongfeng Zhang

Recent advances in Foundation Models such as Large Language Models (LLMs) have propelled them to the forefront of Recommender Systems (RS). Despite their utility, there is a growing concern that LLMs might inadvertently perpetuate societal stereotypes, resulting in unfair recommendations. Since fairness is critical for RS as many users take it for decision-making and demand fulfillment, this paper focuses on user-side fairness for LLM-based recommendation where the users may require a recommender system to be fair on specific sensitive features such as gender or age. In this paper, we dive into the extent of unfairness exhibited by LLM-based recommender models based on both T5 and LLaMA backbones, and discuss appropriate methods for promoting equitable treatment of users in LLM-based recommendation models. We introduce a novel Counterfactually-Fair-Prompt (CFP) method towards Unbiased Foundation mOdels (UFO) for fairness-aware LLM-based recommendation. Experiments are conducted on two real-world datasets, MovieLens-1M and Insurance, and compared with both matching-based and sequential-based fairness-aware recommendation models. Results show that CFP achieves better recommendation performance with a high level of fairness. Data and code are open-sourced at this https URL.

------------

`[2402.14883] Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning <https://arxiv.org/abs/2402.14883>`__ 双i水印:保护LLM微调模型版权

::

    replaced with revised version Wed, 29 May 2024 11:02:16 GMT
    Submission history From: Shen Li [view email]
    [v1] Thu, 22 Feb 2024 04:55:14 UTC (1,510 KB)
    [v2] Wed, 29 May 2024 11:02:16 UTC (1,497 KB)
    Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, Yaliang Li

To support various applications, a prevalent and efficient approach for business owners is leveraging their valuable datasets to fine-tune a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ``Double-I watermark''. Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed "Double-I watermark" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both quantitative and qualitative analyses.

------------

`[2402.17012] Pandora's White-Box: Precise Training Data Detection and Extraction in Large Language Models <https://arxiv.org/abs/2402.17012>`__ 潘多拉的白盒:大型语言模型中的精确训练数据检测和提取

::

    replaced with revised version Tue, 28 May 2024 20:42:01 GMT
    Submission history From: Seth Neel [view email]
    [v1] Mon, 26 Feb 2024 20:41:50 UTC (2,727 KB)
    [v2] Tue, 28 May 2024 20:42:01 UTC (15,045 KB)
    Jeffrey G. Wang, Jason Wang, Marvin Li, Seth Neel

In this paper we develop state-of-the-art privacy attacks against Large Language Models (LLMs), where an adversary with some access to the model tries to learn something about the underlying training data. Our headline results are new membership inference attacks (MIAs) against pretrained LLMs that perform hundreds of times better than baseline attacks, and a pipeline showing that over 50% (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, pretraining and fine-tuning data, and both MIAs and training data extraction. For pretraining data, we propose two new MIAs: a supervised neural network classifier that predicts training data membership on the basis of (dimensionality-reduced) model gradients, as well as a variant of this attack that only requires logit access to the model which leverages recent model-stealing work on LLMs. To our knowledge this is the first MIA that explicitly incorporates model-stealing information. Both attacks outperform existing black-box baselines, and our supervised attack closes the gap between MIA attack success against LLMs and the strongest known attacks for other machine learning models. In fine-tuning, we find that a simple attack based on the ratio of the loss between the base and fine-tuned models is able to achieve near-perfect MIA performance; we then leverage our MIA to extract a large fraction of the fine-tuning dataset from fine-tuned Pythia and Llama models. Taken together, these results represent the strongest existing privacy attacks against both pretrained and fine-tuned LLMs for MIAs and training data extraction, which are of independent scientific interest and have important practical implications for LLM security, privacy, and copyright issues.

------------

`[2405.15739] Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias <https://arxiv.org/abs/2405.15739>`__ 大型语言模型反映了具有较高引用偏差的人类引用模式

::

    replaced with revised version Wed, 29 May 2024 12:50:49 GMT
    Submission history From: Andres Algaba [view email]
    [v1] Fri, 24 May 2024 17:34:32 UTC (6,476 KB)
    [v2] Wed, 29 May 2024 12:50:49 UTC (6,204 KB)
    Andres Algaba, Carmen Mazijn, Vincent Holst, Floriano Tori, Sylvia Wenmackers, Vincent Ginis

Citation practices are crucial in shaping the structure of scientific knowledge, yet they are often influenced by contemporary norms and biases. The emergence of Large Language Models (LLMs) like GPT-4 introduces a new dynamic to these practices. Interestingly, the characteristics and potential biases of references recommended by LLMs that entirely rely on their parametric knowledge, and not on search or retrieval-augmented generation, remain unexplored. Here, we analyze these characteristics in an experiment using a dataset of 166 papers from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4's knowledge cut-off date, encompassing 3,066 references in total. In our experiment, GPT-4 was tasked with suggesting scholarly references for the anonymized in-text citations within these papers. Our findings reveal a remarkable similarity between human and LLM citation patterns, but with a more pronounced high citation bias in GPT-4, which persists even after controlling for publication year, title length, number of authors, and venue. Additionally, we observe a large consistency between the characteristics of GPT-4's existing and non-existent generated references, indicating the model's internalization of citation patterns. By analyzing citation graphs, we show that the references recommended by GPT-4 are embedded in the relevant citation context, suggesting an even deeper conceptual internalization of the citation networks. While LLMs can aid in citation generation, they may also amplify existing biases and introduce new ones, potentially skewing scientific knowledge dissemination. Our results underscore the need for identifying the model's biases and for developing balanced methods to interact with LLMs in general.

------------

`[2405.18386] Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning <https://arxiv.org/abs/2405.18386>`__ Instruction - musicgen:通过指令调优解锁音乐语言模型的文本到音乐编辑

::

    replaced with revised version Wed, 29 May 2024 17:05:32 GMT
    Submission history From: Yixiao Zhang [view email]
    [v1] Tue, 28 May 2024 17:27:20 UTC (4,196 KB)
    [v2] Wed, 29 May 2024 17:05:32 UTC (4,196 KB)
    Yixiao Zhang, Yukara Ikemiya, Woosung Choi, Naoki Murata, Marco A. Mart\'inez-Ram\'irez, Liwei Lin, Gus Xia, Wei-Hsiang Liao, Yuki Mitsufuji, Simon Dixon

Recent advances in text-to-music editing, which employ text queries to modify music (e.g.\ by changing its style or adjusting instrumental components), present unique challenges and opportunities for AI-assisted music creation. Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction. To Combine the strengths and address these limitations, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems. Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio inputs concurrently and yield the desired edited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, yet it achieves superior performance across all tasks compared to existing baselines, and demonstrates performance comparable to the models trained for specific tasks. This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments.

------------

`[2402.09989] LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition <https://arxiv.org/abs/2402.09989>`__ 作为桥梁的llm:基础多模态命名实体识别的重构

::

    replaced with revised version Wed, 29 May 2024 16:59:59 GMT
    Submission history From: Jinyuan Li [view email]
    [v1] Thu, 15 Feb 2024 14:54:33 UTC (9,777 KB)
    [v2] Sat, 17 Feb 2024 19:41:57 UTC (9,777 KB)
    [v3] Sun, 31 Mar 2024 07:47:59 UTC (9,815 KB)
    [v4] Wed, 29 May 2024 16:59:59 UTC (12,491 KB)
    Jinyuan Li, Han Li, Di Sun, Jiahao Wang, Wenkun Zhang, Zan Wang, Gang Pan

Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two major limitations of existing GMNER methods. 2) The introduction of entity expansion expression and Visual Entailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG). It enables RiVEG to effortlessly inherit the Visual Entailment and Visual Grounding capabilities of any current or prospective multimodal pretraining models. Extensive experiments demonstrate that RiVEG outperforms state-of-the-art methods on the existing GMNER dataset and achieves absolute leads of 10.65%, 6.21%, and 8.83% in all three subtasks.

------------

`[2403.17344] Disambiguate Entity Matching using Large Language Models through Relation Discovery <https://arxiv.org/abs/2403.17344>`__ 基于关系发现的大型语言模型消歧实体匹配

::

    replaced with revised version Wed, 29 May 2024 14:40:55 GMT
    Submission history From: Zezhou Huang [view email]
    [v1] Tue, 26 Mar 2024 03:07:32 UTC (629 KB)
    [v2] Wed, 29 May 2024 14:40:55 UTC (251 KB)
    Zezhou Huang

Entity matching is a critical challenge in data integration and cleaning, central to tasks like fuzzy joins and deduplication. Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from large language models (LLMs) like GPT. However, the core challenge in entity matching extends beyond term fuzziness to the ambiguity in defining what constitutes a "match," especially when integrating with external databases. This ambiguity arises due to varying levels of detail and granularity among entities, complicating exact matches. We propose a novel approach that shifts focus from purely identifying semantic similarities to understanding and defining the "relations" between entities as crucial for resolving ambiguities in matching. By predefining a set of relations relevant to the task at hand, our method allows analysts to navigate the spectrum of similarity more effectively, from exact matches to conceptually related entities.

------------

`[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models <https://arxiv.org/abs/2310.12945>`__ 3D- gpt:基于大型语言模型的程序性3D建模

::

    replaced with revised version Wed, 29 May 2024 01:35:15 GMT
    Submission history From: Junlin Han [view email]
    [v1] Thu, 19 Oct 2023 17:41:48 UTC (19,923 KB)
    [v2] Wed, 29 May 2024 01:35:15 UTC (19,923 KB)
    Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, Stephen Gould

In the pursuit of efficient automated content creation, procedural generation, leveraging modifiable parameters and rule-based systems, emerges as a promising approach. Nonetheless, it could be a demanding endeavor, given its intricate nature necessitating a deep understanding of rules, algorithms, and parameters. To reduce workload, we introduce 3D-GPT, a framework utilizing large language models~(LLMs) for instruction-driven 3D modeling. 3D-GPT positions LLMs as proficient problem solvers, dissecting the procedural 3D modeling tasks into accessible segments and appointing the apt agent for each task. 3D-GPT integrates three core agents: the task dispatch agent, the conceptualization agent, and the modeling agent. They collaboratively achieve two objectives. First, it enhances concise initial scene descriptions, evolving them into detailed forms while dynamically adapting the text based on subsequent instructions. Second, it integrates procedural generation, extracting parameter values from enriched text to effortlessly interface with 3D software for asset creation. Our empirical investigations confirm that 3D-GPT not only interprets and executes instructions, delivering reliable results but also collaborates effectively with human designers. Furthermore, it seamlessly integrates with Blender, unlocking expanded manipulation possibilities. Our work highlights the potential of LLMs in 3D modeling, offering a basic framework for future advancements in scene generation and animation.

------------

----------
Index (92)
----------

`[2405.18581] Unleashing the Potential of Text-attributed Graphs: Automatic Relation Decomposition via Large Language Models <https://arxiv.org/abs/2405.18581>`__ 释放文本属性图的潜力:基于大型语言模型的自动关系分解

`[2405.18636] ChatGPT as the Marketplace of Ideas: Should Truth-Seeking Be the Goal of AI Content Governance? <https://arxiv.org/abs/2405.18636>`__ ChatGPT作为思想的市场:寻求真理应该是AI内容治理的目标吗?

`[2405.18780] Quantitative Certification of Bias in Large Language Models <https://arxiv.org/abs/2405.18780>`__ 大型语言模型偏差的定量证明

`[2405.18870] LLMs achieve adult human performance on higher-order theory of mind tasks <https://arxiv.org/abs/2405.18870>`__ llm在高阶心智理论任务上实现成人的表现

`[2405.19032] Large Language Models for Code Summarization <https://arxiv.org/abs/2405.19032>`__ 用于代码摘要的大型语言模型

`[2405.19132] Analyzing Chat Protocols of Novice Programmers Solving Introductory Programming Tasks with ChatGPT <https://arxiv.org/abs/2405.19132>`__ 用ChatGPT分析新手程序员解决编程入门任务时的聊天协议

`[2405.19255] Towards Next-Generation Urban Decision Support Systems through AI-Powered Generation of Scientific Ontology using Large Language Models -- A Case in Optimizing Intermodal Freight Transportation <https://arxiv.org/abs/2405.19255>`__ 通过使用大型语言模型人工智能驱动的科学本体生成走向下一代城市决策支持系统——以优化联运货物运输为例

`[2405.19313] Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice <https://arxiv.org/abs/2405.19313>`__ 经过算术训练的语言模型可以预测人类的风险和跨期选择

`[2405.18492] LLMs and Memorization: On Quality and Specificity of Copyright Compliance <https://arxiv.org/abs/2405.18492>`__ LLMs和记忆:关于版权遵守的质量和特异性

`[2405.18540] Learning diverse attacks on large language models for robust red-teaming and safety tuning <https://arxiv.org/abs/2405.18540>`__ 在大型语言模型上学习多种攻击以进行鲁棒红队和安全调优

`[2405.18638] ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models <https://arxiv.org/abs/2405.18638>`__ 考虑人工评估框架:重新思考人工对生成式大型语言模型的评估

`[2405.18649] Training LLMs to Better Self-Debug and Explain Code <https://arxiv.org/abs/2405.18649>`__ 培训llm，使其更好地自我调试和解释代码

`[2405.18662] Understanding Intrinsic Socioeconomic Biases in Large Language Models <https://arxiv.org/abs/2405.18662>`__ 理解大型语言模型中的内在社会经济偏见

`[2405.18682] Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine Reading Comprehension <https://arxiv.org/abs/2405.18682>`__ GPT能重新定义医学理解吗?评估GPT在生物医学机器阅读理解上的表现

`[2405.18719] Contextual Position Encoding: Learning to Count What's Important <https://arxiv.org/abs/2405.18719>`__ 上下文位置编码:学习计算什么是重要的

`[2405.18741] Genshin: General Shield for Natural Language Processing with Large Language Models <https://arxiv.org/abs/2405.18741>`__ Genshin:基于大型语言模型的自然语言处理通用盾

`[2405.18822] Toxicity Detection for Free <https://arxiv.org/abs/2405.18822>`__ 免费进行毒性检测

`[2405.18906] Language Generation with Strictly Proper Scoring Rules <https://arxiv.org/abs/2405.18906>`__ 严格适当的评分规则的语言生成

`[2405.18915] Towards Faithful Chain-of-Thought: Large Language Models are Bridging Reasoners <https://arxiv.org/abs/2405.18915>`__ 走向忠实的思维链:大型语言模型是推理机的桥梁

`[2405.18952] Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets <https://arxiv.org/abs/2405.18952>`__ 你确定吗?再次排序:为更好的偏好数据集进行重复排序

`[2405.19010] Evaluating the External and Parametric Knowledge Fusion of Large Language Models <https://arxiv.org/abs/2405.19010>`__ 大型语言模型的外部和参数化知识融合评估

`[2405.19041] BLSP-KD: Bootstrapping Language-Speech Pre-training via Knowledge Distillation <https://arxiv.org/abs/2405.19041>`__ BLSP-KD:基于知识蒸馏的自助语言-语音预训练

`[2405.19086] MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors <https://arxiv.org/abs/2405.19086>`__ MEMoE:基于混合专家适配器的模型编辑增强

`[2405.19262] Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models <https://arxiv.org/abs/2405.19262>`__ 弱到强搜索:通过搜索小语言模型来对齐大型语言模型

`[2405.19265] AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data <https://arxiv.org/abs/2405.19265>`__ AlchemistCoder:通过对多源数据的后见之明调整来协调和诱导代码能力

`[2405.19266] PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications <https://arxiv.org/abs/2405.19266>`__ 儿科sgpt:用于儿科应用的大型语言模型作为中文医疗助理

`[2405.19285] MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection <https://arxiv.org/abs/2405.19285>`__ 大规模多语言抽象语义表示:幻觉检测的数据集和基线

`[2405.19299] Expert-Guided Extinction of Toxic Tokens for Debiased Generation <https://arxiv.org/abs/2405.19299>`__ 专家指导的去偏代有毒代币灭绝

`[2405.19323] Are Large Language Models Chameleons? <https://arxiv.org/abs/2405.19323>`__ 大型语言模型是变色龙吗?

`[2405.19327] MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series <https://arxiv.org/abs/2405.19327>`__ MAP-Neo:高性能透明双语大型语言模型系列

`[2405.18572] Low-rank finetuning for LLMs: A fairness perspective <https://arxiv.org/abs/2405.18572>`__ llm的低秩微调:公平视角

`[2405.18634] A Theoretical Understanding of Self-Correction through In-context Alignment <https://arxiv.org/abs/2405.18634>`__ 通过上下文对齐对自我矫正的理论理解

`[2405.18641] Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning <https://arxiv.org/abs/2405.18641>`__ 针对有害微调的大型语言模型的惰性安全对齐

`[2405.18710] To FP8 and Back Again: Quantifying the Effects of Reducing Precision on LLM Training Stability <https://arxiv.org/abs/2405.18710>`__ 量化降低精度对LLM训练稳定性的影响

`[2405.18765] Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI <https://arxiv.org/abs/2405.18765>`__ 脑机接口中基于大量脑电图数据学习通用表示的大型脑模型

`[2405.18832] MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models <https://arxiv.org/abs/2405.18832>`__ MoNDE:面向大规模稀疏模型的近数据专家混合

`[2405.18886] Compressing Large Language Models using Low Rank and Low Precision Decomposition <https://arxiv.org/abs/2405.18886>`__ 基于低秩低精度分解的大型语言模型压缩

`[2405.19024] Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory <https://arxiv.org/abs/2405.19024>`__ 逆凹效用强化学习是逆博弈论

`[2405.19026] DiveR-CT: Diversity-enhanced Red Teaming with Relaxing Constraints <https://arxiv.org/abs/2405.19026>`__ 潜水员- ct:放宽约束的多样性增强红色团队

`[2405.19107] Offline Regularised Reinforcement Learning for Large Language Models Alignment <https://arxiv.org/abs/2405.19107>`__ 面向大型语言模型对齐的离线正则化强化学习

`[2405.19119] Can Graph Learning Improve Task Planning? <https://arxiv.org/abs/2405.19119>`__ 图学习能改善任务规划吗?

`[2405.19320] Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF <https://arxiv.org/abs/2405.19320>`__ 价值激励偏好优化:线上线下RLHF的统一方法

`[2405.19332] Self-Exploring Language Models: Active Preference Elicitation for Online Alignment <https://arxiv.org/abs/2405.19332>`__ 自探索语言模型:在线对齐的主动偏好诱导

`[2405.11591] Generative Students: Using LLM-Simulated Student Profiles to Support Question Item Evaluation <https://arxiv.org/abs/2405.11591>`__ 生成型学生:使用llm模拟的学生档案来支持问题项目评估

`[2405.18620] RealitySummary: On-Demand Mixed Reality Document Enhancement using Large Language Models <https://arxiv.org/abs/2405.18620>`__ RealitySummary:基于大型语言模型的按需混合现实文档增强

`[2405.18632] Large Language Models as Partners in Student Essay Evaluation <https://arxiv.org/abs/2405.18632>`__ 大型语言模型在学生论文评估中的合作伙伴

`[2405.18732] Gemini & Physical World: Large Language Models Can Estimate the Intensity of Earthquake Shaking from Multi-Modal Social Media Posts <https://arxiv.org/abs/2405.18732>`__ Gemini & Physical World:大型语言模型可以从多模态社交媒体帖子中估计地震震动的强度

`[2405.18672] LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained Image Classification <https://arxiv.org/abs/2405.18672>`__ 基于llm的可解释细粒度图像分类的层次概念分解

`[2405.18937] Kestrel: Point Grounding Multimodal LLM for Part-Aware 3D Vision-Language Understanding <https://arxiv.org/abs/2405.18937>`__ Kestrel:基于点接地多模态LLM的部分感知3D视觉-语言理解

`[2405.19076] Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials Analysis and Design <https://arxiv.org/abs/2405.19076>`__ cepalo:面向仿生材料分析与设计的多模态视觉-语言模型

`[2405.19335] X-VILA: Cross-Modality Alignment for Large Language Model <https://arxiv.org/abs/2405.19335>`__ X-VILA:大型语言模型的跨模态对齐

`[2405.19103] Voice Jailbreak Attacks Against GPT-4o <https://arxiv.org/abs/2405.19103>`__ 针对GPT-4o的语音越狱攻击

`[2308.15030] SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget <https://arxiv.org/abs/2308.15030>`__ SwapMoE:提供可调内存预算的基于moe的现成大型语言模型

`[2405.17888] Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment <https://arxiv.org/abs/2405.17888>`__ 从SFT数据中获得更多的能量:来自人类演示的奖励学习改进了SFT对LLM的对齐

`[2307.01379] Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models <https://arxiv.org/abs/2307.01379>`__ 将注意力转移到相关性:自由形式大型语言模型的预测不确定性量化

`[2307.02179] Open-Source LLMs for Text Annotation: A Practical Guide for Model Setting and Fine-Tuning <https://arxiv.org/abs/2307.02179>`__ 用于文本注释的开源llm:模型设置和微调的实用指南

`[2310.13571] Why Can Large Language Models Generate Correct Chain-of-Thoughts? <https://arxiv.org/abs/2310.13571>`__ 为什么大型语言模型可以生成正确的思维链?

`[2310.18913] Debiasing Algorithm through Model Adaptation <https://arxiv.org/abs/2310.18913>`__ 基于模型自适应的去偏算法

`[2311.01041] Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism <https://arxiv.org/abs/2311.01041>`__ 学会拒绝:通过知识范围限制和拒绝机制使大型语言模型更加可控和可靠

`[2402.01822] Building Guardrails for Large Language Models <https://arxiv.org/abs/2402.01822>`__ 为大型语言模型建立护栏

`[2402.09025] SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks <https://arxiv.org/abs/2402.09025>`__ SLEB:通过冗余验证和消除变压器块精简llm

`[2402.13963] Towards Building Multilingual Language Model for Medicine <https://arxiv.org/abs/2402.13963>`__ 面向医学的多语言语言模型构建

`[2402.17463] Training-Free Long-Context Scaling of Large Language Models <https://arxiv.org/abs/2402.17463>`__ 大型语言模型的免训练长上下文扩展

`[2403.01774] WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations <https://arxiv.org/abs/2403.01774>`__ WebCiteS:中文网页搜索结果的带引用属性查询摘要

`[2403.15112] Text clustering with LLM embeddings <https://arxiv.org/abs/2403.15112>`__ 基于LLM嵌入的文本聚类

`[2403.19285] Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation <https://arxiv.org/abs/2403.19285>`__ 超越单词匹配:语法提高了机器翻译中的上下文示例选择

`[2404.14963] Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems <https://arxiv.org/abs/2404.14963>`__ 在GSM8K上实现>97%:对问题的深入理解使llm更好地解决数学应用题

`[2405.06800] LLM-Generated Black-box Explanations Can Be Adversarially Helpful <https://arxiv.org/abs/2405.06800>`__ llm生成的黑箱解释可能有不利的帮助

`[2405.11039] CC-GPX: Extracting High-Quality Annotated Geospatial Data from Common Crawl <https://arxiv.org/abs/2405.11039>`__ CC-GPX:从普通爬行器中提取高质量标注地理空间数据

`[2405.12689] Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text <https://arxiv.org/abs/2405.12689>`__ 识别人工智能的触觉:识别文本中由llm转述的span

`[2405.13967] DeTox: Toxic Subspace Projection for Model Editing <https://arxiv.org/abs/2405.13967>`__ DeTox:用于模型编辑的有毒子空间投影

`[2405.14782] Lessons from the Trenches on Reproducible Evaluation of Language Models <https://arxiv.org/abs/2405.14782>`__ 语言模型可重复性评估的经验教训

`[2405.15924] SLIDE: A Framework Integrating Small and Large Language Models for Open-Domain Dialogues Evaluation <https://arxiv.org/abs/2405.15924>`__ SLIDE:用于开放域对话评估的小型和大型语言模型集成框架

`[2405.16282] Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models <https://arxiv.org/abs/2405.16282>`__ 幕后的信心:大型语言模型的信心概率对齐研究

`[2305.16338] Think Before You Act: Decision Transformers with Working Memory <https://arxiv.org/abs/2305.16338>`__ 三思而后行:基于工作记忆的决策transformer

`[2310.10692] ACES: Generating Diverse Programming Puzzles with with Autotelic Generative Models <https://arxiv.org/abs/2310.10692>`__ ACES:使用Autotelic生成模型生成多样化编程难题

`[2311.09735] GEO: Generative Engine Optimization <https://arxiv.org/abs/2311.09735>`__ GEO:生成引擎优化

`[2401.09074] Code Simulation Challenges for Large Language Models <https://arxiv.org/abs/2401.09074>`__ 大型语言模型的代码模拟挑战

`[2402.05015] A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules? <https://arxiv.org/abs/2402.05015>`__ 冷静看待用于材料发现的llm:它们实际上有利于分子的贝叶斯优化吗?

`[2403.05612] Unfamiliar Finetuning Examples Control How Language Models Hallucinate <https://arxiv.org/abs/2403.05612>`__ 不熟悉的微调示例控制了语言模型的幻觉

`[2404.07774] Sketch-Plan-Generalize: Continual Few-Shot Learning of Inductively Generalizable Spatial Concepts <https://arxiv.org/abs/2404.07774>`__ 草图-规划-概化:归纳概化空间概念的连续少次学习

`[2404.16767] REBEL: Reinforcement Learning via Regressing Relative Rewards <https://arxiv.org/abs/2404.16767>`__ REBEL:基于相对奖励回归的强化学习

`[2405.16406] SpinQuant: LLM quantization with learned rotations <https://arxiv.org/abs/2405.16406>`__ SpinQuant:具有学习旋转的LLM量化

`[2405.17618] Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales <https://arxiv.org/abs/2405.17618>`__ 对称强化学习损失用于不同任务和模型规模的鲁棒学习

`[2305.12090] UP5: Unbiased Foundation Model for Fairness-aware Recommendation <https://arxiv.org/abs/2305.12090>`__ UP5:公平感知推荐的无偏基础模型

`[2402.14883] Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning <https://arxiv.org/abs/2402.14883>`__ 双i水印:保护LLM微调模型版权

`[2402.17012] Pandora's White-Box: Precise Training Data Detection and Extraction in Large Language Models <https://arxiv.org/abs/2402.17012>`__ 潘多拉的白盒:大型语言模型中的精确训练数据检测和提取

`[2405.15739] Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias <https://arxiv.org/abs/2405.15739>`__ 大型语言模型反映了具有较高引用偏差的人类引用模式

`[2405.18386] Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning <https://arxiv.org/abs/2405.18386>`__ Instruction - musicgen:通过指令调优解锁音乐语言模型的文本到音乐编辑

`[2402.09989] LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition <https://arxiv.org/abs/2402.09989>`__ 作为桥梁的llm:基础多模态命名实体识别的重构

`[2403.17344] Disambiguate Entity Matching using Large Language Models through Relation Discovery <https://arxiv.org/abs/2403.17344>`__ 基于关系发现的大型语言模型消歧实体匹配

`[2310.12945] 3D-GPT: Procedural 3D Modeling with Large Language Models <https://arxiv.org/abs/2310.12945>`__ 3D- gpt:基于大型语言模型的程序性3D建模

