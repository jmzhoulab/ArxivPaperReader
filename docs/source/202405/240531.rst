240531
========

----------
Survey (2)
----------

`[2405.20183] A Survey Study on the State of the Art of Programming Exercise Generation using Large Language Models <https://arxiv.org/abs/2405.20183>`__ 基于大型语言模型的编程习题生成研究现状综述

::

    Thu, 30 May 2024 15:49:34 GMT
    Eduard Frankford, Ingo H\"ohn, Clemens Sauerwein, Ruth Breu

This paper analyzes Large Language Models (LLMs) with regard to their programming exercise generation capabilities. Through a survey study, we defined the state of the art, extracted their strengths and weaknesses and finally proposed an evaluation matrix, helping researchers and educators to decide which LLM is the best fitting for the programming exercise generation use case. We also found that multiple LLMs are capable of producing useful programming exercises. Nevertheless, there exist challenges like the ease with which LLMs might solve exercises generated by LLMs. This paper contributes to the ongoing discourse on the integration of LLMs in education.

------------

`[2405.17935] Tool Learning with Large Language Models: A Survey <https://arxiv.org/abs/2405.17935>`__ 基于大型语言模型的工具学习综述

::

    replaced with revised version Thu, 30 May 2024 11:01:10 GMT
    Submission history From: Changle Qu [view email]
    [v1] Tue, 28 May 2024 08:01:26 UTC (928 KB)
    [v2] Thu, 30 May 2024 11:01:10 UTC (929 KB)
    Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen

Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs. We first explore the "why" by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of "how", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area. We also maintain a GitHub repository to continually keep track of the relevant papers and resources in this rising area at \url{this https URL}.

------------

-------------
Benchmark (5)
-------------

`[2405.19444] MathChat: Benchmarking Mathematical Reasoning and Instruction Following in Multi-Turn Interactions <https://arxiv.org/abs/2405.19444>`__ MathChat:多轮交互中数学推理和指导的基准测试

::

    Wed, 29 May 2024 18:45:55 GMT
    Zhenwen Liang, Dian Yu, Wenhao Yu, Wenlin Yao, Zhihan Zhang, Xiangliang Zhang, Dong Yu

Large language models (LLMs) have demonstrated impressive capabilities in mathematical problem solving, particularly in single turn question answering formats. However, real world scenarios often involve mathematical question answering that requires multi turn or interactive information exchanges, and the performance of LLMs on these tasks is still underexplored. This paper introduces MathChat, a comprehensive benchmark specifically designed to evaluate LLMs across a broader spectrum of mathematical tasks. These tasks are structured to assess the models' abilities in multiturn interactions and open ended generation. We evaluate the performance of various SOTA LLMs on the MathChat benchmark, and we observe that while these models excel in single turn question answering, they significantly underperform in more complex scenarios that require sustained reasoning and dialogue understanding. To address the above limitations of existing LLMs when faced with multiturn and open ended tasks, we develop MathChat sync, a synthetic dialogue based math dataset for LLM finetuning, focusing on improving models' interaction and instruction following capabilities in conversations. Experimental results emphasize the need for training LLMs with diverse, conversational instruction tuning datasets like MathChatsync. We believe this work outlines one promising direction for improving the multiturn mathematical reasoning abilities of LLMs, thus pushing forward the development of LLMs that are more adept at interactive mathematical problem solving and real world applications.

------------

`[2405.19856] DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories <https://arxiv.org/abs/2405.19856>`__ DevEval:一个与真实代码库一致的人工注释代码生成基准

::

    Thu, 30 May 2024 09:03:42 GMT
    Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Huanyu Liu, Hao Zhu, Lecheng Wang, Kaibo Liu, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming Zhang, Yuqi Zhu, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, Yongbin Li

How to evaluate the coding abilities of Large Language Models (LLMs) remains an open question. We find that existing benchmarks are poorly aligned with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs.
To address the knowledge gap, we propose a new benchmark named DevEval, which has three advances. (1) DevEval aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) DevEval is annotated by 13 developers and contains comprehensive annotations (e.g., requirements, original repositories, reference code, and reference dependencies). (3) DevEval comprises 1,874 testing samples from 117 repositories, covering 10 popular domains (e.g., Internet, Database). Based on DevEval, we propose repository-level code generation and evaluate 8 popular LLMs on DevEval (e.g., gpt-4, gpt-3.5, StarCoder 2, DeepSeek Coder, CodeLLaMa).
Our experiments reveal these LLMs' coding abilities in real-world code repositories. For example, in our experiments, the highest Pass@1 of gpt-4-turbo is only 53.04%. We also analyze LLMs' failed cases and summarize their shortcomings. We hope DevEval can facilitate the development of LLMs in real code repositories. DevEval, prompts, and LLMs' predictions have been released.

------------

`[2404.07972] OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments <https://arxiv.org/abs/2404.07972>`__ OSWorld:面向真实计算机环境开放式任务的多模态智能体基准测试

::

    replaced with revised version Thu, 30 May 2024 08:55:12 GMT
    Submission history From: Tianbao Xie [view email]
    [v1] Thu, 11 Apr 2024 17:56:05 UTC (40,911 KB)
    [v2] Thu, 30 May 2024 08:55:12 UTC (40,913 KB)
    Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu

Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at this https URL.

------------

`[2405.17732] C$^{3}$Bench: A Comprehensive Classical Chinese Understanding Benchmark for Large Language Models <https://arxiv.org/abs/2405.17732>`__ C$^{3}$Bench:面向大型语言模型的综合古文理解基准

::

    replaced with revised version Thu, 30 May 2024 11:32:05 GMT
    Submission history From: Jiahuan Cao [view email]
    [v1] Tue, 28 May 2024 01:23:58 UTC (1,853 KB)
    [v2] Thu, 30 May 2024 11:32:05 UTC (1,853 KB)
    Jiahuan Cao, Yongxin Shi, Dezhi Peng, Yang Liu and Lianwen Jin

Classical Chinese Understanding (CCU) holds significant value in preserving and exploration of the outstanding traditional Chinese culture. Recently, researchers have attempted to leverage the potential of Large Language Models (LLMs) for CCU by capitalizing on their remarkable comprehension and semantic capabilities. However, no comprehensive benchmark is available to assess the CCU capabilities of LLMs. To fill this gap, this paper introduces C$^{3}$bench, a Comprehensive Classical Chinese understanding benchmark, which comprises 50,000 text pairs for five primary CCU tasks, including classification, retrieval, named entity recognition, punctuation, and translation. Furthermore, the data in C$^{3}$bench originates from ten different domains, covering most of the categories in classical Chinese. Leveraging the proposed C$^{3}$bench, we extensively evaluate the quantitative performance of 15 representative LLMs on all five CCU tasks. Our results not only establish a public leaderboard of LLMs' CCU capabilities but also gain some findings. Specifically, existing LLMs are struggle with CCU tasks and still inferior to supervised models. Additionally, the results indicate that CCU is a task that requires special attention. We believe this study could provide a standard benchmark, comprehensive baselines, and valuable insights for the future advancement of LLM-based CCU research. The evaluation pipeline and dataset are available at \url{this https URL}.

------------

`[2310.12815] Formalizing and Benchmarking Prompt Injection Attacks and Defenses <https://arxiv.org/abs/2310.12815>`__ 对提示注入攻击和防御进行形式化和基准测试

::

    replaced with revised version Thu, 30 May 2024 17:09:56 GMT
    Submission history From: Yupei Liu [view email]
    [v1] Thu, 19 Oct 2023 15:12:09 UTC (3,941 KB)
    [v2] Thu, 30 May 2024 17:09:56 UTC (8,303 KB)
    Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong

A prompt injection attack aims to inject malicious instruction/data into the input of an LLM-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 LLMs and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses. To facilitate research on this topic, we make our platform public at this https URL.

------------

---------------
Accelerate (15)
---------------

`[2405.20015] Efficient LLM-Jailbreaking by Introducing Visual Modality <https://arxiv.org/abs/2405.20015>`__ 通过引入视觉模式实现高效的llm越狱

::

    Thu, 30 May 2024 12:50:32 GMT
    Zhenxing Niu, Yuyao Sun, Haodong Ren, Haoxuan Ji, Quan Wang, Xiaoke Ma, Gang Hua, Rong Jin

This paper focuses on jailbreaking attacks against large language models (LLMs), eliciting them to generate objectionable content in response to harmful user queries. Unlike previous LLM-jailbreaks that directly orient to LLMs, our approach begins by constructing a multimodal large language model (MLLM) through the incorporation of a visual module into the target LLM. Subsequently, we conduct an efficient MLLM-jailbreak to generate jailbreaking embeddings embJS. Finally, we convert the embJS into text space to facilitate the jailbreaking of the target LLM. Compared to direct LLM-jailbreaking, our approach is more efficient, as MLLMs are more vulnerable to jailbreaking than pure LLM. Additionally, to improve the attack success rate (ASR) of jailbreaking, we propose an image-text semantic matching scheme to identify a suitable initial input. Extensive experiments demonstrate that our approach surpasses current state-of-the-art methods in terms of both efficiency and effectiveness. Moreover, our approach exhibits superior cross-class jailbreaking capabilities.

------------

`[2405.20202] One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments <https://arxiv.org/abs/2405.20202>`__ 一个QuantLLM为所有人:一次微调量化llm以有效部署

::

    Thu, 30 May 2024 16:05:15 GMT
    Ke Yi, Yuhui Xu, Heng Chang, Chen Tang, Yuan Meng, Tong Zhang, Jia Li

Large Language Models (LLMs) have advanced rapidly but face significant memory demands. While quantization has shown promise for LLMs, current methods typically require lengthy training to alleviate the performance degradation from quantization loss. However, deploying LLMs across diverse scenarios with different resource constraints, e.g., servers and personal computers, requires repeated training per application, which amplifies the lengthy training problem. Given that, it is advantageous to train a once-for-all (OFA) supernet capable of yielding diverse optimal subnets for downstream applications through one-shot training. Nonetheless, the scale of current language models impedes efficiency and amplifies interference from weight sharing between subnets. We make an initial attempt to extend the once-for-all framework to large language models. Specifically, we decouple shared weights to eliminate the interference and incorporate Low-Rank adapters for training efficiency. Furthermore, we observe the imbalance allocation of training resources from the traditional uniform sampling. A non-parametric scheduler is introduced to adjust the sampling rate for each quantization configuration, achieving a more balanced allocation among subnets with varying demands. We validate the approach on LLaMA2 families, and downstream evaluation confirms our ability to maintain high performance while significantly reducing deployment time faced with multiple scenarios.

------------

`[2405.19635] GKT: A Novel Guidance-Based Knowledge Transfer Framework For Efficient Cloud-edge Collaboration LLM Deployment <https://arxiv.org/abs/2405.19635>`__ 

::

    Thu, 30 May 2024 02:37:35 GMT
    Yao Yao, Zuchao Li, Hai Zhao

The burgeoning size of Large Language Models (LLMs) has led to enhanced capabilities in generating responses, albeit at the expense of increased inference times and elevated resource demands. Existing methods of acceleration, predominantly hinged on knowledge distillation, generally necessitate fine-tuning of considerably large models, such as Llama-7B, posing a challenge for average users. Furthermore, present techniques for expediting inference and reducing costs operate independently. To address these issues, we introduce a novel and intuitive Guidance-based Knowledge Transfer (GKT) framework. This approach leverages a larger LLM as a ''teacher'' to create guidance prompts, paired with a smaller ''student'' model to finalize responses. Remarkably, GKT requires no fine-tuning and doesn't necessitate the teacher and student models to have the same vocabulary, allowing for extensive batch generation to accelerate the process while ensuring user customization.
GKT can be seamlessly integrated into cloud-edge collaboration architectures, and is versatile enough for plug-and-play application across various models. It excels in both efficiency and affordability, epitomizing a ''cheap and cheerful'' solution. GKT achieves a maximum accuracy improvement of 14.18%, along with a 10.72 times speed-up on GSM8K and an accuracy improvement of 14.00 % along with a 7.73 times speed-up in CSQA. When utilizing ChatGPT as teacher model and Llama2-70B as the student model, we can achieve 95.00% of ChatGPT's performance at 52% of the cost. The results highlight substantial enhancements in accuracy and processing speed on the GSM8K and CSQA datasets, surpassing the performance of using either the student or teacher models in isolation.

------------

`[2405.19715] SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths <https://arxiv.org/abs/2405.19715>`__ specdec++:自适应候选长度增强推测解码

::

    Thu, 30 May 2024 05:49:38 GMT
    Kaixuan Huang, Xudong Guo, Mengdi Wang

Speculative decoding reduces the inference latency of a target large language model via utilizing a smaller and faster draft model. Its performance depends on a hyperparameter K -- the candidate length, i.e., the number of candidate tokens for the target model to verify in each round. However, previous methods often use simple heuristics to choose K, which may result in sub-optimal performance. We study the choice of the candidate length K and formulate it as a Markov Decision Process. We theoretically show that the optimal policy of this Markov decision process takes the form of a threshold policy, i.e., the current speculation should stop and be verified when the probability of getting a rejection exceeds a threshold value. Motivated by this theory, we propose SpecDec++, an enhanced version of speculative decoding that adaptively determines the candidate length on the fly. We augment the draft model with a trained acceptance prediction head to predict the conditional acceptance probability of the candidate tokens. SpecDec++ will stop the current speculation when the predicted probability that at least one token gets rejected exceeds a threshold. We implement SpecDec++ and apply it to the llama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup on the Alpaca dataset (an additional 7.2% improvement over the baseline speculative decoding). On the GSM8K and HumanEval datasets, our method achieves a 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement), respectively.

------------

`[2405.20314] S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs <https://arxiv.org/abs/2405.20314>`__ S3D:一种简单且低成本的低内存gpu自推测解码方案

::

    Thu, 30 May 2024 17:54:35 GMT
    Wei Zhong and Manasa Bharadwaj

Speculative decoding (SD) has attracted a significant amount of research attention due to the substantial speedup it can achieve for LLM inference.
However, despite the high speedups they offer, speculative decoding methods often achieve optimal performance on high-end devices or with a substantial GPU memory overhead. Given limited memory and the necessity of quantization, a high-performing model on a high-end GPU can slow down by up to 7 times. To this end, we propose Skippy Simultaneous Speculative Decoding (or S3D), a cost-effective self-speculative SD method based on simultaneous multi-token decoding and mid-layer skipping. When compared against recent effective open-source SD systems, our method has achieved one of the top performance-memory ratios while requiring minimal architecture changes and training data. Leveraging our memory efficiency, we created a smaller yet more effective SD model based on Phi-3. It is 1.4 to 2 times faster than the quantized EAGLE model and operates in half-precision while using less VRAM.

------------

`[2405.19888] Parrot: Efficient Serving of LLM-based Applications with Semantic Variable <https://arxiv.org/abs/2405.19888>`__ Parrot:基于语义变量的llm应用的高效服务

::

    Thu, 30 May 2024 09:46:36 GMT
    Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu

The rise of large language models (LLMs) has enabled LLM-based applications (a.k.a. AI agents or co-pilots), a new software paradigm that combines the strength of LLM and conventional software. Diverse LLM applications from different tenants could design complex workflows using multiple LLM requests to accomplish one task. However, they have to use the over-simplified request-level API provided by today's public LLM services, losing essential application-level information. Public LLM services have to blindly optimize individual LLM requests, leading to sub-optimal end-to-end performance of LLM applications.
This paper introduces Parrot, an LLM service system that focuses on the end-to-end experience of LLM-based applications. Parrot proposes Semantic Variable, a unified abstraction to expose application-level knowledge to public LLM services. A Semantic Variable annotates an input/output variable in the prompt of a request, and creates the data pipeline when connecting multiple LLM requests, providing a natural way to program LLM applications. Exposing Semantic Variables to the public LLM service allows it to perform conventional data flow analysis to uncover the correlation across multiple LLM requests.
This correlation opens a brand-new optimization space for the end-to-end performance of LLM-based applications. Extensive evaluations demonstrate that Parrot can achieve up to an order-of-magnitude improvement for popular and practical use cases of LLM applications.

------------

`[2401.09967] Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access <https://arxiv.org/abs/2401.09967>`__ 基于草图引导的约束解码增强无Logit访问的黑盒大型语言模型

::

    replaced with revised version Wed, 29 May 2024 20:23:39 GMT
    Submission history From: Saibo Geng [view email]
    [v1] Thu, 18 Jan 2024 13:31:24 UTC (2,568 KB)
    [v2] Wed, 29 May 2024 20:23:39 UTC (4,366 KB)
    Saibo Geng, Berkay D\"oner, Chris Wendler, Martin Josifoski, Robert West

Constrained decoding, a technique for enforcing constraints on language model outputs, offers a way to control text generation without retraining or architectural modifications. Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a novel approach to constrained decoding for blackbox LLMs, which operates without access to the logits of the blackbox LLM. SGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a "sketch" for further elaboration. This approach is complementary to traditional logit-based techniques and enables the application of constrained decoding in settings where full model transparency is unavailable. We demonstrate the efficacy of SGCD through experiments in closed information extraction and constituency parsing, showing how it enhances the utility and flexibility of blackbox LLMs for complex NLP tasks.

------------

`[2402.06967] Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue <https://arxiv.org/abs/2402.06967>`__ 指导一次，在多轮中持续聊天:对话的有效调优框架

::

    replaced with revised version Thu, 30 May 2024 04:57:36 GMT
    Submission history From: Jian Wang [view email]
    [v1] Sat, 10 Feb 2024 14:52:52 UTC (365 KB)
    [v2] Thu, 30 May 2024 04:57:36 UTC (362 KB)
    Jian Wang, Chak Tou Leong, Jiashuo Wang, Dongding Lin, Wenjie Li, Xiao-Yong Wei

Tuning language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner often leads to unsatisfactory chat consistency for the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. With this in mind, we propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models. The adapters make use of respective utterances round by round in alternating order and they are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our framework performs superior to traditional fine-tuning and harbors the tremendous potential for improving dialogue consistency.

------------

`[2402.12174] BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence <https://arxiv.org/abs/2402.12174>`__ BIDER:通过关键支持证据桥接知识不一致性以实现高效检索增强的llm

::

    replaced with revised version Thu, 30 May 2024 11:26:58 GMT
    Submission history From: Jiajie Jin [view email]
    [v1] Mon, 19 Feb 2024 14:28:31 UTC (238 KB)
    [v2] Thu, 30 May 2024 11:26:58 UTC (238 KB)
    Jiajie Jin, Yutao Zhu, Yujia Zhou, Zhicheng Dou

Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy. However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM's information acquisition preferences through reinforcement learning. Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE simulation effectively equips LLMs with essential information for accurate question answering.

------------

`[2402.14800] Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models <https://arxiv.org/abs/2402.14800>`__ 并非所有专家都是平等的:高效的专家修剪和跳过专家混合的大型语言模型

::

    replaced with revised version Thu, 30 May 2024 16:24:16 GMT
    Submission history From: Aojun Zhou [view email]
    [v1] Thu, 22 Feb 2024 18:56:07 UTC (262 KB)
    [v2] Thu, 30 May 2024 16:24:16 UTC (267 KB)
    Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng Li

A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Data and code will be available at this https URL.

------------

`[2402.14808] RelayAttention for Efficient Large Language Model Serving with Long System Prompts <https://arxiv.org/abs/2402.14808>`__ RelayAttention用于支持长系统提示的高效大型语言模型

::

    replaced with revised version Thu, 30 May 2024 05:09:25 GMT
    Submission history From: Lei Zhu [view email]
    [v1] Thu, 22 Feb 2024 18:58:28 UTC (21,563 KB)
    [v2] Thu, 29 Feb 2024 16:09:58 UTC (21,563 KB)
    [v3] Thu, 30 May 2024 05:09:25 UTC (21,671 KB)
    Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W.H. Lau

A practical large language model (LLM) service may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (\ie, key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens. RelayAttention is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention. We have observed significant performance improvements to a production-level system, vLLM, through integration with RelayAttention. The improvements are even more profound with longer system prompts.

------------

`[2403.09919] Recurrent Drafter for Fast Speculative Decoding in Large Language Models <https://arxiv.org/abs/2403.09919>`__ 大型语言模型快速推测解码的循环草稿

::

    replaced with revised version Thu, 30 May 2024 17:55:19 GMT
    Submission history From: Aonan Zhang [view email]
    [v1] Thu, 14 Mar 2024 23:40:56 UTC (590 KB)
    [v2] Fri, 22 Mar 2024 16:06:42 UTC (590 KB)
    [v3] Thu, 30 May 2024 17:55:19 UTC (590 KB)
    Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng

In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attention structure only for inference in Medusa. We empirically demonstrate the effectiveness of the proposed method on several popular open source language models, along with a comprehensive analysis of the trade-offs involved in adopting this approach.

------------

`[2404.00242] DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference <https://arxiv.org/abs/2404.00242>`__ DeFT:基于Flash Tree-attention的高效树结构LLM推理解码

::

    replaced with revised version Wed, 29 May 2024 18:46:41 GMT
    Submission history From: Jinwei Yao [view email]
    [v1] Sat, 30 Mar 2024 04:34:54 UTC (5,798 KB)
    [v2] Wed, 29 May 2024 18:46:41 UTC (2,586 KB)
    Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin

Given the increasing demand for tree-structured interactions with LLMs, we introduce DeFT (Decoding with Flash Tree-Attention), an IO-aware tree attention algorithm tailored for tree-structured inference. Unlike traditional sequence-based decoding, tree-structured decoding better accommodates modern task requirements, including self-consistency, few-shot prompting, multi-step reasoning, and multi-model/head coordination. However, existing sequence-based inference systems are ill-suited for tree-structured decoding, resulting in redundancy in computation, memory footprints, and memory access, thereby undermining inference efficiency. To address this challenge, DeFT maintains memory-efficient attention calculation with low memory footprints through two key stages: (1) QKV Preparation: We propose a KV-Guided Grouping Strategy with Tree Split to intelligently group QKV, optimizing GPU resource utilization while minimizing memory reads/writes for KV cache between GPU global memory and on-chip shared memory; (2)Attention Calculation: We compute partial attention of each QKV group in a fused kernel and employ a Tree-topology-aware Global Reduction strategy to obtain final attention. By reducing 73-99% KV cache IO and nearly 100% IO for partial results during attention calculation (e.g., Softmax), DeFT achieves up to 2.52/3.82x speedup in the end-to-end/attention latency across three practical tree-based workloads: namely, few-shot prompting, multi-step reasoning, and speculative decoding, over state-of-the-art attention algorithms.

------------

`[2402.01869] InferCept: Efficient Intercept Support for Augmented Large Language Model Inference <https://arxiv.org/abs/2402.01869>`__ InferCept:增强大型语言模型推理的高效拦截支持

::

    replaced with revised version Thu, 30 May 2024 04:18:03 GMT
    Submission history From: Yiying Zhang [view email]
    [v1] Fri, 2 Feb 2024 19:47:57 UTC (913 KB)
    [v2] Thu, 30 May 2024 04:18:03 UTC (991 KB)
    Reyna Abhyankar, Zijian He, Vikranth Srivatsa, Hao Zhang, Yiying Zhang

Large language models are increasingly integrated with external environments, tools, and agents like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat each external interaction as the end of LLM generation and form a new request when the interaction finishes, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents InferCept, the first LLM inference framework targeting augmented LLMs and supporting the efficient interception of LLM generation. InferCept minimizes the GPU resource waste caused by LLM interceptions and dedicates saved memory for serving more requests. InferCept improves the overall serving throughput by 1.6x-2x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.

------------

`[2310.09497] A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models <https://arxiv.org/abs/2310.09497>`__ 一种面向大型语言模型的有效高效零样本排序的Setwise方法

::

    replaced with revised version Thu, 30 May 2024 10:03:27 GMT
    Submission history From: Shengyao Zhuang [view email]
    [v1] Sat, 14 Oct 2023 05:20:02 UTC (3,020 KB)
    [v2] Thu, 30 May 2024 10:03:27 UTC (2,763 KB)
    Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido Zuccon

We propose a novel zero-shot document ranking approach based on Large Language Models (LLMs): the Setwise prompting approach. Our approach complements existing prompting approaches for LLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative evaluation within a consistent experimental framework and considering factors like model size, token consumption, latency, among others, we show that existing approaches are inherently characterised by trade-offs between effectiveness and efficiency. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. Our Setwise approach, instead, reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, compared to previous methods. This significantly improves the efficiency of LLM-based zero-shot ranking, while also retaining high zero-shot ranking effectiveness. We make our code and results publicly available at \url{this https URL}.

------------

-----------------------
In-Context Learning (3)
-----------------------

`[2405.19874] Is In-Context Learning Sufficient for Instruction Following in LLMs? <https://arxiv.org/abs/2405.19874>`__ 

::

    Thu, 30 May 2024 09:28:56 GMT
    Hao Zhao, Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion

In-context learning (ICL) allows LLMs to learn from examples without changing their weights, which is a particularly promising capability for long-context LLMs that can potentially learn from many examples. Recently, Lin et al. (2024) proposed URIAL, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance. In this work, we show that, while effective, ICL alignment with URIAL still underperforms compared to instruction fine-tuning on established benchmarks such as MT-Bench and AlpacaEval 2.0 (LC), especially with more capable base LMs. Unlike for tasks such as classification, translation, or summarization, adding more ICL demonstrations for long-context LLMs does not systematically improve instruction following performance. To address this limitation, we derive a greedy selection approach for ICL examples that noticeably improves performance, yet without bridging the gap to instruction fine-tuning. Finally, we provide a series of ablation studies to better understand the reasons behind the remaining gap, and we show how some aspects of ICL depart from the existing knowledge and are specific to the instruction tuning setting. Overall, our work advances the understanding of ICL as an alignment technique. We provide our code at https://github.com/tml-epfl/icl-alignment.

------------

`[2405.19592] Why Larger Language Models Do In-context Learning Differently? <https://arxiv.org/abs/2405.19592>`__ 为什么大型语言模型的上下文学习方式不同?

::

    Thu, 30 May 2024 01:11:35 GMT
    Zhenmei Shi, Junyi Wei, Zhuoyan Xu, Yingyu Liang

Large language models (LLM) have emerged as a powerful tool for AI, with the key ability of in-context learning (ICL), where they can perform well on unseen tasks based on a brief series of task examples without necessitating any adjustments to the model parameters. One recent interesting mysterious observation is that models of different scales may have different ICL behaviors: larger models tend to be more sensitive to noise in the test context. This work studies this observation theoretically aiming to improve the understanding of LLM and ICL. We analyze two stylized settings: (1) linear regression with one-layer single-head linear transformers and (2) parity classification with two-layer multiple attention heads transformers (non-linear data and non-linear model). In both settings, we give closed-form optimal solutions and find that smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise while larger ones are more easily distracted, leading to different ICL behaviors. This sheds light on where transformers pay attention to and how that affects ICL. Preliminary experimental results on large base and chat models provide positive support for our analysis.

------------

`[2403.11904] CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification <https://arxiv.org/abs/2403.11904>`__ CICLe:面向大规模多类食品风险分类的保形上下文学习

::

    replaced with revised version Thu, 30 May 2024 08:37:45 GMT
    Submission history From: Korbinian Randl [view email]
    [v1] Mon, 18 Mar 2024 16:04:55 UTC (8,048 KB)
    [v2] Tue, 2 Apr 2024 10:25:34 UTC (8,048 KB)
    [v3] Thu, 30 May 2024 08:37:45 UTC (8,050 KB)
    Korbinian Randl, John Pavlopoulos, Aron Henriksson, Tony Lindgren

Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.

------------

-------------
Reasoning (6)
-------------

`[2405.19444] MathChat: Benchmarking Mathematical Reasoning and Instruction Following in Multi-Turn Interactions <https://arxiv.org/abs/2405.19444>`__ MathChat:多轮交互中数学推理和指导的基准测试

::

    Wed, 29 May 2024 18:45:55 GMT
    Zhenwen Liang, Dian Yu, Wenhao Yu, Wenlin Yao, Zhihan Zhang, Xiangliang Zhang, Dong Yu

Large language models (LLMs) have demonstrated impressive capabilities in mathematical problem solving, particularly in single turn question answering formats. However, real world scenarios often involve mathematical question answering that requires multi turn or interactive information exchanges, and the performance of LLMs on these tasks is still underexplored. This paper introduces MathChat, a comprehensive benchmark specifically designed to evaluate LLMs across a broader spectrum of mathematical tasks. These tasks are structured to assess the models' abilities in multiturn interactions and open ended generation. We evaluate the performance of various SOTA LLMs on the MathChat benchmark, and we observe that while these models excel in single turn question answering, they significantly underperform in more complex scenarios that require sustained reasoning and dialogue understanding. To address the above limitations of existing LLMs when faced with multiturn and open ended tasks, we develop MathChat sync, a synthetic dialogue based math dataset for LLM finetuning, focusing on improving models' interaction and instruction following capabilities in conversations. Experimental results emphasize the need for training LLMs with diverse, conversational instruction tuning datasets like MathChatsync. We believe this work outlines one promising direction for improving the multiturn mathematical reasoning abilities of LLMs, thus pushing forward the development of LLMs that are more adept at interactive mathematical problem solving and real world applications.

------------

`[2405.19737] Beyond Imitation: Learning Key Reasoning Steps from Dual Chain-of-Thoughts in Reasoning Distillation <https://arxiv.org/abs/2405.19737>`__ 超越模仿:推理蒸馏中从双思维链学习关键推理步骤

::

    Thu, 30 May 2024 06:32:11 GMT
    Chengwei Dai, Kun Li, Wei Zhou, Songlin Hu

As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts (CoTs) reasoning abilities, practical resource constraints drive efforts to distill these capabilities into more compact Smaller Language Models (SLMs). We find that CoTs consist mainly of simple reasoning forms, with a small proportion ($\approx 4.7\%$) of key reasoning steps that truly impact conclusions. However, previous distillation methods typically involve supervised fine-tuning student SLMs only on correct CoTs data produced by teacher LLMs, resulting in students struggling to learn the key reasoning steps, instead imitating the teacher's reasoning forms and making errors or omissions on these steps. To address these issues, drawing an analogy to human learning, where analyzing mistakes according to correct solutions often reveals the crucial steps leading to successes or failures, we propose mistak\textbf{E}-\textbf{D}riven key reason\textbf{I}ng step distilla\textbf{T}ion (\textbf{EDIT}), a novel method that further aids SLMs learning key reasoning steps rather than mere simple fine-tuning. Firstly, to expose these crucial steps in CoTs, we design specific prompts to generate dual CoTs data with similar reasoning paths but divergent conclusions. Then, we apply the minimum edit distance algorithm on the dual CoTs data to locate these key steps and optimize the likelihood of these steps. Extensive experiments validate the effectiveness of EDIT across both in-domain and out-of-domain benchmark reasoning datasets. Further analysis shows that EDIT can generate high-quality CoTs with more correct key reasoning steps. Notably, we also explore how different mistake patterns affect performance and find that EDIT benefits more from logical errors than from knowledge or mathematical calculation errors in dual CoTs\footnote{Code can be found at \url{https://github.com/C-W-D/EDIT}}.

------------

`[2405.19842] Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation <https://arxiv.org/abs/2405.19842>`__ 通过级联分解CoTs蒸馏提高学生推理的泛化能力

::

    Thu, 30 May 2024 08:49:34 GMT
    Chengwei Dai, Kun Li, Wei Zhou, Songlin Hu

Large language models (LLMs) exhibit enhanced reasoning at larger scales, driving efforts to distill these capabilities into smaller models via teacher-student learning. Previous works simply fine-tune student models on teachers' generated Chain-of-Thoughts (CoTs) data. Although these methods enhance in-domain (IND) reasoning performance, they struggle to generalize to out-of-domain (OOD) tasks. We believe that the widespread spurious correlations between questions and answers may lead the model to preset a specific answer which restricts the diversity and generalizability of its reasoning process. In this paper, we propose Cascading Decomposed CoTs Distillation (CasCoD) to address these issues by decomposing the traditional single-step learning process into two cascaded learning steps. Specifically, by restructuring the training objectives -- removing the answer from outputs and concatenating the question with the rationale as input -- CasCoD's two-step learning process ensures that students focus on learning rationales without interference from the preset answers, thus improving reasoning generalizability. Extensive experiments demonstrate the effectiveness of CasCoD on both IND and OOD benchmark reasoning datasets. Code can be found at https://github.com/C-W-D/CasCoD.

------------

`[2405.20139] GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning <https://arxiv.org/abs/2405.20139>`__ GNN-RAG:面向大规模语言模型推理的图神经检索

::

    Thu, 30 May 2024 15:14:24 GMT
    Costas Mavromatis, George Karypis

Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph. Question Answering over KGs (KGQA) is the task of answering natural questions grounding the reasoning to the information provided by the KG. Large Language Models (LLMs) are the state-of-the-art models for QA tasks due to their remarkable ability to understand natural language. On the other hand, Graph Neural Networks (GNNs) have been widely used for KGQA as they can handle the complex graph information stored in the KG. In this work, we introduce GNN-RAG, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.
First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and answer candidates are extracted to represent KG reasoning paths.
The extracted paths are verbalized and given as input for LLM reasoning with RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA. Furthermore, we develop a retrieval augmentation (RA) technique to further boost KGQA performance with GNN-RAG.
Experimental results show that GNN-RAG achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5% points at answer F1.

------------

`[2405.20163] Reasoning about concepts with LLMs: Inconsistencies abound <https://arxiv.org/abs/2405.20163>`__ 用llm进行概念推理:不一致的地方很多

::

    Thu, 30 May 2024 15:38:54 GMT
    Rosario Uceda-Sosa, Karthikeyan Natesan Ramamurthy, Maria Chang, Moninder Singh

The ability to summarize and organize knowledge into abstract concepts is key to learning and reasoning. Many industrial applications rely on the consistent and systematic use of concepts, especially when dealing with decision-critical knowledge. However, we demonstrate that, when methodically questioned, large language models (LLMs) often display and demonstrate significant inconsistencies in their knowledge. Computationally, the basic aspects of the conceptualization of a given domain can be represented as Is-A hierarchies in a knowledge graph (KG) or ontology, together with a few properties or axioms that enable straightforward reasoning. We show that even simple ontologies can be used to reveal conceptual inconsistencies across several LLMs. We also propose strategies that domain experts can use to evaluate and improve the coverage of key domain concepts in LLMs of various sizes. In particular, we have been able to significantly enhance the performance of LLMs of various sizes with openly available weights using simple knowledge-graph (KG) based prompting strategies.

------------

`[2405.19954] GenKubeSec: LLM-Based Kubernetes Misconfiguration Detection, Localization, Reasoning, and Remediation <https://arxiv.org/abs/2405.19954>`__ GenKubeSec:基于llm的Kubernetes错误配置检测、定位、推理和修复

::

    Thu, 30 May 2024 11:18:52 GMT
    Ehud Malul, Yair Meidan, Dudu Mimran, Yuval Elovici, Asaf Shabtai

A key challenge associated with Kubernetes configuration files (KCFs) is that they are often highly complex and error-prone, leading to security vulnerabilities and operational setbacks. Rule-based (RB) tools for KCF misconfiguration detection rely on static rule sets, making them inherently limited and unable to detect newly-discovered misconfigurations. RB tools also suffer from misdetection, since mistakes are likely when coding the detection rules. Recent methods for detecting and remediating KCF misconfigurations are limited in terms of their scalability and detection coverage, or due to the fact that they have high expertise requirements and do not offer automated remediation along with misconfiguration detection. Novel approaches that employ LLMs in their pipeline rely on API-based, general-purpose, and mainly commercial models. Thus, they pose security challenges, have inconsistent classification performance, and can be costly. In this paper, we propose GenKubeSec, a comprehensive and adaptive, LLM-based method, which, in addition to detecting a wide variety of KCF misconfigurations, also identifies the exact location of the misconfigurations and provides detailed reasoning about them, along with suggested remediation. When empirically compared with three industry-standard RB tools, GenKubeSec achieved equivalent precision (0.990) and superior recall (0.999). When a random sample of KCFs was examined by a Kubernetes security expert, GenKubeSec's explanations as to misconfiguration localization, reasoning and remediation were 100% correct, informative and useful. To facilitate further advancements in this domain, we share the unique dataset we collected, a unified misconfiguration index we developed for label standardization, our experimentation code, and GenKubeSec itself as an open-source tool.

------------

-----------
ToolUse (2)
-----------

`[2405.20245] Retrieval Augmented Structured Generation: Business Document Information Extraction As Tool Use <https://arxiv.org/abs/2405.20245>`__ 检索增强的结构化生成:作为工具使用的业务文档信息提取

::

    Thu, 30 May 2024 16:54:42 GMT
    Franz Louis Cesista, Rui Aguiar, Jason Kim, Paolo Acilo

Business Document Information Extraction (BDIE) is the problem of transforming a blob of unstructured information (raw text, scanned documents, etc.) into a structured format that downstream systems can parse and use. It has two main tasks: Key-Information Extraction (KIE) and Line Items Recognition (LIR). In this paper, we argue that BDIE is best modeled as a Tool Use problem, where the tools are these downstream systems. We then present Retrieval Augmented Structured Generation (RASG), a novel general framework for BDIE that achieves state of the art (SOTA) results on both KIE and LIR tasks on BDIE benchmarks.
The contributions of this paper are threefold: (1) We show, with ablation benchmarks, that Large Language Models (LLMs) with RASG are already competitive with or surpasses current SOTA Large Multimodal Models (LMMs) without RASG on BDIE benchmarks. (2) We propose a new metric class for Line Items Recognition, General Line Items Recognition Metric (GLIRM), that is more aligned with practical BDIE use cases compared to existing metrics, such as ANLS*, DocILE, and GriTS. (3) We provide a heuristic algorithm for backcalculating bounding boxes of predicted line items and tables without the need for vision encoders.
Finally, we claim that, while LMMs might sometimes offer marginal performance benefits, LLMs + RASG is oftentimes superior given real-world applications and constraints of BDIE.

------------

`[2405.17935] Tool Learning with Large Language Models: A Survey <https://arxiv.org/abs/2405.17935>`__ 基于大型语言模型的工具学习综述

::

    replaced with revised version Thu, 30 May 2024 11:01:10 GMT
    Submission history From: Changle Qu [view email]
    [v1] Tue, 28 May 2024 08:01:26 UTC (928 KB)
    [v2] Thu, 30 May 2024 11:01:10 UTC (929 KB)
    Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen

Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs. We first explore the "why" by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of "how", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area. We also maintain a GitHub repository to continually keep track of the relevant papers and resources in this rising area at \url{this https URL}.

------------

-----------------------
Retrieval-Augmented (9)
-----------------------

`[2405.19631] Leveraging Open-Source Large Language Models for encoding Social Determinants of Health using an Intelligent Router <https://arxiv.org/abs/2405.19631>`__ 利用开源大型语言模型利用智能路由器编码健康的社会决定因素

::

    Thu, 30 May 2024 02:33:28 GMT
    Akul Goel, Surya Narayanan Hari, Belinda Waltman, Matt Thomson

Social Determinants of Health (SDOH) play a significant role in patient health outcomes. The Center of Disease Control (CDC) introduced a subset of ICD-10 codes called Z-codes in an attempt to officially recognize and measure SDOH in the health care system. However, these codes are rarely annotated in a patient's Electronic Health Record (EHR), and instead, in many cases, need to be inferred from clinical notes. Previous research has shown that large language models (LLMs) show promise on extracting unstructured data from EHRs.
However, with thousands of models to choose from with unique architectures and training sets, it's difficult to choose one model that performs the best on coding tasks. Further, clinical notes contain trusted health information making the use of closed-source language models from commercial vendors difficult, so the identification of open source LLMs that can be run within health organizations and exhibits high performance on SDOH tasks is an urgent problem.
Here, we introduce an intelligent routing system for SDOH coding that uses a language model router to direct medical record data to open source LLMs that demonstrate optimal performance on specific SDOH codes. The intelligent routing system exhibits state of the art performance of 97.4% accuracy averaged across 5 codes, including homelessness and food insecurity, on par with closed models such as GPT-4o. In order to train the routing system and validate models, we also introduce a synthetic data generation and validation paradigm to increase the scale of training data without needing privacy protected medical records.
Together, we demonstrate an architecture for intelligent routing of inputs to task-optimal language models to achieve high performance across a set of medical coding sub-tasks.

------------

`[2405.19519] Two-layer retrieval augmented generation framework for low-resource medical question-answering: proof of concept using Reddit data <https://arxiv.org/abs/2405.19519>`__ 面向低资源医疗问答的两层检索增强生成框架:基于Reddit数据的概念证明

::

    Wed, 29 May 2024 20:56:52 GMT
    Sudeshna Das, Yao Ge, Yuting Guo, Swati Rajwal, JaMor Hairston, Jeanne Powell, Drew Walker, Snigdha Peddireddy, Sahithi Lakamana, Selen Bozkurt, Matthew Reyna, Reza Sameni, Yunyu Xiao, Sangmi Kim, Rasheeta Chandler, Natalie Hernandez, Danielle Mowery, Rachel Wightman, Jennifer Love, Anthony Spadaro, Jeanmarie Perrone, Abeed Sarker

Retrieval augmented generation (RAG) provides the capability to constrain generative model outputs, and mitigate the possibility of hallucination, by providing relevant in-context text. The number of tokens a generative large language model (LLM) can incorporate as context is finite, thus limiting the volume of knowledge from which to generate an answer. We propose a two-layer RAG framework for query-focused answer generation and evaluate a proof-of-concept for this framework in the context of query-focused summary generation from social media forums, focusing on emerging drug-related information. The evaluations demonstrate the effectiveness of the two-layer framework in resource constrained settings to enable researchers in obtaining near real-time data from users.

------------

`[2405.19670] One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2405.19670>`__ 一个代币就能帮上忙!为检索增强的大型语言模型学习可扩展和可插拔的虚拟token

::

    Thu, 30 May 2024 03:44:54 GMT
    Yutao Zhu, Zhaoheng Huang, Zhicheng Dou, Ji-Rong Wen

Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune the LLMs to adapt to RAG scenarios.
Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capacities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across nine question-answering tasks demonstrate the superiority of our approach.

------------

`[2405.20139] GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning <https://arxiv.org/abs/2405.20139>`__ GNN-RAG:面向大规模语言模型推理的图神经检索

::

    Thu, 30 May 2024 15:14:24 GMT
    Costas Mavromatis, George Karypis

Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph. Question Answering over KGs (KGQA) is the task of answering natural questions grounding the reasoning to the information provided by the KG. Large Language Models (LLMs) are the state-of-the-art models for QA tasks due to their remarkable ability to understand natural language. On the other hand, Graph Neural Networks (GNNs) have been widely used for KGQA as they can handle the complex graph information stored in the KG. In this work, we introduce GNN-RAG, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.
First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and answer candidates are extracted to represent KG reasoning paths.
The extracted paths are verbalized and given as input for LLM reasoning with RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA. Furthermore, we develop a retrieval augmentation (RA) technique to further boost KGQA performance with GNN-RAG.
Experimental results show that GNN-RAG achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5% points at answer F1.

------------

`[2405.20245] Retrieval Augmented Structured Generation: Business Document Information Extraction As Tool Use <https://arxiv.org/abs/2405.20245>`__ 检索增强的结构化生成:作为工具使用的业务文档信息提取

::

    Thu, 30 May 2024 16:54:42 GMT
    Franz Louis Cesista, Rui Aguiar, Jason Kim, Paolo Acilo

Business Document Information Extraction (BDIE) is the problem of transforming a blob of unstructured information (raw text, scanned documents, etc.) into a structured format that downstream systems can parse and use. It has two main tasks: Key-Information Extraction (KIE) and Line Items Recognition (LIR). In this paper, we argue that BDIE is best modeled as a Tool Use problem, where the tools are these downstream systems. We then present Retrieval Augmented Structured Generation (RASG), a novel general framework for BDIE that achieves state of the art (SOTA) results on both KIE and LIR tasks on BDIE benchmarks.
The contributions of this paper are threefold: (1) We show, with ablation benchmarks, that Large Language Models (LLMs) with RASG are already competitive with or surpasses current SOTA Large Multimodal Models (LMMs) without RASG on BDIE benchmarks. (2) We propose a new metric class for Line Items Recognition, General Line Items Recognition Metric (GLIRM), that is more aligned with practical BDIE use cases compared to existing metrics, such as ANLS*, DocILE, and GriTS. (3) We provide a heuristic algorithm for backcalculating bounding boxes of predicted line items and tables without the need for vision encoders.
Finally, we claim that, while LMMs might sometimes offer marginal performance benefits, LLMs + RASG is oftentimes superior given real-world applications and constraints of BDIE.

------------

`[2405.19893] Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts <https://arxiv.org/abs/2405.19893>`__ 相似性并不是你所需要的全部:赋予检索增强生成多层次的思想

::

    Thu, 30 May 2024 09:50:38 GMT
    Chunjing Gan, Dan Yang, Binbin Hu, Hanxiao Zhang, Siyuan Li, Ziqi Liu, Yue Shen, Lin Ju, Zhiqiang Zhang, Jinjie Gu, Lei Liang, Jun Zhou

In recent years, large language models (LLMs) have made remarkable achievements in various domains. However, the untimeliness and cost of knowledge updates coupled with hallucination issues of LLMs have curtailed their applications in knowledge intensive tasks, where retrieval augmented generation (RAG) can be of help. Nevertheless, existing retrieval augmented models typically use similarity as a bridge between queries and documents and follow a retrieve then read procedure. In this work, we argue that similarity is not always the panacea and totally relying on similarity would sometimes degrade the performance of retrieval augmented generation. To this end, we propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented Generation framework. To begin with, beyond existing similarity oriented thought, we embrace a small scale utility model that draws supervision from an LLM for utility oriented thought and further come up with a smarter model by comprehensively combining the similarity and utility oriented thoughts.
Furthermore, given the fact that the retrieved document set tends to be huge and using them in isolation makes it difficult to capture the commonalities and characteristics among them, we propose to make an LLM as a task adaptive summarizer to endow retrieval augmented generation with compactness-oriented thought. Finally, with multi layered thoughts from the precedent stages, an LLM is called for knowledge augmented generation. Extensive experiments on knowledge-intensive tasks have demonstrated the superiority of MetRag.

------------

`[2402.12052] Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs <https://arxiv.org/abs/2402.12052>`__ 小模型，大见解:利用精简的代理模型来决定何时为llm检索什么

::

    replaced with revised version Thu, 30 May 2024 12:03:51 GMT
    Submission history From: Jiejun Tan [view email]
    [v1] Mon, 19 Feb 2024 11:11:08 UTC (1,679 KB)
    [v2] Thu, 22 Feb 2024 03:23:55 UTC (1,680 KB)
    [v3] Thu, 30 May 2024 12:03:51 UTC (1,676 KB)
    Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, Ji-Rong Wen

The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the missing knowledge in questions that the LLM does not know. Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs.

------------

`[2402.12174] BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence <https://arxiv.org/abs/2402.12174>`__ BIDER:通过关键支持证据桥接知识不一致性以实现高效检索增强的llm

::

    replaced with revised version Thu, 30 May 2024 11:26:58 GMT
    Submission history From: Jiajie Jin [view email]
    [v1] Mon, 19 Feb 2024 14:28:31 UTC (238 KB)
    [v2] Thu, 30 May 2024 11:26:58 UTC (238 KB)
    Jiajie Jin, Yutao Zhu, Yujia Zhou, Zhicheng Dou

Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy. However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM's information acquisition preferences through reinforcement learning. Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE simulation effectively equips LLMs with essential information for accurate question answering.

------------

`[2403.01567] ReMatch: Retrieval Enhanced Schema Matching with LLMs <https://arxiv.org/abs/2403.01567>`__ ReMatch:使用llm检索增强的模式匹配

::

    replaced with revised version Thu, 30 May 2024 14:33:46 GMT
    Submission history From: Menachem Brief [view email]
    [v1] Sun, 3 Mar 2024 17:14:40 UTC (611 KB)
    [v2] Thu, 30 May 2024 14:33:46 UTC (606 KB)
    Eitam Sheetrit, Menachem Brief, Moshik Mishaeli, Oren Elisha

Schema matching is a crucial task in data integration, involving the alignment of a source schema with a target schema to establish correspondence between their elements. This task is challenging due to textual and semantic heterogeneity, as well as differences in schema sizes. Although machine-learning-based solutions have been explored in numerous studies, they often suffer from low accuracy, require manual mapping of the schemas for model training, or need access to source schema data which might be unavailable due to privacy concerns. In this paper we present a novel method, named ReMatch, for matching schemas using retrieval-enhanced Large Language Models (LLMs). Our method avoids the need for predefined mapping, any model training, or access to data in the source database. Our experimental results on large real-world schemas demonstrate that ReMatch is an effective matcher. By eliminating the requirement for training data, ReMatch becomes a viable solution for real-world scenarios.

------------

---------
Agent (9)
---------

`[2405.19425] Adaptive In-conversation Team Building for Language Model Agents <https://arxiv.org/abs/2405.19425>`__ 语言模型agent自适应会话团队构建

::

    Wed, 29 May 2024 18:08:37 GMT
    Linxin Song, Jiale Liu, Jieyu Zhang, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, Chi Wang

Leveraging multiple large language model (LLM) agents has shown to be a promising approach for tackling complex tasks, while the effective design of multiple agents for a particular application remains an art. It is thus intriguing to answer a critical question: Given a task, how can we build a team of LLM agents to solve it effectively? Our new adaptive team-building paradigm offers a flexible solution, realized through a novel agent design named Captain Agent. It dynamically forms and manages teams for each step of a task-solving process, utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. It allows for a flexible yet structured approach to problem-solving and can help reduce redundancy and enhance output diversity. A comprehensive evaluation across six real-world scenarios demonstrates that Captain Agent significantly outperforms existing multi-agent methods with 21.94% improvement in average accuracy, providing outstanding performance without requiring task-specific prompt engineering.

------------

`[2405.20252] Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt Optimization <https://arxiv.org/abs/2405.20252>`__ 面向零样本提示优化的分层多智能体工作流

::

    Thu, 30 May 2024 17:05:45 GMT
    Yuchi Liu, Jaskirat Singh, Gaowen Liu, Ali Payani, Liang Zheng

Large language models (LLMs) have shown great progress in responding to user questions, allowing for a multitude of diverse applications. Yet, the quality of LLM outputs heavily depends on the prompt design, where a good prompt might enable the LLM to answer a very challenging question correctly. Therefore, recent works have developed many strategies for improving the prompt, including both manual crafting and in-domain optimization. However, their efficacy in unrestricted scenarios remains questionable, as the former depends on human design for specific questions and the latter usually generalizes poorly to unseen scenarios. To address these problems, we give LLMs the freedom to design the best prompts according to themselves. Specifically, we include a hierarchy of LLMs, first constructing a prompt with precise instructions and accurate wording in a hierarchical manner, and then using this prompt to generate the final answer to the user query. We term this pipeline Hierarchical Multi-Agent Workflow, or HMAW. In contrast with prior works, HMAW imposes no human restriction and requires no training, and is completely task-agnostic while capable of adjusting to the nuances of the underlying task. Through both quantitative and qualitative experiments across multiple benchmarks, we verify that despite its simplicity, the proposed approach can create detailed and suitable prompts, further boosting the performance of current LLMs.

------------

`[2405.20267] Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions <https://arxiv.org/abs/2405.20267>`__ LLM的自动竞技场:通过代理peer -battle和委员会讨论实现LLM评估的自动化

::

    Thu, 30 May 2024 17:19:19 GMT
    Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Deli Zhao, Lidong Bing

As LLMs evolve on a daily basis, there is an urgent need for a trustworthy evaluation method that can provide robust evaluation results in a timely fashion. Currently, as static benchmarks are prone to contamination concerns, users tend to trust human voting platforms, such as Chatbot Arena. However, human annotations require extensive manual efforts. To provide an automatic, robust, and trustworthy evaluation framework, we innovatively propose the Auto-Arena of LLMs, which automates the entire evaluation process with LLM agents. Firstly, an examiner LLM devises queries. Then, a pair of candidate LLMs engage in a multi-round peer-battle around the query, during which the LLM's true performance gaps become visible. Finally, a committee of LLM judges collectively discuss and determine the winner, which alleviates bias and promotes fairness. In our extensive experiment on the 17 newest LLMs, Auto-Arena shows the highest correlation with human preferences, providing a promising alternative to human evaluation platforms.

------------

`[2405.20318] CausalQuest: Collecting Natural Causal Questions for AI Agents <https://arxiv.org/abs/2405.20318>`__ CausalQuest:为AI智能体收集自然因果问题

::

    Thu, 30 May 2024 17:55:28 GMT
    Roberto Ceraolo, Dmitrii Kharlapenko, Am\'elie Reymond, Rada Mihalcea, Mrinmaya Sachan, Bernhard Sch\"olkopf, Zhijing Jin

Humans have an innate drive to seek out causality. Whether fuelled by curiosity or specific goals, we constantly question why things happen, how they are interconnected, and many other related phenomena. To develop AI agents capable of addressing this natural human quest for causality, we urgently need a comprehensive dataset of natural causal questions. Unfortunately, existing datasets either contain only artificially-crafted questions that do not reflect real AI usage scenarios or have limited coverage of questions from specific sources. To address this gap, we present CausalQuest, a dataset of 13,500 naturally occurring questions sourced from social networks, search engines, and AI assistants. We formalize the definition of causal questions and establish a taxonomy for finer-grained classification. Through a combined effort of human annotators and large language models (LLMs), we carefully label the dataset. We find that 42% of the questions humans ask are indeed causal, with the majority seeking to understand the causes behind given effects. Using this dataset, we train efficient classifiers (up to 2.85B parameters) for the binary task of identifying causal questions, achieving high performance with F1 scores of up to 0.877. We conclude with a rich set of future research directions that can build upon our data and models.

------------

`[2405.20309] Large Language Models Can Self-Improve At Web Agent Tasks <https://arxiv.org/abs/2405.20309>`__ 大型语言模型可以在Web Agent任务中自我改进

::

    Thu, 30 May 2024 17:52:36 GMT
    Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, Sepp Hochreiter

Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e.
fine-tuning on data generated by the model itself. In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark. In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31\% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement.

------------

`[2404.07972] OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments <https://arxiv.org/abs/2404.07972>`__ OSWorld:面向真实计算机环境开放式任务的多模态智能体基准测试

::

    replaced with revised version Thu, 30 May 2024 08:55:12 GMT
    Submission history From: Tianbao Xie [view email]
    [v1] Thu, 11 Apr 2024 17:56:05 UTC (40,911 KB)
    [v2] Thu, 30 May 2024 08:55:12 UTC (40,913 KB)
    Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu

Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at this https URL.

------------

`[2405.16510] Meta-Task Planning for Language Agents <https://arxiv.org/abs/2405.16510>`__ 语言智能体的元任务规划

::

    replaced with revised version Thu, 30 May 2024 12:40:06 GMT
    Submission history From: Cong Zhang [view email]
    [v1] Sun, 26 May 2024 10:33:17 UTC (548 KB)
    [v2] Tue, 28 May 2024 13:56:40 UTC (548 KB)
    [v3] Thu, 30 May 2024 12:40:06 UTC (548 KB)
    Cong Zhang, Derrick Goh Xin Deik, Dexun Li, Hao Zhang, Yong Liu

The rapid advancement of neural language models has sparked a new surge of intelligent agent research. Unlike traditional agents, large language model-based agents (LLM agents) have emerged as a promising paradigm for achieving artificial general intelligence (AGI) due to their superior reasoning and generalization capabilities. Effective planning is crucial for the success of LLM agents in real-world tasks, making it a highly pursued topic in the community. Current planning methods typically translate tasks into executable action sequences. However, determining a feasible or optimal sequence for complex tasks at fine granularity, which often requires compositing long chains of heterogeneous actions, remains challenging. This paper introduces Meta-Task Planning (MTP), a zero-shot methodology for collaborative LLM-based multi-agent systems that simplifies complex task planning by decomposing it into a hierarchy of subordinate tasks, or meta-tasks. Each meta-task is then mapped into executable actions. MTP was assessed on two rigorous benchmarks, TravelPlanner and API-Bank. Notably, MTP achieved an average $\sim40\%$ success rate on TravelPlanner, significantly higher than the state-of-the-art (SOTA) baseline ($2.92\%$), and outperforming $LLM_{api}$-4 with ReAct on API-Bank by $\sim14\%$, showing the immense potential of integrating LLM with multi-agent systems.

------------

`[2306.16092] Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model <https://arxiv.org/abs/2306.16092>`__ Chatlaw:基于知识图谱增强的专家混合大型语言模型的多agent协作法律助理

::

    replaced with revised version Thu, 30 May 2024 13:46:00 GMT
    Submission history From: Jiaxi Cui [view email]
    [v1] Wed, 28 Jun 2023 10:48:34 UTC (1,040 KB)
    [v2] Thu, 30 May 2024 13:46:00 UTC (2,294 KB)
    Jiaxi Cui, Munan Ning, Zongjian Li, Bohua Chen, Yang Yan, Hao Li, Bin Ling, Yonghong Tian, and Li Yuan

AI legal assistants based on Large Language Models (LLMs) can provide accessible legal consulting services, but the hallucination problem poses potential legal risks. This paper presents Chatlaw, an innovative legal assistant utilizing a Mixture-of-Experts (MoE) model and a multi-agent system to enhance the reliability and accuracy of AI-driven legal services. By integrating knowledge graphs with artificial screening, we construct a high-quality legal dataset to train the MoE model. This model utilizes different experts to address various legal issues, optimizing the accuracy of legal responses. Additionally, Standardized Operating Procedures (SOP), modeled after real law firm workflows, significantly reduce errors and hallucinations in legal services. Our MoE model outperforms GPT-4 in the Lawbench and Unified Qualification Exam for Legal Professionals by 7.73% in accuracy and 11 points, respectively, and also surpasses other models in multiple dimensions during real-case consultations, demonstrating our robust capability for legal consultation.

------------

`[2401.14016] Towards Uncertainty-Aware Language Agent <https://arxiv.org/abs/2401.14016>`__ 不确定性感知语言Agent研究

::

    replaced with revised version Thu, 30 May 2024 13:26:38 GMT
    Submission history From: Jiuzhou Han [view email]
    [v1] Thu, 25 Jan 2024 08:48:21 UTC (1,043 KB)
    [v2] Thu, 8 Feb 2024 03:53:34 UTC (1,865 KB)
    [v3] Thu, 30 May 2024 13:26:38 UTC (7,946 KB)
    Jiuzhou Han and Wray Buntine and Ehsan Shareghi

While Language Agents have achieved promising success by placing Large Language Models at the core of a more versatile design that dynamically interacts with the external world, the existing approaches neglect the notion of uncertainty during these interactions. We present the Uncertainty-Aware Language Agent (UALA), a framework that orchestrates the interaction between the agent and the external world using uncertainty quantification. Compared with other well-known counterparts like ReAct, our extensive experiments across 3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes demonstrate that UALA brings a significant improvement of performance, while having a substantially lower reliance on the external world (i.e., reduced number of tool calls and tokens). Our analyses provide various insights including the great potential of UALA compared with agent fine-tuning, and underscore the unreliability of verbalised confidence of LLMs as a proxy for uncertainty.

------------

-----------
Other (103)
-----------

`[2405.19544] One-Shot Safety Alignment for Large Language Models via Optimal Dualization <https://arxiv.org/abs/2405.19544>`__ 基于最优对偶的大型语言模型单次安全对齐

::

    Wed, 29 May 2024 22:12:52 GMT
    Xinmeng Huang, Shuo Li, Edgar Dobriban, Osbert Bastani, Hamed Hassani, Dongsheng Ding

The growing safety concerns surrounding Large Language Models (LLMs) raise an urgent need to align them with diverse human preferences to simultaneously enhance their helpfulness and safety. A promising approach is to enforce safety constraints through Reinforcement Learning from Human Feedback (RLHF). For such constrained RLHF, common Lagrangian-based primal-dual policy optimization methods are computationally expensive and often unstable. This paper presents a dualization perspective that reduces constrained alignment to an equivalent unconstrained alignment problem. We do so by pre-optimizing a smooth and convex dual function that has a closed form. This shortcut eliminates the need for cumbersome primal-dual policy iterations, thus greatly reducing the computational burden and improving training stability. Our strategy leads to two practical algorithms in model-based and preference-based scenarios (MoCAN and PeCAN, respectively). A broad range of experiments demonstrate the effectiveness of our methods.

------------

`[2405.19561] Quo Vadis ChatGPT? From Large Language Models to Large Knowledge Models <https://arxiv.org/abs/2405.19561>`__ Quo Vadis ChatGPT?从大型语言模型到大型知识模型

::

    Wed, 29 May 2024 23:06:54 GMT
    Venkat Venkatasubramanian and Arijit Chakraborty

The startling success of ChatGPT and other large language models (LLMs) using transformer-based generative neural network architecture in applications such as natural language processing and image synthesis has many researchers excited about potential opportunities in process systems engineering (PSE). The almost human-like performance of LLMs in these areas is indeed very impressive, surprising, and a major breakthrough. Their capabilities are very useful in certain tasks, such as writing first drafts of documents, code writing assistance, text summarization, etc. However, their success is limited in highly scientific domains as they cannot yet reason, plan, or explain due to their lack of in-depth domain knowledge. This is a problem in domains such as chemical engineering as they are governed by fundamental laws of physics and chemistry (and biology), constitutive relations, and highly technical knowledge about materials, processes, and systems. Although purely data-driven machine learning has its immediate uses, the long-term success of AI in scientific and engineering domains would depend on developing hybrid AI systems that use first principles and technical knowledge effectively. We call these hybrid AI systems Large Knowledge Models (LKMs), as they will not be limited to only NLP-based techniques or NLP-like applications. In this paper, we discuss the challenges and opportunities in developing such systems in chemical engineering.

------------

`[2405.19616] Easy Problems That LLMs Get Wrong <https://arxiv.org/abs/2405.19616>`__ llm会出错的简单问题

::

    Thu, 30 May 2024 02:09:51 GMT
    Sean Williams, James Huckle

We introduce a comprehensive Linguistic Benchmark designed to evaluate the limitations of Large Language Models (LLMs) in domains such as logical reasoning, spatial intelligence, and linguistic understanding, among others.
Through a series of straightforward questions, it uncovers the significant limitations of well-regarded models to perform tasks that humans manage with ease. It also highlights the potential of prompt engineering to mitigate some errors and underscores the necessity for better training methodologies. Our findings stress the importance of grounding LLMs with human reasoning and common sense, emphasising the need for human-in-the-loop for enterprise applications. We hope this work paves the way for future research to enhance the usefulness and reliability of new models.

------------

`[2405.19686] Knowledge Graph Tuning: Real-time Large Language Model Personalization based on Human Feedback <https://arxiv.org/abs/2405.19686>`__ 知识图谱调优:基于人工反馈的实时大规模语言模型个性化

::

    Thu, 30 May 2024 04:57:03 GMT
    Jingwei Sun, Zhixu Du, Yiran Chen

Large language models (LLMs) have demonstrated remarkable proficiency in a range of natural language processing tasks. Once deployed, LLMs encounter users with personalized factual knowledge, and such personalized knowledge is consistently reflected through users' interactions with the LLMs. To enhance user experience, real-time model personalization is essential, allowing LLMs to adapt user-specific knowledge based on user feedback during human-LLM interactions. Existing methods mostly require back-propagation to finetune the model parameters, which incurs high computational and memory costs. In addition, these methods suffer from low interpretability, which will cause unforeseen impacts on model performance during long-term use, where the user's personalized knowledge is accumulated extensively.To address these challenges, we propose Knowledge Graph Tuning (KGT), a novel approach that leverages knowledge graphs (KGs) to personalize LLMs. KGT extracts personalized factual knowledge triples from users' queries and feedback and optimizes KGs without modifying the LLM parameters. Our method improves computational and memory efficiency by avoiding back-propagation and ensures interpretability by making the KG adjustments comprehensible to humans.Experiments with state-of-the-art LLMs, including GPT-2, Llama2, and Llama3, show that KGT significantly improves personalization performance while reducing latency and GPU memory costs.
Ultimately, KGT offers a promising solution of effective, efficient, and interpretable real-time LLM personalization during user interactions with the LLMs.

------------

`[2405.19694] Grade Like a Human: Rethinking Automated Assessment with Large Language Models <https://arxiv.org/abs/2405.19694>`__ 像人一样评分:用大型语言模型重新思考自动评估

::

    Thu, 30 May 2024 05:08:15 GMT
    Wenjing Xie, Juxin Niu, Chun Jason Xue, Nan Guan

While large language models (LLMs) have been used for automated grading, they have not yet achieved the same level of performance as humans, especially when it comes to grading complex questions. Existing research on this topic focuses on a particular step in the grading procedure: grading using predefined rubrics. However, grading is a multifaceted procedure that encompasses other crucial steps, such as grading rubrics design and post-grading review. There has been a lack of systematic research exploring the potential of LLMs to enhance the entire grading~process.
In this paper, we propose an LLM-based grading system that addresses the entire grading procedure, including the following key components: 1) Developing grading rubrics that not only consider the questions but also the student answers, which can more accurately reflect students' performance. 2) Under the guidance of grading rubrics, providing accurate and consistent scores for each student, along with customized feedback. 3) Conducting post-grading review to better ensure accuracy and fairness. Additionally, we collected a new dataset named OS from a university operating system course and conducted extensive experiments on both our new dataset and the widely used Mohler dataset.
Experiments demonstrate the effectiveness of our proposed approach, providing some new insights for developing automated grading systems based on LLMs.

------------

`[2405.19850] Deciphering Human Mobility: Inferring Semantics of Trajectories with Large Language Models <https://arxiv.org/abs/2405.19850>`__ 解读人类移动:基于大型语言模型推断轨迹语义

::

    Thu, 30 May 2024 08:55:48 GMT
    Yuxiao Luo, Zhongcai Cao, Xin Jin, Kang Liu, Ling Yin

Understanding human mobility patterns is essential for various applications, from urban planning to public safety. The individual trajectory such as mobile phone location data, while rich in spatio-temporal information, often lacks semantic detail, limiting its utility for in-depth mobility analysis. Existing methods can infer basic routine activity sequences from this data, lacking depth in understanding complex human behaviors and users' characteristics.
Additionally, they struggle with the dependency on hard-to-obtain auxiliary datasets like travel surveys. To address these limitations, this paper defines trajectory semantic inference through three key dimensions: user occupation category, activity sequence, and trajectory description, and proposes the Trajectory Semantic Inference with Large Language Models (TSI-LLM) framework to leverage LLMs infer trajectory semantics comprehensively and deeply. We adopt spatio-temporal attributes enhanced data formatting (STFormat) and design a context-inclusive prompt, enabling LLMs to more effectively interpret and infer the semantics of trajectory data. Experimental validation on real-world trajectory datasets demonstrates the efficacy of TSI-LLM in deciphering complex human mobility patterns. This study explores the potential of LLMs in enhancing the semantic analysis of trajectory data, paving the way for more sophisticated and accessible human mobility research.

------------

`[2405.19877] KNOW: A Real-World Ontology for Knowledge Capture with Large Language Models <https://arxiv.org/abs/2405.19877>`__ KNOW:使用大型语言模型进行知识捕获的真实世界本体

::

    Thu, 30 May 2024 09:32:14 GMT
    Arto Bendiken

We present KNOW--the Knowledge Navigator Ontology for the World--the first ontology designed to capture everyday knowledge to augment large language models (LLMs) in real-world generative AI use cases such as personal AI assistants. Our domain is human life, both its everyday concerns and its major milestones. We have limited the initial scope of the modeled concepts to only established human universals: spacetime (places, events) plus social (people, groups, organizations). The inclusion criteria for modeled concepts are pragmatic, beginning with universality and utility. We compare and contrast previous work such as Schema.org and Cyc--as well as attempts at a synthesis of knowledge graphs and language models--noting how LLMs already encode internally much of the commonsense tacit knowledge that took decades to capture in the Cyc project. We also make available code-generated software libraries for the 12 most popular programming languages, enabling the direct use of ontology concepts in software engineering. We emphasize simplicity and developer experience in promoting AI interoperability.

------------

`[2405.19946] Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf <https://arxiv.org/abs/2405.19946>`__ 

::

    Thu, 30 May 2024 11:07:06 GMT
    Xuanfa Jin, Ziyan Wang, Yali Du, Meng Fang, Haifeng Zhang, Jun Wang

Communication is a fundamental aspect of human society, facilitating the exchange of information and beliefs among people. Despite the advancements in large language models (LLMs), recent agents built with these often neglect the control over discussion tactics, which are essential in communication scenarios and games. As a variant of the famous communication game Werewolf, One Night Ultimate Werewolf (ONUW) requires players to develop strategic discussion policies due to the potential role changes that increase the uncertainty and complexity of the game. In this work, we first present the existence of the Perfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with discussion and one without. The results showcase that the discussion greatly changes players' utilities by affecting their beliefs, emphasizing the significance of discussion tactics. Based on the insights obtained from the analyses, we propose an RL-instructed language agent framework, where a discussion policy trained by reinforcement learning (RL) is employed to determine appropriate discussion tactics to adopt. Our experimental results on several ONUW game settings demonstrate the effectiveness and generalizability of our proposed framework.

------------

`[2405.20213] PostDoc: Generating Poster from a Long Multimodal Document Using Deep Submodular Optimization <https://arxiv.org/abs/2405.20213>`__ 博士后:使用深度次模优化从长多模态文档生成海报

::

    Thu, 30 May 2024 16:16:25 GMT
    Vijay Jaisankar, Sambaran Bandyopadhyay, Kalp Vyas, Varre Chaitanya, Shwetha Somasundaram

A poster from a long input document can be considered as a one-page easy-to-read multimodal (text and images) summary presented on a nice template with good design elements. Automatic transformation of a long document into a poster is a very less studied but challenging task. It involves content summarization of the input document followed by template generation and harmonization. In this work, we propose a novel deep submodular function which can be trained on ground truth summaries to extract multimodal content from the document and explicitly ensures good coverage, diversity and alignment of text and images. Then, we use an LLM based paraphraser and propose to generate a template with various design aspects conditioned on the input content. We show the merits of our approach through extensive automated and human evaluations.

------------

`[2405.20234] Context Injection Attacks on Large Language Models <https://arxiv.org/abs/2405.20234>`__ 针对大型语言模型的上下文注入攻击

::

    Thu, 30 May 2024 16:36:47 GMT
    Cheng'an Wei, Kai Chen, Yue Zhao, Yujia Gong, Lu Xiang, and Shenchen Zhu

Large Language Models (LLMs) such as ChatGPT and Llama-2 have become prevalent in real-world applications, exhibiting impressive text generation performance. LLMs are fundamentally developed from a scenario where the input data remains static and lacks a clear structure. To behave interactively over time, LLM-based chat systems must integrate additional contextual information (i.e., chat history) into their inputs, following a pre-defined structure. This paper identifies how such integration can expose LLMs to misleading context from untrusted sources and fail to differentiate between system and user inputs, allowing users to inject context. We present a systematic methodology for conducting context injection attacks aimed at eliciting disallowed responses by introducing fabricated context. This could lead to illegal actions, inappropriate content, or technology misuse. Our context fabrication strategies, acceptance elicitation and word anonymization, effectively create misleading contexts that can be structured with attacker-customized prompt templates, achieving injection through malicious user messages. Comprehensive evaluations on real-world LLMs such as ChatGPT and Llama-2 confirm the efficacy of the proposed attack with success rates reaching 97%. We also discuss potential countermeasures that can be adopted for attack detection and developing more secure models. Our findings provide insights into the challenges associated with the real-world deployment of LLMs for interactive and structured data scenarios.

------------

`[2405.19433] Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals <https://arxiv.org/abs/2405.19433>`__ 超越共识:诊断理论基础基于语言信息反事实的自动作文评分方法的对齐

::

    Wed, 29 May 2024 18:16:32 GMT
    Yupei Wang, Renfen Hu, Zhe Zhao

While current automated essay scoring (AES) methods show high agreement with human raters, their scoring mechanisms are not fully explored. Our proposed method, using counterfactual intervention assisted by Large Language Models (LLMs), reveals that when scoring essays, BERT-like models primarily focus on sentence-level features, while LLMs are attuned to conventions, language complexity, as well as organization, indicating a more comprehensive alignment with scoring rubrics. Moreover, LLMs can discern counterfactual interventions during feedback. Our approach improves understanding of neural AES methods and can also apply to other domains seeking transparency in model-driven decisions.
The codes and data will be released at GitHub.

------------

`[2405.19487] A Full-duplex Speech Dialogue Scheme Based On Large Language Models <https://arxiv.org/abs/2405.19487>`__ 基于大型语言模型的全双工语音对话方案

::

    Wed, 29 May 2024 20:05:46 GMT
    Peng Wang, Songshuo Lu, Yaohua Tang, Sijie Yan, Yuanjun Xiong, Wei Xia

We present a generative dialogue system capable of operating in a full-duplex manner, allowing for seamless interaction. It is based on a large language model (LLM) carefully aligned to be aware of a perception module, a motor function module, and the concept of a simple finite state machine (called neural FSM) with two states. The perception and motor function modules operate simultaneously, allowing the system to simultaneously speak and listen to the user. The LLM generates textual tokens for inquiry responses and makes autonomous decisions to start responding to, wait for, or interrupt the user by emitting control tokens to the neural FSM. All these tasks of the LLM are carried out as next token prediction on a serialized view of the dialogue in real-time. In automatic quality evaluations simulating real-life interaction, the proposed system reduces the average conversation response latency by more than 3 folds compared with LLM-based half-duplex dialogue systems while responding within less than 500 milliseconds in more than 50% of evaluated interactions. Running a LLM with only 8 billion parameters, our system exhibits a 8% higher interruption precision rate than the best available commercial LLM for voice-based dialogue.

------------

`[2405.19563] Unlearning Climate Misinformation in Large Language Models <https://arxiv.org/abs/2405.19563>`__ 大型语言模型中气候假信息的遗忘

::

    Wed, 29 May 2024 23:11:53 GMT
    Michael Fore, Simranjit Singh, Chaehong Lee, Amritanshu Pandey, Antonios Anastasopoulos, Dimitrios Stamoulis

Misinformation regarding climate change is a key roadblock in addressing one of the most serious threats to humanity. This paper investigates factual accuracy in large language models (LLMs) regarding climate information. Using true/false labeled Q&A data for fine-tuning and evaluating LLMs on climate-related claims, we compare open-source models, assessing their ability to generate truthful responses to climate change questions. We investigate the detectability of models intentionally poisoned with false climate information, finding that such poisoning may not affect the accuracy of a model's responses in other domains. Furthermore, we compare the effectiveness of unlearning algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually grounding LLMs on climate change topics. Our evaluation reveals that unlearning algorithms can be effective for nuanced conceptual claims, despite previous findings suggesting their inefficacy in privacy contexts. These insights aim to guide the development of more factually reliable LLMs and highlight the need for additional work to secure LLMs against misinformation attacks.

------------

`[2405.19648] Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach <https://arxiv.org/abs/2405.19648>`__ 大型语言模型生成中的幻觉检测:标记概率方法

::

    Thu, 30 May 2024 03:00:47 GMT
    Ernesto Quevedo, Jorge Yero, Rachel Koerner, Pablo Rivas, Tomas Cerny

Concerns regarding the propensity of Large Language Models (LLMs) to produce inaccurate outputs, also known as hallucinations, have escalated. Detecting them is vital for ensuring the reliability of applications relying on LLM-generated content. Current methods often demand substantial resources and rely on extensive LLMs or employ supervised learning with multidimensional features or intricate linguistic and semantic analyses difficult to reproduce and largely depend on using the same LLM that hallucinated. This paper introduces a supervised learning approach employing two simple classifiers utilizing only four numerical features derived from tokens and vocabulary probabilities obtained from other LLM evaluators, which are not necessarily the same. The method yields promising results, surpassing state-of-the-art outcomes in multiple tasks across three different benchmarks. Additionally, we provide a comprehensive examination of the strengths and weaknesses of our approach, highlighting the significance of the features utilized and the LLM employed as an evaluator. We have released our code publicly at https://github.com/Baylor-AI/HalluDetect.

------------

`[2405.19660] PATIENT-{\Psi}: Using Large Language Models to Simulate Patients for Training Mental Health Professionals <https://arxiv.org/abs/2405.19660>`__ PATIENT-{\Psi}:使用大型语言模型模拟患者以培训心理健康专业人员

::

    Thu, 30 May 2024 03:20:56 GMT
    Ruiyi Wang, Stephanie Milani, Jamie C. Chiu, Shaun M. Eack, Travis Labrum, Samuel M. Murphy, Nev Jones, Kate Hardy, Hong Shen, Fei Fang, Zhiyu Zoey Chen

Mental illness remains one of the most critical public health issues, with a significant gap between the available mental health support and patient needs.
Many mental health professionals highlight a disconnect between their training and real-world patient interactions, leaving some trainees feeling unprepared and potentially affecting their early career success. In this paper, we propose PATIENT-{\Psi}, a novel patient simulation framework for cognitive behavior therapy (CBT) training. To build PATIENT-{\Psi}, we constructed diverse patient profiles and their corresponding cognitive models based on CBT principles, and then used large language models (LLMs) programmed with the patient cognitive models to act as a simulated therapy patient. We propose an interactive training scheme, PATIENT-{\Psi}-TRAINER, for mental health trainees to practice a key skill in CBT -- formulating the cognitive model of the patient -- through role-playing a therapy session with PATIENT-{\Psi}. To evaluate PATIENT-{\Psi}, we conducted a user study of 4 mental health trainees and 10 experts. The results demonstrate that practice using PATIENT-{\Psi}-TRAINER greatly enhances the perceived skill acquisition and confidence of the trainees beyond existing forms of training such as textbooks, videos, and role-play with non-patients.
Based on the experts' perceptions, PATIENT-{\Psi} is perceived to be closer to real patient interactions than GPT-4, and PATIENT-{\Psi}-TRAINER holds strong promise to improve trainee competencies. Our pioneering patient simulation training framework, using LLMs, holds great potential to enhance and advance mental health training, ultimately leading to improved patient care and outcomes. We will release all our data, code, and the training platform.

------------

`[2405.19740] PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations <https://arxiv.org/abs/2405.19740>`__ PertEval:用知识不变的扰动揭示llm的真实知识能力

::

    Thu, 30 May 2024 06:38:32 GMT
    Jiatong Li, Renjun Hu, Kunzhe Huang, Yan Zhuang, Qi Liu, Mengxiao Zhu, Xing Shi, Wei Lin

Expert-designed close-ended benchmarks serve as vital tools in assessing the knowledge capacity of large language models (LLMs). Despite their widespread use, concerns have mounted regarding their reliability due to limited test scenarios and an unavoidable risk of data contamination. To rectify this, we present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge capacity through knowledge-invariant perturbations. These perturbations employ human-like restatement techniques to generate on-the-fly test samples from static benchmarks, meticulously retaining knowledge-critical content while altering irrelevant details. Our toolkit further includes a suite of transition analyses that compare performance on raw vs. perturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six state-of-the-art LLMs are re-evaluated using PertEval. Results reveal significantly inflated performance of the LLMs on raw benchmarks, including an absolute 21% overestimation for GPT-4. Additionally, through a nuanced response pattern analysis, we discover that PertEval retains LLMs' uncertainty to specious knowledge, potentially being resolved through rote memorization and leading to inflated performance.
We also find that the detailed transition analyses by PertEval could illuminate weaknesses in existing LLMs' knowledge mastery and guide the development of refinement. Given these insights, we posit that PertEval can act as an essential tool that, when applied alongside any close-ended benchmark, unveils the true knowledge capacity of LLMs, marking a significant step toward more trustworthy LLM evaluation.

------------

`[2405.19744] X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions <https://arxiv.org/abs/2405.19744>`__ X-Instruction:基于自策划跨语言指令的低资源语言模型对齐

::

    Thu, 30 May 2024 06:45:23 GMT
    Chong Li, Wen Yang, Jiajun Zhang, Jinliang Lu, Shaonan Wang, Chengqing Zong

Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples into these languages can be a solution but unreliable, leading to responses with translation errors and lacking language-specific or cultural knowledge. To address this issue, we propose a novel method to construct cross-lingual instruction following samples with instruction in English and response in low-resource languages. Specifically, the language model first learns to generate appropriate English instructions according to the natural web texts in other languages as responses. The candidate cross-lingual instruction tuning samples are further refined and diversified. We have employed this method to build a large-scale cross-lingual instruction tuning dataset on 10 languages, namely X-Instruction. The instruction data built using our method incorporate more language-specific knowledge compared with the naive translation method. Experimental results have shown that the response quality of the model tuned on X-Instruction greatly exceeds the model distilled from a powerful teacher model, reaching or even surpassing the ones of ChatGPT. In addition, we find that models tuned on cross-lingual instruction following samples can follow the instruction in the output language without further tuning.

------------

`[2405.19763] Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding <https://arxiv.org/abs/2405.19763>`__ 基于标签敏感奖励的自然语言理解强化学习

::

    Thu, 30 May 2024 07:19:31 GMT
    Kuo Liao, Shuang Li, Meng Zhao, Liqun Liu, Mengge Xue, Zhenyu Hu, Honglin Han, Chengguo Yin

Recent strides in large language models (LLMs) have yielded remarkable performance, leveraging reinforcement learning from human feedback (RLHF) to significantly enhance generation and alignment capabilities. However, RLHF encounters numerous challenges, including the objective mismatch issue, leading to suboptimal performance in Natural Language Understanding (NLU) tasks. To address this limitation, we propose a novel Reinforcement Learning framework enhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs in NLU tasks. By incorporating label-sensitive pairs into reinforcement learning, our method aims to adeptly capture nuanced label-sensitive semantic features during RL, thereby enhancing natural language understanding.
Experiments conducted on five diverse foundation models across eight tasks showcase promising results. In comparison to Supervised Fine-tuning models (SFT), RLLR demonstrates an average performance improvement of 1.54%. Compared with RLHF models, the improvement averages at 0.69%. These results reveal the effectiveness of our method for LLMs in NLU tasks. Code and data available at: https://github.com/MagiaSN/ACL2024_RLLR.

------------

`[2405.19787] From Symbolic Tasks to Code Generation: Diversification Yields Better Task Performers <https://arxiv.org/abs/2405.19787>`__ 从符号任务到代码生成:多样化产生更好的任务执行者

::

    Thu, 30 May 2024 07:54:07 GMT
    Dylan Zhang, Justin Wang, Francois Charton

Instruction tuning -- tuning large language models on instruction-output pairs -- is a promising technique for making models better adapted to the real world. Yet, the key factors driving the model's capability to understand and follow instructions not seen during training remain under-explored. Our investigation begins with a series of synthetic experiments within the theoretical framework of a Turing-complete algorithm called Markov algorithm, which allows fine-grained control over the instruction-tuning data.
Generalization and robustness with respect to the training distribution emerge once a diverse enough set of tasks is provided, even though very few examples are provided for each task. We extend these initial results to a real-world application scenario of code generation and find that a more diverse instruction set, extending beyond code-related tasks, improves the performance of code generation. Our observations suggest that a more diverse semantic space for instruction-tuning sets greatly improves the model's ability to follow instructions and perform tasks.

------------

`[2405.19793] PDDLEGO: Iterative Planning in Textual Environments <https://arxiv.org/abs/2405.19793>`__ PDDLEGO:文本环境中的迭代规划

::

    Thu, 30 May 2024 08:01:20 GMT
    Li Zhang, Peter Jansen, Tianyi Zhang, Peter Clark, Chris Callison-Burch, Niket Tandon

Planning in textual environments have been shown to be a long-standing challenge even for current models. A recent, promising line of work uses LLMs to generate a formal representation of the environment that can be solved by a symbolic planner. However, existing methods rely on a fully-observed environment where all entity states are initially known, so a one-off representation can be constructed, leading to a complete plan. In contrast, we tackle partially-observed environments where there is initially no sufficient information to plan for the end-goal. We propose PDDLEGO that iteratively construct a planning representation that can lead to a partial plan for a given sub-goal. By accomplishing the sub-goal, more information is acquired to augment the representation, eventually achieving the end-goal. We show that plans produced by few-shot PDDLEGO are 43% more efficient than generating plans end-to-end on the Coin Collector simulation, with strong performance (98%) on the more complex Cooking World simulation where end-to-end LLMs fail to generate coherent plans (4%).

------------

`[2405.19795] SLM as Guardian: Pioneering AI Safety with Small Language Models <https://arxiv.org/abs/2405.19795>`__ SLM守护:用小型语言模型引领人工智能安全

::

    Thu, 30 May 2024 08:03:15 GMT
    Ohjoon Kwon, Donghyeon Jeon, Nayoung Choi, Gyu-Hwung Cho, Changbong Kim, Hyunwoo Lee, Inho Kang, Sun Kim, Taiwoo Park

Most prior safety research of large language models (LLMs) has focused on enhancing the alignment of LLMs to better suit the safety requirements of humans. However, internalizing such safeguard features into larger models brought challenges of higher training cost and unintended degradation of helpfulness. To overcome such challenges, a modular approach employing a smaller LLM to detect harmful user queries is regarded as a convenient solution in designing LLM-based system with safety requirements.
In this paper, we leverage a smaller LLM for both harmful query detection and safeguard response generation. We introduce our safety requirements and the taxonomy of harmfulness categories, and then propose a multi-task learning mechanism fusing the two tasks into a single model. We demonstrate the effectiveness of our approach, providing on par or surpassing harmful query detection and safeguard response performance compared to the publicly available LLMs.

------------

`[2405.19799] Unsupervised Mutual Learning of Dialogue Discourse Parsing and Topic Segmentation <https://arxiv.org/abs/2405.19799>`__ 对话篇章分析与话题分割的无监督互学习

::

    Thu, 30 May 2024 08:10:50 GMT
    Jiahui Xu, Feng Jiang, Anningzhe Gao, Haizhou Li

The advancement of large language models (LLMs) has propelled the development of dialogue systems. Unlike the popular ChatGPT-like assistant model, which only satisfies the user's preferences, task-oriented dialogue systems have also faced new requirements and challenges in the broader business field. They are expected to provide correct responses at each dialogue turn, at the same time, achieve the overall goal defined by the task. By understanding rhetorical structures and topic structures via topic segmentation and discourse parsing, a dialogue system may do a better planning to achieve both objectives. However, while both structures belong to discourse structure in linguistics, rhetorical structure and topic structure are mostly modeled separately or with one assisting the other in the prior work. The interaction between these two structures has not been considered for joint modeling and mutual learning.
Furthermore, unsupervised learning techniques to achieve the above are not well explored. To fill this gap, we propose an unsupervised mutual learning framework of two structures leveraging the global and local connections between them. We extend the topic modeling between non-adjacent discourse units to ensure global structural relevance with rhetorical structures. We also incorporate rhetorical structures into the topic structure through a graph neural network model to ensure local coherence consistency. Finally, we utilize the similarity between the two fused structures for mutual learning. The experimental results demonstrate that our methods outperform all strong baselines on two dialogue rhetorical datasets (STAC and Molweni), as well as dialogue topic datasets (Doc2Dial and TIAGE).

------------

`[2405.19846] Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model <https://arxiv.org/abs/2405.19846>`__ Quest:面向大型语言模型长上下文扩展的以查询为中心的数据合成方法

::

    Thu, 30 May 2024 08:50:55 GMT
    Chaochen Gao, Xing Wu, Qi Fu, Songlin Hu

Large language models, initially pre-trained with a limited context length, can better handle longer texts by continuing training on a corpus with extended contexts. However, obtaining effective long-context data is challenging due to the scarcity and uneven distribution of long documents across different domains. To address this issue, we propose a Query-centric data synthesis method, abbreviated as Quest. Quest is an interpretable method based on the observation that documents retrieved by similar queries are relevant but low-redundant, thus well-suited for synthesizing long-context data. The method is also scalable and capable of constructing large amounts of long-context data. Using Quest, we synthesize a long-context dataset up to 128k context length, significantly outperforming other data synthesis methods on multiple long-context benchmark datasets. In addition, we further verify that the Quest method is predictable through scaling law experiments, making it a reliable solution for advancing long-context models.

------------

`[2405.20079] Student Answer Forecasting: Transformer-Driven Answer Choice Prediction for Language Learning <https://arxiv.org/abs/2405.20079>`__ 学生答案预测:面向语言学习的transformer驱动答案选择预测

::

    Thu, 30 May 2024 14:09:43 GMT
    Elena Grazia Gado, Tommaso Martorella, Luca Zunino, Paola Mejia-Domenzain, Vinitra Swamy, Jibril Frej, Tanja K\"aser

Intelligent Tutoring Systems (ITS) enhance personalized learning by predicting student answers to provide immediate and customized instruction.
However, recent research has primarily focused on the correctness of the answer rather than the student's performance on specific answer choices, limiting insights into students' thought processes and potential misconceptions. To address this gap, we present MCQStudentBert, an answer forecasting model that leverages the capabilities of Large Language Models (LLMs) to integrate contextual understanding of students' answering history along with the text of the questions and answers. By predicting the specific answer choices students are likely to make, practitioners can easily extend the model to new answer choices or remove answer choices for the same multiple-choice question (MCQ) without retraining the model. In particular, we compare MLP, LSTM, BERT, and Mistral 7B architectures to generate embeddings from students' past interactions, which are then incorporated into a finetuned BERT's answer-forecasting mechanism. We apply our pipeline to a dataset of language learning MCQ, gathered from an ITS with over 10,000 students to explore the predictive accuracy of MCQStudentBert, which incorporates student interaction patterns, in comparison to correct answer prediction and traditional mastery-learning feature-based approaches. This work opens the door to more personalized content, modularization, and granular support.

------------

`[2405.20089] The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities <https://arxiv.org/abs/2405.20089>`__ 微调悖论:在不牺牲LLM能力的情况下提高翻译质量

::

    Thu, 30 May 2024 14:25:56 GMT
    David Stap, Eva Hasler, Bill Byrne, Christof Monz, Ke Tran

Fine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality. However, it is unclear what is the impact of fine-tuning on desirable LLM behaviors that are not present in neural machine translation models, such as steerability, inherent document-level translation abilities, and the ability to produce less literal translations. We perform an extensive translation evaluation on the LLaMA and Falcon family of models with model size ranging from 7 billion up to 65 billion parameters. Our results show that while fine-tuning improves the general translation quality of LLMs, several abilities degrade. In particular, we observe a decline in the ability to perform formality steering, to produce technical translations through few-shot examples, and to perform document-level translation. On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data. We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality. Our findings emphasize the need for fine-tuning strategies that preserve the benefits of LLMs for machine translation.

------------

`[2405.20092] Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation <https://arxiv.org/abs/2405.20092>`__ 分而治之达到了共识:在代码生成中释放函数的强大功能

::

    Thu, 30 May 2024 14:31:33 GMT
    Jingchang Chen, Hongxuan Tang, Zheng Chu, Qianglong Chen, Zekun Wang, Ming Liu, Bing Qin

Despite recent progress made by large language models in code generation, they still struggle with programs that meet complex requirements. Recent work utilizes plan-and-solve decomposition to decrease the complexity and leverage self-tests to refine the generated program. Yet, planning deep-inside requirements in advance can be challenging, and the tests need to be accurate to accomplish self-improvement. To this end, we propose FunCoder, a code generation framework incorporating the divide-and-conquer strategy with functional consensus. Specifically, FunCoder recursively branches off sub-functions as smaller goals during code generation, represented by a tree hierarchy. These sub-functions are then composited to attain more complex objectives. Additionally, we designate functions via a consensus formed by identifying similarities in program behavior, mitigating error propagation.
FunCoder outperforms state-of-the-art methods by +9.8% on average in HumanEval, MBPP, xCodeEval and MATH with GPT-3.5 and GPT-4. Moreover, our method demonstrates superiority on smaller models: With FunCoder, StableCode-3b surpasses GPT-3.5 by +18.6% and achieves 97.7% of GPT-4's performance on HumanEval. Further analysis reveals that our proposed dynamic function decomposition is capable of handling complex requirements, and the functional consensus prevails over self-testing in correctness evaluation.

------------

`[2405.20175] InstructionCP: A fast approach to transfer Large Language Models into target language <https://arxiv.org/abs/2405.20175>`__ InstructionCP:一种大规模语言模型到目标语言的快速转换方法

::

    Thu, 30 May 2024 15:45:13 GMT
    Kuang-Ming Chen, Hung-yi Lee

The rapid development of large language models (LLMs) in recent years has largely focused on English, resulting in models that respond exclusively in English. To adapt these models to other languages, continual pre-training (CP) is often employed, followed by supervised fine-tuning (SFT) to maintain conversational abilities. However, CP and SFT can reduce a model's ability to filter harmful content. We propose Instruction Continual Pre-training (InsCP), which integrates instruction tags into the CP process to prevent loss of conversational proficiency while acquiring new languages. Our experiments demonstrate that InsCP retains conversational and Reinforcement Learning from Human Feedback (RLHF) abilities. Empirical evaluations on language alignment, reliability, and knowledge benchmarks confirm the efficacy of InsCP. Notably, this approach requires only 0.1 billion tokens of high-quality instruction-following data, thereby reducing resource consumption.

------------

`[2405.20179] Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning CodeLLMs <https://arxiv.org/abs/2405.20179>`__ robot - instruct:面向codellm微调的模拟器增强指令对齐

::

    Thu, 30 May 2024 15:47:54 GMT
    Zichao Hu, Junyi Jessy Li, Arjun Guha, Joydeep Biswas

Large language models (LLMs) have shown great promise at generating robot programs from natural language given domain-specific robot application programming interfaces (APIs). However, the performance gap between proprietary LLMs and smaller open-weight LLMs remains wide. This raises a question: Can we fine-tune smaller open-weight LLMs for generating domain-specific robot programs to close the performance gap with proprietary LLMs? While Self-Instruct is a promising solution by generating a diverse set of training data, it cannot verify the correctness of these programs. In contrast, a robot simulator with a well-defined world can identify execution errors but limits the diversity of programs that it can verify. In this work, we introduce Robo-Instruct, which brings the best of both worlds -- it promotes the diversity of Self-Instruct while providing the correctness of simulator-based checking. Robo-Instruct introduces RoboSim to synthesize a consistent world state on the fly by inferring properties relevant to the program being checked, and simulating actions accordingly. Furthermore, the instructions and programs generated by Self-Instruct may be subtly inconsistent -- such as the program missing a step implied by the instruction. Robo-Instruct further addresses this with InstAlign, an instruction-program alignment procedure that revises the task instruction to reflect the actual results of the generated program. Given a few seed task descriptions and the robot APIs, Robo-Instruct is capable of generating a training dataset using only a small open-weight model. This dataset can then be used to fine-tune small open-weight language models, enabling them to match or even exceed the performance of several proprietary LLMs, such as GPT-3.5-Turbo and Gemini-Pro.

------------

`[2405.20192] TAIA: Large Language Models are Out-of-Distribution Data Learners <https://arxiv.org/abs/2405.20192>`__ TAIA:大型语言模型是分布外的数据学习者

::

    Thu, 30 May 2024 15:57:19 GMT
    Shuyang Jiang, Yusheng Liao, Ya Zhang, Yu Wang, Yanfeng Wang

Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or harmless content generation, it is nearly impossible to obtain a large volume of high-quality data that matches the downstream distribution. To improve the performance of LLMs in data-scarce domains with domain-mismatched data, we re-evaluated the Transformer architecture and discovered that not all parameter updates during fine-tuning contribute positively to downstream performance. Our analysis reveals that within the self-attention and feed-forward networks, only the fine-tuned attention parameters are particularly beneficial when the training set's distribution does not fully align with the test set. Based on this insight, we propose an effective inference-time intervention method: \uline{T}raining \uline{A}ll parameters but \uline{I}nferring with only \uline{A}ttention (\trainallInfAttn). We empirically validate \trainallInfAttn using two general instruction-tuning datasets and evaluate it on seven downstream tasks involving math, reasoning, and knowledge understanding across LLMs of different parameter sizes and fine-tuning techniques. Our comprehensive experiments demonstrate that \trainallInfAttn achieves superior improvements compared to both the fully fine-tuned model and the base model in most scenarios, with significant performance gains. The high tolerance of \trainallInfAttn to data mismatches makes it resistant to jailbreaking tuning and enhances specialized tasks using general data.

------------

`[2405.20215] TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models <https://arxiv.org/abs/2405.20215>`__ TS-Align:面向大规模语言模型可扩展迭代微调的师生协作框架

::

    Thu, 30 May 2024 16:17:40 GMT
    Chen Zhang, Chengguang Tang, Dading Chong, Ke Shi, Guohua Tang, Feng Jiang, Haizhou Li

Mainstream approaches to aligning large language models (LLMs) heavily rely on human preference data, particularly when models require periodic updates.
The standard process for iterative alignment of LLMs involves collecting new human feedback for each update. However, the data collection process is costly and challenging to scale. To address this issue, we introduce the "TS-Align" framework, which fine-tunes a policy model using pairwise feedback data automatically mined from its outputs. This automatic mining process is efficiently accomplished through the collaboration between a large-scale teacher model and a small-scale student model. The policy fine-tuning process can be iteratively repeated using on-policy generations within our proposed teacher-student collaborative framework. Through extensive experiments, we demonstrate that our final aligned policy outperforms the base policy model with an average win rate of 69.7% across seven conversational or instruction-following datasets. Furthermore, we show that the ranking capability of the teacher is effectively distilled into the student through our pipeline, resulting in a small-scale yet effective reward model for policy model alignment.

------------

`[2405.20253] Evaluating Large Language Model Biases in Persona-Steered Generation <https://arxiv.org/abs/2405.20253>`__ 角色引导生成中的大型语言模型偏差评估

::

    Thu, 30 May 2024 17:06:03 GMT
    Andy Liu, Mona Diab, Daniel Fried

The task of persona-steered text generation requires large language models (LLMs) to generate text that reflects the distribution of views that an individual fitting a persona could have. People have multifaceted personas, but prior work on bias in LLM-generated opinions has only explored multiple-choice settings or one-dimensional personas. We define an incongruous persona as a persona with multiple traits where one trait makes its other traits less likely in human survey data, e.g. political liberals who support increased military spending. We find that LLMs are 9.7% less steerable towards incongruous personas than congruous ones, sometimes generating the stereotypical stance associated with its demographic rather than the target stance. Models that we evaluate that are fine-tuned with Reinforcement Learning from Human Feedback (RLHF) are more steerable, especially towards stances associated with political liberals and women, but present significantly less diverse views of personas.
We also find variance in LLM steerability that cannot be predicted from multiple-choice opinion evaluation. Our results show the importance of evaluating models in open-ended text generation, as it can surface new LLM opinion biases. Moreover, such a setup can shed light on our ability to steer models toward a richer and more diverse range of viewpoints.

------------

`[2405.20285] Who Writes the Review, Human or AI? <https://arxiv.org/abs/2405.20285>`__ 谁写评论，人类还是人工智能?

::

    Thu, 30 May 2024 17:38:44 GMT
    Panagiotis C. Theocharopoulos, Spiros V. Georgakopoulos, Sotiris K. Tasoulis, Vassilis P. Plagianakos

With the increasing use of Artificial Intelligence in Natural Language Processing, concerns have been raised regarding the detection of AI-generated text in various domains. This study aims to investigate this issue by proposing a methodology to accurately distinguish AI-generated and human-written book reviews. Our approach utilizes transfer learning, enabling the model to identify generated text across different topics while improving its ability to detect variations in writing style and vocabulary. To evaluate the effectiveness of the proposed methodology, we developed a dataset consisting of real book reviews and AI-generated reviews using the recently proposed Vicuna open-source language model. The experimental results demonstrate that it is feasible to detect the original source of text, achieving an accuracy rate of 96.86%. Our efforts are oriented toward the exploration of the capabilities and limitations of Large Language Models in the context of text identification.
Expanding our knowledge in these aspects will be valuable for effectively navigating similar models in the future and ensuring the integrity and authenticity of human-generated content.

------------

`[2405.20304] Group Robust Preference Optimization in Reward-free RLHF <https://arxiv.org/abs/2405.20304>`__ 

::

    Thu, 30 May 2024 17:50:04 GMT
    Shyam Sundhar Ramesh, Yifan Hu, Iason Chaimalas, Viraj Mehta, Pier Giuseppe Sessa, Haitham Bou Ammar, Ilija Bogunovic

Adapting large language models (LLMs) for specific tasks usually involves fine-tuning through reinforcement learning with human feedback (RLHF) on preference data. While these data often come from diverse labelers' groups (e.g., different demographics, ethnicities, company teams, etc.), traditional RLHF approaches adopt a "one-size-fits-all" approach, i.e., they indiscriminately assume and optimize a single preference model, thus not being robust to unique characteristics and needs of the various groups. To address this limitation, we propose a novel Group Robust Preference Optimization (GRPO) method to align LLMs to individual groups' preferences robustly. Our approach builds upon reward-free direct preference optimization methods, but unlike previous approaches, it seeks a robust policy which maximizes the worst-case group performance. To achieve this, GRPO adaptively and sequentially weights the importance of different groups, prioritizing groups with worse cumulative loss. We theoretically study the feasibility of GRPO and analyze its convergence for the log-linear policy class. By fine-tuning LLMs with GRPO using diverse group-based global opinion data, we significantly improved performance for the worst-performing groups, reduced loss imbalances across groups, and improved probability accuracies compared to non-robust baselines.

------------

`[2405.20315] ANAH: Analytical Annotation of Hallucinations in Large Language Models <https://arxiv.org/abs/2405.20315>`__ ANAH:大型语言模型中幻觉的分析注释

::

    Thu, 30 May 2024 17:54:40 GMT
    Ziwei Ji, Yuzhe Gu, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen

Reducing the `$\textit{hallucination}$' problem of Large Language Models (LLMs) is crucial for their wide applications. A comprehensive and fine-grained measurement of the hallucination is the first key step for the governance of this issue but is under-explored in the community. Thus, we present $\textbf{ANAH}$, a bilingual dataset that offers $\textbf{AN}$alytical $\textbf{A}$nnotation of $\textbf{H}$allucinations in LLMs within Generative Question Answering. Each answer sentence in our dataset undergoes rigorous annotation, involving the retrieval of a reference fragment, the judgment of the hallucination type, and the correction of hallucinated content. ANAH consists of ~12k sentence-level annotations for ~4.3k LLM responses covering over 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer and use ANAH to train and evaluate hallucination annotators. We conduct extensive experiments on studying generative and discriminative annotators and show that, although current open-source LLMs have difficulties in fine-grained hallucination annotation, the generative annotator trained with ANAH can surpass all open-source LLMs and GPT-3.5, obtain performance competitive with GPT-4, and exhibits better generalization ability on unseen questions.

------------

`[2405.20335] Xwin-LM: Strong and Scalable Alignment Practice for LLMs <https://arxiv.org/abs/2405.20335>`__ Xwin-LM:强大且可扩展的llm校准实践

::

    Thu, 30 May 2024 17:59:31 GMT
    Bolin Ni, JingCheng Hu, Yixuan Wei, Houwen Peng, Zheng Zhang, Gaofeng Meng, Han Hu

In this work, we present Xwin-LM, a comprehensive suite of alignment methodologies for large language models (LLMs). This suite encompasses several key techniques, including supervised finetuning (SFT), reward modeling (RM), rejection sampling finetuning (RS), and direct preference optimization (DPO).
The key components are as follows: (1) Xwin-LM-SFT, models initially finetuned with high-quality instruction data; (2) Xwin-Pair, a large-scale, multi-turn preference dataset meticulously annotated using GPT-4; (3) Xwin-RM, reward models trained on Xwin-Pair, developed at scales of 7B, 13B, and 70B parameters; (4) Xwin-Set, a multiwise preference dataset in which each prompt is linked to 64 unique responses generated by Xwin-LM-SFT and scored by Xwin-RM; (5) Xwin-LM-RS, models finetuned with the highest-scoring responses from Xwin-Set; (6) Xwin-LM-DPO, models further optimized on Xwin-Set using the DPO algorithm. Our evaluations on AlpacaEval and MT-bench demonstrate consistent and significant improvements across the pipeline, demonstrating the strength and scalability of Xwin-LM. The repository https://github.com/Xwin-LM/Xwin-LM will be continually updated to foster community research.

------------

`[2405.19534] Preference Learning Algorithms Do Not Learn Preference Rankings <https://arxiv.org/abs/2405.19534>`__ 偏好学习算法不学习偏好排序

::

    Wed, 29 May 2024 21:29:44 GMT
    Angelica Chen, Sadhika Malladi, Lily H. Zhang, Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, Kyunghyun Cho

Preference learning algorithms (e.g., RLHF and DPO) are frequently used to steer LLMs to produce generations that are more preferred by humans, but our understanding of their inner workings is still limited. In this work, we study the conventional wisdom that preference learning trains models to assign higher likelihoods to more preferred outputs than less preferred outputs, measured via $\textit{ranking accuracy}$. Surprisingly, we find that most state-of-the-art preference-tuned models achieve a ranking accuracy of less than 60% on common preference datasets. We furthermore derive the $\textit{idealized ranking accuracy}$ that a preference-tuned LLM would achieve if it optimized the DPO or RLHF objective perfectly. We demonstrate that existing models exhibit a significant $\textit{alignment gap}$ -- $\textit{i.e.}$, a gap between the observed and idealized ranking accuracies. We attribute this discrepancy to the DPO objective, which is empirically and theoretically ill-suited to fix even mild ranking errors in the reference model, and derive a simple and efficient formula for quantifying the difficulty of learning a given preference datapoint. Finally, we demonstrate that ranking accuracy strongly correlates with the empirically popular win rate metric when the model is close to the reference model used in the objective, shedding further light on the differences between on-policy (e.g., RLHF) and off-policy (e.g., DPO) preference learning algorithms.

------------

`[2405.19550] Stress-Testing Capability Elicitation With Password-Locked Models <https://arxiv.org/abs/2405.19550>`__ 密码锁定模型的压力测试能力诱导

::

    Wed, 29 May 2024 22:26:26 GMT
    Ryan Greenblatt, Fabien Roger, Dmitrii Krasheninnikov, David Krueger

To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM's full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task. In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked models, LLMs fine-tuned such that some of their capabilities are deliberately hidden.
Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise.
Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password. We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords.
Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. Overall, our findings suggest that fine-tuning is an effective method of eliciting hidden capabilities of current models, but may be unreliable when high-quality demonstrations are not available, e.g. as may be the case when models' (hidden) capabilities exceed those of human demonstrators.

------------

`[2405.19653] SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems <https://arxiv.org/abs/2405.19653>`__ SysCaps:复杂系统仿真代理的语言接口

::

    Thu, 30 May 2024 03:12:04 GMT
    Patrick Emami, Zhaonan Li, Saumya Sinha, Truc Nguyen

Data-driven simulation surrogates help computational scientists study complex systems. They can also help inform impactful policy decisions. We introduce a learning framework for surrogate modeling where language is used to interface with the underlying system being simulated. We call a language description of a system a "system caption", or SysCap. To address the lack of datasets of paired natural language SysCaps and simulation runs, we use large language models (LLMs) to synthesize high-quality captions. Using our framework, we train multimodal text and timeseries regression models for two real-world simulators of complex energy systems. Our experiments demonstrate the feasibility of designing language interfaces for real-world surrogate models at comparable accuracy to standard baselines. We qualitatively and quantitatively show that SysCaps unlock text-prompt-style surrogate modeling and new generalization abilities beyond what was previously possible. We will release the generated SysCaps datasets and our code to support follow-on studies.

------------

`[2405.19807] MetaCURL: Non-stationary Concave Utility Reinforcement Learning <https://arxiv.org/abs/2405.19807>`__ MetaCURL:非平稳凹效用强化学习

::

    Thu, 30 May 2024 08:17:00 GMT
    Bianca Marin Moreno (UGA, Thoth, EDF R&D, FiME Lab), Margaux Br\'eg\`ere (LPSM, EDF R&D), Pierre Gaillard (UGA, Thoth), Nadia Oudjane (EDF R&D, FiME Lab)

We explore online learning in episodic loop-free Markov decision processes on non-stationary environments (changing losses and probability transitions). Our focus is on the Concave Utility Reinforcement Learning problem (CURL), an extension of classical RL for handling convex performance criteria in state-action distributions induced by agent policies. While various machine learning problems can be written as CURL, its non-linearity invalidates traditional Bellman equations. Despite recent solutions to classical CURL, none address non-stationary MDPs. This paper introduces MetaCURL, the first CURL algorithm for non-stationary MDPs. It employs a meta-algorithm running multiple black-box algorithms instances over different intervals, aggregating outputs via a sleeping expert framework. The key hurdle is partial information due to MDP uncertainty. Under partial information on the probability transitions (uncertainty and non-stationarity coming only from external noise, independent of agent state-action pairs), we achieve optimal dynamic regret without prior knowledge of MDP changes. Unlike approaches for RL, MetaCURL handles full adversarial losses, not just stochastic ones. We believe our approach for managing non-stationarity with experts can be of interest to the RL community.

------------

`[2405.19883] From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems <https://arxiv.org/abs/2405.19883>`__ 从言语到行动:揭示llm驱动的自主系统的理论基础

::

    Thu, 30 May 2024 09:42:54 GMT
    Jianliang He, Siyu Chen, Fengzhuo Zhang, Zhuoran Yang

In this work, from a theoretical lens, we aim to understand why large language model (LLM) empowered agents are able to solve decision-making problems in the physical world. To this end, consider a hierarchical reinforcement learning (RL) model where the LLM Planner and the Actor perform high-level task planning and low-level execution, respectively. Under this model, the LLM Planner navigates a partially observable Markov decision process (POMDP) by iteratively generating language-based subgoals via prompting. Under proper assumptions on the pretraining data, we prove that the pretrained LLM Planner effectively performs Bayesian aggregated imitation learning (BAIL) through in-context learning. Additionally, we highlight the necessity for exploration beyond the subgoals derived from BAIL by proving that naively executing the subgoals returned by LLM leads to a linear regret. As a remedy, we introduce an $\epsilon$-greedy exploration strategy to BAIL, which is proven to incur sublinear regret when the pretraining error is small. Finally, we extend our theoretical framework to include scenarios where the LLM Planner serves as a world model for inferring the transition model of the environment and to multi-agent settings, enabling coordination among multiple Actors.

------------

`[2405.20003] Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities <https://arxiv.org/abs/2405.20003>`__ 核语言熵:语义相似度对llm的细粒度不确定性量化

::

    Thu, 30 May 2024 12:42:05 GMT
    Alexander Nikitin, Jannik Kossen, Yarin Gal, Pekka Marttinen

Uncertainty quantification in Large Language Models (LLMs) is crucial for applications where safety and reliability are important. In particular, uncertainty can be used to improve the trustworthiness of LLMs by detecting factually incorrect model responses, commonly called hallucinations.
Critically, one should seek to capture the model's semantic uncertainty, i.e., the uncertainty over the meanings of LLM outputs, rather than uncertainty over lexical or syntactic variations that do not affect answer correctness. To address this problem, we propose Kernel Language Entropy (KLE), a novel method for uncertainty estimation in white- and black-box LLMs. KLE defines positive semidefinite unit trace kernels to encode the semantic similarities of LLM outputs and quantifies uncertainty using the von Neumann entropy. It considers pairwise semantic dependencies between answers (or semantic clusters), providing more fine-grained uncertainty estimates than previous methods based on hard clustering of answers. We theoretically prove that KLE generalizes the previous state-of-the-art method called semantic entropy and empirically demonstrate that it improves uncertainty quantification performance across multiple natural language generation datasets and LLM architectures.

------------

`[2405.20313] Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation <https://arxiv.org/abs/2405.20313>`__ 基于序列增强SE(3)流匹配的条件蛋白主干生成

::

    Thu, 30 May 2024 17:53:50 GMT
    Guillaume Huguet, James Vuckovic, Kilian Fatras, Eric Thibodeau-Laufer, Pablo Lemos, Riashat Islam, Cheng-Hao Liu, Jarrid Rector-Brooks, Tara Akhound-Sadegh, Michael Bronstein, Alexander Tong, Avishek Joey Bose

Proteins are essential for almost all biological processes and derive their diverse functions from complex 3D structures, which are in turn determined by their amino acid sequences. In this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FoldFlow-2, a novel sequence-conditioned SE(3)-equivariant flow matching model for protein structure generation. FoldFlow-2 presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase diversity and novelty of generated samples -- crucial for de-novo drug design -- we train FoldFlow-2 at scale on a new dataset that is an order of magnitude larger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering. We further demonstrate the ability to align FoldFlow-2 to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FoldFlow-2 outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FoldFlow-2 makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody.

------------

`[2405.19358] Robustifying Safety-Aligned Large Language Models through Clean Data Curation <https://arxiv.org/abs/2405.19358>`__ 通过干净数据管理对安全对齐的大型语言模型的鲁棒性

::

    Fri, 24 May 2024 04:50:38 GMT
    Xiaoqun Liu, Jiacheng Liang, Muchao Ye and Zhaohan Xi

Large language models (LLMs) are vulnerable when trained on datasets containing harmful content, which leads to potential jailbreaking attacks in two scenarios: the integration of harmful texts within crowdsourced data used for pre-training and direct tampering with LLMs through fine-tuning. In both scenarios, adversaries can compromise the safety alignment of LLMs, exacerbating malfunctions. Motivated by the need to mitigate these adversarial influences, our research aims to enhance safety alignment by either neutralizing the impact of malicious texts in pre-training datasets or increasing the difficulty of jailbreaking during downstream fine-tuning. In this paper, we propose a data curation framework designed to counter adversarial impacts in both scenarios. Our method operates under the assumption that we have no prior knowledge of attack details, focusing solely on curating clean texts. We introduce an iterative process aimed at revising texts to reduce their perplexity as perceived by LLMs, while simultaneously preserving their text quality. By pre-training or fine-tuning LLMs with curated clean texts, we observe a notable improvement in LLM robustness regarding safety alignment against harmful queries. For instance, when pre-training LLMs using a crowdsourced dataset containing 5\% harmful instances, adding an equivalent amount of curated texts significantly mitigates the likelihood of providing harmful responses in LLMs and reduces the attack success rate by 71\%. Our study represents a significant step towards mitigating the risks associated with training-based jailbreaking and fortifying the secure utilization of LLMs.

------------

`[2405.19360] ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users <https://arxiv.org/abs/2405.19360>`__ ART:文本到图像模型的自动red - teamaming，以保护良性用户

::

    Fri, 24 May 2024 07:44:27 GMT
    Guanlin Li, Kangjie Chen, Shudong Zhang, Jie Zhang, Tianwei Zhang

Large-scale pre-trained generative models are taking the world by storm, due to their abilities in generating creative content. Meanwhile, safeguards for these generative models are developed, to protect users' rights and safety, most of which are designed for large language models. Existing methods primarily focus on jailbreak and adversarial attacks, which mainly evaluate the model's safety under malicious prompts. Recent work found that manually crafted safe prompts can unintentionally trigger unsafe generations. To further systematically evaluate the safety risks of text-to-image models, we propose a novel Automatic Red-Teaming framework, ART. Our method leverages both vision language model and large language model to establish a connection between unsafe generations and their prompts, thereby more efficiently identifying the model's vulnerabilities. With our comprehensive experiments, we reveal the toxicity of the popular open-source text-to-image models. The experiments also validate the effectiveness, adaptability, and great diversity of ART.
Additionally, we introduce three large-scale red-teaming datasets for studying the safety risks associated with text-to-image models. Datasets and models can be found in https://github.com/GuanlinLee/ART.

------------

`[2405.19366] ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with LLM-Enhanced Cardiological Text <https://arxiv.org/abs/2405.19366>`__ ECG语义集成器(ESI):基于llm增强心脏病文本预训练的ECG基础模型

::

    Sun, 26 May 2024 06:45:39 GMT
    Han Yu, Peikun Guo, Akane Sano

The utilization of deep learning on electrocardiogram (ECG) analysis has brought the advanced accuracy and efficiency of cardiac healthcare diagnostics.
By leveraging the capabilities of deep learning in semantic understanding, especially in feature extraction and representation learning, this study introduces a new multimodal contrastive pretaining framework that aims to improve the quality and robustness of learned representations of 12-lead ECG signals. Our framework comprises two key components, including Cardio Query Assistant (CQA) and ECG Semantics Integrator(ESI). CQA integrates a retrieval-augmented generation (RAG) pipeline to leverage large language models (LLMs) and external medical knowledge to generate detailed textual descriptions of ECGs. The generated text is enriched with information about demographics and waveform patterns. ESI integrates both contrastive and captioning loss to pretrain ECG encoders for enhanced representations. We validate our approach through various downstream tasks, including arrhythmia detection and ECG-based subject identification. Our experimental results demonstrate substantial improvements over strong baselines in these tasks. These baselines encompass supervised and self-supervised learning methods, as well as prior multimodal pretraining approaches.

------------

`[2405.19495] Qiskit Code Assistant: Training LLMs for generating Quantum Computing Code <https://arxiv.org/abs/2405.19495>`__ Qiskit Code Assistant:训练llm生成量子计算代码

::

    Wed, 29 May 2024 20:21:00 GMT
    Nicolas Dupuis, Luca Buratti, Sanjay Vishwakarma, Aitana Viudes Forrat, David Kremer, Ismael Faro, Ruchir Puri, and Juan Cruz-Benito

Code Large Language Models (Code LLMs) have emerged as powerful tools, revolutionizing the software development landscape by automating the coding process and reducing time and effort required to build applications. This paper focuses on training Code LLMs to specialize in the field of quantum computing.
We begin by discussing the unique needs of quantum computing programming, which differ significantly from classical programming approaches or languages. A Code LLM specializing in quantum computing requires a foundational understanding of quantum computing and quantum information theory. However, the scarcity of available quantum code examples and the rapidly evolving field, which necessitates continuous dataset updates, present significant challenges.
Moreover, we discuss our work on training Code LLMs to produce high-quality quantum code using the Qiskit library. This work includes an examination of the various aspects of the LLMs used for training and the specific training conditions, as well as the results obtained with our current models. To evaluate our models, we have developed a custom benchmark, similar to HumanEval, which includes a set of tests specifically designed for the field of quantum computing programming using Qiskit. Our findings indicate that our model outperforms existing state-of-the-art models in quantum computing tasks.
We also provide examples of code suggestions, comparing our model to other relevant code LLMs. Finally, we introduce a discussion on the potential benefits of Code LLMs for quantum computing computational scientists, researchers, and practitioners. We also explore various features and future work that could be relevant in this context.

------------

`[2405.19581] Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases <https://arxiv.org/abs/2405.19581>`__ 源代码基础模型是可迁移的二进制分析知识库

::

    Thu, 30 May 2024 00:17:44 GMT
    Zian Su, Xiangzhe Xu, Ziyang Huang, Kaiyuan Zhang, Xiangyu Zhang

Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap.
Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE.
However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a GPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute increase in token-level precision and recall for name recovery, respectively.
These results highlight the effectiveness of our approach in automating and improving binary code analysis.

------------

`[2405.19677] Large Language Model Watermark Stealing With Mixed Integer Programming <https://arxiv.org/abs/2405.19677>`__ 基于混合整数规划的大型语言模型水印窃取

::

    Thu, 30 May 2024 04:11:17 GMT
    Zhaoxi Zhang and Xiaomei Zhang and Yanjun Zhang and Leo Yu Zhang and Chao Chen and Shengshan Hu and Asif Gill and Shirui Pan

The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM's parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.

------------

`[2405.19783] Instruction-Guided Visual Masking <https://arxiv.org/abs/2405.19783>`__ 指令引导的视觉掩蔽

::

    Thu, 30 May 2024 07:48:32 GMT
    Jinliang Zheng, Jianxiong Li, Sijie Cheng, Yinan Zheng, Jiaming Li, Jihao Liu, Yu Liu, Jingjing Liu, Xianyuan Zhan

Instruction following is crucial in contemporary LLM. However, when extended to multimodal setting, it often suffers from misalignment between specific textual instruction and targeted local region of an image. To achieve more accurate and nuanced multimodal instruction following, we introduce Instruction-guided Visual Masking (IVM), a new versatile visual grounding model that is compatible with diverse multimodal models, such as LMM and robot model.
By constructing visual masks for instruction-irrelevant regions, IVM-enhanced multimodal models can effectively focus on task-relevant image regions to better align with complex instructions. Specifically, we design a visual masking data generation pipeline and create an IVM-Mix-1M dataset with 1 million image-instruction pairs. We further introduce a new learning technique, Discriminator Weighted Supervised Learning (DWSL) for preferential IVM training that prioritizes high-quality data samples. Experimental results on generic multimodal tasks such as VQA and embodied robotic control demonstrate the versatility of IVM, which as a plug-and-play tool, significantly boosts the performance of diverse multimodal models, yielding new state-of-the-art results across challenging multimodal benchmarks. Code is available at https://github.com/2toinf/IVM.

------------

`[2405.20081] NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models <https://arxiv.org/abs/2405.20081>`__ NoiseBoost:用噪声扰动缓解多模态大型语言模型的幻觉

::

    Thu, 30 May 2024 14:11:27 GMT
    Kai Wu, Boyuan Jiang, Zhengkai Jiang, Qingdong He, Donghao Luo, Shengzhi Wang, Qingwen Liu, Chengjie Wang

Multimodal large language models (MLLMs) contribute a powerful mechanism to understanding visual information building on large language models. However, MLLMs are notorious for suffering from hallucinations, especially when generating lengthy, detailed descriptions for images. Our analysis reveals that hallucinations stem from the inherent summarization mechanism of large language models, leading to excessive dependence on linguistic tokens while neglecting vision information. In this paper, we propose NoiseBoost, a broadly applicable and simple method for alleviating hallucinations for MLLMs through the integration of noise feature perturbations. Noise perturbation acts as a regularizer, facilitating a balanced distribution of attention weights among visual and linguistic tokens. Despite its simplicity, NoiseBoost consistently enhances the performance of MLLMs across common training strategies, including supervised fine-tuning and reinforcement learning. Further, NoiseBoost pioneerly enables semi-supervised learning for MLLMs, unleashing the power of unlabeled data. Comprehensive experiments demonstrate that NoiseBoost improves dense caption accuracy by 8.1% with human evaluation and achieves comparable results with 50% of the data by mining unlabeled data. Code and models are available at https://kaiwu5.github.io/noiseboost.

------------

`[2405.20132] LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics <https://arxiv.org/abs/2405.20132>`__ LLaMEA:自动生成元启发式的大型语言模型进化算法

::

    Thu, 30 May 2024 15:10:59 GMT
    Niki van Stein and Thomas B\"ack

Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to understand natural language and generate complex code snippets. This paper introduces a novel Large Language Model Evolutionary Algorithm (LLaMEA) framework, leveraging GPT models for the automated generation and refinement of algorithms. Given a set of criteria and a task definition (the search space), LLaMEA iteratively generates, mutates and selects algorithms based on performance metrics and feedback from runtime evaluations. This framework offers a unique approach to generating optimized algorithms without requiring extensive prior expertise. We show how this framework can be used to generate novel black-box metaheuristic optimization algorithms automatically. LLaMEA generates multiple algorithms that outperform state-of-the-art optimization algorithms (Covariance Matrix Adaptation Evolution Strategy and Differential Evolution) on the five dimensional black box optimization benchmark (BBOB). The results demonstrate the feasibility of the framework and identify future directions for automated generation and optimization of algorithms via LLMs.

------------

`[2405.20189] Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory <https://arxiv.org/abs/2405.20189>`__ Nadine: llm驱动的智能社交机器人，具有情感能力和类似人类的记忆

::

    Thu, 30 May 2024 15:55:41 GMT
    Hangyeol Kang, Maher Ben Moussa, Nadia Magnenat-Thalmann

In this work, we describe our approach to developing an intelligent and robust social robotic system for the Nadine social robot platform. We achieve this by integrating Large Language Models (LLMs) and skilfully leveraging the powerful reasoning and instruction-following capabilities of these types of models to achieve advanced human-like affective and cognitive capabilities.
This approach is novel compared to the current state-of-the-art LLM-based agents which do not implement human-like long-term memory or sophisticated emotional appraisal. The naturalness of social robots, consisting of multiple modules, highly depends on the performance and capabilities of each component of the system and the seamless integration of the components. We built a social robot system that enables generating appropriate behaviours through multimodal input processing, bringing episodic memories accordingly to the recognised user, and simulating the emotional states of the robot induced by the interaction with the human partner. In particular, we introduce an LLM-agent frame for social robots, SoR-ReAct, serving as a core component for the interaction module in our system. This design has brought forth the advancement of social robots and aims to increase the quality of human-robot interaction.

------------

`[2405.20279] CV-VAE: A Compatible Video VAE for Latent Generative Video Models <https://arxiv.org/abs/2405.20279>`__ 

::

    Thu, 30 May 2024 17:33:10 GMT
    Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, Ying Shan

Spatio-temporal compression of videos, utilizing networks such as Variational Autoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other video generative models. For instance, many LLM-like video models learn the distribution of discrete tokens derived from 3D VAEs within the VQVAE framework, while most diffusion-based video models capture the distribution of continuous latent extracted by 2D VAEs without quantization. The temporal compression is simply realized by uniform frame sampling which results in unsmooth motion between consecutive frames. Currently, there lacks of a commonly used continuous video (3D) VAE for latent diffusion-based video models in the research community. Moreover, since current diffusion-based approaches are often implemented using pre-trained text-to-image (T2I) models, directly training a video VAE without considering the compatibility with existing T2I models will result in a latent space gap between them, which will take huge computational resources for training to bridge the gap even with the T2I models as initialization. To address this issue, we propose a method for training a video VAE of latent video models, namely CV-VAE, whose latent space is compatible with that of a given image VAE, e.g., image VAE of Stable Diffusion (SD). The compatibility is achieved by the proposed novel latent space regularization, which involves formulating a regularization loss using the image VAE. Benefiting from the latent space compatibility, video models can be trained seamlessly from pre-trained T2I or video models in a truly spatio-temporally compressed latent space, rather than simply sampling video frames at equal intervals. With our CV-VAE, existing video models can generate four times more frames with minimal finetuning. Extensive experiments are conducted to demonstrate the effectiveness of the proposed video VAE.

------------

`[2405.20319] ParSEL: Parameterized Shape Editing with Language <https://arxiv.org/abs/2405.20319>`__ ParSEL:基于语言的参数化形状编辑

::

    Thu, 30 May 2024 17:55:46 GMT
    Aditya Ganeshan, Ryan Y. Huang, Xianghao Xu, R. Kenny Jones, Daniel Ritchie

The ability to edit 3D assets from natural language presents a compelling paradigm to aid in the democratization of 3D content creation. However, while natural language is often effective at communicating general intent, it is poorly suited for specifying precise manipulation. To address this gap, we introduce ParSEL, a system that enables controllable editing of high-quality 3D assets from natural language. Given a segmented 3D mesh and an editing request, ParSEL produces a parameterized editing program. Adjusting the program parameters allows users to explore shape variations with a precise control over the magnitudes of edits. To infer editing programs which align with an input edit request, we leverage the abilities of large-language models (LLMs).
However, while we find that LLMs excel at identifying initial edit operations, they often fail to infer complete editing programs, and produce outputs that violate shape semantics. To overcome this issue, we introduce Analytical Edit Propagation (AEP), an algorithm which extends a seed edit with additional operations until a complete editing program has been formed. Unlike prior methods, AEP searches for analytical editing operations compatible with a range of possible user edits through the integration of computer algebra systems for geometric analysis. Experimentally we demonstrate ParSEL's effectiveness in enabling controllable editing of 3D objects through natural language requests over alternative system designs.

------------

`[2405.19716] Enhancing Large Vision Language Models with Self-Training on Image Comprehension <https://arxiv.org/abs/2405.19716>`__ 基于图像理解自训练的大型视觉语言模型增强

::

    Thu, 30 May 2024 05:53:49 GMT
    Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, James Zou, Kai-Wei Chang, Wei Wang

Large vision language models (LVLMs) integrate large language models (LLMs) with pre-trained vision encoders, thereby activating the perception capability of the model to understand image inputs for different queries and conduct subsequent reasoning. Improving this capability requires high-quality vision-language data, which is costly and labor-intensive to acquire.
Self-training approaches have been effective in single-modal settings to alleviate the need for labeled data by leveraging model's own generation.
However, effective self-training remains a challenge regarding the unique visual perception and reasoning capability of LVLMs. To address this, we introduce Self-Training on Image Comprehension (STIC), which emphasizes a self-training approach specifically for image comprehension. First, the model self-constructs a preference dataset for image descriptions using unlabeled images. Preferred responses are generated through a step-by-step prompt, while dis-preferred responses are generated from either corrupted images or misleading prompts. To further self-improve reasoning on the extracted visual information, we let the model reuse a small portion of existing instruction-tuning data and append its self-generated image descriptions to the prompts. We validate the effectiveness of STIC across seven different benchmarks, demonstrating substantial performance gains of 4.0% on average while using 70% less supervised fine-tuning data than the current method.
Further studies investigate various components of STIC and highlight its potential to leverage vast quantities of unlabeled images for self-training.
Code and data are made publicly available.

------------

`[2405.19732] Two Optimizers Are Better Than One: LLM Catalyst for Enhancing Gradient-Based Optimization <https://arxiv.org/abs/2405.19732>`__ 两个优化器优于一个:用于增强基于梯度的优化的LLM催化剂

::

    Thu, 30 May 2024 06:24:14 GMT
    Zixian Guo, Ming Liu, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo

Learning a skill generally relies on both practical experience by doer and insightful high-level guidance by instructor. Will this strategy also work well for solving complex non-convex optimization problems? Here, a common gradient-based optimizer acts like a disciplined doer, making locally optimal update at each step. Recent methods utilize large language models (LLMs) to optimize solutions for concrete problems by inferring from natural language instructions, akin to a high-level instructor. In this paper, we show that these two optimizers are complementary to each other, suggesting a collaborative optimization approach. The gradient-based optimizer and LLM-based optimizer are combined in an interleaved manner. We instruct LLMs using task descriptions and timely optimization trajectories recorded during gradient-based optimization. Inferred results from LLMs are used as restarting points for the next stage of gradient optimization. By leveraging both the locally rigorous gradient-based optimizer and the high-level deductive LLM-based optimizer, our combined optimization method consistently yields improvements over competitive baseline prompt tuning methods. Our results demonstrate the synergistic effect of conventional gradient-based optimization and the inference ability of LLMs. The code is released at https://github.com/guozix/LLM-catalyst.

------------

`[2402.06782] Debating with More Persuasive LLMs Leads to More Truthful Answers <https://arxiv.org/abs/2402.06782>`__ 与更有说服力的llm进行辩论，会得到更真实的答案

::

    replaced with revised version Thu, 30 May 2024 13:59:34 GMT
    Submission history From: Akbir M Khan Mr [view email]
    [v1] Fri, 9 Feb 2024 21:05:01 UTC (7,563 KB)
    [v2] Thu, 15 Feb 2024 22:09:52 UTC (7,563 KB)
    [v3] Thu, 30 May 2024 13:59:34 UTC (7,579 KB)
    Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rockt\"aschel and Ethan Perez

Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.

------------

`[2402.09764] Aligning Crowd Feedback via Distributional Preference Reward Modeling <https://arxiv.org/abs/2402.09764>`__ 基于分布偏好奖励模型的群体反馈对齐

::

    replaced with revised version Thu, 30 May 2024 15:39:17 GMT
    Submission history From: Dexun Li [view email]
    [v1] Thu, 15 Feb 2024 07:29:43 UTC (1,795 KB)
    [v2] Wed, 21 Feb 2024 07:56:28 UTC (1,794 KB)
    [v3] Thu, 30 May 2024 15:39:17 UTC (6,577 KB)
    Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang, Yong Liu

Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference. However, the conventional reward modelling is predominantly dependent on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in skewed models that reflect the inclinations of these annotators, thereby failing to adequately represent the wider population's expectations. We propose the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with diverse human preferences. To this end, we characterize multiple preferences by a categorical distribution and introduce a Bayesian updater to accommodate shifted or new preferences. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to fine-tune an LLM policy to generate responses favoured by the population. Our experiments show that DPRM significantly enhances the alignment of LLMs with population preference, yielding more accurate, unbiased, and contextually appropriate responses.

------------

`[2402.18496] Language Models Represent Beliefs of Self and Others <https://arxiv.org/abs/2402.18496>`__ 语言模型代表了自我和他人的信念

::

    replaced with revised version Thu, 30 May 2024 12:43:01 GMT
    Submission history From: Wentao Zhu [view email]
    [v1] Wed, 28 Feb 2024 17:25:59 UTC (2,666 KB)
    [v2] Thu, 29 Feb 2024 13:22:17 UTC (2,665 KB)
    [v3] Thu, 30 May 2024 12:43:01 UTC (3,979 KB)
    Wentao Zhu, Zhining Zhang, Yizhou Wang

Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.

------------

`[2404.08706] Game Generation via Large Language Models <https://arxiv.org/abs/2404.08706>`__ 

::

    replaced with revised version Thu, 30 May 2024 03:17:00 GMT
    Submission history From: Chengpeng Hu [view email]
    [v1] Thu, 11 Apr 2024 10:06:05 UTC (764 KB)
    [v2] Thu, 30 May 2024 03:17:00 UTC (764 KB)
    Chengpeng Hu, Yunlong Zhao, Jialin Liu

Recently, the emergence of large language models (LLMs) has unlocked new opportunities for procedural content generation. However, recent attempts mainly focus on level generation for specific games with defined game rules such as Super Mario Bros. and Zelda. This paper investigates the game generation via LLMs. Based on video game description language, this paper proposes an LLM-based framework to generate game rules and levels simultaneously. Experiments demonstrate how the framework works with prompts considering different combinations of context. Our findings extend the current applications of LLMs and offer new insights for generating new games in the area of procedural content generation.

------------

`[2405.17956] Hybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives <https://arxiv.org/abs/2405.17956>`__ 混合偏好优化:用辅助目标扩充直接偏好优化

::

    replaced with revised version Wed, 29 May 2024 20:48:47 GMT
    Submission history From: Anirudhan Badrinath [view email]
    [v1] Tue, 28 May 2024 08:35:48 UTC (344 KB)
    [v2] Wed, 29 May 2024 20:48:47 UTC (344 KB)
    Anirudhan Badrinath, Prabhat Agarwal, Jiajing Xu

For aligning large language models (LLMs), prior work has leveraged reinforcement learning via human feedback (RLHF) or variations of direct preference optimization (DPO). While DPO offers a simpler framework based on maximum likelihood estimation, it compromises on the ability to tune language models to easily maximize non-differentiable and non-binary objectives according to the LLM designer's preferences (e.g., using simpler language or minimizing specific kinds of harmful content). These may neither align with user preferences nor even be able to be captured tractably by binary preference data. To leverage the simplicity and performance of DPO with the generalizability of RL, we propose a hybrid approach between DPO and RLHF. With a simple augmentation to the implicit reward decomposition of DPO, we allow for tuning LLMs to maximize a set of arbitrary auxiliary rewards using offline RL. The proposed method, Hybrid Preference Optimization (HPO), shows the ability to effectively generalize to both user preferences and auxiliary designer objectives, while preserving alignment performance across a range of challenging benchmarks and model sizes.

------------

`[2305.12392] PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs <https://arxiv.org/abs/2305.12392>`__ PiVe:基于迭代验证的提示提高llm基于图的生成能力

::

    replaced with revised version Thu, 30 May 2024 13:23:24 GMT
    Submission history From: Jiuzhou Han [view email]
    [v1] Sun, 21 May 2023 08:11:24 UTC (120 KB)
    [v2] Thu, 8 Feb 2024 04:04:25 UTC (315 KB)
    [v3] Thu, 30 May 2024 13:23:24 UTC (388 KB)
    Jiuzhou Han, Nigel Collier, Wray Buntine, Ehsan Shareghi

Large language models (LLMs) have shown great abilities of solving various natural language tasks in different domains. Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation. We propose a framework, Prompting with Iterative Verification (PiVe), to improve graph-based generative capability of LLMs. We show how a small language model could be trained to act as a verifier module for the output of an LLM~(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions. We also show how the verifier module could apply iterative corrections offline for a more cost-effective solution to the text-to-graph generation task. Experiments on three graph-based datasets show consistent improvement gained via PiVe. Additionally, we create GenWiki-HIQ and highlight that the verifier module can be used as a data augmentation tool to help improve the quality of automatically generated parallel text-graph datasets.

------------

`[2309.08952] Cross-Lingual Knowledge Editing in Large Language Models <https://arxiv.org/abs/2309.08952>`__ 大型语言模型中的跨语言知识编辑

::

    replaced with revised version Thu, 30 May 2024 13:49:47 GMT
    Submission history From: Jiaan Wang [view email]
    [v1] Sat, 16 Sep 2023 11:07:52 UTC (397 KB)
    [v2] Thu, 30 May 2024 13:49:47 UTC (464 KB)
    Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao, Jiarong Xu, Fandong Meng

Knowledge editing aims to change language models' performance on several special cases (i.e., editing scope) by infusing the corresponding expected knowledge into them. With the recent advancements in large language models (LLMs), knowledge editing has been shown as a promising technique to adapt LLMs to new knowledge without retraining from scratch. However, most of the previous studies neglect the multi-lingual nature of some main-stream LLMs (e.g., LLaMA, ChatGPT and GPT-4), and typically focus on monolingual scenarios, where LLMs are edited and evaluated in the same language. As a result, it is still unknown the effect of source language editing on a different target language. In this paper, we aim to figure out this cross-lingual effect in knowledge editing. Specifically, we first collect a large-scale cross-lingual synthetic dataset by translating ZsRE from English to Chinese. Then, we conduct English editing on various knowledge editing methods covering different paradigms, and evaluate their performance in Chinese, and vice versa. To give deeper analyses of the cross-lingual effect, the evaluation includes four aspects, i.e., reliability, generality, locality and portability. Furthermore, we analyze the inconsistent behaviors of the edited models and discuss their specific challenges. Data and codes are available at this https URL

------------

`[2310.08975] ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models <https://arxiv.org/abs/2310.08975>`__ ChatKBQA:基于微调大型语言模型的知识库问答生成-检索框架

::

    replaced with revised version Thu, 30 May 2024 12:39:51 GMT
    Submission history From: Haoran Luo [view email]
    [v1] Fri, 13 Oct 2023 09:45:14 UTC (3,949 KB)
    [v2] Thu, 30 May 2024 12:39:51 UTC (4,683 KB)
    Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei Lin, Yifan Zhu, Luu Anh Tuan

Knowledge Base Question Answering (KBQA) aims to answer natural language questions over large-scale knowledge bases (KBs), which can be summarized into two crucial steps: knowledge retrieval and semantic parsing. However, three core challenges remain: inefficient knowledge retrieval, mistakes of retrieval adversely impacting semantic parsing, and the complexity of previous KBQA methods. To tackle these challenges, we introduce ChatKBQA, a novel and simple generate-then-retrieve KBQA framework, which proposes first generating the logical form with fine-tuned LLMs, then retrieving and replacing entities and relations with an unsupervised retrieval method, to improve both generation and retrieval more directly. Experimental results show that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This work can also be regarded as a new paradigm for combining LLMs with knowledge graphs (KGs) for interpretable and knowledge-required question answering. Our code is publicly available.

------------

`[2311.15316] Sibyl: Sensible Empathetic Dialogue Generation with Visionary Commonsense Knowledge <https://arxiv.org/abs/2311.15316>`__ Sibyl:有远见的常识的明智的共情对话生成

::

    replaced with revised version Thu, 30 May 2024 06:18:20 GMT
    Submission history From: Lanrui Wang [view email]
    [v1] Sun, 26 Nov 2023 14:35:23 UTC (749 KB)
    [v2] Thu, 30 May 2024 06:18:20 UTC (735 KB)
    Lanrui Wang, Jiangnan Li, Chenxu Yang, Zheng Lin, Hongyin Tang, Huan Liu, Xiaolei Huang, Yanan Cao, Jingang Wang, Weiping Wang

Recently, there has been a heightened interest in building chatbots based on Large Language Models (LLMs) to emulate human-like qualities in dialogues, including expressing empathy and offering emotional support. Despite having access to commonsense knowledge to better understand the psychological aspects and causality of dialogue context, even these powerful LLMs struggle to achieve the goals of empathy and emotional support. As current approaches do not adequately anticipate dialogue future, they may mislead language models to ignore complex dialogue goals of empathy and emotional support, resulting in unsupportive responses lacking empathy. To address this issue, we present an innovative framework named Sensible Empathetic Dialogue Generation with Visionary Commonsense Knowledge (Sibyl). Designed to concentrate on the imminent dialogue future, this paradigm directs LLMs toward the implicit requirements of the conversation, aiming to provide more sensible responses. Experimental results demonstrate that incorporating our paradigm for acquiring commonsense knowledge into LLMs comprehensively enhances the quality of their responses.

------------

`[2401.02415] LLaMA Pro: Progressive LLaMA with Block Expansion <https://arxiv.org/abs/2401.02415>`__ 羊驼Pro:带区块扩展的渐进式羊驼

::

    replaced with revised version Thu, 30 May 2024 04:45:34 GMT
    Submission history From: Chengyue Wu [view email]
    [v1] Thu, 4 Jan 2024 18:59:12 UTC (3,730 KB)
    [v2] Thu, 30 May 2024 04:45:34 UTC (3,993 KB)
    Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ying Shan, Ping Luo

Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.

------------

`[2401.06102] Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models <https://arxiv.org/abs/2401.06102>`__ Patchscopes:用于检查语言模型隐藏表示的统一框架

::

    replaced with revised version Thu, 30 May 2024 02:52:08 GMT
    Submission history From: Asma Ghandeharioun [view email]
    [v1] Thu, 11 Jan 2024 18:33:48 UTC (760 KB)
    [v2] Fri, 12 Jan 2024 17:54:18 UTC (760 KB)
    [v3] Thu, 30 May 2024 02:52:08 UTC (825 KB)
    Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, Mor Geva

Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.

------------

`[2402.01349] Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models <https://arxiv.org/abs/2402.01349>`__ 答案之外:对大型语言模型评估的选择题回答合理性述评

::

    replaced with revised version Thu, 30 May 2024 01:57:14 GMT
    Submission history From: Haochun Wang [view email]
    [v1] Fri, 2 Feb 2024 12:07:00 UTC (289 KB)
    [v2] Thu, 30 May 2024 01:57:14 UTC (410 KB)
    Haochun Wang, Sendong Zhao, Zewen Qiang, Nuwa Xi, Bing Qin, Ting Liu

In the field of natural language processing (NLP), Large Language Models (LLMs) have precipitated a paradigm shift, markedly enhancing performance in natural language generation tasks. Despite these advancements, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the utilization of Multiple Choice Question Answering (MCQA) as a benchmark for LLMs has gained considerable traction. This study first investigates the limitations of MCQA as an evaluation method for LLMs and then analyzes the fundamental reason for the limitations of MCQA, that while LLMs may select the correct answers, it is possible that they also recognize other wrong options as correct. Finally, we propose a dataset augmenting method for Multiple-Choice Questions (MCQs), MCQA+, that can more accurately reflect the performance of the model, which underscores the need for more robust evaluation mechanisms in assessing the performance of LLMs.

------------

`[2402.03271] Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models <https://arxiv.org/abs/2402.03271>`__ 思维的不确定性:不确定性感知规划增强了大型语言模型中的信息搜索

::

    replaced with revised version Thu, 30 May 2024 14:03:35 GMT
    Submission history From: Zhiyuan Hu [view email]
    [v1] Mon, 5 Feb 2024 18:28:44 UTC (669 KB)
    [v2] Thu, 30 May 2024 14:03:35 UTC (736 KB)
    Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, Bryan Hooi

In the face of uncertainty, the ability to *seek information* is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an *uncertainty-aware simulation approach* which enables the model to simulate possible future scenarios and how likely they are to occur, 2) *uncertainty-based rewards* motivated by information gain which incentivizes the model to seek information, and 3) a *reward propagation scheme* to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting, and the `20 Questions` game, UoT achieves an average performance improvement of 38.1% in the rate of successful task completion across multiple LLMs compared with direct prompting and also improves efficiency (i.e., the number of questions needed to complete the task). Our code has been released [here](this https URL)

------------

`[2402.10466] Large Language Models as Zero-shot Dialogue State Tracker through Function Calling <https://arxiv.org/abs/2402.10466>`__ 通过函数调用实现大型语言模型的零样本对话状态跟踪

::

    replaced with revised version Thu, 30 May 2024 04:19:54 GMT
    Submission history From: Zekun Li [view email]
    [v1] Fri, 16 Feb 2024 06:13:18 UTC (8,006 KB)
    [v2] Wed, 1 May 2024 17:58:35 UTC (8,088 KB)
    [v3] Thu, 2 May 2024 01:44:08 UTC (8,088 KB)
    [v4] Thu, 30 May 2024 04:19:54 UTC (7,828 KB)
    Zekun Li, Zhiyu Zoey Chen, Mike Ross, Patrick Huber, Seungwhan Moon, Zhaojiang Lin, Xin Luna Dong, Adithya Sagar, Xifeng Yan, Paul A. Crook

Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available at this https URL

------------

`[2402.11505] Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources <https://arxiv.org/abs/2402.11505>`__ 异构任务和客户端资源下大型语言模型的联邦微调

::

    replaced with revised version Thu, 30 May 2024 15:46:10 GMT
    Submission history From: Daoyuan Chen [view email]
    [v1] Sun, 18 Feb 2024 08:32:59 UTC (582 KB)
    [v2] Thu, 30 May 2024 15:46:10 UTC (363 KB)
    Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, Yaliang Li

Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients. This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the ``bucket effect'' in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving thousands of clients performing heterogeneous NLP tasks and client resources, our experiments validate the efficacy of FlexLoRA, with the federated global model achieving consistently better improvement over SOTA FL methods in downstream NLP task performance across various heterogeneous distributions. FlexLoRA's practicality is further underscored by our theoretical analysis and its seamless integration with existing LoRA-based FL methods, offering a path toward cross-device, privacy-preserving federated tuning for LLMs.

------------

`[2402.12786] Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations <https://arxiv.org/abs/2402.12786>`__ 改进大型语言模型，以捕获不同的说话风格并在口语对话中作出正确反应

::

    replaced with revised version Thu, 30 May 2024 09:06:34 GMT
    Submission history From: Guan-Ting Lin [view email]
    [v1] Tue, 20 Feb 2024 07:51:43 UTC (1,889 KB)
    [v2] Thu, 30 May 2024 09:06:34 UTC (1,889 KB)
    Guan-Ting Lin, Cheng-Han Chiang, Hung-yi Lee

In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly. Our goal is to teach the LLM that "even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different". Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses will be different. To teach LLMs to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles. Based on extensive experiments, we show that Spoken-LLM outperforms text-only baselines and prior speech LLMs methods.

------------

`[2402.13494] GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis <https://arxiv.org/abs/2402.13494>`__ GradSafe:基于安全关键梯度分析的llm越狱提示信息检测

::

    replaced with revised version Wed, 29 May 2024 21:45:35 GMT
    Submission history From: Yueqi Xie [view email]
    [v1] Wed, 21 Feb 2024 03:09:21 UTC (3,319 KB)
    [v2] Wed, 29 May 2024 21:45:35 UTC (10,201 KB)
    Yueqi Xie, Minghong Fang, Renjie Pi, Neil Gong

Large Language Models (LLMs) face threats from jailbreak prompts. Existing methods for detecting jailbreak prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects jailbreak prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our method is grounded in a pivotal observation: the gradients of an LLM's loss for jailbreak prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect jailbreak prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its extensive finetuning with a large dataset, in detecting jailbreak prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on ToxicChat and XSTest. The source code is available at this https URL.

------------

`[2402.14700] Unveiling Linguistic Regions in Large Language Models <https://arxiv.org/abs/2402.14700>`__ 大型语言模型中语言区域的揭示

::

    replaced with revised version Thu, 30 May 2024 17:31:46 GMT
    Submission history From: Zhihao Zhang [view email]
    [v1] Thu, 22 Feb 2024 16:56:13 UTC (2,348 KB)
    [v2] Mon, 27 May 2024 06:35:25 UTC (5,223 KB)
    [v3] Thu, 30 May 2024 17:31:46 UTC (5,219 KB)
    Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, Xuanjing Huang

Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependence, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct monolingual regions exist for different languages, and disruption to these specific regions substantially reduces the LLMs' proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common phenomenon observed during further pre-training of LLMs. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence.

------------

`[2402.15159] Machine Unlearning of Pre-trained Large Language Models <https://arxiv.org/abs/2402.15159>`__ 预训练大型语言模型的机器遗忘

::

    replaced with revised version Thu, 30 May 2024 15:44:51 GMT
    Submission history From: Jin Yao [view email]
    [v1] Fri, 23 Feb 2024 07:43:26 UTC (6,971 KB)
    [v2] Tue, 27 Feb 2024 05:23:35 UTC (7,013 KB)
    [v3] Thu, 30 May 2024 15:44:51 UTC (7,033 KB)
    Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, Xiang Yue

This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development.

------------

`[2403.07088] SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation <https://arxiv.org/abs/2403.07088>`__ SPA:面向计算友好的云端和设备上协作Seq2seq个性化生成

::

    replaced with revised version Thu, 30 May 2024 05:21:23 GMT
    Submission history From: Yanming Liu [view email]
    [v1] Mon, 11 Mar 2024 18:26:02 UTC (1,663 KB)
    [v2] Sat, 25 May 2024 11:19:31 UTC (910 KB)
    [v3] Thu, 30 May 2024 05:21:23 UTC (910 KB)
    Yanming Liu, Xinyue Peng, Jiannan Cao, Le Dai, Xingzu Liu, Weihao Liu, Mingbang Wang

Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require substantial memory storage on low-resource devices. More critically, the computational speed on these devices is also severely limited. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and featured personal feature. Further more, SPA provides a framework to keep feature-base parameters on low computational devices while leave the parameters containing general information on the high computational devices.

------------

`[2403.15112] Text clustering with LLM embeddings <https://arxiv.org/abs/2403.15112>`__ 基于LLM嵌入的文本聚类

::

    replaced with revised version Thu, 30 May 2024 15:17:55 GMT
    Submission history From: Nuno Fachada [view email]
    [v1] Fri, 22 Mar 2024 11:08:48 UTC (83 KB)
    [v2] Wed, 29 May 2024 10:16:13 UTC (87 KB)
    [v3] Thu, 30 May 2024 15:17:55 UTC (87 KB)
    Alina Petukhova, Jo\~ao P. Matos-Carvalho, Nuno Fachada

Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. However, the effectiveness of text clustering heavily relies on the choice of textual embeddings and clustering algorithms. We argue that recent advances in large language models (LLMs) can potentially improve this task. In this research, we investigated how different textual embeddings -- particularly those used in LLMs -- and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and model size adjustment. Findings reveal that LLM embeddings excel at capturing subtleties in structured language, while BERT leads the lightweight options in performance. In addition, we observe that increasing model dimensionality and employing summarization techniques do not consistently lead to improvements in clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a complex balance between the need for refined text representation and computational feasibility in text clustering applications. This study extends traditional text clustering frameworks by incorporating embeddings from LLMs, providing a path for improved methodologies, while informing new avenues for future research in various types of textual analysis.

------------

`[2403.17760] Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons <https://arxiv.org/abs/2403.17760>`__ 构造是如此困难，即使是大型语言模型也会因为错误的原因使它们正确

::

    replaced with revised version Wed, 29 May 2024 23:41:37 GMT
    Submission history From: Shijia Zhou [view email]
    [v1] Tue, 26 Mar 2024 14:51:12 UTC (9,009 KB)
    [v2] Wed, 29 May 2024 23:41:37 UTC (9,009 KB)
    Shijia Zhou, Leonie Weissweiler, Taiqi He, Hinrich Sch\"utze, David R. Mortensen, Lori Levin

In this paper, we make a contribution that can be understood from two perspectives: from an NLP perspective, we introduce a small challenge dataset for NLI with large lexical overlap, which minimises the possibility of models discerning entailment solely based on token distinctions, and show that GPT-4 and Llama 2 fail it with strong bias. We then create further challenging sub-tasks in an effort to explain this failure. From a Computational Linguistics perspective, we identify a group of constructions with three classes of adjectives which cannot be distinguished by surface features. This enables us to probe for LLM's understanding of these constructions in various ways, and we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads.

------------

`[2404.12715] Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration <https://arxiv.org/abs/2404.12715>`__ 深度并行协作的异构大型语言模型集成学习

::

    replaced with revised version Thu, 30 May 2024 16:59:56 GMT
    Submission history From: Yichong Huang [view email]
    [v1] Fri, 19 Apr 2024 08:52:22 UTC (7,419 KB)
    [v2] Thu, 30 May 2024 16:59:56 UTC (7,657 KB)
    Yichong Huang, Xiaocheng Feng, Baohang Li, Yang Xiang, Hui Wang, Bing Qin, Ting Liu

Large language models (LLMs) exhibit complementary strengths in various tasks, motivating the research of LLM ensembling. However, existing work focuses on training an extra reward model or fusion model to select or combine all candidate answers, posing a great challenge to the generalization on unseen data distributions. Besides, prior methods use textual responses as communication media, ignoring the valuable information in the internal representations. In this work, we propose a training-free ensemble framework DeePEn, fusing the informative probability distributions yielded by different LLMs at each decoding step. Unfortunately, the vocabulary discrepancy between heterogeneous LLMs directly makes averaging the distributions unfeasible due to the token misalignment. To address this challenge, DeePEn maps the probability distribution of each model from its own probability space to a universal relative space based on the relative representation theory, and performs aggregation. Next, we devise a search-based inverse transformation to transform the aggregated result back to the probability space of one of the ensembling LLMs (main model), in order to determine the next token. We conduct extensive experiments on ensembles of different number of LLMs, ensembles of LLMs with different architectures, and ensembles between the LLM and the specialist model. Experimental results show that (i) DeePEn achieves consistent improvements across six benchmarks covering subject examination, reasoning, and knowledge, (ii) a well-performing specialist model can benefit from a less effective LLM through distribution fusion, and (iii) DeePEn has complementary strengths with other ensemble methods such as voting.

------------

`[2405.11577] A Multi-Perspective Analysis of Memorization in Large Language Models <https://arxiv.org/abs/2405.11577>`__ 大型语言模型记忆的多角度分析

::

    replaced with revised version Thu, 30 May 2024 05:13:19 GMT
    Submission history From: Bowen Chen [view email]
    [v1] Sun, 19 May 2024 15:00:50 UTC (5,855 KB)
    [v2] Mon, 27 May 2024 04:41:02 UTC (6,220 KB)
    [v3] Thu, 30 May 2024 05:13:19 UTC (9,006 KB)
    Bowen Chen, Namgi Han, Yusuke Miyao

Large Language Models (LLMs), trained on massive corpora with billions of parameters, show unprecedented performance in various fields. Though surprised by their excellent performances, researchers also noticed some special behaviors of those LLMs. One of those behaviors is memorization, in which LLMs can generate the same content used to train them. Though previous research has discussed memorization, the memorization of LLMs still lacks explanation, especially the cause of memorization and the dynamics of generating them. In this research, we comprehensively discussed memorization from various perspectives and extended the discussion scope to not only just the memorized content but also less and unmemorized content. Through various studies, we found that: (1) Through experiments, we revealed the relation of memorization between model size, continuation size, and context size. Further, we showed how unmemorized sentences transition to memorized sentences. (2) Through embedding analysis, we showed the distribution and decoding dynamics across model size in embedding space for sentences with different memorization scores. The n-gram statistics analysis presents d (3) An analysis over n-gram and entropy decoding dynamics discovered a boundary effect when the model starts to generate memorized sentences or unmemorized sentences. (4)We trained a Transformer model to predict the memorization of different models, showing that it is possible to predict memorizations by context.

------------

`[2405.15525] Sparse Matrix in Large Language Model Fine-tuning <https://arxiv.org/abs/2405.15525>`__ 大型语言模型微调中的稀疏矩阵

::

    replaced with revised version Thu, 30 May 2024 00:08:51 GMT
    Submission history From: Haoze He [view email]
    [v1] Fri, 24 May 2024 13:12:14 UTC (1,196 KB)
    [v2] Thu, 30 May 2024 00:08:51 UTC (1,196 KB)
    Haoze He, Juncheng Billy Li, Xuan Jiang, Heather Miller

LoRA and its variants have become popular parameter-efficient fine-tuning (PEFT) methods due to their ability to avoid excessive computational costs. However, an accuracy gap often exists between PEFT methods and full fine-tuning (FT), and this gap has yet to be systematically studied. In this work, we introduce a method for selecting sparse sub-matrices that aim to minimize the performance gap between PEFT vs. full fine-tuning (FT) while also reducing both fine-tuning computational cost and memory cost. Our Sparse Matrix Tuning (SMT) method begins by identifying the most significant sub-matrices in the gradient update, updating only these blocks during the fine-tuning process. In our experiments, we demonstrate that SMT consistently surpasses other PEFT baseline (e.g. LoRA and DoRA) in fine-tuning popular large language models such as LLaMA across a broad spectrum of tasks, while reducing the GPU memory footprint by 67% compared to FT. We also examine how the performance of LoRA and DoRA tends to plateau and decline as the number of trainable parameters increases, in contrast, our SMT method does not suffer from such issue.

------------

`[2405.15924] SLIDE: A Framework Integrating Small and Large Language Models for Open-Domain Dialogues Evaluation <https://arxiv.org/abs/2405.15924>`__ SLIDE:用于开放域对话评估的小型和大型语言模型集成框架

::

    replaced with revised version Thu, 30 May 2024 02:13:56 GMT
    Submission history From: Kun Zhao [view email]
    [v1] Fri, 24 May 2024 20:32:49 UTC (352 KB)
    [v2] Wed, 29 May 2024 15:10:27 UTC (445 KB)
    [v3] Thu, 30 May 2024 02:13:56 UTC (8,247 KB)
    Kun Zhao, Bohao Yang, Chen Tang, Chenghua Lin, Liang Zhan

The long-standing one-to-many problem of gold standard responses in open-domain dialogue systems presents challenges for automatic evaluation metrics. Though prior works have demonstrated some success by applying powerful Large Language Models (LLMs), existing approaches still struggle with the one-to-many problem, and exhibit subpar performance in domain-specific scenarios. We assume the commonsense reasoning biases within LLMs may hinder their performance in domainspecific evaluations. To address both issues, we propose a novel framework SLIDE (Small and Large Integrated for Dialogue Evaluation), that leverages both a small, specialised model (SLM), and LLMs for the evaluation of open domain dialogues. Our approach introduces several techniques: (1) Contrastive learning to differentiate between robust and non-robust response embeddings; (2) A novel metric for semantic sensitivity that combines embedding cosine distances with similarity learned through neural networks, and (3) a strategy for incorporating the evaluation results from both the SLM and LLMs. Our empirical results demonstrate that our approach achieves state-of-the-art performance in both the classification and evaluation tasks, and additionally the SLIDE evaluator exhibits better correlation with human judgements. Our code is available at https:// this http URL.

------------

`[2405.16295] Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data <https://arxiv.org/abs/2405.16295>`__ 开源语言模型在医学文本数据汇总中的对比分析

::

    replaced with revised version Wed, 29 May 2024 20:40:32 GMT
    Submission history From: Yuhao Chen [view email]
    [v1] Sat, 25 May 2024 16:16:22 UTC (758 KB)
    [v2] Tue, 28 May 2024 02:22:20 UTC (754 KB)
    [v3] Wed, 29 May 2024 20:40:32 UTC (1,569 KB)
    Yuhao Chen, Zhimu Wang, Bo Wen, Farhana Zulkernine

Unstructured text in medical notes and dialogues contains rich information. Recent advancements in Large Language Models (LLMs) have demonstrated superior performance in question answering and summarization tasks on unstructured text data, outperforming traditional text analysis approaches. However, there is a lack of scientific studies in the literature that methodically evaluate and report on the performance of different LLMs, specifically for domain-specific data such as medical chart notes. We propose an evaluation approach to analyze the performance of open-source LLMs such as Llama2 and Mistral for medical summarization tasks, using GPT-4 as an assessor. Our innovative approach to quantitative evaluation of LLMs can enable quality control, support the selection of effective LLMs for specific tasks, and advance knowledge discovery in digital health.

------------

`[2405.17743] ORLM: Training Large Language Models for Optimization Modeling <https://arxiv.org/abs/2405.17743>`__ ORLM:面向优化建模的大型语言模型训练

::

    replaced with revised version Thu, 30 May 2024 02:12:05 GMT
    Submission history From: Zhengyang Tang [view email]
    [v1] Tue, 28 May 2024 01:55:35 UTC (212 KB)
    [v2] Thu, 30 May 2024 02:12:05 UTC (212 KB)
    Zhengyang Tang, Chenyu Huang, Xin Zheng, Shixi Hu, Zizhuo Wang, Dongdong Ge, Benyou Wang

Large Language Models (LLMs) have emerged as powerful tools for tackling complex Operations Research (OR) problem by providing the capacity in automating optimization modeling. However, current methodologies heavily rely on prompt engineering (e.g., multi-agent cooperation) with proprietary LLMs, raising data privacy concerns that could be prohibitive in industry applications. To tackle this issue, we propose training open-source LLMs for optimization modeling. We identify four critical requirements for the training dataset of OR LLMs, design and implement OR-Instruct, a semi-automated process for creating synthetic data tailored to specific requirements. We also introduce the IndustryOR benchmark, the first industrial benchmark for testing LLMs on solving real-world OR problems. We apply the data from OR-Instruct to various open-source LLMs of 7b size (termed as ORLMs), resulting in a significantly improved capability for optimization modeling. Our best-performing ORLM achieves state-of-the-art performance on the NL4OPT, MAMO, and IndustryOR benchmarks. Our code and data are available at \url{this https URL}.

------------

`[2405.18719] Contextual Position Encoding: Learning to Count What's Important <https://arxiv.org/abs/2405.18719>`__ 上下文位置编码:学习计算什么是重要的

::

    replaced with revised version Thu, 30 May 2024 17:51:53 GMT
    Submission history From: Jason Weston [view email]
    [v1] Wed, 29 May 2024 02:57:15 UTC (2,991 KB)
    [v2] Thu, 30 May 2024 17:51:53 UTC (2,991 KB)
    Olga Golovneva, Tianlu Wang, Jason Weston, Sainbayar Sukhbaatar

The attention mechanism is a critical component of Large Language Models (LLMs) that allows tokens in a sequence to interact with each other, but is order-invariant. Incorporating position encoding (PE) makes it possible to address by position, such as attending to the i-th token. However, current PE methods use token counts to derive position, and thus cannot generalize to higher levels of abstraction, such as attending to the i-th sentence. In this paper, we propose a new position encoding method, Contextual Position Encoding (CoPE), that allows positions to be conditioned on context by incrementing position only on certain tokens determined by the model. This allows more general position addressing such as attending to the $i$-th particular word, noun, or sentence. We show that CoPE can solve the selective copy, counting and Flip-Flop tasks where popular position embeddings fail, and improves perplexity on language modeling and coding tasks.

------------

`[2405.19327] MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series <https://arxiv.org/abs/2405.19327>`__ MAP-Neo:高性能透明双语大型语言模型系列

::

    replaced with revised version Thu, 30 May 2024 17:17:21 GMT
    Submission history From: Tianyu Zheng [view email]
    [v1] Wed, 29 May 2024 17:57:16 UTC (3,776 KB)
    [v2] Thu, 30 May 2024 17:17:21 UTC (3,777 KB)
    Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen

Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.

------------

`[2402.02446] LQER: Low-Rank Quantization Error Reconstruction for LLMs <https://arxiv.org/abs/2402.02446>`__ LQER: llm的低秩量化误差重构

::

    replaced with revised version Thu, 30 May 2024 09:49:47 GMT
    Submission history From: Cheng Zhang [view email]
    [v1] Sun, 4 Feb 2024 10:59:52 UTC (403 KB)
    [v2] Mon, 4 Mar 2024 12:18:47 UTC (396 KB)
    [v3] Thu, 30 May 2024 09:49:47 UTC (766 KB)
    Cheng Zhang, Jianyi Cheng, George A. Constantinides, and Yiren Zhao

Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We open-source our framework at this https URL

------------

`[2402.05140] Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains <https://arxiv.org/abs/2402.05140>`__ Tag-LLM:将通用llm重新用于专业领域

::

    replaced with revised version Thu, 30 May 2024 17:37:06 GMT
    Submission history From: Junhong Shen [view email]
    [v1] Tue, 6 Feb 2024 20:11:54 UTC (1,320 KB)
    [v2] Thu, 30 May 2024 17:37:06 UTC (1,265 KB)
    Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, Nicolo Fusi

Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from task functions, our method enables zero-shot generalization to unseen problems through diverse combinations of the input tags. It also boosts LLM's performance in various specialized domains, such as predicting protein or chemical properties and modeling drug-target interactions, outperforming expert models tailored to these tasks.

------------

`[2403.00131] UNITS: A Unified Multi-Task Time Series Model <https://arxiv.org/abs/2403.00131>`__ 单位:统一的多任务时间序列模型

::

    replaced with revised version Wed, 29 May 2024 18:11:04 GMT
    Submission history From: Shanghua Gao [view email]
    [v1] Thu, 29 Feb 2024 21:25:58 UTC (1,184 KB)
    [v2] Wed, 29 May 2024 18:11:04 UTC (1,094 KB)
    Shanghua Gao, Teddy Koker, Owen Queen, Thomas Hartvigsen, Theodoros Tsiligkaridis, Marinka Zitnik

Advances in time series models are driving a shift from conventional deep learning methods to pre-trained foundational models. While pre-trained transformers and reprogrammed text-based LLMs report state-of-the-art results, the best-performing architectures vary significantly across tasks, and models often have limited scope, such as focusing only on time series forecasting. Models that unify predictive and generative time series tasks under a single framework remain challenging to achieve. We introduce UniTS, a multi-task time series model that uses task tokenization to express predictive and generative tasks within a single model. UniTS leverages a modified transformer block designed to obtain universal time series representations. This design induces transferability from a heterogeneous, multi-domain pre-training dataset-often with diverse dynamic patterns, sampling rates, and temporal scales-to many downstream datasets, which can also be diverse in task specifications and data domains. Across 38 datasets spanning human activity sensors, healthcare, engineering, and finance domains, UniTS model performs favorably against 12 forecasting models, 20 classification models, 18 anomaly detection models, and 16 imputation models, including repurposed text-based LLMs. UniTS demonstrates effective few-shot and prompt learning capabilities when evaluated on new data domains and tasks. In the conventional single-task setting, UniTS outperforms strong task-specialized time series models. The source code and datasets are available at this https URL.

------------

`[2405.14852] PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression <https://arxiv.org/abs/2405.14852>`__ PV-Tuning:极限LLM压缩的超越直通估计

::

    replaced with revised version Thu, 30 May 2024 15:01:49 GMT
    Submission history From: Denis Mazur [view email]
    [v1] Thu, 23 May 2024 17:57:04 UTC (498 KB)
    [v2] Thu, 30 May 2024 15:01:49 UTC (500 KB)
    Vladimir Malinovskii, Denis Mazur, Ivan Ilin, Denis Kuznedelev, Konstantin Burlachenko, Kai Yi, Dan Alistarh, Peter Richtarik

There has been significant interest in "extreme" compression of large language models (LLMs), i.e., to 1-2 bits per parameter, which allows such models to be executed efficiently on resource-constrained devices. Existing work focused on improved one-shot quantization techniques and weight representations; yet, purely post-training approaches are reaching diminishing returns in terms of the accuracy-vs-bit-width trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include fine-tuning (part of) the compressed parameters over a limited amount of calibration data; however, such fine-tuning techniques over compressed weights often make exclusive use of straight-through estimators (STE), whose performance is not well-understood in this setting. In this work, we question the use of STE for extreme LLM compression, showing that it can be sub-optimal, and perform a systematic study of quantization-aware fine-tuning strategies for LLMs. We propose PV-Tuning - a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases. On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral. Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama 2 family models at 2 bits per parameter.

------------

`[2405.14918] AnalogCoder: Analog Circuit Design via Training-Free Code Generation <https://arxiv.org/abs/2405.14918>`__ Analog coder:基于无训练代码生成的模拟电路设计

::

    replaced with revised version Thu, 30 May 2024 16:04:44 GMT
    Submission history From: Yao Lai [view email]
    [v1] Thu, 23 May 2024 17:13:52 UTC (2,660 KB)
    [v2] Thu, 30 May 2024 16:04:44 UTC (2,660 KB)
    Yao Lai, Sungyoung Lee, Guojin Chen, Souradip Poddar, Mengkang Hu, David Z. Pan, Ping Luo

Analog circuit design is a significant task in modern chip technology, focusing on the selection of component types, connectivity, and parameters to ensure proper circuit functionality. Despite advances made by Large Language Models (LLMs) in digital circuit design, the complexity and scarcity of data in analog circuitry pose significant challenges. To mitigate these issues, we introduce AnalogCoder, the first training-free LLM agent for designing analog circuits through Python code generation. Firstly, AnalogCoder incorporates a feedback-enhanced flow with tailored domain-specific prompts, enabling the automated and self-correcting design of analog circuits with a high success rate. Secondly, it proposes a circuit tool library to archive successful designs as reusable modular sub-circuits, simplifying composite circuit creation. Thirdly, extensive experiments on a benchmark designed to cover a wide range of analog circuit tasks show that AnalogCoder outperforms other LLM-based methods. It has successfully designed 20 circuits, 5 more than standard GPT-4o. We believe AnalogCoder can significantly improve the labor-intensive chip design process, enabling non-experts to design analog circuits efficiently.

------------

`[2307.06616] SecureFalcon: Are We There Yet in Automated Software Vulnerability Detection with LLMs? <https://arxiv.org/abs/2307.06616>`__ SecureFalcon:使用LLMs进行自动化软件漏洞检测的工作已经完成了吗?

::

    replaced with revised version Wed, 29 May 2024 18:22:48 GMT
    Submission history From: Mohamed Amine Ferrag [view email]
    [v1] Thu, 13 Jul 2023 08:34:09 UTC (247 KB)
    [v2] Wed, 29 May 2024 18:22:48 UTC (2,793 KB)
    Mohamed Amine Ferrag, Ammar Battah, Norbert Tihanyi, Ridhi Jain, Diana Maimut, Fatima Alwahedi, Thierry Lestable, Narinderjit Singh Thandi, Abdechakour Mechri, Merouane Debbah, Lucas C. Cordeiro

Software vulnerabilities can cause numerous problems, including crashes, data loss, and security breaches. These issues greatly compromise quality and can negatively impact the market adoption of software applications and systems. Traditional bug-fixing methods, such as static analysis, often produce false positives. While bounded model checking, a form of Formal Verification (FV), can provide more accurate outcomes compared to static analyzers, it demands substantial resources and significantly hinders developer productivity. Can Machine Learning (ML) achieve accuracy comparable to FV methods and be used in popular instant code completion frameworks in near real-time? In this paper, we introduce SecureFalcon, an innovative model architecture with only 121 million parameters derived from the Falcon-40B model and explicitly tailored for classifying software vulnerabilities. To achieve the best performance, we trained our model using two datasets, namely the FormAI dataset and the FalconVulnDB. The FalconVulnDB is a combination of recent public datasets, namely the SySeVR framework, Draper VDISC, Bigvul, Diversevul, SARD Juliet, and ReVeal datasets. These datasets contain the top 25 most dangerous software weaknesses, such as CWE-119, CWE-120, CWE-476, CWE-122, CWE-190, CWE-121, CWE-78, CWE-787, CWE-20, and CWE-762. SecureFalcon achieves 94% accuracy in binary classification and up to 92% in multiclassification, with instant CPU inference times. It outperforms existing models such as BERT, RoBERTa, CodeBERT, and traditional ML algorithms, promising to push the boundaries of software vulnerability detection and instant code completion frameworks.

------------

`[2312.12728] Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy <https://arxiv.org/abs/2312.12728>`__ Lookahead:具有无损生成精度的大型语言模型推理加速框架

::

    replaced with revised version Thu, 30 May 2024 11:25:08 GMT
    Submission history From: Yao Zhao [view email]
    [v1] Wed, 20 Dec 2023 02:55:15 UTC (259 KB)
    [v2] Thu, 4 Jan 2024 06:33:52 UTC (259 KB)
    [v3] Thu, 30 May 2024 11:25:08 UTC (195 KB)
    Yao Zhao, Zhitian Xie, Chen Liang, Chenyi Zhuang, Jinjie Gu

As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.
Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our LLM-based scenarios, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this process, our framework, named \textit{lookahead}, introduces a \textit{multi-branch} strategy. Instead of generating a single token at a time, we propose a Trie-based retrieval and verification mechanism to be able to accept several tokens at a forward step. Our strategy offers two distinct advantages: (1) it guarantees absolute correctness of the output, avoiding any approximation algorithms, and (2) the worst-case performance of our approach is equivalent to the conventional process. We conduct extensive experiments to demonstrate the significant improvements achieved by applying our inference acceleration framework. Our framework is widely deployed in Alipay since April 2023, and obtain remarkable 2.66x to 6.26x speedup. Our code is available at this https URL.

------------

`[2401.17981] Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study <https://arxiv.org/abs/2401.17981>`__ 用视觉检测模型增强多模态大型语言模型:实证研究

::

    replaced with revised version Thu, 30 May 2024 15:09:49 GMT
    Submission history From: Daoyuan Chen [view email]
    [v1] Wed, 31 Jan 2024 16:38:32 UTC (2,381 KB)
    [v2] Thu, 30 May 2024 15:09:49 UTC (3,097 KB)
    Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen

Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition (OCR) models to improve fine-grained understanding and reduce hallucination in responses. We investigate the embedding-based infusion of textual detection information, the impact of such infusion on MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic and extensive experiments with representative models such as LLaVA-1.5, DINO, PaddleOCRv2, and Grounding DINO, revealing that our simple yet general approach not only refines MLLMs' performance in fine-grained visual tasks but also maintains their original strengths. Notably, the enhanced LLaVA-1.5 outperforms its original 7B/13B models on all 10 benchmarks, achieving an improvement of up to 12.5% on the normalized average score. We release our codes to facilitate further exploration into the fine-grained multimodal capabilities of MLLMs.

------------

`[2403.15401] Large Language Model for Mental Health: A Systematic Review <https://arxiv.org/abs/2403.15401>`__ 心理健康的大型语言模型:系统回顾

::

    replaced with revised version Wed, 29 May 2024 21:55:17 GMT
    Submission history From: Zhijun Guo [view email]
    [v1] Mon, 19 Feb 2024 17:58:41 UTC (855 KB)
    [v2] Wed, 29 May 2024 21:55:17 UTC (1,212 KB)
    Zhijun Guo, Alvina Lai, Johan Hilge Thygesen, Joseph Farrington, Thomas Keen and Kezhi Li

Large language models (LLMs) have attracted significant attention for potential applications in digital health, while their application in mental health is subject to ongoing debate. This systematic review aims to evaluate the usage of LLMs in mental health, focusing on their strengths and limitations in early screening, digital interventions, and clinical applications. Adhering to PRISMA guidelines, we searched PubMed, IEEE Xplore, Scopus, and the JMIR using keywords: 'mental health OR mental illness OR mental disorder OR psychiatry' AND 'large language models'. We included articles published between January 1, 2017, and December 31, 2023, excluding non-English articles. 30 articles were evaluated, which included research on mental illness and suicidal ideation detection through text (n=12), usage of LLMs for mental health conversational agents (CAs) (n=5), and other applications and evaluations of LLMs in mental health (n=13). LLMs exhibit substantial effectiveness in detecting mental health issues and providing accessible, de-stigmatized eHealth services. However, the current risks associated with the clinical use might surpass their benefits. The study identifies several significant issues: the lack of multilingual datasets annotated by experts, concerns about the accuracy and reliability of the content generated, challenges in interpretability due to the 'black box' nature of LLMs, and persistent ethical dilemmas. These include the lack of a clear ethical framework, concerns about data privacy, and the potential for over-reliance on LLMs by both therapists and patients, which could compromise traditional medical practice. Despite these issues, the rapid development of LLMs underscores their potential as new clinical aids, emphasizing the need for continued research and development in this area.

------------

`[2404.02637] Vocabulary Attack to Hijack Large Language Model Applications <https://arxiv.org/abs/2404.02637>`__ 劫持大型语言模型应用程序的词汇攻击

::

    replaced with revised version Thu, 30 May 2024 06:28:31 GMT
    Submission history From: Christoph Neumann [view email]
    [v1] Wed, 3 Apr 2024 10:54:07 UTC (26 KB)
    [v2] Thu, 30 May 2024 06:28:31 UTC (27 KB)
    Patrick Levi and Christoph P. Neumann

The fast advancements in Large Language Models (LLMs) are driving an increasing number of applications. Together with the growing number of users, we also see an increasing number of attackers who try to outsmart these systems. They want the model to reveal confidential information, specific false information, or offensive behavior. To this end, they manipulate their instructions for the LLM by inserting separators or rephrasing them systematically until they reach their goal. Our approach is different. It inserts words from the model vocabulary. We find these words using an optimization procedure and embeddings from another LLM (attacker LLM). We prove our approach by goal hijacking two popular open-source LLMs from the Llama2 and the Flan-T5 families, respectively. We present two main findings. First, our approach creates inconspicuous instructions and therefore it is hard to detect. For many attack cases, we find that even a single word insertion is sufficient. Second, we demonstrate that we can conduct our attack using a different model than the target model to conduct our attack with.

------------

`[2405.06289] Look Once to Hear: Target Speech Hearing with Noisy Examples <https://arxiv.org/abs/2405.06289>`__ 一看一听:带噪声的目标语音听力

::

    replaced with revised version Wed, 29 May 2024 19:00:39 GMT
    Submission history From: Bandhav Veluri [view email]
    [v1] Fri, 10 May 2024 07:44:18 UTC (24,321 KB)
    [v2] Mon, 13 May 2024 17:05:26 UTC (24,321 KB)
    [v3] Wed, 29 May 2024 19:00:39 UTC (24,321 KB)
    Bandhav Veluri, Malek Itani, Tuochao Chen, Takuya Yoshioka, Shyamnath Gollakota

In crowded settings, the human brain can focus on speech from a target speaker, given prior knowledge of how they sound. We introduce a novel intelligent hearable system that achieves this capability, enabling target speech hearing to ignore all interfering speech and noise, but the target speaker. A naive approach is to require a clean speech example to enroll the target speaker. This is however not well aligned with the hearable application domain since obtaining a clean example is challenging in real world scenarios, creating a unique user interface problem. We present the first enrollment interface where the wearer looks at the target speaker for a few seconds to capture a single, short, highly noisy, binaural example of the target speaker. This noisy example is used for enrollment and subsequent speech extraction in the presence of interfering speakers and noise. Our system achieves a signal quality improvement of 7.01 dB using less than 5 seconds of noisy enrollment audio and can process 8 ms of audio chunks in 6.24 ms on an embedded CPU. Our user studies demonstrate generalization to real-world static and mobile speakers in previously unseen indoor and outdoor multipath environments. Finally, our enrollment interface for noisy examples does not cause performance degradation compared to clean examples, while being convenient and user-friendly. Taking a step back, this paper takes an important step towards enhancing the human auditory perception with artificial intelligence. We provide code and data at: this https URL.

------------

`[2405.16133] Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting <https://arxiv.org/abs/2405.16133>`__ 揭示llm生成的代码:基于代码重写的零样本合成代码检测器

::

    replaced with revised version Thu, 30 May 2024 02:12:47 GMT
    Submission history From: Tong Ye [view email]
    [v1] Sat, 25 May 2024 08:57:28 UTC (1,779 KB)
    [v2] Thu, 30 May 2024 02:12:47 UTC (1,788 KB)
    Tong Ye, Yangkai Du, Tengfei Ma, Lingfei Wu, Xuhong Zhang, Shouling Ji, Wenhai Wang

Large Language Models (LLMs) have exhibited remarkable proficiency in generating code. However, the misuse of LLM-generated (Synthetic) code has prompted concerns within both educational and industrial domains, highlighting the imperative need for the development of synthetic code detectors. Existing methods for detecting LLM-generated content are primarily tailored for general text and often struggle with code content due to the distinct grammatical structure of programming languages and massive "low-entropy" tokens. Building upon this, our work proposes a novel zero-shot synthetic code detector based on the similarity between the code and its rewritten variants. Our method relies on the intuition that the differences between the LLM-rewritten and original codes tend to be smaller when the original code is synthetic. We utilize self-supervised contrastive learning to train a code similarity model and assess our approach on two synthetic code detection benchmarks. Our results demonstrate a notable enhancement over existing synthetic content detectors designed for general texts, with an improvement of 20.5% in the APPS benchmark and 29.1% in the MBPP benchmark.

------------

`[2405.17503] Code Repair with LLMs gives an Exploration-Exploitation Tradeoff <https://arxiv.org/abs/2405.17503>`__ 使用llm进行代码修复提供了探索-利用的权衡

::

    replaced with revised version Thu, 30 May 2024 15:20:19 GMT
    Submission history From: Hao Tang [view email]
    [v1] Sun, 26 May 2024 04:00:30 UTC (2,301 KB)
    [v2] Thu, 30 May 2024 15:20:19 UTC (2,301 KB)
    Hao Tang, Keya Hu, Jin Peng Zhou, Sicheng Zhong, Wei-Long Zheng, Xujie Si, Kevin Ellis

Iteratively improving and repairing source code with large language models (LLMs), known as refinement, has emerged as a popular way of generating programs that would be too complex to construct in one shot. Given a bank of test cases, together with a candidate program, an LLM can improve that program by being prompted with failed test cases. But it remains an open question how to best iteratively refine code, with prior work employing simple greedy or breadth-first strategies. We show here that refinement exposes an explore-exploit tradeoff: exploit by refining the program that passes the most test cases, or explore by refining a lesser considered program. We frame this as an arm-acquiring bandit problem, which we solve with Thompson Sampling. The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls.

------------

`[2310.07968] Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation <https://arxiv.org/abs/2310.07968>`__ 思考、行动和提问:开放世界交互式个性化机器人导航

::

    replaced with revised version Wed, 29 May 2024 21:06:18 GMT
    Submission history From: Yinpei Dai [view email]
    [v1] Thu, 12 Oct 2023 01:17:56 UTC (3,325 KB)
    [v2] Sun, 25 Feb 2024 04:47:50 UTC (3,490 KB)
    [v3] Tue, 19 Mar 2024 01:32:19 UTC (3,493 KB)
    [v4] Wed, 29 May 2024 21:06:18 UTC (3,493 KB)
    Yinpei Dai, Run Peng, Sikai Li, Joyce Chai

Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION), which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion and the efficiency of navigation and interaction remains challenging for all methods. We further provide more findings on the impact of diverse user feedback forms on the agents' performance. Code is available at this https URL.

------------

`[2405.12107] Imp: Highly Capable Large Multimodal Models for Mobile Devices <https://arxiv.org/abs/2405.12107>`__ Imp:面向移动设备的高性能大型多模态模型

::

    replaced with revised version Thu, 30 May 2024 02:47:10 GMT
    Submission history From: Zhou Yu [view email]
    [v1] Mon, 20 May 2024 15:23:19 UTC (15,647 KB)
    [v2] Thu, 30 May 2024 02:47:10 UTC (15,647 KB)
    Zhenwei Shao, Zhou Yu, Jun Yu, Xuecheng Ouyang, Lihao Zheng, Zhenbiao Gai, Mingyang Wang, Jiajun Ding

By harnessing the capabilities of large language models (LLMs), recent large multimodal models (LMMs) have shown remarkable versatility in open-world multimodal understanding. Nevertheless, they are usually parameter-heavy and computation-intensive, thus hindering their applicability in resource-constrained scenarios. To this end, several lightweight LMMs have been proposed successively to maximize the capabilities under constrained scale (e.g., 3B). Despite the encouraging results achieved by these methods, most of them only focus on one or two aspects of the design space, and the key design choices that influence model capability have not yet been thoroughly investigated. In this paper, we conduct a systematic study for lightweight LMMs from the aspects of model architecture, training strategy, and training data. Based on our findings, we obtain Imp -- a family of highly capable LMMs at the 2B-4B scales. Notably, our Imp-3B model steadily outperforms all the existing lightweight LMMs of similar size, and even surpasses the state-of-the-art LMMs at the 13B scale. With low-bit quantization and resolution reduction techniques, our Imp model can be deployed on a Qualcomm Snapdragon 8Gen3 mobile chip with a high inference speed of about 13 tokens/s.

------------

`[2306.08121] Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations <https://arxiv.org/abs/2306.08121>`__ 语义IDs的更好泛化:推荐排序的案例研究

::

    replaced with revised version Thu, 30 May 2024 05:53:39 GMT
    Submission history From: Xinyang Yi [view email]
    [v1] Tue, 13 Jun 2023 20:34:15 UTC (5,741 KB)
    [v2] Thu, 30 May 2024 05:53:39 UTC (10,797 KB)
    Anima Singh, Trung Vu, Nikhil Mehta, Raghunandan Keshavan, Maheswaran Sathiamoorthy, Yilin Zheng, Lichan Hong, Lukasz Heldt, Li Wei, Devansh Tandon, Ed H. Chi, Xinyang Yi

Randomly-hashed item ids are used ubiquitously in recommendation models. However, the learned representations from random hashing prevents generalization across similar items, causing problems of learning unseen and long-tail items, especially when item corpus is large, power-law distributed, and evolving dynamically. In this paper, we propose using content-derived features as a replacement for random ids. We show that simply replacing ID features with content-based embeddings can cause a drop in quality due to reduced memorization capability. To strike a good balance of memorization and generalization, we propose to use Semantic IDs -- a compact discrete item representation learned from frozen content embeddings using RQ-VAE that captures the hierarchy of concepts in items -- as a replacement for random item ids. Similar to content embeddings, the compactness of Semantic IDs poses a problem of easy adaption in recommendation models. We propose novel methods for adapting Semantic IDs in industry-scale ranking models, through hashing sub-pieces of of the Semantic-ID sequences. In particular, we find that the SentencePiece model that is commonly used in LLM tokenization outperforms manually crafted pieces such as N-grams. To the end, we evaluate our approaches in a real-world ranking model for YouTube recommendations. Our experiments demonstrate that Semantic IDs can replace the direct use of video IDs by improving the generalization ability on new and long-tail item slices without sacrificing overall model quality.

------------

`[2405.01964] Understanding LLMs Requires More Than Statistical Generalization <https://arxiv.org/abs/2405.01964>`__ 理解llm需要的不仅仅是统计泛化

::

    replaced with revised version Wed, 29 May 2024 18:22:26 GMT
    Submission history From: Patrik Reizinger [view email]
    [v1] Fri, 3 May 2024 09:41:39 UTC (152 KB)
    [v2] Wed, 29 May 2024 18:22:26 UTC (154 KB)
    Patrik Reizinger, Szilvia Ujv\'ary, Anna M\'esz\'aros, Anna Kerekes, Wieland Brendel, Ferenc Husz\'ar

The last decade has seen blossoming research in deep learning theory attempting to answer, "Why does deep learning generalize?" A powerful shift in perspective precipitated this progress: the study of overparametrized models in the interpolation regime. In this paper, we argue that another perspective shift is due, since some of the desirable qualities of LLMs are not a consequence of good statistical generalization and require a separate theoretical explanation. Our core argument relies on the observation that AR probabilistic models are inherently non-identifiable: models zero or near-zero KL divergence apart -- thus, equivalent test loss -- can exhibit markedly different behaviors. We support our position with mathematical examples and empirical observations, illustrating why non-identifiability has practical relevance through three case studies: (1) the non-identifiability of zero-shot rule extrapolation; (2) the approximate non-identifiability of in-context learning; and (3) the non-identifiability of fine-tunability. We review promising research directions focusing on LLM-relevant generalization measures, transferability, and inductive biases.

------------

-----------
Index (103)
-----------

`[2405.19544] One-Shot Safety Alignment for Large Language Models via Optimal Dualization <https://arxiv.org/abs/2405.19544>`__ 基于最优对偶的大型语言模型单次安全对齐

`[2405.19561] Quo Vadis ChatGPT? From Large Language Models to Large Knowledge Models <https://arxiv.org/abs/2405.19561>`__ Quo Vadis ChatGPT?从大型语言模型到大型知识模型

`[2405.19616] Easy Problems That LLMs Get Wrong <https://arxiv.org/abs/2405.19616>`__ llm会出错的简单问题

`[2405.19686] Knowledge Graph Tuning: Real-time Large Language Model Personalization based on Human Feedback <https://arxiv.org/abs/2405.19686>`__ 知识图谱调优:基于人工反馈的实时大规模语言模型个性化

`[2405.19694] Grade Like a Human: Rethinking Automated Assessment with Large Language Models <https://arxiv.org/abs/2405.19694>`__ 像人一样评分:用大型语言模型重新思考自动评估

`[2405.19850] Deciphering Human Mobility: Inferring Semantics of Trajectories with Large Language Models <https://arxiv.org/abs/2405.19850>`__ 解读人类移动:基于大型语言模型推断轨迹语义

`[2405.19877] KNOW: A Real-World Ontology for Knowledge Capture with Large Language Models <https://arxiv.org/abs/2405.19877>`__ KNOW:使用大型语言模型进行知识捕获的真实世界本体

`[2405.19946] Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf <https://arxiv.org/abs/2405.19946>`__

`[2405.20213] PostDoc: Generating Poster from a Long Multimodal Document Using Deep Submodular Optimization <https://arxiv.org/abs/2405.20213>`__ 博士后:使用深度次模优化从长多模态文档生成海报

`[2405.20234] Context Injection Attacks on Large Language Models <https://arxiv.org/abs/2405.20234>`__ 针对大型语言模型的上下文注入攻击

`[2405.19433] Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals <https://arxiv.org/abs/2405.19433>`__ 超越共识:诊断理论基础基于语言信息反事实的自动作文评分方法的对齐

`[2405.19487] A Full-duplex Speech Dialogue Scheme Based On Large Language Models <https://arxiv.org/abs/2405.19487>`__ 基于大型语言模型的全双工语音对话方案

`[2405.19563] Unlearning Climate Misinformation in Large Language Models <https://arxiv.org/abs/2405.19563>`__ 大型语言模型中气候假信息的遗忘

`[2405.19648] Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach <https://arxiv.org/abs/2405.19648>`__ 大型语言模型生成中的幻觉检测:标记概率方法

`[2405.19660] PATIENT-{\Psi}: Using Large Language Models to Simulate Patients for Training Mental Health Professionals <https://arxiv.org/abs/2405.19660>`__ PATIENT-{\Psi}:使用大型语言模型模拟患者以培训心理健康专业人员

`[2405.19740] PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations <https://arxiv.org/abs/2405.19740>`__ PertEval:用知识不变的扰动揭示llm的真实知识能力

`[2405.19744] X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions <https://arxiv.org/abs/2405.19744>`__ X-Instruction:基于自策划跨语言指令的低资源语言模型对齐

`[2405.19763] Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding <https://arxiv.org/abs/2405.19763>`__ 基于标签敏感奖励的自然语言理解强化学习

`[2405.19787] From Symbolic Tasks to Code Generation: Diversification Yields Better Task Performers <https://arxiv.org/abs/2405.19787>`__ 从符号任务到代码生成:多样化产生更好的任务执行者

`[2405.19793] PDDLEGO: Iterative Planning in Textual Environments <https://arxiv.org/abs/2405.19793>`__ PDDLEGO:文本环境中的迭代规划

`[2405.19795] SLM as Guardian: Pioneering AI Safety with Small Language Models <https://arxiv.org/abs/2405.19795>`__ SLM守护:用小型语言模型引领人工智能安全

`[2405.19799] Unsupervised Mutual Learning of Dialogue Discourse Parsing and Topic Segmentation <https://arxiv.org/abs/2405.19799>`__ 对话篇章分析与话题分割的无监督互学习

`[2405.19846] Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model <https://arxiv.org/abs/2405.19846>`__ Quest:面向大型语言模型长上下文扩展的以查询为中心的数据合成方法

`[2405.20079] Student Answer Forecasting: Transformer-Driven Answer Choice Prediction for Language Learning <https://arxiv.org/abs/2405.20079>`__ 学生答案预测:面向语言学习的transformer驱动答案选择预测

`[2405.20089] The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities <https://arxiv.org/abs/2405.20089>`__ 微调悖论:在不牺牲LLM能力的情况下提高翻译质量

`[2405.20092] Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation <https://arxiv.org/abs/2405.20092>`__ 分而治之达到了共识:在代码生成中释放函数的强大功能

`[2405.20175] InstructionCP: A fast approach to transfer Large Language Models into target language <https://arxiv.org/abs/2405.20175>`__ InstructionCP:一种大规模语言模型到目标语言的快速转换方法

`[2405.20179] Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning CodeLLMs <https://arxiv.org/abs/2405.20179>`__ robot - instruct:面向codellm微调的模拟器增强指令对齐

`[2405.20192] TAIA: Large Language Models are Out-of-Distribution Data Learners <https://arxiv.org/abs/2405.20192>`__ TAIA:大型语言模型是分布外的数据学习者

`[2405.20215] TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models <https://arxiv.org/abs/2405.20215>`__ TS-Align:面向大规模语言模型可扩展迭代微调的师生协作框架

`[2405.20253] Evaluating Large Language Model Biases in Persona-Steered Generation <https://arxiv.org/abs/2405.20253>`__ 角色引导生成中的大型语言模型偏差评估

`[2405.20285] Who Writes the Review, Human or AI? <https://arxiv.org/abs/2405.20285>`__ 谁写评论，人类还是人工智能?

`[2405.20304] Group Robust Preference Optimization in Reward-free RLHF <https://arxiv.org/abs/2405.20304>`__

`[2405.20315] ANAH: Analytical Annotation of Hallucinations in Large Language Models <https://arxiv.org/abs/2405.20315>`__ ANAH:大型语言模型中幻觉的分析注释

`[2405.20335] Xwin-LM: Strong and Scalable Alignment Practice for LLMs <https://arxiv.org/abs/2405.20335>`__ Xwin-LM:强大且可扩展的llm校准实践

`[2405.19534] Preference Learning Algorithms Do Not Learn Preference Rankings <https://arxiv.org/abs/2405.19534>`__ 偏好学习算法不学习偏好排序

`[2405.19550] Stress-Testing Capability Elicitation With Password-Locked Models <https://arxiv.org/abs/2405.19550>`__ 密码锁定模型的压力测试能力诱导

`[2405.19653] SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems <https://arxiv.org/abs/2405.19653>`__ SysCaps:复杂系统仿真代理的语言接口

`[2405.19807] MetaCURL: Non-stationary Concave Utility Reinforcement Learning <https://arxiv.org/abs/2405.19807>`__ MetaCURL:非平稳凹效用强化学习

`[2405.19883] From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems <https://arxiv.org/abs/2405.19883>`__ 从言语到行动:揭示llm驱动的自主系统的理论基础

`[2405.20003] Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities <https://arxiv.org/abs/2405.20003>`__ 核语言熵:语义相似度对llm的细粒度不确定性量化

`[2405.20313] Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation <https://arxiv.org/abs/2405.20313>`__ 基于序列增强SE(3)流匹配的条件蛋白主干生成

`[2405.19358] Robustifying Safety-Aligned Large Language Models through Clean Data Curation <https://arxiv.org/abs/2405.19358>`__ 通过干净数据管理对安全对齐的大型语言模型的鲁棒性

`[2405.19360] ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users <https://arxiv.org/abs/2405.19360>`__ ART:文本到图像模型的自动red - teamaming，以保护良性用户

`[2405.19366] ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with LLM-Enhanced Cardiological Text <https://arxiv.org/abs/2405.19366>`__ ECG语义集成器(ESI):基于llm增强心脏病文本预训练的ECG基础模型

`[2405.19495] Qiskit Code Assistant: Training LLMs for generating Quantum Computing Code <https://arxiv.org/abs/2405.19495>`__ Qiskit Code Assistant:训练llm生成量子计算代码

`[2405.19581] Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases <https://arxiv.org/abs/2405.19581>`__ 源代码基础模型是可迁移的二进制分析知识库

`[2405.19677] Large Language Model Watermark Stealing With Mixed Integer Programming <https://arxiv.org/abs/2405.19677>`__ 基于混合整数规划的大型语言模型水印窃取

`[2405.19783] Instruction-Guided Visual Masking <https://arxiv.org/abs/2405.19783>`__ 指令引导的视觉掩蔽

`[2405.20081] NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models <https://arxiv.org/abs/2405.20081>`__ NoiseBoost:用噪声扰动缓解多模态大型语言模型的幻觉

`[2405.20132] LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics <https://arxiv.org/abs/2405.20132>`__ LLaMEA:自动生成元启发式的大型语言模型进化算法

`[2405.20189] Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory <https://arxiv.org/abs/2405.20189>`__ Nadine: llm驱动的智能社交机器人，具有情感能力和类似人类的记忆

`[2405.20279] CV-VAE: A Compatible Video VAE for Latent Generative Video Models <https://arxiv.org/abs/2405.20279>`__

`[2405.20319] ParSEL: Parameterized Shape Editing with Language <https://arxiv.org/abs/2405.20319>`__ ParSEL:基于语言的参数化形状编辑

`[2405.19716] Enhancing Large Vision Language Models with Self-Training on Image Comprehension <https://arxiv.org/abs/2405.19716>`__ 基于图像理解自训练的大型视觉语言模型增强

`[2405.19732] Two Optimizers Are Better Than One: LLM Catalyst for Enhancing Gradient-Based Optimization <https://arxiv.org/abs/2405.19732>`__ 两个优化器优于一个:用于增强基于梯度的优化的LLM催化剂

`[2402.06782] Debating with More Persuasive LLMs Leads to More Truthful Answers <https://arxiv.org/abs/2402.06782>`__ 与更有说服力的llm进行辩论，会得到更真实的答案

`[2402.09764] Aligning Crowd Feedback via Distributional Preference Reward Modeling <https://arxiv.org/abs/2402.09764>`__ 基于分布偏好奖励模型的群体反馈对齐

`[2402.18496] Language Models Represent Beliefs of Self and Others <https://arxiv.org/abs/2402.18496>`__ 语言模型代表了自我和他人的信念

`[2404.08706] Game Generation via Large Language Models <https://arxiv.org/abs/2404.08706>`__

`[2405.17956] Hybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives <https://arxiv.org/abs/2405.17956>`__ 混合偏好优化:用辅助目标扩充直接偏好优化

`[2305.12392] PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs <https://arxiv.org/abs/2305.12392>`__ PiVe:基于迭代验证的提示提高llm基于图的生成能力

`[2309.08952] Cross-Lingual Knowledge Editing in Large Language Models <https://arxiv.org/abs/2309.08952>`__ 大型语言模型中的跨语言知识编辑

`[2310.08975] ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models <https://arxiv.org/abs/2310.08975>`__ ChatKBQA:基于微调大型语言模型的知识库问答生成-检索框架

`[2311.15316] Sibyl: Sensible Empathetic Dialogue Generation with Visionary Commonsense Knowledge <https://arxiv.org/abs/2311.15316>`__ Sibyl:有远见的常识的明智的共情对话生成

`[2401.02415] LLaMA Pro: Progressive LLaMA with Block Expansion <https://arxiv.org/abs/2401.02415>`__ 羊驼Pro:带区块扩展的渐进式羊驼

`[2401.06102] Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models <https://arxiv.org/abs/2401.06102>`__ Patchscopes:用于检查语言模型隐藏表示的统一框架

`[2402.01349] Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models <https://arxiv.org/abs/2402.01349>`__ 答案之外:对大型语言模型评估的选择题回答合理性述评

`[2402.03271] Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models <https://arxiv.org/abs/2402.03271>`__ 思维的不确定性:不确定性感知规划增强了大型语言模型中的信息搜索

`[2402.10466] Large Language Models as Zero-shot Dialogue State Tracker through Function Calling <https://arxiv.org/abs/2402.10466>`__ 通过函数调用实现大型语言模型的零样本对话状态跟踪

`[2402.11505] Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources <https://arxiv.org/abs/2402.11505>`__ 异构任务和客户端资源下大型语言模型的联邦微调

`[2402.12786] Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations <https://arxiv.org/abs/2402.12786>`__ 改进大型语言模型，以捕获不同的说话风格并在口语对话中作出正确反应

`[2402.13494] GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis <https://arxiv.org/abs/2402.13494>`__ GradSafe:基于安全关键梯度分析的llm越狱提示信息检测

`[2402.14700] Unveiling Linguistic Regions in Large Language Models <https://arxiv.org/abs/2402.14700>`__ 大型语言模型中语言区域的揭示

`[2402.15159] Machine Unlearning of Pre-trained Large Language Models <https://arxiv.org/abs/2402.15159>`__ 预训练大型语言模型的机器遗忘

`[2403.07088] SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation <https://arxiv.org/abs/2403.07088>`__ SPA:面向计算友好的云端和设备上协作Seq2seq个性化生成

`[2403.15112] Text clustering with LLM embeddings <https://arxiv.org/abs/2403.15112>`__ 基于LLM嵌入的文本聚类

`[2403.17760] Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons <https://arxiv.org/abs/2403.17760>`__ 构造是如此困难，即使是大型语言模型也会因为错误的原因使它们正确

`[2404.12715] Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration <https://arxiv.org/abs/2404.12715>`__ 深度并行协作的异构大型语言模型集成学习

`[2405.11577] A Multi-Perspective Analysis of Memorization in Large Language Models <https://arxiv.org/abs/2405.11577>`__ 大型语言模型记忆的多角度分析

`[2405.15525] Sparse Matrix in Large Language Model Fine-tuning <https://arxiv.org/abs/2405.15525>`__ 大型语言模型微调中的稀疏矩阵

`[2405.15924] SLIDE: A Framework Integrating Small and Large Language Models for Open-Domain Dialogues Evaluation <https://arxiv.org/abs/2405.15924>`__ SLIDE:用于开放域对话评估的小型和大型语言模型集成框架

`[2405.16295] Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data <https://arxiv.org/abs/2405.16295>`__ 开源语言模型在医学文本数据汇总中的对比分析

`[2405.17743] ORLM: Training Large Language Models for Optimization Modeling <https://arxiv.org/abs/2405.17743>`__ ORLM:面向优化建模的大型语言模型训练

`[2405.18719] Contextual Position Encoding: Learning to Count What's Important <https://arxiv.org/abs/2405.18719>`__ 上下文位置编码:学习计算什么是重要的

`[2405.19327] MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series <https://arxiv.org/abs/2405.19327>`__ MAP-Neo:高性能透明双语大型语言模型系列

`[2402.02446] LQER: Low-Rank Quantization Error Reconstruction for LLMs <https://arxiv.org/abs/2402.02446>`__ LQER: llm的低秩量化误差重构

`[2402.05140] Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains <https://arxiv.org/abs/2402.05140>`__ Tag-LLM:将通用llm重新用于专业领域

`[2403.00131] UNITS: A Unified Multi-Task Time Series Model <https://arxiv.org/abs/2403.00131>`__ 单位:统一的多任务时间序列模型

`[2405.14852] PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression <https://arxiv.org/abs/2405.14852>`__ PV-Tuning:极限LLM压缩的超越直通估计

`[2405.14918] AnalogCoder: Analog Circuit Design via Training-Free Code Generation <https://arxiv.org/abs/2405.14918>`__ Analog coder:基于无训练代码生成的模拟电路设计

`[2307.06616] SecureFalcon: Are We There Yet in Automated Software Vulnerability Detection with LLMs? <https://arxiv.org/abs/2307.06616>`__ SecureFalcon:使用LLMs进行自动化软件漏洞检测的工作已经完成了吗?

`[2312.12728] Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy <https://arxiv.org/abs/2312.12728>`__ Lookahead:具有无损生成精度的大型语言模型推理加速框架

`[2401.17981] Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study <https://arxiv.org/abs/2401.17981>`__ 用视觉检测模型增强多模态大型语言模型:实证研究

`[2403.15401] Large Language Model for Mental Health: A Systematic Review <https://arxiv.org/abs/2403.15401>`__ 心理健康的大型语言模型:系统回顾

`[2404.02637] Vocabulary Attack to Hijack Large Language Model Applications <https://arxiv.org/abs/2404.02637>`__ 劫持大型语言模型应用程序的词汇攻击

`[2405.06289] Look Once to Hear: Target Speech Hearing with Noisy Examples <https://arxiv.org/abs/2405.06289>`__ 一看一听:带噪声的目标语音听力

`[2405.16133] Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting <https://arxiv.org/abs/2405.16133>`__ 揭示llm生成的代码:基于代码重写的零样本合成代码检测器

`[2405.17503] Code Repair with LLMs gives an Exploration-Exploitation Tradeoff <https://arxiv.org/abs/2405.17503>`__ 使用llm进行代码修复提供了探索-利用的权衡

`[2310.07968] Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation <https://arxiv.org/abs/2310.07968>`__ 思考、行动和提问:开放世界交互式个性化机器人导航

`[2405.12107] Imp: Highly Capable Large Multimodal Models for Mobile Devices <https://arxiv.org/abs/2405.12107>`__ Imp:面向移动设备的高性能大型多模态模型

`[2306.08121] Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations <https://arxiv.org/abs/2306.08121>`__ 语义IDs的更好泛化:推荐排序的案例研究

`[2405.01964] Understanding LLMs Requires More Than Statistical Generalization <https://arxiv.org/abs/2405.01964>`__ 理解llm需要的不仅仅是统计泛化

