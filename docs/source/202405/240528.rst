240528
========

----------
Survey (3)
----------

`[2405.16640] A Survey of Multimodal Large Language Model from A Data-centric Perspective <https://arxiv.org/abs/2405.16640>`__

::

    Sun, 26 May 2024 17:31:21 GMT
    Tianyi Bai, Hao Liang, Binwang Wan, Ling Yang, Bozhou Li, Yifan Wang, Bin Cui, Conghui He, Binhang Yuan, and Wentao Zhang

Human beings perceive the world through diverse senses such as sight, smell, hearing, and touch. Similarly, multimodal large language models (MLLMs) enhance the capabilities of traditional large language models by integrating and processing data from multiple modalities including text, vision, audio, video, and 3D environments. Data plays a pivotal role in the development and refinement of these models. In this survey, we comprehensively review the literature on MLLMs from a data-centric perspective. Specifically, we explore methods for preparing multimodal data during the pretraining and adaptation phases of MLLMs. Additionally, we analyze the evaluation methods for datasets and review benchmarks for evaluating MLLMs. Our survey also outlines potential future research directions. This work aims to provide researchers with a detailed understanding of the data-driven aspects of MLLMs, fostering further exploration and innovation in this field.

------------

`[2310.05269] Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications <https://arxiv.org/abs/2310.05269>`__

::

    replaced with revised version Sun, 26 May 2024 02:37:36 GMT
    Submission history From: Navid Ayoobi [view email]
    [v1] Sun, 8 Oct 2023 19:54:26 UTC (2,245 KB)
    [v2] Sun, 15 Oct 2023 05:12:38 UTC (2,165 KB)
    [v3] Sun, 26 May 2024 02:37:36 UTC (1,337 KB)
    Azim Akhtarshenas, Mohammad Ali Vahedifar, Navid Ayoobi, Behrouz Maham, Tohid Alizadeh, Sina Ebrahimi, David L\'opez-P\'erez

Robust machine learning (ML) models can be developed by leveraging large volumes of data and distributing the computational tasks across numerous devices or servers. Federated learning (FL) is a technique in the realm of ML that facilitates this goal by utilizing cloud infrastructure to enable collaborative model training among a network of decentralized devices. Beyond distributing the computational load, FL targets the resolution of privacy issues and the reduction of communication costs simultaneously. To protect user privacy, FL requires users to send model updates rather than transmitting large quantities of raw and potentially confidential data. Specifically, individuals train ML models locally using their own data and then upload the results in the form of weights and gradients to the cloud for aggregation into the global model. This strategy is also advantageous in environments with limited bandwidth or high communication costs, as it prevents the transmission of large data volumes. With the increasing volume of data and rising privacy concerns, alongside the emergence of large-scale ML models like Large Language Models (LLMs), FL presents itself as a timely and relevant solution. It is therefore essential to review current FL algorithms to guide future research that meets the rapidly evolving ML demands. This survey provides a comprehensive analysis and comparison of the most recent FL algorithms, evaluating them on various fronts including mathematical frameworks, privacy protection, resource allocation, and applications. Beyond summarizing existing FL methods, this survey identifies potential gaps, open areas, and future challenges based on the performance reports and algorithms used in recent studies. This survey enables researchers to readily identify existing limitations in the FL field for further exploration.

------------

`[2405.11299] The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving <https://arxiv.org/abs/2405.11299>`__

::

    replaced with revised version Mon, 27 May 2024 01:09:07 GMT
    Submission history From: Yizhou Shan [view email]
    [v1] Sat, 18 May 2024 14:00:04 UTC (379 KB)
    [v2] Mon, 27 May 2024 01:09:07 UTC (379 KB)
    Pai Zeng, Zhenyu Ning, Jieru Zhao, Weihao Cui, Mengwei Xu, Liwei Guo, Xusheng Chen, Yizhou Shan

We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.

------------

-------------
Benchmark (9)
-------------

`[2405.17378] RTL-Repo: A Benchmark for Evaluating LLMs on Large-Scale RTL Design Projects <https://arxiv.org/abs/2405.17378>`__

::

    Mon, 27 May 2024 17:36:01 GMT
    Ahmed Allam, Mohamed Shalan

Large Language Models (LLMs) have demonstrated potential in assisting with Register Transfer Level (RTL) design tasks. Nevertheless, there remains to be a significant gap in benchmarks that accurately reflect the complexity of real-world RTL projects. To address this, this paper presents RTL-Repo, a benchmark specifically designed to evaluate LLMs on large-scale RTL design projects. RTL-Repo includes a comprehensive dataset of more than 4000 Verilog code samples extracted from public GitHub repositories, with each sample providing the full context of the corresponding repository. We evaluate several state-of-the-art models on the RTL-Repo benchmark, including GPT-4, GPT-3.5, Starcoder2, alongside Verilog-specific models like VeriGen and RTLCoder, and compare their performance in generating Verilog code for complex projects. The RTL-Repo benchmark provides a valuable resource for the hardware design community to assess and compare LLMs' performance in real-world RTL design scenarios and train LLMs specifically for Verilog code generation in complex, multi-file RTL projects. RTL-Repo is open-source and publicly available on Github.

------------

`[2405.16473] M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought <https://arxiv.org/abs/2405.16473>`__

::

    Sun, 26 May 2024 07:56:30 GMT
    Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, Wanxiang Che

Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) Domain missing, thereby hindering the development of MCoT.
Motivated by this, we introduce a novel benchmark (M$^3$CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT.
Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M$^3$CoT and there remains a large gap between existing VLLMs and human performance in M$^3$CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M$^3$CoT can serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.

------------

`[2405.16546] Cocktail: A Comprehensive Information Retrieval Benchmark with LLM-Generated Documents Integration <https://arxiv.org/abs/2405.16546>`__

::

    Sun, 26 May 2024 12:30:20 GMT
    Sunhao Dai, Weihao Liu, Yuqi Zhou, Liang Pang, Rongju Ruan, Gang Wang, Zhenhua Dong, Jun Xu, Ji-Rong Wen

The proliferation of Large Language Models (LLMs) has led to an influx of AI-generated content (AIGC) on the internet, transforming the corpus of Information Retrieval (IR) systems from solely human-written to a coexistence with LLM-generated content. The impact of this surge in AIGC on IR systems remains an open question, with the primary challenge being the lack of a dedicated benchmark for researchers. In this paper, we introduce Cocktail, a comprehensive benchmark tailored for evaluating IR models in this mixed-sourced data landscape of the LLM era. Cocktail consists of 16 diverse datasets with mixed human-written and LLM-generated corpora across various text retrieval tasks and domains. Additionally, to avoid the potential bias from previously included dataset information in LLMs, we also introduce an up-to-date dataset, named NQ-UTD, with queries derived from recent events. Through conducting over 1,000 experiments to assess state-of-the-art retrieval models against the benchmarked datasets in Cocktail, we uncover a clear trade-off between ranking performance and source bias in neural retrieval models, highlighting the necessity for a balanced approach in designing future IR systems. We hope Cocktail can serve as a foundational resource for IR research in the LLM era, with all data and code publicly available at \url{https://github.com/KID-22/Cocktail}.

------------

`[2310.12086] FactCHD: Benchmarking Fact-Conflicting Hallucination Detection <https://arxiv.org/abs/2310.12086>`__

::

    replaced with revised version Sun, 26 May 2024 16:37:01 GMT
    Submission history From: Ningyu Zhang [view email]
    [v1] Wed, 18 Oct 2023 16:27:49 UTC (4,534 KB)
    [v2] Thu, 18 Jan 2024 16:20:06 UTC (4,532 KB)
    [v3] Sun, 26 May 2024 16:37:01 UTC (4,530 KB)
    Xiang Chen, Duanzheng Song, Honghao Gui, Chenxi Wang, Ningyu Zhang, Yong Jiang, Fei Huang, Chengfei Lv, Dan Zhang, Huajun Chen

Despite their impressive generative capabilities, LLMs are hindered by fact-conflicting hallucinations in real-world applications. The accurate identification of hallucinations in texts generated by LLMs, especially in complex inferential scenarios, is a relatively unexplored area. To address this gap, we present FactCHD, a dedicated benchmark designed for the detection of fact-conflicting hallucinations from LLMs. FactCHD features a diverse dataset that spans various factuality patterns, including vanilla, multi-hop, comparison, and set operation. A distinctive element of FactCHD is its integration of fact-based evidence chains, significantly enhancing the depth of evaluating the detectors' explanations. Experiments on different LLMs expose the shortcomings of current approaches in detecting factual errors accurately. Furthermore, we introduce Truth-Triangulator that synthesizes reflective considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming to yield more credible detection through the amalgamation of predictive results and evidence. The benchmark dataset is available at this https URL.

------------

`[2402.14992] tinyBenchmarks: evaluating LLMs with fewer examples <https://arxiv.org/abs/2402.14992>`__

::

    replaced with revised version Sun, 26 May 2024 22:27:23 GMT
    Submission history From: Felipe Maia Polo [view email]
    [v1] Thu, 22 Feb 2024 22:05:23 UTC (5,640 KB)
    [v2] Sun, 26 May 2024 22:27:23 UTC (5,468 KB)
    Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, Mikhail Yurochkin

The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.

------------

`[2403.01031] Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks <https://arxiv.org/abs/2403.01031>`__

::

    replaced with revised version Fri, 24 May 2024 20:24:36 GMT
    Submission history From: Gagan Bhatia [view email]
    [v1] Fri, 1 Mar 2024 23:38:02 UTC (46,486 KB)
    [v2] Fri, 24 May 2024 20:24:36 UTC (47,219 KB)
    Fakhraddin Alwajih, El Moatez Billah Nagoudi, Gagan Bhatia, Abdelrahman Mohamed, Muhammad Abdul-Mageed

Multimodal large language models (MLLMs) have proven effective in a wide range of tasks requiring complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with large speaker populations such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed \textit{Peacock}, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce ~\textit{Henna}, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic culture, setting the first stone for culturally-aware Arabic MLLMs.The GitHub repository for the \textit{Peacock} project is available at \url{this https URL}.

------------

`[2404.12464] NormAd: A Benchmark for Measuring the Cultural Adaptability of Large Language Models <https://arxiv.org/abs/2404.12464>`__

::

    replaced with revised version Mon, 27 May 2024 00:06:31 GMT
    Submission history From: Abhinav Rao [view email]
    [v1] Thu, 18 Apr 2024 18:48:50 UTC (8,921 KB)
    [v2] Thu, 23 May 2024 17:49:51 UTC (8,930 KB)
    [v3] Mon, 27 May 2024 00:06:31 UTC (8,930 KB)
    Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, Maarten Sap

The integration of Large Language Models (LLMs) into various global cultures fundamentally presents a cultural challenge: LLMs must navigate interactions, respect social norms, and avoid transgressing cultural boundaries. However, it is still unclear if LLMs can adapt their outputs to diverse cultural norms. Our study focuses on this aspect. We introduce NormAd, a novel dataset, which includes 2.6k stories that represent social and cultural norms from 75 countries, to assess the ability of LLMs to adapt to different granular levels of socio-cultural contexts such as the country of origin, its associated cultural values, and prevalent social norms. Our study reveals that LLMs struggle with cultural reasoning across all contextual granularities, showing stronger adaptability to English-centric cultures over those from the Global South. Even with explicit social norms, the top-performing model, Mistral-7b-Instruct, achieves only 81.8\% accuracy, lagging behind the 95.6\% achieved by humans. Evaluation on NormAd further reveals that LLMs struggle to adapt to stories involving gift-giving across cultures. Due to inherent agreement or sycophancy biases, LLMs find it considerably easier to assess the social acceptability of stories that adhere to cultural norms than those that deviate from them. Our benchmark measures the cultural adaptability (or lack thereof) of LLMs, emphasizing the potential to make these technologies more equitable and useful for global audiences. We release the NormAd dataset and its associated code on GitHub.

------------

`[2403.10943] MIntRec 2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations <https://arxiv.org/abs/2403.10943>`__

::

    replaced with revised version Mon, 27 May 2024 03:14:08 GMT
    Submission history From: Hanlei Zhang [view email]
    [v1] Sat, 16 Mar 2024 15:14:15 UTC (3,509 KB)
    [v2] Wed, 20 Mar 2024 02:52:42 UTC (3,509 KB)
    [v3] Mon, 27 May 2024 03:14:08 UTC (3,509 KB)
    Hanlei Zhang, Xin Wang, Hua Xu, Qianrui Zhou, Kai Gao, Jianhua Su, jinyue Zhao, Wenrui Li, Yanting Chen

Multimodal intent recognition poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. Existing benchmark datasets are limited in scale and suffer from difficulties in handling out-of-scope samples that arise in multi-turn conversational interactions. We introduce MIntRec 2.0, a large-scale benchmark dataset for multimodal intent recognition in multi-party conversations. It contains 1,245 dialogues with 15,040 samples, each annotated within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope samples, it also includes 5,736 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world scenarios. Furthermore, we provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. We establish a general framework supporting the organization of single-turn and multi-turn dialogue data, modality feature extraction, multimodal fusion, as well as in-scope classification and out-of-scope detection. Evaluation benchmarks are built using classic multimodal fusion methods, ChatGPT, and human evaluators. While existing methods incorporating nonverbal information yield improvements, effectively leveraging context information and detecting out-of-scope samples remains a substantial challenge. Notably, large language models exhibit a significant performance gap compared to humans, highlighting the limitations of machine learning methods in the cognitive intent understanding task. We believe that MIntRec 2.0 will serve as a valuable resource, providing a pioneering foundation for research in human-machine conversational interactions, and significantly facilitating related applications. The full dataset and codes are available at this https URL.

------------

`[2405.14191] S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models <https://arxiv.org/abs/2405.14191>`__

::

    replaced with revised version Mon, 27 May 2024 08:27:29 GMT
    Submission history From: Xiaohan Yuan [view email]
    [v1] Thu, 23 May 2024 05:34:31 UTC (1,110 KB)
    [v2] Mon, 27 May 2024 08:27:29 UTC (1,092 KB)
    Xiaohan Yuan, Jinfeng Li, Dongxia Wang, Yuefeng Chen, Xiaofeng Mao, Longtao Huang, Hui Xue, Wenhai Wang, Kui Ren, Jingyi Wang

Large Language Models have gained considerable attention for their revolutionary capabilities. However, there is also growing concern on their safety implications, making a comprehensive safety evaluation for LLMs urgently needed before model deployment. In this work, we propose S-Eval, a new comprehensive, multi-dimensional and open-ended safety evaluation benchmark. At the core of S-Eval is a novel LLM-based automatic test prompt generation and selection framework, which trains an expert testing LLM Mt combined with a range of test selection strategies to automatically construct a high-quality test suite for the safety evaluation. The key to the automation of this process is a novel expert safety-critique LLM Mc able to quantify the riskiness score of an LLM's response, and additionally produce risk tags and explanations. Besides, the generation process is also guided by a carefully designed risk taxonomy with four different levels, covering comprehensive and multi-dimensional safety risks of concern. Based on these, we systematically construct a new and large-scale safety evaluation benchmark for LLMs consisting of 220,000 evaluation prompts, including 20,000 base risk prompts (10,000 in Chinese and 10,000 in English) and 200,000 corresponding attack prompts derived from 10 popular adversarial instruction attacks against LLMs. Moreover, considering the rapid evolution of LLMs and accompanied safety threats, S-Eval can be flexibly configured and adapted to include new risks, attacks and models. S-Eval is extensively evaluated on 20 popular and representative LLMs. The results confirm that S-Eval can better reflect and inform the safety risks of LLMs compared to existing benchmarks. We also explore the impacts of parameter scales, language environments, and decoding parameters on the evaluation, providing a systematic methodology for evaluating the safety of LLMs.

------------

---------------
Accelerate (17)
---------------

`[2405.16122] Prompt Optimization with EASE? Efficient Ordering-aware Automated Selection of Exemplars <https://arxiv.org/abs/2405.16122>`__

::

    Sat, 25 May 2024 08:23:05 GMT
    Zhaoxuan Wu, Xiaoqiang Lin, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low

Large language models (LLMs) have shown impressive capabilities in real-world applications. The capability of in-context learning (ICL) allows us to adapt an LLM to downstream tasks by including input-label exemplars in the prompt without model fine-tuning. However, the quality of these exemplars in the prompt greatly impacts performance, highlighting the need for an effective automated exemplar selection method. Recent studies have explored retrieval-based approaches to select exemplars tailored to individual test queries, which can be undesirable due to extra test-time computation and an increased risk of data exposure. Moreover, existing methods fail to adequately account for the impact of exemplar ordering on the performance. On the other hand, the impact of the instruction, another essential component in the prompt given to the LLM, is often overlooked in existing exemplar selection methods.
To address these challenges, we propose a novel method named EASE, which leverages the hidden embedding from a pre-trained language model to represent ordered sets of exemplars and uses a neural bandit algorithm to optimize the sets of exemplars while accounting for exemplar ordering. Our EASE can efficiently find an ordered set of exemplars that performs well for all test queries from a given task, thereby eliminating test-time computation.
Importantly, EASE can be readily extended to jointly optimize both the exemplars and the instruction. Through extensive empirical evaluations (including novel tasks), we demonstrate the superiority of EASE over existing methods, and reveal practical insights about the impact of exemplar selection on ICL, which may be of independent interest. Our code is available at https://github.com/ZhaoxuanWu/EASE-Prompt-Optimization.

------------

`[2405.16057] SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models <https://arxiv.org/abs/2405.16057>`__

::

    Sat, 25 May 2024 04:55:27 GMT
    Xudong Lu, Aojun Zhou, Yuhui Xu, Renrui Zhang, Peng Gao, Hongsheng Li

Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a Sparsity-Preserved Parameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, keeping the structure and sparsity of pruned pre-trained models intact. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP.

------------

`[2405.16178] Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection <https://arxiv.org/abs/2405.16178>`__

::

    Sat, 25 May 2024 11:10:04 GMT
    Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, Jindong Chen

Large language models (LLMs) augmented with retrieval exhibit robust performance and extensive versatility by incorporating external contexts.
However, the input length grows linearly in the number of retrieved documents, causing a dramatic increase in latency. In this paper, we propose a novel paradigm named Sparse RAG, which seeks to cut computation costs through sparsity. Specifically, Sparse RAG encodes retrieved documents in parallel, which eliminates latency introduced by long-range attention of retrieved documents. Then, LLMs selectively decode the output by only attending to highly relevant caches auto-regressively, which are chosen via prompting LLMs with special control tokens. It is notable that Sparse RAG combines the assessment of each individual document and the generation of the response into a single process. The designed sparse mechanism in a RAG system can facilitate the reduction of the number of documents loaded during decoding for accelerating the inference of the RAG system. Additionally, filtering out undesirable contexts enhances the model's focus on relevant context, inherently improving its generation quality. Evaluation results of two datasets show that Sparse RAG can strike an optimal balance between generation quality and computational efficiency, demonstrating its generalizability across both short- and long-form generation tasks.

------------

`[2405.16552] SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation <https://arxiv.org/abs/2405.16552>`__

::

    Sun, 26 May 2024 12:43:18 GMT
    Ziqin Luo, Haixia Han, Haokun Zhao, Guochao Jiang, Chengyu Du, Tingyun Li, Jiaqing Liang, Deqing Yang, Yanghua Xiao

Existing Large Language Models (LLMs) generate text through unidirectional autoregressive decoding methods to respond to various user queries. These methods tend to consider token selection in a simple sequential manner, making it easy to fall into suboptimal options when encountering uncertain tokens, referred to as chaotic points in our work. Many chaotic points exist in texts generated by LLMs, and they often significantly affect the quality of subsequently generated tokens, which can interfere with LLMs' generation. This paper proposes Self-Evaluation Decoding, SED, a decoding method for enhancing model generation. Analogous to the human decision-making process, SED integrates speculation and evaluation steps into the decoding process, allowing LLMs to make more careful decisions and thus optimize token selection at chaotic points. Experimental results across various tasks using different LLMs demonstrate SED's effectiveness.

------------

`[2405.17202] Efficient multi-prompt evaluation of LLMs <https://arxiv.org/abs/2405.17202>`__

::

    Mon, 27 May 2024 14:24:47 GMT
    Felipe Maia Polo, Ronald Xu, Lucas Weber, M\'irian Silva, Onkar Bhardwaj, Leshem Choshen, Allysson Flavio Melo de Oliveira, Yuekai Sun, Mikhail Yurochkin

Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry. For example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Our code and data can be found at https://github.com/felipemaiapolo/prompt-eval.

------------

`[2405.17337] Cost-efficient Knowledge-based Question Answering with Large Language Models <https://arxiv.org/abs/2405.17337>`__

::

    Mon, 27 May 2024 16:37:34 GMT
    Junnan Dong, Qinggang Zhang, Chuang Zhou, Hao Chen, Daochen Zha, Xiao Huang

Knowledge-based question answering (KBQA) is widely used in many scenarios that necessitate domain knowledge. Large language models (LLMs) bring opportunities to KBQA, while their costs are significantly higher and absence of domain-specific knowledge during pre-training. We are motivated to combine LLMs and prior small models on knowledge graphs (KGMs) for both inferential accuracy and cost saving. However, it remains challenging since accuracy and cost are not readily combined in the optimization as two distinct metrics. It is also laborious for model selection since different models excel in diverse knowledge. To this end, we propose Coke, a novel cost-efficient strategy for KBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize calls to LLMs within limited budgets. We first formulate the accuracy expectation with a cluster-level Thompson Sampling for either KGMs or LLMs. A context-aware policy is optimized to further distinguish the expert model subject to the question semantics. The overall decision is bounded by the cost regret according to historical expenditure on failures. Extensive experiments showcase the superior performance of Coke, which moves the Pareto frontier with up to 20.89% saving of GPT-4 fees while achieving a 2.74% higher accuracy on the benchmark datasets.

------------

`[2405.17381] Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention <https://arxiv.org/abs/2405.17381>`__

::

    Mon, 27 May 2024 17:38:13 GMT
    Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong

We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.

------------

`[2405.17386] MindMerger: Efficient Boosting LLM Reasoning in non-English Languages <https://arxiv.org/abs/2405.17386>`__

::

    Mon, 27 May 2024 17:41:54 GMT
    Zixian Huang, Wenhao Zhu, Gong Cheng, Lei Li, Fei Yuan

Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages. To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English. Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs. In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance. Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs. Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages. Without updating the parameters of LLMs, the average accuracy improved by 6.7% and 8.0% across all languages and low-resource languages on the MGSM dataset, respectively.

------------

`[2405.16755] CHESS: Contextual Harnessing for Efficient SQL Synthesis <https://arxiv.org/abs/2405.16755>`__

::

    Mon, 27 May 2024 01:54:16 GMT
    Shayan Talaei, Mohammadreza Pourreza, Yu-Chen Chang, Azalia Mirhoseini, Amin Saberi

Utilizing large language models (LLMs) for transforming natural language questions into SQL queries (text-to-SQL) is a promising yet challenging approach, particularly when applied to real-world databases with complex and extensive schemas. In particular, effectively incorporating data catalogs and database values for SQL generation remains an obstacle, leading to suboptimal solutions. We address this problem by proposing a new pipeline that effectively retrieves relevant data and context, selects an efficient schema, and synthesizes correct and efficient SQL queries. To increase retrieval precision, our pipeline introduces a hierarchical retrieval method leveraging model-generated keywords, locality-sensitive hashing indexing, and vector databases. Additionally, we have developed an adaptive schema pruning technique that adjusts based on the complexity of the problem and the model's context size. Our approach generalizes to both frontier proprietary models like GPT-4 and open-source models such as Llama-3-70B. Through a series of ablation studies, we demonstrate the effectiveness of each component of our pipeline and its impact on the end-to-end performance. Our method achieves new state-of-the-art performance on the cross-domain challenging BIRD dataset.

------------

`[2405.17258] $\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning <https://arxiv.org/abs/2405.17258>`__

::

    Mon, 27 May 2024 15:15:08 GMT
    Runqian Wang, Soumya Ghosh, David Cox, Diego Antognini, Aude Oliva, Rogerio Feris, Leonid Karlinsky

Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters. These additional LoRA parameters are specific to the base model being adapted. When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained. Such re-training requires access to the data used to train the LoRA for the original base model. This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data. To address this challenge, we propose $\textit{Trans-LoRA}$ -- a novel method for lossless, nearly data-free transfer of LoRAs across base models. Our approach relies on synthetic data to transfer LoRA modules. Using large language models, we design a synthetic data generator to approximate the data-generating process of the $\textit{observed}$ task data subset. Training on the resulting synthetic dataset transfers LoRA modules to new models. We show the effectiveness of our approach using both LLama and Gemma model families. Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks.

------------

`[2405.16241] FastQuery: Communication-efficient Embedding Table Query for Private LLM Inference <https://arxiv.org/abs/2405.16241>`__

::

    Sat, 25 May 2024 13:58:45 GMT
    Chenqi Lin, Tianshi Xu, Zebin Yang, Runsheng Wang, Ru Huang, Meng Li

With the fast evolution of large language models (LLMs), privacy concerns with user queries arise as they may contain sensitive information. Private inference based on homomorphic encryption (HE) has been proposed to protect user query privacy. However, a private embedding table query has to be formulated as a HE-based matrix-vector multiplication problem and suffers from enormous computation and communication overhead. We observe the overhead mainly comes from the neglect of 1) the one-hot nature of user queries and 2) the robustness of the embedding table to low bit-width quantization noise. Hence, in this paper, we propose a private embedding table query optimization framework, dubbed FastQuery. FastQuery features a communication-aware embedding table quantization algorithm and a one-hot-aware dense packing algorithm to simultaneously reduce both the computation and communication costs. Compared to prior-art HE-based frameworks, e.g., Cheetah, Iron, and Bumblebee, FastQuery achieves more than $4.3\times$, $2.7\times$, $1.3\times$ latency reduction, respectively and more than $75.7\times$, $60.2\times$, $20.2\times$ communication reduction, respectively, on both LLAMA-7B and LLAMA-30B.

------------

`[2405.14314] Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration <https://arxiv.org/abs/2405.14314>`__

::

    replaced with revised version Sun, 26 May 2024 02:31:15 GMT
    Submission history From: Yang Zhang [view email]
    [v1] Thu, 23 May 2024 08:33:19 UTC (4,522 KB)
    [v2] Sun, 26 May 2024 02:31:15 UTC (4,522 KB)
    Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Zhen Wang, Xuelong Li

Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at this https URL.

------------

`[2310.07075] Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding <https://arxiv.org/abs/2310.07075>`__

::

    replaced with revised version Sun, 26 May 2024 02:16:18 GMT
    Submission history From: Kexun Zhang [view email]
    [v1] Tue, 10 Oct 2023 23:37:53 UTC (505 KB)
    [v2] Sun, 26 May 2024 02:16:18 UTC (609 KB)
    Kexun Zhang, Hongqiao Chen, Lei Li, William Wang

Instruction-tuned large language models (LLMs) excel at many tasks but often fail to use external tools due to complicated and unfamiliar syntax constraints. While extensive fine-tuning and prompting can mitigate the issue, these approaches are expensive and hard to generalize. Furthermore, because syntax constraints are only learned implicitly during fine-tuning, models still make frequent syntax errors. Motivated by the fact that these constraints can be better satisfied explicitly with constrained decoding, we propose TOOLDEC, a decoding algorithm using finite state machines to force LLMs to follow tool syntax. Our experiments show that TOOLDEC eliminates all syntax errors, achieving significantly better performance on various base models and benchmarks. More surprisingly, when applied to generalist out-of-the-box LLMs such as Mistral-Instruct, TOOLDEC improves its accuracy in tool use from the initial 0% to an impressive 52%, matching the performance of specialized fine-tuned models such as ToolLLM.

------------

`[2311.02851] Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding <https://arxiv.org/abs/2311.02851>`__

::

    replaced with revised version Mon, 27 May 2024 07:52:38 GMT
    Submission history From: Jiali Zeng [view email]
    [v1] Mon, 6 Nov 2023 03:41:57 UTC (9,019 KB)
    [v2] Mon, 27 May 2024 07:52:38 UTC (9,325 KB)
    Jiali Zeng and Fandong Meng and Yongjing Yin and Jie Zhou

Contemporary translation engines based on the encoder-decoder framework have made significant strides in development. However, the emergence of Large Language Models (LLMs) has disrupted their position by presenting the potential for achieving superior translation quality. To uncover the circumstances in which LLMs excel and explore how their strengths can be harnessed to enhance translation quality, we first conduct a comprehensive analysis to assess the strengths and limitations of various commercial NMT systems and MT-oriented LLMs. Our findings indicate that neither NMT nor MT-oriented LLMs alone can effectively address all the translation issues, but MT-oriented LLMs show promise as a complementary solution to NMT systems. Building upon these insights, we propose Cooperative Decoding (CoDec), which treats NMT systems as a pretranslation model and MT-oriented LLMs as a supplemental solution to handle complex scenarios beyond the capability of NMT alone. Experimental results on the WMT22 test sets and a newly collected test set WebCrawl demonstrate the effectiveness and efficiency of CoDec, highlighting its potential as a robust solution for combining NMT systems with MT-oriented LLMs in the field of machine translation.

------------

`[2403.01251] Accelerating Greedy Coordinate Gradient via Probe Sampling <https://arxiv.org/abs/2403.01251>`__

::

    replaced with revised version Mon, 27 May 2024 07:02:28 GMT
    Submission history From: Yiran Zhao [view email]
    [v1] Sat, 2 Mar 2024 16:23:44 UTC (3,827 KB)
    [v2] Mon, 27 May 2024 07:02:28 UTC (4,022 KB)
    Yiran Zhao, Wenyue Zheng, Tianle Cai, Xuan Long Do, Kenji Kawaguchi, Anirudh Goyal, Michael Shieh

Safety of Large Language Models (LLMs) has become a critical issue given their rapid progresses. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing adversarial prompts to break the aligned LLMs, but optimization of GCG is time-consuming. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called $\texttt{Probe sampling}$. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates. Probe sampling achieves up to $5.6$ times speedup using Llama2-7b-chat and leads to equal or improved attack success rate (ASR) on the AdvBench. Furthermore, probe sampling is also able to accelerate other prompt optimization techniques and adversarial methods, leading to acceleration of $1.8\times$ for AutoPrompt, $2.4\times$ for APE and $2.4\times$ for AutoDAN.

------------

`[2402.16902] PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA <https://arxiv.org/abs/2402.16902>`__

::

    replaced with revised version Mon, 27 May 2024 02:24:25 GMT
    Submission history From: Sheng Wang [view email]
    [v1] Sat, 24 Feb 2024 13:39:05 UTC (160 KB)
    [v2] Mon, 27 May 2024 02:24:25 UTC (163 KB)
    Sheng Wang, Boyang Xue, Jiacheng Ye, Jiyue Jiang, Liheng Chen, Lingpeng Kong, Chuan Wu

With the rapid scaling of large language models (LLMs), serving numerous low-rank adaptations (LoRAs) concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA retains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA still outperforms LoRA on multiple instruction tuning datasets. Subsequently, an ablation study is conducted to validate the necessity of individual components and highlight the superiority of PRoLoRA over three potential variants. Hopefully, the conspicuously higher parameter efficiency can establish PRoLoRA as a resource-friendly alternative to LoRA.

------------

`[2403.17919] LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning <https://arxiv.org/abs/2403.17919>`__

::

    replaced with revised version Sat, 25 May 2024 10:20:27 GMT
    Submission history From: Rui Pan [view email]
    [v1] Tue, 26 Mar 2024 17:55:02 UTC (2,185 KB)
    [v2] Thu, 28 Mar 2024 15:44:39 UTC (1,661 KB)
    [v3] Sat, 25 May 2024 10:20:27 UTC (1,404 KB)
    Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang

The machine learning community has witnessed impressive advancements since large language models (LLMs) first appeared. Yet, their massive memory consumption has become a significant roadblock to large-scale training. For instance, a 7B model typically requires at least 60 GB of GPU memory with full parameter training, which presents challenges for researchers without access to high-resource environments. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem. However, in most large-scale fine-tuning settings, their performance does not reach the level of full parameter training because they confine the parameter search to a low-rank subspace. Attempting to complement this deficiency, we investigate the layerwise properties of LoRA on fine-tuning tasks and observe an unexpected but consistent skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freezes most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over 10%-35% in terms of MT-Bench score while achieving on-par or better performance in MMLU, AGIEval and WinoGrande. On large models, specifically LLaMA-2-70B, LISA surpasses LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.

------------

-----------------------
In-Context Learning (3)
-----------------------

`[2405.15984] Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models <https://arxiv.org/abs/2405.15984>`__

::

    Fri, 24 May 2024 23:56:36 GMT
    Simon Chi Lok Yu, Jie He, Pasquale Minervini, Jeff Z. Pan

With the emergence of large language models, such as LLaMA and OpenAI GPT-3, In-Context Learning (ICL) gained significant attention due to its effectiveness and efficiency. However, ICL is very sensitive to the choice, order, and verbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented ICL methods try to address this problem by leveraging retrievers to extract semantically related examples as demonstrations. While this approach yields more accurate results, its robustness against various types of adversarial attacks, including perturbations on test samples, demonstrations, and retrieved data, remains under-explored. Our study reveals that retrieval-augmented models can enhance robustness against test sample attacks, outperforming vanilla ICL with a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit overconfidence in the demonstrations, leading to a 2% increase in ASR for demonstration attacks. Adversarial training can help improve the robustness of ICL methods to adversarial attacks; however, such a training scheme can be too costly in the context of LLMs. As an alternative, we introduce an effective training-free adversarial defence method, DARD, which enriches the example pool with those attacked samples. We show that DARD yields improvements in performance and robustness, achieving a 15% reduction in ASR over the baselines. Code and data are released to encourage further research: https://github.com/simonucl/adv-retreival-icl

------------

`[2405.17062] Unifying Demonstration Selection and Compression for In-Context Learning <https://arxiv.org/abs/2405.17062>`__

::

    Mon, 27 May 2024 11:31:58 GMT
    Jun Gao

In-context learning (ICL) facilitates large language models (LLMs) exhibiting spectacular emergent capabilities in various scenarios. Unfortunately, introducing demonstrations easily makes the prompt length explode, bringing a significant burden to hardware. In addition, random demonstrations usually achieve limited improvements in ICL, necessitating demonstration selection among accessible candidates. Previous studies introduce extra modules to perform demonstration compression or selection independently. In this paper, we propose an ICL framework UniICL, which Unifies demonstration selection and compression, and final response generation via a single frozen LLM.
Specifically, UniICL first projects actual demonstrations and inference text inputs into short virtual tokens, respectively. Then, virtual tokens are applied to select suitable demonstrations by measuring semantic similarity within latent space among candidate demonstrations and inference input.
Finally, inference text inputs together with selected virtual demonstrations are fed into the same frozen LLM for response generation. Notably, UniICL is a parameter-efficient framework that only contains 17M trainable parameters originating from the projection layer. We conduct experiments and analysis over in- and out-domain datasets of both generative and understanding tasks, encompassing ICL scenarios with plentiful and limited demonstration candidates.
Results show that UniICL effectively unifies $12 \times$ compression, demonstration selection, and response generation, efficiently scaling up the baseline from 4-shot to 64-shot ICL in IMDb with 24 GB CUDA allocation

------------

`[2405.17264] On the Noise Robustness of In-Context Learning for Text Generation <https://arxiv.org/abs/2405.17264>`__

::

    Mon, 27 May 2024 15:22:58 GMT
    Hongfu Gao, Feipeng Zhang, Wenyu Jiang, Jun Shu, Feng Zheng, Hongxin Wei

Large language models (LLMs) have shown impressive performance on downstream tasks by in-context learning (ICL), which heavily relies on the quality of demonstrations selected from a large set of annotated examples. Recent works claim that in-context learning is robust to noisy demonstrations in text classification. In this work, we show that, on text generation tasks, noisy annotations significantly hurt the performance of in-context learning. To circumvent the issue, we propose a simple and effective approach called Local Perplexity Ranking (LPR), which replaces the "noisy" candidates with their nearest neighbors that are more likely to be clean. Our method is motivated by analyzing the perplexity deviation caused by noisy labels and decomposing perplexity into inherent perplexity and matching perplexity. Our key idea behind LPR is thus to decouple the matching perplexity by performing the ranking among the neighbors in semantic space. Our approach can prevent the selected demonstrations from including mismatched input-label pairs while preserving the effectiveness of the original selection methods. Extensive experiments demonstrate the effectiveness of LPR, improving the EM score by up to 18.75 on common benchmarks with noisy annotations.

------------

-------------
Reasoning (5)
-------------

`[2405.16802] AutoCV: Empowering Reasoning with Automated Process Labeling via Confidence Variation <https://arxiv.org/abs/2405.16802>`__

::

    Mon, 27 May 2024 03:44:24 GMT
    Jianqiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yingjia Wan, Yinya Huang, Zhijiang Guo

In this work, we propose a novel method named \textbf{Auto}mated Process Labeling via \textbf{C}onfidence \textbf{V}ariation (\textbf{\textsc{AutoCV}}) to enhance the reasoning capabilities of large language models (LLMs) by automatically annotating the reasoning steps. Our approach begins by training a verification model on the correctness of final answers, enabling it to generate automatic process annotations. This verification model assigns a confidence score to each reasoning step, indicating the probability of arriving at the correct final answer from that point onward. We detect relative changes in the verification's confidence scores across reasoning steps to automatically annotate the reasoning process. This alleviates the need for numerous manual annotations or the high computational costs associated with model-induced annotation approaches. We experimentally validate that the confidence variations learned by the verification model trained on the final answer correctness can effectively identify errors in the reasoning steps.
Subsequently, we demonstrate that the process annotations generated by \textsc{AutoCV} can improve the accuracy of the verification model in selecting the correct answer from multiple outputs generated by LLMs. Notably, we achieve substantial improvements across five datasets in mathematics and commonsense reasoning. The source code of \textsc{AutoCV} is available at \url{https://github.com/rookie-joe/AUTOCV}.

------------

`[2405.17386] MindMerger: Efficient Boosting LLM Reasoning in non-English Languages <https://arxiv.org/abs/2405.17386>`__

::

    Mon, 27 May 2024 17:41:54 GMT
    Zixian Huang, Wenhao Zhu, Gong Cheng, Lei Li, Fei Yuan

Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages. To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English. Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs. In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance. Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs. Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages. Without updating the parameters of LLMs, the average accuracy improved by 6.7% and 8.0% across all languages and low-resource languages on the MGSM dataset, respectively.

------------

`[2405.16265] MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time <https://arxiv.org/abs/2405.16265>`__

::

    Sat, 25 May 2024 15:07:33 GMT
    Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Boxing Chen

Although Large Language Models (LLMs) achieve remarkable performance across various tasks, they often struggle with complex reasoning tasks, such as answering mathematical questions. Recent efforts to address this issue have primarily focused on leveraging mathematical datasets through supervised fine-tuning or self-improvement techniques. However, these methods often depend on high-quality datasets that are difficult to prepare, or they require substantial computational resources for fine-tuning. Inspired by findings that LLMs know how to produce right answer but struggle to select the correct reasoning path, we propose a purely inference-based searching method called MindStar (M*), which treats reasoning tasks as search problems. This method utilizes a step-wise reasoning approach to navigate the tree space. To enhance search efficiency, we propose two tree-search ideas to identify the optimal reasoning paths. We evaluate the M* framework on both the GSM8K and MATH datasets, comparing its performance with existing open and closed-source LLMs.
Our results demonstrate that M* significantly enhances the reasoning abilities of open-source models, such as Llama-2-13B and Mistral-7B, and achieves comparable performance to GPT-3.5 and Grok-1, but with substantially reduced model size and computational costs.

------------

`[2310.18659] DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy <https://arxiv.org/abs/2310.18659>`__

::

    replaced with revised version Sun, 26 May 2024 14:47:13 GMT
    Submission history From: Hongda Sun [view email]
    [v1] Sat, 28 Oct 2023 10:05:51 UTC (358 KB)
    [v2] Sun, 26 May 2024 14:47:13 UTC (725 KB)
    Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, Rui Yan

Recent advances in large language models (LLMs) have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior studies have focused on modeling reasoning steps using various thought structures like chains, trees, or graphs. However, LLM-based reasoning still encounters the following challenges: (1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps. To this end, we propose DetermLR, a novel perspective that rethinks the reasoning process as an evolution from indeterminacy to determinacy. First, we categorize known conditions into two types: determinate and indeterminate premises This provides an oveall direction for the reasoning process and guides LLMs in converting indeterminate data into progressively determinate insights. Subsequently, we leverage quantitative measurements to prioritize more relevant premises to explore new insights. Furthermore, we automate the storage and extraction of available premises and reasoning paths with reasoning memory, preserving historical reasoning details for subsequent reasoning steps. Comprehensive experimental results demonstrate that DetermLR surpasses all baselines on various logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and LogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR achieves higher accuracy with fewer reasoning steps, highlighting its superior efficiency and effectiveness in solving logical reasoning tasks.

------------

`[2404.06479] Text-Based Reasoning About Vector Graphics <https://arxiv.org/abs/2404.06479>`__

::

    replaced with revised version Fri, 24 May 2024 19:40:26 GMT
    Submission history From: Zhenhailong Wang [view email]
    [v1] Tue, 9 Apr 2024 17:30:18 UTC (2,755 KB)
    [v2] Wed, 10 Apr 2024 02:12:27 UTC (2,755 KB)
    [v3] Fri, 24 May 2024 19:40:26 UTC (2,755 KB)
    Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li, Jiajun Wu, Heng Ji

While large multimodal models excel in broad vision-language benchmarks, they often struggle with tasks requiring precise perception of low-level visual details, such as comparing line lengths or solving simple mazes. In particular, this failure mode persists in question-answering tasks about vector graphics -- images composed purely of 2D objects and shapes. To address this challenge, we propose the Visually Descriptive Language Model (VDLM), which performs text-based reasoning about vector graphics. VDLM leverages Scalable Vector Graphics (SVG) for a more precise visual description and first uses an off-the-shelf raster-to-SVG algorithm for encoding. Since existing language models cannot understand raw SVGs in a zero-shot setting, VDLM then bridges SVG with pretrained language models through a newly introduced intermediate symbolic representation, Primal Visual Description (PVD), comprising primitive attributes (e.g., shape, position, measurement) with their corresponding predicted values. PVD is task-agnostic and represents visual primitives that are universal across all vector graphics. It can be learned with procedurally generated (SVG, PVD) pairs and also enables the direct use of LLMs for generalization to complex reasoning tasks. By casting an image to a text-based representation, we can leverage the power of language models to learn alignment from SVG to visual primitives and generalize to unseen question-answering tasks. Empirical results show that VDLM achieves stronger zero-shot performance compared to state-of-the-art LMMs, such as GPT-4V, in various low-level multimodal perception and reasoning tasks on vector graphics. We additionally present extensive analyses on VDLM's performance, demonstrating that our framework offers better interpretability due to its disentangled perception and reasoning processes. Project page: this https URL

------------

-----------
ToolUse (7)
-----------

`[2405.16089] COLT: Towards Completeness-Oriented Tool Retrieval for Large Language Models <https://arxiv.org/abs/2405.16089>`__

::

    Sat, 25 May 2024 06:41:23 GMT
    Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen

Recently, the integration of external tools with Large Language Models (LLMs) has emerged as a promising approach to overcome the inherent constraints of their pre-training data. However, realworld applications often involve a diverse range of tools, making it infeasible to incorporate all tools directly into LLMs due to constraints on input length and response time. Therefore, to fully exploit the potential of tool-augmented LLMs, it is crucial to develop an effective tool retrieval system. Existing tool retrieval methods techniques mainly rely on semantic matching between user queries and tool descriptions, which often results in the selection of redundant tools. As a result, these methods fail to provide a complete set of diverse tools necessary for addressing the multifaceted problems encountered by LLMs. In this paper, we propose a novel modelagnostic COllaborative Learning-based Tool Retrieval approach, COLT, which captures not only the semantic similarities between user queries and tool descriptions but also takes into account the collaborative information of tools. Specifically, we first fine-tune the PLM-based retrieval models to capture the semantic relationships between queries and tools in the semantic learning stage. Subsequently, we construct three bipartite graphs among queries, scenes, and tools and introduce a dual-view graph collaborative learning framework to capture the intricate collaborative relationships among tools during the collaborative learning stage. Extensive experiments on both the open benchmark and the newly introduced ToolLens dataset show that COLT achieves superior performance. Notably, the performance of BERT-mini (11M) with our proposed model framework outperforms BERT-large (340M), which has 30 times more parameters. Additionally, we plan to publicly release the ToolLens dataset to support further research in tool retrieval.

------------

`[2405.16376] STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making <https://arxiv.org/abs/2405.16376>`__

::

    Sat, 25 May 2024 23:25:10 GMT
    Chuanhao Li, Runhan Yang, Tiankai Li, Milad Bafarassat, Kourosh Sharifi, Dirk Bergemann, and Zhuoran Yang

Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities. However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information. These deficiencies hinder their performance in strategic and interactive tasks that demand adherence to nuanced game rules, long-term planning, exploration in unknown environments, and anticipation of opponents' moves. To overcome these obstacles, this paper presents a novel LLM agent framework equipped with memory and specialized tools to enhance their strategic decision-making capabilities. We deploy the tools in a number of economically important environments, in particular bilateral bargaining and multi-agent and dynamic mechanism design. We employ quantitative metrics to assess the framework's performance in various strategic decision-making problems. Our findings establish that our enhanced framework significantly improves the strategic decision-making capability of LLMs. While we highlight the inherent limitations of current LLM models, we demonstrate the improvements through targeted enhancements, suggesting a promising direction for future developments in LLM applications for interactive environments.

------------

`[2405.16533] Chain of Tools: Large Language Model is an Automatic Multi-tool Learner <https://arxiv.org/abs/2405.16533>`__

::

    Sun, 26 May 2024 11:40:58 GMT
    Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Suzan Verberne, Zhaochun Ren

Augmenting large language models (LLMs) with external tools has emerged as a promising approach to extend their utility, empowering them to solve practical tasks. Existing work typically empowers LLMs as tool users with a manually designed workflow, where the LLM plans a series of tools in a step-by-step manner, and sequentially executes each tool to obtain intermediate results until deriving the final answer. However, they suffer from two challenges in realistic scenarios: (1) The handcrafted control flow is often ad-hoc and constraints the LLM to local planning; (2) The LLM is instructed to use only manually demonstrated tools or well-trained Python functions, which limits its generalization to new tools. In this work, we first propose Automatic Tool Chain (ATC), a framework that enables the LLM to act as a multi-tool user, which directly utilizes a chain of tools through programming. To scale up the scope of the tools, we next propose a black-box probing method. This further empowers the LLM as a tool learner that can actively discover and document tool usages, teaching themselves to properly master new tools. For a comprehensive evaluation, we build a challenging benchmark named ToolFlow, which diverges from previous benchmarks by its long-term planning scenarios and complex toolset. Experiments on both existing datasets and ToolFlow illustrate the superiority of our framework. Analysis on different settings also validates the effectiveness and the utility of our black-box probing algorithm.

------------

`[2310.07075] Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding <https://arxiv.org/abs/2310.07075>`__

::

    replaced with revised version Sun, 26 May 2024 02:16:18 GMT
    Submission history From: Kexun Zhang [view email]
    [v1] Tue, 10 Oct 2023 23:37:53 UTC (505 KB)
    [v2] Sun, 26 May 2024 02:16:18 UTC (609 KB)
    Kexun Zhang, Hongqiao Chen, Lei Li, William Wang

Instruction-tuned large language models (LLMs) excel at many tasks but often fail to use external tools due to complicated and unfamiliar syntax constraints. While extensive fine-tuning and prompting can mitigate the issue, these approaches are expensive and hard to generalize. Furthermore, because syntax constraints are only learned implicitly during fine-tuning, models still make frequent syntax errors. Motivated by the fact that these constraints can be better satisfied explicitly with constrained decoding, we propose TOOLDEC, a decoding algorithm using finite state machines to force LLMs to follow tool syntax. Our experiments show that TOOLDEC eliminates all syntax errors, achieving significantly better performance on various base models and benchmarks. More surprisingly, when applied to generalist out-of-the-box LLMs such as Mistral-Instruct, TOOLDEC improves its accuracy in tool use from the initial 0% to an impressive 52%, matching the performance of specialized fine-tuned models such as ToolLLM.

------------

`[2402.11903] DiLA: Enhancing LLM Tool Learning with Differential Logic Layer <https://arxiv.org/abs/2402.11903>`__

::

    replaced with revised version Sat, 25 May 2024 01:46:17 GMT
    Submission history From: Yu Zhang [view email]
    [v1] Mon, 19 Feb 2024 07:38:57 UTC (517 KB)
    [v2] Sat, 25 May 2024 01:46:17 UTC (560 KB)
    Yu Zhang, Hui-Ling Zhen, Zehua Pei, Yingzhao Lian, Lihao Yin, Mingxuan Yuan, Bei Yu

Considering the challenges faced by large language models (LLMs) in logical reasoning and planning, prior efforts have sought to augment LLMs with access to external solvers. While progress has been made on simple reasoning problems, solving classical constraint satisfaction problems, such as the Boolean Satisfiability Problem (SAT) and Graph Coloring Problem (GCP), remains difficult for off-the-shelf solvers due to their intricate expressions and exponential search spaces. In this paper, we propose a novel differential logic layer-aided language modeling (DiLA) approach, where logical constraints are integrated into the forward and backward passes of a network layer, to provide another option for LLM tool learning. In DiLA, LLM aims to transform the language description to logic constraints and identify initial solutions of the highest quality, while the differential logic layer focuses on iteratively refining the LLM-prompted solution. Leveraging the logic layer as a bridge, DiLA enhances the logical reasoning ability of LLMs on a range of reasoning problems encoded by Boolean variables, guaranteeing the efficiency and correctness of the solution process. We evaluate the performance of DiLA on two classic reasoning problems and empirically demonstrate its consistent outperformance against existing prompt-based and solver-aided approaches.

------------

`[2403.03031] Learning to Use Tools via Cooperative and Interactive Agents <https://arxiv.org/abs/2403.03031>`__

::

    replaced with revised version Sun, 26 May 2024 11:49:56 GMT
    Submission history From: Zhengliang Shi [view email]
    [v1] Tue, 5 Mar 2024 15:08:16 UTC (1,443 KB)
    [v2] Sun, 26 May 2024 11:49:56 UTC (1,446 KB)
    Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, Zhaochun Ren

Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the superiority of our ConAgents (e.g., 6 point improvement over the SOTA baseline). We further provide fine-granularity analysis for the efficiency and consistency of our framework.

------------

`[2309.11489] Text2Reward: Reward Shaping with Language Models for Reinforcement Learning <https://arxiv.org/abs/2309.11489>`__

::

    replaced with revised version Sat, 25 May 2024 06:42:10 GMT
    Submission history From: Tianbao Xie [view email]
    [v1] Wed, 20 Sep 2023 17:39:13 UTC (13,066 KB)
    [v2] Thu, 21 Sep 2023 15:17:09 UTC (13,066 KB)
    [v3] Sat, 25 May 2024 06:42:10 UTC (15,540 KB)
    Tianbao Xie and Siheng Zhao and Chen Henry Wu and Yitao Liu and Qian Luo and Victor Zhong and Yanchao Yang and Tao Yu

Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation and shaping of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates shaped dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at this https URL .

------------

------------------------
Retrieval-Augmented (10)
------------------------

`[2405.15984] Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models <https://arxiv.org/abs/2405.15984>`__

::

    Fri, 24 May 2024 23:56:36 GMT
    Simon Chi Lok Yu, Jie He, Pasquale Minervini, Jeff Z. Pan

With the emergence of large language models, such as LLaMA and OpenAI GPT-3, In-Context Learning (ICL) gained significant attention due to its effectiveness and efficiency. However, ICL is very sensitive to the choice, order, and verbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented ICL methods try to address this problem by leveraging retrievers to extract semantically related examples as demonstrations. While this approach yields more accurate results, its robustness against various types of adversarial attacks, including perturbations on test samples, demonstrations, and retrieved data, remains under-explored. Our study reveals that retrieval-augmented models can enhance robustness against test sample attacks, outperforming vanilla ICL with a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit overconfidence in the demonstrations, leading to a 2% increase in ASR for demonstration attacks. Adversarial training can help improve the robustness of ICL methods to adversarial attacks; however, such a training scheme can be too costly in the context of LLMs. As an alternative, we introduce an effective training-free adversarial defence method, DARD, which enriches the example pool with those attacked samples. We show that DARD yields improvements in performance and robustness, achieving a 15% reduction in ASR over the baselines. Code and data are released to encourage further research: https://github.com/simonucl/adv-retreival-icl

------------

`[2405.16089] COLT: Towards Completeness-Oriented Tool Retrieval for Large Language Models <https://arxiv.org/abs/2405.16089>`__

::

    Sat, 25 May 2024 06:41:23 GMT
    Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen

Recently, the integration of external tools with Large Language Models (LLMs) has emerged as a promising approach to overcome the inherent constraints of their pre-training data. However, realworld applications often involve a diverse range of tools, making it infeasible to incorporate all tools directly into LLMs due to constraints on input length and response time. Therefore, to fully exploit the potential of tool-augmented LLMs, it is crucial to develop an effective tool retrieval system. Existing tool retrieval methods techniques mainly rely on semantic matching between user queries and tool descriptions, which often results in the selection of redundant tools. As a result, these methods fail to provide a complete set of diverse tools necessary for addressing the multifaceted problems encountered by LLMs. In this paper, we propose a novel modelagnostic COllaborative Learning-based Tool Retrieval approach, COLT, which captures not only the semantic similarities between user queries and tool descriptions but also takes into account the collaborative information of tools. Specifically, we first fine-tune the PLM-based retrieval models to capture the semantic relationships between queries and tools in the semantic learning stage. Subsequently, we construct three bipartite graphs among queries, scenes, and tools and introduce a dual-view graph collaborative learning framework to capture the intricate collaborative relationships among tools during the collaborative learning stage. Extensive experiments on both the open benchmark and the newly introduced ToolLens dataset show that COLT achieves superior performance. Notably, the performance of BERT-mini (11M) with our proposed model framework outperforms BERT-large (340M), which has 30 times more parameters. Additionally, we plan to publicly release the ToolLens dataset to support further research in tool retrieval.

------------

`[2405.16178] Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection <https://arxiv.org/abs/2405.16178>`__

::

    Sat, 25 May 2024 11:10:04 GMT
    Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, Jindong Chen

Large language models (LLMs) augmented with retrieval exhibit robust performance and extensive versatility by incorporating external contexts.
However, the input length grows linearly in the number of retrieved documents, causing a dramatic increase in latency. In this paper, we propose a novel paradigm named Sparse RAG, which seeks to cut computation costs through sparsity. Specifically, Sparse RAG encodes retrieved documents in parallel, which eliminates latency introduced by long-range attention of retrieved documents. Then, LLMs selectively decode the output by only attending to highly relevant caches auto-regressively, which are chosen via prompting LLMs with special control tokens. It is notable that Sparse RAG combines the assessment of each individual document and the generation of the response into a single process. The designed sparse mechanism in a RAG system can facilitate the reduction of the number of documents loaded during decoding for accelerating the inference of the RAG system. Additionally, filtering out undesirable contexts enhances the model's focus on relevant context, inherently improving its generation quality. Evaluation results of two datasets show that Sparse RAG can strike an optimal balance between generation quality and computational efficiency, demonstrating its generalizability across both short- and long-form generation tasks.

------------

`[2405.16420] M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions <https://arxiv.org/abs/2405.16420>`__

::

    Sun, 26 May 2024 04:03:13 GMT
    Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, Wei Shi

Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant memories from an external database. However, existing RAG methods typically organize all memories in a whole database, potentially limiting focus on crucial memories and introducing noise. In this paper, we introduce a multiple partition paradigm for RAG (called M-RAG), where each database partition serves as a basic unit for RAG execution. Based on this paradigm, we propose a novel framework that leverages LLMs with Multi-Agent Reinforcement Learning to optimize different language generation tasks explicitly. Through comprehensive experiments conducted on seven datasets, spanning three language generation tasks and involving three distinct language model architectures, we confirm that M-RAG consistently outperforms various baseline methods, achieving improvements of 11%, 8%, and 12% for text summarization, machine translation, and dialogue generation, respectively.

------------

`[2405.16933] Empowering Large Language Models to Set up a Knowledge Retrieval Indexer via Self-Learning <https://arxiv.org/abs/2405.16933>`__

::

    Mon, 27 May 2024 08:26:45 GMT
    Xun Liang, Simin Niu, Zhiyu li, Sensen Zhang, Shichao Song, Hanyu Wang, Jiawei Yang, Feiyu Xiong, Bo Tang, Chenyang Xi

Retrieval-Augmented Generation (RAG) offers a cost-effective approach to injecting real-time knowledge into large language models (LLMs). Nevertheless, constructing and validating high-quality knowledge repositories require considerable effort. We propose a pre-retrieval framework named Pseudo-Graph Retrieval-Augmented Generation (PG-RAG), which conceptualizes LLMs as students by providing them with abundant raw reading materials and encouraging them to engage in autonomous reading to record factual information in their own words.
The resulting concise, well-organized mental indices are interconnected through common topics or complementary facts to form a pseudo-graph database. During the retrieval phase, PG-RAG mimics the human behavior in flipping through notes, identifying fact paths and subsequently exploring the related contexts.
Adhering to the principle of the path taken by many is the best, it integrates highly corroborated fact paths to provide a structured and refined sub-graph assisting LLMs. We validated PG-RAG on three specialized question-answering datasets. In single-document tasks, PG-RAG significantly outperformed the current best baseline, KGP-LLaMA, across all key evaluation metrics, with an average overall performance improvement of 11.6%. Specifically, its BLEU score increased by approximately 14.3%, and the QE-F1 metric improved by 23.7%. In multi-document scenarios, the average metrics of PG-RAG were at least 2.35% higher than the best baseline. Notably, the BLEU score and QE-F1 metric showed stable improvements of around 7.55% and 12.75%, respectively. Our code: https://github.com/IAAR-Shanghai/PGRAG.

------------

`[2405.15784] CLARINET: Augmenting Language Models to Ask Clarification Questions for Retrieval <https://arxiv.org/abs/2405.15784>`__

::

    Sun, 28 Apr 2024 18:21:31 GMT
    Yizhou Chi, Jessy Lin, Kevin Lin, Dan Klein

Users often make ambiguous requests that require clarification. We study the problem of asking clarification questions in an information retrieval setting, where systems often face ambiguous search queries and it is challenging to turn the uncertainty in the retrieval model into a natural language question. We present CLARINET, a system that asks informative clarification questions by choosing questions whose answers would maximize certainty in the correct candidate. Our approach works by augmenting a large language model (LLM) to condition on a retrieval distribution, finetuning end-to-end to generate the question that would have maximized the rank of the true candidate at each turn.
When evaluated on a real-world retrieval dataset of users searching for books, our system outperforms traditional heuristics such as information gain on retrieval success by 17% and vanilla-prompted LLMs by 39% relative.

------------

`[2405.15792] IQLS: Framework for leveraging Metadata to enable Large Language Model based queries to complex, versatile Data <https://arxiv.org/abs/2405.15792>`__

::

    Sat, 4 May 2024 13:44:05 GMT
    Sami Azirar, Hossam A. Gabbar, Chaouki Regoui

As the amount and complexity of data grows, retrieving it has become a more difficult task that requires greater knowledge and resources. This is especially true for the logistics industry, where new technologies for data collection provide tremendous amounts of interconnected real-time data. The Intelligent Query and Learning System (IQLS) simplifies the process by allowing natural language use to simplify data retrieval . It maps structured data into a framework based on the available metadata and available data models. This framework creates an environment for an agent powered by a Large Language Model. The agent utilizes the hierarchical nature of the data to filter iteratively by making multiple small context-aware decisions instead of one-shot data retrieval. After the Data filtering, the IQLS enables the agent to fulfill tasks given by the user query through interfaces. These interfaces range from multimodal transportation information retrieval to route planning under multiple constraints. The latter lets the agent define a dynamic object, which is determined based on the query parameters. This object represents a driver capable of navigating a road network. The road network is depicted as a graph with attributes based on the data. Using a modified version of the Dijkstra algorithm, the optimal route under the given constraints can be determined. Throughout the entire process, the user maintains the ability to interact and guide the system. The IQLS is showcased in a case study on the Canadian logistics sector, allowing geospatial, visual, tabular and text data to be easily queried semantically in natural language.

------------

`[2405.16546] Cocktail: A Comprehensive Information Retrieval Benchmark with LLM-Generated Documents Integration <https://arxiv.org/abs/2405.16546>`__

::

    Sun, 26 May 2024 12:30:20 GMT
    Sunhao Dai, Weihao Liu, Yuqi Zhou, Liang Pang, Rongju Ruan, Gang Wang, Zhenhua Dong, Jun Xu, Ji-Rong Wen

The proliferation of Large Language Models (LLMs) has led to an influx of AI-generated content (AIGC) on the internet, transforming the corpus of Information Retrieval (IR) systems from solely human-written to a coexistence with LLM-generated content. The impact of this surge in AIGC on IR systems remains an open question, with the primary challenge being the lack of a dedicated benchmark for researchers. In this paper, we introduce Cocktail, a comprehensive benchmark tailored for evaluating IR models in this mixed-sourced data landscape of the LLM era. Cocktail consists of 16 diverse datasets with mixed human-written and LLM-generated corpora across various text retrieval tasks and domains. Additionally, to avoid the potential bias from previously included dataset information in LLMs, we also introduce an up-to-date dataset, named NQ-UTD, with queries derived from recent events. Through conducting over 1,000 experiments to assess state-of-the-art retrieval models against the benchmarked datasets in Cocktail, we uncover a clear trade-off between ranking performance and source bias in neural retrieval models, highlighting the necessity for a balanced approach in designing future IR systems. We hope Cocktail can serve as a foundational resource for IR research in the LLM era, with all data and code publicly available at \url{https://github.com/KID-22/Cocktail}.

------------

`[2405.15452] Leveraging Logical Rules in Knowledge Editing: A Cherry on the Top <https://arxiv.org/abs/2405.15452>`__

::

    replaced with revised version Mon, 27 May 2024 11:24:59 GMT
    Submission history From: Lijie Hu [view email]
    [v1] Fri, 24 May 2024 11:30:00 UTC (10,324 KB)
    [v2] Mon, 27 May 2024 11:24:59 UTC (9,983 KB)
    Keyuan Cheng, Muhammad Asif Ali, Shu Yang, Gang Lin, Yuxuan Zhai, Haoyang Fei, Ke Xu, Lu Yu, Lijie Hu, and Di Wang

Multi-hop Question Answering (MQA) under knowledge editing (KE) is a key challenge in Large Language Models (LLMs). While best-performing solutions in this domain use a plan and solve paradigm to split a question into sub-questions followed by response generation, we claim that this approach is sub-optimal as it fails for hard to decompose questions, and it does not explicitly cater to correlated knowledge updates resulting as a consequence of knowledge edits. This has a detrimental impact on the overall consistency of the updated knowledge. To address these issues, in this paper, we propose a novel framework named RULE-KE, i.e., RULE based Knowledge Editing, which is a cherry on the top for augmenting the performance of all existing MQA methods under KE. Specifically, RULE-KE leverages rule discovery to discover a set of logical rules. Then, it uses these discovered rules to update knowledge about facts highly correlated with the edit. Experimental evaluation using existing and newly curated datasets (i.e., RKE-EVAL) shows that RULE-KE helps augment both performances of parameter-based and memory-based solutions up to 92% and 112.9%, respectively.

------------

`[2402.07630] G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering <https://arxiv.org/abs/2402.07630>`__

::

    replaced with revised version Mon, 27 May 2024 04:04:40 GMT
    Submission history From: Xiaoxin He [view email]
    [v1] Mon, 12 Feb 2024 13:13:04 UTC (20,324 KB)
    [v2] Thu, 14 Mar 2024 05:04:40 UTC (20,325 KB)
    [v3] Mon, 27 May 2024 04:04:40 UTC (20,596 KB)
    Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi

Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\footnote{Our codes and datasets are available at: \url{this https URL}}

------------

----------
Agent (16)
----------

`[2405.15821] Reinforcing Language Agents via Policy Optimization with Action Decomposition <https://arxiv.org/abs/2405.15821>`__

::

    Thu, 23 May 2024 14:01:44 GMT
    Muning Wen, Ziyu Wan, Weinan Zhang, Jun Wang, Ying Wen

Language models as intelligent agents push the boundaries of sequential decision-making agents but struggle with limited knowledge of environmental dynamics and exponentially huge action space. Recent efforts like GLAM and TWOSOME manually constrain the action space to a restricted subset and employ reinforcement learning to align agents' knowledge with specific environments.
However, they overlook fine-grained credit assignments for intra-action tokens, which is essential for efficient language agent optimization, and rely on human's prior knowledge to restrict action space. This paper proposes decomposing language agent optimization from the action level to the token level, offering finer supervision for each intra-action token and manageable optimization complexity in environments with unrestricted action spaces.
Beginning with the simplification of flattening all actions, we theoretically explore the discrepancies between action-level optimization and this naive token-level optimization. We then derive the Bellman backup with Action Decomposition (BAD) to integrate credit assignments for both intra-action and inter-action tokens, effectively eliminating the discrepancies. Implementing BAD within the PPO algorithm, we introduce Policy Optimization with Action Decomposition (POAD). POAD benefits from a finer-grained credit assignment process and lower optimization complexity, leading to enhanced learning efficiency and generalization abilities in aligning language agents with interactive environments. We validate POAD across diverse testbeds, with results affirming the advantages of our approach and the correctness of our theoretical analysis.

------------

`[2405.16205] GeneAgent: Self-verification Language Agent for Gene Set Knowledge Discovery using Domain Databases <https://arxiv.org/abs/2405.16205>`__

::

    Sat, 25 May 2024 12:35:15 GMT
    Zhizheng Wang, Qiao Jin, Chih-Hsuan Wei, Shubo Tian, Po-Ting Lai, Qingqing Zhu, Chi-Ping Day, Christina Ross, Zhiyong Lu

Gene set knowledge discovery is essential for advancing human functional genomics. Recent studies have shown promising performance by harnessing the power of Large Language Models (LLMs) on this task. Nonetheless, their results are subject to several limitations common in LLMs such as hallucinations. In response, we present GeneAgent, a first-of-its-kind language agent featuring self-verification capability. It autonomously interacts with various biological databases and leverages relevant domain knowledge to improve accuracy and reduce hallucination occurrences. Benchmarking on 1,106 gene sets from different sources, GeneAgent consistently outperforms standard GPT-4 by a significant margin. Moreover, a detailed manual review confirms the effectiveness of the self-verification module in minimizing hallucinations and generating more reliable analytical narratives. To demonstrate its practical utility, we apply GeneAgent to seven novel gene sets derived from mouse B2905 melanoma cell lines, with expert evaluations showing that GeneAgent offers novel insights into gene functions and subsequently expedites knowledge discovery.

------------

`[2405.16247] AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning <https://arxiv.org/abs/2405.16247>`__

::

    Sat, 25 May 2024 14:11:44 GMT
    Minghao Chen, Yihang Li, Yanting Yang, Shiyu Yu, Binbin Lin, Xiaofei He

Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments.
AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce \textit{case-conditioned prompting} strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\% with GPT-4-turbo and 86.2\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The source code will be available soon.

------------

`[2405.16334] Devil's Advocate: Anticipatory Reflection for LLM Agents <https://arxiv.org/abs/2405.16334>`__

::

    Sat, 25 May 2024 19:20:15 GMT
    Haoyu Wang and Tao Li and Zhiwei Deng and Dan Roth and Yang Li

In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks.
Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions. We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement. By deploying and experimenting with this methodology - a zero-shot approach - within WebArena for practical tasks in web environments, our agent demonstrates superior performance over existing zero-shot methods. The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions needed to achieve a task.

------------

`[2405.16510] Meta-Task Planning for Language Agents <https://arxiv.org/abs/2405.16510>`__

::

    Sun, 26 May 2024 10:33:17 GMT
    Cong Zhang, Deik Derrick Goh Xin, Dexun Li, Hao Zhang, Yong Liu

The rapid advancement of neural language models has sparked a new surge of intelligent agent research. Unlike traditional agents, large language model-based agents (LLM agents) have emerged as a promising paradigm for achieving artificial general intelligence (AGI) due to their superior reasoning and generalization capabilities. Effective planning is crucial for the success of LLM agents in real-world tasks, making it a highly pursued topic in the community. Current planning methods typically translate tasks into executable action sequences. However, determining a feasible or optimal sequence for complex tasks at fine granularity, which often requires compositing long chains of heterogeneous actions, remains challenging. This paper introduces Meta-Task Planning (MTP), a zero-shot methodology for collaborative LLM-based multi-agent systems that simplifies complex task planning by decomposing it into a hierarchy of subordinate tasks, or meta-tasks. Each meta-task is then mapped into executable actions. MTP was assessed on two rigorous benchmarks, TravelPlanner and API-Bank. Notably, MTP achieved an average $\sim40\%$ success rate on TravelPlanner, significantly higher than the state-of-the-art (SOTA) baseline ($2.92\%$), and outperforming $LLM_{api}$-4 with ReAct on API-Bank by $\sim14\%$, showing the immense potential of integrating LLM with multi-agent systems.

------------

`[2405.16751] LLM-Based Cooperative Agents using Information Relevance and Plan Validation <https://arxiv.org/abs/2405.16751>`__

::

    Mon, 27 May 2024 01:47:14 GMT
    SeungWon Seo, Junhyeok Lee, SeongRae Noh, HyeongYeop Kang

We address the challenge of multi-agent cooperation, where agents achieve a common goal by interacting with a 3D scene and cooperating with decentralized agents under complex partial observations. This involves managing communication costs and optimizing interaction trajectories in dynamic environments. Our research focuses on three primary limitations of existing cooperative agent systems. Firstly, current systems demonstrate inefficiency in managing acquired information through observation, resulting in declining planning performance as the environment becomes more complex with additional objects or goals.
Secondly, the neglect of false plans in partially observable settings leads to suboptimal cooperative performance, as agents struggle to adapt to environmental changes influenced by the unseen actions of other agents. Lastly, the failure to incorporate spatial data into decision-making processes restricts the agent's ability to construct optimized trajectories. To overcome these limitations, we propose the RElevance and Validation-Enhanced Cooperative Language Agent (REVECA), a novel cognitive architecture powered by GPT-3.5.
REVECA leverages relevance assessment, plan validation, and spatial information to enhance the efficiency and robustness of agent cooperation in dynamic and partially observable environments while minimizing continuous communication costs and effectively managing irrelevant dummy objects. Our extensive experiments demonstrate the superiority of REVECA over previous approaches, including those driven by GPT-4.0. Additionally, a user study highlights REVECA's potential for achieving trustworthy human-AI cooperation. We expect that REVECA will have significant applications in gaming, XR applications, educational tools, and humanoid robots, contributing to substantial economic, commercial, and academic advancements.

------------

`[2405.16887] A Large Language Model-based multi-agent manufacturing system for intelligent shopfloor <https://arxiv.org/abs/2405.16887>`__

::

    Mon, 27 May 2024 07:10:04 GMT
    Zhen Zhao, Dunbing Tang, Haihua Zhu, Zequn Zhang, Kai Chen, Changchun Liu, Yuchen Ji

As productivity advances, the demand of customers for multi-variety and small-batch production is increasing, thereby putting forward higher requirements for manufacturing systems. When production tasks frequent changes due to this demand, traditional manufacturing systems often cannot response promptly. The multi-agent manufacturing system is proposed to address this problem. However, because of technical limitations, the negotiation among agents in this kind of system is realized through predefined heuristic rules, which is not intelligent enough to deal with the multi-variety and small batch production. To this end, a Large Language Model-based (LLM-based) multi-agent manufacturing system for intelligent shopfloor is proposed in the present study. This system delineates the diverse agents and defines their collaborative methods. The roles of the agents encompass Machine Server Agent (MSA), Bid Inviter Agent (BIA), Bidder Agent (BA), Thinking Agent (TA), and Decision Agent (DA). Due to the support of LLMs, TA and DA acquire the ability of analyzing the shopfloor condition and choosing the most suitable machine, as opposed to executing a predefined program artificially. The negotiation between BAs and BIA is the most crucial step in connecting manufacturing resources.
With the support of TA and DA, BIA will finalize the distribution of orders, relying on the information of each machine returned by BA. MSAs bears the responsibility for connecting the agents with the physical shopfloor. This system aims to distribute and transmit workpieces through the collaboration of the agents with these distinct roles, distinguishing it from other scheduling approaches. Comparative experiments were also conducted to validate the performance of this system.

------------

`[2405.17009] Position: Foundation Agents as the Paradigm Shift for Decision Making <https://arxiv.org/abs/2405.17009>`__

::

    Mon, 27 May 2024 09:54:50 GMT
    Xiaoqian Liu, Xingzhou Lou, Jianbin Jiao, Junge Zhang

Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies. Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization. In contrast, foundation models in language and vision has showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents. This proposal is underpinned by the formulation of foundation agents with its fundamental characteristics and challenges motivated by the success of large language models (LLMs). Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs. Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future.

------------

`[2405.16376] STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making <https://arxiv.org/abs/2405.16376>`__

::

    Sat, 25 May 2024 23:25:10 GMT
    Chuanhao Li, Runhan Yang, Tiankai Li, Milad Bafarassat, Kourosh Sharifi, Dirk Bergemann, and Zhuoran Yang

Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities. However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information. These deficiencies hinder their performance in strategic and interactive tasks that demand adherence to nuanced game rules, long-term planning, exploration in unknown environments, and anticipation of opponents' moves. To overcome these obstacles, this paper presents a novel LLM agent framework equipped with memory and specialized tools to enhance their strategic decision-making capabilities. We deploy the tools in a number of economically important environments, in particular bilateral bargaining and multi-agent and dynamic mechanism design. We employ quantitative metrics to assess the framework's performance in various strategic decision-making problems. Our findings establish that our enhanced framework significantly improves the strategic decision-making capability of LLMs. While we highlight the inherent limitations of current LLM models, we demonstrate the improvements through targeted enhancements, suggesting a promising direction for future developments in LLM applications for interactive environments.

------------

`[2402.12275] WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment <https://arxiv.org/abs/2402.12275>`__

::

    replaced with revised version Sun, 26 May 2024 04:24:04 GMT
    Submission history From: Hao Tang [view email]
    [v1] Mon, 19 Feb 2024 16:39:18 UTC (3,620 KB)
    [v2] Sun, 26 May 2024 04:24:04 UTC (2,671 KB)
    Hao Tang, Darren Key, Kevin Ellis

We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We define this optimism as a logical constraint between a program and a planner. We study our agent on gridworlds, and on task planning, finding our approach is more sample-efficient compared to deep RL, more compute-efficient compared to ReAct-style agents, and that it can transfer its knowledge across environments by editing its code.

------------

`[2405.14314] Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration <https://arxiv.org/abs/2405.14314>`__

::

    replaced with revised version Sun, 26 May 2024 02:31:15 GMT
    Submission history From: Yang Zhang [view email]
    [v1] Thu, 23 May 2024 08:33:19 UTC (4,522 KB)
    [v2] Sun, 26 May 2024 02:31:15 UTC (4,522 KB)
    Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Zhen Wang, Xuelong Li

Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at this https URL.

------------

`[2310.02124] Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View <https://arxiv.org/abs/2310.02124>`__

::

    replaced with revised version Mon, 27 May 2024 11:12:45 GMT
    Submission history From: Shumin Deng [view email]
    [v1] Tue, 3 Oct 2023 15:05:52 UTC (6,891 KB)
    [v2] Mon, 26 Feb 2024 17:24:35 UTC (35,453 KB)
    [v3] Mon, 27 May 2024 11:12:45 UTC (37,971 KB)
    Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, Shumin Deng

As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets\footnote{\url{this https URL}.}, hoping to catalyze further research in this promising avenue.

------------

`[2401.05268] AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning <https://arxiv.org/abs/2401.05268>`__

::

    replaced with revised version Sun, 26 May 2024 15:31:24 GMT
    Submission history From: Shuofei Qiao [view email]
    [v1] Wed, 10 Jan 2024 16:57:24 UTC (7,414 KB)
    [v2] Wed, 17 Jan 2024 17:57:24 UTC (7,415 KB)
    [v3] Fri, 16 Feb 2024 16:19:25 UTC (564 KB)
    [v4] Sun, 26 May 2024 15:31:24 UTC (563 KB)
    Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen

Language agents have achieved considerable performance on various complex question-answering tasks by planning with external tools. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework for QA that does not rely on large-scale annotated data and synthetic planning trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. Further analysis demonstrates the effectiveness of the division-of-labor strategy, with the trajectory quality generated by AutoAct generally outperforming that of others. Code will be available at this https URL.

------------

`[2403.03031] Learning to Use Tools via Cooperative and Interactive Agents <https://arxiv.org/abs/2403.03031>`__

::

    replaced with revised version Sun, 26 May 2024 11:49:56 GMT
    Submission history From: Zhengliang Shi [view email]
    [v1] Tue, 5 Mar 2024 15:08:16 UTC (1,443 KB)
    [v2] Sun, 26 May 2024 11:49:56 UTC (1,446 KB)
    Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, Zhaochun Ren

Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the superiority of our ConAgents (e.g., 6 point improvement over the SOTA baseline). We further provide fine-granularity analysis for the efficiency and consistency of our framework.

------------

`[2403.16843] Do LLM Agents Have Regret? A Case Study in Online Learning and Games <https://arxiv.org/abs/2403.16843>`__

::

    replaced with revised version Sun, 26 May 2024 22:32:25 GMT
    Submission history From: Chanwoo Park [view email]
    [v1] Mon, 25 Mar 2024 15:04:11 UTC (8,530 KB)
    [v2] Sun, 26 May 2024 22:32:25 UTC (8,373 KB)
    Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang

Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \emph{unsupervised} training loss of \emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases.

------------

`[2405.14767] FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models <https://arxiv.org/abs/2405.14767>`__

::

    replaced with revised version Mon, 27 May 2024 12:43:42 GMT
    Submission history From: Hongyang Yang [view email]
    [v1] Thu, 23 May 2024 16:35:20 UTC (5,190 KB)
    [v2] Mon, 27 May 2024 12:43:42 UTC (5,190 KB)
    Hongyang Yang, Boyu Zhang, Neng Wang, Cheng Guo, Xiaoli Zhang, Likun Lin, Junlin Wang, Tianyu Zhou, Mao Guan, Runjia Zhang, Christina Dan Wang

As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community. These challenges impede the AI community's ability to enhance financial tasks effectively. Acknowledging financial analysis's critical role, we aim to devise financial-specialized LLM-based toolchains and democratize access to them through open-source initiatives, promoting wider AI adoption in financial decision-making. In this paper, we introduce FinRobot, a novel open-source AI agent platform supporting multiple financially specialized AI agents, each powered by LLM. Specifically, the platform consists of four major layers: 1) the Financial AI Agents layer that formulates Financial Chain-of-Thought (CoT) by breaking sophisticated financial problems down into logical sequences; 2) the Financial LLM Algorithms layer dynamically configures appropriate model application strategies for specific tasks; 3) the LLMOps and DataOps layer produces accurate models by applying training/fine-tuning techniques and using task-relevant data; 4) the Multi-source LLM Foundation Models layer that integrates various LLMs and enables the above layers to access them directly. Finally, FinRobot provides hands-on for both professional-grade analysts and laypersons to utilize powerful AI techniques for advanced financial analysis. We open-source FinRobot at \url{this https URL}.

------------

-----------
Other (169)
-----------

`[2405.15808] Ensuring Ground Truth Accuracy in Healthcare with the EVINCE framework <https://arxiv.org/abs/2405.15808>`__

::

    Mon, 20 May 2024 18:26:36 GMT
    Edward Y. Chang

Misdiagnosis is a significant issue in healthcare, leading to harmful consequences for patients. The propagation of mislabeled data through machine learning models into clinical practice is unacceptable. This paper proposes EVINCE, a system designed to 1) improve diagnosis accuracy and 2) rectify misdiagnoses and minimize training data errors. EVINCE stands for Entropy Variation through Information Duality with Equal Competence, leveraging this novel theory to optimize the diagnostic process using multiple Large Language Models (LLMs) in a structured debate framework. Our empirical study verifies EVINCE to be effective in achieving its design goals.

------------

`[2405.16136] C3LLM: Conditional Multimodal Content Generation Using Large Language Models <https://arxiv.org/abs/2405.16136>`__

::

    Sat, 25 May 2024 09:10:12 GMT
    Zixuan Wang, Qinkai Duan, Yu-Wing Tai, Chi-Keung Tang

We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner.
Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding "acoustic vocabulary" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.

------------

`[2405.16413] Augmented Risk Prediction for the Onset of Alzheimer's Disease from Electronic Health Records with Large Language Models <https://arxiv.org/abs/2405.16413>`__

::

    Sun, 26 May 2024 03:05:10 GMT
    Jiankun Wang, Sumyeong Ahn, Taykhoom Dalal, Xiaodan Zhang, Weishen Pan, Qiannan Zhang, Bin Chen, Hiroko H. Dodge, Fei Wang, Jiayu Zhou

Alzheimer's disease (AD) is the fifth-leading cause of death among Americans aged 65 and older. Screening and early detection of AD and related dementias (ADRD) are critical for timely intervention and for identifying clinical trial participants. The widespread adoption of electronic health records (EHRs) offers an important resource for developing ADRD screening tools such as machine learning based predictive models. Recent advancements in large language models (LLMs) demonstrate their unprecedented capability of encoding knowledge and performing reasoning, which offers them strong potential for enhancing risk prediction. This paper proposes a novel pipeline that augments risk prediction by leveraging the few-shot inference power of LLMs to make predictions on cases where traditional supervised learning methods (SLs) may not excel.
Specifically, we develop a collaborative pipeline that combines SLs and LLMs via a confidence-driven decision-making mechanism, leveraging the strengths of SLs in clear-cut cases and LLMs in more complex scenarios. We evaluate this pipeline using a real-world EHR data warehouse from Oregon Health \& Science University (OHSU) Hospital, encompassing EHRs from over 2.5 million patients and more than 20 million patient encounters. Our results show that our proposed approach effectively combines the power of SLs and LLMs, offering significant improvements in predictive performance. This advancement holds promise for revolutionizing ADRD screening and early detection practices, with potential implications for better strategies of patient management and thus improving healthcare.

------------

`[2405.16434] The Importance of Directional Feedback for LLM-based Optimizers <https://arxiv.org/abs/2405.16434>`__

::

    Sun, 26 May 2024 05:22:35 GMT
    Allen Nie, Ching-An Cheng, Andrey Kolobov, Adith Swaminathan

We study the potential of using large language models (LLMs) as an interactive optimizer for solving maximization problems in a text space using natural language and numerical feedback. Inspired by the classical optimization literature, we classify the natural language feedback into directional and non-directional, where the former is a generalization of the first-order feedback to the natural language space. We find that LLMs are especially capable of optimization when they are provided with {directional feedback}.
Based on this insight, we design a new LLM-based optimizer that synthesizes directional feedback from the historical optimization trace to achieve reliable improvement over iterations. Empirically, we show our LLM-based optimizer is more stable and efficient in solving optimization problems, from maximizing mathematical functions to optimizing prompts for writing poems, compared with existing techniques.

------------

`[2405.16567] Automatic Jailbreaking of the Text-to-Image Generative AI Systems <https://arxiv.org/abs/2405.16567>`__

::

    Sun, 26 May 2024 13:32:24 GMT
    Minseon Kim, Hyomin Lee, Boqing Gong, Huishuai Zhang, Sung Ju Hwang

Recent AI systems have shown extremely powerful performance, even surpassing human performance, on various tasks such as information retrieval, language generation, and image generation based on large language models (LLMs). At the same time, there are diverse safety risks that can cause the generation of malicious contents by circumventing the alignment in LLMs, which are often referred to as jailbreaking. However, most of the previous works only focused on the text-based jailbreaking in LLMs, and the jailbreaking of the text-to-image (T2I) generation system has been relatively overlooked. In this paper, we first evaluate the safety of the commercial T2I generation systems, such as ChatGPT, Copilot, and Gemini, on copyright infringement with naive prompts. From this empirical study, we find that Copilot and Gemini block only 12\% and 17\% of the attacks with naive prompts, respectively, while ChatGPT blocks 84\% of them. Then, we further propose a stronger automated jailbreaking pipeline for T2I generation systems, which produces prompts that bypass their safety guards. Our automated jailbreaking framework leverages an LLM optimizer to generate prompts to maximize degree of violation from the generated images without any weight updates or gradient computation. Surprisingly, our simple yet effective approach successfully jailbreaks the ChatGPT with 11.0\% block rate, making it generate copyrighted contents in 76\% of the time. Finally, we explore various defense strategies, such as post-generation filtering and machine unlearning techniques, but found that they were inadequate, which suggests the necessity of stronger defense mechanisms.

------------

`[2405.16588] Attaining Human`s Desirable Outcomes in Human-AI Interaction via Structural Causal Games <https://arxiv.org/abs/2405.16588>`__

::

    Sun, 26 May 2024 14:42:49 GMT
    Anjie Liu, Jianhong Wang, Haoxuan Li, Xu Chen, Jun Wang, Samuel Kaski and Mengyue Yang

In human-AI interaction, a prominent goal is to attain human`s desirable outcome with the assistance of AI agents, which can be ideally delineated as a problem of seeking the optimal Nash Equilibrium that matches the human`s desirable outcome. However, reaching the outcome is usually challenging due to the existence of multiple Nash Equilibria that are related to the assisting task but do not correspond to the human`s desirable outcome. To tackle this issue, we employ a theoretical framework called structural causal game (SCG) to formalize the human-AI interactive process. Furthermore, we introduce a strategy referred to as pre-policy intervention on the SCG to steer AI agents towards attaining the human`s desirable outcome. In more detail, a pre-policy is learned as a generalized intervention to guide the agents` policy selection, under a transparent and interpretable procedure determined by the SCG. To make the framework practical, we propose a reinforcement learning-like algorithm to search out this pre-policy. The proposed algorithm is tested in both gridworld environments and realistic dialogue scenarios with large language models, demonstrating its adaptability in a broader class of problems and potential effectiveness in real-world situations.

------------

`[2405.17044] Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models <https://arxiv.org/abs/2405.17044>`__

::

    Mon, 27 May 2024 11:00:51 GMT
    Xuemei Gu, Mario Krenn

Advanced artificial intelligence (AI) systems with access to millions of research papers could inspire new research ideas that may not be conceived by humans alone. However, how interesting are these AI-generated ideas, and how can we improve their quality? Here, we introduce SciMuse, a system that uses an evolving knowledge graph built from more than 58 million scientific papers to generate personalized research ideas via an interface to GPT-4. We conducted a large-scale human evaluation with over 100 research group leaders from the Max Planck Society, who ranked more than 4,000 personalized research ideas based on their level of interest. This evaluation allows us to understand the relationships between scientific interest and the core properties of the knowledge graph. We find that data-efficient machine learning can predict research interest with high precision, allowing us to optimize the interest-level of generated research ideas. This work represents a step towards an artificial scientific muse that could catalyze unforeseen collaborations and suggest interesting avenues for scientists.

------------

`[2405.17345] Exploring and steering the moral compass of Large Language Models <https://arxiv.org/abs/2405.17345>`__

::

    Mon, 27 May 2024 16:49:22 GMT
    Alejandro Tlaie

Large Language Models (LLMs) have become central to advancing automation and decision-making across various sectors, raising significant ethical questions.
This study proposes a comprehensive comparative analysis of the most advanced LLMs to assess their moral profiles. We subjected several state-of-the-art models to a selection of ethical dilemmas and found that all the proprietary ones are mostly utilitarian and all of the open-weights ones align mostly with values-based ethics. Furthermore, when using the Moral Foundations Questionnaire, all models we probed - except for Llama 2- displayed a strong liberal bias. Lastly, in order to causally intervene in one of the studied models, we propose a novel similarity-specific activation steering technique.
Using this method, we were able to reliably steer the model's moral compass to different ethical schools. All of these results showcase that there is an ethical dimension in already deployed LLMs, an aspect that is generally overlooked.

------------

`[2405.15818] DuanzAI: Slang-Enhanced LLM with Prompt for Humor Understanding <https://arxiv.org/abs/2405.15818>`__

::

    Thu, 23 May 2024 03:13:50 GMT
    Yesian Rohn

Language's complexity is evident in the rich tapestry of slang expressions, often laden with humor and cultural nuances. This linguistic phenomenon has become increasingly prevalent, especially in digital communication. However, existing AI models, including ChatGPT-3.5, face challenges in comprehending these nuances, particularly in Chinese slang. In this study, we present DuanzAI, an innovative approach enhancing Large Language Models (LLMs) with deep Chinese slang comprehension. Leveraging curated datasets and advanced techniques, DuanzAI bridges the gap between human expression and AI comprehension, enabling contextually relevant responses. Our experiments contrast LLMs' performance with a custom Punchline Entity Recognition (PER) system, integrating phonetic matching and pinyin2hanzi techniques. Applying these insights, we developed ChatDAI, an advanced chatbot and released our code at \url{https://github.com/YesianRohn/DuanzAI}.

------------

`[2405.15924] SLIDE: A Framework Integrating Small and Large Language Models for Open-Domain Dialogues Evaluation <https://arxiv.org/abs/2405.15924>`__

::

    Fri, 24 May 2024 20:32:49 GMT
    Kun Zhao, Bohao Yang, Chen Tang, Chenghua Lin, Liang Zhan

The long-standing one-to-many problem of gold standard responses in open-domain dialogue systems presents challenges for automatic evaluation metrics. Though prior works have demonstrated some success by applying powerful Large Language Models (LLMs), existing approaches still struggle with the one-to-many problem, and exhibit subpar performance in domain-specific scenarios. We assume the commonsense reasoning biases within LLMs may hinder their performance in domainspecific evaluations. To address both issues, we propose a novel framework SLIDE (Small and Large Integrated for Dialogue Evaluation), that leverages both a small, specialised model (SLM), and LLMs for the evaluation of open domain dialogues. Our approach introduces several techniques: (1) Contrastive learning to differentiate between robust and non-robust response embeddings; (2) A novel metric for semantic sensitivity that combines embedding cosine distances with similarity learned through neural networks, and (3) a strategy for incorporating the evaluation results from both the SLM and LLMs. Our empirical results demonstrate that our approach achieves state-of-the-art performance in both the classification and evaluation tasks, and additionally the SLIDE evaluator exhibits better correlation with human judgements. Our code is available at https:// github.com/hegehongcha/SLIDE-ACL2024.

------------

`[2405.15936] Zero-Shot Spam Email Classification Using Pre-trained Large Language Models <https://arxiv.org/abs/2405.15936>`__

::

    Fri, 24 May 2024 20:55:49 GMT
    Sergio Rojas-Galeano

This paper investigates the application of pre-trained large language models (LLMs) for spam email classification using zero-shot prompting. We evaluate the performance of both open-source (Flan-T5) and proprietary LLMs (ChatGPT, GPT-4) on the well-known SpamAssassin dataset. Two classification approaches are explored: (1) truncated raw content from email subject and body, and (2) classification based on summaries generated by ChatGPT. Our empirical analysis, leveraging the entire dataset for evaluation without further training, reveals promising results. Flan-T5 achieves a 90% F1-score on the truncated content approach, while GPT-4 reaches a 95% F1-score using summaries. While these initial findings on a single dataset suggest the potential for classification pipelines of LLM-based subtasks (e.g., summarisation and classification), further validation on diverse datasets is necessary. The high operational costs of proprietary models, coupled with the general inference costs of LLMs, could significantly hinder real-world deployment for spam filtering.

------------

`[2405.16042] Incremental Comprehension of Garden-Path Sentences by Large Language Models: Semantic Interpretation, Syntactic Re-Analysis, and Attention <https://arxiv.org/abs/2405.16042>`__

::

    Sat, 25 May 2024 03:36:13 GMT
    Andrew Li, Xianle Feng, Siddhant Narang, Austin Peng, Tianle Cai, Raj Sanjay Shah, Sashank Varma

When reading temporarily ambiguous garden-path sentences, misinterpretations sometimes linger past the point of disambiguation. This phenomenon has traditionally been studied in psycholinguistic experiments using online measures such as reading times and offline measures such as comprehension questions. Here, we investigate the processing of garden-path sentences and the fate of lingering misinterpretations using four large language models (LLMs): GPT-2, LLaMA-2, Flan-T5, and RoBERTa. The overall goal is to evaluate whether humans and LLMs are aligned in their processing of garden-path sentences and in the lingering misinterpretations past the point of disambiguation, especially when extra-syntactic information (e.g., a comma delimiting a clause boundary) is present to guide processing. We address this goal using 24 garden-path sentences that have optional transitive and reflexive verbs leading to temporary ambiguities. For each sentence, there are a pair of comprehension questions corresponding to the misinterpretation and the correct interpretation. In three experiments, we (1) measure the dynamic semantic interpretations of LLMs using the question-answering task; (2) track whether these models shift their implicit parse tree at the point of disambiguation (or by the end of the sentence); and (3) visualize the model components that attend to disambiguating information when processing the question probes. These experiments show promising alignment between humans and LLMs in the processing of garden-path sentences, especially when extra-syntactic information is available to guide processing.

------------

`[2405.16064] Keypoint-based Progressive Chain-of-Thought Distillation for LLMs <https://arxiv.org/abs/2405.16064>`__

::

    Sat, 25 May 2024 05:27:38 GMT
    Kaituo Feng, Changsheng Li, Xiaolu Zhang, Jun Zhou, Ye Yuan, Guoren Wang

Chain-of-thought distillation is a powerful technique for transferring reasoning abilities from large language models (LLMs) to smaller student models. Previous methods typically require the student to mimic the step-by-step rationale produced by LLMs, often facing the following challenges: (i) Tokens within a rationale vary in significance, and treating them equally may fail to accurately mimic keypoint tokens, leading to reasoning errors. (ii) They usually distill knowledge by consistently predicting all the steps in a rationale, which falls short in distinguishing the learning order of step generation. This diverges from the human cognitive progression of starting with easy tasks and advancing to harder ones, resulting in sub-optimal outcomes. To this end, we propose a unified framework, called KPOD, to address these issues.
Specifically, we propose a token weighting module utilizing mask learning to encourage accurate mimicry of keypoint tokens by the student during distillation. Besides, we develop an in-rationale progressive distillation strategy, starting with training the student to generate the final reasoning steps and gradually extending to cover the entire rationale. To accomplish this, a weighted token generation loss is proposed to assess step reasoning difficulty, and a value function is devised to schedule the progressive distillation by considering both step difficulty and question diversity.
Extensive experiments on four reasoning benchmarks illustrate our KPOD outperforms previous methods by a large margin.

------------

`[2405.16129] iREL at SemEval-2024 Task 9: Improving Conventional Prompting Methods for Brain Teasers <https://arxiv.org/abs/2405.16129>`__

::

    Sat, 25 May 2024 08:50:51 GMT
    Harshit Gupta, Manav Chaudhary, Tathagata Raha, Shivansh Subramanian and Vasudeva Varma

This paper describes our approach for SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense. The BRAINTEASER task comprises multiple-choice Question Answering designed to evaluate the models' lateral thinking capabilities. It consists of Sentence Puzzle and Word Puzzle subtasks that require models to defy default common-sense associations and exhibit unconventional thinking. We propose a unique strategy to improve the performance of pre-trained language models, notably the Gemini 1.0 Pro Model, in both subtasks. We employ static and dynamic few-shot prompting techniques and introduce a model-generated reasoning strategy that utilizes the LLM's reasoning capabilities to improve performance. Our approach demonstrated significant improvements, showing that it performed better than the baseline models by a considerable margin but fell short of performing as well as the human annotators, thus highlighting the efficacy of the proposed strategies.

------------

`[2405.16150] 5W1H Extraction With Large Language Models <https://arxiv.org/abs/2405.16150>`__

::

    Sat, 25 May 2024 09:42:58 GMT
    Yang Cao, Yangsong Lan, Feiyan Zhai, Piji Li

The extraction of essential news elements through the 5W1H framework (\textit{What}, \textit{When}, \textit{Where}, \textit{Why}, \textit{Who}, and \textit{How}) is critical for event extraction and text summarization. The advent of Large language models (LLMs) such as ChatGPT presents an opportunity to address language-related tasks through simple prompts without fine-tuning models with much time. While ChatGPT has encountered challenges in processing longer news texts and analyzing specific attributes in context, especially answering questions about \textit{What}, \textit{Why}, and \textit{How}. The effectiveness of extraction tasks is notably dependent on high-quality human-annotated datasets. However, the absence of such datasets for the 5W1H extraction increases the difficulty of fine-tuning strategies based on open-source LLMs. To address these limitations, first, we annotate a high-quality 5W1H dataset based on four typical news corpora (\textit{CNN/DailyMail}, \textit{XSum}, \textit{NYT}, \textit{RA-MDS}); second, we design several strategies from zero-shot/few-shot prompting to efficient fine-tuning to conduct 5W1H aspects extraction from the original news documents. The experimental results demonstrate that the performance of the fine-tuned models on our labelled dataset is superior to the performance of ChatGPT. Furthermore, we also explore the domain adaptation capability by testing the source-domain (e.g. NYT) models on the target domain corpus (e.g.
CNN/DailyMail) for the task of 5W1H extraction.

------------

`[2405.16229] No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks <https://arxiv.org/abs/2405.16229>`__

::

    Sat, 25 May 2024 13:38:40 GMT
    Chak Tou Leong, Yi Cheng, Kaishuai Xu, Jian Wang, Hanlin Wang, Wenjie Li

The existing safety alignment of Large Language Models (LLMs) is found fragile and could be easily attacked through different strategies, such as through fine-tuning on a few harmful examples or manipulating the prefix of the generation results. However, the attack mechanisms of these strategies are still underexplored. In this paper, we ask the following question: \textit{while these approaches can all significantly compromise safety, do their attack mechanisms exhibit strong similarities?} To answer this question, we break down the safeguarding process of an LLM when encountered with harmful instructions into three stages: (1) recognizing harmful instructions, (2) generating an initial refusing tone, and (3) completing the refusal response.
Accordingly, we investigate whether and how different attack strategies could influence each stage of this safeguarding process. We utilize techniques such as logit lens and activation patching to identify model components that drive specific behavior, and we apply cross-model probing to examine representation shifts after an attack. In particular, we analyze the two most representative types of attack approaches: Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA). Surprisingly, we find that their attack mechanisms diverge dramatically. Unlike ISA, EHA tends to aggressively target the harmful recognition stage. While both EHA and ISA disrupt the latter two stages, the extent and mechanisms of their attacks differ significantly. Our findings underscore the importance of understanding LLMs' internal safeguarding process and suggest that diverse defense mechanisms are required to effectively cope with various types of attacks.

------------

`[2405.16277] Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge <https://arxiv.org/abs/2405.16277>`__

::

    Sat, 25 May 2024 15:28:22 GMT
    Brendan Park, Madeline Janecek, Naser Ezzati-Jivan, Yifeng Li, and Ali Emami

Large Language Models (LLMs) have demonstrated remarkable success in tasks like the Winograd Schema Challenge (WSC), showcasing advanced textual common-sense reasoning. However, applying this reasoning to multimodal domains, where understanding text and images together is essential, remains a substantial challenge. To address this, we introduce WinoVis, a novel dataset specifically designed to probe text-to-image models on pronoun disambiguation within multimodal contexts. Utilizing GPT-4 for prompt generation and Diffusion Attentive Attribution Maps (DAAM) for heatmap analysis, we propose a novel evaluation framework that isolates the models' ability in pronoun disambiguation from other visual processing challenges. Evaluation of successive model versions reveals that, despite incremental advancements, Stable Diffusion 2.0 achieves a precision of 56.7% on WinoVis, only marginally surpassing random guessing. Further error analysis identifies important areas for future research aimed at advancing text-to-image models in their ability to interpret and interact with the complex visual world.

------------

`[2405.16281] ConStat: Performance-Based Contamination Detection in Large Language Models <https://arxiv.org/abs/2405.16281>`__

::

    Sat, 25 May 2024 15:36:37 GMT
    Jasper Dekoninck, Mark Niklas M\"uller, Martin Vechev

Public benchmarks play an essential role in the evaluation of large language models. However, data contamination can lead to inflated performance, rendering them unreliable for model comparison. It is therefore crucial to detect contamination and estimate its impact on measured performance. Unfortunately, existing detection methods can be easily evaded and fail to quantify contamination. To overcome these limitations, we propose a novel definition of contamination as artificially inflated and non-generalizing benchmark performance instead of the inclusion of benchmark samples in the training data.
This perspective enables us to detect any model with inflated performance, i.e., performance that does not generalize to rephrased samples, synthetic samples from the same distribution, or different benchmarks for the same task.
Based on this insight, we develop ConStat, a statistical method that reliably detects and quantifies contamination by comparing performance between a primary and reference benchmark relative to a set of reference models. We demonstrate the effectiveness of ConStat in an extensive evaluation of diverse model architectures, benchmarks, and contamination scenarios and find high levels of contamination in multiple popular models including Mistral, Llama, Yi, and the top-3 Open LLM Leaderboard models.

------------

`[2405.16282] Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models <https://arxiv.org/abs/2405.16282>`__

::

    Sat, 25 May 2024 15:42:04 GMT
    Abhishek Kumar, Robert Morabito, Sanzhar Umbet, Jad Kabbara, and Ali Emami

As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM's internal confidence, quantified by token probabilities, to the confidence conveyed in the model's response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model's confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI's GPT-4 showed the strongest confidence-probability alignment, with an average Spearman's $\hat{\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness.

------------

`[2405.16284] Generating clickbait spoilers with an ensemble of large language models <https://arxiv.org/abs/2405.16284>`__

::

    Sat, 25 May 2024 15:49:08 GMT
    Mateusz Wo\'zny, Mateusz Lango

Clickbait posts are a widespread problem in the webspace. The generation of spoilers, i.e. short texts that neutralize clickbait by providing information that satisfies the curiosity induced by it, is one of the proposed solutions to the problem. Current state-of-the-art methods are based on passage retrieval or question answering approaches and are limited to generating spoilers only in the form of a phrase or a passage. In this work, we propose an ensemble of fine-tuned large language models for clickbait spoiler generation. Our approach is not limited to phrase or passage spoilers, but is also able to generate multipart spoilers that refer to several non-consecutive parts of text.
Experimental evaluation demonstrates that the proposed ensemble model outperforms the baselines in terms of BLEU, METEOR and BERTScore metrics.

------------

`[2405.16295] Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data <https://arxiv.org/abs/2405.16295>`__

::

    Sat, 25 May 2024 16:16:22 GMT
    Yuhao Chen, Zhimu Wang, Bo Wen, Farhana Zulkernine

Unstructured text in medical notes and dialogues contains rich information.
Recent advancements in Large Language Models (LLMs) have demonstrated superior performance in question answering and summarization tasks on unstructured text data, outperforming traditional text analysis approaches. However, there is a lack of scientific studies in the literature that methodically evaluate and report on the performance of different LLMs, specifically for domain-specific data such as medical chart notes. We propose an evaluation approach to analyze the performance of open-source LLMs such as Llama2 and Mistral for medical summarization tasks, using GPT-4 as an assessor. Our innovative approach to quantitative evaluation of LLMs can enable quality control, support the selection of effective LLMs for specific tasks, and advance knowledge discovery in digital health.

------------

`[2405.16388] Multi-Reference Preference Optimization for Large Language Models <https://arxiv.org/abs/2405.16388>`__

::

    Sun, 26 May 2024 00:29:04 GMT
    Hung Le, Quan Tran, Dung Nguyen, Kien Do, Saloni Mittal, Kelechi Ogueji, Svetha Venkatesh

How can Large Language Models (LLMs) be aligned with human intentions and values? A typical solution is to gather human preference on model outputs and finetune the LLMs accordingly while ensuring that updates do not deviate too far from a reference model. Recent approaches, such as direct preference optimization (DPO), have eliminated the need for unstable and sluggish reinforcement learning optimization by introducing close-formed supervised losses. However, a significant limitation of the current approach is its design for a single reference model only, neglecting to leverage the collective power of numerous pretrained LLMs. To overcome this limitation, we introduce a novel closed-form formulation for direct preference optimization using multiple reference models. The resulting algorithm, Multi-Reference Preference Optimization (MRPO), leverages broader prior knowledge from diverse reference models, substantially enhancing preference learning capabilities compared to the single-reference DPO. Our experiments demonstrate that LLMs finetuned with MRPO generalize better in various preference data, regardless of data scarcity or abundance. Furthermore, MRPO effectively finetunes LLMs to exhibit superior performance in several downstream natural language processing tasks such as GSM8K and TruthfulQA.

------------

`[2405.16402] Assessing Empathy in Large Language Models with Real-World Physician-Patient Interactions <https://arxiv.org/abs/2405.16402>`__

::

    Sun, 26 May 2024 01:58:57 GMT
    Man Luo, Christopher J. Warren, Lu Cheng, Haidar M. Abdul-Muhsin, Imon Banerjee

The integration of Large Language Models (LLMs) into the healthcare domain has the potential to significantly enhance patient care and support through the development of empathetic, patient-facing chatbots. This study investigates an intriguing question Can ChatGPT respond with a greater degree of empathy than those typically offered by physicians? To answer this question, we collect a de-identified dataset of patient messages and physician responses from Mayo Clinic and generate alternative replies using ChatGPT. Our analyses incorporate novel empathy ranking evaluation (EMRank) involving both automated metrics and human assessments to gauge the empathy level of responses. Our findings indicate that LLM-powered chatbots have the potential to surpass human physicians in delivering empathetic communication, suggesting a promising avenue for enhancing patient care and reducing professional burnout. The study not only highlights the importance of empathy in patient interactions but also proposes a set of effective automatic empathy ranking metrics, paving the way for the broader adoption of LLMs in healthcare.

------------

`[2405.16412] KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge <https://arxiv.org/abs/2405.16412>`__

::

    Sun, 26 May 2024 03:04:26 GMT
    Pengcheng Jiang, Lang Cao, Cao Xiao, Parminder Bhatia, Jimeng Sun, Jiawei Han

Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of 14.4%, 13.5%, and 11.9% in the Hits@10 metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of 12.6%, 6.7%, and 17.7% compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.

------------

`[2405.16433] CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling <https://arxiv.org/abs/2405.16433>`__

::

    Sun, 26 May 2024 05:18:00 GMT
    Chenhao Zhang, Renhao Li, Minghuan Tan, Min Yang, Jingwei Zhu, Di Yang, Jiahao Zhao, Guancheng Ye, Chengming Li, Xiping Hu, Derek F. Wong

Using large language models (LLMs) to assist psychological counseling is a significant but challenging task at present. Attempts have been made on improving empathetic conversations or acting as effective assistants in the treatment with LLMs. However, the existing datasets lack consulting knowledge, resulting in LLMs lacking professional consulting competence. Moreover, how to automatically evaluate multi-turn dialogues within the counseling process remains an understudied area. To bridge the gap, we propose CPsyCoun, a report-based multi-turn dialogue reconstruction and evaluation framework for Chinese psychological counseling. To fully exploit psychological counseling reports, a two-phase approach is devised to construct high-quality dialogues while a comprehensive evaluation benchmark is developed for the effective automatic evaluation of multi-turn psychological consultations. Competitive experimental results demonstrate the effectiveness of our proposed framework in psychological counseling. We open-source the datasets and model for future research at https://github.com/CAS-SIAT-XinHai/CPsyCoun

------------

`[2405.16482] DarijaBanking: A New Resource for Overcoming Language Barriers in Banking Intent Detection for Moroccan Arabic Speakers <https://arxiv.org/abs/2405.16482>`__

::

    Sun, 26 May 2024 08:33:28 GMT
    Abderrahman Skiredj, Ferdaous Azhari, Ismail Berrada, Saad Ezzini

Navigating the complexities of language diversity is a central challenge in developing robust natural language processing systems, especially in specialized domains like banking. The Moroccan Dialect (Darija) serves as the common language that blends cultural complexities, historical impacts, and regional differences. The complexities of Darija present a special set of challenges for language models, as it differs from Modern Standard Arabic with strong influence from French, Spanish, and Tamazight, it requires a specific approach for effective communication. To tackle these challenges, this paper introduces \textbf{DarijaBanking}, a novel Darija dataset aimed at enhancing intent classification in the banking domain, addressing the critical need for automatic banking systems (e.g., chatbots) that communicate in the native language of Moroccan clients. DarijaBanking comprises over 1,800 parallel high-quality queries in Darija, Modern Standard Arabic (MSA), English, and French, organized into 24 intent classes. We experimented with various intent classification methods, including full fine-tuning of monolingual and multilingual models, zero-shot learning, retrieval-based approaches, and Large Language Model prompting. One of the main contributions of this work is BERTouch, our BERT-based language model for intent classification in Darija.
BERTouch achieved F1-scores of 0.98 for Darija and 0.96 for MSA on DarijaBanking, outperforming the state-of-the-art alternatives including GPT-4 showcasing its effectiveness in the targeted application.

------------

`[2405.16571] A Preliminary Empirical Study on Prompt-based Unsupervised Keyphrase Extraction <https://arxiv.org/abs/2405.16571>`__

::

    Sun, 26 May 2024 13:37:57 GMT
    Mingyang Song, Yi Feng, Liping Jing

Pre-trained large language models can perform natural language processing downstream tasks by conditioning on human-designed prompts. However, a prompt-based approach often requires "prompt engineering" to design different prompts, primarily hand-crafted through laborious trial and error, requiring human intervention and expertise. It is a challenging problem when constructing a prompt-based keyphrase extraction method. Therefore, we investigate and study the effectiveness of different prompts on the keyphrase extraction task to verify the impact of the cherry-picked prompts on the performance of extracting keyphrases. Extensive experimental results on six benchmark keyphrase extraction datasets and different pre-trained large language models demonstrate that (1) designing complex prompts may not necessarily be more effective than designing simple prompts; (2) individual keyword changes in the designed prompts can affect the overall performance; (3) designing complex prompts achieve better performance than designing simple prompts when facing long documents.

------------

`[2405.16579] Automatically Generating Numerous Context-Driven SFT Data for LLMs across Diverse Granularity <https://arxiv.org/abs/2405.16579>`__

::

    Sun, 26 May 2024 14:14:18 GMT
    Shanghaoran Quan

Constructing high-quality query-response pairs from custom corpus is crucial for supervised fine-tuning (SFT) large language models (LLMs) in many applications, like creating domain-specific AI assistants or roleplaying agents. However, sourcing this data through human annotation is costly, and existing automated methods often fail to capture the diverse range of contextual granularity and tend to produce homogeneous data. To tackle these issues, we introduce a novel method named AugCon, capable of automatically generating context-driven SFT data across multiple levels of granularity with high diversity, quality and fidelity. AugCon begins by generating queries using the Context-Split-Tree (CST), an innovative approach for recursively deriving queries and splitting context to cover full granularity. Then, we train a scorer through contrastive learning to collaborate with CST to rank and refine queries. Finally, a synergistic integration of self-alignment and self-improving is introduced to obtain high-fidelity responses.
Extensive experiments are conducted incorporating both human and automatic evaluations, encompassing a test scenario and four widely-used benchmarks in English and Chinese. The results highlight the significant advantages of AugCon in producing high diversity, quality, and fidelity SFT data against several state-of-the-art methods. All of our code, dataset, and fine-tuned model will be available at: https://github.com/quanshr/AugCon.

------------

`[2405.16631] Let Silence Speak: Enhancing Fake News Detection with Generated Comments from Large Language Models <https://arxiv.org/abs/2405.16631>`__

::

    Sun, 26 May 2024 17:09:23 GMT
    Qiong Nan, Qiang Sheng, Juan Cao, Beizhe Hu, Danding Wang, Jintao Li

Fake news detection plays a crucial role in protecting social media users and maintaining a healthy news ecosystem. Among existing works, comment-based fake news detection methods are empirically shown as promising because comments could reflect users' opinions, stances, and emotions and deepen models' understanding of fake news. Unfortunately, due to exposure bias and users' different willingness to comment, it is not easy to obtain diverse comments in reality, especially for early detection scenarios. Without obtaining the comments from the ``silent'' users, the perceived opinions may be incomplete, subsequently affecting news veracity judgment. In this paper, we explore the possibility of finding an alternative source of comments to guarantee the availability of diverse comments, especially those from silent users.
Specifically, we propose to adopt large language models (LLMs) as a user simulator and comment generator, and design GenFEND, a generated feedback-enhanced detection framework, which generates comments by prompting LLMs with diverse user profiles and aggregating generated comments from multiple subpopulation groups. Experiments demonstrate the effectiveness of GenFEND and further analysis shows that the generated comments cover more diverse users and could even be more effective than actual comments.

------------

`[2405.16661] RLSF: Reinforcement Learning via Symbolic Feedback <https://arxiv.org/abs/2405.16661>`__

::

    Sun, 26 May 2024 18:49:59 GMT
    Piyush Jha, Prithwish Jana, Arnav Arora, Vijay Ganesh

In recent years, large language models (LLMs) have had a dramatic impact on various sub-fields of AI, most notably on natural language understanding tasks.
However, there is widespread agreement that the logical reasoning capabilities of contemporary LLMs are, at best, fragmentary (i.e., may work well on some problem instances but fail dramatically on others). While traditional LLM fine-tuning approaches (e.g., those that use human feedback) do address this problem to some degree, they suffer from many issues, including unsound black-box reward models, difficulties in collecting preference data, and sparse scalar reward values.
To address these challenges, we propose a new training/fine-tuning paradigm we refer to as Reinforcement Learning via Symbolic Feedback (RLSF), which is aimed at enhancing the reasoning capabilities of LLMs. In the RLSF setting, the LLM that is being trained/fine-tuned is considered as the RL agent, while the environment is allowed access to reasoning or domain knowledge tools (e.g., solvers, algebra systems). Crucially, in RLSF, these reasoning tools can provide feedback to the LLMs via poly-sized certificates (e.g., proofs), that characterize errors in the LLM-generated object with respect to some correctness specification. The ability of RLSF-based training/fine-tuning to leverage certificate-generating symbolic tools enables sound fine-grained (token-level) reward signals to LLMs, and thus addresses the limitations of traditional reward models mentioned above. Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs outperforms traditional approaches on two different applications, namely, program synthesis from natural language pseudo-code to programming language (C++) and solving the Game of 24.

------------

`[2405.16681] Triple Preference Optimization: Achieving Better Alignment with Less Data in a Single Step Optimization <https://arxiv.org/abs/2405.16681>`__

::

    Sun, 26 May 2024 20:18:11 GMT
    Amir Saeidi, Shivanshu Verma, Aswin RRV and Chitta Baral

Large Language Models (LLMs) perform well across diverse tasks, but aligning them with human demonstrations is challenging. Recently, Reinforcement Learning (RL)-free methods like Direct Preference Optimization (DPO) have emerged, offering improved stability and scalability while retaining competitive performance relative to RL-based methods. However, while RL-free methods deliver satisfactory performance, they require significant data to develop a robust Supervised Fine-Tuned (SFT) model and an additional step to fine-tune this model on a preference dataset, which constrains their utility and scalability. In this paper, we introduce Triple Preference Optimization (TPO), a new preference learning method designed to align an LLM with three preferences without requiring a separate SFT step and using considerably less data. Through a combination of practical experiments and theoretical analysis, we show the efficacy of TPO as a single-step alignment strategy. Specifically, we fine-tuned the Phi-2 (2.7B) and Mistral (7B) models using TPO directly on the UltraFeedback dataset, achieving superior results compared to models aligned through other methods such as SFT, DPO, KTO, IPO, CPO, and ORPO.
Moreover, the performance of TPO without the SFT component led to notable improvements in the MT-Bench score, with increases of +1.27 and +0.63 over SFT and DPO, respectively. Additionally, TPO showed higher average accuracy, surpassing DPO and SFT by 4.2% and 4.97% on the Open LLM Leaderboard benchmarks. Our code is publicly available at https://github.com/sahsaeedi/triple-preference-optimization .

------------

`[2405.16702] Accurate and Nuanced Open-QA Evaluation Through Textual Entailment <https://arxiv.org/abs/2405.16702>`__

::

    Sun, 26 May 2024 21:33:27 GMT
    Peiran Yao, Denilson Barbosa

Open-domain question answering (Open-QA) is a common task for evaluating large language models (LLMs). However, current Open-QA evaluations are criticized for the ambiguity in questions and the lack of semantic understanding in evaluators. Complex evaluators, powered by foundation models or LLMs and pertaining to semantic equivalence, still deviate from human judgments by a large margin. We propose to study the entailment relations of answers to identify more informative and more general system answers, offering a much closer evaluation to human judgment on both NaturalQuestions and TriviaQA while being learning-free. The entailment-based evaluation we propose allows the assignment of bonus or partial marks by quantifying the inference gap between answers, enabling a nuanced ranking of answer correctness that has higher AUC than current methods.

------------

`[2405.16714] Crafting Interpretable Embeddings by Asking LLMs Questions <https://arxiv.org/abs/2405.16714>`__

::

    Sun, 26 May 2024 22:30:29 GMT
    Vinamra Benara, Chandan Singh, John X. Morris, Richard Antonello, Ion Stoica, Alexander G. Huth, Jianfeng Gao

Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks. However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability. Here, we ask whether we can obtain interpretable embeddings through LLM prompting. We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights.
We use QA-Emb to flexibly generate interpretable models for predicting fMRI voxel responses to language stimuli. QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions. This paves the way towards building flexible feature spaces that can concretize and evaluate our understanding of semantic brain representations. We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks.

------------

`[2405.16720] Large Scale Knowledge Washing <https://arxiv.org/abs/2405.16720>`__

::

    Sun, 26 May 2024 23:29:49 GMT
    Yu Wang, Ruihan Wu, Zexue He, Xiusi Chen, Julian McAuley

Large language models show impressive abilities in memorizing world knowledge, which leads to concerns regarding memorization of private information, toxic or sensitive knowledge, and copyrighted content. We introduce the problem of Large Scale Knowledge Washing, focusing on "unlearning" extensive amounts of factual knowledge. Previous unlearning methods usually define the reverse loss and update the model via backpropagation, which may affect the model's fluency and reasoning ability or even destroy the model due to extensive training with the reverse loss.
Existing works introduce additional data from downstream tasks to prevent the model from losing capabilities, which requires downstream task awareness.
Controlling the tradeoff of unlearning and maintaining existing capabilities is also challenging. To this end, we propose LAW (Large Scale Washing) to update the MLP layers in decoder-only large language models to perform knowledge washing, as inspired by model editing methods and based on the hypothesis that knowledge and reasoning are disentanglable. We derive a new objective with the knowledge to be unlearned to update the weights of certain MLP layers.
Experimental results demonstrate the effectiveness of LAW in forgetting target knowledge while maintaining reasoning ability. The code will be open-sourced at https://github.com/wangyu-ustc/LargeScaleWashing.

------------

`[2405.16806] Entity Alignment with Noisy Annotations from Large Language Models <https://arxiv.org/abs/2405.16806>`__

::

    Mon, 27 May 2024 03:52:55 GMT
    Shengyuan Chen, Qinggang Zhang, Junnan Dong, Wen Hua, Qing Li, Xiao Huang

Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs. While existing methods heavily rely on human-generated labels, it is prohibitively expensive to incorporate cross-domain experts for annotation in real-world scenarios. The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information. However, it is nontrivial to directly apply LLMs for EA since the annotation space in real-world KGs is large. LLMs could also generate noisy labels that may mislead the alignment. To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA. Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure.
Moreover, we introduce an unsupervised label refiner to continuously enhance label accuracy through in-depth probabilistic reasoning. We iteratively optimize the policy based on the feedback from a base EA model. Extensive experiments demonstrate the advantages of LLM4EA on four benchmark datasets in terms of effectiveness, robustness, and efficiency.

------------

`[2405.16821] Perturbation-Restrained Sequential Model Editing <https://arxiv.org/abs/2405.16821>`__

::

    Mon, 27 May 2024 04:40:56 GMT
    Jun-Yu Ma, Hong Wang, Hao-Xiang Xu, Zhen-Hua Ling, Jia-Chen Gu

Model editing is an emerging field that focuses on updating the knowledge embedded within large language models (LLMs) without extensive retraining.
However, current model editing methods significantly compromise the general abilities of LLMs as the number of edits increases, and this trade-off poses a substantial challenge to the continual learning of LLMs. In this paper, we first theoretically analyze that the factor affecting the general abilities in sequential model editing lies in the condition number of the edited matrix. The condition number of a matrix represents its numerical sensitivity, and therefore can be used to indicate the extent to which the original knowledge associations stored in LLMs are perturbed after editing. Subsequently, statistical findings demonstrate that the value of this factor becomes larger as the number of edits increases, thereby exacerbating the deterioration of general abilities. To this end, a framework termed Perturbation Restraint on Upper bouNd for Editing (PRUNE) is proposed, which applies the condition number restraints in sequential editing. These restraints can lower the upper bound on perturbation to edited models, thus preserving the general abilities.
Systematically, we conduct experiments employing three popular editing methods on three LLMs across four representative downstream tasks. Evaluation results show that PRUNE can preserve considerable general abilities while maintaining the editing performance effectively in sequential model editing. The code and data are available at https://github.com/mjy1111/PRUNE.

------------

`[2405.16856] Can We Trust LLMs? Mitigate Overconfidence Bias in LLMs through Knowledge Transfer <https://arxiv.org/abs/2405.16856>`__

::

    Mon, 27 May 2024 06:06:36 GMT
    Haoyan Yang, Yixuan Wang, Xingyin Xu, Hanyuan Zhang, Yirong Bian

The study explores mitigating overconfidence bias in LLMs to improve their reliability. We introduce a knowledge transfer (KT) method utilizing chain of thoughts, where "big" LLMs impart knowledge to "small" LLMs via detailed, sequential reasoning paths. This method uses advanced reasoning of larger models to fine-tune smaller models, enabling them to produce more accurate predictions with calibrated confidence. Experimental evaluation using multiple-choice questions and sentiment analysis across diverse datasets demonstrated the KT method's superiority over the vanilla and question-answer pair (QA) fine-tuning methods. The most significant improvement in three key metrics, where the KT method outperformed the vanilla and QA methods by an average of 55.3% and 43.1%, respectively. These findings underscore the KT method's potential in enhancing model trustworthiness and accuracy, offering precise outputs with well-matched confidence levels across various contexts.

------------

`[2405.16884] Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching <https://arxiv.org/abs/2405.16884>`__

::

    Mon, 27 May 2024 07:05:27 GMT
    Tianshu Wang, Hongyu Lin, Xiaoyang Chen, Xianpei Han, Hao Wang, Zhenyu Zeng, Le Sun

Entity matching (EM) is a critical step in entity resolution. Recently, entity matching based on large language models (LLMs) has shown great promise.
However, current LLM-based entity matching approaches typically follow a binary matching paradigm that ignores the global consistency between different records. In this paper, we investigate various methodologies for LLM-based entity matching that incorporate record interactions from different perspectives. Specifically, we comprehensively compare three representative strategies: matching, comparing, and selecting, and analyze their respective advantages and challenges in diverse scenarios. Based on our findings, we further design a compositional entity matching (ComEM) framework that leverages the composition of multiple strategies and LLMs. In this way, ComEM can benefit from the advantages of different sides and achieve improvements in both effectiveness and efficiency. Experimental results show that ComEM not only achieves significant performance gains on various datasets but also reduces the cost of LLM-based entity matching in real-world application.

------------

`[2405.16908] Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words? <https://arxiv.org/abs/2405.16908>`__

::

    Mon, 27 May 2024 07:56:23 GMT
    Gal Yona, Roee Aharoni, Mor Geva

We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response should reflect this uncertainty by hedging its answer (e.g., "I'm not sure, but I think..."). We formalize faithful response uncertainty based on the gap between the model's intrinsic confidence in the assertions it makes and the decisiveness by which they are conveyed. This example-level metric reliably indicates whether the model reflects its uncertainty, as it penalizes both excessive and insufficient hedging. We evaluate a variety of aligned LLMs at faithfully communicating uncertainty on several knowledge-intensive question answering tasks. Our results provide strong evidence that modern LLMs are poor at faithfully conveying their uncertainty, and that better alignment is necessary to improve their trustworthiness.

------------

`[2405.16964] Exploring the LLM Journey from Cognition to Expression with Linear Representations <https://arxiv.org/abs/2405.16964>`__

::

    Mon, 27 May 2024 08:57:04 GMT
    Yuzi Yan, Jialian Li, Yipin Zhang and Dong Yan

This paper presents an in-depth examination of the evolution and interplay of cognitive and expressive capabilities in large language models (LLMs), with a specific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese and English) LLM series. We define and explore the model's cognitive and expressive capabilities through linear representations across three critical phases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Cognitive capability is defined as the quantity and quality of information conveyed by the neuron output vectors within the network, similar to the neural signal processing in human cognition. Expressive capability is defined as the model's capability to produce word-level output.
Our findings unveil a sequential development pattern, where cognitive abilities are largely established during Pretraining, whereas expressive abilities predominantly advance during SFT and RLHF. Statistical analyses confirm a significant correlation between the two capabilities, suggesting that cognitive capacity may limit expressive potential. The paper also explores the theoretical underpinnings of these divergent developmental trajectories and their connection to the LLMs' architectural design. Moreover, we evaluate various optimization-independent strategies, such as few-shot learning and repeated sampling, which bridge the gap between cognitive and expressive capabilities. This research reveals the potential connection between the hidden space and the output space, contributing valuable insights into the interpretability and controllability of their training processes.

------------

`[2405.17039] BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation <https://arxiv.org/abs/2405.17039>`__

::

    Mon, 27 May 2024 10:45:49 GMT
    Chengxing Jia, Pengyuan Wang, Ziniu Li, Yi-Chen Li, Zhilong Zhang, Nan Tang, Yang Yu

Large language models (LLMs) have catalyzed a paradigm shift in natural language processing, yet their limited controllability poses a significant challenge for downstream applications. We aim to address this by drawing inspiration from the neural mechanisms of the human brain, specifically Broca's and Wernicke's areas, which are crucial for language generation and comprehension, respectively. In particular, Broca's area receives cognitive decision signals from Wernicke's area, treating the language generation as an intricate decision-making process, which differs from the fully auto-regressive language generation of existing LLMs. In a similar vein, our proposed system, the BWArea model, conceptualizes language generation as a decision-making task.
This model has three components: a language world model, an inverse dynamics model, and a cognitive policy. Like Wernicke's area, the inverse dynamics model is designed to deduce the underlying cognitive intentions, or latent actions, behind each token. The BWArea model is amenable to both pre-training and fine-tuning like existing LLMs. With 30B clean pre-training tokens, we have trained a BWArea model, which achieves competitive performance with LLMs of equal size (1B parameters). Unlike fully auto-regressive LLMs, its pre-training performance does not degenerate if dirty data unintentionally appears. This shows the advantage of a decomposed structure of BWArea model in reducing efforts in laborious data selection and labeling. Finally, we reveal that the BWArea model offers enhanced controllability via fine-tuning the cognitive policy with downstream reward metrics, thereby facilitating alignment with greater simplicity. On 9 out of 10 tasks from two suites, TextWorld and BigBench Hard, our method shows superior performance to auto-regressive LLMs.

------------

`[2405.17052] SelfCP: Compressing Long Prompt to 1/12 Using the Frozen Large Language Model Itself <https://arxiv.org/abs/2405.17052>`__

::

    Mon, 27 May 2024 11:14:55 GMT
    Jun Gao

Long prompt leads to huge hardware costs when using Large Language Models (LLMs). Unfortunately, many tasks, such as summarization, inevitably introduce long task-inputs, and the wide application of in-context learning easily makes the prompt length explode. Inspired by the language understanding ability of LLMs, this paper proposes SelfCP, which uses the LLM \textbf{itself} to \textbf{C}ompress long \textbf{P}rompt into compact virtual tokens. SelfCP applies a general frozen LLM twice, first as an encoder to compress the prompt and then as a decoder to generate responses. Specifically, given a long prompt, we place special tokens within the lengthy segment for compression and signal the LLM to generate $k$ virtual tokens. Afterward, the virtual tokens concatenate with the uncompressed prompt and are fed into the same LLM to generate the response. In general, SelfCP facilitates the unconditional and conditional compression of prompts, fitting both standard tasks and those with specific objectives. Since the encoder and decoder are frozen, SelfCP only contains 17M trainable parameters and allows for convenient adaptation across various backbones. We implement SelfCP with two LLM backbones and evaluate it in both in- and out-domain tasks. Results show that the compressed virtual tokens can substitute $12 \times$ larger original prompts effectively

------------

`[2405.17057] ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation <https://arxiv.org/abs/2405.17057>`__

::

    Mon, 27 May 2024 11:27:00 GMT
    Houxing Ren, Mingjie Zhan, Zhongyuan Wu, Aojun Zhou, Junting Pan, Hongsheng Li

Code generation plays a crucial role in various tasks, such as code auto-completion and mathematical reasoning. Previous work has proposed numerous methods to enhance code generation performance, including integrating feedback from the compiler. Inspired by this, we present ReflectionCoder, a novel approach that effectively leverages reflection sequences constructed by integrating compiler feedback to improve one-off code generation performance.
Furthermore, we propose reflection self-distillation and dynamically masked distillation to effectively utilize these reflection sequences. Extensive experiments on three benchmarks, i.e., HumanEval (+), MBPP (+), and MultiPl-E, demonstrate that models fine-tuned with our method achieve state-of-the-art performance. Notably, ReflectionCoder-DeepSeek-Coder-33B reaches pass@1 of 82.9 (76.8) on HumanEval (+) and 84.1 (72.0) on MBPP (+), on par with GPT-3.5-Turbo and Claude-3-opus, and surpasses early GPT-4. Beyond the code domain, we believe this approach can benefit other domains that focus on final results and require long reasoning paths. Code and data are available at https://github.com/SenseLLM/ReflectionCoder.

------------

`[2405.17067] Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization <https://arxiv.org/abs/2405.17067>`__

::

    Mon, 27 May 2024 11:39:59 GMT
    Dixuan Wang, Yanda Li, Junyuan Jiang, Zepeng Ding, Guochao Jiang, Jiaqing Liang, Deqing Yang

Large Language Models (LLMs) have shown remarkable capabilities in language understanding and generation. Nonetheless, it was also witnessed that LLMs tend to produce inaccurate responses to specific queries. This deficiency can be traced to the tokenization step LLMs must undergo, which is an inevitable limitation inherent to all LLMs. In fact, incorrect tokenization is the critical point that hinders LLMs in understanding the input precisely, thus leading to unsatisfactory output. To demonstrate this flaw of LLMs, we construct an adversarial dataset, named as $\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which draws upon the vocabularies of various open-source LLMs to challenge LLMs' tokenization. ADT consists of two subsets: the manually constructed ADT-Human and the automatically generated ADT-Auto. Our empirical results reveal that our ADT is highly effective on challenging the tokenization of leading LLMs, including GPT-4o, Llama-3, Qwen2.5-max and so on, thus degrading these LLMs' capabilities. Moreover, our method of automatic data generation has been proven efficient and robust, which can be applied to any open-source LLMs. To the best of our knowledge, our study is the first to investigating LLMs' vulnerability in terms of challenging their token segmentation, which will shed light on the subsequent research of improving LLMs' capabilities through optimizing their tokenization process and algorithms.

------------

`[2405.17103] Empowering Character-level Text Infilling by Eliminating Sub-Tokens <https://arxiv.org/abs/2405.17103>`__

::

    Mon, 27 May 2024 12:21:48 GMT
    Houxing Ren, Mingjie Zhan, Zhongyuan Wu, Hongsheng Li

In infilling tasks, sub-tokens, representing instances where a complete token is segmented into two parts, often emerge at the boundaries of prefixes, middles, and suffixes. Traditional methods focused on training models at the token level, leading to sub-optimal performance in character-level infilling tasks during the inference stage. Alternately, some approaches considered character-level infilling, but they relied on predicting sub-tokens in inference, yet this strategy diminished ability in character-level infilling tasks due to the large perplexity of the model on sub-tokens. In this paper, we introduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and Ending character constraints. The proposed method addresses character-level infilling tasks by utilizing a line-level format to avoid predicting any sub-token in inference. In addition, we incorporate two special tokens to signify the rest of the incomplete lines, thereby enhancing generation guidance. Extensive experiments demonstrate that our proposed approach surpasses previous methods, offering a significant advantage. Code is available at https://github.com/SenseLLM/FIM-SE.

------------

`[2405.17129] TEII: Think, Explain, Interact and Iterate with Large Language Models to Solve Cross-lingual Emotion Detection <https://arxiv.org/abs/2405.17129>`__

::

    Mon, 27 May 2024 12:47:40 GMT
    Long Cheng, Qihao Shao, Christine Zhao, Sheng Bi, Gina-Anne Levow

Cross-lingual emotion detection allows us to analyze global trends, public opinion, and social phenomena at scale. We participated in the Explainability of Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score of 0.6046 on the evaluation set for the emotion detection sub-task. Our system outperformed the baseline by more than 0.16 F1-score absolute, and ranked second amongst competing systems. We conducted experiments using fine-tuning, zero-shot learning, and few-shot learning for Large Language Model (LLM)-based models as well as embedding-based BiLSTM and KNN for non-LLM-based techniques.
Additionally, we introduced two novel methods: the Multi-Iteration Agentic Workflow and the Multi-Binary-Classifier Agentic Workflow. We found that LLM-based approaches provided good performance on multilingual emotion detection. Furthermore, ensembles combining all our experimented models yielded higher F1-scores than any single approach alone.

------------

`[2405.17220] RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness <https://arxiv.org/abs/2405.17220>`__

::

    Mon, 27 May 2024 14:37:01 GMT
    Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun

Learning from feedback reduces the hallucination of multimodal large language models (MLLMs) by aligning them with human preferences. While traditional methods rely on labor-intensive and time-consuming manual labeling, recent approaches employing models as automatic labelers have shown promising results without human intervention. However, these methods heavily rely on costly proprietary models like GPT-4V, resulting in scalability issues. Moreover, this paradigm essentially distills the proprietary models to provide a temporary solution to quickly bridge the performance gap. As this gap continues to shrink, the community is soon facing the essential challenge of aligning MLLMs using labeler models of comparable capability. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm for super GPT-4V trustworthiness. RLAIF-V maximally exploits the open-source feedback from two perspectives, including high-quality feedback data and online feedback learning algorithm. Extensive experiments on seven benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models without sacrificing performance on other tasks. Using a 34B model as labeler, RLAIF-V 7B model reduces object hallucination by 82.9\% and overall hallucination by 42.1\%, outperforming the labeler model.
Remarkably, RLAIF-V also reveals the self-alignment potential of open-source MLLMs, where a 12B model can learn from the feedback of itself to achieve less than 29.5\% overall hallucination rate, surpassing GPT-4V (45.9\%) by a large margin. The results shed light on a promising route to enhance the efficacy of leading-edge MLLMs.

------------

`[2405.17249] Assessing LLMs Suitability for Knowledge Graph Completion <https://arxiv.org/abs/2405.17249>`__

::

    Mon, 27 May 2024 15:04:50 GMT
    Vasile Ionut Remus Iga and Gheorghe Cosmin Silaghi

Recent work shown the capability of Large Language Models (LLMs) to solve tasks related to Knowledge Graphs, such as Knowledge Graph Completion, even in Zero- or Few-Shot paradigms. However, they are known to hallucinate answers, or output results in a non-deterministic manner, thus leading to wrongly reasoned responses, even if they satisfy the user's demands. To highlight opportunities and challenges in knowledge graphs-related tasks, we experiment with two distinguished LLMs, namely Mixtral-8x7B-Instruct-v0.1, and gpt-3.5-turbo-0125, on Knowledge Graph Completion for static knowledge graphs, using prompts constructed following the TELeR taxonomy, in Zero- and One-Shot contexts, on a Task-Oriented Dialogue system use case. When evaluated using both strict and flexible metrics measurement manners, our results show that LLMs could be fit for such a task if prompts encapsulate sufficient information and relevant examples.

------------

`[2405.17402] THREAD: Thinking Deeper with Recursive Spawning <https://arxiv.org/abs/2405.17402>`__

::

    Mon, 27 May 2024 17:51:24 GMT
    Philip Schroeder, Nathaniel Morgan, Hongyin Luo, James Glass

Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD). THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads. By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work. In effect, this enables the model to adapt, as needed, the amount of intermediate work used to produce tokens. We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively decompose the given task or question into progressively simpler sub-problems that can be solved by separate child threads. We test THREAD, implemented using a few-shot learning approach, on diverse benchmarks for agent tasks and data-grounded question answering.
THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD outperforms existing frameworks by 10% to 50% absolute points with smaller models, including Llama-3-8b and CodeLlama-7b.

------------

`[2405.17428] NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models <https://arxiv.org/abs/2405.17428>`__

::

    Mon, 27 May 2024 17:59:45 GMT
    Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping

Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model with a variety of architectural designs and training procedures to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility. For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For model training, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval datasets into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. Combining these techniques, our NV-Embed model, using only publicly available data, has achieved a record-high score of 69.32, ranking No. 1 on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval, reranking, classification, clustering, and semantic textual similarity tasks.
Notably, our model also attains the highest score of 59.36 on 15 retrieval tasks in the MTEB benchmark (also known as BEIR). We will open-source the model at: https://huggingface.co/nvidia/NV-Embed-v1.

------------

`[2405.15861] Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization <https://arxiv.org/abs/2405.15861>`__

::

    Fri, 24 May 2024 18:07:05 GMT
    Zhe Li, Bicheng Ying, Zidong Liu, Haibo Yang

Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. However, the substantial communication costs associated with FL pose a significant challenge to its efficiency. Specifically, in each communication round, the communication costs scale linearly with the model's dimension, which presents a formidable obstacle, especially in large model scenarios. Despite various communication efficient strategies, the intrinsic dimension-dependent communication cost remains a major bottleneck for current FL implementations.
In this paper, we introduce a novel dimension-free communication strategy for FL, leveraging zero-order optimization techniques. We propose a new algorithm, FedDisco, which facilitates the transmission of only a constant number of scalar values between clients and the server in each communication round, thereby reducing the communication cost from $\mathscr{O}(d)$ to $\mathscr{O}(1)$, where $d$ is the dimension of the model parameters.
Theoretically, in non-convex functions, we prove that our algorithm achieves state-of-the-art rates, which show a linear speedup of the number of clients and local steps under standard assumptions and dimension-free rate for low effective rank scenarios. Empirical evaluations through classic deep learning training and large language model fine-tuning substantiate significant reductions in communication overhead compared to traditional FL approaches.

------------

`[2405.15877] Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications <https://arxiv.org/abs/2405.15877>`__

::

    Fri, 24 May 2024 18:40:20 GMT
    Yang Li, Changsheng Zhao, Hyungtak Lee, Ernie Chang, Yangyang Shi, Vikas Chandra

Large language models (LLMs) significantly enhance the performance of various applications, but they are computationally intensive and energy-demanding. This makes it challenging to deploy them on devices with limited resources, such as personal computers and mobile/wearable devices, and results in substantial inference costs in resource-rich environments like cloud servers. To extend the use of LLMs, we introduce a low-rank decomposition approach to effectively compress these models, tailored to the requirements of specific applications.
We observe that LLMs pretrained on general datasets contain many redundant components not needed for particular applications. Our method focuses on identifying and removing these redundant parts, retaining only the necessary elements for the target applications. Specifically, we represent the weight matrices of LLMs as a linear combination of base components. We then prune the irrelevant bases and enhance the model with new bases beneficial for specific applications. Deep compression results on the Llama 2-7b and -13B models, conducted on target applications including mathematical reasoning and code generation, show that our method significantly reduces model size while maintaining comparable accuracy to state-of-the-art low-rank compression techniques.

------------

`[2405.15943] Transformers represent belief state geometry in their residual stream <https://arxiv.org/abs/2405.15943>`__

::

    Fri, 24 May 2024 21:14:10 GMT
    Adam S. Shai, Sarah E. Marzen, Lucas Teixeira, Alexander Gietelink Oldenziel, Paul M. Riechers

What computational structure are we building into large language models when we train them on next-token prediction? Here, we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data-generating process. Leveraging the theory of optimal prediction, we anticipate and then find that belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure. We investigate cases where the belief state geometry is represented in the final residual stream or distributed across the residual streams of multiple layers, providing a framework to explain these observations. Furthermore we demonstrate that the inferred belief states contain information about the entire future, beyond the local next-token prediction that the transformers are explicitly trained on.
Our work provides a framework connecting the structure of training data to the computational structure and representations that transformers use to carry out their behavior.

------------

`[2405.16203] Evolutionary Large Language Model for Automated Feature Transformation <https://arxiv.org/abs/2405.16203>`__

::

    Sat, 25 May 2024 12:27:21 GMT
    Nanxu Gong, Chandan K.Reddy, Wangyang Ying, Yanjie Fu

Feature transformation aims to reconstruct the feature space of raw features to enhance the performance of downstream models. However, the exponential growth in the combinations of features and operations poses a challenge, making it difficult for existing methods to efficiently explore a wide space.
Additionally, their optimization is solely driven by the accuracy of downstream models in specific domains, neglecting the acquisition of general feature knowledge. To fill this research gap, we propose an evolutionary LLM framework for automated feature transformation. This framework consists of two parts: 1) constructing a multi-population database through an RL data collector while utilizing evolutionary algorithm strategies for database maintenance, and 2) utilizing the ability of Large Language Model (LLM) in sequence understanding, we employ few-shot prompts to guide LLM in generating superior samples based on feature transformation sequence distinction. Leveraging the multi-population database initially provides a wide search scope to discover excellent populations. Through culling and evolution, the high-quality populations are afforded greater opportunities, thereby furthering the pursuit of optimal individuals. Through the integration of LLMs with evolutionary algorithms, we achieve efficient exploration within a vast space, while harnessing feature knowledge to propel optimization, thus realizing a more adaptable search paradigm. Finally, we empirically demonstrate the effectiveness and generality of our proposed method.

------------

`[2405.16325] SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs <https://arxiv.org/abs/2405.16325>`__

::

    Sat, 25 May 2024 18:43:05 GMT
    Mohammad Mozaffari, Amir Yazdanbakhsh, Zhao Zhang, Maryam Mehri Dehnavi

We propose SLoPe, a Double-Pruned Sparse Plus Lazy Low-rank Adapter Pretraining method for LLMs that improves the accuracy of sparse LLMs while accelerating their pretraining and inference and reducing their memory footprint. Sparse pretraining of LLMs reduces the accuracy of the model, to overcome this, prior work uses dense models during fine-tuning. SLoPe improves the accuracy of sparsely pretrained models by adding low-rank adapters in the final 1% iterations of pretraining without adding significant overheads to the model pretraining and inference. In addition, SLoPe uses a double-pruned backward pass formulation that prunes the transposed weight matrix using N:M sparsity structures to enable an accelerated sparse backward pass. SLoPe accelerates the training and inference of models with billions of parameters up to $1.14\times$ and $1.34\times$ respectively (OPT-33B and OPT-66B) while reducing their memory usage by up to $0.77\times$ and $0.51\times$ for training and inference respectively.

------------

`[2405.16405] Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level <https://arxiv.org/abs/2405.16405>`__

::

    Sun, 26 May 2024 02:12:02 GMT
    Runlin Lei, Yuwei Hu, Yuchen Ren, Zhewei Wei

Graph Neural Networks (GNNs) excel across various applications but remain vulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs), which inject malicious nodes into the original graph and pose realistic threats. Text-attributed graphs (TAGs), where nodes are associated with textual features, are crucial due to their prevalence in real-world applications and are commonly used to evaluate these vulnerabilities. However, existing research only focuses on embedding-level GIAs, which inject node embeddings rather than actual textual content, limiting their applicability and simplifying detection.
In this paper, we pioneer the exploration of GIAs at the text level, presenting three novel attack designs that inject textual content into the graph. Through theoretical and empirical analysis, we demonstrate that text interpretability, a factor previously overlooked at the embedding level, plays a crucial role in attack strength. Among the designs we investigate, the Word-frequency-based Text-level GIA (WTGIA) is particularly notable for its balance between performance and interpretability. Despite the success of WTGIA, we discover that defenders can easily enhance their defenses with customized text embedding methods or large language model (LLM)--based predictors. These insights underscore the necessity for further research into the potential and practical significance of text-level GIAs.

------------

`[2405.16406] SpinQuant -- LLM quantization with learned rotations <https://arxiv.org/abs/2405.16406>`__

::

    Sun, 26 May 2024 02:15:49 GMT
    Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, Tijmen Blankevoort

Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Recent findings suggest that rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures, and find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zero-shot reasoning performance. As a result, we propose SpinQuant that optimizes (or learns) the rotation matrices with Cayley optimization on a small validation set. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points. SpinQuant also outperforms concurrent work QuaRot, which applies random rotations to remove outliers. In particular, for LLaMA-2 7B/LLaMA-3 8B models that are hard to quantize, SpinQuant reduces the gap to full precision by 30.2%/34.1% relative to QuaRot.

------------

`[2405.16436] Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer <https://arxiv.org/abs/2405.16436>`__

::

    Sun, 26 May 2024 05:38:50 GMT
    Zhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, Zhaoran Wang

Aligning generative models with human preference via RLHF typically suffers from overoptimization, where an imperfectly learned reward model can misguide the generative model to output undesired responses. We investigate this problem in a principled manner by identifying the source of the misalignment as a form of distributional shift and uncertainty in learning human preferences. To mitigate overoptimization, we first propose a theoretical algorithm that chooses the best policy for an adversarially chosen reward model; one that simultaneously minimizes the maximum likelihood estimation of the loss and a reward penalty term. Here, the reward penalty term is introduced to prevent the policy from choosing actions with spurious high proxy rewards, resulting in provable sample efficiency of the algorithm under a partial coverage style condition. Moving from theory to practice, the proposed algorithm further enjoys an equivalent but surprisingly easy-to-implement reformulation. Using the equivalence between reward models and the corresponding optimal policy, the algorithm features a simple objective that combines: (i) a preference optimization loss that directly aligns the policy with human preference, and (ii) a supervised learning loss that explicitly imitates the policy with a (suitable) baseline distribution. In the context of aligning large language models (LLM), this objective fuses the direct preference optimization (DPO) loss with the supervised fune-tuning (SFT) loss to help mitigate the overoptimization towards undesired responses, for which we name the algorithm Regularized Preference Optimization (RPO). Experiments of aligning LLMs demonstrate the improved performance of RPO compared with DPO baselines. Our work sheds light on the interplay between preference optimization and SFT in tuning LLMs with both theoretical guarantees and empirical evidence.

------------

`[2405.16444] CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion <https://arxiv.org/abs/2405.16444>`__

::

    Sun, 26 May 2024 06:00:17 GMT
    Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang

Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, and when they are not, their precomputed KV caches cannot be directly used since they ignore the text's cross-attention with the preceding text in the LLM input. Thus, the benefits of reusing KV caches remain largely unrealized.
This paper tackles just one question: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? We present CacheBlend, a scheme that reuses the pre-computed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime,the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job,allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3X and increases the inference throughput by 2.8-5X, compared with full KV recompute, without compromising generation quality or incurring more storage cost.

------------

`[2405.16450] Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search <https://arxiv.org/abs/2405.16450>`__

::

    Sun, 26 May 2024 06:33:48 GMT
    Max Liu, Chan-Hung Yu, Wei-Hsu Lee, Cheng-Wei Hung, Yen-Chun Chen, Shao-Hua Sun

Programmatic reinforcement learning (PRL) has been explored for representing policies through programs as a means to achieve interpretability and generalization. Despite promising outcomes, current state-of-the-art PRL methods are hindered by sample inefficiency, necessitating tens of millions of program-environment interactions. To tackle this challenge, we introduce a novel LLM-guided search framework (LLM-GS). Our key insight is to leverage the programming expertise and common sense reasoning of LLMs to enhance the efficiency of assumption-free, random-guessing search methods. We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy - an LLM is instructed to initially generate Python codes and then convert them into DSL programs. To further optimize the LLM-generated programs, we develop a search algorithm named Scheduled Hill Climbing, designed to efficiently explore the programmatic search space to consistently improve the programs. Experimental results in the Karel domain demonstrate the superior effectiveness and efficiency of our LLM-GS framework. Extensive ablation studies further verify the critical role of our Pythonic-DSL strategy and Scheduled Hill Climbing algorithm.

------------

`[2405.16528] LoQT: Low Rank Adapters for Quantized Training <https://arxiv.org/abs/2405.16528>`__

::

    Sun, 26 May 2024 11:29:57 GMT
    Sebastian Loeschcke, Mads Toftrup, Michael J. Kastoryano, Serge Belongie and V\'esteinn Sn{\ae}bjarnarson

Training of large neural networks requires significant computational resources. Despite advances using low-rank adapters and quantization, pretraining of models such as LLMs on consumer hardware has not been possible without model sharding, offloading during training, or per-layer gradient updates. To address these limitations, we propose LoQT, a method for efficiently training quantized models. LoQT uses gradient-based tensor factorization to initialize low-rank trainable weight matrices that are periodically merged into quantized full-rank weight matrices. Our approach is suitable for both pretraining and fine-tuning of models, which we demonstrate experimentally for language modeling and downstream task adaptation. We find that LoQT enables efficient training of models up to 7B parameters on a consumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B parameter model using per-layer gradient updates on the same hardware.

------------

`[2405.16581] On Bits and Bandits: Quantifying the Regret-Information Trade-off <https://arxiv.org/abs/2405.16581>`__

::

    Sun, 26 May 2024 14:18:38 GMT
    Itai Shufaro, Nadav Merlis, Nir Weinberger, Shie Mannor

In interactive decision-making tasks, information can be acquired by direct interactions, through receiving indirect feedback, and from external knowledgeable sources. We examine the trade-off between the information an agent accumulates and the regret it suffers. We show that information from external sources, measured in bits, can be traded off for regret, measured in reward. We invoke information-theoretic methods for obtaining regret lower bounds, that also allow us to easily re-derive several known lower bounds. We then generalize a variety of interactive decision-making tasks with external information to a new setting. Using this setting, we introduce the first Bayesian regret lower bounds that depend on the information an agent accumulates. These lower bounds also prove the near-optimality of Thompson sampling for Bayesian problems. Finally, we demonstrate the utility of these bounds in improving the performance of a question-answering task with large language models, allowing us to obtain valuable insights.

------------

`[2405.16587] Cost-Effective Online Multi-LLM Selection with Versatile Reward Models <https://arxiv.org/abs/2405.16587>`__

::

    Sun, 26 May 2024 14:38:24 GMT
    Xiangxiang Dai, Jin Li, Xutong Liu, Anqi Yu, John C.S. Lui

With the rapid advancement of large language models (LLMs), the diversity of multi-LLM tasks and the variability in their pricing structures have become increasingly important, as costs can vary greatly between different LLMs. To tackle these challenges, we introduce the \textit{C2MAB-V}, a \underline{C}ost-effective \underline{C}ombinatorial \underline{M}ulti-armed \underline{B}andit with \underline{V}ersatile reward models for optimal LLM selection and usage. This online model differs from traditional static approaches or those reliant on a single LLM without cost consideration. With multiple LLMs deployed on a scheduling cloud and a local server dedicated to handling user queries, \textit{C2MAB-V} facilitates the selection of multiple LLMs over a combinatorial search space, specifically tailored for various collaborative task types with different reward models. Based on our designed online feedback mechanism and confidence bound technique, \textit{C2MAB-V} can effectively address the multi-LLM selection challenge by managing the exploration-exploitation trade-off across different models, while also balancing cost and reward for diverse tasks. The NP-hard integer linear programming problem for selecting multiple LLMs with trade-off dilemmas is addressed by: i) decomposing the integer problem into a relaxed form by the local server, ii) utilizing a discretization rounding scheme that provides optimal LLM combinations by the scheduling cloud, and iii) continual online updates based on feedback. Theoretically, we prove that \textit{C2MAB-V} offers strict guarantees over versatile reward models, matching state-of-the-art results for regret and violations in some degenerate cases. Empirically, we show that \textit{C2MAB-V} effectively balances performance and cost-efficiency with nine LLMs for three application scenarios.

------------

`[2405.16747] Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective <https://arxiv.org/abs/2405.16747>`__

::

    Mon, 27 May 2024 01:31:40 GMT
    Akiyoshi Tomihari and Issei Sato

The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely attributed to the preservation of pre-trained features, achieved through a near-optimal linear head obtained during LP. However, despite the widespread use of large language models, the exploration of complex architectures such as Transformers remains limited. In this paper, we analyze the training dynamics of LP-FT for classification models on the basis of the neural tangent kernel (NTK) theory. Our analysis decomposes the NTK matrix into two components, highlighting the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage. We also observe a significant increase in the linear head norm during LP, stemming from training with the cross-entropy (CE) loss, which effectively minimizes feature changes.
Furthermore, we find that this increased norm can adversely affect model calibration, a challenge that can be addressed by temperature scaling.
Additionally, we extend our analysis with the NTK to the low-rank adaptation (LoRA) method and validate its effectiveness. Our experiments with a Transformer-based model on natural language processing tasks across multiple benchmarks confirm our theoretical analysis and demonstrate the effectiveness of LP-FT in fine-tuning language models. Code is available at https://github.com/tom4649/lp-ft_ntk.

------------

`[2405.16833] Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models <https://arxiv.org/abs/2405.16833>`__

::

    Mon, 27 May 2024 05:04:05 GMT
    Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, Chun-Ying Huang

While large language models (LLMs) such as Llama-2 or GPT-4 have shown impressive zero-shot performance, fine-tuning is still necessary to enhance their performance for customized datasets, domain-specific tasks, or other private needs. However, fine-tuning all parameters of LLMs requires significant hardware resources, which can be impractical for typical users. Therefore, parameter-efficient fine-tuning such as LoRA have emerged, allowing users to fine-tune LLMs without the need for considerable computing resources, with little performance degradation compared to fine-tuning all parameters.
Unfortunately, recent studies indicate that fine-tuning can increase the risk to the safety of LLMs, even when data does not contain malicious content. To address this challenge, we propose Safe LoRA, a simple one-liner patch to the original LoRA implementation by introducing the projection of LoRA weights from selected layers to the safety-aligned subspace, effectively reducing the safety risks in LLM fine-tuning while maintaining utility. It is worth noting that Safe LoRA is a training-free and data-free approach, as it only requires the knowledge of the weights from the base and aligned LLMs. Our extensive experiments demonstrate that when fine-tuning on purely malicious data, Safe LoRA retains similar safety performance as the original aligned model.
Moreover, when the fine-tuning dataset contains a mixture of both benign and malicious data, Safe LoRA mitigates the negative effect made by malicious data while preserving performance on downstream tasks.

------------

`[2405.16918] The Uncanny Valley: Exploring Adversarial Robustness from a Flatness Perspective <https://arxiv.org/abs/2405.16918>`__

::

    Mon, 27 May 2024 08:10:46 GMT
    Nils Philipp Walter, Linara Adilova, Jilles Vreeken, Michael Kamp

Flatness of the loss surface not only correlates positively with generalization but is also related to adversarial robustness, since perturbations of inputs relate non-linearly to perturbations of weights. In this paper, we empirically analyze the relation between adversarial examples and relative flatness with respect to the parameters of one layer. We observe a peculiar property of adversarial examples: during an iterative first-order white-box attack, the flatness of the loss surface measured around the adversarial example first becomes sharper until the label is flipped, but if we keep the attack running it runs into a flat uncanny valley where the label remains flipped. We find this phenomenon across various model architectures and datasets. Our results also extend to large language models (LLMs), but due to the discrete nature of the input space and comparatively weak attacks, the adversarial examples rarely reach a truly flat region. Most importantly, this phenomenon shows that flatness alone cannot explain adversarial robustness unless we can also guarantee the behavior of the function around the examples.
We theoretically connect relative flatness to adversarial robustness by bounding the third derivative of the loss surface, underlining the need for flatness in combination with a low global Lipschitz constant for a robust model.

------------

`[2405.17088] Phase Transitions in the Output Distribution of Large Language Models <https://arxiv.org/abs/2405.17088>`__

::

    Mon, 27 May 2024 12:04:36 GMT
    Julian Arnold, Flemming Holtorf, Frank Sch\"afer, Niels L\"orch

In a physical system, changing parameters such as temperature can induce a phase transition: an abrupt change from one state of matter to another.
Analogous phenomena have recently been observed in large language models.
Typically, the task of identifying phase transitions requires human analysis and some prior understanding of the system to narrow down which low-dimensional properties to monitor and analyze. Statistical methods for the automated detection of phase transitions from data have recently been proposed within the physics community. These methods are largely system agnostic and, as shown here, can be adapted to study the behavior of large language models. In particular, we quantify distributional changes in the generated output via statistical distances, which can be efficiently estimated with access to the probability distribution over next-tokens. This versatile approach is capable of discovering new phases of behavior and unexplored transitions -- an ability that is particularly exciting in light of the rapid development of language models and their emergent capabilities.

------------

`[2405.17216] Autoformalizing Euclidean Geometry <https://arxiv.org/abs/2405.17216>`__

::

    Mon, 27 May 2024 14:35:10 GMT
    Logan Murphy, Kaiyu Yang, Jialiang Sun, Zhaoyu Li, Anima Anandkumar, Xujie Si

Autoformalization involves automatically translating informal math into formal theorems and proofs that are machine-verifiable. Euclidean geometry provides an interesting and controllable domain for studying autoformalization.
In this paper, we introduce a neuro-symbolic framework for autoformalizing Euclidean geometry, which combines domain knowledge, SMT solvers, and large language models (LLMs). One challenge in Euclidean geometry is that informal proofs rely on diagrams, leaving gaps in texts that are hard to formalize. To address this issue, we use theorem provers to fill in such diagrammatic information automatically, so that the LLM only needs to autoformalize the explicit textual steps, making it easier for the model. We also provide automatic semantic evaluation for autoformalized theorem statements. We construct LeanEuclid, an autoformalization benchmark consisting of problems from Euclid's Elements and the UniGeo dataset formalized in the Lean proof assistant. Experiments with GPT-4 and GPT-4V show the capability and limitations of state-of-the-art LLMs on autoformalizing geometry problems. The data and code are available at https://github.com/loganrjmurphy/LeanEuclid.

------------

`[2405.17233] CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs <https://arxiv.org/abs/2405.17233>`__

::

    Mon, 27 May 2024 14:49:39 GMT
    Haoyu Wang, Bei Liu, Hang Shao, Bo Xiao, Ke Zeng, Guanglu Wan, Yanmin Qian

Parameter quantization for Large Language Models (LLMs) has attracted increasing attentions recently in reducing memory costs and improving computational efficiency. Early approaches have been widely adopted. However, the existing methods suffer from poor performance in low-bit (such as 2 to 3 bits) scenarios. In this paper, we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ) framework by introducing three different types of adaptive strategies for LLM quantization. Firstly, a K-Means clustering based algorithm is proposed that allows dynamic generation of quantization centroids for each column of a parameter matrix. Secondly, we design an outlier-guided adaptive precision search strategy which can dynamically assign varying bit-widths to different columns. Finally, a dynamic outlier reservation scheme is developed to retain some parameters in their original float point precision, in trade off of boosted model performance.
Experiments on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate that our methods achieve the state-of-the-art results across different bit settings, especially in extremely low-bit scenarios. Code will be released soon.

------------

`[2405.17247] An Introduction to Vision-Language Modeling <https://arxiv.org/abs/2405.17247>`__

::

    Mon, 27 May 2024 15:01:23 GMT
    Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar Ma\~nas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu Sharma, Hu Xu, Xiaoqing Ellen Tan, Megan Richards, Samuel Lavoie, Pietro Astolfi, Reyhane Askari Hemmat, Jun Chen, Kushal Tirumala, Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika Chaudhuri, Zechun Liu, Xilun Chen, Quentin Garrido, Karen Ullrich, Aishwarya Agrawal, Kate Saenko, Asli Celikyilmaz, Vikas Chandra

Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos.

------------

`[2405.17346] Prompt Optimization with Human Feedback <https://arxiv.org/abs/2405.17346>`__

::

    Mon, 27 May 2024 16:49:29 GMT
    Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low

Large language models (LLMs) have demonstrated remarkable performances in various tasks. However, the performance of LLMs heavily depends on the input prompt, which has given rise to a number of recent works on prompt optimization. However, previous works often require the availability of a numeric score to assess the quality of every prompt. Unfortunately, when a human user interacts with a black-box LLM, attaining such a score is often infeasible and unreliable. Instead, it is usually significantly easier and more reliable to obtain preference feedback from a human user, i.e., showing the user the responses generated from a pair of prompts and asking the user which one is preferred. Therefore, in this paper, we study the problem of prompt optimization with human feedback (POHF), in which we aim to optimize the prompt for a black-box LLM using only human preference feedback. Drawing inspiration from dueling bandits, we design a theoretically principled strategy to select a pair of prompts to query for preference feedback in every iteration, and hence introduce our algorithm named automated POHF (APOHF). We apply our APOHF algorithm to various tasks, including optimizing user instructions, prompt optimization for text-to-image generative models, and response optimization with human feedback (i.e., further refining the response using a variant of our APOHF). The results demonstrate that our APOHF can efficiently find a good prompt using a small number of preference feedback instances. Our code can be found at \url{https://github.com/xqlin98/APOHF}.

------------

`[2405.17374] Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models <https://arxiv.org/abs/2405.17374>`__

::

    Mon, 27 May 2024 17:31:56 GMT
    ShengYun Peng, Pin-Yu Chen, Matthew Hull, Duen Horng Chau

Safety alignment is the key to guiding the behaviors of large language models (LLMs) that are in line with human preferences and restrict harmful behaviors at inference time, but recent studies show that it can be easily compromised by finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin": randomly perturbing model weights maintains the safety level of the original aligned model in its local neighborhood. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community.

------------

`[2405.17382] ReMoDetect: Reward Models Recognize Aligned LLM's Generations <https://arxiv.org/abs/2405.17382>`__

::

    Mon, 27 May 2024 17:38:33 GMT
    Hyunseok Lee, Jihoon Tack, Jinwoo Shin

The remarkable capabilities and easy accessibility of large language models (LLMs) have significantly increased societal risks (e.g., fake news generation), necessitating the development of LLM-generated text (LGT) detection methods for safe usage. However, detecting LGTs is challenging due to the vast number of LLMs, making it impractical to account for each LLM individually; hence, it is crucial to identify the common characteristics shared by these models. In this paper, we draw attention to a common feature of recent powerful LLMs, namely the alignment training, i.e., training LLMs to generate human-preferable texts. Our key finding is that as these aligned LLMs are trained to maximize the human preferences, they generate texts with higher estimated preferences even than human-written texts; thus, such texts are easily detected by using the reward model (i.e., an LLM trained to model human preference distribution). Based on this finding, we propose two training schemes to further improve the detection ability of the reward model, namely (i) continual preference fine-tuning to make the reward model prefer aligned LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a rephrased texts from human-written texts using aligned LLMs), which serves as a median preference text corpus between LGTs and human-written texts to learn the decision boundary better. We provide an extensive evaluation by considering six text domains across twelve aligned LLMs, where our method demonstrates state-of-the-art results. Code is available at https://github.com/hyunseoklee-ai/reward_llm_detect.

------------

`[2405.15880] HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis <https://arxiv.org/abs/2405.15880>`__

::

    Fri, 24 May 2024 18:45:51 GMT
    Shraddha Barke, Emmanuel Anaya Gonzalez, Saketh Ram Kasibatla, Taylor Berg-Kirkpatrick, Nadia Polikarpova

Many structured prediction and reasoning tasks can be framed as program synthesis problems, where the goal is to generate a program in a domain-specific language (DSL) that transforms input data into the desired output. Unfortunately, purely neural approaches, such as large language models (LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while purely symbolic methods based on combinatorial search scale poorly to complex problems. Motivated by these limitations, we introduce a hybrid approach, where LLM completions for a given task are used to learn a task-specific, context-free surrogate model, which is then used to guide program synthesis. We evaluate this hybrid approach on three domains, and show that it outperforms both unguided search and direct sampling from LLMs, as well as existing program synthesizers.

------------

`[2405.15902] Hacc-Man: An Arcade Game for Jailbreaking LLMs <https://arxiv.org/abs/2405.15902>`__

::

    Fri, 24 May 2024 19:55:20 GMT
    Matheus Valentim, Jeanette Falk, Nanna Inie

The recent leaps in complexity and fluency of Large Language Models (LLMs) mean that, for the first time in human history, people can interact with computers using natural language alone. This creates monumental possibilities of automation and accessibility of computing, but also raises severe security and safety threats: When everyone can interact with LLMs, everyone can potentially break into the systems running LLMs. All it takes is creative use of language. This paper presents Hacc-Man, a game which challenges its players to "jailbreak" an LLM: subvert the LLM to output something that it is not intended to. Jailbreaking is at the intersection between creative problem solving and LLM security. The purpose of the game is threefold: 1. To heighten awareness of the risks of deploying fragile LLMs in everyday systems, 2. To heighten people's self-efficacy in interacting with LLMs, and 3. To discover the creative problem solving strategies, people deploy in this novel context.

------------

`[2405.15928] PatchProt: Hydrophobic patch prediction using protein foundation models <https://arxiv.org/abs/2405.15928>`__

::

    Fri, 24 May 2024 20:37:02 GMT
    Dea Gogishvili, Emmanuel Minois-Genin, Jan van Eck, Sanne Abeln

Hydrophobic patches on protein surfaces play important functional roles in protein-protein and protein-ligand interactions. Large hydrophobic surfaces are also involved in the progression of aggregation diseases. Predicting exposed hydrophobic patches from a protein sequence has been shown to be a difficult task. Fine-tuning foundation models allows for adapting a model to the specific nuances of a new task using a much smaller dataset. Additionally, multi-task deep learning offers a promising solution for addressing data gaps, simultaneously outperforming single-task methods. In this study, we harnessed a recently released leading large language model ESM-2. Efficient fine-tuning of ESM-2 was achieved by leveraging a recently developed parameter-efficient fine-tuning method. This approach enabled comprehensive training of model layers without excessive parameters and without the need to include a computationally expensive multiple sequence analysis. We explored several related tasks, at local (residue) and global (protein) levels, to improve the representation of the model. As a result, our fine-tuned ESM-2 model, PatchProt, cannot only predict hydrophobic patch areas but also outperforms existing methods at predicting primary tasks, including secondary structure and surface accessibility predictions. Importantly, our analysis shows that including related local tasks can improve predictions on more difficult global tasks. This research sets a new standard for sequence-based protein property prediction and highlights the remarkable potential of fine-tuning foundation models enriching the model representation by training over related tasks.

------------

`[2405.15960] Human-Centered Automation <https://arxiv.org/abs/2405.15960>`__

::

    Fri, 24 May 2024 22:12:28 GMT
    Carlos Toxtli

The rapid advancement of Generative Artificial Intelligence (AI), such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLM), has the potential to revolutionize the way we work and interact with digital systems across various industries. However, the current state of software automation, such as Robotic Process Automation (RPA) frameworks, often requires domain expertise and lacks visibility and intuitive interfaces, making it challenging for users to fully leverage these technologies. This position paper argues for the emerging area of Human-Centered Automation (HCA), which prioritizes user needs and preferences in the design and development of automation systems. Drawing on empirical evidence from human-computer interaction research and case studies, we highlight the importance of considering user perspectives in automation and propose a framework for designing human-centric automation solutions. The paper discusses the limitations of existing automation approaches, the challenges in integrating AI and RPA, and the benefits of human-centered automation for productivity, innovation, and democratizing access to these technologies. We emphasize the importance of open-source solutions and provide examples of how HCA can empower individuals and organizations in the era of rapidly progressing AI, helping them remain competitive. The paper also explores pathways to achieve more advanced and context-aware automation solutions. We conclude with a call to action for researchers and practitioners to focus on developing automation technologies that adapt to user needs, provide intuitive interfaces, and leverage the capabilities of high-end AI to create a more accessible and user-friendly future of automation.

------------

`[2405.16133] Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting <https://arxiv.org/abs/2405.16133>`__

::

    Sat, 25 May 2024 08:57:28 GMT
    Tong Ye, Yangkai Du, Tengfei Ma, Lingfei Wu, Xuhong Zhang, Shouling Ji, Wenhai Wang

Large Language Models (LLMs) have exhibited remarkable proficiency in generating code. However, the misuse of LLM-generated (Synthetic) code has prompted concerns within both educational and industrial domains, highlighting the imperative need for the development of synthetic code detectors. Existing methods for detecting LLM-generated content are primarily tailored for general text and often struggle with code content due to the distinct grammatical structure of programming languages and massive "low-entropy" tokens. Building upon this, our work proposes a novel zero-shot synthetic code detector based on the similarity between the code and its rewritten variants. Our method relies on the intuition that the differences between the LLM-rewritten and original codes tend to be smaller when the original code is synthetic. We utilize self-supervised contrastive learning to train a code similarity model and assess our approach on two synthetic code detection benchmarks. Our results demonstrate a notable enhancement over existing synthetic content detectors designed for general texts, with an improvement of 20.5% in the APPS benchmark and 29.1% in the MBPP benchmark.

------------

`[2405.16310] An Empirical Exploration of Trust Dynamics in LLM Supply Chains <https://arxiv.org/abs/2405.16310>`__

::

    Sat, 25 May 2024 17:37:56 GMT
    Agathe Balayn, Mireia Yurrita, Fanny Rancourt, Fabio Casati, Ujwal Gadiraju

With the widespread proliferation of AI systems, trust in AI is an important and timely topic to navigate. Researchers so far have largely employed a myopic view of this relationship. In particular, a limited number of relevant trustors (e.g., end-users) and trustees (i.e., AI systems) have been considered, and empirical explorations have remained in laboratory settings, potentially overlooking factors that impact human-AI relationships in the real world. In this paper, we argue for broadening the scope of studies addressing `trust in AI' by accounting for the complex and dynamic supply chains that AI systems result from. AI supply chains entail various technical artifacts that diverse individuals, organizations, and stakeholders interact with, in a variety of ways. We present insights from an in-situ, empirical study of LLM supply chains. Our work reveals additional types of trustors and trustees and new factors impacting their trust relationships. These relationships were found to be central to the development and adoption of LLMs, but they can also be the terrain for uncalibrated trust and reliance on untrustworthy LLMs. Based on these findings, we discuss the implications for research on `trust in AI'. We highlight new research opportunities and challenges concerning the appropriate study of inter-actor relationships across the supply chain and the development of calibrated trust and meaningful reliance behaviors. We also question the meaning of building trust in the LLM supply chain.

------------

`[2405.16363] LLMs for User Interest Exploration: A Hybrid Approach <https://arxiv.org/abs/2405.16363>`__

::

    Sat, 25 May 2024 21:57:36 GMT
    Jianling Wang, Haokai Lu, Yifan Liu, He Ma, Yueqi Wang, Yang Gu, Shuzhou Zhang, Ningren (Peter) Han, Shuchao Bi, Lexi Baugher, Ed Chi and Minmin Chen

Traditional recommendation systems are subject to a strong feedback loop by learning from and reinforcing past user-item interactions, which in turn limits the discovery of novel user interests. To address this, we introduce a hybrid hierarchical framework combining Large Language Models (LLMs) and classic recommendation models for user interest exploration. The framework controls the interfacing between the LLMs and the classic recommendation models through "interest clusters", the granularity of which can be explicitly determined by algorithm designers. It recommends the next novel interests by first representing "interest clusters" using language, and employs a fine-tuned LLM to generate novel interest descriptions that are strictly within these predefined clusters. At the low level, it grounds these generated interests to an item-level policy by restricting classic recommendation models, in this case a transformer-based sequence recommender to return items that fall within the novel clusters generated at the high level. We showcase the efficacy of this approach on an industrial-scale commercial platform serving billions of users.
Live experiments show a significant increase in both exploration of novel interests and overall user enjoyment of the platform.

------------

`[2405.16766] Reframing the Relationship in Out-of-Distribution Detection <https://arxiv.org/abs/2405.16766>`__

::

    Mon, 27 May 2024 02:27:28 GMT
    YuXiao Lee, Xiaofeng Cao

The remarkable achievements of Large Language Models (LLMs) have captivated the attention of both academia and industry, transcending their initial role in dialogue generation. The utilization of LLMs as intermediary agents in various tasks has yielded promising results, sparking a wave of innovation in artificial intelligence. Building on these breakthroughs, we introduce a novel approach that integrates the agent paradigm into the Out-of-distribution (OOD) detection task, aiming to enhance its robustness and adaptability. Our proposed method, Concept Matching with Agent (CMA), employs neutral prompts as agents to augment the CLIP-based OOD detection process. These agents function as dynamic observers and communication hubs, interacting with both In-distribution (ID) labels and data inputs to form vector triangle relationships. This triangular framework offers a more nuanced approach than the traditional binary relationship, allowing for better separation and identification of ID and OOD inputs. Our extensive experimental results showcase the superior performance of CMA over both zero-shot and training-required methods in a diverse array of real-world scenarios.

------------

`[2405.16792] Laurel: Generating Dafny Assertions Using Large Language Models <https://arxiv.org/abs/2405.16792>`__

::

    Mon, 27 May 2024 03:26:01 GMT
    Eric Mugnier, Emmanuel Anaya Gonzalez, Ranjit Jhala, Nadia Polikarpova, Yuanyuan Zhou

Dafny is a popular verification language, which automates proofs by outsourcing them to an SMT solver. This automation is not perfect, however, and the solver often requires guidance in the form of helper assertions creating a burden for the proof engineer. In this paper, we propose Laurel, a tool that uses large language models (LLMs) to automatically generate helper assertions for Dafny programs. To improve the success rate of LLMs in this task, we design two domain-specific prompting techniques. First, we help the LLM determine the location of the missing assertion by analyzing the verifier's error message and inserting an assertion placeholder at that location. Second, we provide the LLM with example assertions from the same codebase, which we select based on a new lemma similarity metric. We evaluate our techniques on a dataset of helper assertions we extracted from three real-world Dafny codebases. Our evaluation shows that Laurel is able to generate over 50% of the required helper assertions given only a few attempts, making LLMs a usable and affordable tool to further automate practical program verification.

------------

`[2405.17053] WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence <https://arxiv.org/abs/2405.17053>`__

::

    Mon, 27 May 2024 11:18:25 GMT
    Jiawei Shao, Jingwen Tong, Qiong Wu, Wei Guo, Zijian Li, Zehong Lin, Jun Zhang

The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed. Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems. However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding. To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks. We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution. Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning. Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks. Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research.

------------

`[2405.17104] LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding <https://arxiv.org/abs/2405.17104>`__

::

    Mon, 27 May 2024 12:23:08 GMT
    Haoyu Zhao, Wenhang Ge, Ying-cong Chen

Visual grounding is an essential tool that links user-provided text queries with query-specific regions within an image. Despite advancements in visual grounding models, their ability to comprehend complex queries remains limited.
To overcome this limitation, we introduce LLM-Optic, an innovative method that utilizes Large Language Models (LLMs) as an optical lens to enhance existing visual grounding models in comprehending complex text queries involving intricate text structures, multiple objects, or object spatial relationships, situations that current models struggle with. LLM-Optic first employs an LLM as a Text Grounder to interpret complex text queries and accurately identify objects the user intends to locate. Then a pre-trained visual grounding model is used to generate candidate bounding boxes given the refined query by the Text Grounder. After that, LLM-Optic annotates the candidate bounding boxes with numerical marks to establish a connection between text and specific image regions, thereby linking two distinct modalities. Finally, it employs a Large Multimodal Model (LMM) as a Visual Grounder to select the marked candidate objects that best correspond to the original text query. Through LLM-Optic, we have achieved universal visual grounding, which allows for the detection of arbitrary objects specified by arbitrary human language input. Importantly, our method achieves this enhancement without requiring additional training or fine-tuning. Extensive experiments across various challenging benchmarks demonstrate that LLM-Optic achieves state-of-the-art zero-shot visual grounding capabilities.

------------

`[2405.17430] Matryoshka Multimodal Models <https://arxiv.org/abs/2405.17430>`__

::

    Mon, 27 May 2024 17:59:56 GMT
    Mu Cai, Jianwei Yang, Jianfeng Gao, Yong Jae Lee

Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM).
However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, we propose M3: Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. Our approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2) M3 provides a framework for analyzing the granularity needed for existing datasets, where we find that COCO-style benchmarks only need around ~9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) Our approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where our investigation reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.

------------

`[2405.15787] Extracting chemical food safety hazards from the scientific literature automatically using large language models <https://arxiv.org/abs/2405.15787>`__

::

    Wed, 1 May 2024 08:02:10 GMT
    Neris \"Ozen, Wenjuan Mu, Esther D. van Asselt, Leonieke M. van den Bulk

The number of scientific articles published in the domain of food safety has consistently been increasing over the last few decades. It has therefore become unfeasible for food safety experts to read all relevant literature related to food safety and the occurrence of hazards in the food chain. However, it is important that food safety experts are aware of the newest findings and can access this information in an easy and concise way. In this study, an approach is presented to automate the extraction of chemical hazards from the scientific literature through large language models. The large language model was used out-of-the-box and applied on scientific abstracts; no extra training of the models or a large computing cluster was required. Three different styles of prompting the model were tested to assess which was the most optimal for the task at hand. The prompts were optimized with two validation foods (leafy greens and shellfish) and the final performance of the best prompt was evaluated using three test foods (dairy, maize and salmon). The specific wording of the prompt was found to have a considerable effect on the results. A prompt breaking the task down into smaller steps performed best overall. This prompt reached an average accuracy of 93% and contained many chemical contaminants already included in food monitoring programs, validating the successful retrieval of relevant hazards for the food safety domain. The results showcase how valuable large language models can be for the task of automatic information extraction from the scientific literature.

------------

`[2405.16700] Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs <https://arxiv.org/abs/2405.16700>`__

::

    Sun, 26 May 2024 21:31:59 GMT
    Mustafa Shukor, Matthieu Cord

Large Language Models (LLMs) have demonstrated impressive performance on multimodal tasks, without any multimodal finetuning. They are the building block for Large Multimodal Models, yet, we still lack a proper understanding of their success. In this work, we expose frozen LLMs to image, video, audio and text inputs and analyse their internal representation aiming to understand their generalization beyond textual inputs.
Findings. Perceptual tokens (1) are easily distinguishable from textual ones inside LLMs, with significantly different representations, and complete translation to textual tokens does not exist. Yet, (2) both perceptual and textual tokens activate similar LLM weights. Despite being different, (3) perceptual and textual tokens are implicitly aligned inside LLMs, we call this the implicit multimodal alignment (IMA), and argue that this is linked to architectural design, helping LLMs to generalize. This provide more evidence to believe that the generalization of LLMs to multimodal inputs is mainly due to their architecture.
Implications. (1) We find a positive correlation between the implicit alignment score and the task performance, suggesting that this could act as a proxy metric for model evaluation and selection. (2) A negative correlation exists regarding hallucinations, revealing that this problem is mainly due to misalignment between the internal perceptual and textual representations. (3) Perceptual tokens change slightly throughout the model, thus, we propose different approaches to skip computations (e.g. in FFN layers), and significantly reduce the inference cost. (4) Due to the slowly changing embeddings across layers, and the high overlap between textual and multimodal activated weights, we compress LLMs by keeping only 1 subnetwork that works well across a wide range of multimodal tasks. Paper code: https://github.com/mshukor/ima-lmms.

------------

`[2405.15842] Model Cascading for Code: Reducing Inference Costs with Model Cascading for LLM Based Code Generation <https://arxiv.org/abs/2405.15842>`__

::

    Fri, 24 May 2024 16:20:04 GMT
    Boyuan Chen, Mingzhi Zhu, Brendan Dolan-Gavitt, Muhammad Shafique, Siddharth Garg

The rapid development of large language models (LLMs) has led to significant advancements in code completion tasks. While larger models have higher accuracy, they also cost much more to run. Meanwhile, model cascading has been proven effective to conserve computational resources while enhancing accuracy in LLMs on natural language generation tasks. It generates output with the smallest model in a set, and only queries the larger models when it fails to meet predefined quality criteria. However, this strategy has not been used in code completion tasks, primarily because assessing the quality of code completions differs substantially from assessing natural language, where the former relies heavily on the functional correctness. To address this, we propose letting each model generate and execute a set of test cases for their solutions, and use the test results as the cascading threshold. We show that our model cascading strategy reduces computational costs while increases accuracy compared to generating the output with a single model. We also introduce a heuristics to determine the optimal combination of the number of solutions, test cases, and test lines each model should generate, based on the budget. Compared to speculative decoding, our method works on black-box models, having the same level of cost-accuracy trade-off, yet providing much more choices based on the server's budget. Ours is the first work to optimize cost-accuracy trade-off for LLM code generation with model cascading.

------------

`[2405.16236] A statistical framework for weak-to-strong generalization <https://arxiv.org/abs/2405.16236>`__

::

    Sat, 25 May 2024 13:54:05 GMT
    Seamus Somerstep, Felipe Maia Polo, Moulinath Banerjee, Ya'acov Ritov, Mikhail Yurochkin, Yuekai Sun

Modern large language model (LLM) alignment techniques rely on human feedback, but it is unclear whether the techniques fundamentally limit the capabilities of aligned LLMs. In particular, it is unclear whether it is possible to align (stronger) LLMs with superhuman capabilities with (weaker) human feedback without degrading their capabilities. This is an instance of the weak-to-strong generalization problem: using weaker (less capable) feedback to train a stronger (more capable) model. We prove that weak-to-strong generalization is possible by eliciting latent knowledge from pre-trained LLMs.
In particular, we cast the weak-to-strong generalization problem as a transfer learning problem in which we wish to transfer a latent concept from a weak model to a strong pre-trained model. We prove that a naive fine-tuning approach suffers from fundamental limitations, but an alternative refinement-based approach suggested by the problem structure provably overcomes the limitations of fine-tuning. Finally, we demonstrate the practical applicability of the refinement approach with three LLM alignment tasks.

------------

`[2405.16455] On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization <https://arxiv.org/abs/2405.16455>`__

::

    Sun, 26 May 2024 07:00:05 GMT
    Jiancong Xiao, Ziniu Li, Xingyu Xie, Emily Getzen, Cong Fang, Qi Long, Weijie J. Su

Accurately aligning large language models (LLMs) with human preferences is crucial for informing fair, economically sound, and statistically efficient decision-making processes. However, we argue that reinforcement learning from human feedback (RLHF) -- the predominant approach for aligning LLMs with human preferences through a reward model -- suffers from an inherent algorithmic bias due to its Kullback--Leibler-based regularization in optimization. In extreme cases, this bias could lead to a phenomenon we term preference collapse, where minority preferences are virtually disregarded. To mitigate this algorithmic bias, we introduce preference matching (PM) RLHF, a novel approach that provably aligns LLMs with the preference distribution of the reward model under the Bradley--Terry--Luce/Plackett--Luce model. Central to our approach is a PM regularizer that takes the form of the negative logarithm of the LLM's policy probability distribution over responses, which helps the LLM balance response diversification and reward maximization. Notably, we obtain this regularizer by solving an ordinary differential equation that is necessary for the PM property. For practical implementation, we introduce a conditional variant of PM RLHF that is tailored to natural language generation. Finally, we empirically validate the effectiveness of conditional PM RLHF through experiments on the OPT-1.3B and Llama-2-7B models, demonstrating a 29% to 41% improvement in alignment with human preferences, as measured by a certain metric, compared to standard RLHF.

------------

`[2310.09130] Split-and-Denoise: Protect large language model inference with local differential privacy <https://arxiv.org/abs/2310.09130>`__

::

    replaced with revised version Sat, 25 May 2024 01:28:17 GMT
    Submission history From: Peihua Mai [view email]
    [v1] Fri, 13 Oct 2023 14:17:33 UTC (831 KB)
    [v2] Sat, 30 Dec 2023 01:57:16 UTC (1,841 KB)
    [v3] Sat, 25 May 2024 01:28:17 UTC (2,439 KB)
    Peihua Mai, Ran Yan, Zhe Huang, Youjia Yang, Yan Pang

Large Language Models (LLMs) excel in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the risk of privacy leakage due to direct text transmission to servers remains a critical concern. To address this, we introduce Split-N-Denoise (SnD), an private inference framework that splits the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD's effectiveness in optimizing the privacy-utility tradeoff across various LLM architectures and diverse downstream tasks. The results reveal an improvement in performance under the same privacy budget compared to the baselines by over 10\% on average, offering clients a privacy-preserving solution for local privacy protection.

------------

`[2403.03997] Guiding Enumerative Program Synthesis with Large Language Models <https://arxiv.org/abs/2403.03997>`__

::

    replaced with revised version Mon, 27 May 2024 12:18:40 GMT
    Submission history From: Yixuan Li [view email]
    [v1] Wed, 6 Mar 2024 19:13:53 UTC (48 KB)
    [v2] Mon, 27 May 2024 12:18:40 UTC (234 KB)
    Yixuan Li, Julian Parsert, Elizabeth Polgreen

Pre-trained Large Language Models (LLMs) are beginning to dominate the discourse around automatic code generation with natural language specifications. In contrast, the best-performing synthesizers in the domain of formal synthesis with precise logical specifications are still based on enumerative algorithms. In this paper, we evaluate the abilities of LLMs to solve formal synthesis benchmarks by carefully crafting a library of prompts for the domain. When one-shot synthesis fails, we propose a novel enumerative synthesis algorithm, which integrates calls to an LLM into a weighted probabilistic search. This allows the synthesizer to provide the LLM with information about the progress of the enumerator, and the LLM to provide the enumerator with syntactic guidance in an iterative loop. We evaluate our techniques on benchmarks from the Syntax-Guided Synthesis (SyGuS) competition. We find that GPT-3.5 as a stand-alone tool for formal synthesis is easily outperformed by state-of-the-art formal synthesis algorithms, but our approach integrating the LLM into an enumerative synthesis algorithm shows significant performance gains over both the LLM and the enumerative synthesizer alone and the winning SyGuS competition tool.

------------

`[2403.10171] AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation <https://arxiv.org/abs/2403.10171>`__

::

    replaced with revised version Mon, 27 May 2024 05:03:09 GMT
    Submission history From: Arkajit Datta [view email]
    [v1] Fri, 15 Mar 2024 10:27:17 UTC (1,488 KB)
    [v2] Mon, 27 May 2024 05:03:09 UTC (1,486 KB)
    Arkajit Datta, Tushar Verma, Rajat Chawla, Mukunda N.S, Ishaan Bhola

In recent advancements within the domain of Large Language Models (LLMs), there has been a notable emergence of agents capable of addressing Robotic Process Automation (RPA) challenges through enhanced cognitive capabilities and sophisticated reasoning. This development heralds a new era of scalability and human-like adaptability in goal attainment. In this context, we introduce AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic Operations and Deep Exploration). AUTONODE employs advanced neuro-graphical techniques to facilitate autonomous navigation and task execution on web interfaces, thereby obviating the necessity for predefined scripts or manual intervention. Our engine empowers agents to comprehend and implement complex workflows, adapting to dynamic web environments with unparalleled efficiency. Our methodology synergizes cognitive functionalities with robotic automation, endowing AUTONODE with the ability to learn from experience. We have integrated an exploratory module, DoRA (Discovery and mapping Operation for graph Retrieval Agent), which is instrumental in constructing a knowledge graph that the engine utilizes to optimize its actions and achieve objectives with minimal supervision. The versatility and efficacy of AUTONODE are demonstrated through a series of experiments, highlighting its proficiency in managing a diverse array of web-based tasks, ranging from data extraction to transaction processing.

------------

`[2403.12451] INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations <https://arxiv.org/abs/2403.12451>`__

::

    replaced with revised version Mon, 27 May 2024 04:30:01 GMT
    Submission history From: Lirui Luo [view email]
    [v1] Tue, 19 Mar 2024 05:21:20 UTC (4,298 KB)
    [v2] Mon, 27 May 2024 04:30:01 UTC (8,554 KB)
    Lirui Luo, Guoxi Zhang, Hongming Xu, Yaodong Yang, Cong Fang, Qing Li

Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising paradigm for explainable decision-making, characterized by the interpretability of symbolic policies. NS-RL entails structured state representations for tasks with visual observations, but previous methods are unable to refine the structured states with rewards due to a lack of efficiency. Accessibility also remains to be an issue, as extensive domain knowledge is required to interpret symbolic policies. In this paper, we present a framework for learning structured states and symbolic policies jointly, whose key idea is to distill vision foundation models into a scalable perception module and refine it during policy learning. Moreover, we design a pipeline to generate language explanations for policies and decisions using large language models. In experiments on nine Atari tasks, we verify the efficacy of our approach, and we also present explanations for policies and decisions.

------------

`[2404.14786] RealTCD: Temporal Causal Discovery from Interventional Data with Large Language Model <https://arxiv.org/abs/2404.14786>`__

::

    replaced with revised version Sun, 26 May 2024 13:08:00 GMT
    Submission history From: Peiwen Li [view email]
    [v1] Tue, 23 Apr 2024 06:52:40 UTC (506 KB)
    [v2] Sun, 26 May 2024 13:08:00 UTC (1,362 KB)
    Peiwen Li, Xin Wang, Zeyang Zhang, Yuan Meng, Fang Shen, Yue Li, Jialong Wang, Yang Li, Wenweu Zhu

In the field of Artificial Intelligence for Information Technology Operations, causal discovery is pivotal for operation and maintenance of graph construction, facilitating downstream industrial tasks such as root cause analysis. Temporal causal discovery, as an emerging method, aims to identify temporal causal relationships between variables directly from observations by utilizing interventional data. However, existing methods mainly focus on synthetic datasets with heavy reliance on intervention targets and ignore the textual information hidden in real-world systems, failing to conduct causal discovery for real industrial scenarios. To tackle this problem, in this paper we propose to investigate temporal causal discovery in industrial scenarios, which faces two critical challenges: 1) how to discover causal relationships without the interventional targets that are costly to obtain in practice, and 2) how to discover causal relations via leveraging the textual information in systems which can be complex yet abundant in industrial contexts. To address these challenges, we propose the RealTCD framework, which is able to leverage domain knowledge to discover temporal causal relationships without interventional targets. Specifically, we first develop a score-based temporal causal discovery method capable of discovering causal relations for root cause analysis without relying on interventional targets through strategic masking and regularization. Furthermore, by employing Large Language Models (LLMs) to handle texts and integrate domain knowledge, we introduce LLM-guided meta-initialization to extract the meta-knowledge from textual information hidden in systems to boost the quality of discovery. We conduct extensive experiments on simulation and real-world datasets to show the superiority of our proposed RealTCD framework over existing baselines in discovering temporal causal structures.

------------

`[2405.14391] Explainable Few-shot Knowledge Tracing <https://arxiv.org/abs/2405.14391>`__

::

    replaced with revised version Sun, 26 May 2024 03:43:33 GMT
    Submission history From: Haoxuan Li [view email]
    [v1] Thu, 23 May 2024 10:07:21 UTC (948 KB)
    [v2] Sun, 26 May 2024 03:43:33 UTC (948 KB)
    Haoxuan Li and Jifan Yu and Yuanxin Ouyang and Zhuang Liu and Wenge Rong and Juanzi Li and Zhang Xiong

Knowledge tracing (KT), aiming to mine students' mastery of knowledge by their exercise records and predict their performance on future test questions, is a critical task in educational assessment. While researchers achieved tremendous success with the rapid development of deep learning techniques, current knowledge tracing tasks fall into the cracks from real-world teaching scenarios. Relying heavily on extensive student data and solely predicting numerical performances differs from the settings where teachers assess students' knowledge state from limited practices and provide explanatory feedback. To fill this gap, we explore a new task formulation: Explainable Few-shot Knowledge Tracing. By leveraging the powerful reasoning and generation abilities of large language models (LLMs), we then propose a cognition-guided framework that can track the student knowledge from a few student records while providing natural language explanations. Experimental results from three widely used datasets show that LLMs can perform comparable or superior to competitive deep knowledge tracing methods. We also discuss potential directions and call for future improvements in relevant topics.

------------

`[2405.15766] Enhancing Adverse Drug Event Detection with Multimodal Dataset: Corpus Creation and Model Development <https://arxiv.org/abs/2405.15766>`__

::

    replaced with revised version Mon, 27 May 2024 02:55:45 GMT
    Submission history From: Pranab Sahoo [view email]
    [v1] Fri, 24 May 2024 17:58:42 UTC (6,866 KB)
    [v2] Mon, 27 May 2024 02:55:45 UTC (6,866 KB)
    Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Aman Chadha and Samrat Mondal

The mining of adverse drug events (ADEs) is pivotal in pharmacovigilance, enhancing patient safety by identifying potential risks associated with medications, facilitating early detection of adverse events, and guiding regulatory decision-making. Traditional ADE detection methods are reliable but slow, not easily adaptable to large-scale operations, and offer limited information. With the exponential increase in data sources like social media content, biomedical literature, and Electronic Medical Records (EMR), extracting relevant ADE-related information from these unstructured texts is imperative. Previous ADE mining studies have focused on text-based methodologies, overlooking visual cues, limiting contextual comprehension, and hindering accurate interpretation. To address this gap, we present a MultiModal Adverse Drug Event (MMADE) detection dataset, merging ADE-related textual information with visual aids. Additionally, we introduce a framework that leverages the capabilities of LLMs and VLMs for ADE detection by generating detailed descriptions of medical images depicting ADEs, aiding healthcare professionals in visually identifying adverse events. Using our MMADE dataset, we showcase the significance of integrating visual cues from images to enhance overall performance. This approach holds promise for patient safety, ADE awareness, and healthcare accessibility, paving the way for further exploration in personalized healthcare.

------------

`[2210.03696] LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models <https://arxiv.org/abs/2210.03696>`__

::

    replaced with revised version Sat, 25 May 2024 04:28:15 GMT
    Submission history From: Simin Chen [view email]
    [v1] Fri, 7 Oct 2022 17:01:01 UTC (1,076 KB)
    [v2] Sat, 25 May 2024 04:28:15 UTC (2,797 KB)
    Xiaoning Feng, Xiaohong Han, Simin Chen, Wei Yang

In this paper, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art LLMs. By analyzing the working mechanism and implementation of 20,543 public-accessible LLMs, we observe a fundamental property in LLMs that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that LLMs would have to go through enough iterations to satisfy the pre-configured threshold. We present \tool, which can work under both white-box setting and black-box setting. In the white-box scenario, \tool develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level. In the black-box scenario, \tool employs a causal inference-based approach to find critical tokens and similarly applies three levels of imperceptible perturbation to them. Both the white-box and black-box settings effectively delay the appearance of EOS, compelling these inputs to reach the naturally-unreachable threshold. To demonstrate the effectiveness of \tool, we conduct a systematic evaluation on nine public-available LLMs: Google T5, AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT and Salesforce CodeGen. Experimental results show that \tool can increase on average LLMs' response latency and energy consumption by 325\% to 3244\% and 344\% to 3616\%, respectively, by perturbing just one character or token in the input sentence.

------------

`[2302.10205] ChatIE: Zero-Shot Information Extraction via Chatting with ChatGPT <https://arxiv.org/abs/2302.10205>`__

::

    replaced with revised version Mon, 27 May 2024 07:08:31 GMT
    Submission history From: Xiang Wei [view email]
    [v1] Mon, 20 Feb 2023 12:57:12 UTC (10,298 KB)
    [v2] Mon, 27 May 2024 07:08:31 UTC (1,834 KB)
    Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wenjuan Han

Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources.

------------

`[2303.13013] GesGPT: Speech Gesture Synthesis With Text Parsing from GPT <https://arxiv.org/abs/2303.13013>`__

::

    replaced with revised version Mon, 27 May 2024 10:21:12 GMT
    Submission history From: Nan Gao [view email]
    [v1] Thu, 23 Mar 2023 03:30:30 UTC (8,831 KB)
    [v2] Mon, 27 May 2024 10:21:12 UTC (3,793 KB)
    Nan Gao, Zeyu Zhao, Zhi Zeng, Shuwu Zhang, Dongdong Weng, Yihua Bao

Gesture synthesis has gained significant attention as a critical research field, aiming to produce contextually appropriate and natural gestures corresponding to speech or textual input. Although deep learning-based approaches have achieved remarkable progress, they often overlook the rich semantic information present in the text, leading to less expressive and meaningful gestures. In this letter, we propose GesGPT, a novel approach to gesture generation that leverages the semantic analysis capabilities of large language models , such as ChatGPT. By capitalizing on the strengths of LLMs for text analysis, we adopt a controlled approach to generate and integrate professional gestures and base gestures through a text parsing script, resulting in diverse and meaningful gestures. Firstly, our approach involves the development of prompt principles that transform gesture generation into an intention classification problem using ChatGPT. We also conduct further analysis on emphasis words and semantic words to aid in gesture generation. Subsequently, we construct a specialized gesture lexicon with multiple semantic annotations, decoupling the synthesis of gestures into professional gestures and base gestures. Finally, we merge the professional gestures with base gestures. Experimental results demonstrate that GesGPT effectively generates contextually appropriate and expressive gestures.

------------

`[2304.11090] Towards Responsible and Safe AI in the Era of Foudnation Models: A Reference Architecture for Designing Foundation Model based Systems <https://arxiv.org/abs/2304.11090>`__

::

    replaced with revised version Sun, 26 May 2024 23:51:04 GMT
    Submission history From: Qinghua Lu [view email]
    [v1] Thu, 13 Apr 2023 05:01:03 UTC (430 KB)
    [v2] Tue, 23 May 2023 10:10:07 UTC (431 KB)
    [v3] Fri, 8 Dec 2023 05:27:24 UTC (465 KB)
    [v4] Sun, 26 May 2024 23:51:04 UTC (478 KB)
    Qinghua Lu, Liming Zhu, Xiwei Xu, Zhenchang Xing, Jon Whittle

The release of ChatGPT, Gemini, and other large language model has drawn huge interests on foundations models. There is a broad consensus that foundations models will be the fundamental building blocks for future AI systems. However, there is a lack of systematic guidance on the architecture design. Particularly, the the rapidly growing capabilities of foundations models can eventually absorb other components of AI systems, posing challenges of moving boundary and interface evolution in architecture design. Furthermore, incorporating foundations models into AI systems raises significant concerns about responsible and safe AI due to their opaque nature and rapidly advancing intelligence. To address these challenges, the paper first presents an architecture evolution of AI systems in the era of foundation models, transitioning from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithic architecture". The paper then identifies key design decisions and proposes a pattern-oriented reference architecture for designing responsible foundation-model-based systems. The patterns can enable the potential of foundation models while ensuring associated risks.

------------

`[2305.15067] Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References <https://arxiv.org/abs/2305.15067>`__

::

    replaced with revised version Sat, 25 May 2024 03:39:55 GMT
    Submission history From: Tianyi Tang [view email]
    [v1] Wed, 24 May 2023 11:53:29 UTC (161 KB)
    [v2] Wed, 3 Apr 2024 15:52:28 UTC (166 KB)
    [v3] Sat, 25 May 2024 03:39:55 UTC (165 KB)
    Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang, Dongdong Zhang, Wayne Xin Zhao, Tom Kocmi, Furu Wei

Most research about natural language generation (NLG) relies on evaluation benchmarks with limited references for a sample, which may result in poor correlations with human judgements. The underlying reason is that one semantic meaning can actually be expressed in different forms, and the evaluation with a single or few references may not accurately reflect the quality of the model's hypotheses. To address this issue, this paper presents a simple and effective method, named Div-Ref, to enhance existing evaluation benchmarks by enriching the number of references. We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible. We conduct comprehensive experiments to empirically demonstrate that diversifying the expression of reference can significantly enhance the correlation between automatic evaluation and human evaluation. This idea is compatible with recent LLM-based evaluation which can similarly derive advantages from incorporating multiple references. We strongly encourage future generation benchmarks to include more references, even if they are generated by LLMs, which is once for all. We release all the code and data at this https URL to facilitate research.

------------

`[2308.11534] PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator <https://arxiv.org/abs/2308.11534>`__

::

    replaced with revised version Mon, 27 May 2024 16:32:16 GMT
    Submission history From: Chuyi Kong [view email]
    [v1] Mon, 21 Aug 2023 06:51:56 UTC (685 KB)
    [v2] Wed, 23 Aug 2023 14:33:53 UTC (685 KB)
    [v3] Mon, 9 Oct 2023 15:39:21 UTC (2,347 KB)
    [v4] Thu, 12 Oct 2023 08:50:19 UTC (2,346 KB)
    [v5] Mon, 27 May 2024 16:32:16 UTC (3,222 KB)
    Chuyi Kong, Yaxin Fan, Xiang Wan, Feng Jiang, Benyou Wang

The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT dialogues, as evidenced by Vicuna. However, due to challenges in gathering dialogues involving human participation, current endeavors like Baize and UltraChat rely on ChatGPT conducting roleplay to simulate humans based on instructions, resulting in overdependence on seeds, diminished human-likeness, limited topic diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we propose a paradigm to simulate human behavior better and explore the benefits of incorporating more human-like questions in multi-turn conversations. Specifically, we directly target human questions extracted from genuine human-machine conversations as a learning goal and provide a novel user simulator called `Socratic'. The experimental results show our response model, `PlatoLM', achieves SoTA performance among LLaMA-based 7B models in MT-Bench. Our findings further demonstrate that our method introduces highly human-like questioning patterns and rich topic structures, which can teach the response model better than previous works in multi-round conversations.

------------

`[2309.05447] DoG-Instruct: Towards Premium Instruction-Tuning Data via Text-Grounded Instruction Wrapping <https://arxiv.org/abs/2309.05447>`__

::

    replaced with revised version Sat, 25 May 2024 10:05:52 GMT
    Submission history From: Yongrui Chen [view email]
    [v1] Mon, 11 Sep 2023 13:41:18 UTC (454 KB)
    [v2] Sat, 25 May 2024 10:05:52 UTC (7,396 KB)
    Yongrui Chen, Haiyun Jiang, Xinting Huang, Shuming Shi, Guilin Qi

The improvement of LLMs' instruction-following capabilities relies heavily on the availability of high-quality instruction-response pairs. Unfortunately, the current methods used to collect the pairs suffer from either unaffordable labor costs or severe hallucinations in the self-generation of LLM. To tackle these challenges, this paper proposes a scalable solution. It involves training LLMs to generate instruction-response pairs based on human-written documents, rather than relying solely on self-generation without context. Our proposed method not only exploits the advantages of human-written documents in reducing hallucinations but also utilizes an LLM to wrap the expression of documents, which enables us to bridge the gap between various document styles and the standard AI response. Experiments demonstrate that our method outperforms existing typical methods on multiple benchmarks. In particular, compared to the best-performing baseline, the LLM trained using our generated dataset exhibits a 10\% relative improvement in performance on AlpacaEval, despite utilizing only 1/5 of its training data. Furthermore, a comprehensive manual evaluation validates the quality of the data we generated. Our trained wrapper is publicly available at this https URL.

------------

`[2309.12288] The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A" <https://arxiv.org/abs/2309.12288>`__

::

    replaced with revised version Sun, 26 May 2024 17:45:21 GMT
    Submission history From: Owain Evans [view email]
    [v1] Thu, 21 Sep 2023 17:52:19 UTC (1,320 KB)
    [v2] Fri, 22 Sep 2023 18:08:20 UTC (1,319 KB)
    [v3] Thu, 4 Apr 2024 21:25:17 UTC (1,336 KB)
    [v4] Sun, 26 May 2024 17:45:21 UTC (1,336 KB)
    Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans

We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Valentina Tereshkova was the first woman to travel to space", it will not automatically be able to answer the question, "Who was the first woman to travel to space?". Moreover, the likelihood of the correct answer ("Valentina Tershkova") will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if "A is B" occurs, "B is A" is more likely to occur. It is worth noting, however, that if "A is B" appears in-context, models can deduce the reverse relationship. We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of Abyssal Melodies" and showing that they fail to correctly answer "Who composed Abyssal Melodies?". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly answers questions like the former 79% of the time, compared to 33% for the latter.
Code available at: this https URL.

------------

`[2310.04994] Distantly-Supervised Joint Extraction with Noise-Robust Learning <https://arxiv.org/abs/2310.04994>`__

::

    replaced with revised version Sat, 25 May 2024 18:20:12 GMT
    Submission history From: Yufei Li [view email]
    [v1] Sun, 8 Oct 2023 03:42:15 UTC (808 KB)
    [v2] Sat, 25 May 2024 18:20:12 UTC (8,460 KB)
    Yufei Li, Xiao Yu, Yanghong Guo, Yanchi Liu, Haifeng Chen, Cong Liu

Joint entity and relation extraction is a process that identifies entity pairs and their relations using a single model. We focus on the problem of joint extraction in distantly-labeled data, whose labels are generated by aligning entity mentions with the corresponding entity and relation tags using a knowledge base (KB). One key challenge is the presence of noisy labels arising from both incorrect entity and relation annotations, which significantly impairs the quality of supervised learning. Existing approaches, either considering only one source of noise or making decisions using external knowledge, cannot well-utilize significant information in the training data. We propose DENRL, a generalizable framework that 1) incorporates a lightweight transformer backbone into a sequence labeling scheme for joint tagging, and 2) employs a noise-robust framework that regularizes the tagging model with significant relation patterns and entity-relation dependencies, then iteratively self-adapts to instances with less noise from both sources. Surprisingly, experiments on two benchmark datasets show that DENRL, using merely its own parametric distribution and simple data-driven heuristics, outperforms large language model-based baselines by a large margin with better interpretability.

------------

`[2312.10897] Generalized Category Discovery with Large Language Models in the Loop <https://arxiv.org/abs/2312.10897>`__

::

    replaced with revised version Mon, 27 May 2024 03:27:57 GMT
    Submission history From: An Wenbin [view email]
    [v1] Mon, 18 Dec 2023 02:55:14 UTC (486 KB)
    [v2] Mon, 27 May 2024 03:27:57 UTC (486 KB)
    Wenbin An, Wenkai Shi, Feng Tian, Haonan Lin, QianYing Wang, Yaqiang Wu, Mingxiang Cai, Luyan Wang, Yan Chen, Haiping Zhu, Ping Chen

Generalized Category Discovery (GCD) is a crucial task that aims to recognize both known and novel categories from a set of unlabeled data by utilizing a few labeled data with only known categories. Due to the lack of supervision and category information, current methods usually perform poorly on novel categories and struggle to reveal semantic meanings of the discovered clusters, which limits their applications in the real world. To mitigate the above issues, we propose Loop, an end-to-end active-learning framework that introduces Large Language Models (LLMs) into the training loop, which can boost model performance and generate category names without relying on any human efforts. Specifically, we first propose Local Inconsistent Sampling (LIS) to select samples that have a higher probability of falling to wrong clusters, based on neighborhood prediction consistency and entropy of cluster assignment probabilities. Then we propose a Scalable Query strategy to allow LLMs to choose true neighbors of the selected samples from multiple candidate samples. Based on the feedback from LLMs, we perform Refined Neighborhood Contrastive Learning (RNCL) to pull samples and their neighbors closer to learn clustering-friendly representations. Finally, we select representative samples from clusters corresponding to novel categories to allow LLMs to generate category names for them. Extensive experiments on three benchmark datasets show that Loop outperforms SOTA models by a large margin and generates accurate category names for the discovered clusters. Code and data are available at this https URL.

------------

`[2312.14591] Reasons to Reject? Aligning Language Models with Judgments <https://arxiv.org/abs/2312.14591>`__

::

    replaced with revised version Mon, 27 May 2024 12:22:14 GMT
    Submission history From: Weiwen Xu [view email]
    [v1] Fri, 22 Dec 2023 10:29:43 UTC (449 KB)
    [v2] Mon, 27 May 2024 12:22:14 UTC (472 KB)
    Weiwen Xu, Deng Cai, Zhisong Zhang, Wai Lam, Shuming Shi

As humans, we consistently interact with our peers and receive feedback in the form of natural language. This language feedback allows us to maintain appropriate behavior, and rectify potential errors. The question arises naturally: can we use language feedback to align large language models (LLMs)? In contrast to previous research that aligns LLMs with scalar rewards, we present the first systematic exploration of alignment through the lens of language feedback (i.e., judgment). We start with an in-depth investigation of potential methods that can be adapted for aligning LLMs with judgments, revealing that these methods cannot fully capitalize on judgments. To facilitate more effective utilization of judgments, we propose a novel framework, Contrastive Unlikelihood Training (CUT), that allows for fine-grained inappropriate content detection and correction based on judgments. Our results show that, with merely 1317 off-the-shelf judgment data, CUT (LLaMA2-13b) can beat the 175B DaVinci003 and surpass the best baseline by 48.51 points on AlpacaEval. CUT (LLaMA2-chat-13b) can also align LLMs in an iterative fashion using up-to-date model-specific judgments, improving performance from 81.09 to 91.68 points on AlpacaEval. Further analysis suggests that judgments hold greater potential than rewards in LLM alignment.

------------

`[2401.16332] Tradeoffs Between Alignment and Helpfulness in Language Models with Representation Engineering <https://arxiv.org/abs/2401.16332>`__

::

    replaced with revised version Sun, 26 May 2024 16:07:55 GMT
    Submission history From: Yotam Wolf [view email]
    [v1] Mon, 29 Jan 2024 17:38:14 UTC (1,135 KB)
    [v2] Mon, 5 Feb 2024 14:53:13 UTC (1,091 KB)
    [v3] Sun, 26 May 2024 16:07:55 UTC (2,300 KB)
    Yotam Wolf, Noam Wies, Dorin Shteyman, Binyamin Rothberg, Yoav Levine, and Amnon Shashua

Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. First, we find that under the conditions of our framework, alignment can be guaranteed with representation engineering, and at the same time that helpfulness is harmed in the process. Second, we show that helpfulness is harmed quadratically with the norm of the representation engineering vector, while the alignment increases linearly with it, indicating a regime in which it is efficient to use representation engineering. We validate our findings empirically, and chart the boundaries to the usefulness of representation engineering for alignment.

------------

`[2401.17623] Neighboring Perturbations of Knowledge Editing on Large Language Models <https://arxiv.org/abs/2401.17623>`__

::

    replaced with revised version Mon, 27 May 2024 03:52:43 GMT
    Submission history From: Jia-Chen Gu [view email]
    [v1] Wed, 31 Jan 2024 06:49:36 UTC (561 KB)
    [v2] Mon, 27 May 2024 03:52:43 UTC (589 KB)
    Jun-Yu Ma, Zhen-Hua Ling, Ningyu Zhang, Jia-Chen Gu

Despite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play framework termed Appending via Preservation and Prevention (APP) is proposed to mitigate the neighboring perturbation by maintaining the integrity of the answer list. Experiments demonstrate the effectiveness of APP coupling with four editing methods on four LLMs. The code and data are available at this https URL.

------------

`[2402.00956] Exploring Spatial Schema Intuitions in Large Language and Vision Models <https://arxiv.org/abs/2402.00956>`__

::

    replaced with revised version Mon, 27 May 2024 14:29:31 GMT
    Submission history From: Philipp Wicke [view email]
    [v1] Thu, 1 Feb 2024 19:25:50 UTC (595 KB)
    [v2] Mon, 27 May 2024 14:29:31 UTC (8,506 KB)
    Philipp Wicke and Lennart Wachowiak

Despite the ubiquity of large language models (LLMs) in AI research, the question of embodiment in LLMs remains underexplored, distinguishing them from embodied systems in robotics where sensory perception directly informs physical action. Our investigation navigates the intriguing terrain of whether LLMs, despite their non-embodied nature, effectively capture implicit human intuitions about fundamental, spatial building blocks of language. We employ insights from spatial cognitive foundations developed through early sensorimotor experiences, guiding our exploration through the reproduction of three psycholinguistic experiments. Surprisingly, correlations between model outputs and human responses emerge, revealing adaptability without a tangible connection to embodied experiences. Notable distinctions include polarized language model responses and reduced correlations in vision language models. This research contributes to a nuanced understanding of the interplay between language, spatial experiences, and the computations made by large language models. More at this https URL

------------

`[2402.03190] Unified Hallucination Detection for Multimodal Large Language Models <https://arxiv.org/abs/2402.03190>`__

::

    replaced with revised version Mon, 27 May 2024 11:52:56 GMT
    Submission history From: Xiang Chen [view email]
    [v1] Mon, 5 Feb 2024 16:56:11 UTC (8,789 KB)
    [v2] Fri, 16 Feb 2024 15:40:31 UTC (6,239 KB)
    [v3] Tue, 20 Feb 2024 16:47:16 UTC (6,240 KB)
    [v4] Mon, 27 May 2024 11:52:56 UTC (6,250 KB)
    Xiang Chen and Chenxi Wang and Yida Xue and Ningyu Zhang and Xiaoyan Yang and Qiang Li and Yue Shen and Lei Liang and Jinjie Gu and Huajun Chen

Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.

------------

`[2402.03848] ANLS* -- A Universal Document Processing Metric for Generative Large Language Models <https://arxiv.org/abs/2402.03848>`__

::

    replaced with revised version Sat, 25 May 2024 06:31:45 GMT
    Submission history From: David Peer [view email]
    [v1] Tue, 6 Feb 2024 09:50:08 UTC (184 KB)
    [v2] Tue, 27 Feb 2024 13:14:28 UTC (184 KB)
    [v3] Thu, 21 Mar 2024 05:58:10 UTC (185 KB)
    [v4] Tue, 16 Apr 2024 09:14:46 UTC (186 KB)
    [v5] Sat, 25 May 2024 06:31:45 UTC (186 KB)
    David Peer, Philemon Sch\"opf, Volckmar Nebendahl, Alexander Rietzler, Sebastian Stabinger

Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs.
This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and is still compatible with previously reported ANLS scores. An evaluation of 7 different datasets, 6 different GLLMs and 3 different prompting methods using the ANLS* metric is also provided, demonstrating the importance of the proposed metric.
We also benchmark a novel approach to generate prompts for documents, called SFT, against other prompting techniques such as LATIN. In 27 out of 35 cases, SFT outperforms other techniques and improves the state-of-the-art, sometimes by as much as $18$ percentage points.
Sources are available at this https URL

------------

`[2402.04315] Training Language Models to Generate Text with Citations via Fine-grained Rewards <https://arxiv.org/abs/2402.04315>`__

::

    replaced with revised version Mon, 27 May 2024 09:32:15 GMT
    Submission history From: Chengyu Huang [view email]
    [v1] Tue, 6 Feb 2024 19:00:40 UTC (7,723 KB)
    [v2] Mon, 27 May 2024 09:32:15 UTC (9,421 KB)
    Chengyu Huang, Zeqiu Wu, Yushi Hu, Wenya Wang

While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly prompted LLMs to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller LLMs. In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common LLM training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on Question Answering (QA) datasets taken from the ALCE benchmark and validate the model's generalizability using EXPERTQA. On LLaMA-2-7B, the incorporation of fine-grained rewards achieves the best performance among the baselines, even surpassing that of GPT-3.5-turbo.

------------

`[2402.04624] MEMORYLLM: Towards Self-Updatable Large Language Models <https://arxiv.org/abs/2402.04624>`__

::

    replaced with revised version Sun, 26 May 2024 23:06:32 GMT
    Submission history From: Yu Wang [view email]
    [v1] Wed, 7 Feb 2024 07:14:11 UTC (1,297 KB)
    [v2] Sun, 26 May 2024 23:06:32 UTC (1,931 KB)
    Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, Julian McAuley

Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer. MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates. Our code and model are open-sourced at this https URL.

------------

`[2402.12146] Enabling Weak LLMs to Judge Response Reliability via Meta Ranking <https://arxiv.org/abs/2402.12146>`__

::

    replaced with revised version Sun, 26 May 2024 17:46:42 GMT
    Submission history From: Zijun Liu [view email]
    [v1] Mon, 19 Feb 2024 13:57:55 UTC (8,907 KB)
    [v2] Sun, 26 May 2024 17:46:42 UTC (9,415 KB)
    Zijun Liu, Boqun Kou, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu

Despite the strong performance of large language models (LLMs) across a wide range of tasks, they still have reliability issues. Previous studies indicate that strong LLMs like GPT-4-turbo excel in evaluating the reliability of responses from LLMs, but face efficiency and local deployment issues. Thus, to enable weak LLMs to effectively assess the reliability of LLM responses, we propose a novel cross-query-comparison-based method called $\textit{Meta Ranking}$ (MR). Unlike previous few-shot methods that solely based on in-context learning capabilities in LLMs, MR assesses reliability by pairwisely ranking the target query-response pair with multiple reference query-response pairs. We found that MR is highly effective in error detection for LLM responses, where weak LLMs, such as Phi-2, could surpass strong baselines like GPT-3.5-turbo, requiring only five reference samples and significantly improving efficiency. We further demonstrate that MR can enhance strong LLMs' performance in two practical applications: model cascading and instruction tuning. In model cascading, we combine open- and closed-source LLMs to achieve performance comparable to GPT-4-turbo with lower costs. In instruction tuning, we use MR for iterative training data filtering, significantly reducing data processing time and enabling LLaMA-7B and Phi-2 to surpass Alpaca-13B with fewer training tokens. These results underscore the high potential of MR in both efficiency and effectiveness.

------------

`[2402.12193] A Chinese Dataset for Evaluating the Safeguards in Large Language Models <https://arxiv.org/abs/2402.12193>`__

::

    replaced with revised version Sun, 26 May 2024 17:15:44 GMT
    Submission history From: Yuxia Wang [view email]
    [v1] Mon, 19 Feb 2024 14:56:18 UTC (99 KB)
    [v2] Sun, 26 May 2024 17:15:44 UTC (111 KB)
    Yuxia Wang, Zenan Zhai, Haonan Li, Xudong Han, Lizhi Lin, Zhenxuan Zhang, Jingru Zhao, Preslav Nakov, Timothy Baldwin

Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks are the prevalent type of risk, presenting the major issue with all Chinese LLMs we experimented with. Our data is available at this https URL. Warning: this paper contains example data that may be offensive, harmful, or biased.

------------

`[2402.12847] Instruction-tuned Language Models are Better Knowledge Learners <https://arxiv.org/abs/2402.12847>`__

::

    replaced with revised version Sun, 26 May 2024 03:19:48 GMT
    Submission history From: Zhengbao Jiang [view email]
    [v1] Tue, 20 Feb 2024 09:20:32 UTC (755 KB)
    [v2] Sun, 26 May 2024 03:19:48 UTC (755 KB)
    Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Victoria Lin, Wen-tau Yih, Srinivasan Iyer

In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that pre-instruction-tuning significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.

------------

`[2402.14457] Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts <https://arxiv.org/abs/2402.14457>`__

::

    replaced with revised version Mon, 27 May 2024 09:06:41 GMT
    Submission history From: Pietro Giovanni Bizzaro [view email]
    [v1] Thu, 22 Feb 2024 11:24:45 UTC (757 KB)
    [v2] Mon, 27 May 2024 09:06:41 UTC (401 KB)
    Pietro Giovanni Bizzaro, Elena Della Valentina, Maurizio Napolitano, Nadia Mana and Massimo Zancanaro

In this paper, we propose a new annotation scheme to classify different types of clauses in Terms-and-Conditions contracts with the ultimate goal of supporting legal experts to quickly identify and assess problematic issues in this type of legal documents. To this end, we built a small corpus of Terms-and-Conditions contracts and finalized an annotation scheme of 14 categories, eventually reaching an inter-annotator agreement of 0.92. Then, for 11 of them, we experimented with binary classification tasks using few-shot prompting with a multilingual T5 and two fine-tuned versions of two BERT-based LLMs for Italian. Our experiments showed the feasibility of automatic classification of our categories by reaching accuracies ranging from .79 to .95 on validation tasks.

------------

`[2402.14700] Unveiling Linguistic Regions in Large Language Models <https://arxiv.org/abs/2402.14700>`__

::

    replaced with revised version Mon, 27 May 2024 06:35:25 GMT
    Submission history From: Zhihao Zhang [view email]
    [v1] Thu, 22 Feb 2024 16:56:13 UTC (2,348 KB)
    [v2] Mon, 27 May 2024 06:35:25 UTC (5,223 KB)
    Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, Xuanjing Huang

Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct monolingual regions exist for different languages, and disruption to these specific regions substantially reduces the LLMs' proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common phenomenon observed during further pre-training of LLMs. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence.

------------

`[2402.14710] IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus <https://arxiv.org/abs/2402.14710>`__

::

    replaced with revised version Sun, 26 May 2024 15:54:41 GMT
    Submission history From: Ningyu Zhang [view email]
    [v1] Thu, 22 Feb 2024 17:11:38 UTC (1,480 KB)
    [v2] Mon, 8 Apr 2024 06:01:42 UTC (1,475 KB)
    [v3] Sun, 26 May 2024 15:54:41 UTC (1,477 KB)
    Honghao Gui, Lin Yuan, Hongbin Ye, Ningyu Zhang, Mengshu Sun, Lei Liang, Huajun Chen

Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimentally, IEPile enhance the performance of LLMs for IE, with notable improvements in zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.

------------

`[2402.15238] GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection? <https://arxiv.org/abs/2402.15238>`__

::

    replaced with revised version Mon, 27 May 2024 13:14:12 GMT
    Submission history From: Yiping Jin [view email]
    [v1] Fri, 23 Feb 2024 10:02:01 UTC (1,586 KB)
    [v2] Mon, 27 May 2024 13:14:12 UTC (1,587 KB)
    Yiping Jin, Leo Wanner, Alexander Shvets

Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind "You are just a [slur] to me." However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs). We employ an additional natural language inference (NLI) model to verify the generations. Crowd-sourced annotation demonstrates that the generated test cases are of high quality. Using the new functional tests, we can uncover model weaknesses that would be overlooked using the original HateCheck dataset.

------------

`[2402.15481] Prejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models <https://arxiv.org/abs/2402.15481>`__

::

    replaced with revised version Fri, 24 May 2024 20:02:10 GMT
    Submission history From: Ke Yang [view email]
    [v1] Fri, 23 Feb 2024 18:15:56 UTC (8,549 KB)
    [v2] Mon, 26 Feb 2024 03:55:51 UTC (8,551 KB)
    [v3] Thu, 29 Feb 2024 22:50:10 UTC (8,558 KB)
    [v4] Fri, 24 May 2024 20:02:10 UTC (10,112 KB)
    Y Liu, K Yang, Z Qi, X Liu, Y Yu, C Zhai

This study investigates why and how inconsistency in the generation of Large Language Models (LLMs) might induce or exacerbate societal injustice. For instance, LLMs frequently exhibit contrasting gender stereotypes regarding the same career depending on varied contexts, highlighting the arguably harmful unpredictability of LLMs' behavioral patterns. To augment the existing discrimination assessment with the capability to account for variation in LLM generation, we formulate the Prejudice-Volatility Framework (PVF) that precisely defines behavioral metrics for assessing LLMs, which delineate the probability distribution of LLMs' stereotypes from the perspective of token prediction probability. Specifically, we employ a data-mining approach to approximate the possible applied contexts of LLMs and devise statistical metrics to evaluate the corresponding contextualized societal discrimination risk. Further, we mathematically dissect the aggregated discrimination risk of LLMs into prejudice risk, originating from their system bias, and volatility risk, stemming from their generation inconsistency. While initially intended for assessing discrimination in LLMs, our proposed PVF facilitates the comprehensive and flexible measurement of any inductive biases, including knowledge alongside prejudice, across various modality models.
We apply PVF to 12 most commonly adopted LLMs and compare their risk levels. Our findings reveal that: i) prejudice risk is the primary cause of discrimination risk in LLMs, indicating that inherent biases in these models lead to stereotypical outputs; ii) most LLMs exhibit significant pro-male stereotypes across nearly all careers; iii) alignment with Reinforcement Learning from Human Feedback lowers discrimination by reducing prejudice, but increases volatility; iv) discrimination risk in LLMs correlates with socio-economic factors like profession salaries.

------------

`[2402.16107] FuseChat: Knowledge Fusion of Chat Models <https://arxiv.org/abs/2402.16107>`__

::

    replaced with revised version Mon, 27 May 2024 10:16:51 GMT
    Submission history From: Fanqi Wan [view email]
    [v1] Sun, 25 Feb 2024 15:11:58 UTC (243 KB)
    [v2] Tue, 27 Feb 2024 04:48:36 UTC (243 KB)
    [v3] Sun, 3 Mar 2024 07:21:36 UTC (243 KB)
    [v4] Mon, 27 May 2024 10:16:51 UTC (243 KB)
    Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang, Wei Bi

While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, FuseLLM introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the FuseLLM framework to realize the fusion of chat LLMs, resulting in FuseChat. FuseChat comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs to derive multiple target LLMs of identical structure and size via lightweight fine-tuning. Then, these target LLMs are merged within the parameter space, wherein we propose a novel method for determining the merging weights based on the variation ratio of parameter matrices before and after fine-tuning. We validate our approach using three prominent chat LLMs with diverse architectures and scales, namely NH2-Mixtral-8x7B, NH2-Solar-10.7B, and OpenChat-3.5-7B. Experimental results spanning various chat domains demonstrate the superiority of FuseChat-7B across a broad spectrum of chat LLMs at 7B and 34B scales, even surpassing GPT-3.5 (March) and approaching Mixtral-8x7B-Instruct. Our code, model weights, and data are openly accessible at \url{this https URL}.

------------

`[2403.00812] LoRA Meets Dropout under a Unified Framework <https://arxiv.org/abs/2403.00812>`__

::

    replaced with revised version Mon, 27 May 2024 02:16:43 GMT
    Submission history From: Sheng Wang [view email]
    [v1] Sun, 25 Feb 2024 07:09:10 UTC (10,044 KB)
    [v2] Mon, 27 May 2024 02:16:43 UTC (10,045 KB)
    Sheng Wang, Liheng Chen, Jiyue Jiang, Boyang Xue, Lingpeng Kong, Chuan Wu

With the remarkable capabilities, large language models (LLMs) have emerged as essential elements in numerous NLP applications, while parameter-efficient finetuning, especially LoRA, has gained popularity as a lightweight approach for model customization. Meanwhile, various dropout methods, initially designed for full finetuning with all the parameters updated, alleviates overfitting associated with excessive parameter redundancy. Hence, a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked. To fill this gap, we first confirm that parameter-efficient LoRA is also overfitting-prone. We then revisit transformer-specific dropout methods, and establish their equivalence and distinctions mathematically and empirically. Building upon this comparative analysis, we introduce a unified framework for a comprehensive investigation, which instantiates these methods based on dropping position, structural pattern and compensation measure. Through this framework, we reveal the new preferences and performance comparisons of them when involved with limited trainable parameters. This framework also allows us to amalgamate the most favorable aspects into a novel dropout method named HiddenKey. Extensive experiments verify the remarkable superiority and sufficiency of HiddenKey across multiple models and tasks, which highlights it as the preferred approach for high-performance and parameter-efficient finetuning of LLMs.

------------

`[2403.01241] IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact <https://arxiv.org/abs/2403.01241>`__

::

    replaced with revised version Sat, 25 May 2024 10:33:03 GMT
    Submission history From: Ruikang Liu [view email]
    [v1] Sat, 2 Mar 2024 16:05:26 UTC (33,839 KB)
    [v2] Sat, 25 May 2024 10:33:03 UTC (33,072 KB)
    Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, Chun Yuan

Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance. This paper unveils a previously overlooked type of outliers in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which are crucial to the performance of quantized LLMs. Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model. The approach is simple and easy to combine with existing quantization solutions with no extra inference overhead. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further with minimal training costs. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error. Empirical results show that IntactKV brings consistent improvement over various quantization methods across different LLMs and downstream tasks, leading to the new state-of-the-art for LLM quantization. The codes are available at this https URL.

------------

`[2403.01244] Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal <https://arxiv.org/abs/2403.01244>`__

::

    replaced with revised version Sat, 25 May 2024 12:17:29 GMT
    Submission history From: Jianheng Huang [view email]
    [v1] Sat, 2 Mar 2024 16:11:23 UTC (397 KB)
    [v2] Sat, 25 May 2024 12:17:29 UTC (399 KB)
    Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao, Jinsong Su

Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains.

------------

`[2403.02674] Revisiting Meta-evaluation for Grammatical Error Correction <https://arxiv.org/abs/2403.02674>`__

::

    replaced with revised version Sun, 26 May 2024 12:05:35 GMT
    Submission history From: Masamune Kobayashi [view email]
    [v1] Tue, 5 Mar 2024 05:53:09 UTC (1,405 KB)
    [v2] Sun, 26 May 2024 12:05:35 UTC (1,310 KB)
    Masamune Kobayashi, Masato Mita, Mamoru Komachi

Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges including biases caused by inconsistencies in evaluation granularity, and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models (LLMs), and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation, suggest that edit-based metrics may have been underestimated in existing studies. Furthermore, correlations of most metrics decrease when changing from classical to neural systems, indicating that traditional metrics are relatively poor at evaluating fluently corrected sentences with many edits.

------------

`[2403.02715] Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models <https://arxiv.org/abs/2403.02715>`__

::

    replaced with revised version Sun, 26 May 2024 17:13:32 GMT
    Submission history From: Dong Le Duc [view email]
    [v1] Tue, 5 Mar 2024 07:13:28 UTC (84 KB)
    [v2] Sun, 26 May 2024 17:13:32 UTC (248 KB)
    Sang T. Truong, Duc Q. Nguyen, Toan Nguyen, Dong D. Le, Nhi N. Truong, Tho Quan, Sanmi Koyejo

Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights underscore the significance of meticulous fine-tuning with high-quality datasets in enhancing LLM performance.

------------

`[2403.07088] SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation <https://arxiv.org/abs/2403.07088>`__

::

    replaced with revised version Sat, 25 May 2024 11:19:31 GMT
    Submission history From: Yanming Liu [view email]
    [v1] Mon, 11 Mar 2024 18:26:02 UTC (1,663 KB)
    [v2] Sat, 25 May 2024 11:19:31 UTC (910 KB)
    Yanming Liu, Xinyue Peng, Jiannan Cao, Le Dai, Xingzu Liu, Weihao Liu, Mingbang Wang

Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require substantial memory storage on low-resource devices. More critically, the computational speed on these devices is also severely limited. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and featured personal feature. Further more, SPA provides a framework to keep feature-base parameters on low computational devices while leave the parameters containing general information on the high computational devices.

------------

`[2403.17540] Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction <https://arxiv.org/abs/2403.17540>`__

::

    replaced with revised version Sun, 26 May 2024 11:55:11 GMT
    Submission history From: Masamune Kobayashi [view email]
    [v1] Tue, 26 Mar 2024 09:43:15 UTC (673 KB)
    [v2] Sun, 26 May 2024 11:55:11 UTC (675 KB)
    Masamune Kobayashi, Masato Mita, Mamoru Komachi

Large Language Models (LLMs) have been reported to outperform existing automatic evaluation metrics in some tasks, such as text summarization and machine translation. However, there has been a lack of research on LLMs as evaluators in grammatical error correction (GEC). In this study, we investigate the performance of LLMs in GEC evaluation by employing prompts designed to incorporate various evaluation criteria inspired by previous research. Our extensive experimental results demonstrate that GPT-4 achieved Kendall's rank correlation of 0.662 with human judgments, surpassing all existing methods. Furthermore, in recent GEC evaluations, we have underscored the significance of the LLMs scale and particularly emphasized the importance of fluency among evaluation criteria.

------------

`[2404.07922] LaVy: Vietnamese Multimodal Large Language Model <https://arxiv.org/abs/2404.07922>`__

::

    replaced with revised version Sun, 26 May 2024 10:27:51 GMT
    Submission history From: Tran Chi [view email]
    [v1] Thu, 11 Apr 2024 17:09:28 UTC (11,019 KB)
    [v2] Sat, 13 Apr 2024 13:57:51 UTC (11,019 KB)
    [v3] Tue, 16 Apr 2024 15:33:45 UTC (11,193 KB)
    [v4] Wed, 17 Apr 2024 03:23:33 UTC (11,193 KB)
    [v5] Sun, 26 May 2024 10:27:51 UTC (11,193 KB)
    Chi Tran and Huong Le Thanh

Large Language Models (LLMs) and Multimodal Large language models (MLLMs) have taken the world by storm with impressive abilities in complex reasoning and linguistic comprehension. Meanwhile there are plethora of works related to Vietnamese Large Language Models, the lack of high-quality resources in multimodality limits the progress of Vietnamese MLLMs. In this paper, we pioneer in address this by introducing LaVy, a state-of-the-art Vietnamese MLLM, and we also introduce LaVy-Bench benchmark designated for evaluating MLLMs's understanding on Vietnamese visual language tasks. Our project is public at this https URL

------------

`[2405.09719] Spectral Editing of Activations for Large Language Model Alignment <https://arxiv.org/abs/2405.09719>`__

::

    replaced with revised version Sat, 25 May 2024 16:08:23 GMT
    Submission history From: Yifu Qiu [view email]
    [v1] Wed, 15 May 2024 22:28:23 UTC (628 KB)
    [v2] Sat, 25 May 2024 16:08:23 UTC (634 KB)
    Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, Shay B. Cohen

Large language models (LLMs) often exhibit undesirable behaviours, such as generating untruthful or biased content. Editing their internal representations has been shown to be effective in mitigating such behaviours on top of the existing alignment methods. We propose a novel inference-time editing method, namely spectral editing of activations (SEA), to project the input representations into directions with maximal covariance with the positive demonstrations (e.g., truthful) while minimising covariance with the negative demonstrations (e.g., hallucinated). We also extend our method to non-linear editing using feature functions. We run extensive experiments on benchmarks concerning truthfulness and bias with six open-source LLMs of different sizes and model families. The results demonstrate the superiority of SEA in effectiveness, generalisation to similar tasks, as well as computation and data efficiency. We also show that SEA editing only has a limited negative impact on other model capabilities.

------------

`[2405.10650] SPOR: A Comprehensive and Practical Evaluation Method for Compositional Generalization in Data-to-Text Generation <https://arxiv.org/abs/2405.10650>`__

::

    replaced with revised version Sat, 25 May 2024 19:37:00 GMT
    Submission history From: Ziyao Xu [view email]
    [v1] Fri, 17 May 2024 09:25:30 UTC (265 KB)
    [v2] Mon, 20 May 2024 07:56:47 UTC (265 KB)
    [v3] Tue, 21 May 2024 08:31:05 UTC (265 KB)
    [v4] Wed, 22 May 2024 18:22:23 UTC (265 KB)
    [v5] Sat, 25 May 2024 19:37:00 UTC (266 KB)
    Ziyao Xu, Houfeng Wang

Compositional generalization is an important ability of language models and has many different manifestations. For data-to-text generation, previous research on this ability is limited to a single manifestation called Systematicity and lacks consideration of large language models (LLMs), which cannot fully cover practical application scenarios. In this work, we propose SPOR, a comprehensive and practical evaluation method for compositional generalization in data-to-text generation. SPOR includes four aspects of manifestations (Systematicity, Productivity, Order invariance, and Rule learnability) and allows high-quality evaluation without additional manual annotations based on existing datasets. We demonstrate SPOR on two different datasets and evaluate some existing language models including LLMs. We find that the models are deficient in various aspects of the evaluation and need further improvement. Our work shows the necessity for comprehensive research on different manifestations of compositional generalization in data-to-text generation and provides a framework for evaluation.

------------

`[2405.11577] A Multi-Perspective Analysis of Memorization in Large Language Models <https://arxiv.org/abs/2405.11577>`__

::

    replaced with revised version Mon, 27 May 2024 04:41:02 GMT
    Submission history From: Bowen Chen [view email]
    [v1] Sun, 19 May 2024 15:00:50 UTC (5,855 KB)
    [v2] Mon, 27 May 2024 04:41:02 UTC (6,220 KB)
    Bowen Chen, Namgi Han, Yusuke Miyao

Large Language Models (LLMs), trained on massive corpora with billions of parameters, show unprecedented performance in various fields. Though surprised by their excellent performances, researchers also noticed some special behaviors of those LLMs. One of those behaviors is memorization, in which LLMs can generate the same content used to train them. Though previous research has discussed memorization, the memorization of LLMs still lacks explanation, especially the cause of memorization and the dynamics of generating them. In this research, we comprehensively discussed memorization from various perspectives and extended the discussion scope to not only just the memorized content but also less and unmemorized content. Through various studies, we found that: (1) Through experiments, we revealed the relation of memorization between model size, continuation size, and context size. Further, we showed how unmemorized sentences transition to memorized sentences. (2) Through embedding analysis, we showed the distribution and decoding dynamics across model size in embedding space for sentences with different memorization scores. The n-gram statistics analysis presents d (3) An analysis over n-gram and entropy decoding dynamics discovered a boundary effect when the model starts to generate memorized sentences or unmemorized sentences. (4)We trained a Transformer model to predict the memorization of different models, showing that it is possible to predict memorizations by context.

------------

`[2405.13845] Semantic Density: Uncertainty Quantification in Semantic Space for Large Language Models <https://arxiv.org/abs/2405.13845>`__

::

    replaced with revised version Sat, 25 May 2024 07:20:46 GMT
    Submission history From: Xin Qiu [view email]
    [v1] Wed, 22 May 2024 17:13:49 UTC (137 KB)
    [v2] Sat, 25 May 2024 07:20:46 UTC (137 KB)
    Xin Qiu, Risto Miikkulainen

With the widespread application of Large Language Models (LLMs) to various domains, concerns regarding the trustworthiness of LLMs in safety-critical scenarios have been raised, due to their unpredictable tendency to hallucinate and generate misinformation. Existing LLMs do not have an inherent functionality to provide the users with an uncertainty metric for each response it generates, making it difficult to evaluate trustworthiness. Although a number of works aim to develop uncertainty quantification methods for LLMs, they have fundamental limitations, such as being restricted to classification tasks, requiring additional training and data, considering only lexical instead of semantic information, and being prompt-wise but not response-wise. A new framework is proposed in this paper to address these issues. Semantic density extracts uncertainty information for each response from a probability distribution perspective in semantic space. It has no restriction on task types and is "off-the-shelf" for new models and tasks. Experiments on seven state-of-the-art LLMs, including the latest Llama 3 and Mixtral-8x22B models, on four free-form question-answering benchmarks demonstrate the superior performance and robustness of semantic density compared to prior approaches.

------------

`[2405.13967] DeTox: Toxic Subspace Projection for Model Editing <https://arxiv.org/abs/2405.13967>`__

::

    replaced with revised version Sun, 26 May 2024 03:46:22 GMT
    Submission history From: Rheeya Uppaal [view email]
    [v1] Wed, 22 May 2024 20:08:48 UTC (1,106 KB)
    [v2] Sun, 26 May 2024 03:46:22 UTC (1,114 KB)
    Rheeya Uppaal, Apratim Dey, Yiting He, Yiqiao Zhong, Junjie Hu

Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data. However, these methods are both computationally intensive and lacking in controllability and transparency, making them prone to jailbreaking and inhibiting their widespread use. Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data. In this paper, we introduce a tuning-free alignment alternative (DeTox) and demonstrate its effectiveness under the use case of toxicity reduction. Grounded on theory from factor analysis, DeTox is a sample-efficient model editing approach that identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The toxic sub-space is identified by extracting preference data embeddings from the language model, and removing non-toxic information from these embeddings. We show that DeTox is more sample-efficient than DPO, further showcasing greater robustness to noisy data. Finally, we establish both theoretical and empirical connections between DeTox and DPO, showing that DeTox can be interpreted as a denoised version of a single DPO step.

------------

`[2405.14555] Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models <https://arxiv.org/abs/2405.14555>`__

::

    replaced with revised version Sat, 25 May 2024 15:38:38 GMT
    Submission history From: Ali Emami Dr. [view email]
    [v1] Thu, 23 May 2024 13:35:34 UTC (8,747 KB)
    [v2] Sat, 25 May 2024 15:38:38 UTC (8,747 KB)
    Abhishek Kumar, Sarfaroz Yunusov, and Ali Emami

Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models' outputs toward particular social narratives. This study addresses two such biases within LLMs: representative bias, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and affinity bias, reflecting the models' evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to `bias fingerprints'. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions.

------------

`[2304.06701] Learning Personalized Decision Support Policies <https://arxiv.org/abs/2304.06701>`__

::

    replaced with revised version Mon, 27 May 2024 14:10:24 GMT
    Submission history From: Umang Bhatt [view email]
    [v1] Thu, 13 Apr 2023 17:53:34 UTC (14,748 KB)
    [v2] Mon, 27 May 2024 14:10:24 UTC (15,405 KB)
    Umang Bhatt, Valerie Chen, Katherine M. Collins, Parameswaran Kamalaruban, Emma Kallina, Adrian Weller, Ameet Talwalkar

Individual human decision-makers may benefit from different forms of support to improve decision outcomes, but when each form of support will yield better outcomes? In this work, we posit that personalizing access to decision support tools can be an effective mechanism for instantiating the appropriate use of AI assistance. Specifically, we propose the general problem of learning a decision support policy that, for a given input, chooses which form of support to provide to decision-makers for whom we initially have no prior information. We develop $\texttt{Modiste}$, an interactive tool to learn personalized decision support policies. $\texttt{Modiste}$ leverages stochastic contextual bandit techniques to personalize a decision support policy for each decision-maker and supports extensions to the multi-objective setting to account for auxiliary objectives like the cost of support. We find that personalized policies outperform offline policies, and, in the cost-aware setting, reduce the incurred cost with minimal degradation to performance. Our experiments include various realistic forms of support (e.g., expert consensus and predictions from a large language model) on vision and language tasks. Our human subject experiments validate our computational experiments, demonstrating that personalization can yield benefits in practice for real users, who interact with $\texttt{Modiste}$.

------------

`[2305.16338] Think Before You Act: Decision Transformers with Working Memory <https://arxiv.org/abs/2305.16338>`__

::

    replaced with revised version Mon, 27 May 2024 16:00:31 GMT
    Submission history From: Jikun Kang [view email]
    [v1] Wed, 24 May 2023 01:20:22 UTC (3,022 KB)
    [v2] Mon, 27 May 2024 16:00:31 UTC (3,725 KB)
    Jikun Kang, Romain Laroche, Xingdi Yuan, Adam Trischler, Xue Liu, Jie Fu

Decision Transformer-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and computation. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Inspired by this, we propose a working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in Atari games and Meta-World object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture.

------------

`[2310.05204] Towards Optimizing with Large Language Models <https://arxiv.org/abs/2310.05204>`__

::

    replaced with revised version Mon, 27 May 2024 09:13:26 GMT
    Submission history From: Yun-Da Tsai [view email]
    [v1] Sun, 8 Oct 2023 15:35:00 UTC (460 KB)
    [v2] Tue, 28 Nov 2023 06:38:03 UTC (488 KB)
    [v3] Mon, 27 May 2024 09:13:26 UTC (2,094 KB)
    Pei-Fu Guo, Ying-Hsuan Chen, Yun-Da Tsai, Shou-De Lin

In this work, we conduct an assessment of the optimization capabilities of LLMs across various tasks and data sizes. Each of these tasks corresponds to unique optimization domains, and LLMs are required to execute these tasks with interactive prompting. That is, in each optimization step, the LLM generates new solutions from the past generated solutions with their values, and then the new solutions are evaluated and considered in the next optimization step. Additionally, we introduce three distinct metrics for a comprehensive assessment of task performance from various perspectives. These metrics offer the advantage of being applicable for evaluating LLM performance across a broad spectrum of optimization tasks and are less sensitive to variations in test samples. By applying these metrics, we observe that LLMs exhibit strong optimization capabilities when dealing with small-sized samples. However, their performance is significantly influenced by factors like data size and values, underscoring the importance of further research in the domain of optimization tasks for LLMs.

------------

`[2310.06387] Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations <https://arxiv.org/abs/2310.06387>`__

::

    replaced with revised version Sat, 25 May 2024 07:01:15 GMT
    Submission history From: Zeming Wei [view email]
    [v1] Tue, 10 Oct 2023 07:50:29 UTC (769 KB)
    [v2] Sat, 4 May 2024 17:32:20 UTC (558 KB)
    [v3] Sat, 25 May 2024 07:01:15 UTC (887 KB)
    Zeming Wei, Yifei Wang, Ang Li, Yichuan Mo, Yisen Wang

Large Language Models (LLMs) have shown remarkable success in various tasks, yet their safety and the risk of generating harmful content remain pressing concerns. In this paper, we delve into the potential of In-Context Learning (ICL) to modulate the alignment of LLMs. Specifically, we propose the In-Context Attack (ICA) which employs harmful demonstrations to subvert LLMs, and the In-Context Defense (ICD) which bolsters model resilience through examples that demonstrate refusal to produce harmful responses. We offer theoretical insights to elucidate how a limited set of in-context demonstrations can pivotally influence the safety alignment of LLMs. Through extensive experiments, we demonstrate the efficacy of ICA and ICD in respectively elevating and mitigating the success rates of jailbreaking prompts. Our findings illuminate the profound influence of ICL on LLM behavior, opening new avenues for improving the safety of LLMs.

------------

`[2310.18491] Publicly-Detectable Watermarking for Language Models <https://arxiv.org/abs/2310.18491>`__

::

    replaced with revised version Mon, 27 May 2024 09:24:16 GMT
    Submission history From: Jaiden Fairoze [view email]
    [v1] Fri, 27 Oct 2023 21:08:51 UTC (1,038 KB)
    [v2] Mon, 27 May 2024 09:24:16 UTC (420 KB)
    Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody and Mingyuan Wang

We present a highly detectable, trustless watermarking scheme for LLMs: the detection algorithm contains no secret information, and it is executable by anyone. We embed a publicly-verifiable cryptographic signature into LLM output using rejection sampling. We prove that our scheme is cryptographically correct, sound, and distortion-free. We make novel uses of error-correction techniques to overcome periods of low entropy, a barrier for all prior watermarking schemes. We implement our scheme and make empirical measurements over open models in the 2.7B to 70B parameter range. Our experiments suggest that our formal claims are met in practice.

------------

`[2312.06353] Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes <https://arxiv.org/abs/2312.06353>`__

::

    replaced with revised version Mon, 27 May 2024 08:31:47 GMT
    Submission history From: Zhen Qin [view email]
    [v1] Mon, 11 Dec 2023 13:03:21 UTC (844 KB)
    [v2] Tue, 26 Dec 2023 03:37:35 UTC (844 KB)
    [v3] Wed, 31 Jan 2024 11:49:06 UTC (1,823 KB)
    [v4] Wed, 15 May 2024 14:59:38 UTC (2,075 KB)
    [v5] Mon, 27 May 2024 08:31:47 UTC (2,075 KB)
    Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, Shuiguang Deng

Pre-trained large language models (LLMs) need fine-tuning to improve their responsiveness to natural language instructions. Federated learning offers a way to fine-tune LLMs using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance height possible with full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial problem due to the immense communication cost. This work introduces FedKSeed that employs zeroth-order optimization with a finite set of random seeds. It significantly reduces transmission requirements between the server and clients to just a few random seeds and scalar gradients, amounting to only a few thousand bytes, making federated full-parameter tuning of billion-sized LLMs possible on devices. Building on it, we develop a strategy enabling probability-differentiated seed sampling, prioritizing perturbations with greater impact on model accuracy. Experiments across six scenarios with various LLMs, datasets and data partitions demonstrate that our approach outperforms existing federated LLM fine-tuning methods in both communication efficiency and new task generalization.

------------

`[2402.00854] SymbolicAI: A framework for logic-based approaches combining generative models and solvers <https://arxiv.org/abs/2402.00854>`__

::

    replaced with revised version Mon, 27 May 2024 13:05:13 GMT
    Submission history From: Marius-Constantin Dinu [view email]
    [v1] Thu, 1 Feb 2024 18:50:50 UTC (4,777 KB)
    [v2] Mon, 5 Feb 2024 21:22:18 UTC (4,816 KB)
    [v3] Mon, 27 May 2024 13:05:13 UTC (5,065 KB)
    Marius-Constantin Dinu and Claudiu Leoveanu-Condrei and Markus Holzleitner and Werner Zellinger and Sepp Hochreiter

We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for multi-modal data that connects multi-step generative processes and aligns their outputs with user objectives in complex workflows. As a result, we can transition between the capabilities of various foundation models with in-context learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. Through these operations based on in-context learning our framework enables the creation and evaluation of explainable computational graphs. Finally, we introduce a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the "Vector Embedding for Relational Trajectory Evaluation through Cross-similarity", or VERTEX score for short. The framework codebase and benchmark are linked below.

------------

`[2402.01621] Stochastic Two Points Method for Deep Model Zeroth-order Optimization <https://arxiv.org/abs/2402.01621>`__

::

    replaced with revised version Mon, 27 May 2024 14:56:01 GMT
    Submission history From: Yijiang Pang [view email]
    [v1] Fri, 2 Feb 2024 18:39:40 UTC (3,190 KB)
    [v2] Thu, 23 May 2024 00:01:11 UTC (5,240 KB)
    [v3] Mon, 27 May 2024 14:56:01 UTC (5,242 KB)
    Yijiang Pang, Jiayu Zhou

Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions, and the derived results help understand and inherently connect the two popular types of zeroth-order methods, basic random search and stochastic three-point method. The theoretical properties also shed light on a Variant of S2P (VS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that VS2P is highly effective in optimizing objectives for deep models. It outperforms or achieves competitive performance compared to standard methods across various model types and scales.

------------

`[2402.02314] Selecting Large Language Model to Fine-tune via Rectified Scaling Law <https://arxiv.org/abs/2402.02314>`__

::

    replaced with revised version Mon, 27 May 2024 15:11:22 GMT
    Submission history From: Haowei Lin [view email]
    [v1] Sun, 4 Feb 2024 01:55:00 UTC (9,998 KB)
    [v2] Mon, 27 May 2024 15:11:22 UTC (6,269 KB)
    Haowei Lin, Baizhou Huang, Haotian Ye, Qinyu Chen, Zihao Wang, Sujian Li, Jianzhu Ma, Xiaojun Wan, James Zou, Yitao Liang

The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection.

------------

`[2402.03282] A Theoretical Framework for Partially Observed Reward-States in RLHF <https://arxiv.org/abs/2402.03282>`__

::

    replaced with revised version Mon, 27 May 2024 17:20:41 GMT
    Submission history From: Chinmaya Kausik [view email]
    [v1] Mon, 5 Feb 2024 18:38:55 UTC (64 KB)
    [v2] Mon, 27 May 2024 17:20:41 UTC (65 KB)
    Chinmaya Kausik, Mirco Mutti, Aldo Pacchiano, Ambuj Tewari

The growing deployment of reinforcement learning from human feedback (RLHF) calls for a deeper theoretical investigation of its underlying models. The prevalent models of RLHF do not account for neuroscience-backed, partially-observed "internal states" that can affect human feedback, nor do they accommodate intermediate feedback during an interaction. Both of these can be instrumental in speeding up learning and improving alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We accommodate two kinds of feedback $-$ cardinal and dueling feedback. We first demonstrate that PORRL subsumes a wide class of RL problems, including traditional RL, RLHF, and reward machines. For cardinal feedback, we present two model-based methods (POR-UCRL, POR-UCBVI). We give both cardinal regret and sample complexity guarantees for the methods, showing that they improve over naive history-summarization. We then discuss the benefits of a model-free method like GOLF with naive history-summarization in settings with recursive internal states and dense intermediate feedback. For this purpose, we define a new history aware version of the Bellman-eluder dimension and give a new guarantee for GOLF in our setting, which can be exponentially sharper in illustrative examples. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regret. We then present the first explicit reduction that converts guarantees for cardinal regret to dueling regret. In both feedback settings, we show that our models and guarantees generalize and extend existing ones.

------------

`[2402.05445] Accurate LoRA-Finetuning Quantization of LLMs via Information Retention <https://arxiv.org/abs/2402.05445>`__

::

    replaced with revised version Mon, 27 May 2024 09:20:35 GMT
    Submission history From: Xudong Ma [view email]
    [v1] Thu, 8 Feb 2024 06:53:31 UTC (1,049 KB)
    [v2] Mon, 27 May 2024 09:20:35 UTC (1,051 KB)
    Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu and Michele Magno

The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the state-of-the-art methods. The significant performance gain requires only a tiny 0.31% additional time consumption, revealing the satisfactory efficiency of our IR-QLoRA. We highlight that IR-QLoRA enjoys excellent versatility, compatible with various frameworks (e.g., NormalFloat and Integer quantization) and brings general accuracy gains. The code is available at this https URL.

------------

`[2402.11867] LoRA Training in the NTK Regime has No Spurious Local Minima <https://arxiv.org/abs/2402.11867>`__

::

    replaced with revised version Mon, 27 May 2024 16:35:26 GMT
    Submission history From: Uijeong Jang [view email]
    [v1] Mon, 19 Feb 2024 06:22:09 UTC (2,547 KB)
    [v2] Mon, 27 May 2024 16:35:26 UTC (2,659 KB)
    Uijeong Jang, Jason D. Lee, Ernest K. Ryu

Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.

------------

`[2402.13516] ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models <https://arxiv.org/abs/2402.13516>`__

::

    replaced with revised version Mon, 27 May 2024 15:49:58 GMT
    Submission history From: Chenyang Song [view email]
    [v1] Wed, 21 Feb 2024 03:58:49 UTC (228 KB)
    [v2] Tue, 27 Feb 2024 07:27:07 UTC (229 KB)
    [v3] Mon, 27 May 2024 15:49:58 UTC (195 KB)
    Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun

Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, activation sparsity has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces a simple and effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity while maintaining comparable performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization with a factor smoothly increasing along the multi-stage sine curves. This can enhance activation sparsity and mitigate performance degradation by avoiding radical shifts in activation distributions. With ProSparse, we obtain high sparsity of 89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size MiniCPM-1B, respectively, achieving comparable performance to their original Swish-activated versions. These present the most sparsely activated models among open-source LLaMA versions and competitive end-size models, considerably surpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference acceleration experiments further demonstrate the significant practical acceleration potential of LLMs with higher activation sparsity, obtaining up to 4.52$\times$ inference speedup.

------------

`[2402.15390] Explorations of Self-Repair in Language Models <https://arxiv.org/abs/2402.15390>`__

::

    replaced with revised version Sun, 26 May 2024 20:47:41 GMT
    Submission history From: Cody Rushing [view email]
    [v1] Fri, 23 Feb 2024 15:42:12 UTC (5,231 KB)
    [v2] Sun, 26 May 2024 20:47:41 UTC (5,704 KB)
    Cody Rushing, Neel Nanda

Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor and sparse sets of neurons implementing Anti-Erasure. We additionally discuss the implications of these results for interpretability practitioners and close with a more speculative discussion on the mystery of why self-repair occurs in these models at all, highlighting evidence for the Iterative Inference hypothesis in language models, a framework that predicts self-repair.

------------

`[2402.16354] Language-guided Skill Learning with Temporal Variational Inference <https://arxiv.org/abs/2402.16354>`__

::

    replaced with revised version Mon, 27 May 2024 14:31:38 GMT
    Submission history From: Haotian Fu [view email]
    [v1] Mon, 26 Feb 2024 07:19:23 UTC (15,466 KB)
    [v2] Mon, 27 May 2024 14:31:38 UTC (15,928 KB)
    Haotian Fu, Pratyusha Sharma, Elias Stengel-Eskin, George Konidaris, Nicolas Le Roux, Marc-Alexandre C\^ot\'e, Xingdi Yuan

We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.

------------

`[2402.17501] Intensive Care as One Big Sequence Modeling Problem <https://arxiv.org/abs/2402.17501>`__

::

    replaced with revised version Fri, 24 May 2024 18:50:06 GMT
    Submission history From: Vadim Liventsev [view email]
    [v1] Tue, 27 Feb 2024 13:36:55 UTC (1,622 KB)
    [v2] Fri, 24 May 2024 18:50:06 UTC (1,648 KB)
    Vadim Liventsev, Tobias Fritz

Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a uniform event stream format, train a baseline model and explore its capabilities.

------------

`[2403.02107] Iterated $Q$-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning <https://arxiv.org/abs/2403.02107>`__

::

    replaced with revised version Sat, 25 May 2024 11:42:15 GMT
    Submission history From: Tho Vincent [view email]
    [v1] Mon, 4 Mar 2024 15:07:33 UTC (5,858 KB)
    [v2] Sat, 25 May 2024 11:42:15 UTC (6,682 KB)
    Th\'eo Vincent, Daniel Palenicek, Boris Belousov, Jan Peters, Carlo D'Eramo

The vast majority of Reinforcement Learning methods is largely impacted by the computation effort and data requirements needed to obtain effective estimates of action-value functions, which in turn determine the quality of the overall performance and the sample-efficiency of the learning procedure. Typically, action-value functions are estimated through an iterative scheme that alternates the application of an empirical approximation of the Bellman operator and a subsequent projection step onto a considered function space. It has been observed that this scheme can be potentially generalized to carry out multiple iterations of the Bellman operator at once, benefiting the underlying learning algorithm. However, till now, it has been challenging to effectively implement this idea, especially in high-dimensional problems. In this paper, we introduce iterated $Q$-Network (iQN), a novel principled approach that enables multiple consecutive Bellman updates by learning a tailored sequence of action-value functions where each serves as the target for the next. We show that iQN is theoretically grounded and that it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate the advantages of iQN in Atari $2600$ games and MuJoCo continuous control problems.

------------

`[2404.12404] Exploring Prompting Methods for Mitigating Class Imbalance through Synthetic Data Generation with Large Language Models <https://arxiv.org/abs/2404.12404>`__

::

    replaced with revised version Mon, 27 May 2024 03:29:18 GMT
    Submission history From: Taesung Kim [view email]
    [v1] Mon, 15 Apr 2024 17:49:16 UTC (4,781 KB)
    [v2] Mon, 27 May 2024 03:29:18 UTC (8,544 KB)
    Jinhee Kim, Taesung Kim, Jaegul Choo

Large language models (LLMs) have demonstrated impressive in-context learning capabilities across various domains. Inspired by this, our study explores the effectiveness of LLMs in generating realistic tabular data to mitigate class imbalance. We investigate and identify key prompt design elements such as data format, class presentation, and variable mapping to optimize the generation performance. Our findings indicate that using CSV format, balancing classes, and employing unique variable mapping produces realistic and reliable data, significantly enhancing machine learning performance for minor classes in imbalanced datasets. Additionally, these approaches improve the stability and efficiency of LLM data generation. We validate our approach using six real-world datasets and a toy dataset, achieving state-of-the-art performance in classification tasks. The code is available at: this https URL

------------

`[2405.00675] Self-Play Preference Optimization for Language Model Alignment <https://arxiv.org/abs/2405.00675>`__

::

    replaced with revised version Sun, 26 May 2024 21:50:05 GMT
    Submission history From: Yue Wu [view email]
    [v1] Wed, 1 May 2024 17:59:20 UTC (142 KB)
    [v2] Thu, 23 May 2024 17:58:39 UTC (137 KB)
    [v3] Sun, 26 May 2024 21:50:05 UTC (137 KB)
    Yue Wu and Zhiqing Sun and Huizhuo Yuan and Kaixuan Ji and Yiming Yang and Quanquan Gu

Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed \textit{Self-play Probabilistic Preference Optimization} (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys a theoretical convergence guarantee. Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53\% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.

------------

`[2405.12807] FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information <https://arxiv.org/abs/2405.12807>`__

::

    replaced with revised version Sun, 26 May 2024 10:59:04 GMT
    Submission history From: Dongseong Hwang [view email]
    [v1] Tue, 21 May 2024 13:58:17 UTC (166 KB)
    [v2] Thu, 23 May 2024 14:46:39 UTC (167 KB)
    [v3] Sun, 26 May 2024 10:59:04 UTC (166 KB)
    Dongseong Hwang

This paper establishes a mathematical foundation for the Adam optimizer, elucidating its connection to natural gradient descent through Riemannian and information geometry. We rigorously analyze the diagonal empirical Fisher information matrix (FIM) in Adam, clarifying all detailed approximations and advocating for the use of log probability functions as loss, which should be based on discrete distributions, due to the limitations of empirical FIM. Our analysis uncovers flaws in the original Adam algorithm, leading to proposed corrections such as enhanced momentum calculations, adjusted bias corrections, adaptive epsilon, and gradient clipping. We refine the weight decay term based on our theoretical framework. Our modified algorithm, Fisher Adam (FAdam), demonstrates superior performance across diverse domains including LLM, ASR, and VQ-VAE, achieving state-of-the-art results in ASR.

------------

`[2405.14622] Calibrated Self-Rewarding Vision Language Models <https://arxiv.org/abs/2405.14622>`__

::

    replaced with revised version Sat, 25 May 2024 19:36:07 GMT
    Submission history From: Huaxiu Yao [view email]
    [v1] Thu, 23 May 2024 14:30:33 UTC (1,977 KB)
    [v2] Sat, 25 May 2024 19:36:07 UTC (1,977 KB)
    Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, Huaxiu Yao

Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches may not effectively reflect the target LVLM's preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR enhances performance and reduces hallucinations across ten benchmarks and tasks, achieving substantial improvements over existing methods by 7.62%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning. Our data and code are available at this https URL.

------------

`[2307.05360] Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures <https://arxiv.org/abs/2307.05360>`__

::

    replaced with revised version Fri, 24 May 2024 20:28:09 GMT
    Submission history From: Sayed Erfan Arefin [view email]
    [v1] Mon, 10 Jul 2023 08:20:34 UTC (647 KB)
    [v2] Sun, 16 Jul 2023 04:44:19 UTC (671 KB)
    [v3] Fri, 24 May 2024 20:28:09 UTC (671 KB)
    Sayed Erfan Arefin, Tasnia Ashrafi Heya, Hasan Al-Qudah, Ynes Ineza, Abdul Serwadda

The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these kinds of situations. To infer whether ChatGPT might have directly memorized some of the data that was used to train it, we methodically design an experiment to investigate this phenomena. Making comparisons with human performance whenever feasible, we investigate all the above questions from the context of both its underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics within the main topics, and on problems having varying degrees of difficulty.

------------

`[2402.01591] BAT: Learning to Reason about Spatial Sounds with Large Language Models <https://arxiv.org/abs/2402.01591>`__

::

    replaced with revised version Sat, 25 May 2024 17:05:11 GMT
    Submission history From: Zhisheng Zheng [view email]
    [v1] Fri, 2 Feb 2024 17:34:53 UTC (4,020 KB)
    [v2] Sat, 25 May 2024 17:05:11 UTC (4,020 KB)
    Zhisheng Zheng, Puyuan Peng, Ziyang Ma, Xie Chen, Eunsol Choi, David Harwath

Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B model, BAT transcends standard Sound Event Localization and Detection (SELD) tasks, enabling the model to reason about the relationships between the sounds in its environment. Our experiments demonstrate BAT's superior performance on both spatial sound perception and reasoning, showcasing the immense potential of LLMs in navigating and interpreting complex spatial audio environments.

------------

`[2402.14807] A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health <https://arxiv.org/abs/2402.14807>`__

::

    replaced with revised version Sun, 26 May 2024 22:46:45 GMT
    Submission history From: Nikhil Behari [view email]
    [v1] Thu, 22 Feb 2024 18:58:27 UTC (857 KB)
    [v2] Fri, 23 Feb 2024 22:17:10 UTC (809 KB)
    [v3] Sun, 26 May 2024 22:46:45 UTC (906 KB)
    Nikhil Behari, Edwin Zhang, Yunfan Zhao, Aparna Taneja, Dheeraj Nagaraj, Milind Tambe

Restless multi-armed bandits (RMAB) have demonstrated success in optimizing resource allocation for large beneficiary populations in public health settings. Unfortunately, RMAB models lack flexibility to adapt to evolving public health policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept automated planners across domains of robotic control and navigation. In this paper, we propose a Decision Language Model (DLM) for RMABs, enabling dynamic fine-tuning of RMAB policies in public health settings using human-language commands. We propose using LLMs as automated planners to (1) interpret human policy preference prompts, (2) propose reward functions as code for a multi-agent RMAB environment, and (3) iterate on the generated reward functions using feedback from grounded RMAB simulations. We illustrate the application of DLM in collaboration with ARMMAN, an India-based non-profit promoting preventative care for pregnant mothers, that currently relies on RMAB policies to optimally allocate health worker calls to low-resource populations. We conduct a technology demonstration in simulation using the Gemini Pro model, showing DLM can dynamically shape policy outcomes using only human prompts as input.

------------

`[2404.00675] LLM meets Vision-Language Models for Zero-Shot One-Class Classification <https://arxiv.org/abs/2404.00675>`__

::

    replaced with revised version Mon, 27 May 2024 08:53:15 GMT
    Submission history From: Yassir Bendou [view email]
    [v1] Sun, 31 Mar 2024 12:48:07 UTC (10,979 KB)
    [v2] Tue, 2 Apr 2024 10:59:05 UTC (10,963 KB)
    [v3] Mon, 27 May 2024 08:53:15 UTC (14,346 KB)
    Yassir Bendou, Giulia Lioi, Bastien Pasdeloup, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux and Vincent Gripon

We consider the problem of zero-shot one-class visual classification, extending traditional one-class classification to scenarios where only the label of the target class is available. This method aims to discriminate between positive and negative query samples without requiring examples from the target class. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. To our knowledge, we are the first to demonstrate the ability to discriminate a single category from other semantically related ones using only its label.

------------

`[2405.08755] Distributed Threat Intelligence at the Edge Devices: A Large Language Model-Driven Approach <https://arxiv.org/abs/2405.08755>`__

::

    replaced with revised version Sun, 26 May 2024 06:06:08 GMT
    Submission history From: Syed Mhamudul Hasan [view email]
    [v1] Tue, 14 May 2024 16:40:37 UTC (6,726 KB)
    [v2] Sun, 26 May 2024 06:06:08 UTC (6,726 KB)
    Syed Mhamudul Hasan, Alaa M. Alotaibi, Sajedul Talukder, Abdur R. Shahid

With the proliferation of edge devices, there is a significant increase in attack surface on these devices. The decentralized deployment of threat intelligence on edge devices, coupled with adaptive machine learning techniques such as the in-context learning feature of Large Language Models (LLMs), represents a promising paradigm for enhancing cybersecurity on resource-constrained edge devices. This approach involves the deployment of lightweight machine learning models directly onto edge devices to analyze local data streams, such as network traffic and system logs, in real-time. Additionally, distributing computational tasks to an edge server reduces latency and improves responsiveness while also enhancing privacy by processing sensitive data locally. LLM servers can enable these edge servers to autonomously adapt to evolving threats and attack patterns, continuously updating their models to improve detection accuracy and reduce false positives. Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure and trustworthy knowledge sharing among edge devices, enhancing the collective intelligence of the network and enabling dynamic threat mitigation measures such as device quarantine in response to detected anomalies. The scalability and flexibility of this approach make it well-suited for diverse and evolving network environments, as edge devices only send suspicious information such as network traffic and system log changes, offering a resilient and efficient solution to combat emerging cyber threats at the network edge. Thus, our proposed framework can improve edge computing security by providing better security in cyber threat detection and mitigation by isolating the edge devices from the network.

------------

`[2405.10632] Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks <https://arxiv.org/abs/2405.10632>`__

::

    replaced with revised version Mon, 27 May 2024 12:09:57 GMT
    Submission history From: Lujain Ibrahim [view email]
    [v1] Fri, 17 May 2024 08:49:34 UTC (306 KB)
    [v2] Mon, 20 May 2024 19:06:23 UTC (560 KB)
    [v3] Mon, 27 May 2024 12:09:57 UTC (560 KB)
    Lujain Ibrahim, Saffron Huang, Lama Ahmad, Markus Anderljung

Model evaluations are central to understanding the safety, risks, and societal impacts of AI systems. While most real-world AI applications involve human-AI interaction, most current evaluations (e.g., common benchmarks) of AI models do not. Instead, they incorporate human factors in limited ways, assessing the safety of models in isolation, thereby falling short of capturing the complexity of human-model interactions. In this paper, we discuss and operationalize a definition of an emerging category of evaluations -- "human interaction evaluations" (HIEs) -- which focus on the assessment of human-model interactions or the process and the outcomes of humans using models. First, we argue that HIEs can be used to increase the validity of safety evaluations, assess direct human impact and interaction-specific harms, and guide future assessments of models' societal impact. Second, we propose a safety-focused HIE design framework -- containing a human-LLM interaction taxonomy -- with three stages: (1) identifying the risk or harm area, (2) characterizing the use context, and (3) choosing the evaluation parameters. Third, we apply our framework to two potential evaluations for overreliance and persuasion risks. Finally, we conclude with tangible recommendations for addressing concerns over costs, replicability, and unrepresentativeness of HIEs.

------------

`[2402.16187] No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices <https://arxiv.org/abs/2402.16187>`__

::

    replaced with revised version Sat, 25 May 2024 20:51:49 GMT
    Submission history From: Qi Pang [view email]
    [v1] Sun, 25 Feb 2024 20:24:07 UTC (1,663 KB)
    [v2] Sat, 25 May 2024 20:51:49 UTC (2,746 KB)
    Qi Pang, Shengyuan Hu, Wenting Zheng, Virginia Smith

Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating the misuse of such AI-generated content. However, we show that common design choices in LLM watermarking schemes make the resulting systems surprisingly susceptible to attack -- leading to fundamental trade-offs in robustness, utility, and usability. To navigate these trade-offs, we rigorously study a set of simple yet effective attacks on common watermarking systems, and propose guidelines and defenses for LLM watermarking in practice.

------------

`[2405.12856] LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language <https://arxiv.org/abs/2405.12856>`__

::

    replaced with revised version Sat, 25 May 2024 22:07:48 GMT
    Submission history From: John Bronskill [view email]
    [v1] Tue, 21 May 2024 15:13:12 UTC (3,957 KB)
    [v2] Sat, 25 May 2024 22:07:48 UTC (3,957 KB)
    James Requeima, John Bronskill, Dami Choi, Richard E. Turner, David Duvenaud

Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode.

------------

`[2310.16152] FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering <https://arxiv.org/abs/2310.16152>`__

::

    replaced with revised version Sun, 26 May 2024 03:44:52 GMT
    Submission history From: Md Rafi Ur Rashid [view email]
    [v1] Tue, 24 Oct 2023 19:50:01 UTC (18,423 KB)
    [v2] Sun, 26 May 2024 03:44:52 UTC (21,172 KB)
    Md Rafi Ur Rashid, Vishnu Asutosh Dasu, Kang Gu, Najrin Sultana, Shagufta Mehnaz

Federated learning (FL) has become a key component in various language modeling applications such as machine translation, next-word prediction, and medical record analysis. These applications are trained on datasets from many FL participants that often include privacy-sensitive data, such as healthcare records, phone/credit card numbers, login credentials, etc. Although FL enables computation without necessitating clients to share their raw data, determining the extent of privacy leakage in federated language models is challenging and not straightforward. Moreover, existing attacks aim to extract data regardless of how sensitive or naive it is. To fill this research gap, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated large language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other users in FL even without any cooperation from the server. Our best-performing method improves the membership inference recall by 29% and achieves up to 71% private data reconstruction, evidently outperforming existing attacks with stronger assumptions of adversary capabilities.

------------

`[2402.08699] Unsupervised Evaluation of Code LLMs with Round-Trip Correctness <https://arxiv.org/abs/2402.08699>`__

::

    replaced with revised version Mon, 27 May 2024 10:55:06 GMT
    Submission history From: Miltiadis Allamanis [view email]
    [v1] Tue, 13 Feb 2024 11:08:08 UTC (109 KB)
    [v2] Mon, 27 May 2024 10:55:06 UTC (135 KB)
    Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin

To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing us to expand to a much broader set of domains and tasks which was not previously possible without costly human annotations.

------------

-----------
Index (230)
-----------

`[2405.15808] Ensuring Ground Truth Accuracy in Healthcare with the EVINCE framework <https://arxiv.org/abs/2405.15808>`__

`[2405.15821] Reinforcing Language Agents via Policy Optimization with Action Decomposition <https://arxiv.org/abs/2405.15821>`__

`[2405.16122] Prompt Optimization with EASE? Efficient Ordering-aware Automated Selection of Exemplars <https://arxiv.org/abs/2405.16122>`__

`[2405.16136] C3LLM: Conditional Multimodal Content Generation Using Large Language Models <https://arxiv.org/abs/2405.16136>`__

`[2405.16205] GeneAgent: Self-verification Language Agent for Gene Set Knowledge Discovery using Domain Databases <https://arxiv.org/abs/2405.16205>`__

`[2405.16247] AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning <https://arxiv.org/abs/2405.16247>`__

`[2405.16334] Devil's Advocate: Anticipatory Reflection for LLM Agents <https://arxiv.org/abs/2405.16334>`__

`[2405.16413] Augmented Risk Prediction for the Onset of Alzheimer's Disease from Electronic Health Records with Large Language Models <https://arxiv.org/abs/2405.16413>`__

`[2405.16434] The Importance of Directional Feedback for LLM-based Optimizers <https://arxiv.org/abs/2405.16434>`__

`[2405.16510] Meta-Task Planning for Language Agents <https://arxiv.org/abs/2405.16510>`__

`[2405.16567] Automatic Jailbreaking of the Text-to-Image Generative AI Systems <https://arxiv.org/abs/2405.16567>`__

`[2405.16588] Attaining Human`s Desirable Outcomes in Human-AI Interaction via Structural Causal Games <https://arxiv.org/abs/2405.16588>`__

`[2405.16640] A Survey of Multimodal Large Language Model from A Data-centric Perspective <https://arxiv.org/abs/2405.16640>`__

`[2405.16751] LLM-Based Cooperative Agents using Information Relevance and Plan Validation <https://arxiv.org/abs/2405.16751>`__

`[2405.16887] A Large Language Model-based multi-agent manufacturing system for intelligent shopfloor <https://arxiv.org/abs/2405.16887>`__

`[2405.17009] Position: Foundation Agents as the Paradigm Shift for Decision Making <https://arxiv.org/abs/2405.17009>`__

`[2405.17044] Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models <https://arxiv.org/abs/2405.17044>`__

`[2405.17345] Exploring and steering the moral compass of Large Language Models <https://arxiv.org/abs/2405.17345>`__

`[2405.15818] DuanzAI: Slang-Enhanced LLM with Prompt for Humor Understanding <https://arxiv.org/abs/2405.15818>`__

`[2405.15924] SLIDE: A Framework Integrating Small and Large Language Models for Open-Domain Dialogues Evaluation <https://arxiv.org/abs/2405.15924>`__

`[2405.15936] Zero-Shot Spam Email Classification Using Pre-trained Large Language Models <https://arxiv.org/abs/2405.15936>`__

`[2405.15984] Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models <https://arxiv.org/abs/2405.15984>`__

`[2405.16042] Incremental Comprehension of Garden-Path Sentences by Large Language Models: Semantic Interpretation, Syntactic Re-Analysis, and Attention <https://arxiv.org/abs/2405.16042>`__

`[2405.16057] SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models <https://arxiv.org/abs/2405.16057>`__

`[2405.16064] Keypoint-based Progressive Chain-of-Thought Distillation for LLMs <https://arxiv.org/abs/2405.16064>`__

`[2405.16089] COLT: Towards Completeness-Oriented Tool Retrieval for Large Language Models <https://arxiv.org/abs/2405.16089>`__

`[2405.16129] iREL at SemEval-2024 Task 9: Improving Conventional Prompting Methods for Brain Teasers <https://arxiv.org/abs/2405.16129>`__

`[2405.16150] 5W1H Extraction With Large Language Models <https://arxiv.org/abs/2405.16150>`__

`[2405.16178] Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection <https://arxiv.org/abs/2405.16178>`__

`[2405.16229] No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks <https://arxiv.org/abs/2405.16229>`__

`[2405.16277] Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge <https://arxiv.org/abs/2405.16277>`__

`[2405.16281] ConStat: Performance-Based Contamination Detection in Large Language Models <https://arxiv.org/abs/2405.16281>`__

`[2405.16282] Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models <https://arxiv.org/abs/2405.16282>`__

`[2405.16284] Generating clickbait spoilers with an ensemble of large language models <https://arxiv.org/abs/2405.16284>`__

`[2405.16295] Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data <https://arxiv.org/abs/2405.16295>`__

`[2405.16376] STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making <https://arxiv.org/abs/2405.16376>`__

`[2405.16388] Multi-Reference Preference Optimization for Large Language Models <https://arxiv.org/abs/2405.16388>`__

`[2405.16402] Assessing Empathy in Large Language Models with Real-World Physician-Patient Interactions <https://arxiv.org/abs/2405.16402>`__

`[2405.16412] KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge <https://arxiv.org/abs/2405.16412>`__

`[2405.16420] M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions <https://arxiv.org/abs/2405.16420>`__

`[2405.16433] CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling <https://arxiv.org/abs/2405.16433>`__

`[2405.16482] DarijaBanking: A New Resource for Overcoming Language Barriers in Banking Intent Detection for Moroccan Arabic Speakers <https://arxiv.org/abs/2405.16482>`__

`[2405.16533] Chain of Tools: Large Language Model is an Automatic Multi-tool Learner <https://arxiv.org/abs/2405.16533>`__

`[2405.16552] SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation <https://arxiv.org/abs/2405.16552>`__

`[2405.16571] A Preliminary Empirical Study on Prompt-based Unsupervised Keyphrase Extraction <https://arxiv.org/abs/2405.16571>`__

`[2405.16579] Automatically Generating Numerous Context-Driven SFT Data for LLMs across Diverse Granularity <https://arxiv.org/abs/2405.16579>`__

`[2405.16631] Let Silence Speak: Enhancing Fake News Detection with Generated Comments from Large Language Models <https://arxiv.org/abs/2405.16631>`__

`[2405.16661] RLSF: Reinforcement Learning via Symbolic Feedback <https://arxiv.org/abs/2405.16661>`__

`[2405.16681] Triple Preference Optimization: Achieving Better Alignment with Less Data in a Single Step Optimization <https://arxiv.org/abs/2405.16681>`__

`[2405.16702] Accurate and Nuanced Open-QA Evaluation Through Textual Entailment <https://arxiv.org/abs/2405.16702>`__

`[2405.16714] Crafting Interpretable Embeddings by Asking LLMs Questions <https://arxiv.org/abs/2405.16714>`__

`[2405.16720] Large Scale Knowledge Washing <https://arxiv.org/abs/2405.16720>`__

`[2405.16802] AutoCV: Empowering Reasoning with Automated Process Labeling via Confidence Variation <https://arxiv.org/abs/2405.16802>`__

`[2405.16806] Entity Alignment with Noisy Annotations from Large Language Models <https://arxiv.org/abs/2405.16806>`__

`[2405.16821] Perturbation-Restrained Sequential Model Editing <https://arxiv.org/abs/2405.16821>`__

`[2405.16856] Can We Trust LLMs? Mitigate Overconfidence Bias in LLMs through Knowledge Transfer <https://arxiv.org/abs/2405.16856>`__

`[2405.16884] Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching <https://arxiv.org/abs/2405.16884>`__

`[2405.16908] Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words? <https://arxiv.org/abs/2405.16908>`__

`[2405.16933] Empowering Large Language Models to Set up a Knowledge Retrieval Indexer via Self-Learning <https://arxiv.org/abs/2405.16933>`__

`[2405.16964] Exploring the LLM Journey from Cognition to Expression with Linear Representations <https://arxiv.org/abs/2405.16964>`__

`[2405.17039] BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation <https://arxiv.org/abs/2405.17039>`__

`[2405.17052] SelfCP: Compressing Long Prompt to 1/12 Using the Frozen Large Language Model Itself <https://arxiv.org/abs/2405.17052>`__

`[2405.17057] ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation <https://arxiv.org/abs/2405.17057>`__

`[2405.17062] Unifying Demonstration Selection and Compression for In-Context Learning <https://arxiv.org/abs/2405.17062>`__

`[2405.17067] Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization <https://arxiv.org/abs/2405.17067>`__

`[2405.17103] Empowering Character-level Text Infilling by Eliminating Sub-Tokens <https://arxiv.org/abs/2405.17103>`__

`[2405.17129] TEII: Think, Explain, Interact and Iterate with Large Language Models to Solve Cross-lingual Emotion Detection <https://arxiv.org/abs/2405.17129>`__

`[2405.17202] Efficient multi-prompt evaluation of LLMs <https://arxiv.org/abs/2405.17202>`__

`[2405.17220] RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness <https://arxiv.org/abs/2405.17220>`__

`[2405.17249] Assessing LLMs Suitability for Knowledge Graph Completion <https://arxiv.org/abs/2405.17249>`__

`[2405.17264] On the Noise Robustness of In-Context Learning for Text Generation <https://arxiv.org/abs/2405.17264>`__

`[2405.17337] Cost-efficient Knowledge-based Question Answering with Large Language Models <https://arxiv.org/abs/2405.17337>`__

`[2405.17381] Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention <https://arxiv.org/abs/2405.17381>`__

`[2405.17386] MindMerger: Efficient Boosting LLM Reasoning in non-English Languages <https://arxiv.org/abs/2405.17386>`__

`[2405.17402] THREAD: Thinking Deeper with Recursive Spawning <https://arxiv.org/abs/2405.17402>`__

`[2405.17428] NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models <https://arxiv.org/abs/2405.17428>`__

`[2405.15861] Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization <https://arxiv.org/abs/2405.15861>`__

`[2405.15877] Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications <https://arxiv.org/abs/2405.15877>`__

`[2405.15943] Transformers represent belief state geometry in their residual stream <https://arxiv.org/abs/2405.15943>`__

`[2405.16203] Evolutionary Large Language Model for Automated Feature Transformation <https://arxiv.org/abs/2405.16203>`__

`[2405.16265] MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time <https://arxiv.org/abs/2405.16265>`__

`[2405.16325] SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs <https://arxiv.org/abs/2405.16325>`__

`[2405.16405] Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level <https://arxiv.org/abs/2405.16405>`__

`[2405.16406] SpinQuant -- LLM quantization with learned rotations <https://arxiv.org/abs/2405.16406>`__

`[2405.16436] Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer <https://arxiv.org/abs/2405.16436>`__

`[2405.16444] CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion <https://arxiv.org/abs/2405.16444>`__

`[2405.16450] Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search <https://arxiv.org/abs/2405.16450>`__

`[2405.16528] LoQT: Low Rank Adapters for Quantized Training <https://arxiv.org/abs/2405.16528>`__

`[2405.16581] On Bits and Bandits: Quantifying the Regret-Information Trade-off <https://arxiv.org/abs/2405.16581>`__

`[2405.16587] Cost-Effective Online Multi-LLM Selection with Versatile Reward Models <https://arxiv.org/abs/2405.16587>`__

`[2405.16747] Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective <https://arxiv.org/abs/2405.16747>`__

`[2405.16755] CHESS: Contextual Harnessing for Efficient SQL Synthesis <https://arxiv.org/abs/2405.16755>`__

`[2405.16833] Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models <https://arxiv.org/abs/2405.16833>`__

`[2405.16918] The Uncanny Valley: Exploring Adversarial Robustness from a Flatness Perspective <https://arxiv.org/abs/2405.16918>`__

`[2405.17088] Phase Transitions in the Output Distribution of Large Language Models <https://arxiv.org/abs/2405.17088>`__

`[2405.17216] Autoformalizing Euclidean Geometry <https://arxiv.org/abs/2405.17216>`__

`[2405.17233] CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs <https://arxiv.org/abs/2405.17233>`__

`[2405.17247] An Introduction to Vision-Language Modeling <https://arxiv.org/abs/2405.17247>`__

`[2405.17258] $\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning <https://arxiv.org/abs/2405.17258>`__

`[2405.17346] Prompt Optimization with Human Feedback <https://arxiv.org/abs/2405.17346>`__

`[2405.17374] Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models <https://arxiv.org/abs/2405.17374>`__

`[2405.17378] RTL-Repo: A Benchmark for Evaluating LLMs on Large-Scale RTL Design Projects <https://arxiv.org/abs/2405.17378>`__

`[2405.17382] ReMoDetect: Reward Models Recognize Aligned LLM's Generations <https://arxiv.org/abs/2405.17382>`__

`[2405.15784] CLARINET: Augmenting Language Models to Ask Clarification Questions for Retrieval <https://arxiv.org/abs/2405.15784>`__

`[2405.15792] IQLS: Framework for leveraging Metadata to enable Large Language Model based queries to complex, versatile Data <https://arxiv.org/abs/2405.15792>`__

`[2405.15880] HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis <https://arxiv.org/abs/2405.15880>`__

`[2405.15902] Hacc-Man: An Arcade Game for Jailbreaking LLMs <https://arxiv.org/abs/2405.15902>`__

`[2405.15928] PatchProt: Hydrophobic patch prediction using protein foundation models <https://arxiv.org/abs/2405.15928>`__

`[2405.15960] Human-Centered Automation <https://arxiv.org/abs/2405.15960>`__

`[2405.16133] Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting <https://arxiv.org/abs/2405.16133>`__

`[2405.16241] FastQuery: Communication-efficient Embedding Table Query for Private LLM Inference <https://arxiv.org/abs/2405.16241>`__

`[2405.16310] An Empirical Exploration of Trust Dynamics in LLM Supply Chains <https://arxiv.org/abs/2405.16310>`__

`[2405.16363] LLMs for User Interest Exploration: A Hybrid Approach <https://arxiv.org/abs/2405.16363>`__

`[2405.16473] M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought <https://arxiv.org/abs/2405.16473>`__

`[2405.16766] Reframing the Relationship in Out-of-Distribution Detection <https://arxiv.org/abs/2405.16766>`__

`[2405.16792] Laurel: Generating Dafny Assertions Using Large Language Models <https://arxiv.org/abs/2405.16792>`__

`[2405.17053] WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence <https://arxiv.org/abs/2405.17053>`__

`[2405.17104] LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding <https://arxiv.org/abs/2405.17104>`__

`[2405.17430] Matryoshka Multimodal Models <https://arxiv.org/abs/2405.17430>`__

`[2405.15787] Extracting chemical food safety hazards from the scientific literature automatically using large language models <https://arxiv.org/abs/2405.15787>`__

`[2405.16546] Cocktail: A Comprehensive Information Retrieval Benchmark with LLM-Generated Documents Integration <https://arxiv.org/abs/2405.16546>`__

`[2405.16700] Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs <https://arxiv.org/abs/2405.16700>`__

`[2405.15842] Model Cascading for Code: Reducing Inference Costs with Model Cascading for LLM Based Code Generation <https://arxiv.org/abs/2405.15842>`__

`[2405.16236] A statistical framework for weak-to-strong generalization <https://arxiv.org/abs/2405.16236>`__

`[2405.16455] On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization <https://arxiv.org/abs/2405.16455>`__

`[2310.09130] Split-and-Denoise: Protect large language model inference with local differential privacy <https://arxiv.org/abs/2310.09130>`__

`[2310.18659] DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy <https://arxiv.org/abs/2310.18659>`__

`[2402.12275] WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment <https://arxiv.org/abs/2402.12275>`__

`[2403.03997] Guiding Enumerative Program Synthesis with Large Language Models <https://arxiv.org/abs/2403.03997>`__

`[2403.10171] AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation <https://arxiv.org/abs/2403.10171>`__

`[2403.12451] INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations <https://arxiv.org/abs/2403.12451>`__

`[2404.14786] RealTCD: Temporal Causal Discovery from Interventional Data with Large Language Model <https://arxiv.org/abs/2404.14786>`__

`[2405.14314] Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration <https://arxiv.org/abs/2405.14314>`__

`[2405.14391] Explainable Few-shot Knowledge Tracing <https://arxiv.org/abs/2405.14391>`__

`[2405.15766] Enhancing Adverse Drug Event Detection with Multimodal Dataset: Corpus Creation and Model Development <https://arxiv.org/abs/2405.15766>`__

`[2210.03696] LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models <https://arxiv.org/abs/2210.03696>`__

`[2302.10205] ChatIE: Zero-Shot Information Extraction via Chatting with ChatGPT <https://arxiv.org/abs/2302.10205>`__

`[2303.13013] GesGPT: Speech Gesture Synthesis With Text Parsing from GPT <https://arxiv.org/abs/2303.13013>`__

`[2304.11090] Towards Responsible and Safe AI in the Era of Foudnation Models: A Reference Architecture for Designing Foundation Model based Systems <https://arxiv.org/abs/2304.11090>`__

`[2305.15067] Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References <https://arxiv.org/abs/2305.15067>`__

`[2308.11534] PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator <https://arxiv.org/abs/2308.11534>`__

`[2309.05447] DoG-Instruct: Towards Premium Instruction-Tuning Data via Text-Grounded Instruction Wrapping <https://arxiv.org/abs/2309.05447>`__

`[2309.12288] The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A" <https://arxiv.org/abs/2309.12288>`__

`[2310.02124] Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View <https://arxiv.org/abs/2310.02124>`__

`[2310.04994] Distantly-Supervised Joint Extraction with Noise-Robust Learning <https://arxiv.org/abs/2310.04994>`__

`[2310.07075] Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding <https://arxiv.org/abs/2310.07075>`__

`[2310.12086] FactCHD: Benchmarking Fact-Conflicting Hallucination Detection <https://arxiv.org/abs/2310.12086>`__

`[2311.02851] Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding <https://arxiv.org/abs/2311.02851>`__

`[2312.10897] Generalized Category Discovery with Large Language Models in the Loop <https://arxiv.org/abs/2312.10897>`__

`[2312.14591] Reasons to Reject? Aligning Language Models with Judgments <https://arxiv.org/abs/2312.14591>`__

`[2401.05268] AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning <https://arxiv.org/abs/2401.05268>`__

`[2401.16332] Tradeoffs Between Alignment and Helpfulness in Language Models with Representation Engineering <https://arxiv.org/abs/2401.16332>`__

`[2401.17623] Neighboring Perturbations of Knowledge Editing on Large Language Models <https://arxiv.org/abs/2401.17623>`__

`[2402.00956] Exploring Spatial Schema Intuitions in Large Language and Vision Models <https://arxiv.org/abs/2402.00956>`__

`[2402.03190] Unified Hallucination Detection for Multimodal Large Language Models <https://arxiv.org/abs/2402.03190>`__

`[2402.03848] ANLS* -- A Universal Document Processing Metric for Generative Large Language Models <https://arxiv.org/abs/2402.03848>`__

`[2402.04315] Training Language Models to Generate Text with Citations via Fine-grained Rewards <https://arxiv.org/abs/2402.04315>`__

`[2402.04624] MEMORYLLM: Towards Self-Updatable Large Language Models <https://arxiv.org/abs/2402.04624>`__

`[2402.11903] DiLA: Enhancing LLM Tool Learning with Differential Logic Layer <https://arxiv.org/abs/2402.11903>`__

`[2402.12146] Enabling Weak LLMs to Judge Response Reliability via Meta Ranking <https://arxiv.org/abs/2402.12146>`__

`[2402.12193] A Chinese Dataset for Evaluating the Safeguards in Large Language Models <https://arxiv.org/abs/2402.12193>`__

`[2402.12847] Instruction-tuned Language Models are Better Knowledge Learners <https://arxiv.org/abs/2402.12847>`__

`[2402.14457] Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts <https://arxiv.org/abs/2402.14457>`__

`[2402.14700] Unveiling Linguistic Regions in Large Language Models <https://arxiv.org/abs/2402.14700>`__

`[2402.14710] IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus <https://arxiv.org/abs/2402.14710>`__

`[2402.14992] tinyBenchmarks: evaluating LLMs with fewer examples <https://arxiv.org/abs/2402.14992>`__

`[2402.15238] GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection? <https://arxiv.org/abs/2402.15238>`__

`[2402.15481] Prejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models <https://arxiv.org/abs/2402.15481>`__

`[2402.16107] FuseChat: Knowledge Fusion of Chat Models <https://arxiv.org/abs/2402.16107>`__

`[2403.00812] LoRA Meets Dropout under a Unified Framework <https://arxiv.org/abs/2403.00812>`__

`[2403.01031] Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks <https://arxiv.org/abs/2403.01031>`__

`[2403.01241] IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact <https://arxiv.org/abs/2403.01241>`__

`[2403.01244] Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal <https://arxiv.org/abs/2403.01244>`__

`[2403.01251] Accelerating Greedy Coordinate Gradient via Probe Sampling <https://arxiv.org/abs/2403.01251>`__

`[2403.02674] Revisiting Meta-evaluation for Grammatical Error Correction <https://arxiv.org/abs/2403.02674>`__

`[2403.02715] Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models <https://arxiv.org/abs/2403.02715>`__

`[2403.03031] Learning to Use Tools via Cooperative and Interactive Agents <https://arxiv.org/abs/2403.03031>`__

`[2403.07088] SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation <https://arxiv.org/abs/2403.07088>`__

`[2403.17540] Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction <https://arxiv.org/abs/2403.17540>`__

`[2404.06479] Text-Based Reasoning About Vector Graphics <https://arxiv.org/abs/2404.06479>`__

`[2404.07922] LaVy: Vietnamese Multimodal Large Language Model <https://arxiv.org/abs/2404.07922>`__

`[2404.12464] NormAd: A Benchmark for Measuring the Cultural Adaptability of Large Language Models <https://arxiv.org/abs/2404.12464>`__

`[2405.09719] Spectral Editing of Activations for Large Language Model Alignment <https://arxiv.org/abs/2405.09719>`__

`[2405.10650] SPOR: A Comprehensive and Practical Evaluation Method for Compositional Generalization in Data-to-Text Generation <https://arxiv.org/abs/2405.10650>`__

`[2405.11577] A Multi-Perspective Analysis of Memorization in Large Language Models <https://arxiv.org/abs/2405.11577>`__

`[2405.13845] Semantic Density: Uncertainty Quantification in Semantic Space for Large Language Models <https://arxiv.org/abs/2405.13845>`__

`[2405.13967] DeTox: Toxic Subspace Projection for Model Editing <https://arxiv.org/abs/2405.13967>`__

`[2405.14555] Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models <https://arxiv.org/abs/2405.14555>`__

`[2405.15452] Leveraging Logical Rules in Knowledge Editing: A Cherry on the Top <https://arxiv.org/abs/2405.15452>`__

`[2304.06701] Learning Personalized Decision Support Policies <https://arxiv.org/abs/2304.06701>`__

`[2305.16338] Think Before You Act: Decision Transformers with Working Memory <https://arxiv.org/abs/2305.16338>`__

`[2309.11489] Text2Reward: Reward Shaping with Language Models for Reinforcement Learning <https://arxiv.org/abs/2309.11489>`__

`[2310.05204] Towards Optimizing with Large Language Models <https://arxiv.org/abs/2310.05204>`__

`[2310.05269] Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications <https://arxiv.org/abs/2310.05269>`__

`[2310.06387] Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations <https://arxiv.org/abs/2310.06387>`__

`[2310.18491] Publicly-Detectable Watermarking for Language Models <https://arxiv.org/abs/2310.18491>`__

`[2312.06353] Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes <https://arxiv.org/abs/2312.06353>`__

`[2402.00854] SymbolicAI: A framework for logic-based approaches combining generative models and solvers <https://arxiv.org/abs/2402.00854>`__

`[2402.01621] Stochastic Two Points Method for Deep Model Zeroth-order Optimization <https://arxiv.org/abs/2402.01621>`__

`[2402.02314] Selecting Large Language Model to Fine-tune via Rectified Scaling Law <https://arxiv.org/abs/2402.02314>`__

`[2402.03282] A Theoretical Framework for Partially Observed Reward-States in RLHF <https://arxiv.org/abs/2402.03282>`__

`[2402.05445] Accurate LoRA-Finetuning Quantization of LLMs via Information Retention <https://arxiv.org/abs/2402.05445>`__

`[2402.07630] G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering <https://arxiv.org/abs/2402.07630>`__

`[2402.11867] LoRA Training in the NTK Regime has No Spurious Local Minima <https://arxiv.org/abs/2402.11867>`__

`[2402.13516] ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models <https://arxiv.org/abs/2402.13516>`__

`[2402.15390] Explorations of Self-Repair in Language Models <https://arxiv.org/abs/2402.15390>`__

`[2402.16354] Language-guided Skill Learning with Temporal Variational Inference <https://arxiv.org/abs/2402.16354>`__

`[2402.16902] PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA <https://arxiv.org/abs/2402.16902>`__

`[2402.17501] Intensive Care as One Big Sequence Modeling Problem <https://arxiv.org/abs/2402.17501>`__

`[2403.02107] Iterated $Q$-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning <https://arxiv.org/abs/2403.02107>`__

`[2403.16843] Do LLM Agents Have Regret? A Case Study in Online Learning and Games <https://arxiv.org/abs/2403.16843>`__

`[2403.17919] LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning <https://arxiv.org/abs/2403.17919>`__

`[2404.12404] Exploring Prompting Methods for Mitigating Class Imbalance through Synthetic Data Generation with Large Language Models <https://arxiv.org/abs/2404.12404>`__

`[2405.00675] Self-Play Preference Optimization for Language Model Alignment <https://arxiv.org/abs/2405.00675>`__

`[2405.12807] FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information <https://arxiv.org/abs/2405.12807>`__

`[2405.14622] Calibrated Self-Rewarding Vision Language Models <https://arxiv.org/abs/2405.14622>`__

`[2307.05360] Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures <https://arxiv.org/abs/2307.05360>`__

`[2402.01591] BAT: Learning to Reason about Spatial Sounds with Large Language Models <https://arxiv.org/abs/2402.01591>`__

`[2402.14807] A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health <https://arxiv.org/abs/2402.14807>`__

`[2404.00675] LLM meets Vision-Language Models for Zero-Shot One-Class Classification <https://arxiv.org/abs/2404.00675>`__

`[2405.08755] Distributed Threat Intelligence at the Edge Devices: A Large Language Model-Driven Approach <https://arxiv.org/abs/2405.08755>`__

`[2405.10632] Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks <https://arxiv.org/abs/2405.10632>`__

`[2402.16187] No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices <https://arxiv.org/abs/2402.16187>`__

`[2403.10943] MIntRec 2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations <https://arxiv.org/abs/2403.10943>`__

`[2405.12856] LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language <https://arxiv.org/abs/2405.12856>`__

`[2405.14191] S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models <https://arxiv.org/abs/2405.14191>`__

`[2405.14767] FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models <https://arxiv.org/abs/2405.14767>`__

`[2310.16152] FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering <https://arxiv.org/abs/2310.16152>`__

`[2402.08699] Unsupervised Evaluation of Code LLMs with Round-Trip Correctness <https://arxiv.org/abs/2402.08699>`__

`[2405.11299] The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving <https://arxiv.org/abs/2405.11299>`__

