240529
========

----------
Survey (2)
----------

`[2405.17935] Tool Learning with Large Language Models: A Survey <https://arxiv.org/abs/2405.17935>`__ 基于大型语言模型的工具学习综述

::

    Tue, 28 May 2024 08:01:26 GMT
    Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen

Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs.
We first explore the "why" by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of "how", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area.

------------

`[2403.18969] A Survey on Large Language Models from Concept to Implementation <https://arxiv.org/abs/2403.18969>`__ 大型语言模型从概念到实现综述

::

    replaced with revised version Tue, 28 May 2024 02:34:26 GMT
    Submission history From: Jin Zhao [view email]
    [v1] Wed, 27 Mar 2024 19:35:41 UTC (1,420 KB)
    [v2] Tue, 28 May 2024 02:34:26 UTC (1 KB) (withdrawn)
    Chen Wang, Jin Zhao, Jiaqi Gong

Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting their versatility and the potential they hold for transforming diverse application sectors, thereby offering readers a comprehensive understanding of the current and future landscape of Transformer-based LLMs in practical applications.

------------

-------------
Benchmark (5)
-------------

`[2405.17732] C$^{3}$Bench: A Comprehensive Classical Chinese Understanding Benchmark for Large Language Models <https://arxiv.org/abs/2405.17732>`__ C$^{3}$Bench:面向大型语言模型的综合古文理解基准

::

    Tue, 28 May 2024 01:23:58 GMT
    Jiahuan Cao, Yongxin Shi, Dezhi Peng, Yang Liu and Lianwen Jin

Classical Chinese Understanding (CCU) holds significant value in preserving and exploration of the outstanding traditional Chinese culture. Recently, researchers have attempted to leverage the potential of Large Language Models (LLMs) for CCU by capitalizing on their remarkable comprehension and semantic capabilities. However, no comprehensive benchmark is available to assess the CCU capabilities of LLMs. To fill this gap, this paper introduces C$^{3}$bench, a Comprehensive Classical Chinese understanding benchmark, which comprises 50,000 text pairs for five primary CCU tasks, including classification, retrieval, named entity recognition, punctuation, and translation. Furthermore, the data in C$^{3}$bench originates from ten different domains, covering most of the categories in classical Chinese. Leveraging the proposed C$^{3}$bench, we extensively evaluate the quantitative performance of 15 representative LLMs on all five CCU tasks. Our results not only establish a public leaderboard of LLMs' CCU capabilities but also gain some findings. Specifically, existing LLMs are struggle with CCU tasks and still inferior to supervised models.
Additionally, the results indicate that CCU is a task that requires special attention. We believe this study could provide a standard benchmark, comprehensive baselines, and valuable insights for the future advancement of LLM-based CCU research. The evaluation pipeline and dataset are available at \url{https://github.com/SCUT-DLVCLab/C3bench}.

------------

`[2405.18375] Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning <https://arxiv.org/abs/2405.18375>`__ 泰国Winograd模式:泰国常识推理的基准

::

    Tue, 28 May 2024 17:14:02 GMT
    Phakphum Artkaew

Commonsense reasoning is one of the important aspect of natural language understanding, with several benchmarks developed to evaluate it. However, only a few of these benchmarks are available in languages other than English.
Developing parallel benchmarks facilitates cross-lingual evaluation, enabling a better understanding of different languages. This research introduces a collection of Winograd Schemas in Thai, a novel dataset designed to evaluate commonsense reasoning capabilities in the context of the Thai language.
Through a methodology involving native speakers, professional translators, and thorough validation, the schemas aim to closely reflect Thai language nuances, idioms, and cultural references while maintaining ambiguity and commonsense challenges. We evaluate the performance of popular large language models on this benchmark, revealing their strengths, limitations, and providing insights into the current state-of-the-art. Results indicate that while models like GPT-4 and Claude-3-Opus achieve high accuracy in English, their performance significantly drops in Thai, highlighting the need for further advancements in multilingual commonsense reasoning.

------------

`[2402.14809] CriticBench: Benchmarking LLMs for Critique-Correct Reasoning <https://arxiv.org/abs/2402.14809>`__ CriticBench:评价正确推理的llm基准测试

::

    replaced with revised version Tue, 28 May 2024 14:33:27 GMT
    Submission history From: Zicheng Lin [view email]
    [v1] Thu, 22 Feb 2024 18:59:02 UTC (1,194 KB)
    [v2] Fri, 8 Mar 2024 15:15:47 UTC (1,187 KB)
    [v3] Tue, 28 May 2024 14:33:27 UTC (1,281 KB)
    Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang

The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement.

------------

`[2402.11592] Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark <https://arxiv.org/abs/2402.11592>`__ 访存高效LLM微调的零阶优化:基准

::

    replaced with revised version Tue, 28 May 2024 03:27:06 GMT
    Submission history From: Yihua Zhang [view email]
    [v1] Sun, 18 Feb 2024 14:08:48 UTC (205 KB)
    [v2] Mon, 26 Feb 2024 07:42:22 UTC (206 KB)
    [v3] Tue, 28 May 2024 03:27:06 UTC (196 KB)
    Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen

In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at this https URL .

------------

`[2405.14191] S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models <https://arxiv.org/abs/2405.14191>`__ S-Eval:面向大型语言模型安全性基准评估的自动自适应测试生成

::

    replaced with revised version Tue, 28 May 2024 11:31:31 GMT
    Submission history From: Xiaohan Yuan [view email]
    [v1] Thu, 23 May 2024 05:34:31 UTC (1,110 KB)
    [v2] Mon, 27 May 2024 08:27:29 UTC (1,092 KB)
    [v3] Tue, 28 May 2024 11:31:31 UTC (1,117 KB)
    Xiaohan Yuan, Jinfeng Li, Dongxia Wang, Yuefeng Chen, Xiaofeng Mao, Longtao Huang, Hui Xue, Wenhai Wang, Kui Ren, Jingyi Wang

Large Language Models have gained considerable attention for their revolutionary capabilities. However, there is also growing concern on their safety implications, making a comprehensive safety evaluation for LLMs urgently needed before model deployment. In this work, we propose S-Eval, a new comprehensive, multi-dimensional and open-ended safety evaluation benchmark. At the core of S-Eval is a novel LLM-based automatic test prompt generation and selection framework, which trains an expert testing LLM Mt combined with a range of test selection strategies to automatically construct a high-quality test suite for the safety evaluation. The key to the automation of this process is a novel expert safety-critique LLM Mc able to quantify the riskiness score of an LLM's response, and additionally produce risk tags and explanations. Besides, the generation process is also guided by a carefully designed risk taxonomy with four different levels, covering comprehensive and multi-dimensional safety risks of concern. Based on these, we systematically construct a new and large-scale safety evaluation benchmark for LLMs consisting of 220,000 evaluation prompts, including 20,000 base risk prompts (10,000 in Chinese and 10,000 in English) and 200,000 corresponding attack prompts derived from 10 popular adversarial instruction attacks against LLMs. Moreover, considering the rapid evolution of LLMs and accompanied safety threats, S-Eval can be flexibly configured and adapted to include new risks, attacks and models. S-Eval is extensively evaluated on 20 popular and representative LLMs. The results confirm that S-Eval can better reflect and inform the safety risks of LLMs compared to existing benchmarks. We also explore the impacts of parameter scales, language environments, and decoding parameters on the evaluation, providing a systematic methodology for evaluating the safety of LLMs.

------------

---------------
Accelerate (11)
---------------

`[2405.18377] LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models <https://arxiv.org/abs/2405.18377>`__ LLaMA-NAS:面向大型语言模型的高效神经架构搜索

::

    Tue, 28 May 2024 17:20:44 GMT
    Anthony Sarah, Sharath Nittur Sridhar, Maciej Szankin, Sairam Sundaresan

The abilities of modern large language models (LLMs) in solving natural language processing, complex reasoning, sentiment analysis and other tasks have been extraordinary which has prompted their extensive adoption. Unfortunately, these abilities come with very high memory and computational costs which precludes the use of LLMs on most hardware platforms. To mitigate this, we propose an effective method of finding Pareto-optimal network architectures based on LLaMA2-7B using one-shot NAS. In particular, we fine-tune LLaMA2-7B only once and then apply genetic algorithm-based search to find smaller, less computationally complex network architectures. We show that, for certain standard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily large and complex. More specifically, we demonstrate a 1.5x reduction in model size and 1.3x speedup in throughput for certain tasks with negligible drop in accuracy. In addition to finding smaller, higher-performing network architectures, our method does so more effectively and efficiently than certain pruning or sparsification techniques. Finally, we demonstrate how quantization is complementary to our method and that the size and complexity of the networks we find can be further decreased using quantization. We believe that our work provides a way to automatically create LLMs which can be used on less expensive and more readily available hardware platforms.

------------

`[2405.18292] Semantic are Beacons: A Semantic Perspective for Unveiling Parameter-Efficient Fine-Tuning in Knowledge Learning <https://arxiv.org/abs/2405.18292>`__ 语义是信标:揭示知识学习中参数高效微调的语义视角

::

    Tue, 28 May 2024 15:47:11 GMT
    Renzhi Wang, Piji Li

Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of Large Language Models (LLMs) to various downstream applications. However, the effectiveness of the PEFT diminishes notably when downstream tasks require accurate learning of factual knowledge. In this paper, we adopt a semantic perspective to investigate this phenomenon, uncovering the reasons behind PEFT's limitations in knowledge learning task. Our findings reveal that: (1) PEFT presents a notable risk of pushing the model away from the intended knowledge target; (2) multiple knowledge interfere with each other, and such interference suppresses the learning and expression of knowledge features.
Based on these insights, we introduce a data filtering strategy to exclude data that is detrimental to knowledge learning and a re-weighted learning strategy to make the model attentive to semantic distance during knowledge learning.
Experimental results demonstrate the effectiveness of the proposed method on open-source large language model, further validate the semantic challenge in PEFT, thus paving the way for future research.

------------

`[2405.17470] Athena: Efficient Block-Wise Post-Training Quantization for Large Language Models Using Second-Order Matrix Derivative Information <https://arxiv.org/abs/2405.17470>`__ Athena:基于二阶矩阵导数信息的大型语言模型高效分块后训练量化

::

    Fri, 24 May 2024 03:14:29 GMT
    Yanshu Wang, Wenyang He and Tong Yang

Large Language Models (LLMs) have significantly advanced natural language processing tasks such as machine translation, text generation, and sentiment analysis. However, their large size, often consisting of billions of parameters, poses challenges for storage, computation, and deployment, particularly in resource-constrained environments like mobile devices and edge computing platforms. Effective compression and quantization techniques are crucial for addressing these issues, reducing memory footprint and computational requirements without significantly compromising performance.
Traditional methods that uniformly map parameters to compressed spaces fail to account for the uneven distribution of parameters, leading to substantial accuracy loss. In this work, we propose Athena, a novel algorithm for efficient block-wise post-training quantization of LLMs. Athena leverages Second-Order Matrix Derivative Information to guide the quantization process using the curvature information of the loss landscape. By grouping parameters by columns or rows and iteratively optimizing the quantization process, Athena updates the model parameters and Hessian matrix to achieve significant compression while maintaining high accuracy. This makes Athena a practical solution for deploying LLMs in various settings.

------------

`[2405.17849] I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models <https://arxiv.org/abs/2405.17849>`__ I-LLM:全量化低比特大型语言模型的高效纯整数推理

::

    Tue, 28 May 2024 05:56:11 GMT
    Xing Hu, Yuan Chen, Dawei Yang, Sifan Zhou, Zhihang Yuan, Jiangyong Yu, Chen Xu

Post-training quantization (PTQ) serves as a potent technique to accelerate the inference of large language models (LLMs). Nonetheless, existing works still necessitate a considerable number of floating-point (FP) operations during inference, including additional quantization and de-quantization, as well as non-linear operators such as RMSNorm and Softmax. This limitation hinders the deployment of LLMs on the edge and cloud devices. In this paper, we identify the primary obstacle to integer-only quantization for LLMs lies in the large fluctuation of activations across channels and tokens in both linear and non-linear operations. To address this issue, we propose I-LLM, a novel integer-only fully-quantized PTQ framework tailored for LLMs. Specifically, (1) we develop Fully-Smooth Block-Reconstruction (FSBR) to aggressively smooth inter-channel variations of all activations and weights. (2) to alleviate degradation caused by inter-token variations, we introduce a novel approach called Dynamic Integer-only MatMul (DI-MatMul). This method enables dynamic quantization in full-integer matrix multiplication by dynamically quantizing the input and outputs with integer-only operations. (3) we design DI-ClippedSoftmax, DI-Exp, and DI-Normalization, which utilize bit shift to execute non-linear operators efficiently while maintaining accuracy. The experiment shows that our I-LLM achieves comparable accuracy to the FP baseline and outperforms non-integer quantization methods. For example, I-LLM can operate at W4A4 with negligible loss of accuracy. To our knowledge, we are the first to bridge the gap between integer-only quantization and LLMs. We've published our code on anonymous.4open.science, aiming to contribute to the advancement of this field.

------------

`[2405.18380] OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning <https://arxiv.org/abs/2405.18380>`__ OwLore:离群点加权分层采样低秩投影的内存高效LLM微调

::

    Tue, 28 May 2024 17:22:22 GMT
    Pengxiang Li, Lu Yin, Xiaowei Gao, Shiwei Liu

The rapid advancements in Large Language Models (LLMs) have revolutionized various natural language processing tasks. However, the substantial size of LLMs presents significant challenges in training or fine-tuning. While parameter-efficient approaches such as low-rank adaptation (LoRA) have gained popularity, they often compromise performance compared to full-rank fine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampled Low-Rank Projection (OwLore), a new memory-efficient fine-tuning approach, inspired by the layerwise outlier distribution of LLMs, which dynamically samples pre-trained layers to fine-tune instead of adding additional adaptors.
We first interpret the outlier phenomenon through the lens of Heavy-Tailed Self-Regularization theory (HT-SR), discovering that layers with more outliers tend to be more heavy-tailed and consequently better trained. Inspired by this finding, OwLore strategically assigns higher sampling probabilities to layers with more outliers to better leverage the knowledge stored in pre-trained LLMs.
To further mitigate the memory demands of fine-tuning, we integrate gradient low-rank projection into our approach, which facilitates each layer to be efficiently trained in a low-rank manner. By incorporating the efficient characteristics of low-rank and optimal layerwise sampling, OwLore significantly improves the memory-performance trade-off in LLM pruning. Our extensive experiments across various architectures, including LLaMa2, LLaMa3, and Mistral, demonstrate that OwLore consistently outperforms baseline approaches, including full fine-tuning. Specifically, it achieves up to a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0% improvement on MMLU, and a notable 10% boost on MT-Bench, while being more memory efficient. OwLore allows us to fine-tune LLaMa2-7B with only 21GB of memory.

------------

`[2405.17991] VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections <https://arxiv.org/abs/2405.17991>`__ VeLoRA:基于Rank-1子token投影的内存高效训练

::

    Tue, 28 May 2024 09:23:14 GMT
    Roy Miles, Pradyumna Reddy, Ismail Elezi, Jiankang Deng

Large language models (LLMs) have recently emerged as powerful tools for tackling many language-processing tasks. Despite their success, training and fine-tuning these models is still far too computationally and memory intensive.
In this paper, we identify and characterise the important components needed for effective model convergence using gradient descent. In doing so we find that the intermediate activations used to implement backpropagation can be excessively compressed without incurring any degradation in performance. This result leads us to a cheap and memory-efficient algorithm for both fine-tuning and pre-training LLMs. The proposed algorithm simply divides the tokens up into smaller sub-tokens before projecting them onto a fixed 1-dimensional subspace during the forward pass. These features are then coarsely reconstructed during the backward pass to implement the update rules. We confirm the effectiveness of our algorithm as being complimentary to many state-of-the-art PEFT methods on the VTAB-1k fine-tuning benchmark. Furthermore, we outperform QLoRA for fine-tuning LLaMA and show competitive performance against other memory-efficient pre-training methods on the large-scale C4 dataset.

------------

`[2402.04617] InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory <https://arxiv.org/abs/2402.04617>`__ InfLLM:具有高效上下文记忆的llm免训练长上下文外推

::

    replaced with revised version Tue, 28 May 2024 12:05:12 GMT
    Submission history From: Chaojun Xiao [view email]
    [v1] Wed, 7 Feb 2024 06:50:42 UTC (2,123 KB)
    [v2] Tue, 28 May 2024 12:05:12 UTC (1,324 KB)
    Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun

Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies. Our code can be found in \url{this https URL}.

------------

`[2405.14259] Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Multi-modal Text Recognition <https://arxiv.org/abs/2405.14259>`__ 让我们一步步融合:基于LLMs的生成式融合解码算法用于多模态文本识别

::

    replaced with revised version Tue, 28 May 2024 14:45:30 GMT
    Submission history From: Chan-Jan Hsu [view email]
    [v1] Thu, 23 May 2024 07:39:42 UTC (285 KB)
    [v2] Tue, 28 May 2024 14:45:30 UTC (282 KB)
    Chan-Jan Hsu, Yi-Chang Chen, Feng-Ting Liao, Pei-Chen Ho, Yu-Hsiang Wang, Po-Chun Hsu, Da-shan Shiu

We introduce "Generative Fusion Decoding" (GFD), a novel shallow fusion framework, utilized to integrate Large Language Models (LLMs) into multi-modal text recognition systems such as automatic speech recognition (ASR) and optical character recognition (OCR). We derive the formulas necessary to enable GFD to operate across mismatched token spaces of different models by mapping text token space to byte token space, enabling seamless fusion during the decoding process. The framework is plug-and-play, compatible with various auto-regressive models, and does not require re-training for feature alignment, thus overcoming limitations of previous fusion techniques. We highlight three main advantages of GFD: First, by simplifying the complexity of aligning different model sample spaces, GFD allows LLMs to correct errors in tandem with the recognition model, reducing computation latencies. Second, the in-context learning ability of LLMs is fully capitalized by GFD, increasing robustness in long-form speech recognition and instruction aware speech recognition. Third, GFD enables fusing recognition models deficient in Chinese text recognition with LLMs extensively trained on Chinese. Our evaluation demonstrates that GFD significantly improves performance in ASR and OCR tasks, with ASR reaching state-of-the-art in the NTUML2021 benchmark. GFD provides a significant step forward in model integration, offering a unified solution that could be widely applicable to leveraging existing pre-trained models through step by step fusion.

------------

`[2405.15179] VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks <https://arxiv.org/abs/2405.15179>`__ VB-LoRA:基于向量组的极端参数高效微调

::

    replaced with revised version Mon, 27 May 2024 18:51:57 GMT
    Submission history From: Yang Li [view email]
    [v1] Fri, 24 May 2024 03:24:34 UTC (265 KB)
    [v2] Mon, 27 May 2024 18:51:57 UTC (265 KB)
    Yang Li, Shaobo Han, Shihao Ji

As the adoption of large language models increases and the need for per-user or per-task model customization grows, the parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur substantial storage and transmission costs. To further reduce stored parameters, we introduce a "divide-and-share" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules and layers by sharing parameters globally via a vector bank. As an instantiation of the paradigm to LoRA, our proposed VB-LoRA composites all the low-rank matrices of LoRA from a shared vector bank with a differentiable top-$k$ admixture module. VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods. Extensive experiments demonstrate the effectiveness of VB-LoRA on natural language understanding, natural language generation, and instruction tuning tasks. When fine-tuning the Llama2-13B model, VB-LoRA only uses 0.4% of LoRA's stored parameters, yet achieves superior results. Our source code is available at this https URL.

------------

`[2305.16617] Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model <https://arxiv.org/abs/2305.16617>`__ 基于贝叶斯代理模型的llm生成文本高效检测

::

    replaced with revised version Tue, 28 May 2024 16:10:59 GMT
    Submission history From: Yibo Miao [view email]
    [v1] Fri, 26 May 2023 04:23:10 UTC (706 KB)
    [v2] Tue, 28 May 2024 16:10:59 UTC (513 KB)
    Yibo Miao, Hongcheng Gao, Hao Zhang, Zhijie Deng

The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries.

------------

`[2402.11592] Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark <https://arxiv.org/abs/2402.11592>`__ 访存高效LLM微调的零阶优化:基准

::

    replaced with revised version Tue, 28 May 2024 03:27:06 GMT
    Submission history From: Yihua Zhang [view email]
    [v1] Sun, 18 Feb 2024 14:08:48 UTC (205 KB)
    [v2] Mon, 26 Feb 2024 07:42:22 UTC (206 KB)
    [v3] Tue, 28 May 2024 03:27:06 UTC (196 KB)
    Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen

In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at this https URL .

------------

--------------
Reasoning (11)
--------------

`[2405.18073] Towards Dialogues for Joint Human-AI Reasoning and Value Alignment <https://arxiv.org/abs/2405.18073>`__ 人- ai联合推理与价值对齐的对话

::

    Tue, 28 May 2024 11:29:57 GMT
    Elfia Bezou-Vrakatseli and Oana Cocarascu and Sanjay Modgil

We argue that enabling human-AI dialogue, purposed to support joint reasoning (i.e., 'inquiry'), is important for ensuring that AI decision making is aligned with human values and preferences. In particular, we point to logic-based models of argumentation and dialogue, and suggest that the traditional focus on persuasion dialogues be replaced by a focus on inquiry dialogues, and the distinct challenges that joint inquiry raises. Given recent dramatic advances in the performance of large language models (LLMs), and the anticipated increase in their use for decision making, we provide a roadmap for research into inquiry dialogues for supporting joint human-LLM reasoning tasks that are ethically salient, and that thereby require that decisions are value aligned.

------------

`[2405.18208] A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models <https://arxiv.org/abs/2405.18208>`__ 面向大型语言模型多阶段规划任务的类人推理框架

::

    Tue, 28 May 2024 14:13:32 GMT
    Chengxing Xie, Difan Zou

Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies. However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue. In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties. Existing reasoning approaches have struggled to effectively address this complex task. Our research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems. Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns. Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning. Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness.
Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains $10\times$ the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo.

------------

`[2405.17893] Arithmetic Reasoning with LLM: Prolog Generation & Permutation <https://arxiv.org/abs/2405.17893>`__ 基于LLM的算术推理:Prolog生成与置换

::

    Tue, 28 May 2024 07:13:25 GMT
    Xiaocheng Yang, Bingsen Chen, Yik-Cheung Tam

Instructing large language models (LLMs) to solve elementary school math problems has shown great success using Chain of Thought (CoT). However, the CoT approach relies on an LLM to generate a sequence of arithmetic calculations which can be prone to cascaded calculation errors. We hypothesize that an LLM should focus on extracting predicates and generating symbolic formulas from the math problem description so that the underlying calculation can be done via an external code interpreter. We investigate using LLM to generate Prolog programs to solve mathematical questions. Experimental results show that our Prolog-based arithmetic problem-solving outperforms CoT generation in the GSM8K benchmark across three distinct LLMs. In addition, given the insensitive ordering of predicates and symbolic formulas in Prolog, we propose to permute the ground truth predicates for more robust LLM training via data augmentation.

------------

`[2405.18357] Faithful Logical Reasoning via Symbolic Chain-of-Thought <https://arxiv.org/abs/2405.18357>`__ 通过符号思维链进行忠实的逻辑推理

::

    Tue, 28 May 2024 16:55:33 GMT
    Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, Wynne Hsu

While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at https://github.com/Aiden0526/SymbCoT.

------------

`[2405.18358] MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning <https://arxiv.org/abs/2405.18358>`__ MMCTAgent:面向复杂视觉推理的多模态批判性思维Agent框架

::

    Tue, 28 May 2024 16:55:41 GMT
    Somnath Kumar, Yash Gadhia, Tanuja Ganu and Akshay Nambi

Recent advancements in Multi-modal Large Language Models (MLLMs) have significantly improved their performance in tasks combining vision and language. However, challenges persist in detailed multi-modal understanding, comprehension of complex tasks, and reasoning over multi-modal information.
This paper introduces MMCTAgent, a novel multi-modal critical thinking agent framework designed to address the inherent limitations of current MLLMs in complex visual reasoning tasks. Inspired by human cognitive processes and critical thinking, MMCTAgent iteratively analyzes multi-modal information, decomposes queries, plans strategies, and dynamically evolves its reasoning.
Additionally, MMCTAgent incorporates critical thinking elements such as verification of final answers and self-reflection through a novel approach that defines a vision-based critic and identifies task-specific evaluation criteria, thereby enhancing its decision-making abilities. Through rigorous evaluations across various image and video understanding benchmarks, we demonstrate that MMCTAgent (with and without the critic) outperforms both foundational MLLMs and other tool-augmented pipelines.

------------

`[2405.18375] Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning <https://arxiv.org/abs/2405.18375>`__ 泰国Winograd模式:泰国常识推理的基准

::

    Tue, 28 May 2024 17:14:02 GMT
    Phakphum Artkaew

Commonsense reasoning is one of the important aspect of natural language understanding, with several benchmarks developed to evaluate it. However, only a few of these benchmarks are available in languages other than English.
Developing parallel benchmarks facilitates cross-lingual evaluation, enabling a better understanding of different languages. This research introduces a collection of Winograd Schemas in Thai, a novel dataset designed to evaluate commonsense reasoning capabilities in the context of the Thai language.
Through a methodology involving native speakers, professional translators, and thorough validation, the schemas aim to closely reflect Thai language nuances, idioms, and cultural references while maintaining ambiguity and commonsense challenges. We evaluate the performance of popular large language models on this benchmark, revealing their strengths, limitations, and providing insights into the current state-of-the-art. Results indicate that while models like GPT-4 and Claude-3-Opus achieve high accuracy in English, their performance significantly drops in Thai, highlighting the need for further advancements in multilingual commonsense reasoning.

------------

`[2402.08939] Premise Order Matters in Reasoning with Large Language Models <https://arxiv.org/abs/2402.08939>`__ 前提顺序在大型语言模型推理中很重要

::

    replaced with revised version Tue, 28 May 2024 04:32:09 GMT
    Submission history From: Xinyun Chen [view email]
    [v1] Wed, 14 Feb 2024 04:50:18 UTC (7,955 KB)
    [v2] Mon, 4 Mar 2024 09:21:16 UTC (7,955 KB)
    [v3] Tue, 28 May 2024 04:32:09 UTC (7,957 KB)
    Xinyun Chen, Ryan A. Chi, Xuezhi Wang, Denny Zhou

Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.

------------

`[2402.03686] Are Machines Better at Complex Reasoning? Unveiling Human-Machine Inference Gaps in Entailment Verification <https://arxiv.org/abs/2402.03686>`__ 机器更擅长复杂的推理吗?揭秘蕴涵验证中的人机推理缺口

::

    replaced with revised version Mon, 27 May 2024 18:44:14 GMT
    Submission history From: Soumya Sanyal [view email]
    [v1] Tue, 6 Feb 2024 04:14:09 UTC (835 KB)
    [v2] Thu, 22 Feb 2024 04:13:36 UTC (1,333 KB)
    [v3] Mon, 27 May 2024 18:44:14 UTC (1,614 KB)
    Soumya Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya Wang, Xiang Ren

Making inferences in text comprehension to understand the meaning is essential in language processing. This work studies the entailment verification (EV) problem of multi-sentence premises that requires a system to make multiple inferences implicitly. Studying EV for such complex premises is important because modern NLP problems, such as detecting inconsistent model-generated rationales, require complex multi-hop reasoning. However, current textual inference datasets mostly contain short premises that only partially focus on these challenges. To address this, we compile an EV benchmark that includes datasets from three NLP domains (NLI, contextual QA, and rationales) containing multi-sentence premises. On benchmarking humans and LLMs, we find that LLMs are better than humans in multi-hop reasoning across extended contexts, while humans perform better in simple deductive reasoning tasks. We also finetune a Flan-T5 model for EV using two training objectives to obtain a strong open-source model that outperforms GPT-3.5 and rivals GPT-4. Finally, we use this model to filter out inconsistent model-generated rationales in self-consistency decoding, resulting in a 6% accuracy improvement on average across three MCQ datasets.

------------

`[2402.14809] CriticBench: Benchmarking LLMs for Critique-Correct Reasoning <https://arxiv.org/abs/2402.14809>`__ CriticBench:评价正确推理的llm基准测试

::

    replaced with revised version Tue, 28 May 2024 14:33:27 GMT
    Submission history From: Zicheng Lin [view email]
    [v1] Thu, 22 Feb 2024 18:59:02 UTC (1,194 KB)
    [v2] Fri, 8 Mar 2024 15:15:47 UTC (1,187 KB)
    [v3] Tue, 28 May 2024 14:33:27 UTC (1,281 KB)
    Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang

The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement.

------------

`[2405.16802] AutoCV: Empowering Reasoning with Automated Process Labeling via Confidence Variation <https://arxiv.org/abs/2405.16802>`__ AutoCV:基于置信度变化的自动过程标记授权推理

::

    replaced with revised version Tue, 28 May 2024 09:35:27 GMT
    Submission history From: Jianqiao Lu [view email]
    [v1] Mon, 27 May 2024 03:44:24 UTC (203 KB)
    [v2] Tue, 28 May 2024 09:35:27 UTC (351 KB)
    Jianqiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yingjia Wan, Yinya Huang, Zhijiang Guo

In this work, we propose a novel method named \textbf{Auto}mated Process Labeling via \textbf{C}onfidence \textbf{V}ariation (\textbf{\textsc{AutoCV}}) to enhance the reasoning capabilities of large language models (LLMs) by automatically annotating the reasoning steps. Our approach begins by training a verification model on the correctness of final answers, enabling it to generate automatic process annotations. This verification model assigns a confidence score to each reasoning step, indicating the probability of arriving at the correct final answer from that point onward. We detect relative changes in the verification's confidence scores across reasoning steps to automatically annotate the reasoning process. This alleviates the need for numerous manual annotations or the high computational costs associated with model-induced annotation approaches. We experimentally validate that the confidence variations learned by the verification model trained on the final answer correctness can effectively identify errors in the reasoning steps. Subsequently, we demonstrate that the process annotations generated by \textsc{AutoCV} can improve the accuracy of the verification model in selecting the correct answer from multiple outputs generated by LLMs. Notably, we achieve substantial improvements across five datasets in mathematics and commonsense reasoning. The source code of \textsc{AutoCV} is available at \url{this https URL}.

------------

`[2402.17453] DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning <https://arxiv.org/abs/2402.17453>`__ DS-Agent:通过赋予大型语言模型基于案例的推理实现自动化数据科学

::

    replaced with revised version Tue, 28 May 2024 06:50:38 GMT
    Submission history From: Siyuan Guo [view email]
    [v1] Tue, 27 Feb 2024 12:26:07 UTC (370 KB)
    [v2] Wed, 13 Mar 2024 12:02:25 UTC (370 KB)
    [v3] Sat, 6 Apr 2024 12:28:57 UTC (370 KB)
    [v4] Fri, 24 May 2024 12:40:48 UTC (366 KB)
    [v5] Tue, 28 May 2024 06:50:38 UTC (367 KB)
    Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang

In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100\% success rate in the development stage, while attaining 36\% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \$1.60 and \$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at this https URL.

------------

-----------
ToolUse (4)
-----------

`[2405.17935] Tool Learning with Large Language Models: A Survey <https://arxiv.org/abs/2405.17935>`__ 基于大型语言模型的工具学习综述

::

    Tue, 28 May 2024 08:01:26 GMT
    Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen

Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs.
We first explore the "why" by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of "how", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area.

------------

`[2405.17438] An LLM-Tool Compiler for Fused Parallel Function Calling <https://arxiv.org/abs/2405.17438>`__ 一个用于融合并行函数调用的LLM-Tool编译器

::

    Tue, 7 May 2024 18:55:50 GMT
    Simranjit Singh, Andreas Karatzas, Michael Fore, Iraklis Anagnostopoulos, Dimitrios Stamoulis

State-of-the-art sequential reasoning in Large Language Models (LLMs) has expanded the capabilities of Copilots beyond conversational tasks to complex function calling, managing thousands of API calls. However, the tendency of compositional prompting to segment tasks into multiple steps, each requiring a round-trip to the GPT APIs, leads to increased system latency and costs.
Although recent advancements in parallel function calling have improved tool execution per API call, they may necessitate more detailed in-context instructions and task breakdown at the prompt level, resulting in higher engineering and production costs. Inspired by the hardware design principles of multiply-add (MAD) operations, which fuse multiple arithmetic operations into a single task from the compiler's perspective, we propose LLM-Tool Compiler, which selectively fuses similar types of tool operations under a single function at runtime, presenting them as a unified task to the LLM. This selective fusion inherently enhances parallelization and efficiency.
Benchmarked on a large-scale Copilot platform, LLM-Tool Compiler achieves up to four times more parallel calls than existing methods, reducing token costs and latency by up to 40% and 12%, respectively.

------------

`[2402.10379] DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows <https://arxiv.org/abs/2402.10379>`__ DataDreamer:用于合成数据生成和可重现的LLM工作流的工具

::

    replaced with revised version Mon, 27 May 2024 19:54:44 GMT
    Submission history From: Ajay Patel [view email]
    [v1] Fri, 16 Feb 2024 00:10:26 UTC (688 KB)
    [v2] Mon, 27 May 2024 19:54:44 UTC (689 KB)
    Ajay Patel, Colin Raffel, Chris Callison-Burch

Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at this https URL .

------------

`[2405.16376] STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making <https://arxiv.org/abs/2405.16376>`__ STRIDE:一个工具辅助的战略交互式决策LLM Agent框架

::

    replaced with revised version Tue, 28 May 2024 01:21:19 GMT
    Submission history From: Chuanhao Li [view email]
    [v1] Sat, 25 May 2024 23:25:10 UTC (679 KB)
    [v2] Tue, 28 May 2024 01:21:19 UTC (679 KB)
    Chuanhao Li, Runhan Yang, Tiankai Li, Milad Bafarassat, Kourosh Sharifi, Dirk Bergemann, and Zhuoran Yang

Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities. However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information. These deficiencies hinder their performance in strategic and interactive tasks that demand adherence to nuanced game rules, long-term planning, exploration in unknown environments, and anticipation of opponents' moves. To overcome these obstacles, this paper presents a novel LLM agent framework equipped with memory and specialized tools to enhance their strategic decision-making capabilities. We deploy the tools in a number of economically important environments, in particular bilateral bargaining and multi-agent and dynamic mechanism design. We employ quantitative metrics to assess the framework's performance in various strategic decision-making problems. Our findings establish that our enhanced framework significantly improves the strategic decision-making capability of LLMs. While we highlight the inherent limitations of current LLM models, we demonstrate the improvements through targeted enhancements, suggesting a promising direction for future developments in LLM applications for interactive environments.

------------

-----------------------
Retrieval-Augmented (5)
-----------------------

`[2405.17706] Video Enriched Retrieval Augmented Generation Using Aligned Video Captions <https://arxiv.org/abs/2405.17706>`__ 使用对齐视频字幕的视频丰富检索增强生成

::

    Mon, 27 May 2024 23:39:17 GMT
    Kevin Dela Rosa

In this work, we propose the use of "aligned visual captions" as a mechanism for integrating information contained within videos into retrieval augmented generation (RAG) based chat assistant systems. These captions are able to describe the visual and audio content of videos in a large corpus while having the advantage of being in a textual format that is both easy to reason about & incorporate into large language model (LLM) prompts, but also typically require less multimedia content to be inserted into the multimodal LLM context window, where typical configurations can aggressively fill up the context window by sampling video frames from the source video. Furthermore, visual captions can be adapted to specific use cases by prompting the original foundational model / captioner for particular visual details or fine tuning. In hopes of helping advancing progress in this area, we curate a dataset and describe automatic evaluation procedures on common RAG tasks.

------------

`[2405.18035] Instruction Tuning with Retrieval-based Examples Ranking for Aspect-based Sentiment Analysis <https://arxiv.org/abs/2405.18035>`__ 面向方面情感分析的基于检索示例排序的指令调优

::

    Tue, 28 May 2024 10:39:10 GMT
    Guangmin Zheng, Jin Wang, Liang-Chih Yu, Xuejie Zhang

Aspect-based sentiment analysis (ABSA) identifies sentiment information related to specific aspects and provides deeper market insights to businesses and organizations. With the emergence of large language models (LMs), recent studies have proposed using fixed examples for instruction tuning to reformulate ABSA as a generation task. However, the performance is sensitive to the selection of in-context examples; several retrieval methods are based on surface similarity and are independent of the LM generative objective. This study proposes an instruction learning method with retrieval-based example ranking for ABSA tasks. For each target sample, an LM was applied as a scorer to estimate the likelihood of the output given the input and a candidate example as the prompt, and training examples were labeled as positive or negative by ranking the scores. An alternating training schema is proposed to train both the retriever and LM. Instructional prompts can be constructed using high-quality examples. The LM is used for both scoring and inference, improving the generation efficiency without incurring additional computational costs or training difficulties. Extensive experiments on three ABSA subtasks verified the effectiveness of the proposed method, demonstrating its superiority over various strong baseline models. Code and data are released at https://anonymous.4open.science/r/IT-RER-ABSA-181F.

------------

`[2405.18111] ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator <https://arxiv.org/abs/2405.18111>`__ ATM:对抗性调优多智能体系统构建鲁棒的检索增强生成器

::

    Tue, 28 May 2024 12:18:50 GMT
    Junda Zhu, Lingyong Yan, Haibo Shi, Dawei Yin, Lei Sha

Large language model (LLM) has proven to benefit a lot from retrieval augmentation in alleviating hallucinations confronted with knowledge-intensive questions. Retrieval-augmented generation (RAG) adopts IR-based techniques utilizing semantic-relevant documents as the generator's input context and realizes external knowledge injection. However, on today's Internet which is flooded with content generated by LLMs, there are too many "related yet useless" documents or even fake knowledge fabricated by LLMs, which will introduce extra noise to the generator and distract it from giving correct results. To this end, we regard the training of the RAG generator model as a multi-agent adversarial-defensive system, guiding the generator to have a better taste of whether a specific document helps answer the question through the Adversarial Tuning in a Multi-agent (ATM) system to strengthen the generator's robustness in an RAG pipeline. After rounds of multi-agent iterative tuning, we find that the ATM Generator can eventually discriminate useful documents amongst LLM fabrications and achieve better performance than strong baselines.

------------

`[2405.18414] Don't Forget to Connect! Improving RAG with Graph-based Reranking <https://arxiv.org/abs/2405.18414>`__ 别忘了联系!用基于图的重排序来改进RAG

::

    Tue, 28 May 2024 17:56:46 GMT
    Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, Anton Tsitsulin

Retrieval Augmented Generation (RAG) has greatly improved the performance of Large Language Model (LLM) responses by grounding generation with context from existing documents. These systems work well when documents are clearly relevant to a question context. But what about when a document has partial information, or less obvious connections to the context? And how should we reason about connections between documents? In this work, we seek to answer these two core questions about RAG generation. We introduce G-RAG, a reranker based on graph neural networks (GNNs) between the retriever and reader in RAG. Our method combines both connections between documents and semantic information (via Abstract Meaning Representation graphs) to provide a context-informed ranker for RAG. G-RAG outperforms state-of-the-art approaches while having smaller computational footprint. Additionally, we assess the performance of PaLM 2 as a reranker and find it to significantly underperform G-RAG. This result emphasizes the importance of reranking for RAG even when using Large Language Models.

------------

`[2405.17587] RAGSys: Item-Cold-Start Recommender as RAG System <https://arxiv.org/abs/2405.17587>`__ RAGSys:作为RAG系统的物品冷启动推荐

::

    Mon, 27 May 2024 18:40:49 GMT
    Emile Contal, Garrin McGoldrick

Large Language Models (LLM) hold immense promise for real-world applications, but their generic knowledge often falls short of domain-specific needs.
Fine-tuning, a common approach, can suffer from catastrophic forgetting and hinder generalizability. In-Context Learning (ICL) offers an alternative, which can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant demonstrations for few-shot learning tasks. This paper explores the desired qualities of a demonstration retrieval system for ICL. We argue that ICL retrieval in this context resembles item-cold-start recommender systems, prioritizing discovery and maximizing information gain over strict relevance.
We propose a novel evaluation method that measures the LLM's subsequent performance on NLP tasks, eliminating the need for subjective diversity scores.
Our findings demonstrate the critical role of diversity and quality bias in retrieved demonstrations for effective ICL, and highlight the potential of recommender system techniques in this domain.

------------

----------
Agent (12)
----------

`[2405.17631] BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments <https://arxiv.org/abs/2405.17631>`__ BioDiscoveryAgent:用于设计遗传扰动实验的人工智能Agent

::

    Mon, 27 May 2024 19:57:17 GMT
    Yusuf Roohani, Jian Vora, Qian Huang, Zachary Steinhart, Alexander Marson, Percy Liang, Jure Leskovec

Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities. Here, we develop BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions. We demonstrate our agent on the problem of designing genetic perturbation experiments, where the aim is to find a small subset out of many possible genes that, when perturbed, result in a specific phenotype (e.g., cell growth). Utilizing its biological knowledge, BioDiscoveryAgent can uniquely design new experiments without the need to train a machine learning model or explicitly design an acquisition function. Moreover, BioDiscoveryAgent achieves an average of 18% improvement in detecting desired phenotypes across five datasets, compared to existing Bayesian optimization baselines specifically trained for this task. Our evaluation includes one dataset that is unpublished, ensuring it is not part of the language model's training data. Additionally, BioDiscoveryAgent predicts gene combinations to perturb twice as accurately as a random baseline, a task so far not explored in the context of closed-loop experiment design. The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every stage, representing an accessible new paradigm in the computational design of biological experiments with the potential to augment scientists' capabilities.

------------

`[2405.18092] LLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins <https://arxiv.org/abs/2405.18092>`__ LLM仿真实验:面向Digital Twins过程仿真参数化的大型语言模型多agent系统

::

    Tue, 28 May 2024 11:59:40 GMT
    Yuchen Xia, Daniel Dittler, Nasser Jazdi, Haonan Chen, Michael Weyrich

This paper presents a novel design of a multi-agent system framework that applies a large language model (LLM) to automate the parametrization of process simulations in digital twins. We propose a multi-agent framework that includes four types of agents: observation, reasoning, decision and summarization. By enabling dynamic interaction between LLM agents and simulation model, the developed system can automatically explore the parametrization of the simulation and use heuristic reasoning to determine a set of parameters to control the simulation to achieve an objective. The proposed approach enhances the simulation model by infusing it with heuristics from LLM and enables autonomous search for feasible parametrization to solve a user task.
Furthermore, the system has the potential to increase user-friendliness and reduce the cognitive load on human users by assisting in complex decision-making processes. The effectiveness and functionality of the system are demonstrated through a case study, and the visualized demos are available at a GitHub Repository: https://github.com/YuchenXia/LLMDrivenSimulation

------------

`[2405.18111] ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator <https://arxiv.org/abs/2405.18111>`__ ATM:对抗性调优多智能体系统构建鲁棒的检索增强生成器

::

    Tue, 28 May 2024 12:18:50 GMT
    Junda Zhu, Lingyong Yan, Haibo Shi, Dawei Yin, Lei Sha

Large language model (LLM) has proven to benefit a lot from retrieval augmentation in alleviating hallucinations confronted with knowledge-intensive questions. Retrieval-augmented generation (RAG) adopts IR-based techniques utilizing semantic-relevant documents as the generator's input context and realizes external knowledge injection. However, on today's Internet which is flooded with content generated by LLMs, there are too many "related yet useless" documents or even fake knowledge fabricated by LLMs, which will introduce extra noise to the generator and distract it from giving correct results. To this end, we regard the training of the RAG generator model as a multi-agent adversarial-defensive system, guiding the generator to have a better taste of whether a specific document helps answer the question through the Adversarial Tuning in a Multi-agent (ATM) system to strengthen the generator's robustness in an RAG pipeline. After rounds of multi-agent iterative tuning, we find that the ATM Generator can eventually discriminate useful documents amongst LLM fabrications and achieve better performance than strong baselines.

------------

`[2405.18358] MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning <https://arxiv.org/abs/2405.18358>`__ MMCTAgent:面向复杂视觉推理的多模态批判性思维Agent框架

::

    Tue, 28 May 2024 16:55:41 GMT
    Somnath Kumar, Yash Gadhia, Tanuja Ganu and Akshay Nambi

Recent advancements in Multi-modal Large Language Models (MLLMs) have significantly improved their performance in tasks combining vision and language. However, challenges persist in detailed multi-modal understanding, comprehension of complex tasks, and reasoning over multi-modal information.
This paper introduces MMCTAgent, a novel multi-modal critical thinking agent framework designed to address the inherent limitations of current MLLMs in complex visual reasoning tasks. Inspired by human cognitive processes and critical thinking, MMCTAgent iteratively analyzes multi-modal information, decomposes queries, plans strategies, and dynamically evolves its reasoning.
Additionally, MMCTAgent incorporates critical thinking elements such as verification of final answers and self-reflection through a novel approach that defines a vision-based critic and identifies task-specific evaluation criteria, thereby enhancing its decision-making abilities. Through rigorous evaluations across various image and video understanding benchmarks, we demonstrate that MMCTAgent (with and without the critic) outperforms both foundational MLLMs and other tool-augmented pipelines.

------------

`[2405.18369] PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework <https://arxiv.org/abs/2405.18369>`__ PromptWizard:任务感知agent驱动的提示优化框架

::

    Tue, 28 May 2024 17:08:31 GMT
    Eshaan Agarwal, Vivek Dani, Tanuja Ganu and Akshay Nambi

Large language models (LLMs) have revolutionized AI across diverse domains, showcasing remarkable capabilities. Central to their success is the concept of prompting, which guides model output generation. However, manual prompt engineering is labor-intensive and domain-specific, necessitating automated solutions. This paper introduces PromptWizard, a novel framework leveraging LLMs to iteratively synthesize and refine prompts tailored to specific tasks.
Unlike existing approaches, PromptWizard optimizes both prompt instructions and in-context examples, maximizing model performance. The framework iteratively refines prompts by mutating instructions and incorporating negative examples to deepen understanding and ensure diversity. It further enhances both instructions and examples with the aid of a critic, synthesizing new instructions and examples enriched with detailed reasoning steps for optimal performance. PromptWizard offers several key features and capabilities, including computational efficiency compared to state-of-the-art approaches, adaptability to scenarios with varying amounts of training data, and effectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8 datasets demonstrates PromptWizard's superiority over existing prompt strategies, showcasing its efficacy and scalability in prompt optimization.

------------

`[2403.17209] Generation of Asset Administration Shell with Large Language Model Agents: Towards Semantic Interoperability in Digital Twins in the Context of Industry 4.0 <https://arxiv.org/abs/2403.17209>`__ 具有大型语言模型代理的资产管理外壳的生成:工业4.0背景下面向数字孪生的语义互操作性

::

    replaced with revised version Tue, 28 May 2024 00:00:38 GMT
    Submission history From: Yuchen Xia [view email]
    [v1] Mon, 25 Mar 2024 21:37:30 UTC (1,372 KB)
    [v2] Tue, 28 May 2024 00:00:38 UTC (1,420 KB)
    Yuchen Xia, Zhewen Xiao, Nasser Jazdi and Michael Weyrich

This research introduces a novel approach for achieving semantic interoperability in digital twins and assisting the creation of Asset Administration Shell (AAS) as digital twin model within the context of Industry 4.0. The foundational idea of our research is that the communication based on semantics and the generation of meaningful textual data are directly linked, and we posit that these processes are equivalent if the exchanged information can be serialized in text form. Based on this, we construct a "semantic node" data structure in our research to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process the "semantic node" and generate standardized digital twin models from raw textual data collected from datasheets describing technical assets. Our evaluation demonstrates an effective generation rate of 62-79%, indicating a substantial proportion of the information from the source text can be translated error-free to the target digital twin instance model with the generative capability of large language models. This result has a direct application in the context of Industry 4.0, and the designed system is implemented as a data model generation tool for reducing the manual effort in creating AAS model. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM systems for interpreting technical concepts and translating data. Our findings emphasize LLMs' capability to automate AAS instance creation and contribute to the broader field of semantic interoperability for digital twins in industrial applications. The prototype implementation and evaluation results are presented on our GitHub Repository: this https URL.

------------

`[2405.16334] Devil's Advocate: Anticipatory Reflection for LLM Agents <https://arxiv.org/abs/2405.16334>`__ 魔鬼代言人:LLM代理的预期反思

::

    replaced with revised version Tue, 28 May 2024 03:22:44 GMT
    Submission history From: Haoyu Wang [view email]
    [v1] Sat, 25 May 2024 19:20:15 UTC (1,067 KB)
    [v2] Tue, 28 May 2024 03:22:44 UTC (1,083 KB)
    Haoyu Wang and Tao Li and Zhiwei Deng and Dan Roth and Yang Li

In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks. Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions. We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement. By deploying and experimenting with this methodology - a zero-shot approach - within WebArena for practical tasks in web environments, our agent demonstrates superior performance over existing zero-shot methods. The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions needed to achieve a task.

------------

`[2405.16510] Meta-Task Planning for Language Agents <https://arxiv.org/abs/2405.16510>`__ 语言智能体的元任务规划

::

    replaced with revised version Tue, 28 May 2024 13:56:40 GMT
    Submission history From: Cong Zhang [view email]
    [v1] Sun, 26 May 2024 10:33:17 UTC (548 KB)
    [v2] Tue, 28 May 2024 13:56:40 UTC (548 KB)
    Cong Zhang, Derrick Goh Xin Deik, Dexun Li, Hao Zhang, Yong Liu

The rapid advancement of neural language models has sparked a new surge of intelligent agent research. Unlike traditional agents, large language model-based agents (LLM agents) have emerged as a promising paradigm for achieving artificial general intelligence (AGI) due to their superior reasoning and generalization capabilities. Effective planning is crucial for the success of LLM agents in real-world tasks, making it a highly pursued topic in the community. Current planning methods typically translate tasks into executable action sequences. However, determining a feasible or optimal sequence for complex tasks at fine granularity, which often requires compositing long chains of heterogeneous actions, remains challenging. This paper introduces Meta-Task Planning (MTP), a zero-shot methodology for collaborative LLM-based multi-agent systems that simplifies complex task planning by decomposing it into a hierarchy of subordinate tasks, or meta-tasks. Each meta-task is then mapped into executable actions. MTP was assessed on two rigorous benchmarks, TravelPlanner and API-Bank. Notably, MTP achieved an average $\sim40\%$ success rate on TravelPlanner, significantly higher than the state-of-the-art (SOTA) baseline ($2.92\%$), and outperforming $LLM_{api}$-4 with ReAct on API-Bank by $\sim14\%$, showing the immense potential of integrating LLM with multi-agent systems.

------------

`[2405.17009] Position: Foundation Agents as the Paradigm Shift for Decision Making <https://arxiv.org/abs/2405.17009>`__ 职位:作为决策范式转换的基础代理

::

    replaced with revised version Tue, 28 May 2024 13:00:14 GMT
    Submission history From: Xiaoqian Liu [view email]
    [v1] Mon, 27 May 2024 09:54:50 UTC (1,335 KB)
    [v2] Tue, 28 May 2024 13:00:14 UTC (1,332 KB)
    Xiaoqian Liu, Xingzhou Lou, Jianbin Jiao, Junge Zhang

Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies. Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization. In contrast, foundation models in language and vision have showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents. This proposal is underpinned by the formulation of foundation agents with their fundamental characteristics and challenges motivated by the success of large language models (LLMs). Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs. Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future.

------------

`[2312.17259] Empowering Working Memory for Large Language Model Agents <https://arxiv.org/abs/2312.17259>`__ 大型语言模型智能体工作记忆赋能

::

    replaced with revised version Tue, 28 May 2024 05:34:52 GMT
    Submission history From: Nan Li [view email]
    [v1] Fri, 22 Dec 2023 05:59:00 UTC (361 KB)
    [v2] Tue, 28 May 2024 05:34:52 UTC (413 KB)
    Jing Guo, Nan Li, Jianchuan Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, Ming Xu

Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning. This paper explores the potential of applying cognitive psychology's working memory frameworks, to enhance LLM architecture. The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links. To address this, an innovative model is proposed incorporating a centralized Working Memory Hub and Episodic Buffer access to retain memories across episodes. This architecture aims to provide greater continuity for nuanced contextual reasoning during intricate tasks and collaborative scenarios. While promising, further research is required into optimizing episodic memory encoding, storage, prioritization, retrieval, and security. Overall, this paper provides a strategic blueprint for developing LLM agents with more sophisticated, human-like memory capabilities, highlighting memory mechanisms as a vital frontier in artificial general intelligence.

------------

`[2405.16376] STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making <https://arxiv.org/abs/2405.16376>`__ STRIDE:一个工具辅助的战略交互式决策LLM Agent框架

::

    replaced with revised version Tue, 28 May 2024 01:21:19 GMT
    Submission history From: Chuanhao Li [view email]
    [v1] Sat, 25 May 2024 23:25:10 UTC (679 KB)
    [v2] Tue, 28 May 2024 01:21:19 UTC (679 KB)
    Chuanhao Li, Runhan Yang, Tiankai Li, Milad Bafarassat, Kourosh Sharifi, Dirk Bergemann, and Zhuoran Yang

Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities. However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information. These deficiencies hinder their performance in strategic and interactive tasks that demand adherence to nuanced game rules, long-term planning, exploration in unknown environments, and anticipation of opponents' moves. To overcome these obstacles, this paper presents a novel LLM agent framework equipped with memory and specialized tools to enhance their strategic decision-making capabilities. We deploy the tools in a number of economically important environments, in particular bilateral bargaining and multi-agent and dynamic mechanism design. We employ quantitative metrics to assess the framework's performance in various strategic decision-making problems. Our findings establish that our enhanced framework significantly improves the strategic decision-making capability of LLMs. While we highlight the inherent limitations of current LLM models, we demonstrate the improvements through targeted enhancements, suggesting a promising direction for future developments in LLM applications for interactive environments.

------------

`[2402.17453] DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning <https://arxiv.org/abs/2402.17453>`__ DS-Agent:通过赋予大型语言模型基于案例的推理实现自动化数据科学

::

    replaced with revised version Tue, 28 May 2024 06:50:38 GMT
    Submission history From: Siyuan Guo [view email]
    [v1] Tue, 27 Feb 2024 12:26:07 UTC (370 KB)
    [v2] Wed, 13 Mar 2024 12:02:25 UTC (370 KB)
    [v3] Sat, 6 Apr 2024 12:28:57 UTC (370 KB)
    [v4] Fri, 24 May 2024 12:40:48 UTC (366 KB)
    [v5] Tue, 28 May 2024 06:50:38 UTC (367 KB)
    Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang

In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100\% success rate in the development stage, while attaining 36\% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \$1.60 and \$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at this https URL.

------------

-----------
Other (115)
-----------

`[2405.17533] PAE: LLM-based Product Attribute Extraction for E-Commerce Fashion Trends <https://arxiv.org/abs/2405.17533>`__ PAE:基于llm的电子商务时尚趋势产品属性抽取

::

    Mon, 27 May 2024 17:50:25 GMT
    Apurva Sinha and Ekta Gujral

Product attribute extraction is an growing field in e-commerce business, with several applications including product ranking, product recommendation, future assortment planning and improving online shopping customer experiences.
Understanding the customer needs is critical part of online business, specifically fashion products. Retailers uses assortment planning to determine the mix of products to offer in each store and channel, stay responsive to market dynamics and to manage inventory and catalogs. The goal is to offer the right styles, in the right sizes and colors, through the right channels. When shoppers find products that meet their needs and desires, they are more likely to return for future purchases, fostering customer loyalty. Product attributes are a key factor in assortment planning. In this paper we present PAE, a product attribute extraction algorithm for future trend reports consisting text and images in PDF format. Most existing methods focus on attribute extraction from titles or product descriptions or utilize visual information from existing product images. Compared to the prior works, our work focuses on attribute extraction from PDF files where upcoming fashion trends are explained. This work proposes a more comprehensive framework that fully utilizes the different modalities for attribute extraction and help retailers to plan the assortment in advance. Our contributions are three-fold: (a) We develop PAE, an efficient framework to extract attributes from unstructured data (text and images); (b) We provide catalog matching methodology based on BERT representations to discover the existing attributes using upcoming attribute values; (c) We conduct extensive experiments with several baselines and show that PAE is an effective, flexible and on par or superior (avg 92.5% F1-Score) framework to existing state-of-the-art for attribute value extraction task.

------------

`[2405.17637] The Economic Implications of Large Language Model Selection on Earnings and Return on Investment: A Decision Theoretic Model <https://arxiv.org/abs/2405.17637>`__ 大型语言模型选择对收益和投资回报率的经济影响:一个决策理论模型

::

    Mon, 27 May 2024 20:08:41 GMT
    Geraldo Xex\'eo, Filipe Braida, Marcus Parreiras and Paulo Xavier

Selecting language models in business contexts requires a careful analysis of the final financial benefits of the investment. However, the emphasis of academia and industry analysis of LLM is solely on performance. This work introduces a framework to evaluate LLMs, focusing on the earnings and return on investment aspects that should be taken into account in business decision making. We use a decision-theoretic approach to compare the financial impact of different LLMs, considering variables such as the cost per token, the probability of success in the specific task, and the gain and losses associated with LLMs use. The study reveals how the superior accuracy of more expensive models can, under certain conditions, justify a greater investment through more significant earnings but not necessarily a larger RoI. This article provides a framework for companies looking to optimize their technology choices, ensuring that investment in cutting-edge technology aligns with strategic financial objectives. In addition, we discuss how changes in operational variables influence the economics of using LLMs, offering practical insights for enterprise settings, finding that the predicted gain and loss and the different probabilities of success and failure are the variables that most impact the sensitivity of the models.

------------

`[2405.17741] LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design <https://arxiv.org/abs/2405.17741>`__ LoRA-Switch:通过系统算法协同提高动态LLM适配器的效率

::

    Tue, 28 May 2024 01:53:26 GMT
    Rui Kong, Qiyang Li, Xinyu Fang, Qingtian Feng, Qingfeng He, Yazhu Dong, Weijun Wang, Yuanchun Li, Linghe Kong, Yunxin Liu

Recent literature has found that an effective method to customize or further improve large language models (LLMs) is to add dynamic adapters, such as low-rank adapters (LoRA) with Mixture-of-Experts (MoE) structures. Though such dynamic adapters incur modest computational complexity, they surprisingly lead to huge inference latency overhead, slowing down the decoding speed by 2.5+ times. In this paper, we analyze the fine-grained costs of the dynamic adapters and find that the fragmented CUDA kernel calls are the root cause. Therefore, we propose LoRA-Switch, a system-algorithm co-designed architecture for efficient dynamic adapters. Unlike most existing dynamic structures that adopt layer-wise or block-wise dynamic routing, LoRA-Switch introduces a token-wise routing mechanism. It switches the LoRA adapters and weights for each token and merges them into the backbone for inference. For efficiency, this switching is implemented with an optimized CUDA kernel, which fuses the merging operations for all LoRA adapters at once. Based on experiments with popular open-source LLMs on common benchmarks, our approach has demonstrated similar accuracy improvement as existing dynamic adapters, while reducing the decoding latency by more than 2.4 times.

------------

`[2405.17888] Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment <https://arxiv.org/abs/2405.17888>`__ 从SFT数据中获得更多的能量:来自人类演示的奖励学习改进了SFT对LLM的对齐

::

    Tue, 28 May 2024 07:11:05 GMT
    Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garcia, Mingyi Hong

Aligning human preference and value is an important requirement for contemporary foundation models. State-of-the-art techniques such as Reinforcement Learning from Human Feedback (RLHF) often consist of two stages: 1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from human demonstration data; 2) Preference learning, where preference data is used to learn a reward model, which is in turn used by a reinforcement learning (RL) step to fine-tune the model. Such reward model serves as a proxy to human preference, and it is critical to guide the RL step towards improving the model quality. In this work, we argue that the SFT stage significantly benefits from learning a reward model as well. Instead of using the human demonstration data directly via supervised learning, we propose to leverage an Inverse Reinforcement Learning (IRL) technique to (explicitly or implicitly) build an reward model, while learning the policy model. This approach leads to new SFT algorithms that are not only efficient to implement, but also promote the ability to distinguish between the preferred and non-preferred continuations.
Moreover, we identify a connection between the proposed IRL based approach, and certain self-play approach proposed recently, and showed that self-play is a special case of modeling a reward-learning agent. Theoretically, we show that the proposed algorithms converge to the stationary solutions of the IRL problem. Empirically, we align 1B and 7B models using proposed methods and evaluate them on a reward benchmark model and the HuggingFace Open LLM Leaderboard. The proposed methods show significant performance improvement over existing SFT approaches. Our results indicate that it is beneficial to explicitly or implicitly leverage reward learning throughout the entire alignment process.

------------

`[2405.17902] Boosting Protein Language Models with Negative Sample Mining <https://arxiv.org/abs/2405.17902>`__ 基于负样本挖掘的蛋白质语言模型增强

::

    Tue, 28 May 2024 07:24:20 GMT
    Yaoyao Xu, Xinjian Zhao, Xiaozhuang Song, Benyou Wang, Tianshu Yu

We introduce a pioneering methodology for boosting large language models in the domain of protein representation learning. Our primary contribution lies in the refinement process for correlating the over-reliance on co-evolution knowledge, in a way that networks are trained to distill invaluable insights from negative samples, constituted by protein pairs sourced from disparate categories. By capitalizing on this novel approach, our technique steers the training of transformer-based models within the attention score space. This advanced strategy not only amplifies performance but also reflects the nuanced biological behaviors exhibited by proteins, offering aligned evidence with traditional biological mechanisms such as protein-protein interaction. We experimentally observed improved performance on various tasks over datasets, on top of several well-established large protein models. This innovative paradigm opens up promising horizons for further progress in the realms of protein research and computational biology.

------------

`[2405.17950] Self-Guiding Exploration for Combinatorial Problems <https://arxiv.org/abs/2405.17950>`__ 组合问题的自我引导探索

::

    Tue, 28 May 2024 08:26:54 GMT
    Zangir Iklassov and Yali Du and Farkhad Akimov and Martin Takac

Large Language Models (LLMs) have become pivotal in addressing reasoning tasks across diverse domains, including arithmetic, commonsense, and symbolic reasoning. They utilize prompting techniques such as Exploration-of-Thought, Decomposition, and Refinement to effectively navigate and solve intricate tasks. Despite these advancements, the application of LLMs to Combinatorial Problems (CPs), known for their NP-hardness and critical roles in logistics and resource management remains underexplored. To address this gap, we introduce a novel prompting strategy: Self-Guiding Exploration (SGE), designed to enhance the performance of solving CPs. SGE operates autonomously, generating multiple thought trajectories for each CP task. It then breaks these trajectories down into actionable subtasks, executes them sequentially, and refines the results to ensure optimal outcomes. We present our research as the first to apply LLMs to a broad range of CPs and demonstrate that SGE outperforms existing prompting strategies by over 27.84% in CP optimization performance. Additionally, SGE achieves a 2.46% higher accuracy over the best existing results in other reasoning tasks (arithmetic, commonsense, and symbolic).

------------

`[2405.17956] Hybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives <https://arxiv.org/abs/2405.17956>`__ 混合偏好优化:用辅助目标扩充直接偏好优化

::

    Tue, 28 May 2024 08:35:48 GMT
    Anirudhan Badrinath, Prabhat Agarwal, Jiajing Xu

For aligning large language models (LLMs), prior work has leveraged reinforcement learning via human feedback (RLHF) or variations of direct preference optimization (DPO). While DPO offers a simpler framework based on maximum likelihood estimation, it compromises on the ability to tune language models to easily maximize non-differentiable and non-binary objectives according to the LLM designer's preferences (e.g., using simpler language or minimizing specific kinds of harmful content). These may neither align with user preferences nor even be able to be captured tractably by binary preference data. To leverage the simplicity and performance of DPO with the generalizability of RL, we propose a hybrid approach between DPO and RLHF. With a simple augmentation to the implicit reward decomposition of DPO, we allow for tuning LLMs to maximize a set of arbitrary auxiliary rewards using offline RL.
The proposed method, Hybrid Preference Optimization (HPO), shows the ability to effectively generalize to both user preferences and auxiliary designer objectives, while preserving alignment performance across a range of challenging benchmarks and model sizes.

------------

`[2405.18064] Automated Real-World Sustainability Data Generation from Images of Buildings <https://arxiv.org/abs/2405.18064>`__ 从建筑物图像自动生成真实世界的可持续发展数据

::

    Tue, 28 May 2024 11:24:20 GMT
    Peter J Bentley, Soo Ling Lim, Rajat Mathur, Sid Narang

When data on building features is unavailable, the task of determining how to improve that building in terms of carbon emissions becomes infeasible. We show that from only a set of images, a Large Language Model with appropriate prompt engineering and domain knowledge can successfully estimate a range of building features relevant for sustainability calculations. We compare our novel image-to-data method with a ground truth comprising real building data for 47 apartments and achieve accuracy better than a human performing the same task.
We also demonstrate that the method can generate tailored recommendations to the owner on how best to improve their properties and discuss methods to scale the approach.

------------

`[2405.18166] Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing <https://arxiv.org/abs/2405.18166>`__ 通过特定层编辑保护大型语言模型免受越狱攻击

::

    Tue, 28 May 2024 13:26:12 GMT
    Wei Zhao and Zhe Li and Yige Li and Ye Zhang and Jun Sun

Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs response to harmful prompts and propose a novel defense method termed \textbf{L}ayer-specific \textbf{Ed}iting (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical \textit{safety layers} exist among the early layers of LLMs.
We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from selected target layers can significantly improve the alignment of LLMs against jailbreak attacks.
Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. Our code is available at \url{https://github.com/ledllm/ledllm}.

------------

`[2405.18248] Extreme Value Monte Carlo Tree Search <https://arxiv.org/abs/2405.18248>`__ 极值蒙特卡洛树搜索

::

    Tue, 28 May 2024 14:58:43 GMT
    Masataro Asai, Stephen Wissow

Despite being successful in board games and reinforcement learning (RL), UCT, a Monte-Carlo Tree Search (MCTS) combined with UCB1 Multi-Armed Bandit (MAB), has had limited success in domain-independent planning until recently. Previous work showed that UCB1, designed for $[0,1]$-bounded rewards, is not appropriate for estimating the distance-to-go which are potentially unbounded in $\mathbb{R}$, such as heuristic functions used in classical planning, then proposed combining MCTS with MABs designed for Gaussian reward distributions and successfully improved the performance. In this paper, we further sharpen our understanding of ideal bandits for planning tasks. Existing work has two issues: First, while Gaussian MABs no longer over-specify the distances as $h\in [0,1]$, they under-specify them as $h\in [-\infty,\infty]$ while they are non-negative and can be further bounded in some cases. Second, there is no theoretical justifications for Full-Bellman backup (Schulte & Keller, 2014) that backpropagates minimum/maximum of samples. We identified \emph{extreme value} statistics as a theoretical framework that resolves both issues at once and propose two bandits, UCB1-Uniform/Power, and apply them to MCTS for classical planning. We formally prove their regret bounds and empirically demonstrate their performance in classical planning.

------------

`[2405.18272] Metaheuristics and Large Language Models Join Forces: Towards an Integrated Optimization Approach <https://arxiv.org/abs/2405.18272>`__ 元启发式和大型语言模型联合:走向一种集成优化方法

::

    Tue, 28 May 2024 15:23:46 GMT
    Camilo Chac\'on Sartori, Christian Blum, Filippo Bistaffa, Guillem Rodr\'iguez Corominas

Since the rise of Large Language Models (LLMs) a couple of years ago, researchers in metaheuristics (MHs) have wondered how to use their power in a beneficial way within their algorithms. This paper introduces a novel approach that leverages LLMs as pattern recognition tools to improve MHs. The resulting hybrid method, tested in the context of a social network-based combinatorial optimization problem, outperforms existing state-of-the-art approaches that combine machine learning with MHs regarding the obtained solution quality. By carefully designing prompts, we demonstrate that the output obtained from LLMs can be used as problem knowledge, leading to improved results. Lastly, we acknowledge LLMs' potential drawbacks and limitations and consider it essential to examine them to advance this type of research further.

------------

`[2405.18346] Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation <https://arxiv.org/abs/2405.18346>`__ 智能临床文档:利用生成式AI实现以患者为中心的临床记录生成

::

    Tue, 28 May 2024 16:43:41 GMT
    Anjanava Biswas, Wrick Talukdar

Comprehensive clinical documentation is crucial for effective healthcare delivery, yet it poses a significant burden on healthcare professionals, leading to burnout, increased medical errors, and compromised patient safety.
This paper explores the potential of generative AI (Artificial Intelligence) to streamline the clinical documentation process, specifically focusing on generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan) notes. We present a case study demonstrating the application of natural language processing (NLP) and automatic speech recognition (ASR) technologies to transcribe patient-clinician interactions, coupled with advanced prompting techniques to generate draft clinical notes using large language models (LLMs). The study highlights the benefits of this approach, including time savings, improved documentation quality, and enhanced patient-centered care. Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings.
The findings suggest that generative AI has the potential to revolutionize clinical documentation practices, alleviating administrative burdens and enabling healthcare professionals to focus more on direct patient care.

------------

`[2405.17633] HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs <https://arxiv.org/abs/2405.17633>`__ 发自内心的叙事:与llm在个人故事中追踪同理心和叙事风格

::

    Mon, 27 May 2024 20:00:38 GMT
    Jocelyn Shen, Joel Mire, Hae Won Park, Cynthia Breazeal, Maarten Sap

Empathy serves as a cornerstone in enabling prosocial behaviors, and can be evoked through sharing of personal experiences in stories. While empathy is influenced by narrative content, intuitively, people respond to the way a story is told as well, through narrative style. Yet the relationship between empathy and narrative style is not fully understood. In this work, we empirically examine and quantify this relationship between style and empathy using LLMs and large-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy, HEART (Human Empathy and Narrative Taxonomy) that delineates elements of narrative style that can lead to empathy with the narrator of a story. We establish the performance of LLMs in extracting narrative elements from HEART, showing that prompting with our taxonomy leads to reasonable, human-level annotations beyond what prior lexicon-based methods can do. To show empirical use of our taxonomy, we collect a dataset of empathy judgments of stories via a large-scale crowdsourcing study with N=2,624 participants. We show that narrative elements extracted via LLMs, in particular, vividness of emotions and plot volume, can elucidate the pathways by which narrative style cultivates empathy towards personal stories. Our work suggests that such models can be used for narrative analyses that lead to human-centered social and behavioral insights.

------------

`[2405.17712] CLAIM Your Data: Enhancing Imputation Accuracy with Contextual Large Language Models <https://arxiv.org/abs/2405.17712>`__ 声明数据:利用上下文大型语言模型提高填补精度

::

    Tue, 28 May 2024 00:08:29 GMT
    Ahatsham Hayat and Mohammad Rashedul Hasan

This paper introduces the Contextual Language model for Accurate Imputation Method (CLAIM), a novel strategy that capitalizes on the expansive knowledge and reasoning capabilities of pre-trained large language models (LLMs) to address missing data challenges in tabular datasets. Unlike traditional imputation methods, which predominantly rely on numerical estimations, CLAIM utilizes contextually relevant natural language descriptors to fill missing values. This approach transforms datasets into natural language contextualized formats that are inherently more aligned with LLMs' capabilities, thereby facilitating the dual use of LLMs: first, to generate missing value descriptors, and then, to fine-tune the LLM on the enriched dataset for improved performance in downstream tasks. Our evaluations across diverse datasets and missingness patterns reveal CLAIM's superior performance over existing imputation techniques. Furthermore, our investigation into the effectiveness of context-specific versus generic descriptors for missing data highlights the importance of contextual accuracy in enhancing LLM performance for data imputation. The results underscore CLAIM's potential to markedly improve the reliability and quality of data analysis and machine learning models, offering a more nuanced and effective solution for handling missing data.

------------

`[2405.17740] MobileConvRec: A Conversational Dataset for Mobile Apps Recommendations <https://arxiv.org/abs/2405.17740>`__ MobileConvRec:移动应用推荐对话数据集

::

    Tue, 28 May 2024 01:53:16 GMT
    Srijata Maji and Moghis Fereidouni and Vinaik Chhetri and Umar Farooq and A.B. Siddique

Existing recommendation systems have focused on two paradigms: 1- historical user-item interaction-based recommendations and 2- conversational recommendations. Conversational recommendation systems facilitate natural language dialogues between users and the system, allowing the system to solicit users' explicit needs while enabling users to inquire about recommendations and provide feedback. Due to substantial advancements in natural language processing, conversational recommendation systems have gained prominence.
Existing conversational recommendation datasets have greatly facilitated research in their respective domains. Despite the exponential growth in mobile users and apps in recent years, research in conversational mobile app recommender systems has faced substantial constraints. This limitation can primarily be attributed to the lack of high-quality benchmark datasets specifically tailored for mobile apps. To facilitate research for conversational mobile app recommendations, we introduce MobileConvRec.
MobileConvRec simulates conversations by leveraging real user interactions with mobile apps on the Google Play store, originally captured in large-scale mobile app recommendation dataset MobileRec. The proposed conversational recommendation dataset synergizes sequential user-item interactions, which reflect implicit user preferences, with comprehensive multi-turn conversations to effectively grasp explicit user needs. MobileConvRec consists of over 12K multi-turn recommendation-related conversations spanning 45 app categories.
Moreover, MobileConvRec presents rich metadata for each app such as permissions data, security and privacy-related information, and binary executables of apps, among others. We demonstrate that MobileConvRec can serve as an excellent testbed for conversational mobile app recommendation through a comparative study of several pre-trained large language models.

------------

`[2405.17743] ORLM: Training Large Language Models for Optimization Modeling <https://arxiv.org/abs/2405.17743>`__ ORLM:面向优化建模的大型语言模型训练

::

    Tue, 28 May 2024 01:55:35 GMT
    Zhengyang Tang, Chenyu Huang, Xin Zheng, Shixi Hu, Zizhuo Wang, Dongdong Ge, Benyou Wang

Large Language Models (LLMs) have emerged as powerful tools for complex Operations Research (OR) in automating optimization modeling. However, current methodologies heavily rely on prompt engineering (e.g., multi-agent cooperation) with proprietary LLMs, raising data privacy concerns that could be prohibitive in industry applications. To tackle this issue, we propose training open-source LLMs for optimization modeling. We identify four critical requirements for the training dataset of OR LLMs, design and implement OR-Instruct, a semi-automated process for creating synthetic data tailored to specific requirements. We also introduce the IndustryOR benchmark, the first industrial benchmark for testing LLMs on solving real-world OR problems. We apply the data from OR-Instruct to various open-source LLMs of 7b size (termed as ORLMs), resulting in a significantly improved capability for optimization modeling. Our best-performing ORLM achieves state-of-the-art performance on the NL4OPT, MAMO, and IndustryOR benchmarks. Our code and data will be available at \url{https://github.com/Cardinal-Operations/ORLM}.

------------

`[2405.17755] XL3M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference <https://arxiv.org/abs/2405.17755>`__ XL3M:基于分段推理的免训练LLM长度扩展框架

::

    Tue, 28 May 2024 02:12:35 GMT
    Shengnan Wang, Youhui Bai, Lin Zhang, Pingyi Zhou, Shixiong Zhao, Gong Zhang, Sen Wang, Renhai Chen, Hua Xu, Hongwei Sun

Length generalization failure problem, namely the large language model (LLM) fails to generalize to texts longer than its maximum training length, greatly restricts the application of LLM in the scenarios with streaming long inputs.
To address this problem, the existing methods either require substantial costs or introduce precision loss. In this paper, we empirically find that the accuracy of the LLM's prediction is highly correlated to its certainty. Based on this, we propose an efficient training free framework, named XL3M (it means extra-long large language model), which enables the LLMs trained on short sequences to reason extremely long sequence without any further training or fine-tuning. Under the XL3M framework, the input context will be firstly decomposed into multiple short sub-contexts, where each sub-context contains an independent segment and a common ``question'' which is a few tokens from the end of the original context. Then XL3M gives a method to measure the relevance between each segment and the ``question'', and constructs a concise key context by splicing all the relevant segments in chronological order. The key context is further used instead of the original context to complete the inference task.
Evaluations on comprehensive benchmarks show the superiority of XL3M. Using our framework, a Llama2-7B model is able to reason 20M long sequences on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card.

------------

`[2405.17804] Detection-Correction Structure via General Language Model for Grammatical Error Correction <https://arxiv.org/abs/2405.17804>`__ 基于通用语言模型的语法纠错检测-纠错结构

::

    Tue, 28 May 2024 04:04:40 GMT
    Wei Li, Houfeng Wang

Grammatical error correction (GEC) is a task dedicated to rectifying texts with minimal edits, which can be decoupled into two components: detection and correction. However, previous works have predominantly focused on direct correction, with no prior efforts to integrate both into a single model.
Moreover, the exploration of the detection-correction paradigm by large language models (LLMs) remains underdeveloped. This paper introduces an integrated detection-correction structure, named DeCoGLM, based on the General Language Model (GLM). The detection phase employs a fault-tolerant detection template, while the correction phase leverages autoregressive mask infilling for localized error correction. Through the strategic organization of input tokens and modification of attention masks, we facilitate multi-task learning within a single model. Our model demonstrates competitive performance against the state-of-the-art models on English and Chinese GEC datasets. Further experiments present the effectiveness of the detection-correction structure in LLMs, suggesting a promising direction for GEC.

------------

`[2405.17822] Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action <https://arxiv.org/abs/2405.17822>`__ Conv-CoA:基于对话行动链改进大型语言模型的开放域问答

::

    Tue, 28 May 2024 04:46:52 GMT
    Zhenyu Pan, Haozheng Luo, Manling Li, Han Liu

We present a Conversational Chain-of-Action (Conv-CoA) framework for Open-domain Conversational Question Answering (OCQA). Compared with literature, Conv-CoA addresses three major challenges: (i) unfaithful hallucination that is inconsistent with real-time or domain facts, (ii) weak reasoning performance in conversational scenarios, and (iii) unsatisfying performance in conversational information retrieval. Our key contribution is a dynamic reasoning-retrieval mechanism that extracts the intent of the question and decomposes it into a reasoning chain to be solved via systematic prompting, pre-designed actions, updating the Contextual Knowledge Set (CKS), and a novel Hopfield-based retriever. Methodologically, we propose a resource-efficiency Hopfield retriever to enhance the efficiency and accuracy of conversational information retrieval within our actions. Additionally, we propose a conversational-multi-reference faith score (Conv-MRFS) to verify and resolve conflicts between retrieved knowledge and answers in conversations.
Empirically, we conduct comparisons between our framework and 23 state-of-the-art methods across five different research directions and two public benchmarks. These comparisons demonstrate that our Conv-CoA outperforms other methods in both the accuracy and efficiency dimensions.

------------

`[2405.17830] More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs <https://arxiv.org/abs/2405.17830>`__ 不仅仅是灾难性遗忘:为特定领域的llm集成通用能力

::

    Tue, 28 May 2024 05:00:12 GMT
    Chengyuan Liu, Shihang Wang, Yangyang Kang, Lizhi Qing, Fubang Zhao, Changlong Sun, Kun Kuang, Fei Wu

The performance on general tasks decreases after Large Language Models (LLMs) are fine-tuned on domain-specific tasks, the phenomenon is known as Catastrophic Forgetting (CF). However, this paper presents a further challenge for real application of domain-specific LLMs beyond CF, called General Capabilities Integration (GCI), which necessitates the integration of both the general capabilities and domain knowledge within a single instance. The objective of GCI is not merely to retain previously acquired general capabilities alongside new domain knowledge, but to harmonize and utilize both sets of skills in a cohesive manner to enhance performance on domain-specific tasks. Taking legal domain as an example, we carefully design three groups of training and testing tasks without lacking practicability, and construct the corresponding datasets. To better incorporate general capabilities across domain-specific scenarios, we introduce ALoRA, which utilizes a multi-head attention module upon LoRA, facilitating direct information transfer from preceding tokens to the current one. This enhancement permits the representation to dynamically switch between domain-specific knowledge and general competencies according to the attention. Extensive experiments are conducted on the proposed tasks. The results exhibit the significance of our setting, and the effectiveness of our method.

------------

`[2405.17915] Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models <https://arxiv.org/abs/2405.17915>`__ 长上下文根本不长:大型语言模型长依赖数据的勘探者

::

    Tue, 28 May 2024 07:36:56 GMT
    Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li, Run Luo, Min Yang

Long-context modeling capabilities are important for large language models (LLMs) in various applications. However, directly training LLMs with long context windows is insufficient to enhance this capability since some training samples do not exhibit strong semantic dependencies across long contexts. In this study, we propose a data mining framework \textbf{ProLong} that can assign each training sample with a long dependency score, which can be used to rank and filter samples that are more advantageous for enhancing long-context modeling abilities in LLM training. Specifically, we first use delta perplexity scores to measure the \textit{Dependency Strength} between text segments in a given document. Then we refine this metric based on the \textit{Dependency Distance} of these segments to incorporate spatial relationships across long-contexts. Final results are calibrated with a \textit{Dependency Specificity} metric to prevent trivial dependencies introduced by repetitive patterns. Moreover, a random sampling approach is proposed to optimize the computational efficiency of ProLong. Comprehensive experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities.

------------

`[2405.17931] Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment <https://arxiv.org/abs/2405.17931>`__ 在线合并优化器，以提高奖励和减轻税收

::

    Tue, 28 May 2024 07:53:40 GMT
    Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, Chang Zhou

Effectively aligning Large Language Models (LLMs) with human-centric values while preventing the degradation of abilities acquired through Pre-training and Supervised Fine-tuning (SFT) poses a central challenge in Reinforcement Learning from Human Feedback (RLHF). In this paper, we first discover that interpolating RLHF and SFT model parameters can adjust the trade-off between human preference and basic capabilities, thereby reducing the alignment tax at the cost of alignment reward. Inspired by this, we propose integrating the RL policy and SFT models at each optimization step in RLHF to continuously regulate the training direction, introducing the Online Merging Optimizer.
Specifically, we merge gradients with the parameter differences between SFT and pretrained models, effectively steering the gradient towards maximizing rewards in the direction of SFT optimization. We demonstrate that our optimizer works well with different LLM families, such as Qwen and LLaMA, across various model sizes ranging from 1.8B to 8B, various RLHF algorithms like DPO and KTO, and existing model merging methods. It significantly enhances alignment reward while mitigating alignment tax, achieving higher overall performance across 14 benchmarks.

------------

`[2405.17969] Knowledge Circuits in Pretrained Transformers <https://arxiv.org/abs/2405.17969>`__ 预训练transformer中的知识电路

::

    Tue, 28 May 2024 08:56:33 GMT
    Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen

The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning. The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers. To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head. In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge. The experiments, conducted with GPT2 and TinyLLAMA, has allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model. Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies. Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning. We believe the knowledge circuit holds potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing. Code and data are available in https://github.com/zjunlp/KnowledgeCircuits.

------------

`[2405.17974] Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations <https://arxiv.org/abs/2405.17974>`__ 个性化对话生成的最新趋势:数据集、方法和评估综述

::

    Tue, 28 May 2024 09:04:13 GMT
    Yi-Pei Chen, Noriki Nishida, Hideki Nakayama, Yuji Matsumoto

Enhancing user engagement through personalization in conversational agents has gained significance, especially with the advent of large language models that generate fluent responses. Personalized dialogue generation, however, is multifaceted and varies in its definition -- ranging from instilling a persona in the agent to capturing users' explicit and implicit cues. This paper seeks to systemically survey the recent landscape of personalized dialogue generation, including the datasets employed, methodologies developed, and evaluation metrics applied. Covering 22 datasets, we highlight benchmark datasets and newer ones enriched with additional features. We further analyze 17 seminal works from top conferences between 2021-2023 and identify five distinct types of problems. We also shed light on recent progress by LLMs in personalized dialogue generation. Our evaluation section offers a comprehensive summary of assessment facets and metrics utilized in these works. In conclusion, we discuss prevailing challenges and envision prospect directions for future research in personalized dialogue generation.

------------

`[2405.17977] Aligning to Thousands of Preferences via System Message Generalization <https://arxiv.org/abs/2405.17977>`__ 通过系统消息泛化与数千个首选项对齐

::

    Tue, 28 May 2024 09:06:18 GMT
    Seongyun Lee and Sue Hyun Park and Seungone Kim and Minjoon Seo

Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public's preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual's preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM's generation behavior to better align with the user's intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g., "You are a helpful assistant") which limits their ability to generalize to diverse, unseen system messages. To improve this generalization, we create the Multifaceted Collection, a preference dataset with 192k combinations of values beyond generic helpfulness and harmlessness, spanning 65k user instructions.
Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct) by adding various unseen system messages that reflect user preferences. Janus achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0% margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public's preference as well. Our code, dataset, benchmark, and models are available at https://github.com/kaistAI/Janus.

------------

`[2405.17980] Peering into the Mind of Language Models: An Approach for Attribution in Contextual Question Answering <https://arxiv.org/abs/2405.17980>`__ 窥探语言模型的思维:一种上下文问答归因方法

::

    Tue, 28 May 2024 09:12:44 GMT
    Anirudh Phukan, Shwetha Somasundaram, Apoorv Saxena, Koustava Goswami, Balaji Vasan Srinivasan

With the enhancement in the field of generative artificial intelligence (AI), contextual question answering has become extremely relevant. Attributing model generations to the input source document is essential to ensure trustworthiness and reliability. We observe that when large language models (LLMs) are used for contextual question answering, the output answer often consists of text copied verbatim from the input prompt which is linked together with "glue text" generated by the LLM. Motivated by this, we propose that LLMs have an inherent awareness from where the text was copied, likely captured in the hidden states of the LLM. We introduce a novel method for attribution in contextual question answering, leveraging the hidden state representations of LLMs. Our approach bypasses the need for extensive model retraining and retrieval model overhead, offering granular attributions and preserving the quality of generated answers.
Our experimental results demonstrate that our method performs on par or better than GPT-4 at identifying verbatim copied segments in LLM generations and in attributing these segments to their source. Importantly, our method shows robust performance across various LLM architectures, highlighting its broad applicability. Additionally, we present Verifiability-granular, an attribution dataset which has token level annotations for LLM generations in the contextual question answering setup.

------------

`[2405.17992] fMRI predictors based on language models of increasing complexity recover brain left lateralization <https://arxiv.org/abs/2405.17992>`__ 基于日益复杂的语言模型的fMRI预测器可恢复左脑偏侧化

::

    Tue, 28 May 2024 09:24:52 GMT
    Laurent Bonnasse-Gahot and Christophe Pallier

Over the past decade, studies of naturalistic language processing where participants are scanned while listening to continuous text have flourished.
Using word embeddings at first, then large language models, researchers have created encoding models to analyze the brain signals. Presenting these models with the same text as the participants allows to identify brain areas where there is a significant correlation between the functional magnetic resonance imaging (fMRI) time series and the ones predicted by the models' artificial neurons. One intriguing finding from these studies is that they have revealed highly symmetric bilateral activation patterns, somewhat at odds with the well-known left lateralization of language processing. Here, we report analyses of an fMRI dataset where we manipulate the complexity of large language models, testing 28 pretrained models from 8 different families, ranging from 124M to 14.2B parameters. First, we observe that the performance of models in predicting brain responses follows a scaling law, where the fit with brain activity increases linearly with the logarithm of the number of parameters of the model (and its performance on natural language processing tasks). Second, we show that a left-right asymmetry gradually appears as model size increases, and that the difference in left-right brain correlations also follows a scaling law. Whereas the smallest models show no asymmetry, larger models fit better and better left hemispheric activations than right hemispheric ones. This finding reconciles computational analyses of brain activity using large language models with the classic observation from aphasic patients showing left hemisphere dominance for language.

------------

`[2405.18009] Exploring Context Window of Large Language Models via Decomposed Positional Vectors <https://arxiv.org/abs/2405.18009>`__ 基于分解位置向量的大型语言模型上下文窗口探索

::

    Tue, 28 May 2024 09:50:46 GMT
    Zican Dong, Junyi Li, Xin Men, Wayne Xin Zhao, Bingbing Wang, Zhen Tian, Weipeng Chen, Ji-Rong Wen

Transformer-based large language models (LLMs) typically have a limited context window, resulting in significant performance degradation when processing text beyond the length of the context window. Extensive studies have been proposed to extend the context window and achieve length extrapolation of LLMs, but there is still a lack of in-depth interpretation of these approaches.
In this study, we explore the positional information within and beyond the context window for deciphering the underlying mechanism of LLMs. By using a mean-based decomposition method, we disentangle positional vectors from hidden states of LLMs and analyze their formation and effect on attention.
Furthermore, when texts exceed the context window, we analyze the change of positional vectors in two settings, i.e., direct extrapolation and context window extension. Based on our findings, we design two training-free context window extension methods, positional vector replacement and attention window extension. Experimental results show that our methods can effectively extend the context window length.

------------

`[2405.18027] TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models <https://arxiv.org/abs/2405.18027>`__ TimeChara:角色扮演大型语言模型的时点字符幻觉评估

::

    Tue, 28 May 2024 10:19:18 GMT
    Jaewoo Ahn, Taehyun Lee, Junyoung Lim, Jin-Hwa Kim, Sangdoo Yun, Hwaran Lee, Gunhee Kim

While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.

------------

`[2405.18028] Edinburgh Clinical NLP at MEDIQA-CORR 2024: Guiding Large Language Models with Hints <https://arxiv.org/abs/2405.18028>`__ MEDIQA-CORR 2024爱丁堡临床NLP:用提示指导大型语言模型

::

    Tue, 28 May 2024 10:20:29 GMT
    Aryo Pradipta Gema, Chaeeun Lee, Pasquale Minervini, Luke Daines, T. Ian Simpson, Beatrice Alex

The MEDIQA-CORR 2024 shared task aims to assess the ability of Large Language Models (LLMs) to identify and correct medical errors in clinical notes. In this study, we evaluate the capability of general LLMs, specifically GPT-3.5 and GPT-4, to identify and correct medical errors with multiple prompting strategies. Recognising the limitation of LLMs in generating accurate corrections only via prompting strategies, we propose incorporating error-span predictions from a smaller, fine-tuned model in two ways: 1) by presenting it as a hint in the prompt and 2) by framing it as multiple-choice questions from which the LLM can choose the best correction. We found that our proposed prompting strategies significantly improve the LLM's ability to generate corrections. Our best-performing solution with 8-shot + CoT + hints ranked sixth in the shared task leaderboard. Additionally, our comprehensive analyses show the impact of the location of the error sentence, the prompted role, and the position of the multiple-choice option on the accuracy of the LLM. This prompts further questions about the readiness of LLM to be implemented in real-world clinical settings.

------------

`[2405.18113] Facilitating Multi-Role and Multi-Behavior Collaboration of Large Language Models for Online Job Seeking and Recruiting <https://arxiv.org/abs/2405.18113>`__ 促进在线求职招聘大型语言模型的多角色多行为协作

::

    Tue, 28 May 2024 12:23:16 GMT
    Hongda Sun, Hongzhan Lin, Haiyu Yan, Chen Zhu, Yang Song, Xin Gao, Shuo Shang, Rui Yan

The emergence of online recruitment services has revolutionized the traditional landscape of job seeking and recruitment, necessitating the development of high-quality industrial applications to improve person-job fitting. Existing methods generally rely on modeling the latent semantics of resumes and job descriptions and learning a matching function between them.
Inspired by the powerful role-playing capabilities of Large Language Models (LLMs), we propose to introduce a mock interview process between LLM-played interviewers and candidates. The mock interview conversations can provide additional evidence for candidate evaluation, thereby augmenting traditional person-job fitting based solely on resumes and job descriptions. However, characterizing these two roles in online recruitment still presents several challenges, such as developing the skills to raise interview questions, formulating appropriate answers, and evaluating two-sided fitness. To this end, we propose MockLLM, a novel applicable framework that divides the person-job matching process into two modules: mock interview generation and two-sided evaluation in handshake protocol, jointly enhancing their performance through collaborative behaviors between interviewers and candidates. We design a role-playing framework as a multi-role and multi-behavior paradigm to enable a single LLM agent to effectively behave with multiple functions for both parties. Moreover, we propose reflection memory generation and dynamic prompt modification techniques to refine the behaviors of both sides, enabling continuous optimization of the augmented additional evidence. Extensive experimental results show that MockLLM can achieve the best performance on person-job matching accompanied by high mock interview quality, envisioning its emerging application in real online recruitment in the future.

------------

`[2405.18203] IAPT: Instruction-Aware Prompt Tuning for Large Language Models <https://arxiv.org/abs/2405.18203>`__ IAPT:面向大型语言模型的指令感知提示调优

::

    Tue, 28 May 2024 14:11:01 GMT
    Wei Zhu, Aaron Xuxiang Tian, Congrui Yin, Yuan Ni, Xiaoling Wang, Guotong Xie

Soft prompt tuning is a widely studied parameter-efficient fine-tuning method. However, it has a clear drawback: many soft tokens must be inserted into the input sequences to guarantee downstream performance. As a result, soft prompt tuning is less considered than Low-rank adaptation (LoRA) in the large language modeling (LLM) era. In this work, we propose a novel prompt tuning method, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft tokens. First, we install a parameter-efficient soft prompt generator at each Transformer layer to generate idiosyncratic soft prompts for each input instruction. The generated soft prompts can be seen as a semantic summary of the input instructions and can effectively guide the output generation. Second, the soft prompt generators are modules with a bottleneck architecture consisting of a self-attention pooling operation, two linear projections, and an activation function. Pilot experiments show that prompt generators at different Transformer layers require different activation functions. Thus, we propose to learn the idiosyncratic activation functions for prompt generators automatically with the help of rational functions. We have conducted experiments on various tasks, and the experimental results demonstrate that (a) our IAPT method can outperform the recent baselines with comparable tunable parameters. (b) Our IAPT method is more efficient than LoRA under the single-backbone multi-tenant setting.

------------

`[2405.18241] Active Use of Latent Constituency Representation in both Humans and Large Language Models <https://arxiv.org/abs/2405.18241>`__ 潜在选民表示在人类和大型语言模型中的积极使用

::

    Tue, 28 May 2024 14:50:22 GMT
    Wei Liu, Ming Xiang, Nai Ding

Understanding how sentences are internally represented in the human brain, as well as in large language models (LLMs) such as ChatGPT, is a major challenge for cognitive science. Classic linguistic theories propose that the brain represents a sentence by parsing it into hierarchically organized constituents.
In contrast, LLMs do not explicitly parse linguistic constituents and their latent representations remains poorly explained. Here, we demonstrate that humans and LLMs construct similar latent representations of hierarchical linguistic constituents by analyzing their behaviors during a novel one-shot learning task, in which they infer which words should be deleted from a sentence. Both humans and LLMs tend to delete a constituent, instead of a nonconstituent word string. In contrast, a naive sequence processing model that has access to word properties and ordinal positions does not show this property. Based on the word deletion behaviors, we can reconstruct the latent constituency tree representation of a sentence for both humans and LLMs. These results demonstrate that a latent tree-structured constituency representation can emerge in both the human brain and LLMs.

------------

`[2405.18344] The Battle of LLMs: A Comparative Study in Conversational QA Tasks <https://arxiv.org/abs/2405.18344>`__ llm之战:会话QA任务的比较研究

::

    Tue, 28 May 2024 16:42:43 GMT
    Aryan Rangapur, Aman Rangapur

Large language models have gained considerable interest for their impressive performance on various tasks. Within this domain, ChatGPT and GPT-4, developed by OpenAI, and the Gemini, developed by Google, have emerged as particularly popular among early adopters. Additionally, Mixtral by Mistral AI and Claude by Anthropic are newly released, further expanding the landscape of advanced language models. These models are viewed as disruptive technologies with applications spanning customer service, education, healthcare, and finance.
More recently, Mistral has entered the scene, captivating users with its unique ability to generate creative content. Understanding the perspectives of these users is crucial, as they can offer valuable insights into the potential strengths, weaknesses, and overall success or failure of these technologies in various domains. This research delves into the responses generated by ChatGPT, GPT-4, Gemini, Mixtral and Claude across different Conversational QA corpora.
Evaluation scores were meticulously computed and subsequently compared to ascertain the overall performance of these models. Our study pinpointed instances where these models provided inaccurate answers to questions, offering insights into potential areas where they might be susceptible to errors. In essence, this research provides a comprehensive comparison and evaluation of these state of-the-art language models, shedding light on their capabilities while also highlighting potential areas for improvement

------------

`[2405.18359] Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs <https://arxiv.org/abs/2405.18359>`__ 弥合差距:提高llm多语言性能的动态学习策略

::

    Tue, 28 May 2024 16:56:42 GMT
    Somnath Kumar, Vaibhav Balloli, Mercy Ranjit, Kabir Ahuja, Tanuja Ganu, Sunayana Sitaram, Kalika Bali and Akshay Nambi

Large language models (LLMs) are at the forefront of transforming numerous domains globally. However, their inclusivity and effectiveness remain limited for non-Latin scripts and low-resource languages. This paper tackles the imperative challenge of enhancing the multilingual performance of LLMs without extensive training or fine-tuning. Through systematic investigation and evaluation of diverse languages using popular question-answering (QA) datasets, we present novel techniques that unlock the true potential of LLMs in a polyglot landscape. Our approach encompasses three key strategies that yield significant improvements in multilingual proficiency. First, by meticulously optimizing prompts tailored for polyglot LLMs, we unlock their latent capabilities, resulting in substantial performance boosts across languages.
Second, we introduce a new hybrid approach that synergizes LLM Retrieval Augmented Generation (RAG) with multilingual embeddings and achieves improved multilingual task performance. Finally, we introduce a novel learning approach that dynamically selects the optimal prompt strategy, LLM model, and embedding model per query at run-time. This dynamic adaptation maximizes the efficacy of LLMs across languages, outperforming best static and random strategies.
Additionally, our approach adapts configurations in both offline and online settings, and can seamlessly adapt to new languages and datasets, leading to substantial advancements in multilingual understanding and generation across diverse languages.

------------

`[2405.17440] CataLM: Empowering Catalyst Design Through Large Language Models <https://arxiv.org/abs/2405.17440>`__ CataLM:通过大型语言模型赋能催化剂设计

::

    Mon, 13 May 2024 03:19:47 GMT
    Ludi Wang, Xueqing Chen, Yi Du, Yuanchun Zhou, Yang Gao, Wenjuan Cui

The field of catalysis holds paramount importance in shaping the trajectory of sustainable development, prompting intensive research efforts to leverage artificial intelligence (AI) in catalyst design. Presently, the fine-tuning of open-source large language models (LLMs) has yielded significant breakthroughs across various domains such as biology and healthcare. Drawing inspiration from these advancements, we introduce CataLM Cata}lytic Language Model), a large language model tailored to the domain of electrocatalytic materials. Our findings demonstrate that CataLM exhibits remarkable potential for facilitating human-AI collaboration in catalyst knowledge exploration and design. To the best of our knowledge, CataLM stands as the pioneering LLM dedicated to the catalyst domain, offering novel avenues for catalyst discovery and development.

------------

`[2405.17484] Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation <https://arxiv.org/abs/2405.17484>`__ 通过Householder反射自适应弥合低秩自适应和正交自适应之间的差距

::

    Fri, 24 May 2024 16:18:16 GMT
    Shen Yuan, Haotian Liu, Hongteng Xu

While following different technical routes, both low-rank and orthogonal adaptation techniques can efficiently adapt large-scale pre-training models in specific tasks or domains based on a small piece of trainable parameters. In this study, we bridge the gap between these two techniques, proposing a simple but effective adaptation method based on Householder reflections. Given a pre-trained model, our method fine-tunes its layers by multiplying each frozen weight matrix with an orthogonal matrix constructed by a chain of learnable Householder reflections (HRs). This HR-based orthogonal fine-tuning is equivalent to an adaptive low-rank adaptation. Moreover, we show that the orthogonality of the reflection planes corresponding to the HRs impacts the model capacity and regularity. The analysis motivates us to regularize the orthogonality of the HRs, leading to different implementations of the proposed Householder reflection adaptation (HRA) method. Compared with state-of-the-art methods, HRA achieves superior performance with fewer learnable parameters when adapting large language models and conditional image generators. The code is available at https://github.com/DaShenZi721/HRA

------------

`[2405.17490] Revisit, Extend, and Enhance Hessian-Free Influence Functions <https://arxiv.org/abs/2405.17490>`__ 重新访问、扩展和增强Hessian-Free影响函数

::

    Sat, 25 May 2024 03:43:36 GMT
    Ziao Yang, Han Yue, Jian Chen, Hongfu Liu

Influence functions serve as crucial tools for assessing sample influence in model interpretation, subset training set selection, noisy label detection, and more. By employing the first-order Taylor extension, influence functions can estimate sample influence without the need for expensive model retraining.
However, applying influence functions directly to deep models presents challenges, primarily due to the non-convex nature of the loss function and the large size of model parameters. This difficulty not only makes computing the inverse of the Hessian matrix costly but also renders it non-existent in some cases. Various approaches, including matrix decomposition, have been explored to expedite and approximate the inversion of the Hessian matrix, with the aim of making influence functions applicable to deep models. In this paper, we revisit a specific, albeit naive, yet effective approximation method known as TracIn. This method substitutes the inverse of the Hessian matrix with an identity matrix. We provide deeper insights into why this simple approximation method performs well. Furthermore, we extend its applications beyond measuring model utility to include considerations of fairness and robustness. Finally, we enhance TracIn through an ensemble strategy. To validate its effectiveness, we conduct experiments on synthetic data and extensive evaluations on noisy label detection, sample selection for large language model fine-tuning, and defense against adversarial attacks.

------------

`[2405.17505] Predicting Rental Price of Lane Houses in Shanghai with Machine Learning Methods and Large Language Models <https://arxiv.org/abs/2405.17505>`__ 基于机器学习方法和大型语言模型的上海巷居租金预测

::

    Sun, 26 May 2024 07:01:33 GMT
    Tingting Chen and Shijing Si

Housing has emerged as a crucial concern among young individuals residing in major cities, including Shanghai. Given the unprecedented surge in property prices in this metropolis, young people have increasingly resorted to the rental market to address their housing needs. This study utilizes five traditional machine learning methods: multiple linear regression (MLR), ridge regression (RR), lasso regression (LR), decision tree (DT), and random forest (RF), along with a Large Language Model (LLM) approach using ChatGPT, for predicting the rental prices of lane houses in Shanghai. It applies these methods to examine a public data sample of about 2,609 lane house rental transactions in 2021 in Shanghai, and then compares the results of these methods. In terms of predictive power, RF has achieved the best performance among the traditional methods. However, the LLM approach, particularly in the 10-shot scenario, shows promising results that surpass traditional methods in terms of R-Squared value. The three performance metrics: mean squared error (MSE), mean absolute error (MAE), and R-Squared, are used to evaluate the models. Our conclusion is that while traditional machine learning models offer robust techniques for rental price prediction, the integration of LLM such as ChatGPT holds significant potential for enhancing predictive accuracy.

------------

`[2405.17618] Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales <https://arxiv.org/abs/2405.17618>`__ 对称强化学习损失用于不同任务和模型规模的鲁棒学习

::

    Mon, 27 May 2024 19:28:33 GMT
    Ju-Seung Byun, Andrew Perrault

Reinforcement learning (RL) training is inherently unstable due to factors such as moving targets and high gradient variance. Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) can introduce additional difficulty. Differing preferences can complicate the alignment process, and prediction errors in a trained reward model can become more severe as the LLM generates unseen outputs. To enhance training robustness, RL has adopted techniques from supervised learning, such as ensembles and layer normalization. In this work, we improve the stability of RL training by adapting the reverse cross entropy (RCE) from supervised learning for noisy data to define a symmetric RL loss. We demonstrate performance improvements across various tasks and scales. We conduct experiments in discrete action tasks (Atari games) and continuous action space tasks (MuJoCo benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO), with and without added noise with especially notable performance in SPPO across different hyperparameters. Furthermore, we validate the benefits of the symmetric RL loss when using SPPO for large language models through improved performance in RLHF tasks, such as IMDB positive sentiment sentiment and TL;DR summarization tasks.

------------

`[2405.17627] Salutary Labeling with Zero Human Annotation <https://arxiv.org/abs/2405.17627>`__ 无需人工标注的有益标签

::

    Mon, 27 May 2024 19:49:18 GMT
    Wenxiao Xiao, Hongfu Liu

Active learning strategically selects informative unlabeled data points and queries their ground truth labels for model training. The prevailing assumption underlying this machine learning paradigm is that acquiring these ground truth labels will optimally enhance model performance. However, this assumption may not always hold true or maximize learning capacity, particularly considering the costly labor annotations required for ground truth labels. In contrast to traditional ground truth labeling, this paper proposes salutary labeling, which automatically assigns the most beneficial labels to the most informative samples without human annotation. Specifically, we utilize the influence function, a tool for estimating sample influence, to select newly added samples and assign their salutary labels by choosing the category that maximizes their positive influence. This process eliminates the need for human annotation.
Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our salutary labeling approach over traditional active learning strategies. Additionally, we provide several in-depth explorations and practical applications of large language model (LLM) fine-tuning.

------------

`[2405.17703] Mechanistic Interpretability of Binary and Ternary Transformers <https://arxiv.org/abs/2405.17703>`__ 二元和三元transformer的机制可解释性

::

    Mon, 27 May 2024 23:22:23 GMT
    Jason Li

Recent research (arXiv:2310.11453, arXiv:2402.17764) has proposed binary and ternary transformer networks as a way to significantly reduce memory and improve inference speed in Large Language Models (LLMs) while maintaining accuracy. In this work, we apply techniques from mechanistic interpretability to investigate whether such networks learn distinctly different or similar algorithms when compared to full-precision transformer networks. In particular, we reverse engineer the algorithms learned for the toy problem of modular addition where we find that binary and ternary networks learn similar algorithms as full precision networks. This provides evidence against the possibility of using binary and ternary networks as a more interpretable alternative in the LLM setting.

------------

`[2405.17767] Linguistic Collapse: Neural Collapse in (Large) Language Models <https://arxiv.org/abs/2405.17767>`__ 语言坍缩:(大型)语言模型的神经坍缩

::

    Tue, 28 May 2024 02:46:11 GMT
    Robert Wu and Vardan Papyan

Neural collapse ($\mathcal{NC}$) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension. Recent studies have explored $\mathcal{NC}$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries. Language modeling presents a curious frontier, as \textit{training by token prediction} constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards $\mathcal{NC}$. We find that $\mathcal{NC}$ properties that develop with scaling are linked to generalization. Moreover, there is evidence of some relationship between $\mathcal{NC}$ and generalization independent of scale. Our work therefore underscores the generality of $\mathcal{NC}$ as it extends to the novel and more challenging setting of language modeling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on $\mathcal{NC}$-related properties.

------------

`[2405.17799] Exploring Activation Patterns of Parameters in Language Models <https://arxiv.org/abs/2405.17799>`__ 语言模型中参数激活模式的探索

::

    Tue, 28 May 2024 03:49:54 GMT
    Yudong Wang, Damai Dai, Zhifang Sui

Most work treats large language models as black boxes without in-depth understanding of their internal working mechanism. In order to explain the internal representations of LLMs, we propose a gradient-based metric to assess the activation level of model parameters. Based on this metric, we obtain three preliminary findings. (1) When the inputs are in the same domain, parameters in the shallow layers will be activated densely, which means a larger portion of parameters will have great impacts on the outputs. In contrast, parameters in the deep layers are activated sparsely. (2) When the inputs are across different domains, parameters in shallow layers exhibit higher similarity in the activation behavior than deep layers. (3) In deep layers, the similarity of the distributions of activated parameters is positively correlated to the empirical data relevance. Further, we develop three validation experiments to solidify these findings. (1) Firstly, starting from the first finding, we attempt to configure different prune ratios for different layers, and find this method can benefit model pruning. (2) Secondly, we find that a pruned model based on one calibration set can better handle tasks related to the calibration task than those not related, which validate the second finding. (3) Thirdly, Based on the STS-B and SICK benchmark, we find that two sentences with consistent semantics tend to share similar parameter activation patterns in deep layers, which aligns with our third finding. Our work sheds light on the behavior of parameter activation in LLMs, and we hope these findings will have the potential to inspire more practical applications.

------------

`[2405.18039] Large Language Model-Driven Curriculum Design for Mobile Networks <https://arxiv.org/abs/2405.18039>`__ 大型语言模型驱动的移动网络课程设计

::

    Tue, 28 May 2024 10:50:35 GMT
    Omar Erak, Omar Alhussein, Shimaa Naser, Nouf Alabbasi, De Mi, Sami Muhaidat

This paper proposes a novel framework that leverages large language models (LLMs) to automate curriculum design, thereby enhancing the application of reinforcement learning (RL) in mobile networks. As mobile networks evolve towards the 6G era, managing their increasing complexity and dynamic nature poses significant challenges. Conventional RL approaches often suffer from slow convergence and poor generalization due to conflicting objectives and the large state and action spaces associated with mobile networks. To address these shortcomings, we introduce curriculum learning, a method that systematically exposes the RL agent to progressively challenging tasks, improving convergence and generalization. However, curriculum design typically requires extensive domain knowledge and manual human effort. Our framework mitigates this by utilizing the generative capabilities of LLMs to automate the curriculum design process, significantly reducing human effort while improving the RL agent's convergence and performance. We deploy our approach within a simulated mobile network environment and demonstrate improved RL convergence rates, generalization to unseen scenarios, and overall performance enhancements. As a case study, we consider autonomous coordination and user association in mobile networks. Our obtained results highlight the potential of combining LLM-based curriculum generation with RL for managing next-generation wireless networks, marking a significant step towards fully autonomous network operations.

------------

`[2405.18100] A Pontryagin Perspective on Reinforcement Learning <https://arxiv.org/abs/2405.18100>`__ 庞特里亚金视角下的强化学习

::

    Tue, 28 May 2024 12:05:20 GMT
    Onno Eberhard, Claire Vernade, Michael Muehlebach

Reinforcement learning has traditionally focused on learning state-dependent policies to solve optimal control problems in a closed-loop fashion. In this work, we introduce the paradigm of open-loop reinforcement learning where a fixed action sequence is learned instead. We present three new algorithms: one robust model-based method and two sample-efficient model-free methods. Rather than basing our algorithms on Bellman's equation from dynamic programming, our work builds on Pontryagin's principle from the theory of open-loop optimal control. We provide convergence guarantees and evaluate all methods empirically on a pendulum swing-up task, as well as on two high-dimensional MuJoCo tasks, demonstrating remarkable performance compared to existing baselines.

------------

`[2405.18137] Exploiting LLM Quantization <https://arxiv.org/abs/2405.18137>`__ 利用LLM量化

::

    Tue, 28 May 2024 12:51:01 GMT
    Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, Martin Vechev

Quantization leverages lower-precision weights to reduce the memory usage of large language models (LLMs) and is a key technique for enabling their deployment on commodity hardware. While LLM quantization's impact on utility has been extensively explored, this work for the first time studies its adverse effects from a security perspective. We reveal that widely used quantization methods can be exploited to produce a harmful quantized LLM, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model. We demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii). This procedure results in an LLM that exhibits benign behavior in full precision but when quantized, it follows the adversarial behavior injected in step (i). We experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack. In practice, the adversary could host the resulting full-precision model on an LLM community hub such as Hugging Face, exposing millions of users to the threat of deploying its malicious quantized version on their devices.

------------

`[2405.18218] FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models <https://arxiv.org/abs/2405.18218>`__ FinerCut:大型语言模型的细粒度可解释层修剪

::

    Tue, 28 May 2024 14:21:15 GMT
    Yang Zhang, Yawei Li, Xinpeng Wang, Qianli Shen, Barbara Plank, Bernd Bischl, Mina Rezaei, Kenji Kawaguchi

Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To address these issues, we propose FinerCut, a new form of fine-grained layer pruning, which in contrast to prior work at the transformer block level, considers all self-attention and feed-forward network (FFN) layers within blocks as individual pruning candidates. FinerCut prunes layers whose removal causes minimal alternation to the model's output -- contributing to a new, lean, interpretable, and task-agnostic pruning method. Tested across 9 benchmarks, our approach retains 90% performance of Llama3-8B with 25% layers removed, and 95% performance of Llama3-70B with 30% layers removed, all without fine-tuning or post-pruning reconstruction. Strikingly, we observe intriguing results with FinerCut: 42% (34 out of 80) of the self-attention layers in Llama3-70B can be removed while preserving 99% of its performance -- without additional fine-tuning after removal. Moreover, FinerCut provides a tool to inspect the types and locations of pruned layers, allowing to observe interesting pruning behaviors. For instance, we observe a preference for pruning self-attention layers, often at deeper consecutive decoder layers. We hope our insights inspire future efficient LLM architecture designs.

------------

`[2405.18376] Empowering Source-Free Domain Adaptation with MLLM-driven Curriculum Learning <https://arxiv.org/abs/2405.18376>`__ 用mllm驱动的课程学习实现无源领域自适应

::

    Tue, 28 May 2024 17:18:17 GMT
    Dongjie Chen, Kartik Patwari, Zhengfeng Lai, Sen-ching Cheung, Chen-Nee Chuah

Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to a target domain using only unlabeled target data. Current SFDA methods face challenges in effectively leveraging pre-trained knowledge and exploiting target domain data. Multimodal Large Language Models (MLLMs) offer remarkable capabilities in understanding visual and textual information, but their applicability to SFDA poses challenges such as instruction-following failures, intensive computational demands, and difficulties in performance measurement prior to adaptation. To alleviate these issues, we propose Reliability-based Curriculum Learning (RCL), a novel framework that integrates multiple MLLMs for knowledge exploitation via pseudo-labeling in SFDA. Our framework incorporates proposed Reliable Knowledge Transfer, Self-correcting and MLLM-guided Knowledge Expansion, and Multi-hot Masking Refinement to progressively exploit unlabeled data in the target domain. RCL achieves state-of-the-art (SOTA) performance on multiple SFDA benchmarks, e.g., $\textbf{+9.4%}$ on DomainNet, demonstrating its effectiveness in enhancing adaptability and robustness without requiring access to source data. Code: https://github.com/Dong-Jie-Chen/RCL.

------------

`[2405.17441] When Large Language Models Meet Optical Networks: Paving the Way for Automation <https://arxiv.org/abs/2405.17441>`__ 当大型语言模型遇到光学网络:为自动化铺平道路

::

    Tue, 14 May 2024 10:46:33 GMT
    Danshi Wang, Yidi Wang, Xiaotian Jiang, Yao Zhang, Yue Pang, Min Zhang

Since the advent of GPT, large language models (LLMs) have brought about revolutionary advancements in all walks of life. As a superior natural language processing (NLP) technology, LLMs have consistently achieved state-of-the-art performance on numerous areas. However, LLMs are considered to be general-purpose models for NLP tasks, which may encounter challenges when applied to complex tasks in specialized fields such as optical networks. In this study, we propose a framework of LLM-empowered optical networks, facilitating intelligent control of the physical layer and efficient interaction with the application layer through an LLM-driven agent (AI-Agent) deployed in the control layer. The AI-Agent can leverage external tools and extract domain knowledge from a comprehensive resource library specifically established for optical networks. This is achieved through user input and well-crafted prompts, enabling the generation of control instructions and result representations for autonomous operation and maintenance in optical networks. To improve LLM's capability in professional fields and stimulate its potential on complex tasks, the details of performing prompt engineering, establishing domain knowledge library, and implementing complex tasks are illustrated in this study. Moreover, the proposed framework is verified on two typical tasks: network alarm analysis and network performance optimization. The good response accuracies and sematic similarities of 2,400 test situations exhibit the great potential of LLM in optical networks.

------------

`[2405.17503] Code Repair with LLMs gives an Exploration-Exploitation Tradeoff <https://arxiv.org/abs/2405.17503>`__ 使用llm进行代码修复提供了探索-利用的权衡

::

    Sun, 26 May 2024 04:00:30 GMT
    Hao Tang, Keya Hu, Jin Peng Zhou, Sicheng Zhong, Wei-Long Zheng, Xujie Si, Kevin Ellis

Iteratively improving and repairing source code with large language models (LLMs), known as refinement, has emerged as a popular way of generating programs that would be too complex to construct in one shot. Given a bank of test cases, together with a candidate program, an LLM can improve that program by being prompted with failed test cases. But it remains an open question how to best iteratively refine code, with prior work employing simple greedy or breadth-first strategies. We show here that refinement exposes an explore-exploit tradeoff: exploit by refining the program that passes the most test cases, or explore by refining a lesser considered program. We frame this as an arm-acquiring bandit problem, which we solve with Thompson Sampling. The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls.

------------

`[2405.17728] Facilitating Holistic Evaluations with LLMs: Insights from Scenario-Based Experiments <https://arxiv.org/abs/2405.17728>`__ 用llm促进整体评估:基于场景实验的见解

::

    Tue, 28 May 2024 01:07:06 GMT
    Toru Ishida

Workshop courses designed to foster creativity are gaining popularity.
However, achieving a holistic evaluation that accommodates diverse perspectives is challenging, even for experienced faculty teams. Adequate discussion is essential to integrate varied assessments, but faculty often lack the time for such deliberations. Deriving an average score without discussion undermines the purpose of a holistic evaluation. This paper explores the use of a Large Language Model (LLM) as a facilitator to integrate diverse faculty assessments.
Scenario-based experiments were conducted to determine if the LLM could synthesize diverse evaluations and explain the underlying theories to faculty.
The results were noteworthy, showing that the LLM effectively facilitated faculty discussions. Additionally, the LLM demonstrated the capability to generalize and create evaluation criteria from a single scenario based on its learned domain knowledge.

------------

`[2405.17846] Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs <https://arxiv.org/abs/2405.17846>`__ 基于llm和具身知识图谱的服务机器人安全控制

::

    Tue, 28 May 2024 05:50:25 GMT
    Yong Qi, Gabriel Kyebambo, Siyuan Xie, Wei Shen, Shenghui Wang, Bitao Xie, Bin He, Zhipeng Wang, Shuo Jiang

Safety limitations in service robotics across various industries have raised significant concerns about the need for robust mechanisms ensuring that robots adhere to safe practices, thereby preventing actions that might harm humans or cause property damage. Despite advances, including the integration of Knowledge Graphs (KGs) with Large Language Models (LLMs), challenges in ensuring consistent safety in autonomous robot actions persist. In this paper, we propose a novel integration of Large Language Models with Embodied Robotic Control Prompts (ERCPs) and Embodied Knowledge Graphs (EKGs) to enhance the safety framework for service robots. ERCPs are designed as predefined instructions that ensure LLMs generate safe and precise responses. These responses are subsequently validated by EKGs, which provide a comprehensive knowledge base ensuring that the actions of the robot are continuously aligned with safety protocols, thereby promoting safer operational practices in varied contexts. Our experimental setup involved diverse real-world tasks, where robots equipped with our framework demonstrated significantly higher compliance with safety standards compared to traditional methods. This integration fosters secure human-robot interactions and positions our methodology at the forefront of AI-driven safety innovations in service robotics.

------------

`[2405.18258] Text-only Synthesis for Image Captioning <https://arxiv.org/abs/2405.18258>`__ 图像描述生成的纯文本合成

::

    Tue, 28 May 2024 15:11:17 GMT
    Qing Zhou, Junlin Huang, Qiang Li, Junyu Gao and Qi Wang

From paired image-text training to text-only training for image captioning, the pursuit of relaxing the requirements for high-cost and large-scale annotation of good quality data remains consistent. In this paper, we propose Text-only Synthesis for Image Captioning (ToCa), which further advances this relaxation with fewer human labor and less computing time. Specifically, we deconstruct caption text into structures and lexical words, which serve as the fundamental components of the caption. By combining different structures and lexical words as inputs to the large language model, massive captions that contain various patterns of lexical words are generated. This method not only approaches the target domain but also surpasses it by generating new captions, thereby enhancing the zero-shot generalization ability of the model.
Considering the different levels of data access in the real world, we define three synthesis scenarios: cross-domain synthesis, in-domain synthesis, and data-efficient synthesis. Experiments in these scenarios demonstrate the generalizability, transferability and practicability of ToCa with a nearly 5 CIDEr improvement for zero-shot cross-domain captioning and a maximum increase of over 20 CIDEr for data-efficient captioning.

------------

`[2405.18386] Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning <https://arxiv.org/abs/2405.18386>`__ Instruction - musicgen:通过指令调优解锁音乐语言模型的文本到音乐编辑

::

    Tue, 28 May 2024 17:27:20 GMT
    Yixiao Zhang, Yukara Ikemiya, Woosung Choi, Naoki Murata, Marco A. Mart\'inez-Ram\'irez, Liwei Lin, Gus Xia, Wei-Hsiang Liao, Yuki Mitsufuji, Simon Dixon

Recent advances in text-to-music editing, which employ text queries to modify music (e.g.\ by changing its style or adjusting instrumental components), present unique challenges and opportunities for AI-assisted music creation.
Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction. To Combine the strengths and address these limitations, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems. Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio inputs concurrently and yield the desired edited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, yet it achieves superior performance across all tasks compared to existing baselines, and demonstrates performance comparable to the models trained for specific tasks.
This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments.

------------

`[2405.17658] Generative Query Reformulation Using Ensemble Prompting, Document Fusion, and Relevance Feedback <https://arxiv.org/abs/2405.17658>`__ 基于集成提示、文档融合和相关反馈的生成式查询重构

::

    Mon, 27 May 2024 21:03:26 GMT
    Kaustubh D. Dhole, Ramraj Chandradevan, Eugene Agichtein

Query Reformulation (QR) is a set of techniques used to transform a user's original search query to a text that better aligns with the user's intent and improves their search experience. Recently, zero-shot QR has been a promising approach due to its ability to exploit knowledge inherent in large language models. Inspired by the success of ensemble prompting strategies which have benefited other tasks, we investigate if they can improve query reformulation.
In this context, we propose two ensemble-based prompting techniques, GenQREnsemble and GenQRFusion which leverage paraphrases of a zero-shot instruction to generate multiple sets of keywords to improve retrieval performance ultimately. We further introduce their post-retrieval variants to incorporate relevance feedback from a variety of sources, including an oracle simulating a human user and a "critic" LLM. We demonstrate that an ensemble of query reformulations can improve retrieval effectiveness by up to 18% on nDCG@10 in pre-retrieval settings and 9% on post-retrieval settings on multiple benchmarks, outperforming all previously reported SOTA results. We perform subsequent analyses to investigate the effects of feedback documents, incorporate domain-specific instructions, filter reformulations, and generate fluent reformulations that might be more beneficial to human searchers.
Together, the techniques and the results presented in this paper establish a new state of the art in automated query reformulation for retrieval and suggest promising directions for future research.

------------

`[2405.17890] SLMRec: Empowering Small Language Models for Sequential Recommendation <https://arxiv.org/abs/2405.17890>`__ SLMRec:面向顺序推荐的小型语言模型

::

    Tue, 28 May 2024 07:12:06 GMT
    Wujiang Xu, Zujie Liang, Jiaojiao Han, Xuying Ning, Wenfang Lin, Linxun Chen, Feng Wei, Yongfeng Zhang

The sequential Recommendation (SR) task involves predicting the next item a user is likely to interact with, given their past interactions. The SR models examine the sequence of a user's actions to discern more complex behavioral patterns and temporal dynamics. Recent research demonstrates the great impact of LLMs on sequential recommendation systems, either viewing sequential recommendation as language modeling or serving as the backbone for user representation. Although these methods deliver outstanding performance, there is scant evidence of the necessity of a large language model and how large the language model is needed, especially in the sequential recommendation scene.
Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to apply a LLM-based model in real-world platforms that often need to process billions of traffic logs daily. In this paper, we explore the influence of LLMs' depth by conducting extensive experiments on large-scale industry datasets. Surprisingly, we discover that most intermediate layers of LLMs are redundant. Motivated by this insight, we empower small language models for SR, namely SLMRec, which adopt a simple yet effective knowledge distillation method. Moreover, SLMRec is orthogonal to other post-training efficiency techniques, such as quantization and pruning, so that they can be leveraged in combination. Comprehensive experimental results illustrate that the proposed SLMRec model attains the best performance using only 13% of the parameters found in LLM-based recommendation models, while simultaneously achieving up to 6.6x and 8.0x speedups in training and inference time costs, respectively.

------------

`[2405.17439] An Overview of Machine Learning-Enabled Optimization for Reconfigurable Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large Language Models <https://arxiv.org/abs/2405.17439>`__ 可重构智能表面辅助6G网络的机器学习优化综述:从强化学习到大型语言模型

::

    Thu, 9 May 2024 03:07:59 GMT
    Hao Zhou, Chengming Hu, and Xue Liu

Reconfigurable intelligent surface (RIS) becomes a promising technique for 6G networks by reshaping signal propagation in smart radio environments. However, it also leads to significant complexity for network management due to the large number of elements and dedicated phase-shift optimization. In this work, we provide an overview of machine learning (ML)-enabled optimization for RIS-aided 6G networks. In particular, we focus on various reinforcement learning (RL) techniques, e.g., deep Q-learning, multi-agent reinforcement learning, transfer reinforcement learning, hierarchical reinforcement learning, and offline reinforcement learning. Different from existing studies, this work further discusses how large language models (LLMs) can be combined with RL to handle network optimization problems. It shows that LLM offers new opportunities to enhance the capabilities of RL algorithms in terms of generalization, reward function design, multi-modal information processing, etc. Finally, we identify the future challenges and directions of ML-enabled optimization for RIS-aided 6G networks.

------------

`[2405.18093] Pipette: Automatic Fine-grained Large Language Model Training Configurator for Real-World Clusters <https://arxiv.org/abs/2405.18093>`__ Pipette:面向真实世界集群的自动细粒度大型语言模型训练配置器

::

    Tue, 28 May 2024 11:59:44 GMT
    Jinkyu Yim, Jaeyong Song, Yerim Choi, Jaebeen Lee, Jaewon Jung, Hongsun Jang and Jinho Lee

Training large language models (LLMs) is known to be challenging because of the huge computational and memory capacity requirements. To address these issues, it is common to use a cluster of GPUs with 3D parallelism, which splits a model along the data batch, pipeline stage, and intra-layer tensor dimensions. However, the use of 3D parallelism produces the additional challenge of finding the optimal number of ways on each dimension and mapping the split models onto the GPUs. Several previous studies have attempted to automatically find the optimal configuration, but many of these lacked several important aspects. For instance, the heterogeneous nature of the interconnect speeds is often ignored. While the peak bandwidths for the interconnects are usually made equal, the actual attained bandwidth varies per link in real-world clusters. Combined with the critical path modeling that does not properly consider the communication, they easily fall into sub-optimal configurations.
In addition, they often fail to consider the memory requirement per GPU, often recommending solutions that could not be executed. To address these challenges, we propose Pipette, which is an automatic fine-grained LLM training configurator for real-world clusters. By devising better performance models along with the memory estimator and fine-grained individual GPU assignment, Pipette achieves faster configurations that satisfy the memory constraints. We evaluated Pipette on large clusters to show that it provides a significant speedup over the prior art. The implementation of Pipette is available at https://github.com/yimjinkyu1/date2024_pipette.

------------

`[2308.15030] SwapMoE: Serving Off-the-shelf MoE-based Language Models with Tunable Memory Budget <https://arxiv.org/abs/2308.15030>`__ SwapMoE:提供可调内存预算的基于moe的现成语言模型

::

    replaced with revised version Tue, 28 May 2024 02:08:30 GMT
    Submission history From: Rui Kong [view email]
    [v1] Tue, 29 Aug 2023 05:25:21 UTC (4,047 KB)
    [v2] Thu, 28 Dec 2023 02:53:41 UTC (4,437 KB)
    [v3] Tue, 28 May 2024 02:08:30 UTC (4,719 KB)
    Rui Kong, Yuanchun Li, Qingtian Feng, Weijun Wang, Xiaozhou Ye, Ye Ouyang, Linghe Kong, Yunxin Liu

Mixture of experts (MoE) is a popular technique to improve capacity of Large Language Models (LLMs) with conditionally-activated parallel experts. However, serving MoE models on memory-constrained devices is challenging due to the large parameter size. Typical solutions such as memory swapping or expert pruning may lead to significantly higher latency or severe accuracy loss. In this paper, we introduce SwapMoE, a framework for efficient serving of MoE-based large language models with tunable memory budgets. The main idea of SwapMoE is to keep a small dynamic set of important experts, namely Virtual Experts, in the main memory for inference, while seamlessly maintaining how the Virtual Experts map to the actual experts. Experiments have shown that SwapMoE can reduce the memory footprint while maintaining reasonable accuracy. For example, on text summarization tasks with Switch Transformer, SwapMoE can reduce the memory consumption from 14.2 GiB to 4.7 GiB, together with 50\% latency reduction and a slight Rouge-2 score drop of 0.041.

------------

`[2402.03824] A call for embodied AI <https://arxiv.org/abs/2402.03824>`__ 对具身人工智能的呼唤

::

    replaced with revised version Tue, 28 May 2024 15:07:37 GMT
    Submission history From: Giuseppe Paolo Dr [view email]
    [v1] Tue, 6 Feb 2024 09:11:20 UTC (75 KB)
    [v2] Tue, 28 May 2024 15:07:37 UTC (77 KB)
    Giuseppe Paolo, Jonas Gonzalez-Billandon, Bal\'azs K\'egl

We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models. We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning. By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future Embodied AI research. Highlighting the importance of creating Embodied AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI.

------------

`[2402.18023] Do Large Language Models Mirror Cognitive Language Processing? <https://arxiv.org/abs/2402.18023>`__ 大型语言模型是否反映了认知语言处理?

::

    replaced with revised version Tue, 28 May 2024 05:51:15 GMT
    Submission history From: Yuqi Ren [view email]
    [v1] Wed, 28 Feb 2024 03:38:20 UTC (20,670 KB)
    [v2] Tue, 28 May 2024 05:51:15 UTC (669 KB)
    Yuqi Ren, Renren Jin, Tongxuan Zhang, Deyi Xiong

Large Language Models (LLMs) have demonstrated remarkable abilities in text comprehension and logical reasoning, indicating that the text representations learned by LLMs can facilitate their language processing capabilities. In cognitive science, brain cognitive processing signals are typically utilized to study human language processing. Therefore, it is natural to ask how well the text embeddings from LLMs align with the brain cognitive processing signals, and how training strategies affect the LLM-brain alignment? In this paper, we employ Representational Similarity Analysis (RSA) to measure the alignment between 23 mainstream LLMs and fMRI signals of the brain to evaluate how effectively LLMs simulate cognitive language processing. We empirically investigate the impact of various factors (e.g., pre-training data size, model scaling, alignment training, and prompts) on such LLM-brain alignment. Experimental results indicate that pre-training data size and model scaling are positively correlated with LLM-brain similarity, and alignment training can significantly improve LLM-brain similarity. Explicit prompts contribute to the consistency of LLMs with brain cognitive language processing, while nonsensical noisy prompts may attenuate such alignment. Additionally, the performance of a wide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated with the LLM-brain similarity.

------------

`[2404.08850] Assessing Economic Viability: A Comparative Analysis of Total Cost of Ownership for Domain-Adapted Large Language Models versus State-of-the-art Counterparts in Chip Design Coding Assistance <https://arxiv.org/abs/2404.08850>`__ 评估经济可行性:领域自适应大型语言模型与最先进的芯片设计编码辅助对应模型的总拥有成本比较分析

::

    replaced with revised version Tue, 28 May 2024 17:11:44 GMT
    Submission history From: Amit Sharma [view email]
    [v1] Fri, 12 Apr 2024 23:37:56 UTC (364 KB)
    [v2] Tue, 28 May 2024 17:11:44 UTC (350 KB)
    Amit Sharma, Teodor-Dumitru Ene, Kishor Kunal, Mingjie Liu, Zafar Hasan and Haoxing Ren

This paper presents a comparative analysis of total cost of ownership (TCO) and performance between domain-adapted large language models (LLM) and state-of-the-art (SoTA) LLMs , with a particular emphasis on tasks related to coding assistance for chip design. We examine the TCO and performance metrics of a domain-adaptive LLM, ChipNeMo, against two leading LLMs, Claude 3 Opus and ChatGPT-4 Turbo, to assess their efficacy in chip design coding generation. Through a detailed evaluation of the accuracy of the model, training methodologies, and operational expenditures, this study aims to provide stakeholders with critical information to select the most economically viable and performance-efficient solutions for their specific needs. Our results underscore the benefits of employing domain-adapted models, such as ChipNeMo, that demonstrate improved performance at significantly reduced costs compared to their general-purpose counterparts. In particular, we reveal the potential of domain-adapted LLMs to decrease TCO by approximately 90%-95%, with the cost advantages becoming increasingly evident as the deployment scale expands. With expansion of deployment, the cost benefits of ChipNeMo become more pronounced, making domain-adaptive LLMs an attractive option for organizations with substantial coding needs supported by LLMs

------------

`[2405.15808] Ensuring Ground Truth Accuracy in Healthcare with the EVINCE framework <https://arxiv.org/abs/2405.15808>`__ 使用EVINCE框架确保医疗保健中的基本事实准确性

::

    replaced with revised version Tue, 28 May 2024 05:11:50 GMT
    Submission history From: Edward Chang [view email]
    [v1] Mon, 20 May 2024 18:26:36 UTC (3,902 KB)
    [v2] Tue, 28 May 2024 05:11:50 UTC (2,890 KB)
    Edward Y. Chang

Misdiagnosis is a significant issue in healthcare, leading to harmful consequences for patients. The propagation of mislabeled data through machine learning models into clinical practice is unacceptable. This paper proposes EVINCE, a system designed to 1) improve diagnosis accuracy and 2) rectify misdiagnoses and minimize training data errors. EVINCE stands for Entropy Variation through Information Duality with Equal Competence, leveraging this novel theory to optimize the diagnostic process using multiple Large Language Models (LLMs) in a structured debate framework. Our empirical study verifies EVINCE to be effective in achieving its design goals.

------------

`[2405.16567] Automatic Jailbreaking of the Text-to-Image Generative AI Systems <https://arxiv.org/abs/2405.16567>`__ 文本到图像生成AI系统的自动越狱

::

    replaced with revised version Tue, 28 May 2024 06:37:00 GMT
    Submission history From: Minseon Kim [view email]
    [v1] Sun, 26 May 2024 13:32:24 UTC (13,775 KB)
    [v2] Tue, 28 May 2024 06:37:00 UTC (37,198 KB)
    Minseon Kim, Hyomin Lee, Boqing Gong, Huishuai Zhang, Sung Ju Hwang

Recent AI systems have shown extremely powerful performance, even surpassing human performance, on various tasks such as information retrieval, language generation, and image generation based on large language models (LLMs). At the same time, there are diverse safety risks that can cause the generation of malicious contents by circumventing the alignment in LLMs, which are often referred to as jailbreaking. However, most of the previous works only focused on the text-based jailbreaking in LLMs, and the jailbreaking of the text-to-image (T2I) generation system has been relatively overlooked. In this paper, we first evaluate the safety of the commercial T2I generation systems, such as ChatGPT, Copilot, and Gemini, on copyright infringement with naive prompts. From this empirical study, we find that Copilot and Gemini block only 12% and 17% of the attacks with naive prompts, respectively, while ChatGPT blocks 84% of them. Then, we further propose a stronger automated jailbreaking pipeline for T2I generation systems, which produces prompts that bypass their safety guards. Our automated jailbreaking framework leverages an LLM optimizer to generate prompts to maximize degree of violation from the generated images without any weight updates or gradient computation. Surprisingly, our simple yet effective approach successfully jailbreaks the ChatGPT with 11.0% block rate, making it generate copyrighted contents in 76% of the time. Finally, we explore various defense strategies, such as post-generation filtering and machine unlearning techniques, but found that they were inadequate, which suggests the necessity of stronger defense mechanisms.

------------

`[2303.13013] GesGPT: Speech Gesture Synthesis With Text Parsing from ChatGPT <https://arxiv.org/abs/2303.13013>`__ GesGPT:基于ChatGPT的文本分析语音手势合成

::

    replaced with revised version Tue, 28 May 2024 02:53:44 GMT
    Submission history From: Nan Gao [view email]
    [v1] Thu, 23 Mar 2023 03:30:30 UTC (8,831 KB)
    [v2] Mon, 27 May 2024 10:21:12 UTC (3,793 KB)
    [v3] Tue, 28 May 2024 02:53:44 UTC (3,793 KB)
    Nan Gao, Zeyu Zhao, Zhi Zeng, Shuwu Zhang, Dongdong Weng, Yihua Bao

Gesture synthesis has gained significant attention as a critical research field, aiming to produce contextually appropriate and natural gestures corresponding to speech or textual input. Although deep learning-based approaches have achieved remarkable progress, they often overlook the rich semantic information present in the text, leading to less expressive and meaningful gestures. In this letter, we propose GesGPT, a novel approach to gesture generation that leverages the semantic analysis capabilities of large language models , such as ChatGPT. By capitalizing on the strengths of LLMs for text analysis, we adopt a controlled approach to generate and integrate professional gestures and base gestures through a text parsing script, resulting in diverse and meaningful gestures. Firstly, our approach involves the development of prompt principles that transform gesture generation into an intention classification problem using ChatGPT. We also conduct further analysis on emphasis words and semantic words to aid in gesture generation. Subsequently, we construct a specialized gesture lexicon with multiple semantic annotations, decoupling the synthesis of gestures into professional gestures and base gestures. Finally, we merge the professional gestures with base gestures. Experimental results demonstrate that GesGPT effectively generates contextually appropriate and expressive gestures.

------------

`[2303.14337] SmartBook: AI-Assisted Situation Report Generation for Intelligence Analysts <https://arxiv.org/abs/2303.14337>`__ SmartBook:面向情报分析人员的ai辅助态势报告生成

::

    replaced with revised version Mon, 27 May 2024 23:40:26 GMT
    Submission history From: Revanth Reddy [view email]
    [v1] Sat, 25 Mar 2023 03:03:00 UTC (12,834 KB)
    [v2] Tue, 28 Mar 2023 15:58:44 UTC (12,834 KB)
    [v3] Mon, 27 May 2024 23:40:26 UTC (18,677 KB)
    Revanth Gangi Reddy, Daniel Lee, Yi R. Fung, Khanh Duy Nguyen, Qi Zeng, Manling Li, Ziqi Wang, Clare Voss and Heng Ji

Timely and comprehensive understanding of emerging events is crucial for effective decision-making; automating situation report generation can significantly reduce the time, effort, and cost for intelligence analysts. In this work, we identify intelligence analysts' practices and preferences for AI assistance in situation report generation to guide the design strategies for an effective, trust-building interface that aligns with their thought processes and needs. Next, we introduce SmartBook, an automated framework designed to generate situation reports from large volumes of news data, creating structured reports by automatically discovering event-related strategic questions. These reports include multiple hypotheses (claims), summarized and grounded to sources with factual evidence, to promote in-depth situation understanding. Our comprehensive evaluation of SmartBook, encompassing a user study alongside a content review with an editing study, reveals SmartBook's effectiveness in generating accurate and relevant situation reports. Qualitative evaluations indicate over 80% of questions probe for strategic information, and over 90% of summaries produce tactically useful content, being consistently favored over summaries from a large language model integrated with web search. The editing study reveals that minimal information is removed from the generated text (under 2.5%), suggesting that SmartBook provides analysts with a valuable foundation for situation reports

------------

`[2309.00916] BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing <https://arxiv.org/abs/2309.00916>`__ BLSP:基于延续书写行为对齐的自助语言-语音预训练

::

    replaced with revised version Tue, 28 May 2024 14:26:28 GMT
    Submission history From: Chen Wang [view email]
    [v1] Sat, 2 Sep 2023 11:46:05 UTC (566 KB)
    [v2] Tue, 28 May 2024 14:26:28 UTC (8,201 KB)
    Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, Jiajun Zhang

The emergence of large language models (LLMs) has sparked significant interest in extending their remarkable language capabilities to speech. However, modality alignment between speech and text still remains an open problem. Current solutions can be categorized into two strategies. One is a cascaded approach where outputs (tokens or states) of a separately trained speech recognition system are used as inputs for LLMs, which limits their potential in modeling alignment between speech and text. The other is an end-to-end approach that relies on speech instruction data, which is very difficult to collect in large quantities. In this paper, we address these issues and propose the BLSP approach that Bootstraps Language-Speech Pre-training via behavior alignment of continuation writing. We achieve this by learning a lightweight modality adapter between a frozen speech encoder and an LLM, ensuring that the LLM exhibits the same generation behavior regardless of the modality of input: a speech segment or its transcript. The training process can be divided into two steps. The first step prompts an LLM to generate texts with speech transcripts as prefixes, obtaining text continuations. In the second step, these continuations are used as supervised signals to train the modality adapter in an end-to-end manner. We demonstrate that this straightforward process can extend the capabilities of LLMs to speech, enabling speech recognition, speech translation, spoken language understanding, and speech conversation, even in zero-shot cross-lingual scenarios.

------------

`[2309.12551] Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models <https://arxiv.org/abs/2309.12551>`__ 是否可以将文本修改为目标可读性级别?使用零样本大型语言模型的初步调查

::

    replaced with revised version Mon, 27 May 2024 18:05:31 GMT
    Submission history From: Vatsal Raina [view email]
    [v1] Fri, 22 Sep 2023 00:47:18 UTC (9,383 KB)
    [v2] Mon, 27 May 2024 18:05:31 UTC (2,711 KB)
    Asma Farajidizaji, Vatsal Raina, Mark Gales

Text simplification is a common task where the text is adapted to make it easier to understand. Similarly, text elaboration can make a passage more sophisticated, offering a method to control the complexity of reading comprehension tests. However, text simplification and elaboration tasks are limited to only relatively alter the readability of texts. It is useful to directly modify the readability of any text to an absolute target readability level to cater to a diverse audience. Ideally, the readability of readability-controlled generated text should be independent of the source text. Therefore, we propose a novel readability-controlled text modification task. The task requires the generation of 8 versions at various target readability levels for each input text. We introduce novel readability-controlled text modification metrics. The baselines for this task use ChatGPT and Llama-2, with an extension approach introducing a two-step process (generating paraphrases by passing through the language model twice). The zero-shot approaches are able to push the readability of the paraphrases in the desired direction but the final readability remains correlated with the original text's readability. We also find greater drops in semantic and lexical similarity between the source and target texts with greater shifts in the readability.

------------

`[2310.02932] Assessing Large Language Models on Climate Information <https://arxiv.org/abs/2310.02932>`__ 基于气候信息的大型语言模型评估

::

    replaced with revised version Tue, 28 May 2024 15:36:49 GMT
    Submission history From: Jannis Bulian [view email]
    [v1] Wed, 4 Oct 2023 16:09:48 UTC (3,224 KB)
    [v2] Tue, 28 May 2024 15:36:49 UTC (3,408 KB)
    Jannis Bulian, Mike S. Sch\"afer, Afra Amini, Heidi Lam, Massimiliano Ciaramita, Ben Gaiarin, Michelle Chen H\"ubscher, Christian Buck, Niels G. Mede, Markus Leippold, Nadine Strau{\ss}

As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in critically relevant domains. We present a comprehensive evaluation framework, grounded in science communication research, to assess LLM responses to questions about climate change. Our framework emphasizes both presentational and epistemological adequacy, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues. Our evaluation task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel protocol for scalable oversight that relies on AI Assistance and raters with relevant education. We evaluate several recent LLMs on a set of diverse climate questions. Our results point to a significant gap between surface and epistemological qualities of LLMs in the realm of climate communication.

------------

`[2310.05007] MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering <https://arxiv.org/abs/2310.05007>`__ MinPrompt:基于图的小样本问答最小提示数据增强

::

    replaced with revised version Tue, 28 May 2024 04:10:49 GMT
    Submission history From: Xiusi Chen [view email]
    [v1] Sun, 8 Oct 2023 04:44:36 UTC (7,951 KB)
    [v2] Wed, 22 May 2024 11:43:44 UTC (8,839 KB)
    [v3] Tue, 28 May 2024 04:10:49 UTC (8,840 KB)
    Xiusi Chen, Jyun-Yu Jiang, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Wei Wang

Recent advances in few-shot question answering (QA) mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present MinPrompt, a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that MinPrompt is able to achieve comparable or better results than baselines with a high degree of efficiency, bringing consistent improvements in F-1 scores.

------------

`[2310.14735] Unleashing the potential of prompt engineering: a comprehensive review <https://arxiv.org/abs/2310.14735>`__ 释放prompt engineering的潜力:全面综述

::

    replaced with revised version Tue, 28 May 2024 16:38:19 GMT
    Submission history From: Bensen Chen [view email]
    [v1] Mon, 23 Oct 2023 09:15:18 UTC (238 KB)
    [v2] Fri, 27 Oct 2023 14:22:43 UTC (239 KB)
    [v3] Tue, 28 May 2024 16:38:19 UTC (343 KB)
    Banghao Chen, Zhaofeng Zhang, Nicolas Langren\'e, Shengxin Zhu

This comprehensive review explores the transformative potential of prompt engineering within the realm of large language models (LLMs) and multimodal language models (MMLMs). The development of AI, from its inception in the 1950s to the emergence of neural networks and deep learning architectures, has culminated in sophisticated LLMs like GPT-4 and BERT, as well as MMLMs like DALL-E and CLIP. These models have revolutionized tasks in diverse fields such as workplace automation, healthcare, and education. Prompt engineering emerges as a crucial technique to maximize the utility and accuracy of these models. This paper delves into both foundational and advanced methodologies of prompt engineering, including techniques like Chain of Thought, Self-consistency, and Generated Knowledge, which significantly enhance model performance. Additionally, it examines the integration of multimodal data through innovative approaches such as Multi-modal Prompt Learning (MaPLe), Conditional Prompt Learning, and Context Optimization. Critical to this discussion is the aspect of AI security, particularly adversarial attacks that exploit vulnerabilities in prompt engineering. Strategies to mitigate these risks and enhance model robustness are thoroughly reviewed. The evaluation of prompt methods is addressed through both subjective and objective metrics, ensuring a robust analysis of their efficacy. This review underscores the pivotal role of prompt engineering in advancing AI capabilities, providing a structured framework for future research and application.

------------

`[2311.00117] BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B <https://arxiv.org/abs/2311.00117>`__ BadLlama:低成本移除Llama 2-Chat 13B的安全微调

::

    replaced with revised version Tue, 28 May 2024 10:33:03 GMT
    Submission history From: Pranav Gade [view email]
    [v1] Tue, 31 Oct 2023 19:45:15 UTC (7,171 KB)
    [v2] Thu, 21 Mar 2024 18:40:32 UTC (7,171 KB)
    [v3] Tue, 28 May 2024 10:33:03 UTC (7,171 KB)
    Pranav Gade and Simon Lermen and Charlie Rogers-Smith and Jeffrey Ladish

Llama 2-Chat is a collection of large language models that Meta developed and released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output harmful content, we hypothesize that public access to model weights enables bad actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's capabilities for malicious purposes. We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are released publicly. Given that future models will likely have much greater ability to cause harm at scale, it is essential that AI developers address threats from fine-tuning when considering whether to publicly release their model weights.

------------

`[2311.02790] CausalCite: A Causal Formulation of Paper Citations <https://arxiv.org/abs/2311.02790>`__ CausalCite:论文引用的因果表述

::

    replaced with revised version Mon, 27 May 2024 20:31:14 GMT
    Submission history From: Zhijing Jin [view email]
    [v1] Sun, 5 Nov 2023 23:09:39 UTC (399 KB)
    [v2] Thu, 11 Jan 2024 01:26:28 UTC (440 KB)
    [v3] Mon, 27 May 2024 20:31:14 UTC (441 KB)
    Ishan Kumar, Zhijing Jin, Ehsan Mokhtarian, Siyuan Guo, Yuen Chen, Mrinmaya Sachan, Bernhard Sch\"olkopf

Citation count of a paper is a commonly used proxy for evaluating the significance of a paper in the scientific community. Yet citation measures are widely criticized for failing to accurately reflect the true impact of a paper. Thus, we propose CausalCite, a new way to measure the significance of a paper by assessing the causal impact of the paper on its follow-up papers. CausalCite is based on a novel causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. TextMatch encodes each paper using text embeddings from large language models (LLMs), extracts similar samples by cosine similarity, and synthesizes a counterfactual sample as the weighted average of similar papers according to their similarity values. We demonstrate the effectiveness of CausalCite on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various subfields of AI. We also provide a set of findings that can serve as suggested ways for future researchers to use our metric for a better understanding of the quality of a paper. Our code is available at this https URL.

------------

`[2401.06532] INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning <https://arxiv.org/abs/2401.06532>`__ :通过指令调优释放大型语言模型在搜索中的力量

::

    replaced with revised version Tue, 28 May 2024 13:58:32 GMT
    Submission history From: Yutao Zhu [view email]
    [v1] Fri, 12 Jan 2024 12:10:28 UTC (575 KB)
    [v2] Mon, 19 Feb 2024 10:26:05 UTC (639 KB)
    [v3] Tue, 28 May 2024 13:58:32 UTC (641 KB)
    Yutao Zhu, Peitian Zhang, Chenghao Zhang, Yifei Chen, Binyu Xie, Zheng Liu, Ji-Rong Wen, and Zhicheng Dou

Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating a comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 20 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly available LLMs, such as LLaMA, Mistral, and Phi, in IR tasks. Furthermore, we conduct extensive experiments to analyze the effects of instruction design, template diversity, few-shot demonstrations, and the volume of instructions on performance. We make our dataset and the fine-tuned models publicly accessible at this https URL.

------------

`[2401.13601] MM-LLMs: Recent Advances in MultiModal Large Language Models <https://arxiv.org/abs/2401.13601>`__ MM-LLMs:多模态大型语言模型的最新进展

::

    replaced with revised version Tue, 28 May 2024 05:36:23 GMT
    Submission history From: Duzhen Zhang [view email]
    [v1] Wed, 24 Jan 2024 17:10:45 UTC (5,256 KB)
    [v2] Thu, 25 Jan 2024 03:46:15 UTC (5,256 KB)
    [v3] Sat, 17 Feb 2024 09:17:55 UTC (2,432 KB)
    [v4] Tue, 20 Feb 2024 09:51:37 UTC (2,432 KB)
    [v5] Tue, 28 May 2024 05:36:23 UTC (1,056 KB)
    Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, Dong Yu

In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.

------------

`[2402.05119] A Closer Look at the Limitations of Instruction Tuning <https://arxiv.org/abs/2402.05119>`__ 仔细看看指令调优的局限性

::

    replaced with revised version Tue, 28 May 2024 01:55:38 GMT
    Submission history From: Sreyan Ghosh [view email]
    [v1] Sat, 3 Feb 2024 04:45:25 UTC (5,575 KB)
    [v2] Fri, 16 Feb 2024 22:16:07 UTC (5,780 KB)
    [v3] Wed, 28 Feb 2024 14:47:08 UTC (5,782 KB)
    [v4] Tue, 28 May 2024 01:55:38 UTC (5,776 KB)
    Sreyan Ghosh and Chandra Kiran Reddy Evuru and Sonal Kumar and Ramaneswaran S and Deepali Aneja and Zeyu Jin and Ramani Duraiswami and Dinesh Manocha

Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating responses. (4) Popular methods to improve IT do not lead to performance improvements over a simple LoRA fine-tuned model. Our findings reveal that responses generated solely from pre-trained knowledge consistently outperform responses by models that learn any form of new knowledge from IT on open-source datasets. We hope the insights and challenges revealed in this paper inspire future work in related directions.

------------

`[2402.07776] TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection <https://arxiv.org/abs/2402.07776>`__ TELLER:一种可解释、可泛化、可控制的假新闻检测可信框架

::

    replaced with revised version Tue, 28 May 2024 06:14:34 GMT
    Submission history From: Hui Liu [view email]
    [v1] Mon, 12 Feb 2024 16:41:54 UTC (584 KB)
    [v2] Tue, 28 May 2024 06:14:34 UTC (584 KB)
    Hui Liu, Wenya Wang, Haoru Li, Haoliang Li

The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia. While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs). To address this challenge, we propose {\methodname}, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models. This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above. The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms. Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfulness of the input news across diverse domains and enhancing transparency in the decision-making process. Finally, we present comprehensive evaluation results on four datasets, demonstrating the feasibility and trustworthiness of our proposed framework. Our implementation is available at \url{this https URL}.

------------

`[2402.08219] BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models <https://arxiv.org/abs/2402.08219>`__ bbox适配器:适应黑盒大型语言模型的轻量级适配器

::

    replaced with revised version Tue, 28 May 2024 17:04:45 GMT
    Submission history From: Haotian Sun [view email]
    [v1] Tue, 13 Feb 2024 05:15:46 UTC (13,419 KB)
    [v2] Tue, 28 May 2024 17:04:45 UTC (22,677 KB)
    Haotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, Bo Dai

Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively.

------------

`[2402.09552] STEER: Assessing the Economic Rationality of Large Language Models <https://arxiv.org/abs/2402.09552>`__ STEER:评估大型语言模型的经济合理性

::

    replaced with revised version Tue, 28 May 2024 16:27:56 GMT
    Submission history From: Narun Raman [view email]
    [v1] Wed, 14 Feb 2024 20:05:26 UTC (5,177 KB)
    [v2] Tue, 28 May 2024 16:27:56 UTC (6,180 KB)
    Narun Raman, Taylor Lundy, Samuel Amouyal, Yoav Levine, Kevin Leyton-Brown, Moshe Tennenholtz

There is increasing interest in using LLMs as decision-making "agents." Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions -- and more broadly, determining whether an LLM agent is reliable enough to be trusted -- requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained "elements" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a "STEER report card." Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characterizing the both current state of the art and the impact of different model sizes on models' ability to exhibit rational behavior.

------------

`[2402.10866] EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models <https://arxiv.org/abs/2402.10866>`__ EcoRank:基于大型语言模型的预算约束文本重排序

::

    replaced with revised version Tue, 28 May 2024 02:34:57 GMT
    Submission history From: Muhammad Shihab Rashid [view email]
    [v1] Fri, 16 Feb 2024 18:03:42 UTC (211 KB)
    [v2] Tue, 28 May 2024 02:34:57 UTC (354 KB)
    Muhammad Shihab Rashid, Jannat Ara Meem, Yue Dong, Vagelis Hristidis

Large Language Models (LLMs) have achieved state-of-the-art performance in text re-ranking. This process includes queries and candidate passages in the prompts, utilizing pointwise, listwise, and pairwise prompting strategies. A limitation of these ranking strategies with LLMs is their cost: the process can become expensive due to API charges, which are based on the number of input and output tokens. We study how to maximize the re-ranking performance given a budget, by navigating the vast search spaces of prompt choices, LLM APIs, and budget splits. We propose a suite of budget-constrained methods to perform text re-ranking using a set of LLM APIs. Our most efficient method, called EcoRank, is a two-layered pipeline that jointly optimizes decisions regarding budget allocation across prompt strategies and LLM APIs. Our experimental results on four popular QA and passage reranking datasets show that EcoRank outperforms other budget-aware supervised and unsupervised baselines.

------------

`[2402.10958] Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts <https://arxiv.org/abs/2402.10958>`__ 相对偏好优化:通过对比相同和不同提示的响应来增强LLM对齐

::

    replaced with revised version Mon, 27 May 2024 20:05:03 GMT
    Submission history From: Mingyuan Zhou [view email]
    [v1] Mon, 12 Feb 2024 22:47:57 UTC (512 KB)
    [v2] Mon, 27 May 2024 20:05:03 UTC (494 KB)
    Yueqin Yin, Zhendong Wang, Yi Gu, Hai Huang, Weizhu Chen, Mingyuan Zhou

In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to leverage insights from a more varied set of prompts. Through empirical tests, including dialogue and summarization tasks, and evaluations using the AlpacaEval2.0 leaderboard, RPO has demonstrated a superior ability to align LLMs with user preferences and to improve their adaptability during the training process. Our code can be viewed at this https URL

------------

`[2402.12948] GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick <https://arxiv.org/abs/2402.12948>`__ GumbelSoft:基于GumbelMax-trick的多样化语言模型水印

::

    replaced with revised version Tue, 28 May 2024 04:58:41 GMT
    Submission history From: Jiayi Fu [view email]
    [v1] Tue, 20 Feb 2024 12:05:47 UTC (8,228 KB)
    [v2] Sun, 25 Feb 2024 04:26:13 UTC (8,228 KB)
    [v3] Tue, 28 May 2024 04:58:41 UTC (8,236 KB)
    Jiayi Fu, Xuandong Zhao, Ruihan Yang, Yuansen Zhang, Jiangjie Chen, Yanghua Xiao

Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing other decoding-based watermarking methods by a minimum of 0.1.

------------

`[2402.13669] Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning <https://arxiv.org/abs/2402.13669>`__ 自蒸馏弥合语言模型微调中的分布差距

::

    replaced with revised version Tue, 28 May 2024 06:39:17 GMT
    Submission history From: Zhaorui Yang [view email]
    [v1] Wed, 21 Feb 2024 10:06:08 UTC (8,001 KB)
    [v2] Tue, 28 May 2024 06:39:17 UTC (7,975 KB)
    Zhaorui Yang, Tianyu Pang, Haozhe Feng, Han Wang, Wei Chen, Minfeng Zhu, Qian Liu

The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at this https URL.

------------

`[2402.16107] Knowledge Fusion of Chat LLMs: A Preliminary Technical Report <https://arxiv.org/abs/2402.16107>`__ 知识融合的Chat LLMs:初步技术报告

::

    replaced with revised version Tue, 28 May 2024 09:59:16 GMT
    Submission history From: Fanqi Wan [view email]
    [v1] Sun, 25 Feb 2024 15:11:58 UTC (243 KB)
    [v2] Tue, 27 Feb 2024 04:48:36 UTC (243 KB)
    [v3] Sun, 3 Mar 2024 07:21:36 UTC (243 KB)
    [v4] Mon, 27 May 2024 10:16:51 UTC (243 KB)
    [v5] Tue, 28 May 2024 09:59:16 UTC (243 KB)
    Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang, Wei Bi

Recently, FuseLLM introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the FuseLLM framework to realize the fusion of chat LLMs, resulting in FusionChat. FusionChat comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs to derive multiple target LLMs of identical structure and size via lightweight fine-tuning. Then, these target LLMs are merged within the parameter space, wherein we propose a novel method for determining the merging weights based on the variation ratio of parameter matrices before and after fine-tuning. We validate our approach using three prominent chat LLMs with diverse architectures and scales, namely NH2-Mixtral-8x7B, NH2-Solar-10.7B, and OpenChat-3.5-7B. Experimental results spanning various chat domains demonstrate the superiority of FusionChat-7B across a broad spectrum of chat LLMs at 7B and 34B scales, even surpassing GPT-3.5 (March) and approaching Mixtral-8x7B-Instruct.

------------

`[2402.16159] DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem <https://arxiv.org/abs/2402.16159>`__ DistALANER:开源软件生态系统中远程监督主动学习增强的命名实体识别

::

    replaced with revised version Tue, 28 May 2024 07:54:44 GMT
    Submission history From: Somnath Banerjee [view email]
    [v1] Sun, 25 Feb 2024 17:40:49 UTC (9,323 KB)
    [v2] Mon, 11 Mar 2024 08:11:08 UTC (2,142 KB)
    [v3] Fri, 15 Mar 2024 18:29:52 UTC (2,142 KB)
    [v4] Tue, 28 May 2024 07:54:44 UTC (1,389 KB)
    Somnath Banerjee, Avik Dutta, Aaditya Agrawal, Rima Hazra, Animesh Mukherjee

With the AI revolution in place, the trend for building automated systems to support professionals in different domains such as the open source software systems, healthcare systems, banking systems, transportation systems and many others have become increasingly prominent. A crucial requirement in the automation of support tools for such systems is the early identification of named entities, which serves as a foundation for developing specialized functionalities. However, due to the specific nature of each domain, different technical terminologies and specialized languages, expert annotation of available data becomes expensive and challenging. In light of these challenges, this paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our model significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.

------------

`[2403.00418] LLMs for Targeted Sentiment in News Headlines: Exploring the Descriptive-Prescriptive Dilemma <https://arxiv.org/abs/2403.00418>`__ 新闻标题中目标情绪的llm:探索描述-规定的困境

::

    replaced with revised version Tue, 28 May 2024 11:04:32 GMT
    Submission history From: Laura Majer [view email]
    [v1] Fri, 1 Mar 2024 10:10:34 UTC (7,756 KB)
    [v2] Tue, 28 May 2024 11:04:32 UTC (7,839 KB)
    Jana Juro\v{s}, Laura Majer, Jan \v{S}najder

News headlines often evoke sentiment by intentionally portraying entities in particular ways, making targeted sentiment analysis (TSA) of headlines a worthwhile but difficult task. Due to its subjectivity, creating TSA datasets can involve various annotation paradigms, from descriptive to prescriptive, either encouraging or limiting subjectivity. LLMs are a good fit for TSA due to their broad linguistic and world knowledge and in-context learning abilities, yet their performance depends on prompt design. In this paper, we compare the accuracy of state-of-the-art LLMs and fine-tuned encoder models for TSA of news headlines using descriptive and prescriptive datasets across several languages. Exploring the descriptive--prescriptive continuum, we analyze how performance is affected by prompt prescriptiveness, ranging from plain zero-shot to elaborate few-shot prompts. Finally, we evaluate the ability of LLMs to quantify uncertainty via calibration error and comparison to human label variation. We find that LLMs outperform fine-tuned encoders on descriptive datasets, while calibration and F1-score generally improve with increased prescriptiveness, yet the optimal level varies.

------------

`[2403.03121] Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution <https://arxiv.org/abs/2403.03121>`__ 愤怒的男人，悲伤的女人:大型语言模型反映情绪归因中的性别刻板印象

::

    replaced with revised version Tue, 28 May 2024 14:43:52 GMT
    Submission history From: Flor Miriam Plaza-del-Arco [view email]
    [v1] Tue, 5 Mar 2024 17:04:05 UTC (8,278 KB)
    [v2] Mon, 18 Mar 2024 11:04:44 UTC (8,278 KB)
    [v3] Tue, 28 May 2024 14:43:52 UTC (8,279 KB)
    Flor Miriam Plaza-del-Arco, Amanda Cercas Curry, Alba Curry, Gavin Abercrombie, Dirk Hovy

Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.

------------

`[2403.07378] SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression <https://arxiv.org/abs/2403.07378>`__ SVD-LLM:基于截断感知奇异值分解的大型语言模型压缩

::

    replaced with revised version Tue, 28 May 2024 13:41:26 GMT
    Submission history From: Xin Wang [view email]
    [v1] Tue, 12 Mar 2024 07:31:18 UTC (318 KB)
    [v2] Fri, 15 Mar 2024 02:59:10 UTC (328 KB)
    [v3] Mon, 1 Apr 2024 15:04:15 UTC (382 KB)
    [v4] Tue, 28 May 2024 13:41:26 UTC (360 KB)
    Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang

The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the compressed weight after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation under high compression ratios. We evaluate SVD-LLM on a total of 10 datasets and eight models from three different LLM families at four different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios.

------------

`[2403.14472] Detoxifying Large Language Models via Knowledge Editing <https://arxiv.org/abs/2403.14472>`__ 基于知识编辑的大型语言模型解毒

::

    replaced with revised version Tue, 28 May 2024 09:11:25 GMT
    Submission history From: Ningyu Zhang [view email]
    [v1] Thu, 21 Mar 2024 15:18:30 UTC (9,785 KB)
    [v2] Thu, 28 Mar 2024 15:24:17 UTC (9,786 KB)
    [v3] Sat, 13 Apr 2024 13:39:50 UTC (9,788 KB)
    [v4] Sun, 19 May 2024 10:11:58 UTC (9,785 KB)
    [v5] Tue, 28 May 2024 09:11:25 UTC (9,785 KB)
    Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen

This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments with several knowledge editing approaches, indicating that knowledge editing has the potential to detoxify LLMs with a limited impact on general performance efficiently. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxifying approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code and benchmark are available at this https URL.

------------

`[2404.11999] Token-level Direct Preference Optimization <https://arxiv.org/abs/2404.11999>`__ 令牌级直接偏好优化

::

    replaced with revised version Tue, 28 May 2024 14:37:40 GMT
    Submission history From: Yongcheng Zeng [view email]
    [v1] Thu, 18 Apr 2024 08:49:38 UTC (305 KB)
    [v2] Tue, 28 May 2024 14:37:40 UTC (354 KB)
    Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, Jun Wang

Fine-tuning pre-trained Large Language Models (LLMs) is essential to align them with human values and intentions. This process often utilizes methods like pairwise comparisons and KL divergence against a reference LLM, focusing on the evaluation of full answers generated by the models. However, the generation of these responses occurs in a token level, following a sequential, auto-regressive fashion. In this paper, we introduce Token-level Direct Preference Optimization (TDPO), a novel approach to align LLMs with human preferences by optimizing policy at the token level. Unlike previous methods, which face challenges in divergence efficiency, TDPO incorporates forward KL divergence constraints for each token, improving alignment and diversity. Utilizing the Bradley-Terry model for a token-based reward system, TDPO enhances the regulation of KL divergence, while preserving simplicity without the need for explicit reward modeling. Experimental results across various text tasks demonstrate TDPO's superior performance in balancing alignment with generation diversity. Notably, fine-tuning with TDPO strikes a better balance than DPO in the controlled sentiment generation and single-turn dialogue datasets, and significantly improves the quality of generated responses compared to both DPO and PPO-based RLHF methods. Our code is open-sourced at this https URL.

------------

`[2405.14555] Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models <https://arxiv.org/abs/2405.14555>`__ 细微偏差需要更细微的度量:评估大型语言模型中的代表性和亲和性偏差的双重指标

::

    replaced with revised version Tue, 28 May 2024 03:05:45 GMT
    Submission history From: Ali Emami Dr. [view email]
    [v1] Thu, 23 May 2024 13:35:34 UTC (8,747 KB)
    [v2] Sat, 25 May 2024 15:38:38 UTC (8,747 KB)
    [v3] Tue, 28 May 2024 03:05:45 UTC (8,746 KB)
    Abhishek Kumar, Sarfaroz Yunusov, and Ali Emami

Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models' outputs toward particular social narratives. This study addresses two such biases within LLMs: representative bias, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and affinity bias, reflecting the models' evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to `bias fingerprints'. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions.

------------

`[2405.15077] Eliciting Informative Text Evaluations with Large Language Models <https://arxiv.org/abs/2405.15077>`__ 基于大型语言模型的信息文本评估

::

    replaced with revised version Tue, 28 May 2024 17:55:40 GMT
    Submission history From: Shengwei Xu [view email]
    [v1] Thu, 23 May 2024 21:56:12 UTC (722 KB)
    [v2] Tue, 28 May 2024 17:55:40 UTC (707 KB)
    Yuxuan Lu, Shengwei Xu, Yichi Zhang, Yuqing Kong, Grant Schoenebeck

Peer prediction mechanisms motivate high-quality feedback with provable guarantees. However, current methods only apply to rather simple reports, like multiple-choice or scalar numbers. We aim to broaden these techniques to the larger domain of text-based reports, drawing on the recent developments in large language models. This vastly increases the applicability of peer prediction mechanisms as textual feedback is the norm in a large variety of feedback channels: peer reviews, e-commerce customer reviews, and comments on social media.
We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM) and the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms utilize LLMs as predictors, mapping from one agent's report to a prediction of her peer's report. Theoretically, we show that when the LLM prediction is sufficiently accurate, our mechanisms can incentivize high effort and truth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we confirm the efficacy of our mechanisms through experiments conducted on two real datasets: the Yelp review dataset and the ICLR OpenReview dataset. We highlight the results that on the ICLR dataset, our mechanisms can differentiate three quality levels -- human-written reviews, GPT-4-generated reviews, and GPT-3.5-generated reviews in terms of expected scores. Additionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.

------------

`[2405.16277] Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge <https://arxiv.org/abs/2405.16277>`__ 描绘模糊性:Winograd模式挑战的视觉扭曲

::

    replaced with revised version Tue, 28 May 2024 03:13:46 GMT
    Submission history From: Ali Emami Dr. [view email]
    [v1] Sat, 25 May 2024 15:28:22 UTC (8,320 KB)
    [v2] Tue, 28 May 2024 03:13:46 UTC (8,320 KB)
    Brendan Park, Madeline Janecek, Naser Ezzati-Jivan, Yifeng Li, and Ali Emami

Large Language Models (LLMs) have demonstrated remarkable success in tasks like the Winograd Schema Challenge (WSC), showcasing advanced textual common-sense reasoning. However, applying this reasoning to multimodal domains, where understanding text and images together is essential, remains a substantial challenge. To address this, we introduce WinoVis, a novel dataset specifically designed to probe text-to-image models on pronoun disambiguation within multimodal contexts. Utilizing GPT-4 for prompt generation and Diffusion Attentive Attribution Maps (DAAM) for heatmap analysis, we propose a novel evaluation framework that isolates the models' ability in pronoun disambiguation from other visual processing challenges. Evaluation of successive model versions reveals that, despite incremental advancements, Stable Diffusion 2.0 achieves a precision of 56.7% on WinoVis, only marginally surpassing random guessing. Further error analysis identifies important areas for future research aimed at advancing text-to-image models in their ability to interpret and interact with the complex visual world.

------------

`[2405.16295] Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data <https://arxiv.org/abs/2405.16295>`__ 开源语言模型在医学文本数据汇总中的对比分析

::

    replaced with revised version Tue, 28 May 2024 02:22:20 GMT
    Submission history From: Yuhao Chen [view email]
    [v1] Sat, 25 May 2024 16:16:22 UTC (758 KB)
    [v2] Tue, 28 May 2024 02:22:20 UTC (754 KB)
    Yuhao Chen, Zhimu Wang, Bo Wen, Farhana Zulkernine

Unstructured text in medical notes and dialogues contains rich information. Recent advancements in Large Language Models (LLMs) have demonstrated superior performance in question answering and summarization tasks on unstructured text data, outperforming traditional text analysis approaches. However, there is a lack of scientific studies in the literature that methodically evaluate and report on the performance of different LLMs, specifically for domain-specific data such as medical chart notes. We propose an evaluation approach to analyze the performance of open-source LLMs such as Llama2 and Mistral for medical summarization tasks, using GPT-4 as an assessor. Our innovative approach to quantitative evaluation of LLMs can enable quality control, support the selection of effective LLMs for specific tasks, and advance knowledge discovery in digital health.

------------

`[2405.16720] Large Scale Knowledge Washing <https://arxiv.org/abs/2405.16720>`__ 大规模知识清洗

::

    replaced with revised version Tue, 28 May 2024 15:48:12 GMT
    Submission history From: Yu Wang [view email]
    [v1] Sun, 26 May 2024 23:29:49 UTC (483 KB)
    [v2] Tue, 28 May 2024 15:48:12 UTC (462 KB)
    Yu Wang, Ruihan Wu, Zexue He, Xiusi Chen, Julian McAuley

Large language models show impressive abilities in memorizing world knowledge, which leads to concerns regarding memorization of private information, toxic or sensitive knowledge, and copyrighted content. We introduce the problem of Large Scale Knowledge Washing, focusing on unlearning an extensive amount of factual knowledge. Previous unlearning methods usually define the reverse loss and update the model via backpropagation, which may affect the model's fluency and reasoning ability or even destroy the model due to extensive training with the reverse loss. Existing works introduce additional data from downstream tasks to prevent the model from losing capabilities, which requires downstream task awareness. Controlling the tradeoff of unlearning and maintaining existing capabilities is also challenging. To this end, we propose LAW (Large Scale Washing) to update the MLP layers in decoder-only large language models to perform knowledge washing, as inspired by model editing methods and based on the hypothesis that knowledge and reasoning are disentanglable. We derive a new objective with the knowledge to be unlearned to update the weights of certain MLP layers. Experimental results demonstrate the effectiveness of LAW in forgetting target knowledge while maintaining reasoning ability. The code will be open-sourced at this https URL.

------------

`[2405.16806] Entity Alignment with Noisy Annotations from Large Language Models <https://arxiv.org/abs/2405.16806>`__ 与大型语言模型噪声注释的实体对齐

::

    replaced with revised version Tue, 28 May 2024 07:50:18 GMT
    Submission history From: Shengyuan Chen [view email]
    [v1] Mon, 27 May 2024 03:52:55 UTC (169 KB)
    [v2] Tue, 28 May 2024 07:50:18 UTC (169 KB)
    Shengyuan Chen, Qinggang Zhang, Junnan Dong, Wen Hua, Qing Li, Xiao Huang

Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs. While existing methods heavily rely on human-generated labels, it is prohibitively expensive to incorporate cross-domain experts for annotation in real-world scenarios. The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information. However, it is nontrivial to directly apply LLMs for EA since the annotation space in real-world KGs is large. LLMs could also generate noisy labels that may mislead the alignment. To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA. Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure. Moreover, we introduce an unsupervised label refiner to continuously enhance label accuracy through in-depth probabilistic reasoning. We iteratively optimize the policy based on the feedback from a base EA model. Extensive experiments demonstrate the advantages of LLM4EA on four benchmark datasets in terms of effectiveness, robustness, and efficiency. Codes are available via this https URL.

------------

`[2310.18491] Publicly-Detectable Watermarking for Language Models <https://arxiv.org/abs/2310.18491>`__ 面向语言模型的可公开检测水印

::

    replaced with revised version Tue, 28 May 2024 06:10:45 GMT
    Submission history From: Jaiden Fairoze [view email]
    [v1] Fri, 27 Oct 2023 21:08:51 UTC (1,038 KB)
    [v2] Mon, 27 May 2024 09:24:16 UTC (420 KB)
    [v3] Tue, 28 May 2024 06:10:45 UTC (420 KB)
    Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody and Mingyuan Wang

We present a highly detectable, trustless watermarking scheme for LLMs: the detection algorithm contains no secret information, and it is executable by anyone. We embed a publicly-verifiable cryptographic signature into LLM output using rejection sampling. We prove that our scheme is cryptographically correct, sound, and distortion-free. We make novel uses of error-correction techniques to overcome periods of low entropy, a barrier for all prior watermarking schemes. We implement our scheme and make empirical measurements over open models in the 2.7B to 70B parameter range. Our experiments suggest that our formal claims are met in practice.

------------

`[2312.05516] Stateful Large Language Model Serving with Pensieve <https://arxiv.org/abs/2312.05516>`__ 基于Pensieve的有状态大型语言模型

::

    replaced with revised version Tue, 28 May 2024 04:34:37 GMT
    Submission history From: Lingfan Yu [view email]
    [v1] Sat, 9 Dec 2023 09:55:07 UTC (307 KB)
    [v2] Tue, 28 May 2024 04:34:37 UTC (479 KB)
    Lingfan Yu, Jinyang Li

Large Language Models (LLMs) are wildly popular today and it is important to serve them efficiently. Existing LLM serving systems are stateless across requests. Consequently, when LLMs are used in the common setting of multi-turn conversations, a growing log of the conversation history must be processed alongside any request by the serving system at each turn, resulting in repeated processing.
In this paper, we design Pensieve, a system optimized for multi-turn conversation LLM serving. Pensieve maintains the conversation state across requests by caching previously processed history to avoid duplicate processing. Pensieve's multi-tier caching strategy can utilize both GPU and CPU memory to efficiently store and retrieve cached data. Pensieve also generalizes the recent PagedAttention kernel to support attention between multiple input tokens with a GPU cache spread over non-contiguous memory. Our evaluation shows that Pensieve can achieve 13-58% more throughput compared to vLLM and TensorRT-LLM and significantly reduce latency.

------------

`[2402.01821] Human-like Category Learning by Injecting Ecological Priors from Large Language Models into Neural Networks <https://arxiv.org/abs/2402.01821>`__ 通过将大型语言模型的生态先验注入神经网络进行类人类别学习

::

    replaced with revised version Tue, 28 May 2024 07:40:53 GMT
    Submission history From: Akshay Kumar Jagadish [view email]
    [v1] Fri, 2 Feb 2024 16:32:04 UTC (484 KB)
    [v2] Tue, 28 May 2024 07:40:53 UTC (1,044 KB)
    Akshay K. Jagadish, Julian Coda-Forno, Mirko Thalmann, Eric Schulz, and Marcel Binz

Ecological rationality refers to the notion that humans are rational agents adapted to their environment. However, testing this theory remains challenging due to two reasons: the difficulty in defining what tasks are ecologically valid and building rational models for these tasks. In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge. We tackle the second challenge by deriving rational agents adapted to these tasks using the framework of meta-learning, leading to a class of models called ecologically rational meta-learned inference (ERMI). ERMI quantitatively explains human data better than seven other cognitive models in two different experiments. It additionally matches human behavior on a qualitative level: (1) it finds the same tasks difficult that humans find difficult, (2) it becomes more reliant on an exemplar-based strategy for assigning categories with learning, and (3) it generalizes to unseen stimuli in a human-like way. Furthermore, we show that ERMI's ecologically valid priors allow it to achieve state-of-the-art performance on the OpenML-CC18 classification benchmark.

------------

`[2402.02314] Selecting Large Language Model to Fine-tune via Rectified Scaling Law <https://arxiv.org/abs/2402.02314>`__ 基于修正尺度律的大型语言模型微调选择

::

    replaced with revised version Tue, 28 May 2024 16:16:42 GMT
    Submission history From: Haowei Lin [view email]
    [v1] Sun, 4 Feb 2024 01:55:00 UTC (9,998 KB)
    [v2] Mon, 27 May 2024 15:11:22 UTC (6,269 KB)
    [v3] Tue, 28 May 2024 16:16:42 UTC (6,269 KB)
    Haowei Lin, Baizhou Huang, Haotian Ye, Qinyu Chen, Zihao Wang, Sujian Li, Jianzhu Ma, Xiaojun Wan, James Zou, Yitao Liang

The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, we find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection. The project page is available at this http URL.

------------

`[2402.11867] LoRA Training in the NTK Regime has No Spurious Local Minima <https://arxiv.org/abs/2402.11867>`__ NTK模式下的LoRA训练没有伪局部最小值

::

    replaced with revised version Tue, 28 May 2024 07:05:05 GMT
    Submission history From: Uijeong Jang [view email]
    [v1] Mon, 19 Feb 2024 06:22:09 UTC (2,547 KB)
    [v2] Mon, 27 May 2024 16:35:26 UTC (2,659 KB)
    [v3] Tue, 28 May 2024 07:05:05 UTC (2,660 KB)
    Uijeong Jang, Jason D. Lee, Ernest K. Ryu

Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.

------------

`[2404.02948] PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models <https://arxiv.org/abs/2404.02948>`__ PiSSA:大型语言模型的主奇异值和奇异向量自适应

::

    replaced with revised version Tue, 28 May 2024 14:19:33 GMT
    Submission history From: Meng Fanxu [view email]
    [v1] Wed, 3 Apr 2024 15:06:43 UTC (419 KB)
    [v2] Sun, 14 Apr 2024 15:24:10 UTC (457 KB)
    [v3] Tue, 28 May 2024 14:19:33 UTC (5,562 KB)
    Fanxu Meng, Zhaohui Wang, Muhan Zhang

To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the low-rank adaptation (LoRA) method approximates the model changes $\Delta W \in \mathbb{R}^{m \times n}$ through the product of two matrices $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r \times n}$, where $r \ll \min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA freezes the original model $W$ and updates the "Noise & Zero" adapter, which may lead to slow convergence. To overcome this limitation, we introduce Principal Singular values and Singular vectors Adaptation (PiSSA). PiSSA shares the same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$ with the principal components of the original matrix $W$, and put the remaining components into a residual matrix $W^{res} \in \mathbb{R}^{m \times n}$ which is frozen during fine-tuning. Compared to LoRA, PiSSA updates the principal components while freezing the "residual" parts, allowing faster convergence and enhanced performance. Comparative experiments of PiSSA and LoRA across 12 different models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks, reveal that PiSSA consistently outperforms LoRA under identical experimental setups. On the GSM8K benchmark, Mistral-7B fine-tuned with PiSSA achieves an accuracy of 72.86%, surpassing LoRA's 67.7% by 5.16%. Due to the same architecture, PiSSA is also compatible with quantization to further reduce the memory requirement of fine-tuning. Compared to QLoRA, QPiSSA (PiSSA with 4-bit quantization) exhibits smaller quantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K, QPiSSA attains an accuracy of 86.05%, exceeding the performances of QLoRA at 81.73%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few seconds, presenting a negligible cost for transitioning from LoRA to PiSSA.

------------

`[2404.04102] ROPO: Robust Preference Optimization for Large Language Models <https://arxiv.org/abs/2404.04102>`__ ROPO:大型语言模型的鲁棒偏好优化

::

    replaced with revised version Tue, 28 May 2024 17:11:53 GMT
    Submission history From: Xize Liang [view email]
    [v1] Fri, 5 Apr 2024 13:58:51 UTC (1,919 KB)
    [v2] Tue, 28 May 2024 17:11:53 UTC (1,810 KB)
    Xize Liang, Chao Chen, Shuang Qiu, Jie Wang, Yue Wu, Zhihang Fu, Zhihao Shi, Feng Wu, Jieping Ye

Preference alignment is pivotal for empowering large language models (LLMs) to generate helpful and harmless responses. However, the performance of preference alignment is highly sensitive to the prevalent noise in the preference data. Recent efforts for this problem either marginally alleviate the impact of noise without the ability to actually reduce its presence, or rely on costly teacher LLMs prone to reward misgeneralization. To address these challenges, we propose the RObust Preference Optimization (ROPO) framework, an iterative alignment approach that integrates noise-tolerance and filtering of noisy samples without the aid of external models. Specifically, ROPO iteratively solves a constrained optimization problem, where we dynamically assign a quality-aware weight for each sample and constrain the sum of the weights to the number of samples we intend to retain. For noise-tolerant training and effective noise identification, we derive a robust loss by suppressing the gradients of samples with high uncertainty. We demonstrate both empirically and theoretically that the derived loss is critical for distinguishing noisy samples from clean ones. Furthermore, inspired by our derived loss, we propose a robustness-guided rejection sampling technique to compensate for the potential important information in discarded queries. Experiments on three widely-used datasets with Mistral-7B and Llama-2-7B demonstrate that ROPO significantly outperforms existing preference alignment methods, with its superiority growing as the noise rate increases.

------------

`[2405.00747] Soft Preference Optimization: Aligning Language Models to Expert Distributions <https://arxiv.org/abs/2405.00747>`__ 软偏好优化:语言模型与专家分布对齐

::

    replaced with revised version Mon, 27 May 2024 19:59:00 GMT
    Submission history From: Arsalan Sharifnassab [view email]
    [v1] Tue, 30 Apr 2024 19:48:55 UTC (454 KB)
    [v2] Thu, 23 May 2024 20:32:11 UTC (943 KB)
    [v3] Mon, 27 May 2024 19:59:00 UTC (956 KB)
    Arsalan Sharifnassab, Sina Ghiassian, Saber Salehkaleybar, Surya Kanoria, Dale Schuurmans

We propose Soft Preference Optimization (SPO), a method for aligning generative models, such as Large Language Models (LLMs), with human preferences, without the need for a reward model. SPO optimizes model outputs directly over a preference dataset through a natural loss function that integrates preference loss with a regularization term across the model's entire output distribution rather than limiting it to the preference dataset. Although SPO does not require the assumption of an existing underlying reward model, we demonstrate that, under the Bradley-Terry (BT) model assumption, it converges to a softmax of scaled rewards, with the distribution's "softness" adjustable via the softmax exponent, an algorithm parameter. We showcase SPO's methodology, its theoretical foundation, and its comparative advantages in simplicity, computational efficiency, and alignment precision.

------------

`[2405.12807] FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information <https://arxiv.org/abs/2405.12807>`__ FAdam: Adam是使用对角经验费雪信息的自然梯度优化器

::

    replaced with revised version Tue, 28 May 2024 15:07:28 GMT
    Submission history From: Dongseong Hwang [view email]
    [v1] Tue, 21 May 2024 13:58:17 UTC (166 KB)
    [v2] Thu, 23 May 2024 14:46:39 UTC (167 KB)
    [v3] Sun, 26 May 2024 10:59:04 UTC (166 KB)
    [v4] Tue, 28 May 2024 15:07:28 UTC (167 KB)
    Dongseong Hwang

This paper establishes a mathematical foundation for the Adam optimizer, elucidating its connection to natural gradient descent through Riemannian and information geometry. We rigorously analyze the diagonal empirical Fisher information matrix (FIM) in Adam, clarifying all detailed approximations and advocating for the use of log probability functions as loss, which should be based on discrete distributions, due to the limitations of empirical FIM. Our analysis uncovers flaws in the original Adam algorithm, leading to proposed corrections such as enhanced momentum calculations, adjusted bias corrections, adaptive epsilon, and gradient clipping. We refine the weight decay term based on our theoretical framework. Our modified algorithm, Fisher Adam (FAdam), demonstrates superior performance across diverse domains including LLM, ASR, and VQ-VAE, achieving state-of-the-art results in ASR.

------------

`[2405.14597] Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs <https://arxiv.org/abs/2405.14597>`__ 整数尺度:加快llm细粒度量化的免费午餐

::

    replaced with revised version Tue, 28 May 2024 07:17:47 GMT
    Submission history From: Bo Zhang [view email]
    [v1] Thu, 23 May 2024 14:12:58 UTC (287 KB)
    [v2] Tue, 28 May 2024 07:17:47 UTC (287 KB)
    Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Yifan Lu, Yerui Sun, Lin Ma, Yuchen Xie

We introduce Integer Scale, a novel post-training quantization scheme for large language models that effectively resolves the inference bottleneck in current fine-grained quantization approaches while maintaining similar accuracies. Integer Scale is a free lunch as it requires no extra calibration or fine-tuning which will otherwise incur additional costs. It can be used plug-and-play for most fine-grained quantization methods. Its integration results in at most 1.85x end-to-end speed boost over the original counterpart with comparable accuracy. Additionally, due to the orchestration of the proposed Integer Scale and fine-grained quantization, we resolved the quantization difficulty for Mixtral-8x7B and LLaMA-3 models with negligible performance degradation, and it comes with an end-to-end speed boost of 2.13x, and 2.31x compared with their FP16 versions respectively.

------------

`[2405.17374] Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models <https://arxiv.org/abs/2405.17374>`__ 安全领域导航:在微调大型语言模型中测量风险

::

    replaced with revised version Tue, 28 May 2024 04:58:52 GMT
    Submission history From: ShengYun Peng [view email]
    [v1] Mon, 27 May 2024 17:31:56 UTC (947 KB)
    [v2] Tue, 28 May 2024 04:58:52 UTC (947 KB)
    ShengYun Peng, Pin-Yu Chen, Matthew Hull, Duen Horng Chau

Safety alignment is the key to guiding the behaviors of large language models (LLMs) that are in line with human preferences and restrict harmful behaviors at inference time, but recent studies show that it can be easily compromised by finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin": randomly perturbing model weights maintains the safety level of the original aligned model in its local neighborhood. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community.

------------

`[2310.09676] Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning <https://arxiv.org/abs/2310.09676>`__ 通过预训练和多任务微调掌握基于多模态提示的机器人操作

::

    replaced with revised version Tue, 28 May 2024 01:33:37 GMT
    Submission history From: Jiachen Li [view email]
    [v1] Sat, 14 Oct 2023 22:24:58 UTC (16,411 KB)
    [v2] Tue, 28 May 2024 01:33:37 UTC (4,560 KB)
    Jiachen Li, Qiaozi Gao, Michael Johnston, Xiaofeng Gao, Xuehai He, Suhaila Shakiah, Hangjie Shi, Reza Ghanadan, William Yang Wang

Prompt-based learning has been demonstrated as a compelling paradigm contributing to large language models' tremendous success (LLMs). Inspired by their success in language tasks, existing research has leveraged LLMs in embodied instruction following and task planning. In this work, we tackle the problem of training a robot to understand multimodal prompts, interleaving vision signals with text descriptions. This type of task poses a major challenge to robots' capability to understand the interconnection and complementarity between vision and language signals. In this work, we introduce an effective framework that learns a policy to perform robot manipulation with multimodal prompts from multi-task expert trajectories. Our methods consist of a two-stage training pipeline that performs inverse dynamics pretraining and multi-task finetuning. To facilitate multimodal understanding, we design our multimodal prompt encoder by augmenting a pretrained LM with a residual connection to the visual input and model the dependencies among action dimensions. Empirically, we evaluate the efficacy of our method on the VIMA-BENCH and establish a new state-of-the-art (10% improvement in success rate). Moreover, we demonstrate that our model exhibits remarkable in-context learning ability. Project page: \url{this https URL}.

------------

`[2311.13627] Vamos: Versatile Action Models for Video Understanding <https://arxiv.org/abs/2311.13627>`__ Vamos:视频理解的通用动作模型

::

    replaced with revised version Tue, 28 May 2024 07:18:06 GMT
    Submission history From: Shijie Wang [view email]
    [v1] Wed, 22 Nov 2023 17:44:24 UTC (5,001 KB)
    [v2] Tue, 28 May 2024 07:18:06 UTC (5,499 KB)
    Shijie Wang, Qi Zhao, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, Chen Sun

What makes good representations for video understanding, such as anticipating future activities, or answering video-conditioned questions? While earlier approaches focus on end-to-end learning directly from video pixels, we propose to revisit text-based representations, such as general-purpose video captions, which are interpretable and can be directly consumed by large language models (LLMs). Intuitively, different video understanding tasks may require representations that are complementary and at different granularity. To this end, we propose versatile action models (Vamos), a learning framework powered by a large language model as the ``reasoner'', and can flexibly leverage visual embedding and free-form text descriptions as its input. To interpret the important text evidence for question answering, we generalize the concept bottleneck model to work with tokens and nonlinear models, which uses hard attention to select a small subset of tokens from the free-form text as inputs to the LLM reasoner. We evaluate Vamos on four complementary video understanding benchmarks, Ego4D, NeXT-QA, IntentQA, and EgoSchema, on its capability to model temporal dynamics, encode visual history, and perform reasoning. Surprisingly, we observe that text-based representations consistently achieve competitive performance on all benchmarks, and that visual embeddings provide marginal or no performance improvement, demonstrating the effectiveness of text-based video representation in the LLM era. We also demonstrate that our token bottleneck model is able to select relevant evidence from free-form text, support test-time intervention, and achieves nearly 5 times inference speedup while keeping a competitive question answering performance. Code and models are publicly released at this https URL.

------------

`[2405.14612] Explaining Multi-modal Large Language Models by Analyzing their Vision Perception <https://arxiv.org/abs/2405.14612>`__ 通过分析视觉感知来解释多模态大型语言模型

::

    replaced with revised version Tue, 28 May 2024 11:18:20 GMT
    Submission history From: Loris Giulivi [view email]
    [v1] Thu, 23 May 2024 14:24:23 UTC (23,156 KB)
    [v2] Tue, 28 May 2024 11:18:20 UTC (23,157 KB)
    Loris Giulivi, Giacomo Boracchi

Multi-modal Large Language Models (MLLMs) have demonstrated remarkable capabilities in understanding and generating content across various modalities, such as images and text. However, their interpretability remains a challenge, hindering their adoption in critical applications. This research proposes a novel approach to enhance the interpretability of MLLMs by focusing on the image embedding component. We combine an open-world localization model with a MLLM, thus creating a new architecture able to simultaneously produce text and object localization outputs from the same vision embedding. The proposed architecture greatly promotes interpretability, enabling us to design a novel saliency map to explain any output token, to identify model hallucinations, and to assess model biases through semantic adversarial perturbations.

------------

`[2405.17104] LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding <https://arxiv.org/abs/2405.17104>`__ LLM-Optic:揭示用于通用视觉基础的大型语言模型的能力

::

    replaced with revised version Tue, 28 May 2024 02:17:56 GMT
    Submission history From: Haoyu Zhao [view email]
    [v1] Mon, 27 May 2024 12:23:08 UTC (12,564 KB)
    [v2] Tue, 28 May 2024 02:17:56 UTC (12,564 KB)
    Haoyu Zhao, Wenhang Ge, Ying-cong Chen

Visual grounding is an essential tool that links user-provided text queries with query-specific regions within an image. Despite advancements in visual grounding models, their ability to comprehend complex queries remains limited. To overcome this limitation, we introduce LLM-Optic, an innovative method that utilizes Large Language Models (LLMs) as an optical lens to enhance existing visual grounding models in comprehending complex text queries involving intricate text structures, multiple objects, or object spatial relationships, situations that current models struggle with. LLM-Optic first employs an LLM as a Text Grounder to interpret complex text queries and accurately identify objects the user intends to locate. Then a pre-trained visual grounding model is used to generate candidate bounding boxes given the refined query by the Text Grounder. After that, LLM-Optic annotates the candidate bounding boxes with numerical marks to establish a connection between text and specific image regions, thereby linking two distinct modalities. Finally, it employs a Large Multimodal Model (LMM) as a Visual Grounder to select the marked candidate objects that best correspond to the original text query. Through LLM-Optic, we have achieved universal visual grounding, which allows for the detection of arbitrary objects specified by arbitrary human language input. Importantly, our method achieves this enhancement without requiring additional training or fine-tuning. Extensive experiments across various challenging benchmarks demonstrate that LLM-Optic achieves state-of-the-art zero-shot visual grounding capabilities. Project Page: this https URL.

------------

`[2401.10874] Applications of flow models to the generation of correlated lattice QCD ensembles <https://arxiv.org/abs/2401.10874>`__ 流模型在相关晶格QCD系综生成中的应用

::

    replaced with revised version Tue, 28 May 2024 10:10:26 GMT
    Submission history From: Fernando Romero-López [view email]
    [v1] Fri, 19 Jan 2024 18:33:52 UTC (772 KB)
    [v2] Tue, 28 May 2024 10:10:26 UTC (831 KB)
    Ryan Abbott, Aleksandar Botev, Denis Boyda, Daniel C. Hackett, Gurtej Kanwar, S\'ebastien Racani\`ere, Danilo J. Rezende, Fernando Romero-L\'opez, Phiala E. Shanahan and Julian M. Urban

Machine-learned normalizing flows can be used in the context of lattice quantum field theory to generate statistically correlated ensembles of lattice gauge fields at different action parameters. This work demonstrates how these correlations can be exploited for variance reduction in the computation of observables. Three different proof-of-concept applications are demonstrated using a novel residual flow architecture: continuum limits of gauge theories, the mass dependence of QCD observables, and hadronic matrix elements based on the Feynman-Hellmann approach. In all three cases, it is shown that statistical uncertainties are significantly reduced when machine-learned flows are incorporated as compared with the same calculations performed with uncorrelated ensembles or direct reweighting.

------------

`[2402.01831] Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities <https://arxiv.org/abs/2402.01831>`__ 音频Flamingo:一种具有少样本学习和对话能力的新型音频语言模型

::

    replaced with revised version Tue, 28 May 2024 05:44:53 GMT
    Submission history From: Zhifeng Kong [view email]
    [v1] Fri, 2 Feb 2024 18:58:34 UTC (1,143 KB)
    [v2] Mon, 4 Mar 2024 23:43:42 UTC (1,119 KB)
    [v3] Tue, 28 May 2024 05:44:53 UTC (1,152 KB)
    Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, Bryan Catanzaro

Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is this https URL and the code is open-sourced at this https URL.

------------

`[2402.09179] Instruction Backdoor Attacks Against Customized LLMs <https://arxiv.org/abs/2402.09179>`__ 针对自定义llm的指令后门攻击

::

    replaced with revised version Tue, 28 May 2024 11:36:00 GMT
    Submission history From: Rui Wen [view email]
    [v1] Wed, 14 Feb 2024 13:47:35 UTC (1,567 KB)
    [v2] Thu, 15 Feb 2024 06:15:02 UTC (1,567 KB)
    [v3] Tue, 28 May 2024 11:36:00 UTC (2,928 KB)
    Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, Yang Zhang

The increasing demand for customized Large Language Models (LLMs) has led to the development of solutions like GPTs. These solutions facilitate tailored LLM creation via natural language prompts without coding. However, the trustworthiness of third-party custom versions of LLMs remains an essential concern. In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness. We stress that our attacks do not require fine-tuning or any modification to the backend LLMs, adhering strictly to GPTs development guidelines. We conduct extensive experiments on 6 prominent LLMs and 5 benchmark text classification datasets. The results show that our instruction backdoor attacks achieve the desired attack performance without compromising utility. Additionally, we propose two defense strategies and demonstrate their effectiveness in reducing such attacks. Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs.

------------

-----------
Index (115)
-----------

`[2405.17533] PAE: LLM-based Product Attribute Extraction for E-Commerce Fashion Trends <https://arxiv.org/abs/2405.17533>`__ PAE:基于llm的电子商务时尚趋势产品属性抽取

`[2405.17637] The Economic Implications of Large Language Model Selection on Earnings and Return on Investment: A Decision Theoretic Model <https://arxiv.org/abs/2405.17637>`__ 大型语言模型选择对收益和投资回报率的经济影响:一个决策理论模型

`[2405.17741] LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design <https://arxiv.org/abs/2405.17741>`__ LoRA-Switch:通过系统算法协同提高动态LLM适配器的效率

`[2405.17888] Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment <https://arxiv.org/abs/2405.17888>`__ 从SFT数据中获得更多的能量:来自人类演示的奖励学习改进了SFT对LLM的对齐

`[2405.17902] Boosting Protein Language Models with Negative Sample Mining <https://arxiv.org/abs/2405.17902>`__ 基于负样本挖掘的蛋白质语言模型增强

`[2405.17950] Self-Guiding Exploration for Combinatorial Problems <https://arxiv.org/abs/2405.17950>`__ 组合问题的自我引导探索

`[2405.17956] Hybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives <https://arxiv.org/abs/2405.17956>`__ 混合偏好优化:用辅助目标扩充直接偏好优化

`[2405.18064] Automated Real-World Sustainability Data Generation from Images of Buildings <https://arxiv.org/abs/2405.18064>`__ 从建筑物图像自动生成真实世界的可持续发展数据

`[2405.18166] Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing <https://arxiv.org/abs/2405.18166>`__ 通过特定层编辑保护大型语言模型免受越狱攻击

`[2405.18248] Extreme Value Monte Carlo Tree Search <https://arxiv.org/abs/2405.18248>`__ 极值蒙特卡洛树搜索

`[2405.18272] Metaheuristics and Large Language Models Join Forces: Towards an Integrated Optimization Approach <https://arxiv.org/abs/2405.18272>`__ 元启发式和大型语言模型联合:走向一种集成优化方法

`[2405.18346] Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation <https://arxiv.org/abs/2405.18346>`__ 智能临床文档:利用生成式AI实现以患者为中心的临床记录生成

`[2405.17633] HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs <https://arxiv.org/abs/2405.17633>`__ 发自内心的叙事:与llm在个人故事中追踪同理心和叙事风格

`[2405.17712] CLAIM Your Data: Enhancing Imputation Accuracy with Contextual Large Language Models <https://arxiv.org/abs/2405.17712>`__ 声明数据:利用上下文大型语言模型提高填补精度

`[2405.17740] MobileConvRec: A Conversational Dataset for Mobile Apps Recommendations <https://arxiv.org/abs/2405.17740>`__ MobileConvRec:移动应用推荐对话数据集

`[2405.17743] ORLM: Training Large Language Models for Optimization Modeling <https://arxiv.org/abs/2405.17743>`__ ORLM:面向优化建模的大型语言模型训练

`[2405.17755] XL3M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference <https://arxiv.org/abs/2405.17755>`__ XL3M:基于分段推理的免训练LLM长度扩展框架

`[2405.17804] Detection-Correction Structure via General Language Model for Grammatical Error Correction <https://arxiv.org/abs/2405.17804>`__ 基于通用语言模型的语法纠错检测-纠错结构

`[2405.17822] Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action <https://arxiv.org/abs/2405.17822>`__ Conv-CoA:基于对话行动链改进大型语言模型的开放域问答

`[2405.17830] More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs <https://arxiv.org/abs/2405.17830>`__ 不仅仅是灾难性遗忘:为特定领域的llm集成通用能力

`[2405.17915] Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models <https://arxiv.org/abs/2405.17915>`__ 长上下文根本不长:大型语言模型长依赖数据的勘探者

`[2405.17931] Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment <https://arxiv.org/abs/2405.17931>`__ 在线合并优化器，以提高奖励和减轻税收

`[2405.17969] Knowledge Circuits in Pretrained Transformers <https://arxiv.org/abs/2405.17969>`__ 预训练transformer中的知识电路

`[2405.17974] Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations <https://arxiv.org/abs/2405.17974>`__ 个性化对话生成的最新趋势:数据集、方法和评估综述

`[2405.17977] Aligning to Thousands of Preferences via System Message Generalization <https://arxiv.org/abs/2405.17977>`__ 通过系统消息泛化与数千个首选项对齐

`[2405.17980] Peering into the Mind of Language Models: An Approach for Attribution in Contextual Question Answering <https://arxiv.org/abs/2405.17980>`__ 窥探语言模型的思维:一种上下文问答归因方法

`[2405.17992] fMRI predictors based on language models of increasing complexity recover brain left lateralization <https://arxiv.org/abs/2405.17992>`__ 基于日益复杂的语言模型的fMRI预测器可恢复左脑偏侧化

`[2405.18009] Exploring Context Window of Large Language Models via Decomposed Positional Vectors <https://arxiv.org/abs/2405.18009>`__ 基于分解位置向量的大型语言模型上下文窗口探索

`[2405.18027] TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models <https://arxiv.org/abs/2405.18027>`__ TimeChara:角色扮演大型语言模型的时点字符幻觉评估

`[2405.18028] Edinburgh Clinical NLP at MEDIQA-CORR 2024: Guiding Large Language Models with Hints <https://arxiv.org/abs/2405.18028>`__ MEDIQA-CORR 2024爱丁堡临床NLP:用提示指导大型语言模型

`[2405.18113] Facilitating Multi-Role and Multi-Behavior Collaboration of Large Language Models for Online Job Seeking and Recruiting <https://arxiv.org/abs/2405.18113>`__ 促进在线求职招聘大型语言模型的多角色多行为协作

`[2405.18203] IAPT: Instruction-Aware Prompt Tuning for Large Language Models <https://arxiv.org/abs/2405.18203>`__ IAPT:面向大型语言模型的指令感知提示调优

`[2405.18241] Active Use of Latent Constituency Representation in both Humans and Large Language Models <https://arxiv.org/abs/2405.18241>`__ 潜在选民表示在人类和大型语言模型中的积极使用

`[2405.18344] The Battle of LLMs: A Comparative Study in Conversational QA Tasks <https://arxiv.org/abs/2405.18344>`__ llm之战:会话QA任务的比较研究

`[2405.18359] Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs <https://arxiv.org/abs/2405.18359>`__ 弥合差距:提高llm多语言性能的动态学习策略

`[2405.17440] CataLM: Empowering Catalyst Design Through Large Language Models <https://arxiv.org/abs/2405.17440>`__ CataLM:通过大型语言模型赋能催化剂设计

`[2405.17484] Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation <https://arxiv.org/abs/2405.17484>`__ 通过Householder反射自适应弥合低秩自适应和正交自适应之间的差距

`[2405.17490] Revisit, Extend, and Enhance Hessian-Free Influence Functions <https://arxiv.org/abs/2405.17490>`__ 重新访问、扩展和增强Hessian-Free影响函数

`[2405.17505] Predicting Rental Price of Lane Houses in Shanghai with Machine Learning Methods and Large Language Models <https://arxiv.org/abs/2405.17505>`__ 基于机器学习方法和大型语言模型的上海巷居租金预测

`[2405.17618] Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales <https://arxiv.org/abs/2405.17618>`__ 对称强化学习损失用于不同任务和模型规模的鲁棒学习

`[2405.17627] Salutary Labeling with Zero Human Annotation <https://arxiv.org/abs/2405.17627>`__ 无需人工标注的有益标签

`[2405.17703] Mechanistic Interpretability of Binary and Ternary Transformers <https://arxiv.org/abs/2405.17703>`__ 二元和三元transformer的机制可解释性

`[2405.17767] Linguistic Collapse: Neural Collapse in (Large) Language Models <https://arxiv.org/abs/2405.17767>`__ 语言坍缩:(大型)语言模型的神经坍缩

`[2405.17799] Exploring Activation Patterns of Parameters in Language Models <https://arxiv.org/abs/2405.17799>`__ 语言模型中参数激活模式的探索

`[2405.18039] Large Language Model-Driven Curriculum Design for Mobile Networks <https://arxiv.org/abs/2405.18039>`__ 大型语言模型驱动的移动网络课程设计

`[2405.18100] A Pontryagin Perspective on Reinforcement Learning <https://arxiv.org/abs/2405.18100>`__ 庞特里亚金视角下的强化学习

`[2405.18137] Exploiting LLM Quantization <https://arxiv.org/abs/2405.18137>`__ 利用LLM量化

`[2405.18218] FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models <https://arxiv.org/abs/2405.18218>`__ FinerCut:大型语言模型的细粒度可解释层修剪

`[2405.18376] Empowering Source-Free Domain Adaptation with MLLM-driven Curriculum Learning <https://arxiv.org/abs/2405.18376>`__ 用mllm驱动的课程学习实现无源领域自适应

`[2405.17441] When Large Language Models Meet Optical Networks: Paving the Way for Automation <https://arxiv.org/abs/2405.17441>`__ 当大型语言模型遇到光学网络:为自动化铺平道路

`[2405.17503] Code Repair with LLMs gives an Exploration-Exploitation Tradeoff <https://arxiv.org/abs/2405.17503>`__ 使用llm进行代码修复提供了探索-利用的权衡

`[2405.17728] Facilitating Holistic Evaluations with LLMs: Insights from Scenario-Based Experiments <https://arxiv.org/abs/2405.17728>`__ 用llm促进整体评估:基于场景实验的见解

`[2405.17846] Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs <https://arxiv.org/abs/2405.17846>`__ 基于llm和具身知识图谱的服务机器人安全控制

`[2405.18258] Text-only Synthesis for Image Captioning <https://arxiv.org/abs/2405.18258>`__ 图像描述生成的纯文本合成

`[2405.18386] Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning <https://arxiv.org/abs/2405.18386>`__ Instruction - musicgen:通过指令调优解锁音乐语言模型的文本到音乐编辑

`[2405.17658] Generative Query Reformulation Using Ensemble Prompting, Document Fusion, and Relevance Feedback <https://arxiv.org/abs/2405.17658>`__ 基于集成提示、文档融合和相关反馈的生成式查询重构

`[2405.17890] SLMRec: Empowering Small Language Models for Sequential Recommendation <https://arxiv.org/abs/2405.17890>`__ SLMRec:面向顺序推荐的小型语言模型

`[2405.17439] An Overview of Machine Learning-Enabled Optimization for Reconfigurable Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large Language Models <https://arxiv.org/abs/2405.17439>`__ 可重构智能表面辅助6G网络的机器学习优化综述:从强化学习到大型语言模型

`[2405.18093] Pipette: Automatic Fine-grained Large Language Model Training Configurator for Real-World Clusters <https://arxiv.org/abs/2405.18093>`__ Pipette:面向真实世界集群的自动细粒度大型语言模型训练配置器

`[2308.15030] SwapMoE: Serving Off-the-shelf MoE-based Language Models with Tunable Memory Budget <https://arxiv.org/abs/2308.15030>`__ SwapMoE:提供可调内存预算的基于moe的现成语言模型

`[2402.03824] A call for embodied AI <https://arxiv.org/abs/2402.03824>`__ 对具身人工智能的呼唤

`[2402.18023] Do Large Language Models Mirror Cognitive Language Processing? <https://arxiv.org/abs/2402.18023>`__ 大型语言模型是否反映了认知语言处理?

`[2404.08850] Assessing Economic Viability: A Comparative Analysis of Total Cost of Ownership for Domain-Adapted Large Language Models versus State-of-the-art Counterparts in Chip Design Coding Assistance <https://arxiv.org/abs/2404.08850>`__ 评估经济可行性:领域自适应大型语言模型与最先进的芯片设计编码辅助对应模型的总拥有成本比较分析

`[2405.15808] Ensuring Ground Truth Accuracy in Healthcare with the EVINCE framework <https://arxiv.org/abs/2405.15808>`__ 使用EVINCE框架确保医疗保健中的基本事实准确性

`[2405.16567] Automatic Jailbreaking of the Text-to-Image Generative AI Systems <https://arxiv.org/abs/2405.16567>`__ 文本到图像生成AI系统的自动越狱

`[2303.13013] GesGPT: Speech Gesture Synthesis With Text Parsing from ChatGPT <https://arxiv.org/abs/2303.13013>`__ GesGPT:基于ChatGPT的文本分析语音手势合成

`[2303.14337] SmartBook: AI-Assisted Situation Report Generation for Intelligence Analysts <https://arxiv.org/abs/2303.14337>`__ SmartBook:面向情报分析人员的ai辅助态势报告生成

`[2309.00916] BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing <https://arxiv.org/abs/2309.00916>`__ BLSP:基于延续书写行为对齐的自助语言-语音预训练

`[2309.12551] Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models <https://arxiv.org/abs/2309.12551>`__ 是否可以将文本修改为目标可读性级别?使用零样本大型语言模型的初步调查

`[2310.02932] Assessing Large Language Models on Climate Information <https://arxiv.org/abs/2310.02932>`__ 基于气候信息的大型语言模型评估

`[2310.05007] MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering <https://arxiv.org/abs/2310.05007>`__ MinPrompt:基于图的小样本问答最小提示数据增强

`[2310.14735] Unleashing the potential of prompt engineering: a comprehensive review <https://arxiv.org/abs/2310.14735>`__ 释放prompt engineering的潜力:全面综述

`[2311.00117] BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B <https://arxiv.org/abs/2311.00117>`__ BadLlama:低成本移除Llama 2-Chat 13B的安全微调

`[2311.02790] CausalCite: A Causal Formulation of Paper Citations <https://arxiv.org/abs/2311.02790>`__ CausalCite:论文引用的因果表述

`[2401.06532] INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning <https://arxiv.org/abs/2401.06532>`__ :通过指令调优释放大型语言模型在搜索中的力量

`[2401.13601] MM-LLMs: Recent Advances in MultiModal Large Language Models <https://arxiv.org/abs/2401.13601>`__ MM-LLMs:多模态大型语言模型的最新进展

`[2402.05119] A Closer Look at the Limitations of Instruction Tuning <https://arxiv.org/abs/2402.05119>`__ 仔细看看指令调优的局限性

`[2402.07776] TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection <https://arxiv.org/abs/2402.07776>`__ TELLER:一种可解释、可泛化、可控制的假新闻检测可信框架

`[2402.08219] BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models <https://arxiv.org/abs/2402.08219>`__ bbox适配器:适应黑盒大型语言模型的轻量级适配器

`[2402.09552] STEER: Assessing the Economic Rationality of Large Language Models <https://arxiv.org/abs/2402.09552>`__ STEER:评估大型语言模型的经济合理性

`[2402.10866] EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models <https://arxiv.org/abs/2402.10866>`__ EcoRank:基于大型语言模型的预算约束文本重排序

`[2402.10958] Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts <https://arxiv.org/abs/2402.10958>`__ 相对偏好优化:通过对比相同和不同提示的响应来增强LLM对齐

`[2402.12948] GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick <https://arxiv.org/abs/2402.12948>`__ GumbelSoft:基于GumbelMax-trick的多样化语言模型水印

`[2402.13669] Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning <https://arxiv.org/abs/2402.13669>`__ 自蒸馏弥合语言模型微调中的分布差距

`[2402.16107] Knowledge Fusion of Chat LLMs: A Preliminary Technical Report <https://arxiv.org/abs/2402.16107>`__ 知识融合的Chat LLMs:初步技术报告

`[2402.16159] DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem <https://arxiv.org/abs/2402.16159>`__ DistALANER:开源软件生态系统中远程监督主动学习增强的命名实体识别

`[2403.00418] LLMs for Targeted Sentiment in News Headlines: Exploring the Descriptive-Prescriptive Dilemma <https://arxiv.org/abs/2403.00418>`__ 新闻标题中目标情绪的llm:探索描述-规定的困境

`[2403.03121] Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution <https://arxiv.org/abs/2403.03121>`__ 愤怒的男人，悲伤的女人:大型语言模型反映情绪归因中的性别刻板印象

`[2403.07378] SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression <https://arxiv.org/abs/2403.07378>`__ SVD-LLM:基于截断感知奇异值分解的大型语言模型压缩

`[2403.14472] Detoxifying Large Language Models via Knowledge Editing <https://arxiv.org/abs/2403.14472>`__ 基于知识编辑的大型语言模型解毒

`[2404.11999] Token-level Direct Preference Optimization <https://arxiv.org/abs/2404.11999>`__ 令牌级直接偏好优化

`[2405.14555] Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models <https://arxiv.org/abs/2405.14555>`__ 细微偏差需要更细微的度量:评估大型语言模型中的代表性和亲和性偏差的双重指标

`[2405.15077] Eliciting Informative Text Evaluations with Large Language Models <https://arxiv.org/abs/2405.15077>`__ 基于大型语言模型的信息文本评估

`[2405.16277] Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge <https://arxiv.org/abs/2405.16277>`__ 描绘模糊性:Winograd模式挑战的视觉扭曲

`[2405.16295] Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data <https://arxiv.org/abs/2405.16295>`__ 开源语言模型在医学文本数据汇总中的对比分析

`[2405.16720] Large Scale Knowledge Washing <https://arxiv.org/abs/2405.16720>`__ 大规模知识清洗

`[2405.16806] Entity Alignment with Noisy Annotations from Large Language Models <https://arxiv.org/abs/2405.16806>`__ 与大型语言模型噪声注释的实体对齐

`[2310.18491] Publicly-Detectable Watermarking for Language Models <https://arxiv.org/abs/2310.18491>`__ 面向语言模型的可公开检测水印

`[2312.05516] Stateful Large Language Model Serving with Pensieve <https://arxiv.org/abs/2312.05516>`__ 基于Pensieve的有状态大型语言模型

`[2402.01821] Human-like Category Learning by Injecting Ecological Priors from Large Language Models into Neural Networks <https://arxiv.org/abs/2402.01821>`__ 通过将大型语言模型的生态先验注入神经网络进行类人类别学习

`[2402.02314] Selecting Large Language Model to Fine-tune via Rectified Scaling Law <https://arxiv.org/abs/2402.02314>`__ 基于修正尺度律的大型语言模型微调选择

`[2402.11867] LoRA Training in the NTK Regime has No Spurious Local Minima <https://arxiv.org/abs/2402.11867>`__ NTK模式下的LoRA训练没有伪局部最小值

`[2404.02948] PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models <https://arxiv.org/abs/2404.02948>`__ PiSSA:大型语言模型的主奇异值和奇异向量自适应

`[2404.04102] ROPO: Robust Preference Optimization for Large Language Models <https://arxiv.org/abs/2404.04102>`__ ROPO:大型语言模型的鲁棒偏好优化

`[2405.00747] Soft Preference Optimization: Aligning Language Models to Expert Distributions <https://arxiv.org/abs/2405.00747>`__ 软偏好优化:语言模型与专家分布对齐

`[2405.12807] FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information <https://arxiv.org/abs/2405.12807>`__ FAdam: Adam是使用对角经验费雪信息的自然梯度优化器

`[2405.14597] Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs <https://arxiv.org/abs/2405.14597>`__ 整数尺度:加快llm细粒度量化的免费午餐

`[2405.17374] Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models <https://arxiv.org/abs/2405.17374>`__ 安全领域导航:在微调大型语言模型中测量风险

`[2310.09676] Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning <https://arxiv.org/abs/2310.09676>`__ 通过预训练和多任务微调掌握基于多模态提示的机器人操作

`[2311.13627] Vamos: Versatile Action Models for Video Understanding <https://arxiv.org/abs/2311.13627>`__ Vamos:视频理解的通用动作模型

`[2405.14612] Explaining Multi-modal Large Language Models by Analyzing their Vision Perception <https://arxiv.org/abs/2405.14612>`__ 通过分析视觉感知来解释多模态大型语言模型

`[2405.17104] LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding <https://arxiv.org/abs/2405.17104>`__ LLM-Optic:揭示用于通用视觉基础的大型语言模型的能力

`[2401.10874] Applications of flow models to the generation of correlated lattice QCD ensembles <https://arxiv.org/abs/2401.10874>`__ 流模型在相关晶格QCD系综生成中的应用

`[2402.01831] Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities <https://arxiv.org/abs/2402.01831>`__ 音频Flamingo:一种具有少样本学习和对话能力的新型音频语言模型

`[2402.09179] Instruction Backdoor Attacks Against Customized LLMs <https://arxiv.org/abs/2402.09179>`__ 针对自定义llm的指令后门攻击

