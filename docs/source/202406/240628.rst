240628
========

----------
Survey (3)
----------

`[2406.19097] Fairness and Bias in Multimodal AI: A Survey <https://arxiv.org/abs/2406.19097>`__ 多模态人工智能的公平性和偏见综述

::

    Thu, 27 Jun 2024 11:26:17 GMT
    Tosin Adewumi, Lama Alkhaled, Namrata Gurung, Goya van Boven and Irene Pagliai

The importance of addressing fairness and bias in artificial intelligence (AI) systems cannot be over-emphasized. Mainstream media has been awashed with news of incidents around stereotypes and bias in many of these systems in recent years. In this survey, we fill a gap with regards to the minimal study of fairness and bias in Large Multimodal Models (LMMs) compared to Large Language Models (LLMs), providing 50 examples of datasets and models along with the challenges affecting them; we identify a new category of quantifying bias (preuse), in addition to the two well-known ones in the literature: intrinsic and extrinsic; we critically discuss the various ways researchers are addressing these challenges. Our method involved two slightly different search queries on Google Scholar, which revealed that 33,400 and 538,000 links are the results for the terms "Fairness and bias in Large Multimodal Models" and "Fairness and bias in Large Language Models", respectively. We believe this work contributes to filling this gap and providing insight to researchers and other stakeholders on ways to address the challenge of fairness and bias in multimodal A!.

------------

`[2309.17447] A Large Language Model Approach to Educational Survey Feedback Analysis <https://arxiv.org/abs/2309.17447>`__ 教育调查反馈分析的大型语言模型方法

::

    replaced with revised version Thu, 27 Jun 2024 02:42:36 GMT
    Submission history From: Michael Parker [view email]
    [v1] Fri, 29 Sep 2023 17:57:23 UTC (5,203 KB)
    [v2] Thu, 27 Jun 2024 02:42:36 UTC (5,105 KB)
    Michael J. Parker, Caitlin Anderson, Claire Stone, YeaRim Oh

This paper assesses the potential for the large language models (LLMs) GPT-4 and GPT-3.5 to aid in deriving insight from education feedback surveys. Exploration of LLM use cases in education has focused on teaching and learning, with less exploration of capabilities in education feedback analysis. Survey analysis in education involves goals such as finding gaps in curricula or evaluating teachers, often requiring time-consuming manual processing of textual responses. LLMs have the potential to provide a flexible means of achieving these goals without specialized machine learning models or fine-tuning. We demonstrate a versatile approach to such goals by treating them as sequences of natural language processing (NLP) tasks including classification (multi-label, multi-class, and binary), extraction, thematic analysis, and sentiment analysis, each performed by LLM. We apply these workflows to a real-world dataset of 2500 end-of-course survey comments from biomedical science courses, and evaluate a zero-shot approach (i.e., requiring no examples or labeled training data) across all tasks, reflecting education settings, where labeled data is often scarce. By applying effective prompting practices, we achieve human-level performance on multiple tasks with GPT-4, enabling workflows necessary to achieve typical goals. We also show the potential of inspecting LLMs' chain-of-thought (CoT) reasoning for providing insight that may foster confidence in practice. Moreover, this study features development of a versatile set of classification categories, suitable for various course types (online, hybrid, or in-person) and amenable to customization. Our results suggest that LLMs can be used to derive a range of insights from survey text.

------------

`[2406.08426] Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL <https://arxiv.org/abs/2406.08426>`__ 下一代数据库接口:基于llm的Text-to-SQL综述

::

    replaced with revised version Thu, 27 Jun 2024 13:51:30 GMT
    Submission history From: Zijin Hong [view email]
    [v1] Wed, 12 Jun 2024 17:13:17 UTC (529 KB)
    [v2] Thu, 27 Jun 2024 13:51:30 UTC (694 KB)
    Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, Xiao Huang

Generating accurate SQL according to natural language questions (text-to-SQL) is a long-standing challenge due to the complexities involved in user question understanding, database schema comprehension, and SQL generation. Conventional text-to-SQL systems, comprising human engineering and deep neural networks, have made substantial progress. Subsequently, pre-trained language models (PLMs) have been developed and utilized for text-to-SQL tasks, achieving promising performance. As modern databases become more complex, the corresponding user questions also grow more challenging, leading PLMs with limited comprehension capabilities to produce incorrect SQL. This necessitates more sophisticated and tailored optimization methods for PLMs, which, in turn, restricts the applications of PLM-based systems. Most recently, large language models (LLMs) have demonstrated significant capabilities in natural language understanding as the model scale remains increasing. Therefore, integrating the LLM-based implementation can bring unique opportunities, improvements, and solutions to text-to-SQL research. In this survey, we present a comprehensive review of LLM-based text-to-SQL. Specifically, we propose a brief overview of the technical challenges and the evolutionary process of text-to-SQL. Then, we provide a detailed introduction to the datasets and metrics designed to evaluate text-to-SQL systems. After that, we present a systematic analysis of recent advances in LLM-based text-to-SQL. Finally, we discuss the remaining challenges in this field and propose expectations for future research directions.

------------

-------------
Benchmark (6)
-------------

`[2406.19073] AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database Queries <https://arxiv.org/abs/2406.19073>`__ AMBROSIA:将歧义问题解析为数据库查询的基准

::

    Thu, 27 Jun 2024 10:43:04 GMT
    Irina Saparina and Mirella Lapata

Practical semantic parsers are expected to understand user utterances and map them to executable programs, even when these are ambiguous. We introduce a new benchmark, AMBROSIA, which we hope will inform and inspire the development of text-to-SQL parsers capable of recognizing and interpreting ambiguous requests.
Our dataset contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries. In each case, the ambiguity persists even when the database context is provided. This is achieved through a novel approach that involves controlled generation of databases from scratch. We benchmark various LLMs on AMBROSIA, revealing that even the most advanced models struggle to identify and interpret ambiguity in questions.

------------

`[2406.19314] LiveBench: A Challenging, Contamination-Free LLM Benchmark <https://arxiv.org/abs/2406.19314>`__ LiveBench:一个具有挑战性的、无污染的LLM基准

::

    Thu, 27 Jun 2024 16:47:42 GMT
    Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, Micah Goldblum

Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be immune to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 110B in size. LiveBench is difficult, with top models achieving below 65% accuracy. We release all questions, code, and model answers. Questions will be added and updated on a monthly basis, and we will release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.

------------

`[2406.18627] AssertionBench: A Benchmark to Evaluate Large-Language Models for Assertion Generation <https://arxiv.org/abs/2406.18627>`__ AssertionBench:评估用于断言生成的大型语言模型的基准

::

    Wed, 26 Jun 2024 14:47:28 GMT
    Vaishnavi Pulavarthi, Deeksha Nandal, Soham Dan, Debjit Pal

Assertions have been the de facto collateral for simulation-based and formal verification of hardware designs for over a decade. The quality of hardware verification, \ie, detection and diagnosis of corner-case design bugs, is critically dependent on the quality of the assertions. There has been a considerable amount of research leveraging a blend of data-driven statistical analysis and static analysis to generate high-quality assertions from hardware design source code and design execution trace data. Despite such concerted effort, all prior research struggles to scale to industrial-scale large designs, generates too many low-quality assertions, often fails to capture subtle and non-trivial design functionality, and does not produce any easy-to-comprehend explanations of the generated assertions to understand assertions' suitability to different downstream validation tasks. Recently, with the advent of Large-Language Models (LLMs), there has been a widespread effort to leverage prompt engineering to generate assertions. However, there is little effort to quantitatively establish the effectiveness and suitability of various LLMs for assertion generation. In this paper, we present AssertionBench, a novel benchmark to evaluate LLMs' effectiveness for assertion generation quantitatively. AssertioBench contains 100 curated Verilog hardware designs from OpenCores and formally verified assertions for each design generated from GoldMine and HARM. We use AssertionBench to compare state-of-the-art LLMs to assess their effectiveness in inferring functionally correct assertions for hardware designs. Our experiments demonstrate how LLMs perform relative to each other, the benefits of using more in-context exemplars in generating a higher fraction of functionally correct assertions, and the significant room for improvement for LLM-based assertion generators.

------------

`[2402.09742] AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator <https://arxiv.org/abs/2402.09742>`__ 

::

    replaced with revised version Thu, 27 Jun 2024 15:40:53 GMT
    Submission history From: Zhihao Fan [view email]
    [v1] Thu, 15 Feb 2024 06:46:48 UTC (9,452 KB)
    [v2] Wed, 21 Feb 2024 08:25:25 UTC (9,452 KB)
    [v3] Thu, 27 Jun 2024 15:40:53 UTC (2,883 KB)
    [v4] Fri, 28 Jun 2024 03:11:48 UTC (2,883 KB)
    Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou

Artificial intelligence has significantly advanced healthcare, particularly through large language models (LLMs) that excel in medical question answering benchmarks. However, their real-world clinical application remains limited due to the complexities of doctor-patient interactions. To address this, we introduce \textbf{AI Hospital}, a multi-agent framework simulating dynamic medical interactions between \emph{Doctor} as player and NPCs including \emph{Patient}, \emph{Examiner}, \emph{Chief Physician}. This setup allows for realistic assessments of LLMs in clinical scenarios. We develop the Multi-View Medical Evaluation (MVME) benchmark, utilizing high-quality Chinese medical records and NPCs to evaluate LLMs' performance in symptom collection, examination recommendations, and diagnoses. Additionally, a dispute resolution collaborative mechanism is proposed to enhance diagnostic accuracy through iterative discussions. Despite improvements, current LLMs exhibit significant performance gaps in multi-turn interactions compared to one-step approaches. Our findings highlight the need for further research to bridge these gaps and improve LLMs' clinical diagnostic capabilities. Our data, code, and experimental results are all open-sourced at \url{this https URL}.

------------

`[2402.11175] M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection <https://arxiv.org/abs/2402.11175>`__ M4GT-Bench:黑盒机器生成文本检测评估基准

::

    replaced with revised version Thu, 27 Jun 2024 05:42:12 GMT
    Submission history From: Yuxia Wang [view email]
    [v1] Sat, 17 Feb 2024 02:50:33 UTC (139 KB)
    [v2] Thu, 27 Jun 2024 05:42:12 UTC (170 KB)
    Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Osama Mohanned Afzal, Tarek Mahmoud, Giovanni Puccetti, Thomas Arnold, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, Preslav Nakov

The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multi-domain, and multi-generator corpus of MGTs -- M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed human-machine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at this https URL.

------------

`[2402.16040] EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries <https://arxiv.org/abs/2402.16040>`__ EHRNoteQA:使用出院摘要进行真实世界临床实践的LLM基准

::

    replaced with revised version Thu, 27 Jun 2024 04:28:12 GMT
    Submission history From: Jiyoun Kim [view email]
    [v1] Sun, 25 Feb 2024 09:41:50 UTC (8,319 KB)
    [v2] Tue, 27 Feb 2024 06:25:25 UTC (8,319 KB)
    [v3] Thu, 13 Jun 2024 05:15:33 UTC (988 KB)
    [v4] Thu, 27 Jun 2024 04:28:12 UTC (988 KB)
    Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim, Jeewon Yang, Seunghyun Won, Edward Choi

Discharge summaries in Electronic Health Records (EHRs) are crucial for clinical decision-making, but their length and complexity make information extraction challenging, especially when dealing with accumulated summaries across multiple patient admissions. Large Language Models (LLMs) show promise in addressing this challenge by efficiently analyzing vast and complex data. Existing benchmarks, however, fall short in properly evaluating LLMs' capabilities in this context, as they typically focus on single-note information or limited topics, failing to reflect the real-world inquiries required by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel benchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. Every QA pair is initially generated using GPT-4 and then manually reviewed and refined by three clinicians to ensure clinical relevance. EHRNoteQA includes questions that require information across multiple discharge summaries and covers eight diverse topics, mirroring the complexity and diversity of real clinical inquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice question answering, and propose a reliable evaluation method for each. We evaluate 27 LLMs using EHRNoteQA and examine various factors affecting the model performance (e.g., the length and number of discharge summaries). Furthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations in clinical practice, we measure the correlation between the LLM performance on EHRNoteQA, and the LLM performance manually evaluated by clinicians. Results show that LLM performance on EHRNoteQA have higher correlation with clinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to other benchmarks, demonstrating its practical relevance in evaluating LLMs in clinical settings.

------------

--------------
Accelerate (8)
--------------

`[2406.18832] OutlierTune: Efficient Channel-Wise Quantization for Large Language Models <https://arxiv.org/abs/2406.18832>`__ OutlierTune:大型语言模型的高效通道量化

::

    Thu, 27 Jun 2024 02:02:26 GMT
    Jinguang Wang, Yuexi Yin, Haifeng Sun, Qi Qi, Jingyu Wang, Zirui Zhuang, Tingting Yang, Jianxin Liao

Quantizing the activations of large language models (LLMs) has been a significant challenge due to the presence of structured outliers. Most existing methods focus on the per-token or per-tensor quantization of activations, making it difficult to achieve both accuracy and hardware efficiency. To address this problem, we propose OutlierTune, an efficient per-channel post-training quantization (PTQ) method for the activations of LLMs.
OutlierTune consists of two components: pre-execution of dequantization and symmetrization. The pre-execution of dequantization updates the model weights by the activation scaling factors, avoiding the internal scaling and costly additional computational overheads brought by the per-channel activation quantization. The symmetrization further reduces the quantization differences arising from the weight updates by ensuring the balanced numerical ranges across different activation channels. OutlierTune is easy to implement and hardware-efficient, introducing almost no additional computational overheads during the inference. Extensive experiments show that the proposed framework outperforms existing methods across multiple different tasks. Demonstrating better generalization, this framework improves the Int6 quantization of the instruction-tuning LLMs, such as OPT-IML, to the same level as half-precision (FP16). Moreover, we have shown that the proposed framework is 1.48x faster than the FP16 implementation while reducing approximately 2x memory usage.

------------

`[2406.19223] T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings <https://arxiv.org/abs/2406.19223>`__ T-FREE:通过稀疏表示实现内存高效嵌入的无tokenizer生成llm

::

    Thu, 27 Jun 2024 14:49:08 GMT
    Bj\"orn Deiseroth, Manuel Brack, Patrick Schramowski, Kristian Kersting, Samuel Weinbach

Tokenizers are crucial for encoding information in Large Language Models, but their development has recently stagnated, and they contain inherent weaknesses.
Major limitations include computational overhead, ineffective vocabulary use, and unnecessarily large embedding and head layers. Additionally, their performance is biased towards a reference corpus, leading to reduced effectiveness for underrepresented languages.
To remedy these issues, we propose T-FREE, which directly embeds words through sparse activation patterns over character triplets, and does not require a reference corpus. T-FREE inherently exploits morphological similarities and allows for strong compression of embedding layers. In our exhaustive experimental evaluation, we achieve competitive downstream performance with a parameter reduction of more than 85% on these layers.
Further, T-FREE shows significant improvements in cross-lingual transfer learning.

------------

`[2406.18853] Decoding-Time Language Model Alignment with Multiple Objectives <https://arxiv.org/abs/2406.18853>`__ 

::

    Thu, 27 Jun 2024 02:46:30 GMT
    Ruizhe Shi, Yifang Chen, Yushi Hu, ALisa Liu, Noah Smith, Hannaneh Hajishirzi, Simon Du

Aligning language models (LMs) to human preferences has emerged as a critical pursuit, enabling these models to better serve diverse user needs. Existing methods primarily focus on optimizing LMs for a single reward function, limiting their adaptability to varied objectives. Here, we propose $\textbf{multi-objective decoding (MOD)}$, a decoding-time algorithm that outputs the next token from a linear combination of predictions of all base models, for any given weightings over different objectives. We exploit a common form among a family of $f$-divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy. Theoretically, we show why existing approaches can be sub-optimal even in natural settings and obtain optimality guarantees for our method. Empirical results demonstrate the effectiveness of the algorithm. For example, compared to a parameter-merging baseline, MOD achieves 12.8% overall reward improvement when equally optimizing towards $3$ objectives. Moreover, we experiment with MOD on combining three fully-finetuned LLMs of different model sizes, each aimed at different objectives such as safety, coding, and general user preference. Unlike traditional methods that require careful curation of a mixture of datasets to achieve comprehensive improvement, we can quickly experiment with preference weightings using MOD to find the best combination of models. Our best combination reduces toxicity on Toxigen to nearly 0% and achieves 7.9--33.3% improvement across other three metrics ($\textit{i.e.}$, Codex@1, GSM-COT, BBH-COT).

------------

`[2402.09773] NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models <https://arxiv.org/abs/2402.09773>`__ NutePrune:基于大量教师的大型语言模型高效渐进式修剪

::

    replaced with revised version Thu, 27 Jun 2024 04:49:11 GMT
    Submission history From: Shengrui Li [view email]
    [v1] Thu, 15 Feb 2024 08:03:12 UTC (307 KB)
    [v2] Thu, 27 Jun 2024 04:49:11 UTC (309 KB)
    Shengrui Li, Junzhe Chen, Xueting Han, Jing Bai

The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware. Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models. Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach allows us to leverage numerous teachers with varying capacities to progressively guide the pruned model, enhancing overall performance. Extensive experiments across various tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot experiments, NutePrune retains 97.17% of the performance of the original model at 20% sparsity and 95.07% at 25% sparsity. Our code is available at this https URL.

------------

`[2403.05750] Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text <https://arxiv.org/abs/2403.05750>`__ 解码AI笔:检测AI生成文本的技术与挑战

::

    replaced with revised version Wed, 26 Jun 2024 20:49:32 GMT
    Submission history From: Sara Abdali [view email]
    [v1] Sat, 9 Mar 2024 01:13:54 UTC (56 KB)
    [v2] Thu, 20 Jun 2024 00:05:37 UTC (170 KB)
    [v3] Wed, 26 Jun 2024 20:49:32 UTC (170 KB)
    Sara Abdali, Richard Anarfi, CJ Barberan, Jia He

Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.

------------

`[2406.14833] Efficient Continual Pre-training by Mitigating the Stability Gap <https://arxiv.org/abs/2406.14833>`__ 通过减轻稳定性差距进行有效的持续预训练

::

    replaced with revised version Thu, 27 Jun 2024 08:11:01 GMT
    Submission history From: Yiduo Guo [view email]
    [v1] Fri, 21 Jun 2024 02:28:37 UTC (1,375 KB)
    [v2] Thu, 27 Jun 2024 08:11:01 UTC (1,613 KB)
    Yiduo Guo, Jie Fu, Huishuai Zhang, Dongyan Zhao, Yikang Shen

Continual pre-training has increasingly become the predominant approach for adapting Large Language Models (LLMs) to new domains. This process involves updating the pre-trained LLM with a corpus from a new domain, resulting in a shift in the training distribution. To study the behavior of LLMs during this shift, we measured the model's performance throughout the continual pre-training process. we observed a temporary performance drop at the beginning, followed by a recovery phase, a phenomenon known as the "stability gap," previously noted in vision models classifying new classes. To address this issue and enhance LLM performance within a fixed compute budget, we propose three effective strategies: (1) Continually pre-training the LLM on a subset with a proper size for multiple epochs, resulting in faster performance recovery than pre-training the LLM on a large corpus in a single epoch; (2) Pre-training the LLM only on high-quality sub-corpus, which rapidly boosts domain performance; and (3) Using a data mixture similar to the pre-training data to reduce distribution gap. We conduct various experiments on Llama-family models to validate the effectiveness of our strategies in both medical continual pre-training and instruction tuning. For example, our strategies improve the average medical task performance of the OpenLlama-3B model from 36.2% to 40.7% with only 40% of the original training budget and enhance the average general task performance without causing forgetting. Furthermore, we apply our strategies to the Llama-3-8B model. The resulting model, Llama-3-Physician, achieves the best medical performance among current open-source models, and performs comparably to or even better than GPT-4 on several medical benchmarks. We release our models at \url{this https URL}.

------------

`[2405.16755] CHESS: Contextual Harnessing for Efficient SQL Synthesis <https://arxiv.org/abs/2405.16755>`__ 国际象棋:利用上下文实现高效SQL合成

::

    replaced with revised version Thu, 27 Jun 2024 17:13:32 GMT
    Submission history From: Shayan Talaei [view email]
    [v1] Mon, 27 May 2024 01:54:16 UTC (1,974 KB)
    [v2] Thu, 27 Jun 2024 17:13:32 UTC (1,975 KB)
    Shayan Talaei, Mohammadreza Pourreza, Yu-Chen Chang, Azalia Mirhoseini, Amin Saberi

Utilizing large language models (LLMs) for transforming natural language questions into SQL queries (text-to-SQL) is a promising yet challenging approach, particularly when applied to real-world databases with complex and extensive schemas. In particular, effectively incorporating data catalogs and database values for SQL generation remains an obstacle, leading to suboptimal solutions. We address this problem by proposing a new pipeline that effectively retrieves relevant data and context, selects an efficient schema, and synthesizes correct and efficient SQL queries. To increase retrieval precision, our pipeline introduces a hierarchical retrieval method leveraging model-generated keywords, locality-sensitive hashing indexing, and vector databases. Additionally, we have developed an adaptive schema pruning technique that adjusts based on the complexity of the problem and the model's context size. Our approach generalizes to both frontier proprietary models like GPT-4 and open-source models such as Llama-3-70B. Through a series of ablation studies, we demonstrate the effectiveness of each component of our pipeline and its impact on the end-to-end performance. Our method achieves new state-of-the-art performance on the cross-domain challenging BIRD dataset.

------------

`[2404.14527] M\'elange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity <https://arxiv.org/abs/2404.14527>`__ M\'elange:利用GPU异构性提供高成本高效的大型语言模型

::

    replaced with revised version Wed, 26 Jun 2024 23:39:26 GMT
    Submission history From: Tyler Griggs [view email]
    [v1] Mon, 22 Apr 2024 18:56:18 UTC (4,990 KB)
    [v2] Wed, 26 Jun 2024 23:39:26 UTC (3,842 KB)
    [v3] Fri, 28 Jun 2024 01:24:22 UTC (3,842 KB)
    Tyler Griggs, Xiaoxuan Liu, Jiaxiang Yu, Doyoung Kim, Wei-Lin Chiang, Alvin Cheung, Ion Stoica

Large language models (LLMs) are increasingly integrated into many online services, yet they remain cost-prohibitive to deploy due to the requirement of expensive GPU instances. Prior work has addressed the high cost of LLM serving by improving the inference engine, but less attention has been given to selecting the most cost-efficient GPU type(s) for a specific LLM service. There is a large and growing landscape of GPU types and, within these options, higher cost does not always lead to increased performance. Instead, through a comprehensive investigation, we find that three key LLM service characteristics (request size, request rate, SLO) strongly influence GPU cost efficiency, and differing GPU types are most cost efficient for differing LLM service settings. As a result, the most cost-efficient allocation for a given service is typically a mix of heterogeneous GPU types. Based on this analysis, we introduce Mélange, a GPU allocation framework that navigates these diverse LLM service characteristics and heterogeneous GPU option space to automatically and efficiently derive the minimal-cost GPU allocation for a given LLM service. We formulate the GPU allocation task as a cost-aware bin packing problem where GPUs are bins and items are slices of the service workload. Our formulation's constraints account for a service's unique characteristics, allowing Mélange to be flexible to support diverse service settings and heterogeneity-aware to adapt the GPU allocation to a specific service. Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.

------------

-----------------------
In-Context Learning (1)
-----------------------

`[2406.18770] ADO-LLM: Analog Design Bayesian Optimization with In-Context Learning of Large Language Models <https://arxiv.org/abs/2406.18770>`__ ADO-LLM:基于大型语言模型上下文学习的模拟设计贝叶斯优化

::

    Wed, 26 Jun 2024 21:42:50 GMT
    Yuxuan Yin, Yu Wang, Boxun Xu, Peng Li

Analog circuit design requires substantial human expertise and involvement, which is a significant roadblock to design productivity. Bayesian Optimization (BO), a popular machine learning based optimization strategy, has been leveraged to automate analog design given its applicability across various circuit topologies and technologies. Traditional BO methods employ black box Gaussian Process surrogate models and optimized labeled data queries to find optimization solutions by trading off between exploration and exploitation.
However, the search for the optimal design solution in BO can be expensive from both a computational and data usage point of view, particularly for high dimensional optimization problems. This paper presents ADO-LLM, the first work integrating large language models (LLMs) with Bayesian Optimization for analog design optimization. ADO-LLM leverages the LLM's ability to infuse domain knowledge to rapidly generate viable design points to remedy BO's inefficiency in finding high value design areas specifically under the limited design space coverage of the BO's probabilistic surrogate model. In the meantime, sampling of design points evaluated in the iterative BO process provides quality demonstrations for the LLM to generate high quality design points while leveraging infused broad design knowledge. Furthermore, the diversity brought by BO's exploration enriches the contextual understanding of the LLM and allows it to more broadly search in the design space and prevent repetitive and redundant suggestions. We evaluate the proposed framework on two different types of analog circuits and demonstrate notable improvements in design efficiency and effectiveness.

------------

--------------
Reasoning (11)
--------------

`[2406.18839] Disentangling Knowledge-based and Visual Reasoning by Question Decomposition in KB-VQA <https://arxiv.org/abs/2406.18839>`__ KB-VQA中基于问题分解的知识和视觉推理解缠

::

    Thu, 27 Jun 2024 02:19:38 GMT
    Elham J. Barezi, Parisa Kordjamshidi

We study the Knowledge-Based visual question-answering problem, for which given a question, the models need to ground it into the visual modality to find the answer. Although many recent works use question-dependent captioners to verbalize the given image and use Large Language Models to solve the VQA problem, the research results show they are not reasonably performing for multi-hop questions. Our study shows that replacing a complex question with several simpler questions helps to extract more relevant information from the image and provide a stronger comprehension of it. Moreover, we analyze the decomposed questions to find out the modality of the information that is required to answer them and use a captioner for the visual questions and LLMs as a general knowledge source for the non-visual KB-based questions. Our results demonstrate the positive impact of using simple questions before retrieving visual or non-visual information. We have provided results and analysis on three well-known VQA datasets including OKVQA, A-OKVQA, and KRVQA, and achieved up to 2% improvement in accuracy.

------------

`[2406.18762] Categorical Syllogisms Revisited: A Review of the Logical Reasoning Abilities of LLMs for Analyzing Categorical Syllogism <https://arxiv.org/abs/2406.18762>`__ 重新审视直言三段论:对分析直言三段论的llm逻辑推理能力的回顾

::

    Wed, 26 Jun 2024 21:17:20 GMT
    Shi Zong and Jimmy Lin

There have been a huge number of benchmarks proposed to evaluate how large language models (LLMs) behave for logic inference tasks. However, it remains an open question how to properly evaluate this ability. In this paper, we provide a systematic overview of prior works on the logical reasoning ability of LLMs for analyzing categorical syllogisms. We first investigate all the possible variations for the categorical syllogisms from a purely logical perspective and then examine the underlying configurations (i.e., mood and figure) tested by the existing datasets. Our results indicate that compared to template-based synthetic datasets, crowdsourcing approaches normally sacrifice the coverage of configurations (i.e., mood and figure) of categorical syllogisms for more language variations, thus bringing challenges to fully testing LLMs under different situations. We then proceed to summarize the findings and observations for the performances of LLMs to infer the validity of syllogisms from the current literature. The error rate breakdown analyses suggest that the interpretation of the quantifiers seems to be the current bottleneck that limits the performances of the LLMs and is thus worth more attention. Finally, we discuss several points that might be worth considering when researchers plan on the future release of categorical syllogism datasets. We hope our work will not only provide a timely review of the current literature regarding categorical syllogisms, but also motivate more interdisciplinary research between communities, specifically computational linguists and logicians.

------------

`[2406.18629] Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs <https://arxiv.org/abs/2406.18629>`__ Step-DPO: llm长链推理的分步偏好优化

::

    Wed, 26 Jun 2024 17:43:06 GMT
    Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, Jiaya Jia

Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) due to the extensive and precise chain of reasoning required for accuracy. Ensuring the correctness of each reasoning step is critical. To address this, we aim to enhance the robustness and factuality of LLMs by learning from human feedback. However, Direct Preference Optimization (DPO) has shown limited benefits for long-chain mathematical reasoning, as models employing DPO struggle to identify detailed errors in incorrect answers. This limitation stems from a lack of fine-grained process supervision. We propose a simple, effective, and data-efficient method called Step-DPO, which treats individual reasoning steps as units for preference optimization rather than evaluating answers holistically. Additionally, we have developed a data construction pipeline for Step-DPO, enabling the creation of a high-quality dataset containing 10K step-wise preference pairs. We also observe that in DPO, self-generated data is more effective than data generated by humans or GPT-4, due to the latter's out-of-distribution nature. Our findings demonstrate that as few as 10K preference data pairs and fewer than 500 Step-DPO training steps can yield a nearly 3% gain in accuracy on MATH for models with over 70B parameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves scores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively, surpassing a series of closed-source models, including GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at https://github.com/dvlab-research/Step-DPO.

------------

`[2406.18695] Learning to Correct for QA Reasoning with Black-box LLMs <https://arxiv.org/abs/2406.18695>`__ 学习用黑盒llm纠正QA推理

::

    Wed, 26 Jun 2024 18:57:32 GMT
    Jaehyung Kim, Dongyoung Kim, Yiming Yang

An open challenge in recent machine learning is about how to improve the reasoning capability of large language models (LLMs) in a black-box setting, i.e., without access to detailed information such as output token probabilities. Existing approaches either rely on accessibility (which is often unrealistic) or involve significantly increased train- and inference-time costs. This paper addresses those limitations or shortcomings by proposing a novel approach, namely CoBB (Correct for improving QA reasoning of Black-Box LLMs). It uses a trained adaptation model to perform a seq2seq mapping from the often-imperfect reasonings of the original black-box LLM to the correct or improved reasonings. Specifically, the adaptation model is initialized with a relatively small open-source LLM and adapted over a collection of sub-sampled training pairs. To select the representative pairs of correct and incorrect reasonings, we formulated the dataset construction as an optimization problem that minimizes the statistical divergence between the sampled subset and the entire collection, and solved it via a genetic algorithm. We then train the adaptation model over the sampled pairs by contrasting the likelihoods of correct and incorrect reasonings. Our experimental results demonstrate that CoBB significantly improves reasoning accuracy across various QA benchmarks, compared to the best-performing adaptation baselines.

------------

`[2406.19121] Towards Learning Abductive Reasoning using VSA Distributed Representations <https://arxiv.org/abs/2406.19121>`__ 基于VSA分布式表示的溯因推理学习

::

    Thu, 27 Jun 2024 12:05:55 GMT
    Giacomo Camposampiero, Michael Hersche, Aleksandar Terzi\'c, Roger Wattenhofer, Abu Sebastian, Abbas Rahimi

We introduce the Abductive Rule Learner with Context-awareness (ARLC), a model that solves abstract reasoning tasks based on Learn-VRF. ARLC features a novel and more broadly applicable training objective for abductive reasoning, resulting in better interpretability and higher accuracy when solving Raven's progressive matrices (RPM). ARLC allows both programming domain knowledge and learning the rules underlying a data distribution. We evaluate ARLC on the I-RAVEN dataset, showcasing state-of-the-art accuracy across both in-distribution and out-of-distribution (unseen attribute-rule pairs) tests.
ARLC surpasses neuro-symbolic and connectionist baselines, including large language models, despite having orders of magnitude fewer parameters. We show ARLC's robustness to post-programming training by incrementally learning from examples on top of programmed knowledge, which only improves its performance and does not result in catastrophic forgetting of the programmed solution. We validate ARLC's seamless transfer learning from a 2x2 RPM constellation to unseen constellations. Our code is available at https://github.com/IBM/abductive-rule-learner-with-context-awareness.

------------

`[2406.18626] An LLM-based Knowledge Synthesis and Scientific Reasoning Framework for Biomedical Discovery <https://arxiv.org/abs/2406.18626>`__ 基于llm的生物医学发现知识合成与科学推理框架

::

    Wed, 26 Jun 2024 14:22:46 GMT
    Oskar Wysocki, Magdalena Wysocka, Danilo Carvalho, Alex Teodor Bogatu, Danilo Miranda Gusicuma, Maxime Delmas, Harriet Unsworth, Andre Freitas

We present BioLunar, developed using the Lunar framework, as a tool for supporting biological analyses, with a particular emphasis on molecular-level evidence enrichment for biomarker discovery in oncology. The platform integrates Large Language Models (LLMs) to facilitate complex scientific reasoning across distributed evidence spaces, enhancing the capability for harmonizing and reasoning over heterogeneous data sources. Demonstrating its utility in cancer research, BioLunar leverages modular design, reusable data access and data analysis components, and a low-code user interface, enabling researchers of all programming levels to construct LLM-enabled scientific workflows. By facilitating automatic scientific discovery and inference from heterogeneous evidence, BioLunar exemplifies the potential of the integration between LLMs, specialised databases and biomedical tools to support expert-level knowledge synthesis and discovery.

------------

`[2406.14283] Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning <https://arxiv.org/abs/2406.14283>`__ Q*:通过审慎规划改进llm的多步骤推理

::

    replaced with revised version Thu, 27 Jun 2024 09:44:45 GMT
    Submission history From: Chaojie Wang [view email]
    [v1] Thu, 20 Jun 2024 13:08:09 UTC (512 KB)
    [v2] Mon, 24 Jun 2024 07:50:56 UTC (513 KB)
    [v3] Thu, 27 Jun 2024 09:44:45 UTC (513 KB)
    Chaojie Wang, Yanchen Deng, Zhiyi Lv, Zeng Liang, Jujie He, Shuicheng Yan, An Bo

Large Language Models (LLMs) have demonstrated impressive capability in many natural language tasks. However, the auto-regressive generation process makes LLMs prone to produce errors, hallucinations and inconsistent statements when performing multi-step reasoning. In this paper, by casting multi-step reasoning of LLMs as a heuristic search problem, we aim to alleviate the pathology by introducing Q*, a general, versatile and agile framework for guiding LLMs decoding process with deliberative planning. By learning a plug-and-play Q-value model as heuristic function for estimating expected future rewards, our Q* can effectively guide LLMs to select the most promising next reasoning step without fine-tuning LLMs for the current task, which avoids the significant computational overhead and potential risk of performance degeneration on other tasks. Extensive experiments on GSM8K, MATH and MBPP demonstrate the superiority of our method, contributing to improving the reasoning performance of existing open-source LLMs.

------------

`[2401.03183] Exploring Defeasibility in Causal Reasoning <https://arxiv.org/abs/2401.03183>`__ 探索因果推理的可废止性

::

    replaced with revised version Thu, 27 Jun 2024 10:08:05 GMT
    Submission history From: Shaobo Cui [view email]
    [v1] Sat, 6 Jan 2024 10:08:33 UTC (6,859 KB)
    [v2] Thu, 27 Jun 2024 10:08:05 UTC (4,590 KB)
    Shaobo Cui, Lazar Milikic, Yiyang Feng, Mete Ismayilzada, Debjit Paul, Antoine Bosselut, Boi Faltings

Defeasibility in causal reasoning implies that the causal relationship between cause and effect can be strengthened or weakened. Namely, the causal strength between cause and effect should increase or decrease with the incorporation of strengthening arguments (supporters) or weakening arguments (defeaters), respectively. However, existing works ignore defeasibility in causal reasoning and fail to evaluate existing causal strength metrics in defeasible settings. In this work, we present $\delta$-CAUSAL, the first benchmark dataset for studying defeasibility in causal reasoning. $\delta$-CAUSAL includes around 11K events spanning ten domains, featuring defeasible causality pairs, i.e., cause-effect pairs accompanied by supporters and defeaters. We further show current causal strength metrics fail to reflect the change of causal strength with the incorporation of supporters or defeaters in $\delta$-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation with Attention Rating), a metric that measures causal strength based on token-level causal relationships. CESAR achieves a significant 69.7% relative improvement over existing metrics, increasing from 47.2% to 80.1% in capturing the causal strength change brought by supporters and defeaters. We further demonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and 10.7 points behind humans in generating supporters and defeaters, emphasizing the challenge posed by $\delta$-CAUSAL.

------------

`[2401.08967] ReFT: Reasoning with Reinforced Fine-Tuning <https://arxiv.org/abs/2401.08967>`__ ReFT:强化微调推理

::

    replaced with revised version Thu, 27 Jun 2024 15:29:15 GMT
    Submission history From: Zhanming Jie [view email]
    [v1] Wed, 17 Jan 2024 04:43:21 UTC (3,687 KB)
    [v2] Thu, 27 Jun 2024 15:29:15 UTC (5,080 KB)
    Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li

One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.

------------

`[2402.18344] Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning <https://arxiv.org/abs/2402.18344>`__ 专注于你的问题!在常识推理中解释和缓解有毒的CoT问题

::

    replaced with revised version Thu, 27 Jun 2024 06:54:58 GMT
    Submission history From: Jiachun Li [view email]
    [v1] Wed, 28 Feb 2024 14:09:02 UTC (746 KB)
    [v2] Thu, 27 Jun 2024 06:54:58 UTC (751 KB)
    Jiachun Li, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao

Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model's overall commonsense reasoning performance (increased by 5.5%).

------------

`[2406.13808] Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning? <https://arxiv.org/abs/2406.13808>`__ llm中的低秩知识蒸馏对微电子推理有用吗?

::

    replaced with revised version Thu, 27 Jun 2024 06:37:21 GMT
    Submission history From: Nirjhor Tahmidur Rouf [view email]
    [v1] Wed, 19 Jun 2024 20:14:39 UTC (1,127 KB)
    [v2] Fri, 21 Jun 2024 02:25:57 UTC (1,127 KB)
    [v3] Thu, 27 Jun 2024 06:37:21 UTC (1,126 KB)
    Nirjhor Rouf, Fin Amin, Paul D. Franzon

In this work, we present empirical results regarding the feasibility of using offline large language models (LLMs) in the context of electronic design automation (EDA). The goal is to investigate and evaluate a contemporary language model's (Llama-2-7B) ability to function as a microelectronic Q & A expert as well as its reasoning, and generation capabilities in solving microelectronic-related problems. Llama-2-7B was tested across a variety of adaptation methods, including introducing a novel low-rank knowledge distillation (LoRA-KD) scheme. Our experiments produce both qualitative and quantitative results.

------------

-----------
ToolUse (1)
-----------

`[2406.19228] Tools Fail: Detecting Silent Errors in Faulty Tools <https://arxiv.org/abs/2406.19228>`__ Tools Fail:在故障工具中检测静默错误

::

    Thu, 27 Jun 2024 14:52:34 GMT
    Jimin Sun, So Yeon Min, Yingshan Chang, Yonatan Bisk

Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not in their weights, to perform tasks on the web, and even to control robots.
However, most ontologies and surveys of tool-use have assumed the core challenge for LLMs is choosing the tool. Instead, we introduce a framework for tools more broadly which guides us to explore a model's ability to detect "silent" tool errors, and reflect on how to plan. This more directly aligns with the increasingly popular use of models as tools. We provide an initial approach to failure recovery with promising results both on a controlled calculator setting and embodied agent planning.

------------

------------------------
Retrieval-Augmented (10)
------------------------

`[2406.18676] Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation <https://arxiv.org/abs/2406.18676>`__ 理解LLM需要什么:检索增强生成的双重偏好对齐

::

    Wed, 26 Jun 2024 18:26:53 GMT
    Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, Ji-Rong Wen

Retrieval-augmented generation (RAG) has demonstrated effectiveness in mitigating the hallucination problem of large language models (LLMs). However, the difficulty of aligning the retriever with the diverse LLMs' knowledge preferences inevitably poses an inevitable challenge in developing a reliable RAG system. To address this issue, we propose DPA-RAG, a universal framework designed to align diverse knowledge preferences within RAG systems.
Specifically, we initially introduce a preference knowledge construction pipline and incorporate five novel query augmentation strategies to alleviate preference data scarcity. Based on preference data, DPA-RAG accomplishes both external and internal preference alignment: 1) It jointly integrate pair-wise, point-wise, and contrastive preference alignment abilities into the reranker, achieving external preference alignment among RAG components. 2) It further introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT), enabling LLMs to implicitly capture knowledge aligned with their reasoning preferences, achieving LLMs' internal alignment. Experimental results across four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all baselines and seamlessly integrates both black-box and open-sourced LLM readers. Further qualitative analysis and discussions also provide empirical guidance for achieving reliable RAG systems. Our code is publicly available at https://github.com/dongguanting/DPA-RAG.

------------

`[2406.19215] SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation <https://arxiv.org/abs/2406.19215>`__ SeaKR:自适应检索增强生成的自感知知识检索

::

    Thu, 27 Jun 2024 14:38:33 GMT
    Zijun Yao and Weijian Qi and Liangming Pan and Shulin Cao and Linmei Hu and Weichuan Liu and Lei Hou and Juanzi Li

This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel adaptive RAG model that extracts self-aware uncertainty of LLMs from their internal states. SeaKR activates retrieval when the LLMs present high self-aware uncertainty for generation. To effectively integrate retrieved knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty to preserve the snippet that reduces their uncertainty to the utmost. To facilitate solving complex tasks that require multiple retrievals, SeaKR utilizes their self-aware uncertainty to choose among different reasoning strategies. Our experiments on both complex and simple Question Answering datasets show that SeaKR outperforms existing adaptive RAG methods. We release our code at https://github.com/THU-KEG/SeaKR.

------------

`[2406.19251] AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation <https://arxiv.org/abs/2406.19251>`__ 

::

    Thu, 27 Jun 2024 15:18:21 GMT
    Jia Fu, Xiaoting Qin, Fangkai Yang, Lu Wang, Jue Zhang, Qingwei Lin, Yubo Chen, Dongmei Zhang, Saravan Rajmohan, Qi Zhang

Recent advancements in Large Language Models have transformed ML/AI development, necessitating a reevaluation of AutoML principles for the Retrieval-Augmented Generation (RAG) systems. To address the challenges of hyper-parameter optimization and online adaptation in RAG, we propose the AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical MAB (Hier-MAB) method for efficient exploration of large search spaces. We conduct extensive experiments on tuning hyper-parameters, such as top-k retrieved documents, prompt compression ratio, and embedding methods, using the ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly optimization all three hyper-parameters demonstrate that MAB-based online learning methods can achieve Recall@5 $\approx 0.8$ for scenarios with prominent gradients in search space, using only $\sim20\%$ of the LLM API calls required by the Grid Search approach. Additionally, the proposed Hier-MAB approach outperforms other baselines in more challenging optimization scenarios. The code will be made available at https://aka.ms/autorag.

------------

`[2406.19188] Averaging log-likelihoods in direct alignment <https://arxiv.org/abs/2406.19188>`__ 对数似然的直接对齐平均

::

    Thu, 27 Jun 2024 14:07:38 GMT
    Nathan Grinsztajn, Yannis Flet-Berliac, Mohammad Gheshlaghi Azar, Florian Strub, Bill Wu, Eugene Choi, Chris Cremer, Arash Ahmadian, Yash Chandak, Olivier Pietquin, Matthieu Geist

To better align Large Language Models (LLMs) with human judgment, Reinforcement Learning from Human Feedback (RLHF) learns a reward model and then optimizes it using regularized RL. Recently, direct alignment methods were introduced to learn such a fine-tuned model directly from a preference dataset without computing a proxy reward function. These methods are built upon contrastive losses involving the log-likelihood of (dis)preferred completions according to the trained model. However, completions have various lengths, and the log-likelihood is not length-invariant. On the other side, the cross-entropy loss used in supervised training is length-invariant, as batches are typically averaged token-wise. To reconcile these approaches, we introduce a principled approach for making direct alignment length-invariant. Formally, we introduce a new averaging operator, to be composed with the optimality operator giving the best policy for the underlying RL problem. It translates into averaging the log-likelihood within the loss. We empirically study the effect of such averaging, observing a trade-off between the length of generations and their scores.

------------

`[2406.19292] From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data <https://arxiv.org/abs/2406.19292>`__ 从人工针到真正的干草堆:通过对合成数据的微调提高llm的检索能力

::

    Thu, 27 Jun 2024 16:05:13 GMT
    Zheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, Dimitris Papailiopoulos

Recent studies have shown that Large Language Models (LLMs) struggle to accurately retrieve information and maintain reasoning capabilities when processing long-context inputs. To address these limitations, we propose a finetuning approach utilizing a carefully designed synthetic dataset comprising numerical key-value retrieval tasks. Our experiments on models like GPT-3.5 Turbo and Mistral 7B demonstrate that finetuning LLMs on this dataset significantly improves LLMs' information retrieval and reasoning capabilities in longer-context settings. We present an analysis of the finetuned models, illustrating the transfer of skills from synthetic to real task evaluations (e.g., $10.5\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5 Turbo). We also find that finetuned LLMs' performance on general benchmarks remains almost constant while LLMs finetuned on other baseline long-context augmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B finetuned on our synthetic data cause no performance drop while other baseline data can cause a drop that ranges from $2.33\%$ to $6.19\%$). Our study highlights the potential of finetuning on synthetic data for improving the performance of LLMs on longer-context tasks.

------------

`[2406.18535] DRAK: Unlocking Molecular Insights with Domain-Specific Retrieval-Augmented Knowledge in LLMs <https://arxiv.org/abs/2406.18535>`__ DRAK:用llm中特定领域检索增强知识解锁分子洞见

::

    Mon, 4 Mar 2024 15:04:05 GMT
    Jinzhe Liu, Xiangsheng Huang, Zhuo Chen, Yin Fang

Large Language Models (LLMs) encounter challenges with the unique syntax of specific domains, such as biomolecules. Existing fine-tuning or modality alignment techniques struggle to bridge the domain knowledge gap and understand complex molecular data, limiting LLMs' progress in specialized fields. To overcome these limitations, we propose an expandable and adaptable non-parametric knowledge injection framework named Domain-specific Retrieval-Augmented Knowledge (DRAK), aimed at enhancing reasoning capabilities in specific domains. Utilizing knowledge-aware prompts and gold label-induced reasoning, DRAK has developed profound expertise in the molecular domain and the capability to handle a broad spectrum of analysis tasks. We evaluated two distinct forms of DRAK variants, proving that DRAK exceeds previous benchmarks on six molecular tasks within the Mol-Instructions dataset. Extensive experiments have underscored DRAK's formidable performance and its potential to unlock molecular insights, offering a unified paradigm for LLMs to tackle knowledge-intensive tasks in specific domains. Our code will be available soon.

------------

`[2406.19150] RAVEN: Multitask Retrieval Augmented Vision-Language Learning <https://arxiv.org/abs/2406.19150>`__ 

::

    Thu, 27 Jun 2024 13:08:35 GMT
    Varun Nagaraj Rao, Siddharth Choudhary, Aditya Deshpande, Ravi Kumar Satzoda, Srikar Appalaraju

The scaling of large language models to encode all the world's knowledge in model parameters is unsustainable and has exacerbated resource barriers.
Retrieval-Augmented Generation (RAG) presents a potential solution, yet its application to vision-language models (VLMs) is under explored. Existing methods focus on models designed for single tasks. Furthermore, they're limited by the need for resource intensive pre training, additional parameter requirements, unaddressed modality prioritization and lack of clear benefit over non-retrieval baselines. This paper introduces RAVEN, a multitask retrieval augmented VLM framework that enhances base VLMs through efficient, task specific fine-tuning. By integrating retrieval augmented samples without the need for additional retrieval-specific parameters, we show that the model acquires retrieval properties that are effective across multiple tasks. Our results and extensive ablations across retrieved modalities for the image captioning and VQA tasks indicate significant performance improvements compared to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a +3\% accuracy on specific VQA question types. This underscores the efficacy of applying RAG approaches to VLMs, marking a stride toward more efficient and accessible multimodal learning.

------------

`[2406.19234] Seeing Is Believing: Black-Box Membership Inference Attacks Against Retrieval Augmented Generation <https://arxiv.org/abs/2406.19234>`__ 眼见为实:针对检索增强生成的黑盒成员推断攻击

::

    Thu, 27 Jun 2024 14:58:38 GMT
    Yuying Li, Gaoyang Liu, Yang Yang, Chen Wang

Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that enhances Large Language Models (LLMs) by retrieving relevant knowledge from an external, non-parametric database. This approach aims to mitigate common LLM issues such as hallucinations and outdated knowledge. Although existing research has demonstrated security and privacy vulnerabilities within RAG systems, making them susceptible to attacks like jailbreaks and prompt injections, the security of the RAG system's external databases remains largely underexplored. In this paper, we employ Membership Inference Attacks (MIA) to determine whether a sample is part of the knowledge database of a RAG system, using only black-box API access. Our core hypothesis posits that if a sample is a member, it will exhibit significant similarity to the text generated by the RAG system. To test this, we compute the cosine similarity and the model's perplexity to establish a membership score, thereby building robust features.
We then introduce two novel attack strategies: a Threshold-based Attack and a Machine Learning-based Attack, designed to accurately identify membership.
Experimental validation of our methods has achieved a ROC AUC of 82%.

------------

`[2406.11497] CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation in RAG <https://arxiv.org/abs/2406.11497>`__ CrAM:基于可信度感知的llm注意力修正以对抗RAG中的错误信息

::

    replaced with revised version Thu, 27 Jun 2024 10:18:53 GMT
    Submission history From: Boyi Deng [view email]
    [v1] Mon, 17 Jun 2024 13:01:12 UTC (1,001 KB)
    [v2] Thu, 27 Jun 2024 10:18:53 UTC (1,001 KB)
    Boyi Deng, Wenjie Wang, Fengbin Zhu, Qifan Wang, Fuli Feng

Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large Language Models (LLMs) by referencing external documents. However, the misinformation in external documents may mislead LLMs' generation. To address this issue, we explore the task of "credibility-aware RAG", in which LLMs automatically adjust the influence of retrieved documents based on their credibility scores to counteract misinformation. To this end, we introduce a plug-and-play method named $\textbf{Cr}$edibility-aware $\textbf{A}$ttention $\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in LLMs and adjusts their attention weights based on the credibility of the documents, thereby reducing the impact of low-credibility documents. Experiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and Qwen-7B show that CrAM improves the RAG performance of LLMs against misinformation pollution by over 20%, even surpassing supervised fine-tuning methods.

------------

`[2406.17651] Leveraging Large Language Models for Software Model Completion: Results from Industrial and Public Datasets <https://arxiv.org/abs/2406.17651>`__ 利用大型语言模型完成软件模型:来自工业和公共数据集的结果

::

    replaced with revised version Wed, 26 Jun 2024 17:43:15 GMT
    Submission history From: Christof Tinnes [view email]
    [v1] Tue, 25 Jun 2024 15:43:20 UTC (1,507 KB)
    [v2] Wed, 26 Jun 2024 17:43:15 UTC (1,507 KB)
    Christof Tinnes, Alisa Welter, Sven Apel

Modeling structure and behavior of software systems plays a crucial role in the industrial practice of software engineering. As with other software engineering artifacts, software models are subject to evolution. Supporting modelers in evolving software models with recommendations for model completions is still an open problem, though. In this paper, we explore the potential of large language models for this task. In particular, we propose an approach, retrieval-augmented generation, leveraging large language models, model histories, and retrieval-augmented generation for model completion. Through experiments on three datasets, including an industrial application, one public open-source community dataset, and one controlled collection of simulated model repositories, we evaluate the potential of large language models for model completion with retrieval-augmented generation. We found that large language models are indeed a promising technology for supporting software model evolution (62.30% semantically correct completions on real-world industrial data and up to 86.19% type-correct completions). The general inference capabilities of large language models are particularly useful when dealing with concepts for which there are few, noisy, or no examples at all.

------------

---------
Agent (5)
---------

`[2406.19226] Simulating Classroom Education with LLM-Empowered Agents <https://arxiv.org/abs/2406.19226>`__ 基于llm授权agent的课堂教学模拟

::

    Thu, 27 Jun 2024 14:51:07 GMT
    Zheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhiyuan Liu, Lei Hou, Juanzi Li

Large language models (LLMs) have been employed in various intelligent educational tasks to assist teaching. While preliminary explorations have focused on independent LLM-empowered agents for specific educational tasks, the potential for LLMs within a multi-agent collaborative framework to simulate a classroom with real user participation remains unexplored. In this work, we propose SimClass, a multi-agent classroom simulation framework involving user participation. We recognize representative class roles and introduce a novel class control mechanism for automatic classroom teaching, and conduct user experiments in two real-world courses. Utilizing the Flanders Interactive Analysis System and Community of Inquiry theoretical frame works from educational analysis, we demonstrate that LLMs can simulate traditional classroom interaction patterns effectively while enhancing user's experience.
We also observe emergent group behaviors among agents in SimClass, where agents collaborate to create enlivening interactions in classrooms to improve user learning process. We hope this work pioneers the application of LLM-empowered multi-agent systems in virtual classroom teaching.

------------

`[2406.18702] Simulating The U.S. Senate: An LLM-Driven Agent Approach to Modeling Legislative Behavior and Bipartisanship <https://arxiv.org/abs/2406.18702>`__ 模拟美国参议院:llm驱动的代理方法建模立法行为和两党合作

::

    Wed, 26 Jun 2024 19:10:51 GMT
    Zachary R. Baker and Zarif L. Azher

This study introduces a novel approach to simulating legislative processes using LLM-driven virtual agents, focusing on the U.S. Senate Intelligence Committee. We developed agents representing individual senators and placed them in simulated committee discussions. The agents demonstrated the ability to engage in realistic debate, provide thoughtful reflections, and find bipartisan solutions under certain conditions. Notably, the simulation also showed promise in modeling shifts towards bipartisanship in response to external perturbations. Our results indicate that this LLM-driven approach could become a valuable tool for understanding and potentially improving legislative processes, supporting a broader pattern of findings highlighting how LLM-based agents can usefully model real-world phenomena. Future works will focus on enhancing agent complexity, expanding the simulation scope, and exploring applications in policy testing and negotiation.

------------

`[2310.10701] Theory of Mind for Multi-Agent Collaboration via Large Language Models <https://arxiv.org/abs/2310.10701>`__ 基于大型语言模型的多agent协作心智理论

::

    replaced with revised version Wed, 26 Jun 2024 20:15:34 GMT
    Submission history From: Huao Li [view email]
    [v1] Mon, 16 Oct 2023 07:51:19 UTC (269 KB)
    [v2] Sun, 22 Oct 2023 03:49:34 UTC (529 KB)
    [v3] Wed, 26 Jun 2024 20:15:34 UTC (530 KB)
    Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, Katia Sycara

While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.

------------

`[2402.09742] AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator <https://arxiv.org/abs/2402.09742>`__ 

::

    replaced with revised version Thu, 27 Jun 2024 15:40:53 GMT
    Submission history From: Zhihao Fan [view email]
    [v1] Thu, 15 Feb 2024 06:46:48 UTC (9,452 KB)
    [v2] Wed, 21 Feb 2024 08:25:25 UTC (9,452 KB)
    [v3] Thu, 27 Jun 2024 15:40:53 UTC (2,883 KB)
    [v4] Fri, 28 Jun 2024 03:11:48 UTC (2,883 KB)
    Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou

Artificial intelligence has significantly advanced healthcare, particularly through large language models (LLMs) that excel in medical question answering benchmarks. However, their real-world clinical application remains limited due to the complexities of doctor-patient interactions. To address this, we introduce \textbf{AI Hospital}, a multi-agent framework simulating dynamic medical interactions between \emph{Doctor} as player and NPCs including \emph{Patient}, \emph{Examiner}, \emph{Chief Physician}. This setup allows for realistic assessments of LLMs in clinical scenarios. We develop the Multi-View Medical Evaluation (MVME) benchmark, utilizing high-quality Chinese medical records and NPCs to evaluate LLMs' performance in symptom collection, examination recommendations, and diagnoses. Additionally, a dispute resolution collaborative mechanism is proposed to enhance diagnostic accuracy through iterative discussions. Despite improvements, current LLMs exhibit significant performance gaps in multi-turn interactions compared to one-step approaches. Our findings highlight the need for further research to bridge these gaps and improve LLMs' clinical diagnostic capabilities. Our data, code, and experimental results are all open-sourced at \url{this https URL}.

------------

`[2403.17927] MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution <https://arxiv.org/abs/2403.17927>`__ 

::

    replaced with revised version Thu, 27 Jun 2024 12:40:12 GMT
    Submission history From: Wei Tao [view email]
    [v1] Tue, 26 Mar 2024 17:57:57 UTC (1,202 KB)
    [v2] Thu, 27 Jun 2024 12:40:12 UTC (4,251 KB)
    Wei Tao, Yucheng Zhou, Yanlin Wang, Wenqiang Zhang, Hongyu Zhang, Yu Cheng

In software development, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing code. Large Language Models (LLMs) have shown promise in code generation but face difficulties in resolving Github issues, particularly at the repository level. To overcome this challenge, we empirically study the reason why LLMs fail to resolve GitHub issues and analyze the major factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, significantly outperforming the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the advanced LLM.

------------

----------
Other (75)
----------

`[2406.18740] Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models <https://arxiv.org/abs/2406.18740>`__ 分步重排序:研究大型语言模型重排序的预过滤

::

    Wed, 26 Jun 2024 20:12:24 GMT
    Baharan Nouriinanloo and Maxime Lamothe

Large Language Models (LLMs) have been revolutionizing a myriad of natural language processing tasks with their diverse zero-shot capabilities. Indeed, existing work has shown that LLMs can be used to great effect for many tasks, such as information retrieval (IR), and passage ranking. However, current state-of-the-art results heavily lean on the capabilities of the LLM being used. Currently, proprietary, and very large LLMs such as GPT-4 are the highest performing passage re-rankers. Hence, users without the resources to leverage top of the line LLMs, or ones that are closed source, are at a disadvantage. In this paper, we investigate the use of a pre-filtering step before passage re-ranking in IR. Our experiments show that by using a small number of human generated relevance scores, coupled with LLM relevance scoring, it is effectively possible to filter out irrelevant passages before re-ranking. Our experiments also show that this pre-filtering then allows the LLM to perform significantly better at the re-ranking task. Indeed, our results show that smaller models such as Mixtral can become competitive with much larger proprietary models (e.g., ChatGPT and GPT-4).

------------

`[2406.18776] Implicit Discourse Relation Classification For Nigerian Pidgin <https://arxiv.org/abs/2406.18776>`__ 面向尼日利亚洋泾浜语的隐式篇章关系分类

::

    Wed, 26 Jun 2024 22:10:15 GMT
    Muhammed Saeed, Peter Bourgonje, Vera Demberg

Despite attempts to make Large Language Models multi-lingual, many of the world's languages are still severely under-resourced. This widens the performance gap between NLP and AI applications aimed at well-financed, and those aimed at less-resourced languages. In this paper, we focus on Nigerian Pidgin (NP), which is spoken by nearly 100 million people, but has comparatively very few NLP resources and corpora. We address the task of Implicit Discourse Relation Classification (IDRC) and systematically compare an approach translating NP data to English and then using a well-resourced IDRC tool and back-projecting the labels versus creating a synthetic discourse corpus for NP, in which we translate PDTB and project PDTB labels, and then train an NP IDR classifier. The latter approach of learning a "native" NP classifier outperforms our baseline by 13.27\% and 33.98\% in f$_{1}$ score for 4-way and 11-way classification, respectively.

------------

`[2406.18783] Psychological Profiling in Cybersecurity: A Look at LLMs and Psycholinguistic Features <https://arxiv.org/abs/2406.18783>`__ 

::

    Wed, 26 Jun 2024 23:04:52 GMT
    Jean Marie Tshimula, D'Jeff K. Nkashama, Jean Tshibangu Muabila, Ren\'e Manass\'e Galekwa, Hugues Kanda, Maximilien V. Dialufuma, Mbuyi Mukendi Didier, Kalala Kalonji, Serge Mundele, Patience Kinshie Lenye, Tighana Wenge Basele, Aristarque Ilunga, Christian N. Mayemba, Nathana\"el M. Kasoro, Selain K. Kasereka, Hardy Mikese, Pierre-Martin Tardif, Marc Frappier, Froduald Kabanza, Belkacem Chikhaoui, Shengrui Wang, Ali Mulenda Sumbu, Xavier Ndona, Raoul Kienge-Kienge Intudi

The increasing sophistication of cyber threats necessitates innovative approaches to cybersecurity. In this paper, we explore the potential of psychological profiling techniques, particularly focusing on the utilization of Large Language Models (LLMs) and psycholinguistic features. We investigate the intersection of psychology and cybersecurity, discussing how LLMs can be employed to analyze textual data for identifying psychological traits of threat actors. We explore the incorporation of psycholinguistic features, such as linguistic patterns and emotional cues, into cybersecurity frameworks. \iffalse Through case studies and experiments, we discuss the effectiveness of these methods in enhancing threat detection and mitigation strategies.\fi Our research underscores the importance of integrating psychological perspectives into cybersecurity practices to bolster defense mechanisms against evolving threats.

------------

`[2406.18856] FFN: a Fine-grained Chinese-English Financial Domain Parallel Corpus <https://arxiv.org/abs/2406.18856>`__ FFN:一个细粒度的汉英金融领域平行语料库

::

    Thu, 27 Jun 2024 02:53:55 GMT
    Yuxin Fu, Shijing Si, Leyi Mai, Xi-ang Li

Large Language Models (LLMs) have stunningly advanced the field of machine translation, though their effectiveness within the financial domain remains largely underexplored. To probe this issue, we constructed a fine-grained Chinese-English parallel corpus of financial news called FFN. We acquired financial news articles spanning between January 1st, 2014, to December 31, 2023, from mainstream media websites such as CNN, FOX, and China Daily. The dataset consists of 1,013 main text and 809 titles, all of which have been manually corrected. We measured the translation quality of two LLMs -- ChatGPT and ERNIE-bot, utilizing BLEU, TER and chrF scores as the evaluation metrics.
For comparison, we also trained an OpenNMT model based on our dataset. We detail problems of LLMs and provide in-depth analysis, intending to stimulate further research and solutions in this largely uncharted territory. Our research underlines the need to optimize LLMs within the specific field of financial translation to ensure accuracy and quality.

------------

`[2406.18859] Two-Pronged Human Evaluation of ChatGPT Self-Correction in Radiology Report Simplification <https://arxiv.org/abs/2406.18859>`__ ChatGPT自校正在放射学报告简化中的双管齐下人工评价

::

    Thu, 27 Jun 2024 03:05:35 GMT
    Ziyu Yang, Santhosh Cherian, Slobodan Vucetic

Radiology reports are highly technical documents aimed primarily at doctor-doctor communication. There has been an increasing interest in sharing those reports with patients, necessitating providing them patient-friendly simplifications of the original reports. This study explores the suitability of large language models in automatically generating those simplifications. We examine the usefulness of chain-of-thought and self-correction prompting mechanisms in this domain. We also propose a new evaluation protocol that employs radiologists and laypeople, where radiologists verify the factual correctness of simplifications, and laypeople assess simplicity and comprehension. Our experimental results demonstrate the effectiveness of self-correction prompting in producing high-quality simplifications. Our findings illuminate the preferences of radiologists and laypeople regarding text simplification, informing future research on this topic.

------------

`[2406.18880] SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models <https://arxiv.org/abs/2406.18880>`__ SSP:基于大型语言模型的跨语言迁移到低资源语言的自监督提示

::

    Thu, 27 Jun 2024 04:21:59 GMT
    Vipul Rathore, Aniruddha Deb, Ankish Chandresh, Parag Singla, Mausam

Recently, very large language models (LLMs) have shown exceptional performance on several English NLP tasks with just in-context learning (ICL), but their utility in other languages is still underexplored. We investigate their effectiveness for NLP tasks in low-resource languages (LRLs), especially in the setting of zero-labelled cross-lingual transfer (0-CLT), where no labelled training data for the target language is available -- however training data from one or more related medium-resource languages (MRLs) is utilized, alongside the available unlabeled test data for a target language. We introduce Self-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT setting.
SSP is based on the key observation that LLMs output more accurate labels if in-context exemplars are from the target language (even if their labels are slightly noisy). To operationalize this, since target language training data is not available in 0-CLT, SSP operates in two stages. In Stage I, using source MRL training data, target language's test data is noisily labeled. In Stage II, these noisy test data points are used as exemplars in ICL for further improved labelling. Additionally, our implementation of SSP uses a novel Integer Linear Programming (ILP)-based exemplar selection that balances similarity, prediction confidence (when available) and label coverage. Experiments on three tasks and eleven LRLs (from three regions) demonstrate that SSP strongly outperforms existing SOTA fine-tuned and prompting-based baselines in 0-CLT setup.

------------

`[2406.18895] Can we teach language models to gloss endangered languages? <https://arxiv.org/abs/2406.18895>`__ 我们可以教语言模型来美化濒危语言吗?

::

    Thu, 27 Jun 2024 05:17:04 GMT
    Michael Ginn, Mans Hulden, and Alexis Palmer

Interlinear glossed text (IGT) is a popular format in language documentation projects, where each morpheme is labeled with a descriptive annotation.
Automating the creation of interlinear glossed text can be desirable to reduce annotator effort and maintain consistency across annotated corpora. Prior research has explored a number of statistical and neural methods for automatically producing IGT.
As large language models (LLMs) have showed promising results across multilingual tasks, even for rare, endangered languages, it is natural to wonder whether they can be utilized for the task of generating IGT. We explore whether LLMs can be effective at the task of interlinear glossing with in-context learning, without any traditional training. We propose new approaches for selecting examples to provide in-context, observing that targeted selection can significantly improve performance. We find that LLM-based methods beat standard transformer baselines, despite requiring no training at all. These approaches still underperform state-of-the-art supervised systems for the task, but are highly practical for researchers outside of the NLP community, requiring minimal effort to use.

------------

`[2406.18906] Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets <https://arxiv.org/abs/2406.18906>`__ 是不是十四行诗，机器人?大型模型和数据集的诗歌评估

::

    Thu, 27 Jun 2024 05:36:53 GMT
    Melanie Walsh, Anna Preus, Maria Antoniak

Large language models (LLMs) can now generate and recognize text in a wide range of styles and genres, including highly specialized, creative genres like poetry. But what do LLMs really know about poetry? What can they know about poetry? We develop a task to evaluate how well LLMs recognize a specific aspect of poetry, poetic form, for more than 20 forms and formal elements in the English language. Poetic form captures many different poetic features, including rhyme scheme, meter, and word or line repetition. We use this task to reflect on LLMs' current poetic capabilities, as well as the challenges and pitfalls of creating NLP benchmarks for poetry and for other creative tasks. In particular, we use this task to audit and reflect on the poems included in popular pretraining datasets. Our findings have implications for NLP researchers interested in model evaluation, digital humanities and cultural analytics scholars, and cultural heritage professionals.

------------

`[2406.18916] TrustUQA: A Trustful Framework for Unified Structured Data Question Answering <https://arxiv.org/abs/2406.18916>`__ TrustUQA:一种可信的统一结构化数据问答框架

::

    Thu, 27 Jun 2024 06:13:05 GMT
    Wen Zhang, Long Jin, Yushan Zhu, Jiaoyan Chen, Zhiwei Huang, Junjie Wang, Yin Hua, Lei Liang, Huajun Chen

Natural language question answering (QA) over structured data sources such as tables and knowledge graphs (KGs) have been widely investigated, for example with Large Language Models (LLMs). The main solutions include question to formal query parsing and retrieval-based answer generation. However, current methods of the former often suffer from weak generalization, failing to dealing with multiple sources simultaneously, while the later is limited in trustfulness. In this paper, we propose UnifiedTQA, a trustful QA framework that can simultaneously support multiple types of structured data in a unified way. To this end, it adopts an LLM-friendly and unified knowledge representation method called Condition Graph (CG), and uses an LLM and demonstration-based two-level method for CG querying. For enhancement, it is also equipped with dynamic demonstration retrieval. We have evaluated UnifiedTQA with 5 benchmarks covering 3 types of structured data. It outperforms 2 existing unified structured data QA methods and in comparison with the baselines that are specific to a data type, it achieves state-of-the-art on 2 of them. Further more, we demonstrates potential of our method for more general QA tasks, QA over mixed structured data and QA across structured data.

------------

`[2406.18921] Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data <https://arxiv.org/abs/2406.18921>`__ 

::

    Thu, 27 Jun 2024 06:24:00 GMT
    Yiting Ran, Xintao Wang, Rui Xu, Xinfeng Yuan, Jiaqing Liang, Yanghua Xiao, Deqing Yang

Role-playing agents (RPA) have been a popular application area for large language models (LLMs), attracting significant interest from both industry and academia.While existing RPAs well portray the characters' knowledge and tones, they face challenges in capturing their minds, especially for small role-playing language models (RPLMs). In this paper, we propose to enhance RPLMs via personality-indicative data. Specifically, we leverage questions from psychological scales and distill advanced RPAs to generate dialogues that grasp the minds of characters. Experimental results validate that RPLMs trained with our dataset exhibit advanced role-playing capabilities for both general and personality-related evaluations. Code and data are available at \href{https://github.com/alienet1109/RolePersonality}{this URL}.

------------

`[2406.18966] UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models <https://arxiv.org/abs/2406.18966>`__ UniGen:使用大型语言模型生成文本数据集的统一框架

::

    Thu, 27 Jun 2024 07:56:44 GMT
    Siyuan Wu, Yue Huang, Chujie Gao, Dongping Chen, Qihui Zhang, Yao Wan, Tianyi Zhou, Xiangliang Zhang, Jianfeng Gao, Chaowei Xiao, Lichao Sun

Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents UniGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. UniGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, UniGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by UniGen, and each module within UniGen plays a critical role in this enhancement. Additionally, UniGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that UniGen effectively supports dynamic and evolving benchmarking, and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.

------------

`[2406.19032] Improving Weak-to-Strong Generalization with Reliability-Aware Alignment <https://arxiv.org/abs/2406.19032>`__ 基于可靠性感知的对齐改进弱到强泛化

::

    Thu, 27 Jun 2024 09:37:34 GMT
    Yue Guo, Yi Yang

Large language models (LLMs) are now rapidly advancing and surpassing human abilities on many natural language tasks. However, aligning these super-human LLMs with human knowledge remains challenging because the supervision signals from human annotators may be wrong. This issue, known as the "super-alignment" problem, requires enhancing weak-to-strong generalization, where a strong LLM must generalize from imperfect supervision provided by a weaker source. To address this issue, we propose an approach to improve weak-to-strong generalization by involving the reliability of weak supervision signals in the alignment process. In our method, we query the weak supervisor for multiple answers, estimate the answer reliability, and enhance the alignment process by filtering out uncertain data or re-weighting reliable data. Experiments on four datasets demonstrate that our methods effectively identify the quality of weak labels and significantly enhance weak-to-strong generalization. Our work presents effective techniques for error-robust model alignment, reducing error propagation from noisy supervision and enhancing the accuracy and reliability of LLMs. Codes are publicly available at http://github.com/Irenehere/ReliableAlignment.

------------

`[2406.19065] STBench: Assessing the Ability of Large Language Models in Spatio-Temporal Analysis <https://arxiv.org/abs/2406.19065>`__ STBench:评估大型语言模型在时空分析中的能力

::

    Thu, 27 Jun 2024 10:34:02 GMT
    Wenbin Li, Di Yao, Ruibo Zhao, Wenjie Chen, Zijie Xu, Chengxue Luo, Chang Gong, Quanliang Jing, Haining Tan, Jingping Bi

The rapid evolution of large language models (LLMs) holds promise for reforming the methodology of spatio-temporal data mining. However, current works for evaluating the spatio-temporal understanding capability of LLMs are somewhat limited and biased. These works either fail to incorporate the latest language models or only focus on assessing the memorized spatio-temporal knowledge. To address this gap, this paper dissects LLMs' capability of spatio-temporal data into four distinct dimensions: knowledge comprehension, spatio-temporal reasoning, accurate computation, and downstream applications.
We curate several natural language question-answer tasks for each category and build the benchmark dataset, namely STBench, containing 13 distinct tasks and over 60,000 QA pairs. Moreover, we have assessed the capabilities of 13 LLMs, such as GPT-4o, Gemma and Mistral. Experimental results reveal that existing LLMs show remarkable performance on knowledge comprehension and spatio-temporal reasoning tasks, with potential for further enhancement on other tasks through in-context learning, chain-of-though prompting, and fine-tuning. The code and datasets of STBench are released on https://github.com/LwbXc/STBench.

------------

`[2406.19071] EmPO: Theory-Driven Dataset Construction for Empathetic Response Generation through Preference Optimization <https://arxiv.org/abs/2406.19071>`__ EmPO:基于偏好优化的共情响应生成理论驱动数据集构建

::

    Thu, 27 Jun 2024 10:41:22 GMT
    Ondrej Sotolar

Empathetic response generation is a desirable aspect of conversational agents, crucial for facilitating engaging and emotionally intelligent multi-turn conversations between humans and machines. Leveraging large language models for this task has shown promising results, yet challenges persist in ensuring both the empathetic quality of the responses and retention of the generalization performance of the models. In this paper, we propose a novel approach where we construct theory-driven preference datasets and use them to align LLMs with preference optimization algorithms to address these challenges.
To measure empathetic response generation, we employ the EmpatheticDialogues dataset, assessing empathy with the diff-EPITOME and BERTscore metrics, and evaluate the generalization performance on the MMLU benchmark. We make all datasets, source code, and models publicly available.

------------

`[2406.19102] Statements: Universal Information Extraction from Tables with Large Language Models for ESG KPIs <https://arxiv.org/abs/2406.19102>`__ 

::

    Thu, 27 Jun 2024 11:28:50 GMT
    Lokesh Mishra, Sohayl Dhibi, Yusik Kim, Cesar Berrospi Ramis, Shubham Gupta, Michele Dolfi, Peter Staar

Environment, Social, and Governance (ESG) KPIs assess an organization's performance on issues such as climate change, greenhouse gas emissions, water consumption, waste management, human rights, diversity, and policies. ESG reports convey this valuable quantitative information through tables.
Unfortunately, extracting this information is difficult due to high variability in the table structure as well as content. We propose Statements, a novel domain agnostic data structure for extracting quantitative facts and related information. We propose translating tables to statements as a new supervised deep-learning universal information extraction task. We introduce SemTabNet - a dataset of over 100K annotated tables. Investigating a family of T5-based Statement Extraction Models, our best model generates statements which are 82% similar to the ground-truth (compared to baseline of 21%). We demonstrate the advantages of statements by applying our model to over 2700 tables from ESG reports. The homogeneous nature of statements permits exploratory data analysis on expansive information found in large collections of ESG reports.

------------

`[2406.19116] CHEW: A Dataset of CHanging Events in Wikipedia <https://arxiv.org/abs/2406.19116>`__ CHEW:维基百科中变化事件的数据集

::

    Thu, 27 Jun 2024 11:53:15 GMT
    Hsuvas Borkakoty, Luis Espinosa-Anke

We introduce CHEW, a novel dataset of changing events in Wikipedia expressed in naturally occurring text. We use CHEW for probing LLMs for their timeline understanding of Wikipedia entities and events in generative and classification experiments. Our results suggest that LLMs, despite having temporal information available, struggle to construct accurate timelines. We further show the usefulness of CHEW-derived embeddings for identifying meaning shift.

------------

`[2406.19227] Aligning Teacher with Student Preferences for Tailored Training Data Generation <https://arxiv.org/abs/2406.19227>`__ 为定制训练数据生成调整教师和学生偏好

::

    Thu, 27 Jun 2024 14:51:17 GMT
    Yantao Liu and Zhao Zhang and Zijun Yao and Shulin Cao and Lei Hou and Juanzi Li

Large Language Models (LLMs) have shown significant promise as copilots in various tasks. Local deployment of LLMs on edge devices is necessary when handling privacy-sensitive data or latency-sensitive tasks. The computational constraints of such devices make direct deployment of powerful large-scale LLMs impractical, necessitating the Knowledge Distillation from large-scale models to lightweight models. Lots of work has been done to elicit diversity and quality training examples from LLMs, but little attention has been paid to aligning teacher instructional content based on student preferences, akin to "responsive teaching" in pedagogy. Thus, we propose ARTE, dubbed Aligning TeacheR with StudenT PreferencEs, a framework that aligns the teacher model with student preferences to generate tailored training examples for Knowledge Distillation. Specifically, we elicit draft questions and rationales from the teacher model, then collect student preferences on these questions and rationales using students' performance with in-context learning as a proxy, and finally align the teacher model with student preferences. In the end, we repeat the first step with the aligned teacher model to elicit tailored training examples for the student model on the target task. Extensive experiments on academic benchmarks demonstrate the superiority of ARTE over existing instruction-tuning datasets distilled from powerful LLMs. Moreover, we thoroughly investigate the generalization of ARTE, including the generalization of fine-tuned student models in reasoning ability and the generalization of aligned teacher models to generate tailored training data across tasks and students. In summary, our contributions lie in proposing a novel framework for tailored training example generation, demonstrating its efficacy in experiments, and investigating the generalization of both student & aligned teacher models in ARTE.

------------

`[2406.19238] Revealing Fine-Grained Values and Opinions in Large Language Models <https://arxiv.org/abs/2406.19238>`__ 在大型语言模型中揭示细粒度的值和观点

::

    Thu, 27 Jun 2024 15:01:53 GMT
    Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, and Isabelle Augenstein

Uncovering latent values and opinions in large language models (LLMs) can help identify biases and mitigate potential harm. Recently, this has been approached by presenting LLMs with survey questions and quantifying their stances towards morally and politically charged statements. However, the stances generated by LLMs can vary greatly depending on how they are prompted, and there are many ways to argue for or against a given position. In this work, we propose to address this by analysing a large and robust dataset of 156k LLM responses to the 62 propositions of the Political Compass Test (PCT) generated by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of their generated stances and fine-grained analysis of the plain text justifications for those stances. For fine-grained analysis, we propose to identify tropes in the responses: semantically similar phrases that are recurrent and consistent across different prompts, revealing patterns in the text that a given LLM is prone to produce. We find that demographic features added to prompts significantly affect outcomes on the PCT, reflecting bias, as well as disparities between the results of tests when eliciting closed-form vs.
open domain responses. Additionally, patterns in the plain text rationales via tropes show that similar justifications are repeatedly generated across models and prompts even with disparate stances.

------------

`[2406.19263] Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding <https://arxiv.org/abs/2406.19263>`__ 随时随地阅读:基于镜头树接地的布局感知GUI屏幕阅读

::

    Thu, 27 Jun 2024 15:34:16 GMT
    Yue Fan, Lei Ding, Ching-Chen Kuo, Shan Jiang, Yang Zhao, Xinze Guan, Jie Yang, Yi Zhang, Xin Eric Wang

Graphical User Interfaces (GUIs) are central to our interaction with digital devices. Recently, growing efforts have been made to build models for various GUI understanding tasks. However, these efforts largely overlook an important GUI-referring task: screen reading based on user-indicated points, which we name the Screen Point-and-Read (SPR) task. This task is predominantly handled by rigid accessible screen reading tools, in great need of new models driven by advancements in Multimodal Large Language Models (MLLMs). In this paper, we propose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism, to address the SPR task. Based on the input point coordinate and the corresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout Tree. Based on the tree, our ToL agent not only comprehends the content of the indicated area but also articulates the layout and spatial relationships between elements. Such layout information is crucial for accurately interpreting information on the screen, distinguishing our ToL agent from other screen reading tools. We also thoroughly evaluate the ToL agent against other baselines on a newly proposed SPR benchmark, which includes GUIs from mobile, web, and operating systems. Last but not least, we test the ToL agent on mobile GUI navigation tasks, demonstrating its utility in identifying incorrect actions along the path of agent execution trajectories. Code and data: screen-point-and-read.github.io

------------

`[2406.19271] AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning <https://arxiv.org/abs/2406.19271>`__ AutoPureData:用于LLM微调的Web数据自动过滤

::

    Thu, 27 Jun 2024 15:37:57 GMT
    Praneeth Vadlapati

Up-to-date and reliable Large Language Models (LLMs) are consistently sought after. Typically, LLMs are trained on a fixed dataset and then deployed.
However, the training data continually becomes outdated. Enable automatic training of AI using web data involves significant concerns regarding data quality and safety due to bias, spam, and other unsafe or unwanted text. Pure data is essential for producing reliable models. Training a model on impure data may result in undesirable outcomes. This research proposes a system that collects web data and automatically filters out unwanted text with the assistance of existing trusted AI models. In the experiment, a small sample of web data was collected and filtered, demonstrating the system's effectiveness in purifying the data.

------------

`[2406.19349] IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language <https://arxiv.org/abs/2406.19349>`__ IndoToxic2024:印度尼西亚语仇恨言论和毒性类型的人口统计数据集

::

    Thu, 27 Jun 2024 17:26:38 GMT
    Lucky Susanto, Musa Izzanardi Wijanarko, Prasetia Anugrah Pratama, Traci Hong, Ika Idris, Alham Fikri Aji, Derry Wijaya

Hate speech poses a significant threat to social harmony. Over the past two years, Indonesia has seen a ten-fold increase in the online hate speech ratio, underscoring the urgent need for effective detection mechanisms. However, progress is hindered by the limited availability of labeled data for Indonesian texts. The condition is even worse for marginalized minorities, such as Shia, LGBTQ, and other ethnic minorities because hate speech is underreported and less understood by detection tools. Furthermore, the lack of accommodation for subjectivity in current datasets compounds this issue. To address this, we introduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity classification dataset. Comprising 43,692 entries annotated by 19 diverse individuals, the dataset focuses on texts targeting vulnerable groups in Indonesia, specifically during the hottest political event in the country: the presidential election. We establish baselines for seven binary classification tasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet) fine-tuned for hate speech classification. Furthermore, we demonstrate how incorporating demographic information can enhance the zero-shot performance of the large language model, gpt-3.5-turbo. However, we also caution that an overemphasis on demographic information can negatively impact the fine-tuned model performance due to data fragmentation.

------------

`[2406.19354] Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs? <https://arxiv.org/abs/2406.19354>`__ 模型编辑的基本问题:llm中的理性信念修正应该如何工作?

::

    Thu, 27 Jun 2024 17:33:03 GMT
    Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal

The model editing problem concerns how language models should learn new facts about the world over time. While empirical research on model editing has drawn widespread attention, the conceptual foundations of model editing remain shaky -- perhaps unsurprisingly, since model editing is essentially belief revision, a storied problem in philosophy that has eluded succinct solutions for decades.
Model editing nonetheless demands a solution, since we need to be able to control the knowledge within language models. With this goal in mind, this paper critiques the standard formulation of the model editing problem and proposes a formal testbed for model editing research. We first describe 12 open problems with model editing, based on challenges with (1) defining the problem, (2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the first place. Many of these challenges are extremely difficult to address, e.g.
determining far-reaching consequences of edits, labeling probabilistic entailments between facts, and updating beliefs of agent simulators. Next, we introduce a semi-synthetic dataset for model editing based on Wikidata, where we can evaluate edits against labels given by an idealized Bayesian agent. This enables us to say exactly how belief revision in language models falls short of a desirable epistemic standard. We encourage further research exploring settings where such a gold standard can be compared against. Our code is publicly available at: https://github.com/peterbhase/LLM-belief-revision

------------

`[2406.19356] DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions <https://arxiv.org/abs/2406.19356>`__ DiVERT:数学多项选择题中带有变分误差的干扰项生成

::

    Thu, 27 Jun 2024 17:37:31 GMT
    Nigel Fernandez, Alexander Scarlatos, Simon Woodhead, Andrew Lan

High-quality distractors are crucial to both the assessment and pedagogical value of multiple-choice questions (MCQs), where manually crafting ones that anticipate knowledge deficiencies or misconceptions among real students is difficult. Meanwhile, automated distractor generation, even with the help of large language models (LLMs), remains challenging for subjects like math. It is crucial to not only identify plausible distractors but also understand the error behind them. In this paper, we introduce DiVERT (Distractor Generation with Variational Errors Represented as Text), a novel variational approach that learns an interpretable representation of errors behind distractors in math MCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions used by hundreds of thousands of students, we show that DiVERT, despite using a base open-source LLM with 7B parameters, outperforms state-of-the-art approaches using GPT-4o on downstream distractor generation. We also conduct a human evaluation with math educators and find that DiVERT leads to error labels that are of comparable quality to human-authored ones.

------------

`[2406.19358] The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models <https://arxiv.org/abs/2406.19358>`__ 跨语言情感分析的模型竞技场:大型语言模型时代的比较研究

::

    Thu, 27 Jun 2024 17:38:45 GMT
    Xiliang Zhu, Shayna Gardiner, Tere Rold\'an, David Rossouw

Sentiment analysis serves as a pivotal component in Natural Language Processing (NLP). Advancements in multilingual pre-trained models such as XLM-R and mT5 have contributed to the increasing interest in cross-lingual sentiment analysis. The recent emergence in Large Language Models (LLM) has significantly advanced general NLP tasks, however, the capability of such LLMs in cross-lingual sentiment analysis has not been fully studied. This work undertakes an empirical analysis to compare the cross-lingual transfer capability of public Small Multilingual Language Models (SMLM) like XLM-R, against English-centric LLMs such as Llama-3, in the context of sentiment analysis across English, Spanish, French and Chinese. Our findings reveal that among public models, SMLMs exhibit superior zero-shot cross-lingual performance relative to LLMs. However, in few-shot cross-lingual settings, public LLMs demonstrate an enhanced adaptive potential. In addition, we observe that proprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but are outpaced by public models in few-shot scenarios.

------------

`[2406.19371] Suri: Multi-constraint Instruction Following for Long-form Text Generation <https://arxiv.org/abs/2406.19371>`__ 

::

    Thu, 27 Jun 2024 17:50:35 GMT
    Chau Minh Pham, Simeng Sun, Mohit Iyyer

Existing research on instruction following largely focuses on tasks with simple instructions and short responses. In this work, we explore multi-constraint instruction following for generating long-form text. We create Suri, a dataset with 20K human-written long-form texts paired with LLM-generated backtranslated instructions that contain multiple complex constraints. Because of prohibitive challenges associated with collecting human preference judgments on long-form texts, preference-tuning algorithms such as DPO are infeasible in our setting; thus, we propose Instructional ORPO (I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving negative feedback from dispreferred responses, I-ORPO obtains negative feedback from synthetically corrupted instructions generated by an LLM. Using Suri, we perform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The resulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts (~5K tokens) than base models without significant quality deterioration. Our human evaluation shows that while both SFT and I-ORPO models satisfy most constraints, Suri-I-ORPO generations are generally preferred for their coherent and informative incorporation of the constraints. We release our code at https://github.com/chtmp223/suri.

------------

`[2406.18665] RouteLLM: Learning to Route LLMs with Preference Data <https://arxiv.org/abs/2406.18665>`__ RouteLLM:基于偏好数据的llm路由学习

::

    Wed, 26 Jun 2024 18:10:22 GMT
    Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez, M Waleed Kadous, Ion Stoica

Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs.

------------

`[2406.18678] Few-shot Personalization of LLMs with Mis-aligned Responses <https://arxiv.org/abs/2406.18678>`__ 具有错误响应的llm的少样本个性化

::

    Wed, 26 Jun 2024 18:29:12 GMT
    Jaehyung Kim, Yiming Yang

As the diversity of users increases, the capability of providing personalized responses by large language models (LLMs) has become increasingly important.
Existing approaches have only limited successes in LLM personalization, due to the absence of personalized learning or the reliance on shared personal data.
This paper proposes a new approach for a few-shot personalization of LLMs with their mis-aligned responses (Fermi). Our key idea is to learn a set of personalized prompts for each user by progressively improving the prompts using LLMs, based on user profile (e.g., demographic information) and a few examples of previous opinions. During an iterative process of prompt improvement, we incorporate the contexts of mis-aligned responses by LLMs, which are especially crucial for the effective personalization of LLMs. In addition, we develop an effective inference method to further leverage the context of the test query and the personalized prompts. Our experimental results demonstrate that Fermi significantly improves performance across various benchmarks, compared to the best-performing baselines.

------------

`[2406.18725] Jailbreaking LLMs with Arabic Transliteration and Arabizi <https://arxiv.org/abs/2406.18725>`__ 用阿拉伯音译和Arabizi破解llm

::

    Wed, 26 Jun 2024 19:48:48 GMT
    Mansour Al Ghanim, Saleh Almohaimeed, Mengxin Zheng, Yan Solihin, Qian Lou

This study identifies the potential vulnerabilities of Large Language Models (LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and its various forms. While most research has concentrated on English-based prompt manipulation, our investigation broadens the scope to investigate the Arabic language. We initially tested the AdvBench benchmark in Standardized Arabic, finding that even with prompt manipulation techniques like prefix injection, it was insufficient to provoke LLMs into generating unsafe content. However, when using Arabic transliteration and chatspeak (or arabizi), we found that unsafe content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3 Sonnet. Our findings suggest that using Arabic and its various forms could expose information that might remain hidden, potentially increasing the risk of jailbreak attacks. We hypothesize that this exposure could be due to the model's learned connection to specific words, highlighting the need for more comprehensive safety training across all language forms.

------------

`[2406.18815] MissionGNN: Hierarchical Multimodal GNN-based Weakly Supervised Video Anomaly Recognition with Mission-Specific Knowledge Graph Generation <https://arxiv.org/abs/2406.18815>`__ MissionGNN:基于分层多模态gnn的任务特定知识图谱生成的弱监督视频异常识别

::

    Thu, 27 Jun 2024 01:09:07 GMT
    Sanggeon Yun, Ryozo Masukawa, Minhyoung Na, Mohsen Imani

In the context of escalating safety concerns across various domains, the tasks of Video Anomaly Detection (VAD) and Video Anomaly Recognition (VAR) have emerged as critically important for applications in intelligent surveillance, evidence investigation, violence alerting, etc. These tasks, aimed at identifying and classifying deviations from normal behavior in video data, face significant challenges due to the rarity of anomalies which leads to extremely imbalanced data and the impracticality of extensive frame-level data annotation for supervised learning. This paper introduces a novel hierarchical graph neural network (GNN) based model MissionGNN that addresses these challenges by leveraging a state-of-the-art large language model and a comprehensive knowledge graph for efficient weakly supervised learning in VAR. Our approach circumvents the limitations of previous methods by avoiding heavy gradient computations on large multimodal models and enabling fully frame-level training without fixed video segmentation. Utilizing automated, mission-specific knowledge graph generation, our model provides a practical and efficient solution for real-time video analysis without the constraints of previous segmentation-based or multimodal approaches. Experimental validation on benchmark datasets demonstrates our model's performance in VAD and VAR, highlighting its potential to redefine the landscape of anomaly detection and recognition in video surveillance systems.

------------

`[2406.18851] LICO: Large Language Models for In-Context Molecular Optimization <https://arxiv.org/abs/2406.18851>`__ LICO:面向上下文分子优化的大型语言模型

::

    Thu, 27 Jun 2024 02:43:18 GMT
    Tung Nguyen and Aditya Grover

Optimizing black-box functions is a fundamental problem in science and engineering. To solve this problem, many approaches learn a surrogate function that estimates the underlying objective from limited historical evaluations.
Large Language Models (LLMs), with their strong pattern-matching capabilities via pretraining on vast amounts of data, stand out as a potential candidate for surrogate modeling. However, directly prompting a pretrained language model to produce predictions is not feasible in many scientific domains due to the scarcity of domain-specific data in the pretraining corpora and the challenges of articulating complex problems in natural language. In this work, we introduce LICO, a general-purpose model that extends arbitrary base LLMs for black-box optimization, with a particular application to the molecular domain.
To achieve this, we equip the language model with a separate embedding layer and prediction layer, and train the model to perform in-context predictions on a diverse set of functions defined over the domain. Once trained, LICO can generalize to unseen molecule properties simply via in-context prompting. LICO achieves state-of-the-art performance on PMO, a challenging molecular optimization benchmark comprising over 20 objective functions.

------------

`[2406.18926] Fine-tuned network relies on generic representation to solve unseen cognitive task <https://arxiv.org/abs/2406.18926>`__ 微调网络依赖通用表示来解决未见过的认知任务

::

    Thu, 27 Jun 2024 06:33:41 GMT
    Dongyan Lin

Fine-tuning pretrained language models has shown promising results on a wide range of tasks, but when encountering a novel task, do they rely more on generic pretrained representation, or develop brand new task-specific solutions? Here, we fine-tuned GPT-2 on a context-dependent decision-making task, novel to the model but adapted from neuroscience literature. We compared its performance and internal mechanisms to a version of GPT-2 trained from scratch on the same task. Our results show that fine-tuned models depend heavily on pretrained representations, particularly in later layers, while models trained from scratch develop different, more task-specific mechanisms.
These findings highlight the advantages and limitations of pretraining for task generalization and underscore the need for further investigation into the mechanisms underpinning task-specific fine-tuning in LLMs.

------------

`[2406.19054] A look under the hood of the Interactive Deep Learning Enterprise (No-IDLE) <https://arxiv.org/abs/2406.19054>`__ 交互式深度学习企业的底层架构(No-IDLE)

::

    Thu, 27 Jun 2024 10:01:56 GMT
    Daniel Sonntag, Michael Barz, Thiago Gouv\^ea

This DFKI technical report presents the anatomy of the No-IDLE prototype system (funded by the German Federal Ministry of Education and Research) that provides not only basic and fundamental research in interactive machine learning, but also reveals deeper insights into users' behaviours, needs, and goals. Machine learning and deep learning should become accessible to millions of end users. No-IDLE's goals and scienfific challenges centre around the desire to increase the reach of interactive deep learning solutions for non-experts in machine learning. One of the key innovations described in this technical report is a methodology for interactive machine learning combined with multimodal interaction which will become central when we start interacting with semi-intelligent machines in the upcoming area of neural networks and large language models.

------------

`[2406.19112] A Teacher Is Worth A Million Instructions <https://arxiv.org/abs/2406.19112>`__ 一位老师抵得上一百万个指导

::

    Thu, 27 Jun 2024 11:48:25 GMT
    Nikhil Kothari, Ravindra Nayak, Shreyas Shetty, Amey Patil and Nikesh Garera

Large Language Models(LLMs) have shown exceptional abilities, yet training these models can be quite challenging. There is a strong dependence on the quality of data and finding the best instruction tuning set. Further, the inherent limitations in training methods create substantial difficulties to train relatively smaller models with 7B and 13B parameters. In our research, we suggest an improved training method for these models by utilising knowledge from larger models, such as a mixture of experts (8x7B) architectures. The scale of these larger models allows them to capture a wide range of variations from data alone, making them effective teachers for smaller models. Moreover, we implement a novel post-training domain alignment phase that employs domain-specific expert models to boost domain-specific knowledge during training while preserving the model's ability to generalise. Fine-tuning Mistral 7B and 2x7B with our method surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to $7.9$ in MT-Bench and $93.04\%$ on AlpacaEval.

------------

`[2406.19185] Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion <https://arxiv.org/abs/2406.19185>`__ 对比策略梯度:以监督友好的方式在序列级分数上对齐llm

::

    Thu, 27 Jun 2024 14:03:49 GMT
    Yannis Flet-Berliac, Nathan Grinsztajn, Florian Strub, Eugene Choi, Chris Cremer, Arash Ahmadian, Yash Chandak, Mohammad Gheshlaghi Azar, Olivier Pietquin, Matthieu Geist

Reinforcement Learning (RL) has been used to finetune Large Language Models (LLMs) using a reward model trained from preference data, to better align with human judgment. The recently introduced direct alignment methods, which are often simpler, more stable, and computationally lighter, can more directly achieve this. However, these approaches cannot optimize arbitrary rewards, and the preference-based ones are not the only rewards of interest for LLMs (eg., unit tests for code generation or textual entailment for summarization, among others). RL-finetuning is usually done with a variation of policy gradient, which calls for on-policy or near-on-policy samples, requiring costly generations. We introduce Contrastive Policy Gradient, or CoPG, a simple and mathematically principled new RL algorithm that can estimate the optimal policy even from off-policy data. It can be seen as an off-policy policy gradient approach that does not rely on important sampling techniques and highlights the importance of using (the right) state baseline. We show this approach to generalize the direct alignment method IPO (identity preference optimization) and classic policy gradient. We experiment with the proposed CoPG on a toy bandit problem to illustrate its properties, as well as for finetuning LLMs on a summarization task, using a learned reward function considered as ground truth for the purpose of the experiments.

------------

`[2406.19317] Jump Starting Bandits with LLM-Generated Prior Knowledge <https://arxiv.org/abs/2406.19317>`__ 

::

    Thu, 27 Jun 2024 16:52:19 GMT
    Parand A. Alamdari, Yanshuai Cao, Kevin H. Wilson

We present substantial evidence demonstrating the benefits of integrating Large Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.
Contextual bandits have been widely used in recommendation systems to generate personalized suggestions based on user-specific contexts. We show that LLMs, pre-trained on extensive corpora rich in human knowledge and preferences, can simulate human behaviours well enough to jump-start contextual multi-armed bandits to reduce online learning regret. We propose an initialization algorithm for contextual bandits by prompting LLMs to produce a pre-training dataset of approximate human preferences for the bandit. This significantly reduces online learning regret and data-gathering costs for training such models. Our approach is validated empirically through two sets of experiments with different bandit setups: one which utilizes LLMs to serve as an oracle and a real-world experiment utilizing data from a conjoint survey experiment.

------------

`[2406.19384] The Remarkable Robustness of LLMs: Stages of Inference? <https://arxiv.org/abs/2406.19384>`__ llm的显著鲁棒性:推理阶段?

::

    Thu, 27 Jun 2024 17:57:03 GMT
    Vedang Lad, Wes Gurnee, Max Tegmark

We demonstrate and investigate the remarkable robustness of Large Language Models by deleting and swapping adjacent layers. We find that deleting and swapping interventions retain 72-95\% of the original model's prediction accuracy without fine-tuning, whereas models with more layers exhibit more robustness. Based on the results of the layer-wise intervention and further experiments, we hypothesize the existence of four universal stages of inference across eight different models: detokenization, feature engineering, prediction ensembling, and residual sharpening. The first stage integrates local information, lifting raw token representations into higher-level contextual representations. Next is the iterative refinement of task and entity-specific features. Then, the second half of the model begins with a phase transition, where hidden representations align more with the vocabulary space due to specialized model components. Finally, the last layer sharpens the following token distribution by eliminating obsolete features that add noise to the prediction.

------------

`[2406.18616] Towards Large Language Model Aided Program Refinement <https://arxiv.org/abs/2406.18616>`__ 面向大规模语言模型辅助程序精化的研究

::

    Wed, 26 Jun 2024 04:29:27 GMT
    Yufan Cai, Zhe Hou, Xiaokun Luan, David Miguel Sanan Baena, Yun Lin, Jun Sun and Jin Song Dong

Program refinement involves correctness-preserving transformations from formal high-level specification statements into executable programs.
Traditional verification tool support for program refinement is highly interactive and lacks automation. On the other hand, the emergence of large language models (LLMs) enables automatic code generations from informal natural language specifications. However, code generated by LLMs is often unreliable.
Moreover, the opaque procedure from specification to code provided by LLM is an uncontrolled black box. We propose LLM4PR, a tool that combines formal program refinement techniques with informal LLM-based methods to (1) transform the specification to preconditions and postconditions, (2) automatically build prompts based on refinement calculus, (3) interact with LLM to generate code, and finally, (4) verify that the generated code satisfies the conditions of refinement calculus, thus guaranteeing the correctness of the code. We have implemented our tool using GPT4, Coq, and Coqhammer, and evaluated it on the HumanEval and EvalPlus datasets.

------------

`[2406.18675] Human-AI Collaborative Taxonomy Construction: A Case Study in Profession-Specific Writing Assistants <https://arxiv.org/abs/2406.18675>`__ 人- ai协同分类体系构建:专业写作助手案例研究

::

    Wed, 26 Jun 2024 18:25:06 GMT
    Minhwa Lee, Zae Myung Kim, Vivek A. Khetan and Dongyeop Kang

Large Language Models (LLMs) have assisted humans in several writing tasks, including text revision and story generation. However, their effectiveness in supporting domain-specific writing, particularly in business contexts, is relatively less explored. Our formative study with industry professionals revealed the limitations in current LLMs' understanding of the nuances in such domain-specific writing. To address this gap, we propose an approach of human-AI collaborative taxonomy development to perform as a guideline for domain-specific writing assistants. This method integrates iterative feedback from domain experts and multiple interactions between these experts and LLMs to refine the taxonomy. Through larger-scale experiments, we aim to validate this methodology and thus improve LLM-powered writing assistance, tailoring it to meet the unique requirements of different stakeholder needs.

------------

`[2406.18841] Navigating LLM Ethics: Advancements, Challenges, and Future Directions <https://arxiv.org/abs/2406.18841>`__ 探索LLM伦理:进展、挑战和未来方向

::

    Tue, 14 May 2024 15:03:05 GMT
    Junfeng Jiao, Saleh Afroogh, Yiming Xu, Connor Phillips

This study addresses ethical issues surrounding Large Language Models (LLMs) within the field of artificial intelligence. It explores the common ethical challenges posed by both LLMs and other AI systems, such as privacy and fairness, as well as ethical challenges uniquely arising from LLMs. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to LLMs and distinct from those encountered in traditional AI systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that LLMs play in shaping information dissemination. It proposes mitigation strategies and future directions for LLM ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamic auditing systems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of LLMs, envisioning a future where ethical considerations govern AI advancements in society.

------------

`[2406.18842] The global landscape of academic guidelines for generative AI and Large Language Models <https://arxiv.org/abs/2406.18842>`__ 生成式人工智能和大型语言模型学术指南的全球格局

::

    Sun, 26 May 2024 15:28:24 GMT
    Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit Dhurandhar

The integration of Generative Artificial Intelligence (GAI) and Large Language Models (LLMs) in academia has spurred a global discourse on their potential pedagogical benefits and ethical considerations. Positive reactions highlight some potential, such as collaborative creativity, increased access to education, and empowerment of trainers and trainees. However, negative reactions raise concerns about ethical complexities, balancing innovation and academic integrity, unequal access, and misinformation risks. Through a systematic survey and text-mining-based analysis of global and national directives, insights from independent research, and eighty university-level guidelines, this study provides a nuanced understanding of the opportunities and challenges posed by GAI and LLMs in education. It emphasizes the importance of balanced approaches that harness the benefits of these technologies while addressing ethical considerations and ensuring equitable access and educational outcomes. The paper concludes with recommendations for fostering responsible innovation and ethical practices to guide the integration of GAI and LLMs in academia.

------------

`[2406.19280] HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale <https://arxiv.org/abs/2406.19280>`__ HuatuoGPT-Vision，面向向大规模多模态llm注入医学视觉知识

::

    Thu, 27 Jun 2024 15:50:41 GMT
    Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang

The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements. However, these models still face challenges in medical multimodal capabilities due to limitations in the quantity and quality of medical vision-text data, stemming from data privacy concerns and high annotation costs. While pioneering approaches utilize PubMed's large-scale, de-identified medical image-text pairs to address these limitations, they still fall short due to inherent data noise. To tackle this, we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in an 'unblinded' capacity to denoise and reformat the data, resulting in the creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our validation demonstrates that: (1) PubMedVision can significantly enhance the medical multimodal capabilities of current MLLMs, showing significant improvement in benchmarks including the MMMU Health & Medicine track; (2) manual checks by medical experts and empirical results validate the superior data quality of our dataset compared to other data construction methods. Using PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows superior performance in medical multimodal scenarios among open-source MLLMs.

------------

`[2406.18871] DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text Alignment <https://arxiv.org/abs/2406.18871>`__ DeSTA:基于描述性语音-文本对齐的语音语言模型增强

::

    Thu, 27 Jun 2024 03:52:35 GMT
    Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, He Huang, Boris Ginsburg, Yu-Chiang Frank Wang, Hung-yi Lee

Recent speech language models (SLMs) typically incorporate pre-trained speech models to extend the capabilities from large language models (LLMs). In this paper, we propose a Descriptive Speech-Text Alignment approach that leverages speech captioning to bridge the gap between speech and text modalities, enabling SLMs to interpret and generate comprehensive natural language descriptions, thereby facilitating the capability to understand both linguistic and non-linguistic features in speech. Enhanced with the proposed approach, our model demonstrates superior performance on the Dynamic-SUPERB benchmark, particularly in generalizing to unseen tasks. Moreover, we discover that the aligned model exhibits a zero-shot instruction-following capability without explicit speech instruction tuning. These findings highlight the potential to reshape instruction-following SLMs by incorporating rich, descriptive speech captions.

------------

`[2406.18972] Applying LLMs for Rescoring N-best ASR Hypotheses of Casual Conversations: Effects of Domain Adaptation and Context Carry-over <https://arxiv.org/abs/2406.18972>`__ 用llm重新评分休闲对话的N-best ASR假设:域适应和上下文延续的影响

::

    Thu, 27 Jun 2024 08:03:13 GMT
    Atsunori Ogawa, Naoyuki Kamo, Kohei Matsuura, Takanori Ashihara, Takafumi Moriya, Takatomo Kano, Naohiro Tawara, Marc Delcroix

Large language models (LLMs) have been successfully applied for rescoring automatic speech recognition (ASR) hypotheses. However, their ability to rescore ASR hypotheses of casual conversations has not been sufficiently explored. In this study, we reveal it by performing N-best ASR hypotheses rescoring using Llama2 on the CHiME-7 distant ASR (DASR) task. Llama2 is one of the most representative LLMs, and the CHiME-7 DASR task provides datasets of casual conversations between multiple participants. We investigate the effects of domain adaptation of the LLM and context carry-over when performing N-best rescoring. Experimental results show that, even without domain adaptation, Llama2 outperforms a standard-size domain-adapted Transformer-LM, especially when using a long context. Domain adaptation shortens the context length needed with Llama2 to achieve its best performance, i.e., it reduces the computational cost of Llama2.

------------

`[2406.18556] Renal digital pathology visual knowledge search platform based on language large model and book knowledge <https://arxiv.org/abs/2406.18556>`__ 基于语言大模型和书本知识的肾脏数字病理视觉知识搜索平台

::

    Mon, 27 May 2024 01:03:12 GMT
    Xiaomin Lv, Chong Lai, Liya Ding, Maode Lai, Qingrong Sun

Large models have become mainstream, yet their applications in digital pathology still require exploration. Meanwhile renal pathology images play an important role in the diagnosis of renal diseases. We conducted image segmentation and paired corresponding text descriptions based on 60 books for renal pathology, clustering analysis for all image and text description features based on large models, ultimately building a retrieval system based on the semantic features of large models. Based above analysis, we established a knowledge base of 10,317 renal pathology images and paired corresponding text descriptions, and then we evaluated the semantic feature capabilities of 4 large models, including GPT2, gemma, LLma and Qwen, and the image-based feature capabilities of dinov2 large model. Furthermore, we built a semantic retrieval system to retrieve pathological images based on text descriptions, and named RppD (aidp.zjsru.edu.cn).

------------

`[2406.18583] Lumina-Next: Making Lumina-T2X Stronger and Faster with Next-DiT <https://arxiv.org/abs/2406.18583>`__ Lumina-Next:通过Next-DiT让Lumina-T2X更强大、更快

::

    Wed, 5 Jun 2024 17:53:26 GMT
    Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, Xu Luo, Zehan Wang, Kaipeng Zhang, Xiangyang Zhu, Si Liu, Xiangyu Yue, Dingning Liu, Wanli Ouyang, Ziwei Liu, Yu Qiao, Hongsheng Li, Peng Gao

Lumina-T2X is a nascent family of Flow-based Large Diffusion Transformers that establishes a unified framework for transforming noise into various modalities, such as images and videos, conditioned on text instructions.
Despite its promising capabilities, Lumina-T2X still encounters challenges including training instability, slow inference, and extrapolation artifacts. In this paper, we present Lumina-Next, an improved version of Lumina-T2X, showcasing stronger generation performance with increased training and inference efficiency. We begin with a comprehensive analysis of the Flag-DiT architecture and identify several suboptimal components, which we address by introducing the Next-DiT architecture with 3D RoPE and sandwich normalizations.
To enable better resolution extrapolation, we thoroughly compare different context extrapolation methods applied to text-to-image generation with 3D RoPE, and propose Frequency- and Time-Aware Scaled RoPE tailored for diffusion transformers. Additionally, we introduced a sigmoid time discretization schedule to reduce sampling steps in solving the Flow ODE and the Context Drop method to merge redundant visual tokens for faster network evaluation, effectively boosting the overall sampling speed. Thanks to these improvements, Lumina-Next not only improves the quality and efficiency of basic text-to-image generation but also demonstrates superior resolution extrapolation capabilities and multilingual generation using decoder-based LLMs as the text encoder, all in a zero-shot manner. To further validate Lumina-Next as a versatile generative framework, we instantiate it on diverse tasks including visual recognition, multi-view, audio, music, and point cloud generation, showcasing strong performance across these domains. By releasing all codes and model weights, we aim to advance the development of next-generation generative AI capable of universal modeling.

------------

`[2406.19328] Subtractive Training for Music Stem Insertion using Latent Diffusion Models <https://arxiv.org/abs/2406.19328>`__ 

::

    Thu, 27 Jun 2024 16:59:14 GMT
    Ivan Villa-Renteria, Mason L. Wang, Zachary Shah, Zhe Li, Soohyun Kim, Neelesh Ramachandran, Mert Pilanci

We present Subtractive Training, a simple and novel method for synthesizing individual musical instrument stems given other instruments as context. This method pairs a dataset of complete music mixes with 1) a variant of the dataset lacking a specific stem, and 2) LLM-generated instructions describing how the missing stem should be reintroduced. We then fine-tune a pretrained text-to-audio diffusion model to generate the missing instrument stem, guided by both the existing stems and the text instruction. Our results demonstrate Subtractive Training's efficacy in creating authentic drum stems that seamlessly blend with the existing tracks. We also show that we can use the text instruction to control the generation of the inserted stem in terms of rhythm, dynamics, and genre, allowing us to modify the style of a single instrument in a full song while keeping the remaining instruments the same.
Lastly, we extend this technique to MIDI formats, successfully generating compatible bass, drum, and guitar parts for incomplete arrangements.

------------

`[2309.07683] Assessing the nature of large language models: A caution against anthropocentrism <https://arxiv.org/abs/2309.07683>`__ 评估大型语言模型的性质:对人类中心主义的警告

::

    replaced with revised version Thu, 27 Jun 2024 15:54:58 GMT
    Submission history From: Ann Speed [view email]
    [v1] Thu, 14 Sep 2023 12:58:30 UTC (748 KB)
    [v2] Mon, 5 Feb 2024 19:01:55 UTC (816 KB)
    [v3] Thu, 27 Jun 2024 15:54:58 UTC (808 KB)
    Ann Speed

Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed several LLMs, primarily GPT 3.5, using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans. Our results indicate that LLMs are unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. GPT3.5 did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability notwithstanding, LLMs display what in a human would be considered poor mental health, including low self-esteem, marked dissociation from reality, and in some cases narcissism and psychopathy, despite upbeat and helpful responses.

------------

`[2309.10253] GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts <https://arxiv.org/abs/2309.10253>`__ GPTFUZZER: Red将大型语言模型与自动生成的越狱提示结合起来

::

    replaced with revised version Thu, 27 Jun 2024 16:01:27 GMT
    Submission history From: Jiahao Yu [view email]
    [v1] Tue, 19 Sep 2023 02:19:48 UTC (9,081 KB)
    [v2] Wed, 4 Oct 2023 06:15:12 UTC (3,672 KB)
    [v3] Mon, 24 Jun 2024 15:34:17 UTC (3,302 KB)
    [v4] Thu, 27 Jun 2024 16:01:27 UTC (3,302 KB)
    Jiahao Yu, Xingwei Lin, Zheng Yu, Xinyu Xing

Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging.
In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack.
We evaluate GPTFuzz against various commercial and open-source LLMs, including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our results indicate that GPTFuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz achieves over 90% attack success rates against ChatGPT and Llama-2 models, even with suboptimal initial seed templates. We anticipate that GPTFuzz will be instrumental for researchers and practitioners in examining LLM robustness and will encourage further exploration into enhancing LLM safety.

------------

`[2405.07474] Integrating Intent Understanding and Optimal Behavior Planning for Behavior Tree Generation from Human Instructions <https://arxiv.org/abs/2405.07474>`__ 融合意图理解和最优行为规划的人工指令行为树生成

::

    replaced with revised version Thu, 27 Jun 2024 13:17:58 GMT
    Submission history From: Yishuai Cai [view email]
    [v1] Mon, 13 May 2024 05:23:48 UTC (2,490 KB)
    [v2] Thu, 27 Jun 2024 13:17:58 UTC (2,518 KB)
    Xinglin Chen, Yishuai Cai, Yunxin Mao, Minglong Li, Wenjing Yang, Weixia Xu, Ji Wang

Robots executing tasks following human instructions in domestic or industrial environments essentially require both adaptability and reliability. Behavior Tree (BT) emerges as an appropriate control architecture for these scenarios due to its modularity and reactivity. Existing BT generation methods, however, either do not involve interpreting natural language or cannot theoretically guarantee the BTs' success. This paper proposes a two-stage framework for BT generation, which first employs large language models (LLMs) to interpret goals from high-level instructions, then constructs an efficient goal-specific BT through the Optimal Behavior Tree Expansion Algorithm (OBTEA). We represent goals as well-formed formulas in first-order logic, effectively bridging intent understanding and optimal behavior planning. Experiments in the service robot validate the proficiency of LLMs in producing grammatically correct and accurately interpreted goals, demonstrate OBTEA's superiority over the baseline BT Expansion algorithm in various metrics, and finally confirm the practical deployability of our framework. The project website is this https URL.

------------

`[2310.08279] Enhancing Text-based Knowledge Graph Completion with Zero-Shot Large Language Models: A Focus on Semantic Enhancement <https://arxiv.org/abs/2310.08279>`__ 用零样本大型语言模型增强基于文本的知识图谱补全:语义增强的重点

::

    replaced with revised version Thu, 27 Jun 2024 04:55:28 GMT
    Submission history From: Rui Yang [view email]
    [v1] Thu, 12 Oct 2023 12:31:23 UTC (1,565 KB)
    [v2] Mon, 15 Jan 2024 04:22:59 UTC (358 KB)
    [v3] Thu, 27 Jun 2024 04:55:28 UTC (1,545 KB)
    Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, Yi Zhou

The design and development of text-based knowledge graph completion (KGC) methods leveraging textual entity descriptions are at the forefront of research. These methods involve advanced optimization techniques such as soft prompts and contrastive learning to enhance KGC models. The effectiveness of text-based methods largely hinges on the quality and richness of the training data. Large language models (LLMs) can utilize straightforward prompts to alter text data, thereby enabling data augmentation for KGC. Nevertheless, LLMs typically demand substantial computational resources. To address these issues, we introduce a framework termed constrained prompts for KGC (CP-KGC). This CP-KGC framework designs prompts that adapt to different datasets to enhance semantic richness. Additionally, CP-KGC employs a context constraint strategy to effectively identify polysemous entities within KGC datasets. Through extensive experimentation, we have verified the effectiveness of this framework. Even after quantization, the LLM (Qwen-7B-Chat-int4) still enhances the performance of text-based KGC methods \footnote{Code and datasets are available at \href{this https URL}{this https URL}}. This study extends the performance limits of existing models and promotes further integration of KGC with LLMs.

------------

`[2311.08704] Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains <https://arxiv.org/abs/2311.08704>`__ 大型语言模型能否遵循概念注释准则?科学和金融领域的案例研究

::

    replaced with revised version Thu, 27 Jun 2024 03:48:35 GMT
    Submission history From: Marcio Fonseca [view email]
    [v1] Wed, 15 Nov 2023 05:11:26 UTC (240 KB)
    [v2] Thu, 27 Jun 2024 03:48:35 UTC (246 KB)
    Marcio Fonseca and Shay B. Cohen

Although large language models (LLMs) exhibit remarkable capacity to leverage in-context demonstrations, it is still unclear to what extent they can learn new concepts or facts from ground-truth labels. To address this question, we examine the capacity of instruction-tuned LLMs to follow in-context concept guidelines for sentence labeling tasks. We design guidelines that present different types of factual and counterfactual concept definitions, which are used as prompts for zero-shot sentence classification tasks. Our results show that although concept definitions consistently help in task performance, only the larger models (with 70B parameters or more) have limited ability to work under counterfactual contexts. Importantly, only proprietary models such as GPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is due to more sophisticated alignment methods. Finally, we find that Falcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which indicates that careful fine-tuning is more effective than increasing model scale. Altogether, our simple evaluation method reveals significant gaps in concept understanding between the most capable open-source language models and the leading proprietary APIs.

------------

`[2401.09395] Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions <https://arxiv.org/abs/2401.09395>`__ 通过本体引导的干预评估llm的数学和编码能力

::

    replaced with revised version Thu, 27 Jun 2024 05:23:50 GMT
    Submission history From: Soujanya Poria [view email]
    [v1] Wed, 17 Jan 2024 18:13:07 UTC (1,504 KB)
    [v2] Mon, 19 Feb 2024 01:50:42 UTC (1,693 KB)
    [v3] Wed, 26 Jun 2024 11:43:39 UTC (9,581 KB)
    [v4] Thu, 27 Jun 2024 05:23:50 UTC (9,581 KB)
    Pengfei Hong, Navonil Majumder, Deepanway Ghosal, Somak Aditya, Rada Mihalcea, Soujanya Poria

Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust problem solving skills and structured reasoning abilities in many areas, as defined by our ontology. We open source the datasets and source codes at: this https URL.

------------

`[2401.10415] Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals? <https://arxiv.org/abs/2401.10415>`__ 大型语言模型摘要器能否适应多样化的科学传播目标?

::

    replaced with revised version Thu, 27 Jun 2024 04:00:19 GMT
    Submission history From: Marcio Fonseca [view email]
    [v1] Thu, 18 Jan 2024 23:00:54 UTC (99 KB)
    [v2] Thu, 27 Jun 2024 04:00:19 UTC (106 KB)
    Marcio Fonseca, Shay B. Cohen

In this work, we investigate the controllability of large language models (LLMs) on scientific summarization tasks. We identify key stylistic and content coverage factors that characterize different types of summaries such as paper reviews, abstracts, and lay summaries. By controlling stylistic features, we find that non-fine-tuned LLMs outperform humans in the MuP review generation task, both in terms of similarity to reference summaries and human preferences. Also, we show that we can improve the controllability of LLMs with keyword-based classifier-free guidance (CFG) while achieving lexical overlap comparable to strong fine-tuned baselines on arXiv and PubMed. However, our results also indicate that LLMs cannot consistently generate long summaries with more than 8 sentences. Furthermore, these models exhibit limited capacity to produce highly abstractive lay summaries. Although LLMs demonstrate strong generic summarization competency, sophisticated content control without costly fine-tuning remains an open problem for domain-specific applications.

------------

`[2402.07610] Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping <https://arxiv.org/abs/2402.07610>`__ Step-On-Feet Tuning:通过Bootstrapping扩展llm的自对齐

::

    replaced with revised version Thu, 27 Jun 2024 16:38:35 GMT
    Submission history From: Haoyu Wang [view email]
    [v1] Mon, 12 Feb 2024 12:30:42 UTC (3,387 KB)
    [v2] Thu, 22 Feb 2024 04:53:46 UTC (3,387 KB)
    [v3] Thu, 27 Jun 2024 16:38:35 UTC (4,647 KB)
    Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, Xueqian Wang, Peilin Zhao

Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced few-shot ability to boost zero or one-shot performance. Based on easy-to-hard training recipe, we propose SOFT+ which further boost self-alignment's performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across various classification and generation tasks, highlighting the potential of bootstrapping self-alignment on continually enhancing model alignment performance.

------------

`[2404.11999] Token-level Direct Preference Optimization <https://arxiv.org/abs/2404.11999>`__ 令牌级直接偏好优化

::

    replaced with revised version Thu, 27 Jun 2024 15:27:41 GMT
    Submission history From: Yongcheng Zeng [view email]
    [v1] Thu, 18 Apr 2024 08:49:38 UTC (305 KB)
    [v2] Tue, 28 May 2024 14:37:40 UTC (354 KB)
    [v3] Sun, 2 Jun 2024 16:21:59 UTC (355 KB)
    [v4] Thu, 27 Jun 2024 15:27:41 UTC (354 KB)
    Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, Jun Wang

Fine-tuning pre-trained Large Language Models (LLMs) is essential to align them with human values and intentions. This process often utilizes methods like pairwise comparisons and KL divergence against a reference LLM, focusing on the evaluation of full answers generated by the models. However, the generation of these responses occurs in a token level, following a sequential, auto-regressive fashion. In this paper, we introduce Token-level Direct Preference Optimization (TDPO), a novel approach to align LLMs with human preferences by optimizing policy at the token level. Unlike previous methods, which face challenges in divergence efficiency, TDPO incorporates forward KL divergence constraints for each token, improving alignment and diversity. Utilizing the Bradley-Terry model for a token-based reward system, TDPO enhances the regulation of KL divergence, while preserving simplicity without the need for explicit reward modeling. Experimental results across various text tasks demonstrate TDPO's superior performance in balancing alignment with generation diversity. Notably, fine-tuning with TDPO strikes a better balance than DPO in the controlled sentiment generation and single-turn dialogue datasets, and significantly improves the quality of generated responses compared to both DPO and PPO-based RLHF methods. Our code is open-sourced at this https URL.

------------

`[2405.00253] CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification <https://arxiv.org/abs/2405.00253>`__ CodeHalu:基于执行验证驱动的llm代码幻觉

::

    replaced with revised version Wed, 26 Jun 2024 20:28:36 GMT
    Submission history From: Weixiang Yan [view email]
    [v1] Tue, 30 Apr 2024 23:56:38 UTC (2,368 KB)
    [v2] Wed, 26 Jun 2024 20:28:36 UTC (7,152 KB)
    Yuchen Tian, Weixiang Yan, Qian Yang, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma

Large Language Models (LLMs) have made significant progress in code generation, providing developers with unprecedented automated programming support. However, LLMs often generate code that is syntactically correct and even semantically plausible but may not execute as expected or meet specified requirements. This phenomenon of hallucinations in the code domain has not been systematically explored. To enhance the community's understanding and research on this issue, we introduce the concept of code hallucinations and propose a classification method for code hallucination based on execution verification. We classify code hallucinations into four main types: mapping, naming, resource, and logic hallucinations, with each category further divided into different subcategories to understand and address the unique challenges faced by LLMs in code generation with finer granularity. Additionally, we develop a dynamic detection algorithm named CodeHalu to quantify code hallucinations and establish the CodeHaluEval benchmark, which includes 8,883 samples from 699 tasks to systematically and quantitatively evaluate code hallucinations. By evaluating 17 popular LLMs on this benchmark, we reveal significant differences in their accuracy and reliability in code generation and provide detailed insights for further improving the code generation capabilities of LLMs. The CodeHalu benchmark and code are publicly available at this https URL.

------------

`[2405.10443] Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation <https://arxiv.org/abs/2405.10443>`__ 同时屏蔽，而不是提示优化:面向同传翻译的微调llm范式转变

::

    replaced with revised version Wed, 26 Jun 2024 20:22:31 GMT
    Submission history From: Lizhong Chen [view email]
    [v1] Thu, 16 May 2024 21:07:42 UTC (573 KB)
    [v2] Wed, 26 Jun 2024 20:22:31 UTC (741 KB)
    Matthew Raffel, Victor Agostinelli, Lizhong Chen

Large language models (LLMs) have achieved state-of-the-art performance in various language processing tasks, motivating their adoption in simultaneous translation. Current fine-tuning methods to adapt LLMs for simultaneous translation focus on prompting optimization strategies using either data augmentation or prompt structure modifications. However, these methods suffer from several issues, such as unnecessarily expanded training sets, computational inefficiency from dumping the key and value cache, increased prompt sizes, or restriction to a single decision policy. To eliminate these issues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs for simultaneous translation. It utilizes a novel attention mask approach that models simultaneous translation during fine-tuning by masking attention for a desired decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT 2017 dataset, we have observed a significant translation quality improvement compared to state-of-the-art prompting optimization strategies on five language pairs while reducing the computational cost.

------------

`[2406.00041] QUB-Cirdan at "Discharge Me!": Zero shot discharge letter generation by open-source LLM <https://arxiv.org/abs/2406.00041>`__ QUB-Cirdan在“让我出院!”: Zero shot discharge letter generation by开源LLM

::

    replaced with revised version Thu, 27 Jun 2024 14:31:22 GMT
    Submission history From: Rui Guo [view email]
    [v1] Mon, 27 May 2024 17:55:36 UTC (1,048 KB)
    [v2] Thu, 27 Jun 2024 14:31:22 UTC (1,048 KB)
    Rui Guo, Greg Farnan, Niall McLaughlin, Barry Devereux

The BioNLP ACL'24 Shared Task on Streamlining Discharge Documentation aims to reduce the administrative burden on clinicians by automating the creation of critical sections of patient discharge letters. This paper presents our approach using the Llama3 8B quantized model to generate the "Brief Hospital Course" and "Discharge Instructions" sections. We employ a zero-shot method combined with Retrieval-Augmented Generation (RAG) to produce concise, contextually accurate summaries. Our contributions include the development of a curated template-based approach to ensure reliability and consistency, as well as the integration of RAG for word count prediction. We also describe several unsuccessful experiments to provide insights into our pathway for the competition. Our results demonstrate the effectiveness and efficiency of our approach, achieving high scores across multiple evaluation metrics.

------------

`[2406.11385] MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic <https://arxiv.org/abs/2406.11385>`__ MetaGPT:基于模型独占任务算法的大型语言模型合并

::

    replaced with revised version Thu, 27 Jun 2024 16:01:28 GMT
    Submission history From: Bingning Wang Dr. [view email]
    [v1] Mon, 17 Jun 2024 10:12:45 UTC (521 KB)
    [v2] Thu, 27 Jun 2024 16:01:28 UTC (522 KB)
    Yuyan Zhou, Liang Song, Bingning Wang and Weipeng Chen

The advent of large language models (LLMs) like GPT-4 has catalyzed the exploration of multi-task learning (MTL), in which a single model demonstrates proficiency across diverse tasks. Task arithmetic has emerged as a cost-effective approach for MTL. It enables performance enhancement across multiple tasks by adding their corresponding task vectors to a pre-trained model. However, the current lack of a method that can simultaneously achieve optimal performance, computational efficiency, and data privacy limits their application to LLMs. In this paper, we propose \textbf{M}odel \textbf{E}xclusive \textbf{T}ask \textbf{A}rithmetic for merging \textbf{GPT}-scale models, which formalizes the objective of model merging into a multi-task learning framework, aiming to minimize the average loss difference between the merged model and each individual task model. Since data privacy limits the use of multi-task training data, we leverage LLMs' local linearity and task vectors' orthogonality to separate the data term and scaling coefficients term and derive a model-exclusive task arithmetic method. Our proposed MetaGPT is data-agnostic and bypasses the heavy search process, making it cost-effective and easy to implement for LLMs.Extensive experiments demonstrate that MetaGPT leads to improvements in task arithmetic and achieves state-of-the-art performance on multiple tasks.

------------

`[2406.12036] MedCalc-Bench: Evaluating Large Language Models for Medical Calculations <https://arxiv.org/abs/2406.12036>`__ MedCalc-Bench:医学计算大型语言模型评估

::

    replaced with revised version Thu, 27 Jun 2024 15:25:25 GMT
    Submission history From: Nikhil Khandekar [view email]
    [v1] Mon, 17 Jun 2024 19:07:21 UTC (1,319 KB)
    [v2] Tue, 25 Jun 2024 13:45:49 UTC (1,320 KB)
    [v3] Thu, 27 Jun 2024 15:25:25 UTC (1,320 KB)
    [v4] Sun, 30 Jun 2024 15:12:10 UTC (1,320 KB)
    Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina S Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad W Safranek, Abid A Anwar, Andrew Zhang, Aidan Gilson, Maxwell B Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, and Zhiyong Lu

As opposed to evaluating computation and logic-based reasoning, current benchmarks for evaluating large language models (LLMs) in medicine are primarily focused on question-answering involving domain knowledge and descriptive reasoning. While such qualitative capabilities are vital to medical diagnosis, in real-world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. To this end, we propose MedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical calculation capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks. Each instance in MedCalc-Bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. While our evaluation results show the potential of LLMs in this area, none of them are effective enough for clinical settings. Common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. We hope our study highlights the quantitative knowledge and reasoning gaps in LLMs within medical settings, encouraging future improvements of LLMs for various clinical calculation tasks.

------------

`[2406.12416] Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models <https://arxiv.org/abs/2406.12416>`__ 超越欠对齐:原子偏好增强的大型语言模型事实性调优

::

    replaced with revised version Thu, 27 Jun 2024 12:07:55 GMT
    Submission history From: Hongbang Yuan [view email]
    [v1] Tue, 18 Jun 2024 09:07:30 UTC (688 KB)
    [v2] Thu, 27 Jun 2024 12:07:55 UTC (690 KB)
    Hongbang Yuan, Yubo Chen, Pengfei Cao, Zhuoran Jin, Kang Liu, Jun Zhao

Large language models (LLMs) have achieved remarkable success but still tend to generate factually erroneous responses, a phenomenon known as hallucination. A recent trend is to use preference learning to fine-tune models to align with factuality. However, existing work primarily evaluates fine-tuned models on in-domain (ID) datasets and the factuality on out-of-domain (OOD) datasets remains underexplored. In this paper, we conduct a comprehensive evaluation of the factuality of different models tuned by various preference learning algorithms and demonstrate that their performance on OOD datasets either increases minimally or decreases. Subsequently, we reveal that the main cause of model's failure to uphold factuality under a distribution shift is \textbf{under-alignment}, rather than \textbf{over-alignment}, by analyzing the token distribution shift of the models before and after tuning. Finally, we propose \textbf{APEFT} (\textbf{A}tomic \textbf{P}reference \textbf{E}nhanced \textbf{F}actuality \textbf{T}uning), a framework that enhances model's awareness of factuality at the granularity of individual facts. Extensive experiments demonstrate that APEFT improves model performance by an average of $\boldsymbol{3.45\%}$ on both ID and OOD datasets, which is highly effective.

------------

`[2406.12644] Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models <https://arxiv.org/abs/2406.12644>`__ 层次提示分类法:大型语言模型通用评估框架

::

    replaced with revised version Thu, 27 Jun 2024 14:32:07 GMT
    Submission history From: Devichand Budagam [view email]
    [v1] Tue, 18 Jun 2024 14:12:27 UTC (1,006 KB)
    [v2] Thu, 27 Jun 2024 14:32:07 UTC (1,006 KB)
    Devichand Budagam, Sankalp KJ, Ashutosh Kumar, Vinija Jain, Aman Chadha

Assessing the effectiveness of large language models (LLMs) in addressing diverse tasks is essential for comprehending their strengths and weaknesses. Conventional evaluation techniques typically apply a single prompting strategy uniformly across datasets, not considering the varying degrees of task complexity. We introduce the Hierarchical Prompting Taxonomy (HPT), a taxonomy that employs a Hierarchical Prompt Framework (HPF) composed of five unique prompting strategies, arranged from the simplest to the most complex, to assess LLMs more precisely and to offer a clearer perspective. This taxonomy assigns a score, called the Hierarchical Prompting Score (HP-Score), to datasets as well as LLMs based on the rules of the taxonomy, providing a nuanced understanding of their ability to solve diverse tasks and offering a universal measure of task complexity. Additionally, we introduce the Adaptive Hierarchical Prompt framework, which automates the selection of appropriate prompting strategies for each task. This study compares manual and adaptive hierarchical prompt frameworks using four instruction-tuned LLMs, namely Llama 3 8B, Phi 3 3.8B, Mistral 7B, and Gemma 7B, across four datasets: BoolQ, CommonSenseQA (CSQA), IWSLT-2017 en-fr (IWSLT), and SamSum. Experiments demonstrate the effectiveness of HPT, providing a reliable way to compare different tasks and LLM capabilities. This paper leads to the development of a universal evaluation metric that can be used to evaluate both the complexity of the datasets and the capabilities of LLMs. The implementation of both manual HPF and adaptive HPF is publicly available.

------------

`[2406.13444] VDebugger: Harnessing Execution Feedback for Debugging Visual Programs <https://arxiv.org/abs/2406.13444>`__ VDebugger:利用执行反馈来调试可视化程序

::

    replaced with revised version Thu, 27 Jun 2024 17:09:24 GMT
    Submission history From: Xueqing Wu [view email]
    [v1] Wed, 19 Jun 2024 11:09:16 UTC (3,453 KB)
    [v2] Thu, 27 Jun 2024 17:09:24 UTC (3,453 KB)
    Xueqing Wu, Zongyu Lin, Songyan Zhao, Te-Lin Wu, Pan Lu, Nanyun Peng, Kai-Wei Chang

Visual programs are executable code generated by large language models to address visual reasoning problems. They decompose complex questions into multiple reasoning steps and invoke specialized models for each step to solve the problems. However, these programs are prone to logic errors, with our preliminary evaluation showing that 58% of the total errors are caused by program logic errors. Debugging complex visual programs remains a major bottleneck for visual reasoning. To address this, we introduce VDebugger, a novel critic-refiner framework trained to localize and debug visual programs by tracking execution step by step. VDebugger identifies and corrects program errors leveraging detailed execution feedback, improving interpretability and accuracy. The training data is generated through an automated pipeline that injects errors into correct visual programs using a novel mask-best decoding technique. Evaluations on six datasets demonstrate VDebugger's effectiveness, showing performance improvements of up to 3.2% in downstream task accuracy. Further studies show VDebugger's ability to generalize to unseen tasks, bringing a notable improvement of 2.3% on the unseen COVR task. Code, data and models are made publicly available at this https URL

------------

`[2406.18192] Methodology of Adapting Large English Language Models for Specific Cultural Contexts <https://arxiv.org/abs/2406.18192>`__ 适应特定文化语境的大型英语语言模型的方法论

::

    replaced with revised version Thu, 27 Jun 2024 02:17:19 GMT
    Submission history From: Wenjing Zhang [view email]
    [v1] Wed, 26 Jun 2024 09:16:08 UTC (521 KB)
    [v2] Thu, 27 Jun 2024 02:17:19 UTC (521 KB)
    Wenjing Zhang and Siqi Xiao and Xuejiao Lei and Ning Wang and Huazheng Zhang and Meijuan An and Bikun Yang and Zhaoxiang Liu and Kai Wang and Shiguo Lian

The rapid growth of large language models(LLMs) has emerged as a prominent trend in the field of artificial intelligence. However, current state-of-the-art LLMs are predominantly based on English. They encounter limitations when directly applied to tasks in specific cultural domains, due to deficiencies in domain-specific knowledge and misunderstandings caused by differences in cultural values. To address this challenge, our paper proposes a rapid adaptation method for large models in specific cultural contexts, which leverages instruction-tuning based on specific cultural knowledge and safety values data. Taking Chinese as the specific cultural context and utilizing the LLaMA3-8B as the experimental English LLM, the evaluation results demonstrate that the adapted LLM significantly enhances its capabilities in domain-specific knowledge and adaptability to safety values, while maintaining its original expertise advantages.

------------

`[2406.18294] Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs <https://arxiv.org/abs/2406.18294>`__ 分层上下文剪枝:用存储库级预训练代码llm优化真实世界的代码补全

::

    replaced with revised version Thu, 27 Jun 2024 04:40:52 GMT
    Submission history From: Lei Zhang [view email]
    [v1] Wed, 26 Jun 2024 12:26:16 UTC (509 KB)
    [v2] Thu, 27 Jun 2024 04:40:52 UTC (509 KB)
    Lei Zhang, Yunshui Li, Jiaming Li, Xiaobo Xia, Jiaxi Yang, Run Luo, Minzheng Wang, Longze Chen, Junhao Liu, Min Yang

Some recently developed code large language models (Code LLMs) have been pre-trained on repository-level code data (Repo-Code LLMs), enabling these models to recognize repository structures and utilize cross-file information for code completion. However, in real-world development scenarios, simply concatenating the entire code repository often exceeds the context window limits of these Repo-Code LLMs, leading to significant performance degradation. In this study, we conducted extensive preliminary experiments and analyses on six Repo-Code LLMs. The results indicate that maintaining the topological dependencies of files and increasing the code file content in the completion prompts can improve completion accuracy; pruning the specific implementations of functions in all dependent files does not significantly reduce the accuracy of completions. Based on these findings, we proposed a strategy named Hierarchical Context Pruning (HCP) to construct completion prompts with high informational code content. The HCP models the code repository at the function level, maintaining the topological dependencies between code files while removing a large amount of irrelevant code content, significantly reduces the input length for repository-level code completion. We applied the HCP strategy in experiments with six Repo-Code LLMs, and the results demonstrate that our proposed method can significantly enhance completion accuracy while substantially reducing the length of input. Our code and data are available at this https URL.

------------

`[2401.09181] Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer <https://arxiv.org/abs/2401.09181>`__ 除了反遗忘:基于正向迁移的多模态连续指令调整

::

    replaced with revised version Thu, 27 Jun 2024 02:45:41 GMT
    Submission history From: Junhao Zheng [view email]
    [v1] Wed, 17 Jan 2024 12:44:17 UTC (4,307 KB)
    [v2] Thu, 29 Feb 2024 03:06:42 UTC (4,298 KB)
    [v3] Thu, 27 Jun 2024 02:45:41 UTC (9,595 KB)
    Junhao Zheng, Qianli Ma, Zhen Liu, Binquan Wu, Huawen Feng

Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large Language Models (MLLMs) to meet continuously emerging requirements without expensive retraining. MCIT faces two major obstacles: catastrophic forgetting (where old knowledge is forgotten) and negative forward transfer (where the performance of future tasks is degraded). Although existing methods have greatly alleviated catastrophic forgetting, they still suffer from negative forward transfer. We discover a large discrepancy in different input embeddings by performing singular value decomposition (SVD) on input embeddings. This discrepancy results in the model learning irrelevant information for old and pre-trained tasks, leading to catastrophic forgetting and negative forward transfer. To address these issues, we propose Prompt Tuning with Positive Forward Transfer (Fwd-Prompt), a prompt-based method that projects the prompt gradient to the residual space to minimize interference between tasks and to the pre-trained subspace for reusing pre-trained knowledge. Our experiments demonstrate that Fwd-Prompt achieves state-of-the-art performance while updating fewer parameters and requiring no old samples. Our research illuminates the potential of continuously adapting MLLMs to new tasks under the instruction tuning paradigm and encourages future studies to explore MCIT.

------------

`[2402.05162] Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications <https://arxiv.org/abs/2402.05162>`__ 基于剪枝和低秩修正的安全对齐脆弱性评估

::

    replaced with revised version Thu, 27 Jun 2024 17:23:58 GMT
    Submission history From: Boyi Wei [view email]
    [v1] Wed, 7 Feb 2024 18:34:38 UTC (649 KB)
    [v2] Thu, 27 Jun 2024 17:23:58 UTC (652 KB)
    [v3] Mon, 1 Jul 2024 07:11:17 UTC (577 KB)
    Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson

Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.

------------

`[2402.14905] MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases <https://arxiv.org/abs/2402.14905>`__ MobileLLM:面向设备用例的亚十亿参数语言模型优化

::

    replaced with revised version Thu, 27 Jun 2024 03:53:46 GMT
    Submission history From: Zechun Liu [view email]
    [v1] Thu, 22 Feb 2024 18:58:55 UTC (888 KB)
    [v2] Thu, 27 Jun 2024 03:53:46 UTC (937 KB)
    Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra

This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight-sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover, MobileLLM model family shows significant improvements compared to previous sub-billion models on chat benchmarks, and demonstrates close correctness to LLaMA-v2 7B in API calling tasks, highlighting the capability of small models for common on-device use cases.

------------

`[2403.08819] Thermometer: Towards Universal Calibration for Large Language Models <https://arxiv.org/abs/2403.08819>`__ 温度计:走向大型语言模型的通用校准

::

    replaced with revised version Thu, 27 Jun 2024 16:30:32 GMT
    Submission history From: Soumya Ghosh [view email]
    [v1] Tue, 20 Feb 2024 04:13:48 UTC (2,161 KB)
    [v2] Thu, 27 Jun 2024 16:30:32 UTC (2,599 KB)
    Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory Wornell, Soumya Ghosh

We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.

------------

`[2405.18641] Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning <https://arxiv.org/abs/2405.18641>`__ 针对有害微调的大型语言模型的惰性安全对齐

::

    replaced with revised version Wed, 26 Jun 2024 18:54:59 GMT
    Submission history From: Tiansheng Huang [view email]
    [v1] Tue, 28 May 2024 22:53:43 UTC (1,592 KB)
    [v2] Thu, 30 May 2024 20:03:37 UTC (1,592 KB)
    [v3] Mon, 24 Jun 2024 18:59:50 UTC (1,592 KB)
    [v4] Wed, 26 Jun 2024 18:54:59 UTC (1,592 KB)
    Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu

Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data. First time in the literature, we show that the jail-broken effect can be mitigated by separating states in the finetuning stage to optimize the alignment and user datasets. Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance. By statistical analysis, we show that the \textit{excess drift} towards consensus could be a probable reason for the instability. To remedy this issue, we propose \textbf{L}azy(\textbf{i}) \textbf{s}afety \textbf{a}lignment (\textbf{Lisa}), which introduces a proximal term to constraint the drift of each state. Theoretically, the benefit of the proximal term is supported by the convergence analysis, wherein we show that a sufficient large proximal factor is necessary to guarantee Lisa's convergence. Empirically, our results on four downstream finetuning tasks show that Lisa with a proximal term can significantly increase alignment performance while maintaining the LLM's accuracy on the user tasks. Code is available at \url{this https URL}.

------------

`[2401.15847] Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA <https://arxiv.org/abs/2401.15847>`__ 松饼还是吉娃娃?用多面板VQA挑战多模态大型语言模型

::

    replaced with revised version Thu, 27 Jun 2024 15:38:17 GMT
    Submission history From: Yue Fan [view email]
    [v1] Mon, 29 Jan 2024 02:43:40 UTC (9,357 KB)
    [v2] Mon, 19 Feb 2024 05:14:56 UTC (9,358 KB)
    [v3] Thu, 27 Jun 2024 15:38:17 UTC (9,619 KB)
    Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, Xin Eric Wang

Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Multimodal Large Language Models (MLLMs) tested, even though humans can attain approximately 99% accuracy on these questions. Distinctively, the MultipanelVQA benchmark features synthetically generated multipanel images specifically crafted to isolate and assess the impact of various factors, such as the layout, on MLLMs' multipanel image comprehension abilities. As a result, in addition to benchmarking the capabilities of MLLMs in understanding multipanel images, we analyze various factors of the multipanel image that affect MLLMs' performance with synthetic data and offer insights for enhancement. Code and data are released at this https URL.

------------

`[2406.18069] Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals <https://arxiv.org/abs/2406.18069>`__ 基于可穿戴生物信号的无袖血压测量大型语言模型

::

    replaced with revised version Thu, 27 Jun 2024 03:58:25 GMT
    Submission history From: Zengding Liu [view email]
    [v1] Wed, 26 Jun 2024 04:54:45 UTC (3,386 KB)
    [v2] Thu, 27 Jun 2024 03:58:25 UTC (3,386 KB)
    [v3] Fri, 5 Jul 2024 01:25:13 UTC (3,386 KB)
    Zengding Liu, Chen Chen, Jiannong Cao, Minglei Pan, Jikui Liu, Nan Li, Fen Miao, and Ye Li

Large language models (LLMs) have captured significant interest from both academia and industry due to their impressive performance across various textual tasks. However, the potential of LLMs to analyze physiological time-series data remains an emerging research field. Particularly, there is a notable gap in the utilization of LLMs for analyzing wearable biosignals to achieve cuffless blood pressure (BP) measurement, which is critical for the management of cardiovascular diseases. This paper presents the first work to explore the capacity of LLMs to perform cuffless BP estimation based on wearable biosignals. We extracted physiological features from electrocardiogram (ECG) and photoplethysmogram (PPG) signals and designed context-enhanced prompts by combining these features with BP domain knowledge and user information. Subsequently, we adapted LLMs to BP estimation tasks through fine-tuning. To evaluate the proposed approach, we conducted assessments of ten advanced LLMs using a comprehensive public dataset of wearable biosignals from 1,272 participants. The experimental results demonstrate that the optimally fine-tuned LLM significantly surpasses conventional task-specific baselines, achieving an estimation error of 0.00 $\pm$ 9.25 mmHg for systolic BP and 1.29 $\pm$ 6.37 mmHg for diastolic BP. Notably, the ablation studies highlight the benefits of our context enhancement strategy, leading to an 8.9% reduction in mean absolute error for systolic BP estimation. This paper pioneers the exploration of LLMs for cuffless BP measurement, providing a potential solution to enhance the accuracy of cuffless BP measurement.

------------

`[2406.16562] EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models <https://arxiv.org/abs/2406.16562>`__ EVALALIGN:用人类对齐的数据对多模态llm进行有监督的微调，以评估文本到图像模型

::

    replaced with revised version Thu, 27 Jun 2024 03:57:05 GMT
    Submission history From: Luozheng Qin [view email]
    [v1] Mon, 24 Jun 2024 11:56:15 UTC (2,083 KB)
    [v2] Thu, 27 Jun 2024 03:57:05 UTC (1,277 KB)
    Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, Mengping Yang, Cheng Zhang, Hao Li

The recent advancements in text-to-image generative models have been remarkable. Yet, the field suffers from a lack of evaluation metrics that accurately reflect the performance of these models, particularly lacking fine-grained metrics that can guide the optimization of the models. In this paper, we propose EvalAlign, a metric characterized by its accuracy, stability, and fine granularity. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) pre-trained on extensive datasets. We develop evaluation protocols that focus on two key dimensions: image faithfulness and text-image alignment. Each protocol comprises a set of detailed, fine-grained instructions linked to specific scoring options, enabling precise manual scoring of the generated images. We Supervised Fine-Tune (SFT) the MLLM to align closely with human evaluative judgments, resulting in a robust evaluation model. Our comprehensive tests across 24 text-to-image generation models demonstrate that EvalAlign not only provides superior metric stability but also aligns more closely with human preferences than existing metrics, confirming its effectiveness and utility in model assessment.

------------

`[2305.09605] To smooth a cloud or to pin it down: Guarantees and Insights on Score Matching in Denoising Diffusion Models <https://arxiv.org/abs/2305.09605>`__ 平滑云或固定云:去噪扩散模型中分数匹配的保证和见解

::

    replaced with revised version Wed, 26 Jun 2024 23:41:36 GMT
    Submission history From: Francisco Vargas [view email]
    [v1] Tue, 16 May 2023 16:56:19 UTC (732 KB)
    [v2] Tue, 18 Jun 2024 16:01:52 UTC (10,751 KB)
    [v3] Wed, 26 Jun 2024 23:41:36 UTC (10,751 KB)
    Francisco Vargas, Teodora Reu, Anna Kerekes, Michael M Bronstein

Denoising diffusion models are a class of generative models which have recently achieved state-of-the-art results across many domains. Gradual noise is added to the data using a diffusion process, which transforms the data distribution into a Gaussian. Samples from the generative model are then obtained by simulating an approximation of the time reversal of this diffusion initialized by Gaussian samples. Recent research has explored adapting diffusion models for sampling and inference tasks. In this paper, we leverage known connections to stochastic control akin to the Föllmer drift to extend established neural network approximation results for the Föllmer drift to denoising diffusion models and samplers.

------------

`[2404.07940] InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models <https://arxiv.org/abs/2404.07940>`__ InfiBench:评估代码大型语言模型的问答能力

::

    replaced with revised version Thu, 27 Jun 2024 08:06:15 GMT
    Submission history From: Linyi Li [view email]
    [v1] Mon, 11 Mar 2024 02:06:30 UTC (1,911 KB)
    [v2] Thu, 27 Jun 2024 08:06:15 UTC (8,730 KB)
    Linyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, Hongxia Yang

Large Language Models for code (code LLMs) have witnessed tremendous progress in recent years. With the rapid development of code LLMs, many popular evaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to measure the performance of code LLMs with a particular focus on code generation tasks. However, they are insufficient to cover the full range of expected capabilities of code LLMs, which span beyond code generation to answering diverse coding-related questions. To fill this gap, we propose InfiBench, the first large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages. InfiBench uses four types of model-free automatic metrics to evaluate response correctness where domain experts carefully concretize the criterion for each question. We conduct a systematic evaluation for over 100 latest code LLMs on InfiBench, leading to a series of novel and insightful findings. Our detailed analyses showcase potential directions for further advancement of code LLMs. InfiBench is fully open source and continuously expanding to foster more scientific and systematic practices for code LLM evaluation.

------------

----------
Index (75)
----------

`[2406.18740] Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models <https://arxiv.org/abs/2406.18740>`__ 分步重排序:研究大型语言模型重排序的预过滤

`[2406.18776] Implicit Discourse Relation Classification For Nigerian Pidgin <https://arxiv.org/abs/2406.18776>`__ 面向尼日利亚洋泾浜语的隐式篇章关系分类

`[2406.18783] Psychological Profiling in Cybersecurity: A Look at LLMs and Psycholinguistic Features <https://arxiv.org/abs/2406.18783>`__

`[2406.18856] FFN: a Fine-grained Chinese-English Financial Domain Parallel Corpus <https://arxiv.org/abs/2406.18856>`__ FFN:一个细粒度的汉英金融领域平行语料库

`[2406.18859] Two-Pronged Human Evaluation of ChatGPT Self-Correction in Radiology Report Simplification <https://arxiv.org/abs/2406.18859>`__ ChatGPT自校正在放射学报告简化中的双管齐下人工评价

`[2406.18880] SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models <https://arxiv.org/abs/2406.18880>`__ SSP:基于大型语言模型的跨语言迁移到低资源语言的自监督提示

`[2406.18895] Can we teach language models to gloss endangered languages? <https://arxiv.org/abs/2406.18895>`__ 我们可以教语言模型来美化濒危语言吗?

`[2406.18906] Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets <https://arxiv.org/abs/2406.18906>`__ 是不是十四行诗，机器人?大型模型和数据集的诗歌评估

`[2406.18916] TrustUQA: A Trustful Framework for Unified Structured Data Question Answering <https://arxiv.org/abs/2406.18916>`__ TrustUQA:一种可信的统一结构化数据问答框架

`[2406.18921] Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data <https://arxiv.org/abs/2406.18921>`__

`[2406.18966] UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models <https://arxiv.org/abs/2406.18966>`__ UniGen:使用大型语言模型生成文本数据集的统一框架

`[2406.19032] Improving Weak-to-Strong Generalization with Reliability-Aware Alignment <https://arxiv.org/abs/2406.19032>`__ 基于可靠性感知的对齐改进弱到强泛化

`[2406.19065] STBench: Assessing the Ability of Large Language Models in Spatio-Temporal Analysis <https://arxiv.org/abs/2406.19065>`__ STBench:评估大型语言模型在时空分析中的能力

`[2406.19071] EmPO: Theory-Driven Dataset Construction for Empathetic Response Generation through Preference Optimization <https://arxiv.org/abs/2406.19071>`__ EmPO:基于偏好优化的共情响应生成理论驱动数据集构建

`[2406.19102] Statements: Universal Information Extraction from Tables with Large Language Models for ESG KPIs <https://arxiv.org/abs/2406.19102>`__

`[2406.19116] CHEW: A Dataset of CHanging Events in Wikipedia <https://arxiv.org/abs/2406.19116>`__ CHEW:维基百科中变化事件的数据集

`[2406.19227] Aligning Teacher with Student Preferences for Tailored Training Data Generation <https://arxiv.org/abs/2406.19227>`__ 为定制训练数据生成调整教师和学生偏好

`[2406.19238] Revealing Fine-Grained Values and Opinions in Large Language Models <https://arxiv.org/abs/2406.19238>`__ 在大型语言模型中揭示细粒度的值和观点

`[2406.19263] Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding <https://arxiv.org/abs/2406.19263>`__ 随时随地阅读:基于镜头树接地的布局感知GUI屏幕阅读

`[2406.19271] AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning <https://arxiv.org/abs/2406.19271>`__ AutoPureData:用于LLM微调的Web数据自动过滤

`[2406.19349] IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language <https://arxiv.org/abs/2406.19349>`__ IndoToxic2024:印度尼西亚语仇恨言论和毒性类型的人口统计数据集

`[2406.19354] Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs? <https://arxiv.org/abs/2406.19354>`__ 模型编辑的基本问题:llm中的理性信念修正应该如何工作?

`[2406.19356] DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions <https://arxiv.org/abs/2406.19356>`__ DiVERT:数学多项选择题中带有变分误差的干扰项生成

`[2406.19358] The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models <https://arxiv.org/abs/2406.19358>`__ 跨语言情感分析的模型竞技场:大型语言模型时代的比较研究

`[2406.19371] Suri: Multi-constraint Instruction Following for Long-form Text Generation <https://arxiv.org/abs/2406.19371>`__

`[2406.18665] RouteLLM: Learning to Route LLMs with Preference Data <https://arxiv.org/abs/2406.18665>`__ RouteLLM:基于偏好数据的llm路由学习

`[2406.18678] Few-shot Personalization of LLMs with Mis-aligned Responses <https://arxiv.org/abs/2406.18678>`__ 具有错误响应的llm的少样本个性化

`[2406.18725] Jailbreaking LLMs with Arabic Transliteration and Arabizi <https://arxiv.org/abs/2406.18725>`__ 用阿拉伯音译和Arabizi破解llm

`[2406.18815] MissionGNN: Hierarchical Multimodal GNN-based Weakly Supervised Video Anomaly Recognition with Mission-Specific Knowledge Graph Generation <https://arxiv.org/abs/2406.18815>`__ MissionGNN:基于分层多模态gnn的任务特定知识图谱生成的弱监督视频异常识别

`[2406.18851] LICO: Large Language Models for In-Context Molecular Optimization <https://arxiv.org/abs/2406.18851>`__ LICO:面向上下文分子优化的大型语言模型

`[2406.18926] Fine-tuned network relies on generic representation to solve unseen cognitive task <https://arxiv.org/abs/2406.18926>`__ 微调网络依赖通用表示来解决未见过的认知任务

`[2406.19054] A look under the hood of the Interactive Deep Learning Enterprise (No-IDLE) <https://arxiv.org/abs/2406.19054>`__ 交互式深度学习企业的底层架构(No-IDLE)

`[2406.19112] A Teacher Is Worth A Million Instructions <https://arxiv.org/abs/2406.19112>`__ 一位老师抵得上一百万个指导

`[2406.19185] Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion <https://arxiv.org/abs/2406.19185>`__ 对比策略梯度:以监督友好的方式在序列级分数上对齐llm

`[2406.19317] Jump Starting Bandits with LLM-Generated Prior Knowledge <https://arxiv.org/abs/2406.19317>`__

`[2406.19384] The Remarkable Robustness of LLMs: Stages of Inference? <https://arxiv.org/abs/2406.19384>`__ llm的显著鲁棒性:推理阶段?

`[2406.18616] Towards Large Language Model Aided Program Refinement <https://arxiv.org/abs/2406.18616>`__ 面向大规模语言模型辅助程序精化的研究

`[2406.18675] Human-AI Collaborative Taxonomy Construction: A Case Study in Profession-Specific Writing Assistants <https://arxiv.org/abs/2406.18675>`__ 人- ai协同分类体系构建:专业写作助手案例研究

`[2406.18841] Navigating LLM Ethics: Advancements, Challenges, and Future Directions <https://arxiv.org/abs/2406.18841>`__ 探索LLM伦理:进展、挑战和未来方向

`[2406.18842] The global landscape of academic guidelines for generative AI and Large Language Models <https://arxiv.org/abs/2406.18842>`__ 生成式人工智能和大型语言模型学术指南的全球格局

`[2406.19280] HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale <https://arxiv.org/abs/2406.19280>`__ HuatuoGPT-Vision，面向向大规模多模态llm注入医学视觉知识

`[2406.18871] DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text Alignment <https://arxiv.org/abs/2406.18871>`__ DeSTA:基于描述性语音-文本对齐的语音语言模型增强

`[2406.18972] Applying LLMs for Rescoring N-best ASR Hypotheses of Casual Conversations: Effects of Domain Adaptation and Context Carry-over <https://arxiv.org/abs/2406.18972>`__ 用llm重新评分休闲对话的N-best ASR假设:域适应和上下文延续的影响

`[2406.18556] Renal digital pathology visual knowledge search platform based on language large model and book knowledge <https://arxiv.org/abs/2406.18556>`__ 基于语言大模型和书本知识的肾脏数字病理视觉知识搜索平台

`[2406.18583] Lumina-Next: Making Lumina-T2X Stronger and Faster with Next-DiT <https://arxiv.org/abs/2406.18583>`__ Lumina-Next:通过Next-DiT让Lumina-T2X更强大、更快

`[2406.19328] Subtractive Training for Music Stem Insertion using Latent Diffusion Models <https://arxiv.org/abs/2406.19328>`__

`[2309.07683] Assessing the nature of large language models: A caution against anthropocentrism <https://arxiv.org/abs/2309.07683>`__ 评估大型语言模型的性质:对人类中心主义的警告

`[2309.10253] GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts <https://arxiv.org/abs/2309.10253>`__ GPTFUZZER: Red将大型语言模型与自动生成的越狱提示结合起来

`[2405.07474] Integrating Intent Understanding and Optimal Behavior Planning for Behavior Tree Generation from Human Instructions <https://arxiv.org/abs/2405.07474>`__ 融合意图理解和最优行为规划的人工指令行为树生成

`[2310.08279] Enhancing Text-based Knowledge Graph Completion with Zero-Shot Large Language Models: A Focus on Semantic Enhancement <https://arxiv.org/abs/2310.08279>`__ 用零样本大型语言模型增强基于文本的知识图谱补全:语义增强的重点

`[2311.08704] Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains <https://arxiv.org/abs/2311.08704>`__ 大型语言模型能否遵循概念注释准则?科学和金融领域的案例研究

`[2401.09395] Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions <https://arxiv.org/abs/2401.09395>`__ 通过本体引导的干预评估llm的数学和编码能力

`[2401.10415] Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals? <https://arxiv.org/abs/2401.10415>`__ 大型语言模型摘要器能否适应多样化的科学传播目标?

`[2402.07610] Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping <https://arxiv.org/abs/2402.07610>`__ Step-On-Feet Tuning:通过Bootstrapping扩展llm的自对齐

`[2404.11999] Token-level Direct Preference Optimization <https://arxiv.org/abs/2404.11999>`__ 令牌级直接偏好优化

`[2405.00253] CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification <https://arxiv.org/abs/2405.00253>`__ CodeHalu:基于执行验证驱动的llm代码幻觉

`[2405.10443] Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation <https://arxiv.org/abs/2405.10443>`__ 同时屏蔽，而不是提示优化:面向同传翻译的微调llm范式转变

`[2406.00041] QUB-Cirdan at "Discharge Me!": Zero shot discharge letter generation by open-source LLM <https://arxiv.org/abs/2406.00041>`__ QUB-Cirdan在“让我出院!”: Zero shot discharge letter generation by开源LLM

`[2406.11385] MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic <https://arxiv.org/abs/2406.11385>`__ MetaGPT:基于模型独占任务算法的大型语言模型合并

`[2406.12036] MedCalc-Bench: Evaluating Large Language Models for Medical Calculations <https://arxiv.org/abs/2406.12036>`__ MedCalc-Bench:医学计算大型语言模型评估

`[2406.12416] Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models <https://arxiv.org/abs/2406.12416>`__ 超越欠对齐:原子偏好增强的大型语言模型事实性调优

`[2406.12644] Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models <https://arxiv.org/abs/2406.12644>`__ 层次提示分类法:大型语言模型通用评估框架

`[2406.13444] VDebugger: Harnessing Execution Feedback for Debugging Visual Programs <https://arxiv.org/abs/2406.13444>`__ VDebugger:利用执行反馈来调试可视化程序

`[2406.18192] Methodology of Adapting Large English Language Models for Specific Cultural Contexts <https://arxiv.org/abs/2406.18192>`__ 适应特定文化语境的大型英语语言模型的方法论

`[2406.18294] Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs <https://arxiv.org/abs/2406.18294>`__ 分层上下文剪枝:用存储库级预训练代码llm优化真实世界的代码补全

`[2401.09181] Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer <https://arxiv.org/abs/2401.09181>`__ 除了反遗忘:基于正向迁移的多模态连续指令调整

`[2402.05162] Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications <https://arxiv.org/abs/2402.05162>`__ 基于剪枝和低秩修正的安全对齐脆弱性评估

`[2402.14905] MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases <https://arxiv.org/abs/2402.14905>`__ MobileLLM:面向设备用例的亚十亿参数语言模型优化

`[2403.08819] Thermometer: Towards Universal Calibration for Large Language Models <https://arxiv.org/abs/2403.08819>`__ 温度计:走向大型语言模型的通用校准

`[2405.18641] Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning <https://arxiv.org/abs/2405.18641>`__ 针对有害微调的大型语言模型的惰性安全对齐

`[2401.15847] Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA <https://arxiv.org/abs/2401.15847>`__ 松饼还是吉娃娃?用多面板VQA挑战多模态大型语言模型

`[2406.18069] Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals <https://arxiv.org/abs/2406.18069>`__ 基于可穿戴生物信号的无袖血压测量大型语言模型

`[2406.16562] EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models <https://arxiv.org/abs/2406.16562>`__ EVALALIGN:用人类对齐的数据对多模态llm进行有监督的微调，以评估文本到图像模型

`[2305.09605] To smooth a cloud or to pin it down: Guarantees and Insights on Score Matching in Denoising Diffusion Models <https://arxiv.org/abs/2305.09605>`__ 平滑云或固定云:去噪扩散模型中分数匹配的保证和见解

`[2404.07940] InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models <https://arxiv.org/abs/2404.07940>`__ InfiBench:评估代码大型语言模型的问答能力

