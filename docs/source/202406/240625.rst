240625
========

----------
Survey (8)
----------

`[2402.13446] Large Language Models for Data Annotation: A Survey <https://arxiv.org/abs/2402.13446>`__ 面向数据标注的大型语言模型综述

::

    replaced with revised version Sun, 23 Jun 2024 21:51:45 GMT
    Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu

Categories

------------

`[2402.17944] Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding -- A Survey <https://arxiv.org/abs/2402.17944>`__ 表格数据上的大型语言模型(llm):预测、生成和理解——综述

::

    replaced with revised version Fri, 21 Jun 2024 19:59:54 GMT
    Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, Christos Faloutsos

Categories

------------

`[2403.08319] Knowledge Conflicts for LLMs: A Survey <https://arxiv.org/abs/2403.08319>`__ llm的知识冲突研究综述

::

    replaced with revised version Sat, 22 Jun 2024 08:31:40 GMT
    Submission history From: Rongwu Xu [view email]
    [v1] Wed, 13 Mar 2024 08:02:23 UTC (796 KB)
    [v2] Sat, 22 Jun 2024 08:31:40 UTC (747 KB)
    Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, Wei Xu

This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.

------------

`[2405.13025] A survey on fairness of large language models in e-commerce: progress, application, and challenge <https://arxiv.org/abs/2405.13025>`__ 电子商务大型语言模型公平性研究进展、应用与挑战

::

    replaced with revised version Fri, 21 Jun 2024 21:26:03 GMT
    Submission history From: Shuning Huo [view email]
    [v1] Wed, 15 May 2024 23:25:19 UTC (2,806 KB)
    [v2] Fri, 21 Jun 2024 21:26:03 UTC (2,806 KB)
    Qingyang Ren, Zilin Jiang, Jinghan Cao, Sijia Li, Chiqu Li, Yiyang Liu, Shuning Huo, Tiange He, Yuan Chen

This survey explores the fairness of large language models (LLMs) in e-commerce, examining their progress, applications, and the challenges they face. LLMs have become pivotal in the e-commerce domain, offering innovative solutions and enhancing customer experiences. This work presents a comprehensive survey on the applications and challenges of LLMs in e-commerce. The paper begins by introducing the key principles underlying the use of LLMs in e-commerce, detailing the processes of pretraining, fine-tuning, and prompting that tailor these models to specific needs. It then explores the varied applications of LLMs in e-commerce, including product reviews, where they synthesize and analyze customer feedback; product recommendations, where they leverage consumer data to suggest relevant items; product information translation, enhancing global accessibility; and product question and answer sections, where they automate customer support. The paper critically addresses the fairness challenges in e-commerce, highlighting how biases in training data and algorithms can lead to unfair outcomes, such as reinforcing stereotypes or discriminating against certain groups. These issues not only undermine consumer trust, but also raise ethical and legal concerns. Finally, the work outlines future research directions, emphasizing the need for more equitable and transparent LLMs in e-commerce. It advocates for ongoing efforts to mitigate biases and improve the fairness of these systems, ensuring they serve diverse global markets effectively and ethically. Through this comprehensive analysis, the survey provides a holistic view of the current landscape of LLMs in e-commerce, offering insights into their potential and limitations, and guiding future endeavors in creating fairer and more inclusive e-commerce environments.

------------

`[2405.08011] A Survey of Large Language Models for Graphs <https://arxiv.org/abs/2405.08011>`__ 面向图的大型语言模型综述

::

    replaced with revised version Mon, 24 Jun 2024 10:25:19 GMT
    Submission history From: Xubin Ren [view email]
    [v1] Fri, 10 May 2024 18:05:37 UTC (416 KB)
    [v2] Mon, 24 Jun 2024 10:25:19 UTC (402 KB)
    Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, Chao Huang

Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks. In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category. We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas. This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field. We consistently maintain the related open-source materials at \url{this https URL}.

------------

`[2310.14414] Vision Language Models in Autonomous Driving: A Survey and Outlook <https://arxiv.org/abs/2310.14414>`__ 自动驾驶视觉语言模型综述与展望

::

    replaced with revised version Thu, 20 Jun 2024 19:36:38 GMT
    Submission history From: Xingcheng Zhou [view email]
    [v1] Sun, 22 Oct 2023 21:06:10 UTC (36,879 KB)
    [v2] Thu, 20 Jun 2024 19:36:38 UTC (16,247 KB)
    Xingcheng Zhou, Mingyu Liu, Ekim Yurtsever, Bare Luka Zagar, Walter Zimmer, Hu Cao, Alois C. Knoll

The applications of Vision-Language Models (VLMs) in the field of Autonomous Driving (AD) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By incorporating language data, driving systems can gain a better understanding of real-world environments, thereby enhancing driving safety and efficiency. In this work, we present a comprehensive and systematic survey of the advances in vision language models in this domain, encompassing perception and understanding, navigation and planning, decision-making and control, end-to-end autonomous driving, and data generation. We introduce the mainstream VLM tasks in AD and the commonly utilized metrics. Additionally, we review current studies and applications in various areas and summarize the existing language-enhanced autonomous driving datasets thoroughly. Lastly, we discuss the benefits and challenges of VLMs in AD and provide researchers with the current research gaps and future trends.

------------

`[2403.14734] A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond <https://arxiv.org/abs/2403.14734>`__ 神经代码智能综述:范式、进展与超越

::

    replaced with revised version Sun, 23 Jun 2024 14:31:05 GMT
    Submission history From: Qiushi Sun [view email]
    [v1] Thu, 21 Mar 2024 08:54:56 UTC (2,212 KB)
    [v2] Sun, 23 Jun 2024 14:31:05 UTC (3,197 KB)
    Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, Qipeng Guo, Xipeng Qiu, Pengcheng Yin, Xiaoli Li, Fei Yuan, Lingpeng Kong, Xiang Li, Zhiyong Wu

Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we also observe a co-evolving shift. It spans from initial endeavors to tackling specific scenarios, through exploring a diverse array of tasks during its rapid expansion, to currently focusing on tackling increasingly complex and varied real-world challenges. Building on our examination of the developmental trajectories, we further investigate the emerging synergies between code intelligence and broader machine intelligence, uncovering new cross-domain opportunities and illustrating the substantial influence of code intelligence across various domains. Finally, we delve into both the opportunities and challenges associated with this field, alongside elucidating our insights on the most promising research directions. An ongoing, dynamically updated project and resources associated with this survey have been released at this https URL.

------------

`[2405.13245] A Survey of Robotic Language Grounding: Tradeoffs between Symbols and Embeddings <https://arxiv.org/abs/2405.13245>`__ 机器人语言基础综述:符号和嵌入之间的权衡

::

    replaced with revised version Sat, 22 Jun 2024 13:03:53 GMT
    Submission history From: Jason Xinyu Liu [view email]
    [v1] Tue, 21 May 2024 23:12:03 UTC (130 KB)
    [v2] Sat, 22 Jun 2024 13:03:53 UTC (138 KB)
    Vanya Cohen, Jason Xinyu Liu, Raymond Mooney, Stefanie Tellex, David Watkins

With large language models, robots can understand language more flexibly and more capable than ever before. This survey reviews and situates recent literature into a spectrum with two poles: 1) mapping between language and some manually defined formal representation of meaning, and 2) mapping between language and high-dimensional vector spaces that translate directly to low-level robot policy. Using a formal representation allows the meaning of the language to be precisely represented, limits the size of the learning problem, and leads to a framework for interpretability and formal safety guarantees. Methods that embed language and perceptual data into high-dimensional spaces avoid this manually specified symbolic structure and thus have the potential to be more general when fed enough data but require more data and computing to train. We discuss the benefits and tradeoffs of each approach and finish by providing directions for future work that achieves the best of both worlds.

------------

--------------
Benchmark (15)
--------------

`[2406.16176] GraphEval2000: Benchmarking and Improving Large Language Models on Graph Datasets <https://arxiv.org/abs/2406.16176>`__ graphheval2000:在图数据集上对大型语言模型进行基准测试和改进

::

    Sun, 23 Jun 2024 18:01:56 GMT
    Qiming Wu, Zichen Chen, Will Corcoran, Misha Sra, Ambuj K. Singh

Large language models (LLMs) have achieved remarkable success in natural language processing (NLP), demonstrating significant capabilities in processing and understanding text data. However, recent studies have identified limitations in LLMs' ability to reason about graph-structured data. To address this gap, we introduce GraphEval2000, the first comprehensive graph dataset, comprising 40 graph data structure problems along with 2000 test cases.
Additionally, we introduce an evaluation framework based on GraphEval2000, designed to assess the graph reasoning abilities of LLMs through coding challenges. Our dataset categorizes test cases into four primary and four sub-categories, ensuring a comprehensive evaluation. We evaluate eight popular LLMs on GraphEval2000, revealing that LLMs exhibit a better understanding of directed graphs compared to undirected ones. While private LLMs consistently outperform open-source models, the performance gap is narrowing. Furthermore, to improve the usability of our evaluation framework, we propose Structured Symbolic Decomposition (SSD), an instruction-based method designed to enhance LLM performance on GraphEval2000. Results show that SSD improves the performance of GPT-3.5, GPT-4, and GPT-4o on complex graph problems, with an increase of 11.11\%, 33.37\%, and 33.37\%, respectively.

------------

`[2406.15468] Reasoning or Simply Next Token Prediction? A Benchmark for Stress-Testing Large Language Models <https://arxiv.org/abs/2406.15468>`__ 推理还是简单的下一个Token预测?大型语言模型压力测试的基准

::

    Sat, 15 Jun 2024 05:35:47 GMT
    Wentian Wang, Paul Kantor, Jacob Feldman, Lazaros Gallos, Hao Wang

We propose MMLU-SR, a novel dataset designed to measure the true comprehension abilities of Large Language Models (LLMs) by challenging their performance in question-answering tasks with modified terms. We reasoned that an agent that ``truly'' understands a concept can still evaluate it when key terms are replaced by suitably defined alternate terms, and sought to differentiate such comprehension from mere text replacement. In our study, we modified standardized test questions by replacing a key term with a dummy word along with its definition. The key term could be in the context of questions, answers, or both questions and answers.
Notwithstanding the high scores achieved by recent popular LLMs on the MMLU leaderboard, we found a substantial reduction in model performance after such replacement, suggesting poor comprehension. This new benchmark provides a rigorous benchmark for testing true model comprehension, and poses a challenge to the broader scientific community.

------------

`[2406.15484] JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models <https://arxiv.org/abs/2406.15484>`__ JobFair:大型语言模型性别招聘偏见基准框架

::

    Mon, 17 Jun 2024 09:15:57 GMT
    Ze Wang, Zekun Wu, Xin Guan, Michael Thaler, Adriano Koshiyama, Skylar Lu, Sachin Beepath, Ediz Ertekin Jr., Maria Perez-Ortiz

This paper presents a novel framework for benchmarking hierarchical gender hiring bias in Large Language Models (LLMs) for resume scoring, revealing significant issues of reverse bias and overdebiasing. Our contributions are fourfold: First, we introduce a framework using a real, anonymized resume dataset from the Healthcare, Finance, and Construction industries, meticulously used to avoid confounding factors. It evaluates gender hiring biases across hierarchical levels, including Level bias, Spread bias, Taste-based bias, and Statistical bias. This framework can be generalized to other social traits and tasks easily. Second, we propose novel statistical and computational hiring bias metrics based on a counterfactual approach, including Rank After Scoring (RAS), Rank-based Impact Ratio, Permutation Test-Based Metrics, and Fixed Effects Model-based Metrics. These metrics, rooted in labor economics, NLP, and law, enable holistic evaluation of hiring biases. Third, we analyze hiring biases in ten state-of-the-art LLMs. Six out of ten LLMs show significant biases against males in healthcare and finance. An industry-effect regression reveals that the healthcare industry is the most biased against males. GPT-4o and GPT-3.5 are the most biased models, showing significant bias in all three industries. Conversely, Gemini-1.5-Pro, Llama3-8b-Instruct, and Llama3-70b-Instruct are the least biased. The hiring bias of all LLMs, except for Llama3-8b-Instruct and Claude-3-Sonnet, remains consistent regardless of random expansion or reduction of resume content. Finally, we offer a user-friendly demo to facilitate adoption and practical application of the framework.

------------

`[2406.15627] Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph <https://arxiv.org/abs/2406.15627>`__ 基于LM-Polygraph的大型语言模型不确定性量化方法基准测试

::

    Fri, 21 Jun 2024 20:06:31 GMT
    Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Akim Tsvigun, Daniil Vasilev, Rui Xing, Abdelrahman Boda Sadallah, Lyudmila Rvanova, Sergey Petrakov, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov, Artem Shelmanov

Uncertainty quantification (UQ) is becoming increasingly recognized as a critical component of applications that rely on machine learning (ML). The rapid proliferation of large language models (LLMs) has stimulated researchers to seek efficient and effective approaches to UQ in text generation tasks, as in addition to their emerging capabilities, these models have introduced new challenges for building safe applications. As with other ML models, LLMs are prone to make incorrect predictions, ``hallucinate'' by fabricating claims, or simply generate low-quality output for a given input. UQ is a key element in dealing with these challenges. However research to date on UQ methods for LLMs has been fragmented, with disparate evaluation methods. In this work, we tackle this issue by introducing a novel benchmark that implements a collection of state-of-the-art UQ baselines, and provides an environment for controllable and consistent evaluation of novel techniques by researchers in various text generation tasks. Our benchmark also supports the assessment of confidence normalization methods in terms of their ability to provide interpretable scores. Using our benchmark, we conduct a large-scale empirical investigation of UQ and normalization techniques across nine tasks and shed light on the most promising approaches.

------------

`[2406.15695] SS-Bench: A Benchmark for Social Story Generation and Evaluation <https://arxiv.org/abs/2406.15695>`__ 

::

    Sat, 22 Jun 2024 00:14:48 GMT
    Yi Feng, Mingyang Song, Jiaqi Wang, Mao Zheng, Liping Jing, Jian Yu

Children with Autism Spectrum Disorder (ASD) often misunderstand social situations and struggle to participate in daily routines. Psychology experts write Social Stories under strict constraints of structural clarity, descriptive orientation, and situational safety to enhance their abilities in these regimes. However, Social Stories are costly in creation and often limited in diversity and timeliness. As Large Language Models (LLMs) become increasingly powerful, there is a growing need for more automated, affordable, and accessible methods to generate Social Stories in real-time with broad coverage. Adapting LLMs to meet the unique and strict constraints of Social Stories is a challenging issue. To this end, we propose \textbf{SS-Bench}, a \textbf{S}ocial \textbf{S}tory \textbf{Bench}mark for generating and evaluating Social Stories. Specifically, we develop a constraint-driven strategy named \textbf{\textsc{StarSow}} to hierarchically prompt LLMs to generate Social Stories and build a benchmark, which has been validated through experiments to fine-tune smaller models for generating qualified Social Stories. Additionally, we introduce \textbf{Quality Assessment Criteria}, employed in human and GPT evaluations, to verify the effectiveness of the generated stories. We hope this work benefits the autism community and catalyzes future research focusing on particular groups.

------------

`[2406.15823] CaT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans <https://arxiv.org/abs/2406.15823>`__ CaT-BENCH:计划中因果和时间依赖关系理解的基准语言模型

::

    Sat, 22 Jun 2024 11:46:04 GMT
    Yash Kumar Lal, Vanya Cohen, Nathanael Chambers, Niranjan Balasubramanian, Raymond Mooney

Understanding the abilities of LLMs to reason about natural language plans, such as instructional text and recipes, is critical to reliably using them in decision-making systems. A fundamental aspect of plans is the temporal order in which their steps needs to be executed, which reflects the underlying causal dependencies between them. We introduce CaT-Bench, a benchmark of Step Order Prediction questions, which test whether a step must necessarily occur before or after another in cooking recipe plans. We use this to evaluate how well frontier LLMs understand causal and temporal dependencies. We find that SOTA LLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased towards predicting dependence more often, perhaps relying on temporal order of steps as a heuristic. While prompting for explanations and using few-shot examples improve performance, the best F1 result is only 0.73. Further, human evaluation of explanations along with answer correctness show that, on average, humans do not agree with model reasoning. Surprisingly, we also find that explaining after answering leads to better performance than normal chain-of-thought prompting, and LLM answers are not consistent across questions about the same step pairs. Overall, results show that LLMs' ability to detect dependence between steps has significant room for improvement.

------------

`[2406.16086] SEAM: A Stochastic Benchmark for Multi-Document Tasks <https://arxiv.org/abs/2406.16086>`__ SEAM:多文档任务的随机基准

::

    Sun, 23 Jun 2024 11:57:53 GMT
    Gili Lior, Avi Caciularu, Arie Cattan, Shahar Levy, Ori Shapira, Gabriel Stanovsky

Various tasks, such as summarization, multi-hop question answering, or coreference resolution, are naturally phrased over collections of real-world documents. Such tasks present a unique set of challenges, revolving around the lack of coherent narrative structure across documents, which often leads to contradiction, omission, or repetition of information. Despite their real-world application and challenging properties, there is currently no benchmark which specifically measures the abilities of large language models (LLMs) on multi-document tasks. To bridge this gap, we present SEAM (a Stochastic Evaluation Approach for Multi-document tasks), a conglomerate benchmark over a diverse set of multi-document datasets, setting conventional evaluation criteria, input-output formats, and evaluation protocols. In particular, SEAM addresses the sensitivity of LLMs to minor prompt variations through repeated evaluations, where in each evaluation we sample uniformly at random the values of arbitrary factors (e.g., the order of documents). We evaluate different LLMs on SEAM finding that multi-document tasks pose a significant challenge for LLMs, even for state-of-the-art models with 70B parameters. In addition, we show that the stochastic approach uncovers underlying statistical trends which cannot be observed in a static benchmark. We hope that SEAM will spur progress via consistent and meaningful evaluation of multi-document tasks.

------------

`[2406.15877] BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions <https://arxiv.org/abs/2406.15877>`__ BigCodeBench:对不同函数调用和复杂指令的代码生成进行基准测试

::

    Sat, 22 Jun 2024 15:52:04 GMT
    Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra

Automated software engineering has been greatly empowered by the recent advances in Large Language Models (LLMs) for programming. While current benchmarks have shown that LLMs can perform various software engineering tasks like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks. Solving challenging and practical programming tasks requires the capability of utilizing diverse function calls as tools to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding complex instructions.
Fulfilling both of these characteristics can pose a great challenge for LLMs.
To assess how well LLMs can solve challenging and practical programming tasks, we introduce Bench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained programming tasks. To evaluate LLMs rigorously, each programming task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of Bench, Benchi, that automatically transforms the original docstrings into short instructions only with essential information. Our extensive evaluation of 60 LLMs shows that LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%. The results underscore the need for further advancements in this area.

------------

`[2406.15885] The Music Maestro or The Musically Challenged, A Massive Music Evaluation Benchmark for Large Language Models <https://arxiv.org/abs/2406.15885>`__ 音乐大师或音乐挑战，大型语言模型的大规模音乐评估基准

::

    Sat, 22 Jun 2024 16:24:42 GMT
    Jiajia Li, Lu Yang, Mingni Tang, Cong Chen, Zuchao Li, Ping Wang, Hai Zhao

Benchmark plays a pivotal role in assessing the advancements of large language models (LLMs). While numerous benchmarks have been proposed to evaluate LLMs' capabilities, there is a notable absence of a dedicated benchmark for assessing their musical abilities. To address this gap, we present ZIQI-Eval, a comprehensive and large-scale music benchmark specifically designed to evaluate the music-related capabilities of LLMs. ZIQI-Eval encompasses a wide range of questions, covering 10 major categories and 56 subcategories, resulting in over 14,000 meticulously curated data entries. By leveraging ZIQI-Eval, we conduct a comprehensive evaluation over 16 LLMs to evaluate and analyze LLMs' performance in the domain of music. Results indicate that all LLMs perform poorly on the ZIQI-Eval benchmark, suggesting significant room for improvement in their musical capabilities. With ZIQI-Eval, we aim to provide a standardized and robust evaluation framework that facilitates a comprehensive assessment of LLMs' music-related abilities. The dataset is available at GitHub\footnote{https://github.com/zcli-charlie/ZIQI-Eval} and HuggingFace\footnote{https://huggingface.co/datasets/MYTH-Lab/ZIQI-Eval}.

------------

`[2406.16020] AudioBench: A Universal Benchmark for Audio Large Language Models <https://arxiv.org/abs/2406.16020>`__ audibench:音频大型语言模型通用基准

::

    Sun, 23 Jun 2024 05:40:26 GMT
    Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, Nancy F. Chen

We introduce AudioBench, a new benchmark designed to evaluate audio large language models (AudioLLMs). AudioBench encompasses 8 distinct tasks and 26 carefully selected or newly curated datasets, focusing on speech understanding, voice interpretation, and audio scene understanding. Despite the rapid advancement of large language models, including multimodal versions, a significant gap exists in comprehensive benchmarks for thoroughly evaluating their capabilities. AudioBench addresses this gap by providing relevant datasets and evaluation metrics. In our study, we evaluated the capabilities of four models across various aspects and found that no single model excels consistently across all tasks. We outline the research outlook for AudioLLMs and anticipate that our open-source code, data, and leaderboard will offer a robust testbed for future model developments.

------------

`[2402.01622] TravelPlanner: A Benchmark for Real-World Planning with Language Agents <https://arxiv.org/abs/2402.01622>`__ TravelPlanner:使用语言代理进行现实世界规划的基准

::

    replaced with revised version Sun, 23 Jun 2024 08:50:17 GMT
    Submission history From: Jian Xie [view email]
    [v1] Fri, 2 Feb 2024 18:39:51 UTC (2,873 KB)
    [v2] Mon, 5 Feb 2024 06:48:01 UTC (2,866 KB)
    [v3] Sun, 23 Jun 2024 08:50:17 UTC (2,865 KB)
    Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su

Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.

------------

`[2404.08676] ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming <https://arxiv.org/abs/2404.08676>`__ ALERT:通过红色团队评估大型语言模型安全性的综合基准

::

    replaced with revised version Mon, 24 Jun 2024 08:50:22 GMT
    Submission history From: Simone Tedeschi [view email]
    [v1] Sat, 6 Apr 2024 15:01:47 UTC (1,404 KB)
    [v2] Thu, 20 Jun 2024 07:23:06 UTC (1,149 KB)
    [v3] Mon, 24 Jun 2024 08:50:22 UTC (1,151 KB)
    Simone Tedeschi, Felix Friedrich, Patrick Schramowski, Kristian Kersting, Roberto Navigli, Huu Nguyen, Bo Li

When building Large Language Models (LLMs), it is paramount to bear safety in mind and protect them with guardrails. Indeed, LLMs should never generate content promoting or normalizing harmful, illegal, or unethical behavior that may contribute to harm to individuals or society. This principle applies to both normal and adversarial use. In response, we introduce ALERT, a large-scale benchmark to assess safety based on a novel fine-grained risk taxonomy. It is designed to evaluate the safety of LLMs through red teaming methodologies and consists of more than 45k instructions categorized using our novel taxonomy. By subjecting LLMs to adversarial testing scenarios, ALERT aims to identify vulnerabilities, inform improvements, and enhance the overall safety of the language models. Furthermore, the fine-grained taxonomy enables researchers to perform an in-depth evaluation that also helps one to assess the alignment with various policies. In our experiments, we extensively evaluate 10 popular open- and closed-source LLMs and demonstrate that many of them still struggle to attain reasonable levels of safety.

------------

`[2406.13990] Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation <https://arxiv.org/abs/2406.13990>`__ 推理时去污染:重用泄漏的大型语言模型评估基准

::

    replaced with revised version Sun, 23 Jun 2024 16:46:00 GMT
    Submission history From: Qin Zhu [view email]
    [v1] Thu, 20 Jun 2024 04:35:59 UTC (1,664 KB)
    [v2] Sun, 23 Jun 2024 16:46:00 UTC (1,664 KB)
    Qin Zhu and Qingyuan Cheng and Runyu Peng and Xiaonan Li and Tengxiao Liu and Ru Peng and Xipeng Qiu and Xuanjing Huang

The training process of large language models (LLMs) often involves varying degrees of test data contamination. Although current LLMs are achieving increasingly better performance on various benchmarks, their performance in practical applications does not always match their benchmark results. Leakage of benchmarks can prevent the accurate assessment of LLMs' true performance. However, constructing new benchmarks is costly, labor-intensive and still carries the risk of leakage. Therefore, in this paper, we ask the question, Can we reuse these leaked benchmarks for LLM evaluation? We propose Inference-Time Decontamination (ITD) to address this issue by detecting and rewriting leaked samples without altering their difficulties. ITD can mitigate performance inflation caused by memorizing leaked benchmarks. Our proof-of-concept experiments demonstrate that ITD reduces inflated accuracy by 22.9% on GSM8K and 19.0% on MMLU. On MMLU, using Inference-time Decontamination can lead to a decrease in the results of Phi3 and Mistral by 6.7% and 3.6% respectively. We hope that ITD can provide more truthful evaluation results for large language models.

------------

`[2406.07599] CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence <https://arxiv.org/abs/2406.07599>`__ CTIBench:网络威胁情报llm评估基准

::

    replaced with revised version Mon, 24 Jun 2024 04:14:26 GMT
    Submission history From: Md Tanvirul Alam [view email]
    [v1] Tue, 11 Jun 2024 16:42:02 UTC (7,691 KB)
    [v2] Mon, 24 Jun 2024 04:14:26 UTC (7,691 KB)
    Md Tanvirul Alam, Dipkamal Bhusal, Le Nguyen, Nidhi Rastogi

Cyber threat intelligence (CTI) is crucial in today's cybersecurity landscape, providing essential insights to understand and mitigate the ever-evolving cyber threats. The recent rise of Large Language Models (LLMs) have shown potential in this domain, but concerns about their reliability, accuracy, and hallucinations persist. While existing benchmarks provide general evaluations of LLMs, there are no benchmarks that address the practical and applied aspects of CTI-specific tasks. To bridge this gap, we introduce CTIBench, a benchmark designed to assess LLMs' performance in CTI applications. CTIBench includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art models on these tasks provides insights into their strengths and weaknesses in CTI contexts, contributing to a better understanding of LLM capabilities in CTI.

------------

`[2403.00393] TRUCE: Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs <https://arxiv.org/abs/2403.00393>`__ 休战:防止污染和提高llm比较评估的私人基准

::

    replaced with revised version Mon, 24 Jun 2024 08:28:18 GMT
    Submission history From: Tanmay Rajore [view email]
    [v1] Fri, 1 Mar 2024 09:28:38 UTC (9,848 KB)
    [v2] Mon, 24 Jun 2024 08:28:18 UTC (9,826 KB)
    Tanmay Rajore, Nishanth Chandran, Sunayana Sitaram, Divya Gupta, Rahul Sharma, Kashish Mittal, Manohar Swaminathan

Benchmarking is the de-facto standard for evaluating LLMs, due to its speed, replicability and low cost. However, recent work has pointed out that the majority of the open source benchmarks available today have been contaminated or leaked into LLMs, meaning that LLMs have access to test data during pretraining and/or fine-tuning. This raises serious concerns about the validity of benchmarking studies conducted so far and the future of evaluation using benchmarks. To solve this problem, we propose Private Benchmarking, a solution where test datasets are kept private and models are evaluated without revealing the test data to the model. We describe various scenarios (depending on the trust placed on model owners or dataset owners), and present solutions to avoid data contamination using private benchmarking. For scenarios where the model weights need to be kept private, we describe solutions from confidential computing and cryptography that can aid in private benchmarking. We build an end-to-end system, TRUCE, that enables such private benchmarking showing that the overheads introduced to protect models and benchmark are negligible (in the case of confidential computing) and tractable (when cryptographic security is required). Finally, we also discuss solutions to the problem of benchmark dataset auditing, to ensure that private benchmarks are of sufficiently high quality.

------------

---------------
Accelerate (23)
---------------

`[2406.16218] Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows <https://arxiv.org/abs/2406.16218>`__ Trace是新的AutoDiff——解锁计算工作流的高效优化

::

    Sun, 23 Jun 2024 21:05:31 GMT
    Ching-An Cheng, Allen Nie, Adith Swaminathan

We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback. Website: https://microsoft.github.io/Trace

------------

`[2406.15586] TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship Embeddings <https://arxiv.org/abs/2406.15586>`__ TinyStyler:基于作者嵌入的高效少样本文本风格迁移

::

    Fri, 21 Jun 2024 18:41:22 GMT
    Zachary Horvitz, Ajay Patel, Kanishk Singh, Chris Callison-Burch, Kathleen McKeown, Zhou Yu

The goal of text style transfer is to transform the style of texts while preserving their original meaning, often with only a few examples of the target style. Existing style transfer methods generally rely on the few-shot capabilities of large language models or on complex controllable text generation approaches that are inefficient and underperform on fluency metrics.
We introduce TinyStyler, a lightweight but effective approach, which leverages a small language model (800M params) and pre-trained authorship embeddings to perform efficient, few-shot text style transfer. We evaluate on the challenging task of authorship style transfer and find TinyStyler outperforms strong approaches such as GPT-4. We also evaluate TinyStyler's ability to perform text attribute style transfer (formal $\leftrightarrow$ informal) with automatic and human evaluations and find that the approach outperforms recent controllable text generation methods. Our model has been made publicly available at https://huggingface.co/tinystyler/tinystyler .

------------

`[2406.16306] Cascade Reward Sampling for Efficient Decoding-Time Alignment <https://arxiv.org/abs/2406.16306>`__ 基于级联奖励采样的高效解码时间对齐

::

    Mon, 24 Jun 2024 04:08:35 GMT
    Bolian Li, Yifan Wang, Ananth Grama, Ruqi Zhang

Aligning large language models (LLMs) with human preferences is critical for their deployment. Recently, decoding-time alignment has emerged as an effective plug-and-play technique that requires no fine-tuning of model parameters.
However, generating text that achieves both high reward and high likelihood remains a significant challenge. Existing methods often fail to generate high-reward text or incur substantial computational costs. In this paper, we propose Cascade Reward Sampling (CARDS) to address both issues, guaranteeing the generation of high-reward and high-likelihood text with significantly low costs. Based on our analysis of reward models (RMs) on incomplete text and our observation that high-reward prefixes induce high-reward complete text, we use rejection sampling to iteratively generate small semantic segments to form such prefixes. The segment length is dynamically determined by the predictive uncertainty of LLMs. This strategy guarantees desirable prefixes for subsequent generations and significantly reduces wasteful token re-generations and the number of reward model scoring. Our experiments demonstrate substantial gains in both generation efficiency and alignment ratings compared to the baselines, achieving five times faster text generation and 99\% win-ties in GPT-4/Claude-3 helpfulness evaluation.

------------

`[2406.16450] Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers <https://arxiv.org/abs/2406.16450>`__ 建立在有效的基础上:有效训练具有结构化前馈层的llm

::

    Mon, 24 Jun 2024 08:43:21 GMT
    Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre

State-of-the-art results in large language models (LLMs) often rely on scale, which becomes computationally expensive. This has sparked a research agenda to reduce these models' parameter count and computational costs without significantly impacting their performance. Our study focuses on transformer-based LLMs, specifically targeting the computationally intensive feedforward networks (FFN), which are less studied than attention blocks. We consider three candidate linear layer approximations in the FFN by combining efficient low-rank and block-diagonal matrices. In contrast to many previous works that examined these approximations, our study i) explores these structures from the training-from-scratch perspective, ii) scales up to 1.3B parameters, and iii) is conducted within recent Transformer-based LLMs rather than convolutional architectures. We first demonstrate they can lead to actual computational gains in various scenarios, including online decoding when using a pre-merge technique. Additionally, we propose a novel training regime, called \textit{self-guided training}, aimed at improving the poor training dynamics that these approximations exhibit when used from initialization. Experiments on the large RefinedWeb dataset show that our methods are both efficient and effective for training and inference. Interestingly, these structured FFNs exhibit steeper scaling curves than the original models. Further applying self-guided training to the structured matrices with 32\% FFN parameters and 2.5$\times$ speed-up enables only a 0.4 perplexity increase under the same training FLOPs. Finally, we develop the wide and structured networks surpassing the current medium-sized and large-sized Transformer in perplexity and throughput performance. Our code is available at \url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}.

------------

`[2406.16678] Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation <https://arxiv.org/abs/2406.16678>`__ 分割任意文本:一种鲁棒、高效和自适应的句子分割通用方法

::

    Mon, 24 Jun 2024 14:36:11 GMT
    Markus Frohmann, Igor Sterner, Ivan Vuli\'c, Benjamin Minixhofer, Markus Schedl

Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, we find that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. We introduce a new model - Segment any Text (SaT) - to solve this problem. To enhance robustness, we propose a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, we introduce an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, we introduce architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, we introduce a variant of our model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, our contributions provide a universal approach for segmenting any text. Our method outperforms all baselines - including strong LLMs - across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. Our models and code, including documentation, are available at https://huggingface.co/segment-any-text under the MIT license.

------------

`[2406.16743] Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization <https://arxiv.org/abs/2406.16743>`__ 对抗性对比解码:通过反向提示优化增强大型语言模型的安全对齐

::

    Mon, 24 Jun 2024 15:51:30 GMT
    Zhengyue Zhao, Xiaoyun Zhang, Kaidi Xu, Xing Hu, Rui Zhang, Zidong Du, Qi Guo, Yunji Chen

With the widespread application of Large Language Models (LLMs), it has become a significant concern to ensure their safety and prevent harmful responses. While current safe-alignment methods based on instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) can effectively reduce harmful responses from LLMs, they often require high-quality datasets and heavy computational overhead during model training. Another way to align language models is to modify the logit of tokens in model outputs without heavy training. Recent studies have shown that contrastive decoding can enhance the performance of language models by reducing the likelihood of confused tokens. However, these methods require the manual selection of contrastive models or instruction templates. To this end, we propose Adversarial Contrastive Decoding (ACD), an optimization-based framework to generate two opposite system prompts for prompt-based contrastive decoding. ACD only needs to apply a lightweight prompt tuning on a rather small anchor dataset (< 3 min for each model) without training the target model. Experiments conducted on extensive models and benchmarks demonstrate that the proposed method achieves much better safety performance than previous model training-free decoding methods without sacrificing its original generation ability.

------------

`[2406.16747] Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers <https://arxiv.org/abs/2406.16747>`__ 稀疏即快，少即是多:远程transformer的高效稀疏注意力

::

    Mon, 24 Jun 2024 15:55:59 GMT
    Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu

Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation.
Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks.
Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.

------------

`[2406.16758] Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters <https://arxiv.org/abs/2406.16758>`__ 面向快速多语言LLM推理:推测解码和专业绘图者

::

    Mon, 24 Jun 2024 16:06:50 GMT
    Euiin Yi, Taehyeon Kim, Hongseok Jeung, Du-Seong Chang, Se-Young Yun

Large language models (LLMs) have revolutionized natural language processing and broadened their applicability across diverse commercial applications.
However, the deployment of these models is constrained by high inference time in multilingual settings. To mitigate this challenge, this paper explores a training recipe of an assistant model in speculative decoding, which are leveraged to draft and-then its future tokens are verified by the target LLM.
We show that language-specific draft models, optimized through a targeted pretrain-and-finetune strategy, substantially brings a speedup of inference time compared to the previous methods. We validate these models across various languages in inference time, out-of-domain speedup, and GPT-4o evaluation.

------------

`[2406.16838] From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models <https://arxiv.org/abs/2406.16838>`__ 

::

    Mon, 24 Jun 2024 17:45:59 GMT
    Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, Zaid Harchaoui

One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results.
However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation.
Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores.
Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information.
Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.

------------

`[2406.15527] Data Efficient Evaluation of Large Language Models and Text-to-Image Models via Adaptive Sampling <https://arxiv.org/abs/2406.15527>`__ 基于自适应采样的大型语言模型和文本到图像模型的数据效率评估

::

    Fri, 21 Jun 2024 07:38:55 GMT
    Cong Xu, Gayathri Saranathan, Mahammad Parwez Alam, Arpit Shah, James Lim, Soon Yee Wong, Foltin Martin, Suparna Bhattacharya

Evaluating LLMs and text-to-image models is a computationally intensive task often overlooked. Efficient evaluation is crucial for understanding the diverse capabilities of these models and enabling comparisons across a growing number of new models and benchmarks. To address this, we introduce SubLIME, a data-efficient evaluation framework that employs adaptive sampling techniques, such as clustering and quality-based methods, to create representative subsets of benchmarks. Our approach ensures statistically aligned model rankings compared to full datasets, evidenced by high Pearson correlation coefficients.
Empirical analysis across six NLP benchmarks reveals that: (1) quality-based sampling consistently achieves strong correlations (0.85 to 0.95) with full datasets at a 10\% sampling rate such as Quality SE and Quality CPD (2) clustering methods excel in specific benchmarks such as MMLU (3) no single method universally outperforms others across all metrics. Extending this framework, we leverage the HEIM leaderboard to cover 25 text-to-image models on 17 different benchmarks. SubLIME dynamically selects the optimal technique for each benchmark, significantly reducing evaluation costs while preserving ranking integrity and score distribution. Notably, a minimal sampling rate of 1% proves effective for benchmarks like MMLU. Additionally, we demonstrate that employing difficulty-based sampling to target more challenging benchmark segments enhances model differentiation with broader score distributions. We also combine semantic search, tool use, and GPT-4 review to identify redundancy across benchmarks within specific LLM categories, such as coding benchmarks.
This allows us to further reduce the number of samples needed to maintain targeted rank preservation. Overall, SubLIME offers a versatile and cost-effective solution for the robust evaluation of LLMs and text-to-image models.

------------

`[2406.15567] SAIL: Self-Improving Efficient Online Alignment of Large Language Models <https://arxiv.org/abs/2406.15567>`__ SAIL:大型语言模型的自改进高效在线对齐

::

    Fri, 21 Jun 2024 18:05:35 GMT
    Mucong Ding, Souradip Chakraborty, Vibhu Agrawal, Zora Che, Alec Koppel, Mengdi Wang, Amrit Bedi, Furong Huang

Reinforcement Learning from Human Feedback (RLHF) is a key method for aligning large language models (LLMs) with human preferences. However, current offline alignment approaches like DPO, IPO, and SLiC rely heavily on fixed preference datasets, which can lead to sub-optimal performance. On the other hand, recent literature has focused on designing online RLHF methods but still lacks a unified conceptual formulation and suffers from distribution shift issues. To address this, we establish that online LLM alignment is underpinned by bilevel optimization. By reducing this formulation to an efficient single-level first-order method (using the reward-policy equivalence), our approach generates new samples and iteratively refines model alignment by exploring responses and regulating preference labels. In doing so, we permit alignment methods to operate in an online and self-improving manner, as well as generalize prior online RLHF methods as special cases. Compared to state-of-the-art iterative RLHF methods, our approach significantly improves alignment performance on open-sourced datasets with minimal computational overhead.

------------

`[2406.15758] EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer Tuning and Voting <https://arxiv.org/abs/2406.15758>`__ Edge - llm:通过逐层统一压缩和自适应层调整和投票在边缘设备上实现高效的大型语言模型自适应

::

    Sat, 22 Jun 2024 06:51:47 GMT
    Zhongzhi Yu, Zheng Wang, Yuhan Li, Haoran You, Ruijie Gao, Xiaoya Zhou, Sreenidhi Reedy Bommu, Yang Katie Zhao, Yingyan Celine Lin

Efficient adaption of large language models (LLMs) on edge devices is essential for applications requiring continuous and privacy-preserving adaptation and inference. However, existing tuning techniques fall short because of the high computation and memory overheads. To this end, we introduce a computation- and memory-efficient LLM tuning framework, called Edge-LLM, to facilitate affordable and effective LLM adaptation on edge devices.
Specifically, Edge-LLM features three core components: (1) a layer-wise unified compression (LUC) technique to reduce the computation overhead by generating layer-wise pruning sparsity and quantization bit-width policies, (2) an adaptive layer tuning and voting scheme to reduce the memory overhead by reducing the backpropagation depth, and (3) a complementary hardware scheduling strategy to handle the irregular computation patterns introduced by LUC and adaptive layer tuning, thereby achieving efficient computation and data movements. Extensive experiments demonstrate that Edge-LLM achieves a 2.92x speed up and a 4x memory overhead reduction as compared to vanilla tuning methods with comparable task accuracy. Our code is available at https://github.com/GATECH-EIC/Edge-LLM

------------

`[2406.16565] Noisy Neighbors: Efficient membership inference attacks against LLMs <https://arxiv.org/abs/2406.16565>`__ 噪声邻居:针对llm的高效成员推断攻击

::

    Mon, 24 Jun 2024 12:02:20 GMT
    Filippo Galli and Luca Melis and Tommaso Cucinotta

The potential of transformer-based LLMs risks being hindered by privacy concerns due to their reliance on extensive datasets, possibly including sensitive information. Regulatory measures like GDPR and CCPA call for using robust auditing tools to address potential privacy issues, with Membership Inference Attacks (MIA) being the primary method for assessing LLMs' privacy risks. Differently from traditional MIA approaches, often requiring computationally intensive training of additional models, this paper introduces an efficient methodology that generates \textit{noisy neighbors} for a target sample by adding stochastic noise in the embedding space, requiring operating the target model in inference mode only. Our findings demonstrate that this approach closely matches the effectiveness of employing shadow models, showing its usability in practical privacy auditing scenarios.

------------

`[2406.05673] Flow of Reasoning: Efficient Training of LLM Policy with Divergent Thinking <https://arxiv.org/abs/2406.05673>`__ 推理流程:发散性思维的LLM政策高效训练

::

    replaced with revised version Mon, 24 Jun 2024 15:49:09 GMT
    Submission history From: Fangxu Yu [view email]
    [v1] Sun, 9 Jun 2024 07:06:58 UTC (313 KB)
    [v2] Mon, 24 Jun 2024 15:49:09 UTC (313 KB)
    Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, Lianhui Qin

Divergent thinking, the cognitive process of generating diverse solutions, is a hallmark of human creativity and problem-solving. For machines, sampling diverse solution trajectories in complex reasoning problems is crucial for robust outcomes, data augmentation, and enhanced model generalization. Large language models (LLMs) often struggle with generating high-quality, diverse reasoning. While supervised fine-tuning helps with quality, it requires extensive supervision data to capture the full diversity of solutions. Alternatively, reinforcement learning methods like PPO aim to find limited highest-reward solutions while neglecting the solution diversity, akin to convergent thinking. To address these limitations, we propose Flow of Reasoning (FoR) -- an efficient LLM training approach enabling diverse reasoning with minimal data. FoR formulates multi-step LLM reasoning as a Markovian flow from an initial state to terminal states. The formulation allows to adapt principled GFlowNet approaches to train the LLM as a policy, which is able to sample multiple reasoning paths with probabilities proportional to the unnormalized reward. Empirical results show that, with limited training data (e.g., 15 examples), FoR can discover diverse high-quality solutions that excel greatly beyond current state-of-the-art methods across three tasks, including embodied reasoning (BlocksWorld), math puzzle solving (Game24), and logical reasoning (PrOntoQA). Code is available at this https URL.

------------

`[2401.14280] RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models via Romanization <https://arxiv.org/abs/2401.14280>`__ RomanSetu:通过罗马化高效解锁大型语言模型的多语言能力

::

    replaced with revised version Sun, 23 Jun 2024 11:40:20 GMT
    Submission history From: Jay Gala [view email]
    [v1] Thu, 25 Jan 2024 16:11:41 UTC (7,254 KB)
    [v2] Fri, 8 Mar 2024 18:04:24 UTC (767 KB)
    [v3] Sun, 23 Jun 2024 11:40:20 UTC (8,440 KB)
    Jaavid Aktar Husain, Raj Dabre, Aswanth Kumar, Jay Gala, Thanmay Jayakumar, Ratish Puduppully, Anoop Kunchukuttan

This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages that use non-Roman scripts. We propose an approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Our approach involves the continual pretraining of an English LLM like Llama 2 on romanized text of non-English, non-Roman script languages, followed by instruction tuning on romanized data. The results indicate that romanized text not only reduces token fertility by 2x-4x but also matches or outperforms native script representation across various NLU, NLG, and MT tasks. Moreover, the embeddings computed on romanized text exhibit closer alignment with their English translations than those from the native script. Our approach presents a promising direction for leveraging the power of English LLMs in languages traditionally underrepresented in NLP. Our code is available on this https URL.

------------

`[2402.09801] EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models <https://arxiv.org/abs/2402.09801>`__ EFUF:用于缓解多模态大型语言模型幻觉的高效细粒度遗忘框架

::

    replaced with revised version Mon, 24 Jun 2024 00:50:58 GMT
    Submission history From: Shangyu Xing [view email]
    [v1] Thu, 15 Feb 2024 08:58:03 UTC (387 KB)
    [v2] Mon, 24 Jun 2024 00:50:58 UTC (389 KB)
    Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang and Xinyu Dai

Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.

------------

`[2402.17263] MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning <https://arxiv.org/abs/2402.17263>`__ 

::

    replaced with revised version Mon, 24 Jun 2024 12:45:27 GMT
    Submission history From: Chengshun Shi [view email]
    [v1] Tue, 27 Feb 2024 07:14:12 UTC (6,980 KB)
    [v2] Mon, 24 Jun 2024 12:45:27 UTC (6,981 KB)
    Pengjie Ren, Chengshun Shi, Shiguang Wu, Mengqi Zhang, Zhaochun Ren, Maarten de Rijke, Zhumin Chen, Jiahuan Pei

Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential. The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theoretical analysis and empirical studies on various NLP tasks. Our experimental results show that, compared to LoRA, MELoRA achieves better performance with 8 times fewer trainable parameters on natural language understanding tasks and 36 times fewer trainable parameters on instruction following tasks, which demonstrates the effectiveness of MELoRA.

------------

`[2403.13372] LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models <https://arxiv.org/abs/2403.13372>`__ LlamaFactory: 100多个语言模型的统一高效微调

::

    replaced with revised version Mon, 24 Jun 2024 08:20:04 GMT
    Submission history From: Yaowei Zheng [view email]
    [v1] Wed, 20 Mar 2024 08:08:54 UTC (51 KB)
    [v2] Thu, 21 Mar 2024 08:36:39 UTC (51 KB)
    [v3] Mon, 24 Jun 2024 08:20:04 UTC (55 KB)
    [v4] Thu, 27 Jun 2024 22:44:48 UTC (55 KB)
    Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, Yongqiang Ma

Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at this https URL and received over 25,000 stars and 3,000 forks.

------------

`[2405.04304] Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large Language Models <https://arxiv.org/abs/2405.04304>`__ 动态推测Lookahead加速大型语言模型的推测解码

::

    replaced with revised version Sun, 23 Jun 2024 13:46:23 GMT
    Submission history From: Jonathan Mamou [view email]
    [v1] Tue, 7 May 2024 13:27:52 UTC (674 KB)
    [v2] Tue, 18 Jun 2024 12:34:35 UTC (677 KB)
    [v3] Wed, 19 Jun 2024 08:54:51 UTC (677 KB)
    [v4] Sun, 23 Jun 2024 13:46:23 UTC (677 KB)
    Jonathan Mamou and Oren Pereg and Daniel Korat and Moshe Berchansky and Nadav Timor and Moshe Wasserblat and Roy Schwartz

Speculative decoding is commonly used for reducing the inference latency of large language models. Its effectiveness depends highly on the speculation lookahead (SL)-the number of tokens generated by the draft model at each iteration. In this work we show that the common practice of using the same SL for all iterations (static SL) is suboptimal. We introduce DISCO (DynamIc SpeCulation lookahead Optimization), a novel method for dynamically selecting the SL. Our experiments with four datasets show that DISCO reaches an average speedup of 10% compared to the best static SL baseline, while generating the exact same text.

------------

`[2406.13035] D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models <https://arxiv.org/abs/2406.13035>`__ D2O:大型语言模型高效生成推理的动态判别操作

::

    replaced with revised version Sun, 23 Jun 2024 08:27:48 GMT
    Submission history From: Zhongwei Wan [view email]
    [v1] Tue, 18 Jun 2024 20:01:51 UTC (21,525 KB)
    [v2] Sun, 23 Jun 2024 08:27:48 UTC (21,533 KB)
    Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, Mi Zhang

Efficient inference in Large Language Models (LLMs) is impeded by the growing memory demands of key-value (KV) caching, especially for longer sequences. Traditional KV cache eviction strategies, which prioritize less critical KV-pairs based on attention scores, often degrade generation quality, leading to issues such as context loss or hallucinations. To address this, we introduce Dynamic Discriminative Operations (D2O), a novel method that utilizes two-level discriminative strategies to optimize KV cache size without fine-tuning, while preserving essential context. Initially, by observing varying densities of attention weights between shallow and deep layers, we use this insight to determine which layers should avoid excessive eviction to minimize information loss. Subsequently, for the eviction strategy in each layer, D2O innovatively incorporates a compensation mechanism that maintains a similarity threshold to re-discriminate the importance of previously discarded tokens, determining whether they should be recalled and merged with similar tokens. Our approach not only achieves significant memory savings and enhances inference throughput by more than 3 times but also maintains high-quality long-text generation. Extensive experiments across various benchmarks and LLM architectures have demonstrated that D2O significantly enhances performance with a constrained KV cache budget.

------------

`[2406.15193] Reward Steering with Evolutionary Heuristics for Decoding-time Alignment <https://arxiv.org/abs/2406.15193>`__ 基于进化启发式的解码时间对齐奖励导向

::

    replaced with revised version Mon, 24 Jun 2024 17:36:11 GMT
    Submission history From: Soujanya Poria [view email]
    [v1] Fri, 21 Jun 2024 14:35:16 UTC (2,673 KB)
    [v2] Mon, 24 Jun 2024 17:36:11 UTC (2,674 KB)
    [v3] Tue, 25 Jun 2024 16:55:03 UTC (1,832 KB)
    Chia-Yu Hung, Navonil Majumder, Ambuj Mehrish, Soujanya Poria

The widespread applicability and increasing omnipresence of LLMs have instigated a need to align LLM responses to user and stakeholder preferences. Many preference optimization approaches have been proposed that fine-tune LLM parameters to achieve good alignment. However, such parameter tuning is known to interfere with model performance on many tasks. Moreover, keeping up with shifting user preferences is tricky in such a situation. Decoding-time alignment with reward model guidance solves these issues at the cost of increased inference time. However, most of such methods fail to strike the right balance between exploration and exploitation of reward -- often due to the conflated formulation of these two aspects - to give well-aligned responses. To remedy this we decouple these two aspects and implement them in an evolutionary fashion: exploration is enforced by decoding from mutated instructions and exploitation is represented as the periodic replacement of poorly-rewarded generations with well-rewarded ones. Empirical evidences indicate that this strategy outperforms many preference optimization and decode-time alignment approaches on two widely accepted alignment benchmarks AlpacaEval 2 and MT-Bench. Our implementation will be available at: this https URL.

------------

`[2405.15589] Efficient Adversarial Training in LLMs with Continuous Attacks <https://arxiv.org/abs/2405.15589>`__ 具有连续攻击的llm高效对抗性训练

::

    replaced with revised version Fri, 21 Jun 2024 19:59:31 GMT
    Submission history From: Sophie Xhonneux [view email]
    [v1] Fri, 24 May 2024 14:20:09 UTC (760 KB)
    [v2] Fri, 21 Jun 2024 19:59:31 UTC (759 KB)
    Sophie Xhonneux, Alessandro Sordoni, Stephan G\"unnemann, Gauthier Gidel, Leo Schwinn

Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on four models from different families (Gemma, Phi3, Mistral, Zephyr) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.

------------

`[2401.05391] Efficient LLM inference solution on Intel GPU <https://arxiv.org/abs/2401.05391>`__ 基于Intel GPU的高效LLM推理解决方案

::

    replaced with revised version Sun, 23 Jun 2024 13:03:32 GMT
    Submission history From: Hui Wu [view email]
    [v1] Tue, 19 Dec 2023 05:40:43 UTC (679 KB)
    [v2] Sun, 23 Jun 2024 13:03:32 UTC (1,269 KB)
    Hui Wu, Yi Gan, Feng Yuan, Jing Ma, Wei Zhu, Yutao Xu, Hong Zhu, Yuhua Zhu, Xiaoli Liu, Jinghui Gu, Peng Zhao

Transformer based Large Language Models (LLMs) have been widely used in many fields, and the efficiency of LLM inference becomes hot topic in real applications. However, LLMs are usually complicatedly designed in model structure with massive operations and perform inference in the auto-regressive mode, making it a challenging task to design a system with high efficiency.
In this paper, we propose an efficient LLM inference solution with low latency and high throughput. Firstly, we simplify the LLM decoder layer by fusing data movement and element-wise operations to reduce the memory access frequency and lower system latency. We also propose a segment KV cache policy to keep key/value of the request and response tokens in separate physical memory for effective device memory management, helping enlarge the runtime batch size and improve system throughput. A customized Scaled-Dot-Product-Attention kernel is designed to match our fusion policy based on the segment KV cache solution. We implement our LLM inference solution on Intel GPU and publish it publicly. Compared with the standard HuggingFace implementation, the proposed solution achieves up to 7x lower token latency and 27x higher throughput for some popular LLMs on Intel GPU.

------------

-----------------------
In-Context Learning (7)
-----------------------

`[2406.16007] Distributed Rule Vectors is A Key Mechanism in Large Language Models' In-Context Learning <https://arxiv.org/abs/2406.16007>`__ 分布式规则向量是大型语言模型上下文学习的关键机制

::

    Sun, 23 Jun 2024 04:29:13 GMT
    Bowen Zheng, Ming Ma, Zhongqiao Lin, Tianming Yang

Large Language Models (LLMs) have demonstrated remarkable abilities, one of the most important being In-Context Learning (ICL). With ICL, LLMs can derive the underlying rule from a few demonstrations and provide answers that comply with the rule. Previous work hypothesized that the network creates a "task vector" in specific positions during ICL. Patching the "task vector" allows LLMs to achieve zero-shot performance similar to few-shot learning. However, we discover that such "task vectors" do not exist in tasks where the rule has to be defined through multiple demonstrations. Instead, the rule information provided by each demonstration is first transmitted to its answer position and forms its own rule vector. Importantly, all the rule vectors contribute to the output in a distributed manner. We further show that the rule vectors encode a high-level abstraction of rules extracted from the demonstrations. These results are further validated in a series of tasks that rely on rules dependent on multiple demonstrations. Our study provides novel insights into the mechanism underlying ICL in LLMs, demonstrating how ICL may be achieved through an information aggregation mechanism.

------------

`[2310.09881] In-Context Learning with Iterative Demonstration Selection <https://arxiv.org/abs/2310.09881>`__ 基于迭代演示选择的上下文学习

::

    replaced with revised version Sun, 23 Jun 2024 05:01:28 GMT
    Submission history From: Chengwei Qin [view email]
    [v1] Sun, 15 Oct 2023 16:40:19 UTC (7,641 KB)
    [v2] Sun, 22 Oct 2023 13:40:02 UTC (7,641 KB)
    [v3] Sun, 23 Jun 2024 05:01:28 UTC (8,611 KB)
    Chengwei Qin, Aston Zhang, Chen Chen, Anirudh Dagar, Wenming Ye

Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.

------------

`[2312.17055] Improving In-context Learning via Bidirectional Alignment <https://arxiv.org/abs/2312.17055>`__ 通过双向对齐改进上下文学习

::

    replaced with revised version Mon, 24 Jun 2024 08:34:18 GMT
    Submission history From: Chengwei Qin [view email]
    [v1] Thu, 28 Dec 2023 15:02:03 UTC (7,764 KB)
    [v2] Mon, 24 Jun 2024 08:34:18 UTC (7,759 KB)
    Chengwei Qin, Wenhan Xia, Fangkai Jiao, Chen Chen, Yuchen Hu, Bosheng Ding, Shafiq Joty

Large language models (LLMs) have shown impressive few-shot generalization on many tasks via in-context learning (ICL). Despite their success in showing such emergent abilities, the scale and complexity of larger models also lead to unprecedentedly high computational demands and deployment challenges. In reaction, researchers explore transferring the powerful capabilities of larger models to more efficient and compact models by typically aligning the output of smaller (student) models with that of larger (teacher) models. Existing methods either train student models on the generated outputs of teacher models or imitate their token-level probability distributions. However, these distillation methods pay little to no attention to the input, which also plays a crucial role in ICL. Based on the finding that the performance of ICL is highly sensitive to the selection of demonstration examples, we propose Bidirectional Alignment (BiAlign) to fully leverage the models' preferences for ICL examples to improve the ICL abilities of student models. Specifically, we introduce the alignment of input preferences between student and teacher models by incorporating a novel ranking loss, in addition to aligning the token-level output distribution. With extensive experiments and analysis, we demonstrate that BiAlign can consistently outperform existing baselines on a variety of tasks involving language understanding, reasoning, and coding.

------------

`[2401.12087] Revisiting Demonstration Selection Strategies in In-Context Learning <https://arxiv.org/abs/2401.12087>`__ 情境学习中的示范选择策略研究

::

    replaced with revised version Sun, 23 Jun 2024 13:45:14 GMT
    Submission history From: Keqin Peng [view email]
    [v1] Mon, 22 Jan 2024 16:25:27 UTC (7,711 KB)
    [v2] Sun, 23 Jun 2024 13:45:14 UTC (7,714 KB)
    Keqin Peng, Liang Ding, Yancheng Yuan, Xuebo Liu, Min Zhang, Yuanxin Ouyang, Dacheng Tao

Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model. However, the performance of ICL varies significantly with the choice of demonstrations, and it is still unclear why this happens or what factors will influence its choice. In this work, we first revisit the factors contributing to this variance from both data and model aspects, and find that the choice of demonstration is both data- and model-dependent. We further proposed a data- and model-dependent demonstration selection method, \textbf{TopK + ConE}, based on the assumption that \textit{the performance of a demonstration positively correlates with its contribution to the model's understanding of the test samples}, resulting in a simple and effective recipe for ICL. Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales. Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods. Code will be released.

------------

`[2402.11254] C-ICL: Contrastive In-context Learning for Information Extraction <https://arxiv.org/abs/2402.11254>`__ C-ICL:用于信息提取的上下文对比学习

::

    replaced with revised version Mon, 24 Jun 2024 08:34:56 GMT
    Submission history From: Jian Yang [view email]
    [v1] Sat, 17 Feb 2024 11:28:08 UTC (8,303 KB)
    [v2] Mon, 24 Jun 2024 08:34:56 UTC (8,662 KB)
    Ying Mo, Jiahao Liu, Jian Yang, Qifan Wang, Shun Zhang, Jingang Wang, Zhoujun Li

There has been increasing interest in exploring the capabilities of advanced large language models (LLMs) in the field of information extraction (IE), specifically focusing on tasks related to named entity recognition (NER) and relation extraction (RE). Although researchers are exploring the use of few-shot information extraction through in-context learning with LLMs, they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process. In this paper, we present c-ICL, a novel few-shot technique that leverages both correct and incorrect sample constructions to create in-context learning demonstrations. This approach enhances the ability of LLMs to extract entities and relations by utilizing prompts that incorporate not only the positive samples but also the reasoning behind them. This method allows for the identification and correction of potential interface errors. Specifically, our proposed method taps into the inherent contextual information and valuable information in hard negative samples and the nearest positive neighbors to the test and then applies the in-context learning demonstrations based on LLMs. Our experiments on various datasets indicate that c-ICL outperforms previous few-shot in-context learning methods, delivering substantial enhancements in performance across a broad spectrum of related tasks. These improvements are noteworthy, showcasing the versatility of our approach in miscellaneous scenarios.

------------

`[2406.11629] Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better! <https://arxiv.org/abs/2406.11629>`__ 多镜头语境学习能帮助长语境LLM评判吗?看得多，判断得更好!

::

    replaced with revised version Mon, 24 Jun 2024 16:02:21 GMT
    Submission history From: Mingyang Song [view email]
    [v1] Mon, 17 Jun 2024 15:11:58 UTC (221 KB)
    [v2] Mon, 24 Jun 2024 16:02:21 UTC (235 KB)
    [v3] Sun, 30 Jun 2024 13:31:24 UTC (221 KB)
    Mingyang Song, Mao Zheng, Xuan Luo

Leveraging Large Language Models (LLMs) as judges for judging the performance of LLMs has recently garnered attention. However, this type of approach is affected by the potential biases in LLMs, raising concerns about the reliability of the evaluation results. To mitigate this issue, we propose and study two versions of many-shot in-context prompts, which rely on two existing settings of many-shot ICL for helping GPT-4o-as-a-Judge in single answer grading to mitigate the potential biases in LLMs, Reinforced ICL and Unsupervised ICL. Concretely, the former utilizes in-context examples with model-generated rationales, and the latter without. Based on the designed prompts, we investigate the impact of scaling the number of in-context examples on the consistency and quality of the judgment results. Furthermore, we reveal the symbol bias hidden in the pairwise comparison of GPT-4o-as-a-Judge and propose a simple yet effective approach to mitigate it. Experimental results show that advanced long-context LLMs, such as GPT-4o, perform better in the many-shot regime than in the zero-shot regime. Meanwhile, the experimental results further verify the effectiveness of the symbol bias mitigation approach.

------------

`[2312.02614] Prompt Optimization via Adversarial In-Context Learning <https://arxiv.org/abs/2312.02614>`__ 基于对抗上下文学习的提示优化

::

    replaced with revised version Sat, 22 Jun 2024 15:19:11 GMT
    Submission history From: Long Do Xuan [view email]
    [v1] Tue, 5 Dec 2023 09:44:45 UTC (1,923 KB)
    [v2] Wed, 28 Feb 2024 04:42:46 UTC (2,243 KB)
    [v3] Sat, 22 Jun 2024 15:19:11 UTC (2,245 KB)
    Xuan Long Do, Yiran Zhao, Hannah Brown, Yuxi Xie, James Xu Zhao, Nancy F. Chen, Kenji Kawaguchi, Michael Shieh, Junxian He

We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two-player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on 11 generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, because our method uses pre-trained models and updates only prompts rather than model parameters, it is computationally efficient, easy to extend to any LLM and task, and effective in low-resource settings.

------------

--------------
Reasoning (15)
--------------

`[2406.15468] Reasoning or Simply Next Token Prediction? A Benchmark for Stress-Testing Large Language Models <https://arxiv.org/abs/2406.15468>`__ 推理还是简单的下一个Token预测?大型语言模型压力测试的基准

::

    Sat, 15 Jun 2024 05:35:47 GMT
    Wentian Wang, Paul Kantor, Jacob Feldman, Lazaros Gallos, Hao Wang

We propose MMLU-SR, a novel dataset designed to measure the true comprehension abilities of Large Language Models (LLMs) by challenging their performance in question-answering tasks with modified terms. We reasoned that an agent that ``truly'' understands a concept can still evaluate it when key terms are replaced by suitably defined alternate terms, and sought to differentiate such comprehension from mere text replacement. In our study, we modified standardized test questions by replacing a key term with a dummy word along with its definition. The key term could be in the context of questions, answers, or both questions and answers.
Notwithstanding the high scores achieved by recent popular LLMs on the MMLU leaderboard, we found a substantial reduction in model performance after such replacement, suggesting poor comprehension. This new benchmark provides a rigorous benchmark for testing true model comprehension, and poses a challenge to the broader scientific community.

------------

`[2406.15992] Can LLM Graph Reasoning Generalize beyond Pattern Memorization? <https://arxiv.org/abs/2406.15992>`__ LLM图推理能泛化到模式记忆之外吗?

::

    Sun, 23 Jun 2024 02:59:15 GMT
    Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xiaochuang Han, Tianxing He, Yulia Tsvetkov

Large language models (LLMs) demonstrate great potential for problems with implicit graphical structures, while recent works seek to enhance the graph reasoning capabilities of LLMs through specialized instruction tuning. The resulting 'graph LLMs' are evaluated with in-distribution settings only, thus it remains underexplored whether LLMs are learning generalizable graph reasoning skills or merely memorizing patterns in the synthetic training data.
To this end, we propose the NLGift benchmark, an evaluation suite of LLM graph reasoning generalization: whether LLMs could go beyond semantic, numeric, structural, reasoning patterns in the synthetic training data and improve utility on real-world graph-based tasks. Extensive experiments with two LLMs across four graph reasoning tasks demonstrate that while generalization on simple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to generalize across reasoning and real-world patterns, casting doubt on the benefit of synthetic graph tuning for real-world tasks with underlying network structures. We explore three strategies to improve LLM graph reasoning generalization, and we find that while post-training alignment is most promising for real-world tasks, empowering LLM graph reasoning to go beyond pattern memorization remains an open research question.

------------

`[2406.16490] eagerlearners at SemEval2024 Task 5: The Legal Argument Reasoning Task in Civil Procedure <https://arxiv.org/abs/2406.16490>`__ 参加SemEval2024任务5:民事诉讼中的法律论据推理任务

::

    Mon, 24 Jun 2024 09:57:44 GMT
    Hoorieh Sabzevari, Mohammadmostafa Rostamkhani, Sauleh Eetemadi

This study investigates the performance of the zero-shot method in classifying data using three large language models, alongside two models with large input token sizes and the two pre-trained models on legal data. Our main dataset comes from the domain of U.S. civil procedure. It includes summaries of legal cases, specific questions, potential answers, and detailed explanations for why each solution is relevant, all sourced from a book aimed at law students. By comparing different methods, we aimed to understand how effectively they handle the complexities found in legal datasets. Our findings show how well the zero-shot method of large language models can understand complicated data. We achieved our highest F1 score of 64% in these experiments.

------------

`[2406.16061] PORT: Preference Optimization on Reasoning Traces <https://arxiv.org/abs/2406.16061>`__ PORT:推理轨迹偏好优化

::

    Sun, 23 Jun 2024 09:51:06 GMT
    Salem Lahlou, Abdalgader Abubaker, Hakim Hacid

Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper proposes using preference optimization methods on Chain-of-Thought steps in order to improve the reasoning performances of language models. While the chosen answers are obtained from datasets that include reasoning traces, we propose two complementary schemes for generating rejected answers: digit corruption, and weak LLM prompting. Our approach leads to increased accuracy on the GSM8K, AQuA-RAT, and ARC benchmarks for Falcon2-11B and Mistral-7B. For example, the approach can lead to up to a relative 8.47% increase in accuracy on the GSM8K benchmark without any extra annotations. This work suggests that spending resources on creating more datasets of reasoning traces would further boost LLM performances on informal reasoning tasks.

------------

`[2406.15859] LLM-Powered Explanations: Unraveling Recommendations Through Subgraph Reasoning <https://arxiv.org/abs/2406.15859>`__ llm驱动的解释:通过子图推理解开建议

::

    Sat, 22 Jun 2024 14:14:03 GMT
    Guangsi Shi, Xiaofeng Deng, Linhao Luo, Lijuan Xia, Lei Bao, Bei Ye, Fei Du, Shirui Pan, Yuxiao Li

Recommender systems are pivotal in enhancing user experiences across various web applications by analyzing the complicated relationships between users and items. Knowledge graphs(KGs) have been widely used to enhance the performance of recommender systems. However, KGs are known to be noisy and incomplete, which are hard to provide reliable explanations for recommendation results. An explainable recommender system is crucial for the product development and subsequent decision-making. To address these challenges, we introduce a novel recommender that synergies Large Language Models (LLMs) and KGs to enhance the recommendation and provide interpretable results. Specifically, we first harness the power of LLMs to augment KG reconstruction. LLMs comprehend and decompose user reviews into new triples that are added into KG. In this way, we can enrich KGs with explainable paths that express user preferences. To enhance the recommendation on augmented KGs, we introduce a novel subgraph reasoning module that effectively measures the importance of nodes and discovers reasoning for recommendation. Finally, these reasoning paths are fed into the LLMs to generate interpretable explanations of the recommendation results. Our approach significantly enhances both the effectiveness and interpretability of recommender systems, especially in cross-selling scenarios where traditional methods falter. The effectiveness of our approach has been rigorously tested on four open real-world datasets, with our methods demonstrating a superior performance over contemporary state-of-the-art techniques by an average improvement of 12%. The application of our model in a multinational engineering and technology company cross-selling recommendation system further underscores its practical utility and potential to redefine recommendation practices through improved accuracy and user trust.

------------

`[2406.05673] Flow of Reasoning: Efficient Training of LLM Policy with Divergent Thinking <https://arxiv.org/abs/2406.05673>`__ 推理流程:发散性思维的LLM政策高效训练

::

    replaced with revised version Mon, 24 Jun 2024 15:49:09 GMT
    Submission history From: Fangxu Yu [view email]
    [v1] Sun, 9 Jun 2024 07:06:58 UTC (313 KB)
    [v2] Mon, 24 Jun 2024 15:49:09 UTC (313 KB)
    Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, Lianhui Qin

Divergent thinking, the cognitive process of generating diverse solutions, is a hallmark of human creativity and problem-solving. For machines, sampling diverse solution trajectories in complex reasoning problems is crucial for robust outcomes, data augmentation, and enhanced model generalization. Large language models (LLMs) often struggle with generating high-quality, diverse reasoning. While supervised fine-tuning helps with quality, it requires extensive supervision data to capture the full diversity of solutions. Alternatively, reinforcement learning methods like PPO aim to find limited highest-reward solutions while neglecting the solution diversity, akin to convergent thinking. To address these limitations, we propose Flow of Reasoning (FoR) -- an efficient LLM training approach enabling diverse reasoning with minimal data. FoR formulates multi-step LLM reasoning as a Markovian flow from an initial state to terminal states. The formulation allows to adapt principled GFlowNet approaches to train the LLM as a policy, which is able to sample multiple reasoning paths with probabilities proportional to the unnormalized reward. Empirical results show that, with limited training data (e.g., 15 examples), FoR can discover diverse high-quality solutions that excel greatly beyond current state-of-the-art methods across three tasks, including embodied reasoning (BlocksWorld), math puzzle solving (Game24), and logical reasoning (PrOntoQA). Code is available at this https URL.

------------

`[2406.14283] Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning <https://arxiv.org/abs/2406.14283>`__ Q*:通过审慎规划改进llm的多步骤推理

::

    replaced with revised version Mon, 24 Jun 2024 07:50:56 GMT
    Submission history From: Chaojie Wang [view email]
    [v1] Thu, 20 Jun 2024 13:08:09 UTC (512 KB)
    [v2] Mon, 24 Jun 2024 07:50:56 UTC (513 KB)
    [v3] Thu, 27 Jun 2024 09:44:45 UTC (513 KB)
    Chaojie Wang, Yanchen Deng, Zhiyi Lv, Shuicheng Yan, An Bo

Large Language Models (LLMs) have demonstrated impressive capability in many natural language tasks. However, the auto-regressive generation process makes LLMs prone to produce errors, hallucinations and inconsistent statements when performing multi-step reasoning. In this paper, by casting multi-step reasoning of LLMs as a heuristic search problem, we aim to alleviate the pathology by introducing Q*, a general, versatile and agile framework for guiding LLMs decoding process with deliberative planning. By learning a plug-and-play Q-value model as heuristic function for estimating expected future rewards, our Q* can effectively guide LLMs to select the most promising next reasoning step without fine-tuning LLMs for the current task, which avoids the significant computational overhead and potential risk of performance degeneration on other tasks. Extensive experiments on GSM8K, MATH and MBPP demonstrate the superiority of our method, contributing to improving the reasoning performance of existing open-source LLMs.

------------

`[2309.13007] ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs <https://arxiv.org/abs/2309.13007>`__ 调和:圆桌会议通过在不同的llm之间达成共识来改进推理

::

    replaced with revised version Fri, 21 Jun 2024 19:34:27 GMT
    Submission history From: Swarnadeep Saha [view email]
    [v1] Fri, 22 Sep 2023 17:12:45 UTC (1,058 KB)
    [v2] Wed, 21 Feb 2024 23:07:11 UTC (8,057 KB)
    [v3] Fri, 21 Jun 2024 19:34:27 UTC (8,058 KB)
    Justin Chih-Yao Chen, Swarnadeep Saha, Mohit Bansal

Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents. ReConcile enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that ReConcile significantly improves LLMs' reasoning -- both individually and as a team -- surpassing prior single-agent and multi-agent baselines by up to 11.4% and even outperforming GPT-4 on three datasets. ReConcile also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8% improvement on MATH. Finally, we analyze the individual components of ReConcile, demonstrating that the diversity originating from different models is critical to its superior performance. Code: this https URL

------------

`[2311.09762] Graph Elicitation for Guiding Multi-Step Reasoning in Large Language Models <https://arxiv.org/abs/2311.09762>`__ 用于指导大型语言模型多步骤推理的图启发

::

    replaced with revised version Sat, 22 Jun 2024 18:04:33 GMT
    Submission history From: Jinyoung Park [view email]
    [v1] Thu, 16 Nov 2023 10:36:08 UTC (9,014 KB)
    [v2] Sat, 22 Jun 2024 18:04:33 UTC (9,649 KB)
    Jinyoung Park, Ameen Patel, Omar Zia Khan, Hyunwoo J. Kim, Joo-Kyung Kim

Chain-of-Thought (CoT) prompting along with sub-question generation and answering has enhanced multi-step reasoning capabilities of Large Language Models (LLMs). However, prompting the LLMs to directly generate sub-questions is suboptimal since they sometimes generate redundant or irrelevant questions. To deal with them, we propose a GE-Reasoning method, which directs LLMs to generate proper sub-questions and corresponding answers. Concretely, given an input question, we first prompt the LLM to generate knowledge triplets, forming a graph representation of the question. Unlike conventional knowledge triplets, our approach allows variables as head or tail entities, effectively representing a question as knowledge triplets. Second, for each triplet, the LLM generates a corresponding sub-question and answer along with using knowledge retrieval. If the prediction confidence exceeds a threshold, the sub-question and prediction are incorporated into the prompt for subsequent processing. This approach encourages that sub-questions are grounded in the extracted knowledge triplets, reducing redundancy and irrelevance. Our experiments demonstrate that our approach outperforms previous CoT prompting methods and their variants on multi-hop question answering benchmark datasets.

------------

`[2401.04925] The Impact of Reasoning Step Length on Large Language Models <https://arxiv.org/abs/2401.04925>`__ 推理步长对大型语言模型的影响

::

    replaced with revised version Sat, 22 Jun 2024 08:18:48 GMT
    Submission history From: Mingyu Jin [view email]
    [v1] Wed, 10 Jan 2024 04:37:38 UTC (411 KB)
    [v2] Tue, 16 Jan 2024 17:40:14 UTC (411 KB)
    [v3] Sat, 20 Jan 2024 17:23:31 UTC (411 KB)
    [v4] Sat, 22 Jun 2024 08:18:48 UTC (14,181 KB)
    Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du

Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences. The code is available at this https URL

------------

`[2403.14932] Extending Token Computation for LLM Reasoning <https://arxiv.org/abs/2403.14932>`__ 扩展标记计算用于LLM推理

::

    replaced with revised version Sun, 23 Jun 2024 15:50:48 GMT
    Submission history From: Bingli Liao [view email]
    [v1] Fri, 22 Mar 2024 03:23:58 UTC (1,083 KB)
    [v2] Fri, 5 Apr 2024 10:15:09 UTC (1,083 KB)
    [v3] Sun, 23 Jun 2024 15:50:48 UTC (1,400 KB)
    Bingli Liao, Danilo Vasconcellos Vargas

Large Language Models (LLMs) are pivotal in advancing natural language processing but often struggle with complex reasoning tasks due to inefficient attention distributions. In this paper, we explore the effect of increased computed tokens on LLM performance and introduce a novel method for extending computed tokens in the Chain-of-Thought (CoT) process, utilizing attention mechanism optimization. By fine-tuning an LLM on a domain-specific, highly structured dataset, we analyze attention patterns across layers, identifying inefficiencies caused by non-semantic tokens with outlier high attention scores. To address this, we propose an algorithm that emulates early layer attention patterns across downstream layers to re-balance skewed attention distributions and enhance knowledge abstraction. Our findings demonstrate that our approach not only facilitates a deeper understanding of the internal dynamics of LLMs but also significantly improves their reasoning capabilities, particularly in non-STEM domains. Our study lays the groundwork for further innovations in LLM design, aiming to create more powerful, versatile, and responsible models capable of tackling a broad range of real-world applications.

------------

`[2404.12728] Relevant or Random: Can LLMs Truly Perform Analogical Reasoning? <https://arxiv.org/abs/2404.12728>`__ 相关还是随机:llm能真正进行类比推理吗?

::

    replaced with revised version Sun, 23 Jun 2024 05:18:50 GMT
    Submission history From: Chengwei Qin [view email]
    [v1] Fri, 19 Apr 2024 09:15:07 UTC (8,689 KB)
    [v2] Sun, 23 Jun 2024 05:18:50 UTC (9,147 KB)
    Chengwei Qin, Wenhan Xia, Tan Wang, Fangkai Jiao, Yuchen Hu, Bosheng Ding, Ruirui Chen, Shafiq Joty

Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks. Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts. However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones? In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks. With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance, e.g., 4% performance boost on GSM8K with random biological examples. We find that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs. Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts.

------------

`[2406.07393] Limited Out-of-Context Knowledge Reasoning in Large Language Models <https://arxiv.org/abs/2406.07393>`__ 大型语言模型有限的断章取义知识推理

::

    replaced with revised version Mon, 24 Jun 2024 14:59:54 GMT
    Submission history From: Peng Hu [view email]
    [v1] Tue, 11 Jun 2024 15:58:59 UTC (137 KB)
    [v2] Mon, 24 Jun 2024 14:59:54 UTC (73 KB)
    Peng Hu, Changjiang Gao, Ruiqi Gao, Jiajun Chen, and Shujian Huang

Large Language Models (LLMs) have demonstrated strong capabilities as knowledge bases and significant in-context reasoning capabilities. However, previous work challenges their out-of-context reasoning ability, i.e., the ability to infer information from their training data, instead of from the context or prompt. This paper focuses on a significant facet of out-of-context reasoning: Out-of-Context Knowledge Reasoning (OCKR), which is to combine multiple knowledge to infer new knowledge. We designed a synthetic dataset with seven representative OCKR tasks to systematically assess the OCKR capabilities of LLMs. Using this dataset, we evaluated the LLaMA2-13B-chat model and discovered that its proficiency in this aspect is limited, regardless of whether the knowledge is trained in a separate or adjacent training settings. Moreover, training the model to reason with complete reasoning data did not result in significant improvement. Training the model to perform explicit knowledge retrieval helps in only one of the tasks, indicating that the model's limited OCKR capabilities are due to difficulties in retrieving relevant knowledge. Furthermore, we treat cross-lingual knowledge transfer as a distinct form of OCKR, and evaluate this ability. Our results show that the evaluated model also exhibits limited ability in transferring knowledge across languages. The dataset used in this study is available at this https URL.

------------

`[2406.11012] Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game <https://arxiv.org/abs/2406.11012>`__ Connecting the Dots:使用New York Times Connections文字游戏评估llm的抽象推理能力

::

    replaced with revised version Sat, 22 Jun 2024 15:40:14 GMT
    Submission history From: Tuhin Chakrabarty Mr [view email]
    [v1] Sun, 16 Jun 2024 17:10:32 UTC (5,032 KB)
    [v2] Tue, 18 Jun 2024 15:02:28 UTC (5,032 KB)
    [v3] Sat, 22 Jun 2024 15:40:14 UTC (5,032 KB)
    [v4] Fri, 5 Jul 2024 05:18:00 UTC (5,034 KB)
    Prisha Samadarshi, Mariam Mustafa, Anushka Kulkarni, Raven Rothkopf, Tuhin Chakrabarty, Smaranda Muresan

The New York Times Connections game has emerged as a popular and challenging pursuit for word puzzle enthusiasts. We collect 200 Connections games to evaluate the performance of state-of-the-art large language models (LLMs) against expert and novice human players. Our results show that even the best-performing LLM, GPT-4o, which has otherwise shown impressive reasoning abilities on a wide variety of benchmarks, can only fully solve 8% of the games. Compared to GPT-4o, novice and expert players perform better, with expert human players significantly outperforming GPT-4o. To deepen our understanding we create a taxonomy of the knowledge types required to successfully categorize words in the Connections game, revealing that LLMs struggle with associative, encyclopedic, and linguistic knowledge. Our findings establish the New York Times Connections game as a challenging benchmark for evaluating abstract reasoning capabilities in humans and AI systems.

------------

`[2405.16265] MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time <https://arxiv.org/abs/2405.16265>`__ MindStar:在推理时增强预训练llm的数学推理

::

    replaced with revised version Fri, 21 Jun 2024 22:41:08 GMT
    Submission history From: Jikun Kang [view email]
    [v1] Sat, 25 May 2024 15:07:33 UTC (1,366 KB)
    [v2] Mon, 17 Jun 2024 13:37:39 UTC (1,540 KB)
    [v3] Fri, 21 Jun 2024 22:41:08 UTC (1,541 KB)
    [v4] Wed, 26 Jun 2024 14:01:15 UTC (1,550 KB)
    Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Boxing Chen, Dong Li, Feng Wen, Jianye Hao

Although Large Language Models (LLMs) achieve remarkable performance across various tasks, they often struggle with complex reasoning tasks, such as answering mathematical questions. Recent efforts to address this issue have primarily focused on leveraging mathematical datasets through supervised fine-tuning or self-improvement techniques. However, these methods often depend on high-quality datasets that are difficult to prepare, or they require substantial computational resources for fine-tuning. Inspired by findings that LLMs know how to produce the right answer but struggle to select the correct reasoning path, we propose a purely inference-based searching method -- MindStar (M*). This method formulates reasoning tasks as searching problems and proposes two search ideas to identify the optimal reasoning paths. We evaluate the M* framework on both the GSM8K and MATH datasets, comparing its performance with existing open and closed-source LLMs. Our results demonstrate that M* significantly enhances the reasoning abilities of open-source models, such as Llama-2-13B and Mistral-7B, and achieves comparable performance to GPT-3.5 and Grok-1, but with substantially reduced model size and computational costs.

------------

-----------
ToolUse (3)
-----------

`[2406.15474] WundtGPT: Shaping Large Language Models To Be An Empathetic, Proactive Psychologist <https://arxiv.org/abs/2406.15474>`__ WundtGPT:塑造大型语言模型，成为具有同理心的主动心理学家

::

    Sun, 16 Jun 2024 16:06:38 GMT
    Chenyu Ren, Yazhou Zhang, Daihai He, Jing Qin

Large language models (LLMs) are raging over the medical domain, and their momentum has carried over into the mental health domain, leading to the emergence of few mental health LLMs. Although such mental health LLMs could provide reasonable suggestions for psychological counseling, how to develop an authentic and effective doctor-patient relationship (DPR) through LLMs is still an important problem. To fill this gap, we dissect DPR into two key attributes, i.e., the psychologist's empathy and proactive guidance. We thus present WundtGPT, an empathetic and proactive mental health large language model that is acquired by fine-tuning it with instruction and real conversation between psychologists and patients. It is designed to assist psychologists in diagnosis and help patients who are reluctant to communicate face-to-face understand their psychological conditions. Its uniqueness lies in that it could not only pose purposeful questions to guide patients in detailing their symptoms but also offer warm emotional reassurance. In particular, WundtGPT incorporates Collection of Questions, Chain of Psychodiagnosis, and Empathy Constraints into a comprehensive prompt for eliciting LLMs' questions and diagnoses.
Additionally, WundtGPT proposes a reward model to promote alignment with empathetic mental health professionals, which encompasses two key factors: cognitive empathy and emotional empathy. We offer a comprehensive evaluation of our proposed model. Based on these outcomes, we further conduct the manual evaluation based on proactivity, effectiveness, professionalism and coherence.
We notice that WundtGPT can offer professional and effective consultation. The model is available at huggingface.

------------

`[2403.03031] Learning to Use Tools via Cooperative and Interactive Agents <https://arxiv.org/abs/2403.03031>`__ 通过合作和交互式代理学习使用工具

::

    replaced with revised version Sat, 22 Jun 2024 14:00:56 GMT
    Submission history From: Zhengliang Shi [view email]
    [v1] Tue, 5 Mar 2024 15:08:16 UTC (1,443 KB)
    [v2] Sun, 26 May 2024 11:49:56 UTC (1,446 KB)
    [v3] Fri, 31 May 2024 07:42:44 UTC (1,443 KB)
    [v4] Sat, 22 Jun 2024 14:00:56 UTC (620 KB)
    Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Pengjie Ren, Suzan Verberne, Zhaochun Ren

Tool learning empowers large language models (LLMs) as agents to use external tools and extend their utility. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating execution results into the next action prediction. Despite their progress, these methods suffer from performance degradation when addressing practical tasks due to: (1) the pre-defined pipeline with restricted flexibility to calibrate incorrect actions, and (2) the struggle to adapt a general LLM-based agent to perform a variety of specialized actions. To mitigate these problems, we propose ConAgents, a Cooperative and interactive Agents framework, which coordinates three specialized agents for tool selection, tool execution, and action calibration separately. ConAgents introduces two communication protocols to enable the flexible cooperation of agents. To effectively generalize the ConAgents into open-source models, we also propose specialized action distillation, enhancing their ability to perform specialized actions in our framework. Our extensive experiments on three datasets show that the LLMs, when equipped with the ConAgents, outperform baselines with substantial improvement (i.e., up to 14% higher success rate).

------------

`[2405.05955] Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning <https://arxiv.org/abs/2405.05955>`__ Smurfs:利用具有上下文效率的多熟练智能体进行工具规划

::

    replaced with revised version Mon, 24 Jun 2024 02:44:57 GMT
    Submission history From: Junzhi Chen [view email]
    [v1] Thu, 9 May 2024 17:49:04 UTC (3,003 KB)
    [v2] Wed, 19 Jun 2024 06:18:12 UTC (3,168 KB)
    [v3] Mon, 24 Jun 2024 02:44:57 UTC (3,168 KB)
    Junzhi Chen, Juhao Liang, Benyou Wang

The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance. Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly. This paper introduces `Smurfs', a cutting-edge multi-agent framework designed to revolutionize the application of LLMs. By seamlessly transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs can enhance the model's ability to solve complex tasks at no additional cost. This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents and forming an intelligent multi-agent system. Our empirical investigation on both open-ended task of StableToolBench and closed-ended task on HotpotQA showcases Smurfs' superior capability in intricate tool utilization scenarios. Notably, Smurfs outmatches all the baseline methods in both experiments, setting new state-of-the-art performance. Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy. This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems.

------------

------------------------
Retrieval-Augmented (12)
------------------------

`[2406.15625] Shortcomings of LLMs for Low-Resource Translation: Retrieval and Understanding are Both the Problem <https://arxiv.org/abs/2406.15625>`__ llm在低资源翻译方面的缺点:检索和理解都是问题

::

    Fri, 21 Jun 2024 20:02:22 GMT
    Sara Court and Micha Elsner

This work investigates the in-context learning abilities of pretrained large language models (LLMs) when instructed to translate text from a low-resource language into a high-resource language as part of an automated machine translation pipeline. We conduct a set of experiments translating Southern Quechua to Spanish and examine the informativity of various types of information retrieved from a constrained database of digitized pedagogical materials (dictionaries and grammar lessons) and parallel corpora. Using both automatic and human evaluation of model output, we conduct ablation studies that manipulate (1) context type (morpheme translations, grammar descriptions, and corpus examples), (2) retrieval methods (automated vs. manual), and (3) model type. Our results suggest that even relatively small LLMs are capable of utilizing prompt context for zero-shot low-resource translation when provided a minimally sufficient amount of relevant linguistic information. However, the variable effects of prompt type, retrieval method, model type, and language-specific factors highlight the limitations of using even the best LLMs as translation systems for the majority of the world's 7,000+ languages and their speakers.

------------

`[2406.16079] EERPD: Leveraging Emotion and Emotion Regulation for Improving Personality Detection <https://arxiv.org/abs/2406.16079>`__ EERPD:利用情绪和情绪调节改进人格检测

::

    Sun, 23 Jun 2024 11:18:55 GMT
    Zheng Li,Dawei Zhu,Qilong Ma,Weimin Xiong,Sujian Li

Personality is a fundamental construct in psychology, reflecting an individual's behavior, thinking, and emotional patterns. Previous researches have made some progress in personality detection, primarily by utilizing the whole text to predict personality. However, these studies generally tend to overlook psychological knowledge: they rarely apply the well-established correlations between emotion regulation and personality. Based on this, we propose a new personality detection method called EERPD. This method introduces the use of emotion regulation, a psychological concept highly correlated with personality, for personality prediction. By combining this feature with emotion features, it retrieves few-shot examples and provides process CoTs for inferring labels from text. This approach enhances the understanding of LLM for personality within text and improves the performance in personality detection.
Experimental results demonstrate that EERPD significantly enhances the accuracy and robustness of personality detection, outperforming previous SOTA by 15.05/4.29 in average F1 on the two benchmark datasets.

------------

`[2406.16167] FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models <https://arxiv.org/abs/2406.16167>`__ FS-RAG:一种基于框架语义提高大型语言模型事实准确性的方法

::

    Sun, 23 Jun 2024 17:18:19 GMT
    Harish Tayyar Madabushi

We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models.
Specifically, our method draws on the cognitive linguistic theory of frame semantics for the indexing and retrieval of factual information relevant to helping large language models answer queries. We conduct experiments to demonstrate the effectiveness of this method both in terms of retrieval effectiveness and in terms of the relevance of the frames and frame relations automatically generated. Our results show that this novel mechanism of Frame Semantic-based retrieval, designed to improve Retrieval Augmented Generation (FS-RAG), is effective and offers potential for providing data-driven insights into frame semantics theory. We provide open access to our program code and prompts.

------------

`[2406.16768] WARP: On the Benefits of Weight Averaged Rewarded Policies <https://arxiv.org/abs/2406.16768>`__ 扭曲:体重平均奖励政策的好处

::

    Mon, 24 Jun 2024 16:24:34 GMT
    Alexandre Ram\'e, Johan Ferret, Nino Vieillard, Robert Dadashi, L\'eonard Hussenot, Pierre-Louis Cedoz, Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, Olivier Bachem

Reinforcement learning from human feedback (RLHF) aligns large language models (LLMs) by encouraging their generations to have high rewards, using a reward model trained on human preferences. To prevent the forgetting of pre-trained knowledge, RLHF usually incorporates a KL regularization; this forces the policy to remain close to its supervised fine-tuned initialization, though it hinders the reward optimization. To tackle the trade-off between KL and reward, in this paper we introduce a novel alignment strategy named Weight Averaged Rewarded Policies (WARP). WARP merges policies in the weight space at three distinct stages. First, it uses the exponential moving average of the policy as a dynamic anchor in the KL regularization. Second, it applies spherical interpolation to merge independently fine-tuned policies into a new enhanced one. Third, it linearly interpolates between this merged model and the initialization, to recover features from pre-training. This procedure is then applied iteratively, with each iteration's final model used as an advanced initialization for the next, progressively refining the KL-reward Pareto front, achieving superior rewards at fixed KL. Experiments with GEMMA policies validate that WARP improves their quality and alignment, outperforming other open-source LLMs.

------------

`[2406.16828] Ragnar\"ok: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track <https://arxiv.org/abs/2406.16828>`__ Ragnar\"ok: TREC 2024检索增强生成轨迹的可重用RAG框架和基线

::

    Mon, 24 Jun 2024 17:37:52 GMT
    Ronak Pradeep, Nandan Thakur, Sahel Sharifymoghaddam, Eric Zhang, Ryan Nguyen, Daniel Campos, Nick Craswell, Jimmy Lin

Did you try out the new Bing Search? Or maybe you fiddled around with Google AI~Overviews? These might sound familiar because the modern-day search stack has recently evolved to include retrieval-augmented generation (RAG) systems.
They allow searching and incorporating real-time data into large language models (LLMs) to provide a well-informed, attributed, concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents. Therefore, given these recent advancements, it is crucial to have an arena to build, test, visualize, and systematically evaluate RAG-based search systems. With this in mind, we propose the TREC 2024 RAG Track to foster innovation in evaluating RAG systems. In our work, we lay out the steps we've made towards making this track a reality -- we describe the details of our reusable framework, Ragnar\"ok, explain the curation of the new MS MARCO V2.1 collection choice, release the development topics for the track, and standardize the I/O definitions which assist the end user. Next, using Ragnar\"ok, we identify and provide key industrial baselines such as OpenAI's GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface for an interactive arena allowing benchmarking pairwise RAG systems by crowdsourcing. We open-source our Ragnar\"ok framework and baselines to achieve a unified standard for future RAG systems.

------------

`[2406.16477] DaLPSR: Leverage Degradation-Aligned Language Prompt for Real-World Image Super-Resolution <https://arxiv.org/abs/2406.16477>`__ DaLPSR:利用退化对齐语言提示实现真实世界的图像超分辨率

::

    Mon, 24 Jun 2024 09:30:36 GMT
    Aiwen Jiang, Zhi Wei, Long Peng, Feiqiang Liu, Wenbo Li, Mingwen Wang

Image super-resolution pursuits reconstructing high-fidelity high-resolution counterpart for low-resolution image. In recent years, diffusion-based models have garnered significant attention due to their capabilities with rich prior knowledge. The success of diffusion models based on general text prompts has validated the effectiveness of textual control in the field of text2image.
However, given the severe degradation commonly presented in low-resolution images, coupled with the randomness characteristics of diffusion models, current models struggle to adequately discern semantic and degradation information within severely degraded images. This often leads to obstacles such as semantic loss, visual artifacts, and visual hallucinations, which pose substantial challenges for practical use. To address these challenges, this paper proposes to leverage degradation-aligned language prompt for accurate, fine-grained, and high-fidelity image restoration. Complementary priors including semantic content descriptions and degradation prompts are explored.
Specifically, on one hand, image-restoration prompt alignment decoder is proposed to automatically discern the degradation degree of LR images, thereby generating beneficial degradation priors for image restoration. On the other hand, much richly tailored descriptions from pretrained multimodal large language model elicit high-level semantic priors closely aligned with human perception, ensuring fidelity control for image restoration. Comprehensive comparisons with state-of-the-art methods have been done on several popular synthetic and real-world benchmark datasets. The quantitative and qualitative analysis have demonstrated that the proposed method achieves a new state-of-the-art perceptual quality level, especially in real-world cases based on reference-free metrics.

------------

`[2404.02261] LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages <https://arxiv.org/abs/2404.02261>`__ llm在循环中:利用大型语言模型注释进行低资源语言的主动学习

::

    replaced with revised version Sun, 23 Jun 2024 18:21:01 GMT
    Submission history From: Nataliia Kholodna [view email]
    [v1] Tue, 2 Apr 2024 19:34:22 UTC (357 KB)
    [v2] Sun, 23 Jun 2024 18:21:01 UTC (311 KB)
    Nataliia Kholodna, Sahib Julka, Mohammad Khodadadi, Muhammed Nurullah Gumus, Michael Granitzer

Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential cost savings of at least 42.45 times compared to human annotation. Our proposed solution shows promising potential to substantially reduce both the monetary and computational costs associated with automation in low-resource settings. By bridging the gap between low-resource languages and AI, this approach fosters broader inclusion and shows the potential to enable automation across diverse linguistic landscapes.

------------

`[2405.05955] Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning <https://arxiv.org/abs/2405.05955>`__ Smurfs:利用具有上下文效率的多熟练智能体进行工具规划

::

    replaced with revised version Mon, 24 Jun 2024 02:44:57 GMT
    Submission history From: Junzhi Chen [view email]
    [v1] Thu, 9 May 2024 17:49:04 UTC (3,003 KB)
    [v2] Wed, 19 Jun 2024 06:18:12 UTC (3,168 KB)
    [v3] Mon, 24 Jun 2024 02:44:57 UTC (3,168 KB)
    Junzhi Chen, Juhao Liang, Benyou Wang

The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance. Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly. This paper introduces `Smurfs', a cutting-edge multi-agent framework designed to revolutionize the application of LLMs. By seamlessly transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs can enhance the model's ability to solve complex tasks at no additional cost. This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents and forming an intelligent multi-agent system. Our empirical investigation on both open-ended task of StableToolBench and closed-ended task on HotpotQA showcases Smurfs' superior capability in intricate tool utilization scenarios. Notably, Smurfs outmatches all the baseline methods in both experiments, setting new state-of-the-art performance. Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy. This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems.

------------

`[2406.12566] RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation <https://arxiv.org/abs/2406.12566>`__ RichRAG:在检索增强生成中为多方面查询设计丰富的响应

::

    replaced with revised version Fri, 21 Jun 2024 18:12:15 GMT
    Submission history From: Shuting Wang [view email]
    [v1] Tue, 18 Jun 2024 12:52:51 UTC (588 KB)
    [v2] Fri, 21 Jun 2024 18:12:15 UTC (588 KB)
    Shuting Wang, Xin Yu, Mang Wang, Weipeng Chen, Yutao Zhu, Zhicheng Dou

Retrieval-augmented generation (RAG) effectively addresses issues of static knowledge and hallucination in large language models. Existing studies mostly focus on question scenarios with clear user intents and concise answers. However, it is prevalent that users issue broad, open-ended queries with diverse sub-intents, for which they desire rich and long-form answers covering multiple relevant aspects. To tackle this important yet underexplored problem, we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect explorer to identify potential sub-aspects of input questions, a multi-faceted retriever to build a candidate pool of diverse external documents related to these sub-aspects, and a generative list-wise ranker, which is a key module to provide the top-k most valuable documents for the final generator. These ranked documents sufficiently cover various query aspects and are aware of the generator's preferences, hence incentivizing it to produce rich and comprehensive responses for users. The training of our ranker involves a supervised fine-tuning stage to ensure the basic coverage of documents, and a reinforcement learning stage to align downstream LLM's preferences to the ranking of documents. Experimental results on two publicly available datasets prove that our framework effectively and efficiently provides comprehensive and satisfying responses to users.

------------

`[2406.14745] Relation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks <https://arxiv.org/abs/2406.14745>`__ 检索增强生成框架中基于微调大型语言模型的关系抽取

::

    replaced with revised version Mon, 24 Jun 2024 06:57:05 GMT
    Submission history From: Sefika Efeoglu [view email]
    [v1] Thu, 20 Jun 2024 21:27:57 UTC (2,271 KB)
    [v2] Mon, 24 Jun 2024 06:57:05 UTC (1,885 KB)
    Sefika Efeoglu and Adrian Paschke

Information Extraction (IE) is crucial for converting unstructured data into structured formats like Knowledge Graphs (KGs). A key task within IE is Relation Extraction (RE), which identifies relationships between entities in text. Various RE methods exist, including supervised, unsupervised, weakly supervised, and rule-based approaches. Recent studies leveraging pre-trained language models (PLMs) have shown significant success in this area. In the current era dominated by Large Language Models (LLMs), fine-tuning these models can overcome limitations associated with zero-shot LLM prompting-based RE methods, especially regarding domain adaptation challenges and identifying implicit relations between entities in sentences. These implicit relations, which cannot be easily extracted from a sentence's dependency tree, require logical inference for accurate identification. This work explores the performance of fine-tuned LLMs and their integration into the Retrieval Augmented-based (RAG) RE approach to address the challenges of identifying implicit relations at the sentence level, particularly when LLMs act as generators within the RAG framework. Empirical evaluations on the TACRED, TACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant performance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B, and T5 (Large). Notably, our approach achieves substantial gains on SemEVAL, where implicit relations are common, surpassing previous results on this dataset. Additionally, our method outperforms previous works on TACRED, TACREV, and Re-TACRED, demonstrating exceptional performance across diverse evaluation scenarios.

------------

`[2308.11131] ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation <https://arxiv.org/abs/2308.11131>`__ ReLLa:推荐中用于终身序列行为理解的检索增强大型语言模型

::

    replaced with revised version Mon, 24 Jun 2024 02:44:28 GMT
    Submission history From: Jianghao Lin [view email]
    [v1] Tue, 22 Aug 2023 02:25:04 UTC (189 KB)
    [v2] Fri, 13 Oct 2023 16:13:52 UTC (841 KB)
    [v3] Mon, 19 Feb 2024 01:02:39 UTC (850 KB)
    [v4] Thu, 29 Feb 2024 05:55:30 UTC (850 KB)
    [v5] Mon, 24 Jun 2024 02:44:28 UTC (850 KB)
    [v6] Wed, 26 Jun 2024 08:55:55 UTC (851 KB)
    Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, Weinan Zhang

With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on three real-world public datasets to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension. To be highlighted, with only less than 10% training samples, few-shot ReLLa can outperform traditional CTR models that are trained on the entire training set (e.g., DCNv2, DIN, SIM). The code is available \url{this https URL}.

------------

`[2406.07796] Battling Botpoop using GenAI for Higher Education: A Study of a Retrieval Augmented Generation Chatbots Impact on Learning <https://arxiv.org/abs/2406.07796>`__ 

::

    replaced with revised version Sat, 22 Jun 2024 01:02:54 GMT
    Submission history From: Leonard Ng [view email]
    [v1] Wed, 12 Jun 2024 01:19:36 UTC (14,209 KB)
    [v2] Sat, 22 Jun 2024 01:02:54 UTC (1,122 KB)
    Maung Thway, Jose Recatala-Gomez, Fun Siong Lim, Kedar Hippalgaonkar, Leonard W. T. Ng

Generative artificial intelligence (GenAI) and large language models (LLMs) have simultaneously opened new avenues for enhancing human learning and increased the prevalence of poor-quality information in student response - termed Botpoop. This study introduces Professor Leodar, a custom-built, Singlish-speaking Retrieval Augmented Generation (RAG) chatbot designed to enhance educational while reducing Botpoop. Deployed at Nanyang Technological University, Singapore, Professor Leodar offers a glimpse into the future of AI-assisted learning, offering personalized guidance, 24/7 availability, and contextually relevant information. Through a mixed-methods approach, we examine the impact of Professor Leodar on learning, engagement, and exam preparedness, with 97.1% of participants reporting positive experiences. These findings help define possible roles of AI in education and highlight the potential of custom GenAI chatbots. Our combination of chatbot development, in-class deployment and outcomes study offers a benchmark for GenAI educational tools and is a stepping stone for redefining the interplay between AI and human learning.

------------

---------
Agent (7)
---------

`[2406.15363] Exploring LLM Multi-Agents for ICD Coding <https://arxiv.org/abs/2406.15363>`__ 

::

    Mon, 1 Apr 2024 15:17:39 GMT
    Rumeng Li, Xun Wang, Hong Yu

Large Language Models (LLMs) have demonstrated impressive and diverse abilities that can benefit various domains, such as zero and few-shot information extraction from clinical text without domain-specific training.
However, for the ICD coding task, they often hallucinate key details and produce high recall but low precision results due to the high-dimensional and skewed distribution of the ICD codes. Existing LLM-based methods fail to account for the complex and dynamic interactions among the human agents involved in coding, such as patients, physicians, and coders, and they lack interpretability and reliability. In this paper, we present a novel multi-agent method for ICD coding, which mimics the real-world coding process with five agents: a patient agent, a physician agent, a coder agent, a reviewer agent, and an adjuster agent. Each agent has a specific function and uses a LLM-based model to perform it. We evaluate our method on the MIMIC-III dataset and show that our proposed multi-agent coding framework substantially improves performance on both common and rare codes compared to Zero-shot Chain of Thought (CoT) prompting and self-consistency with CoT. The ablation study confirms the proposed agent roles' efficacy. Our method also matches the state-of-the-art ICD coding methods that require pre-training or fine-tuning, in terms of coding accuracy, rare code accuracy, and explainability.

------------

`[2406.15492] On the Principles behind Opinion Dynamics in Multi-Agent Systems of Large Language Models <https://arxiv.org/abs/2406.15492>`__ 

::

    Tue, 18 Jun 2024 18:37:23 GMT
    Pedro Cisneros-Velarde

We study the evolution of opinions inside a population of interacting large language models (LLMs). Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding. We identify biases that drive the exchange of opinions based on the LLM's tendency to (i) find consensus with the other LLM's opinion, (ii) display caution when specifying funding, and (iii) consider ethical concerns in its opinion. We find these biases are affected by the perceived absence of compelling reasons for opinion change, the perceived willingness to engage in discussion, and the distribution of allocation values. Moreover, tensions among biases can lead to the survival of funding for items with negative connotations. We also find that the final distribution of full, partial, and no funding opinions is more diverse when an LLM freely forms its opinion after an interaction than when its opinion is a multiple-choice selection among the three allocation options. In the latter case, consensus or polarization is generally attained. When agents are aware of past opinions, they seek to maintain consistency with them, and more diverse updating rules emerge. Our study is performed using a Llama 3 LLM.

------------

`[2401.07115] Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models <https://arxiv.org/abs/2401.07115>`__ 开放的模式，封闭的思想?论智能体通过开放的大型语言模型模仿人类性格的能力

::

    replaced with revised version Sun, 23 Jun 2024 19:53:33 GMT
    Submission history From: Andrea Tagarelli [view email]
    [v1] Sat, 13 Jan 2024 16:41:40 UTC (1,642 KB)
    [v2] Sun, 23 Jun 2024 19:53:33 UTC (4,607 KB)
    Lucio La Cava, Andrea Tagarelli

The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology. Scholars have been studying the inherent personalities exhibited by LLMs and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by employing a set of 12 LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five Inventory (BFI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); and $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs.

------------

`[2403.17209] Generation of Asset Administration Shell with Large Language Model Agents: Toward Semantic Interoperability in Digital Twins in the Context of Industry 4.0 <https://arxiv.org/abs/2403.17209>`__ 具有大型语言模型代理的资产管理外壳的生成:工业4.0背景下面向数字孪生的语义互操作性

::

    replaced with revised version Mon, 24 Jun 2024 12:04:06 GMT
    Submission history From: Yuchen Xia [view email]
    [v1] Mon, 25 Mar 2024 21:37:30 UTC (1,372 KB)
    [v2] Tue, 28 May 2024 00:00:38 UTC (1,420 KB)
    [v3] Wed, 19 Jun 2024 10:32:21 UTC (1,444 KB)
    [v4] Mon, 24 Jun 2024 12:04:06 UTC (2,895 KB)
    Yuchen Xia, Zhewen Xiao, Nasser Jazdi and Michael Weyrich

This research introduces a novel approach for achieving semantic interoperability in digital twins and assisting the creation of Asset Administration Shell (AAS) as digital twin model within the context of Industry 4.0. The foundational idea of our research is that the communication based on semantics and the generation of meaningful textual data are directly linked, and we posit that these processes are equivalent if the exchanged information can be serialized in text form. Based on this, we construct a "semantic node" data structure in our research to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process the "semantic node" and generate standardized digital twin models from raw textual data collected from datasheets describing technical assets. Our evaluation demonstrates an effective generation rate of 62-79%, indicating a substantial proportion of the information from the source text can be translated error-free to the target digital twin instance model with the generative capability of large language models. This result has a direct application in the context of Industry 4.0, and the designed system is implemented as a data model generation tool for reducing the manual effort in creating AAS model. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM systems for interpreting technical concepts and translating data. Our findings emphasize LLMs' capability to automate AAS instance creation and contribute to the broader field of semantic interoperability for digital twins in industrial applications. The prototype implementation and evaluation results are presented on our GitHub Repository: this https URL.

------------

`[2402.01622] TravelPlanner: A Benchmark for Real-World Planning with Language Agents <https://arxiv.org/abs/2402.01622>`__ TravelPlanner:使用语言代理进行现实世界规划的基准

::

    replaced with revised version Sun, 23 Jun 2024 08:50:17 GMT
    Submission history From: Jian Xie [view email]
    [v1] Fri, 2 Feb 2024 18:39:51 UTC (2,873 KB)
    [v2] Mon, 5 Feb 2024 06:48:01 UTC (2,866 KB)
    [v3] Sun, 23 Jun 2024 08:50:17 UTC (2,865 KB)
    Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su

Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.

------------

`[2403.03031] Learning to Use Tools via Cooperative and Interactive Agents <https://arxiv.org/abs/2403.03031>`__ 通过合作和交互式代理学习使用工具

::

    replaced with revised version Sat, 22 Jun 2024 14:00:56 GMT
    Submission history From: Zhengliang Shi [view email]
    [v1] Tue, 5 Mar 2024 15:08:16 UTC (1,443 KB)
    [v2] Sun, 26 May 2024 11:49:56 UTC (1,446 KB)
    [v3] Fri, 31 May 2024 07:42:44 UTC (1,443 KB)
    [v4] Sat, 22 Jun 2024 14:00:56 UTC (620 KB)
    Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Pengjie Ren, Suzan Verberne, Zhaochun Ren

Tool learning empowers large language models (LLMs) as agents to use external tools and extend their utility. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating execution results into the next action prediction. Despite their progress, these methods suffer from performance degradation when addressing practical tasks due to: (1) the pre-defined pipeline with restricted flexibility to calibrate incorrect actions, and (2) the struggle to adapt a general LLM-based agent to perform a variety of specialized actions. To mitigate these problems, we propose ConAgents, a Cooperative and interactive Agents framework, which coordinates three specialized agents for tool selection, tool execution, and action calibration separately. ConAgents introduces two communication protocols to enable the flexible cooperation of agents. To effectively generalize the ConAgents into open-source models, we also propose specialized action distillation, enhancing their ability to perform specialized actions in our framework. Our extensive experiments on three datasets show that the LLMs, when equipped with the ConAgents, outperform baselines with substantial improvement (i.e., up to 14% higher success rate).

------------

`[2405.05955] Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning <https://arxiv.org/abs/2405.05955>`__ Smurfs:利用具有上下文效率的多熟练智能体进行工具规划

::

    replaced with revised version Mon, 24 Jun 2024 02:44:57 GMT
    Submission history From: Junzhi Chen [view email]
    [v1] Thu, 9 May 2024 17:49:04 UTC (3,003 KB)
    [v2] Wed, 19 Jun 2024 06:18:12 UTC (3,168 KB)
    [v3] Mon, 24 Jun 2024 02:44:57 UTC (3,168 KB)
    Junzhi Chen, Juhao Liang, Benyou Wang

The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance. Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly. This paper introduces `Smurfs', a cutting-edge multi-agent framework designed to revolutionize the application of LLMs. By seamlessly transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs can enhance the model's ability to solve complex tasks at no additional cost. This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents and forming an intelligent multi-agent system. Our empirical investigation on both open-ended task of StableToolBench and closed-ended task on HotpotQA showcases Smurfs' superior capability in intricate tool utilization scenarios. Notably, Smurfs outmatches all the baseline methods in both experiments, setting new state-of-the-art performance. Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy. This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems.

------------

-----------
Other (156)
-----------

`[2406.15481] CSRT: Evaluation and Analysis of LLMs using Code-Switching Red-Teaming Dataset <https://arxiv.org/abs/2406.15481>`__ CSRT:基于代码切换Red-Teaming数据集的LLMs评估与分析

::

    Mon, 17 Jun 2024 06:08:18 GMT
    Haneul Yoo, Yongjin Yang, Hwaran Lee

Recent studies in large language models (LLMs) shed light on their multilingual ability and safety, beyond conventional tasks in language modeling. Still, current benchmarks reveal their inability to comprehensively evaluate them and are excessively dependent on manual annotations. In this paper, we introduce code-switching red-teaming (CSRT), a simple yet effective red-teaming technique that simultaneously tests multilingual understanding and safety of LLMs. We release the CSRT dataset, which comprises 315 code-switching queries combining up to 10 languages and eliciting a wide range of undesirable behaviors. Through extensive experiments with ten state-of-the-art LLMs, we demonstrate that CSRT significantly outperforms existing multilingual red-teaming techniques, achieving 46.7% more attacks than existing methods in English. We analyze the harmful responses toward the CSRT dataset concerning various aspects under ablation studies with 16K samples, including but not limited to scaling laws, unsafe behavior categories, and input conditions for optimal data generation. Additionally, we validate the extensibility of CSRT, by generating code-switching attack prompts with monolingual data.

------------

`[2406.15513] PKU-SafeRLHF: A Safety Alignment Preference Dataset for Llama Family Models <https://arxiv.org/abs/2406.15513>`__ PKU-SafeRLHF:羊驼家族模型安全对齐偏好数据集

::

    Thu, 20 Jun 2024 18:37:36 GMT
    Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Josef Dai, Boren Zheng, Tianyi Qiu, Boxun Li, Yaodong Yang

In this work, we introduce the PKU-SafeRLHF dataset, designed to promote research on safety alignment in large language models (LLMs). As a sibling project to SafeRLHF and BeaverTails, we separate annotations of helpfulness and harmlessness for question-answering pairs, providing distinct perspectives on these coupled attributes. Overall, we provide 44.6k refined prompts and 265k question-answer pairs with safety meta-labels for 19 harm categories and three severity levels ranging from minor to severe, with answers generated by Llama-family models. Based on this, we collected 166.8k preference data, including dual-preference (helpfulness and harmlessness decoupled) and single-preference data (trade-off the helpfulness and harmlessness from scratch), respectively. Using the large-scale annotation data, we further train severity-sensitive moderation for the risk control of LLMs and safety-centric RLHF algorithms for the safety alignment of LLMs. We believe this dataset will be a valuable resource for the community, aiding in the safe deployment of LLMs.

------------

`[2406.16455] Guardrails for avoiding harmful medical product recommendations and off-label promotion in generative AI models <https://arxiv.org/abs/2406.16455>`__ 生成式AI模型中避免有害医疗产品推荐和标签外推广的护栏

::

    Mon, 24 Jun 2024 08:50:26 GMT
    Daniel Lopez-Martinez

Generative AI (GenAI) models have demonstrated remarkable capabilities in a wide variety of medical tasks. However, as these models are trained using generalist datasets with very limited human oversight, they can learn uses of medical products that have not been adequately evaluated for safety and efficacy, nor approved by regulatory agencies. Given the scale at which GenAI may reach users, unvetted recommendations pose a public health risk. In this work, we propose an approach to identify potentially harmful product recommendations, and demonstrate it using a recent multimodal large language model.

------------

`[2406.16486] Towards Comprehensive Preference Data Collection for Reward Modeling <https://arxiv.org/abs/2406.16486>`__ 面向奖励模型的综合偏好数据收集

::

    Mon, 24 Jun 2024 09:40:39 GMT
    Yulan Hu, Qingyang Li, Sheng Ouyang, Ge Chen, Kaihui Chen, Lijun Mei, Xucheng Ye, Fuzheng Zhang, Yong Liu

Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models (LLMs) with human preferences, thereby enhancing the quality of responses generated. A critical component of RLHF is the reward model, which is trained on preference data and outputs a scalar reward during the inference stage. However, the collection of preference data still lacks thorough investigation. Recent studies indicate that preference data is collected either by AI or humans, where chosen and rejected instances are identified among pairwise responses. We question whether this process effectively filters out noise and ensures sufficient diversity in collected data. To address these concerns, for the first time, we propose a comprehensive framework for preference data collection, decomposing the process into four incremental steps: Prompt Generation, Response Generation, Response Filtering, and Human Labeling. This structured approach ensures the collection of high-quality preferences while reducing reliance on human labor. We conducted comprehensive experiments based on the data collected at different stages, demonstrating the effectiveness of the proposed data collection method.

------------

`[2406.15359] Constructing Multilingual Visual-Text Datasets Revealing Visual Multilingual Ability of Vision Language Models <https://arxiv.org/abs/2406.15359>`__ 揭示视觉语言模型视觉多语言能力的多语言视觉-文本数据集构建

::

    Fri, 29 Mar 2024 10:53:07 GMT
    Jesse Atuhurra, Iqra Ali, Tatsuya Hiraoka, Hidetaka Kamigaito, Tomoya Iwakura, Taro Watanabe

Large language models (LLMs) have increased interest in vision language models (VLMs), which process image-text pairs as input. Studies investigating the visual understanding ability of VLMs have been proposed, but such studies are still preliminary because existing datasets do not permit a comprehensive evaluation of the fine-grained visual linguistic abilities of VLMs across multiple languages. To further explore the strengths of VLMs, such as GPT-4V \cite{openai2023GPT4}, we developed new datasets for the systematic and qualitative analysis of VLMs. Our contribution is four-fold: 1) we introduced nine vision-and-language (VL) tasks (including object recognition, image-text matching, and more) and constructed multilingual visual-text datasets in four languages: English, Japanese, Swahili, and Urdu through utilizing templates containing \textit{questions} and prompting GPT4-V to generate the \textit{answers} and the \textit{rationales}, 2) introduced a new VL task named \textit{unrelatedness}, 3) introduced rationales to enable human understanding of the VLM reasoning process, and 4) employed human evaluation to measure the suitability of proposed datasets for VL tasks. We show that VLMs can be fine-tuned on our datasets. Our work is the first to conduct such analyses in Swahili and Urdu. Also, it introduces \textit{rationales} in VL analysis, which played a vital role in the evaluation.

------------

`[2406.15444] Investigating the Robustness of LLMs on Math Word Problems <https://arxiv.org/abs/2406.15444>`__ 研究llm在数学应用题上的鲁棒性

::

    Thu, 30 May 2024 18:07:13 GMT
    Ujjwala Anantheswaran and Himanshu Gupta and Kevin Scaria and Shreyas Verma and Chitta Baral and Swaroop Mishra

Large Language Models (LLMs) excel at various tasks, including solving math word problems (MWPs), but struggle with real-world problems containing irrelevant information. To address this, we propose a prompting framework that generates adversarial variants of MWPs by adding irrelevant variables. We introduce a dataset, ProbleMATHIC, containing both adversarial and non-adversarial MWPs. Our experiments reveal that LLMs are susceptible to distraction by numerical noise, resulting in an average relative performance drop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2, Mistral) on the adversarial samples from our dataset. Fine-tuning on adversarial training instances improves performance on adversarial MWPs by ~8%, indicating increased robustness to noise and better ability to identify relevant data for reasoning. Finally, to assess the generalizability of our prompting framework, we introduce GSM-8K-Adv, an adversarial variant of the GSM-8K benchmark. LLMs continue to struggle when faced with adversarial information, reducing performance by up to ~6%.

------------

`[2406.15465] RadEx: A Framework for Structured Information Extraction from Radiology Reports based on Large Language Models <https://arxiv.org/abs/2406.15465>`__ RadEx:基于大型语言模型的放射学报告结构化信息提取框架

::

    Fri, 14 Jun 2024 08:17:44 GMT
    Daniel Reichenpfader, Jonas Knupp, Andr\'e Sander, and Kerstin Denecke

Annually and globally, over three billion radiography examinations and computer tomography scans result in mostly unstructured radiology reports containing free text. Despite the potential benefits of structured reporting, its adoption is limited by factors such as established processes, resource constraints and potential loss of information. However, structured information would be necessary for various use cases, including automatic analysis, clinical trial matching, and prediction of health outcomes. This study introduces RadEx, an end-to-end framework comprising 15 software components and ten artifacts to develop systems that perform automated information extraction from radiology reports. It covers the complete process from annotating training data to extracting information by offering a consistent generic information model and setting boundaries for model development. Specifically, RadEx allows clinicians to define relevant information for clinical domains (e.g., mammography) and to create report templates. The framework supports both generative and encoder-only models and the decoupling of information extraction from template filling enables independent model improvements. Developing information extraction systems according to the RadEx framework facilitates implementation and maintenance as components are easily exchangeable, while standardized artifacts ensure interoperability between components.

------------

`[2406.15470] Mental Disorder Classification via Temporal Representation of Text <https://arxiv.org/abs/2406.15470>`__ 基于文本时间表示的精神障碍分类

::

    Sat, 15 Jun 2024 10:53:21 GMT
    Raja Kumar, Kishan Maharaj, Ashita Saxena, Pushpak Bhattacharyya

Mental disorders pose a global challenge, aggravated by the shortage of qualified mental health professionals. Mental disorder prediction from social media posts by current LLMs is challenging due to the complexities of sequential text data and the limited context length of language models. Current language model-based approaches split a single data instance into multiple chunks to compensate for limited context size. The predictive model is then applied to each chunk individually, and the most voted output is selected as the final prediction. This results in the loss of inter-post dependencies and important time variant information, leading to poor performance. We propose a novel framework which first compresses the large sequence of chronologically ordered social media posts into a series of numbers. We then use this time variant representation for mental disorder classification. We demonstrate the generalization capabilities of our framework by outperforming the current SOTA in three different mental conditions: depression, self-harm, and anorexia, with an absolute improvement of 5% in the F1 score. We investigate the situation where current data instances fall within the context length of language models and present empirical results highlighting the importance of temporal properties of textual data. Furthermore, we utilize the proposed framework for a cross-domain study, exploring commonalities across disorders and the possibility of inter-domain data usage.

------------

`[2406.15473] Intertwining CP and NLP: The Generation of Unreasonably Constrained Sentences <https://arxiv.org/abs/2406.15473>`__ 交织CP和NLP:不合理约束句子的生成

::

    Sat, 15 Jun 2024 17:40:49 GMT
    Alexandre Bonlarron, Jean-Charles R\'egin

Constrained text generation remains a challenging task, particularly when dealing with hard constraints. Traditional Natural Language Processing (NLP) approaches prioritize generating meaningful and coherent output. Also, the current state-of-the-art methods often lack the expressiveness and constraint satisfaction capabilities to handle such tasks effectively. This paper presents the Constraints First Framework to remedy this issue. This framework considers a constrained text generation problem as a discrete combinatorial optimization problem. It is solved by a constraint programming method that combines linguistic properties (e.g., n-grams or language level) with other more classical constraints (e.g., the number of characters, syllables, or words).
Eventually, a curation phase allows for selecting the best-generated sentences according to perplexity using a large language model. The effectiveness of this approach is demonstrated by tackling a new more tediously constrained text generation problem: the iconic RADNER sentences problem. This problem aims to generate sentences respecting a set of quite strict rules defined by their use in vision and clinical research. Thanks to our CP-based approach, many new strongly constrained sentences have been successfully generated in an automatic manner. This highlights the potential of our approach to handle unreasonably constrained text generation scenarios.

------------

`[2406.15477] CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics <https://arxiv.org/abs/2406.15477>`__ CrisisSense-LLM:面向灾难信息学多标签社交媒体文本分类的指令微调大型语言模型

::

    Sun, 16 Jun 2024 23:01:10 GMT
    Kai Yin, Chengkai Liu, Ali Mostafavi, Xia Hu

In the field of crisis/disaster informatics, social media is increasingly being used for improving situational awareness to inform response and relief efforts. Efficient and accurate text classification tools have been a focal area of investigation in crisis informatics. However, current methods mostly rely on single-label text classification models, which fails to capture different insights embedded in dynamic and multifaceted disaster-related social media data. This study introduces a novel approach to disaster text classification by enhancing a pre-trained Large Language Model (LLM) through instruction fine-tuning targeted for multi-label classification of disaster-related tweets. Our methodology involves creating a comprehensive instruction dataset from disaster-related tweets, which is then used to fine-tune an open-source LLM, thereby embedding it with disaster-specific knowledge. This fine-tuned model can classify multiple aspects of disaster-related information simultaneously, such as the type of event, informativeness, and involvement of human aid, significantly improving the utility of social media data for situational awareness in disasters. The results demonstrate that this approach enhances the categorization of critical information from social media posts, thereby facilitating a more effective deployment for situational awareness during emergencies. This research paves the way for more advanced, adaptable, and robust disaster management tools, leveraging the capabilities of LLMs to improve real-time situational awareness and response strategies in disaster scenarios.

------------

`[2406.15479] Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging <https://arxiv.org/abs/2406.15479>`__ 

::

    Mon, 17 Jun 2024 02:31:55 GMT
    Zhenyi Lu, Chenghao Fan, Wei Wei, Xiaoye Qu, Dangyang Chen, Yu Cheng

In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $12$ datasets for both discriminative and generative tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. (Our implementation is available in https://github.com/LZY-the-boys/Twin-Mergin.)

------------

`[2406.15480] On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion <https://arxiv.org/abs/2406.15480>`__ 在巨人的肩膀上:毫不费力的弱到强由动态Logits融合

::

    Mon, 17 Jun 2024 03:07:41 GMT
    Chenghao Fan, Zhenyi Lu, Wei Wei, Jie Tian, Xiaoye Qu, Dangyang Chen, Yu Cheng

Efficient fine-tuning of large language models for task-specific applications is imperative, yet the vast number of parameters in these models makes their training increasingly challenging. Despite numerous proposals for effective methods, a substantial memory overhead remains for gradient computations during updates. \thm{Can we fine-tune a series of task-specific small models and transfer their knowledge directly to a much larger model without additional training?} In this paper, we explore weak-to-strong specialization using logit arithmetic, facilitating a direct answer to this question. Existing weak-to-strong methods often employ a static knowledge transfer ratio and a single small model for transferring complex knowledge, which leads to suboptimal performance. % To address this, To surmount these limitations, we propose a dynamic logit fusion approach that works with a series of task-specific small models, each specialized in a different task. This method adaptively allocates weights among these models at each decoding step, learning the weights through Kullback-Leibler divergence constrained optimization problems. We conduct extensive experiments across various benchmarks in both single-task and multi-task settings, achieving leading results. By transferring expertise from the 7B model to the 13B model, our method closes the performance gap by 96.4\% in single-task scenarios and by 86.3\% in multi-task scenarios compared to full fine-tuning of the 13B model. Notably, we achieve surpassing performance on unseen tasks. Moreover, we further demonstrate that our method can effortlessly integrate in-context learning for single tasks and task arithmetic for multi-task scenarios. (Our implementation is available in https://github.com/Facico/Dynamic-Logit-Fusion.)

------------

`[2406.15483] Duplicate Detection with GenAI <https://arxiv.org/abs/2406.15483>`__ 使用GenAI进行重复检测

::

    Mon, 17 Jun 2024 06:42:13 GMT
    Ian Ormesher

Customer data is often stored as records in Customer Relations Management systems (CRMs). Data which is manually entered into such systems by one of more users over time leads to data replication, partial duplication or fuzzy duplication. This in turn means that there no longer a single source of truth for customers, contacts, accounts, etc. Downstream business processes become increasing complex and contrived without a unique mapping between a record in a CRM and the target customer. Current methods to detect and de-duplicate records use traditional Natural Language Processing techniques known as Entity Matching. In this paper we show how using the latest advancements in Large Language Models and Generative AI can vastly improve the identification and repair of duplicated records. On common benchmark datasets we find an improvement in the accuracy of data de-duplication rates from 30 percent using NLP techniques to almost 60 percent using our proposed method.

------------

`[2406.15486] Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention <https://arxiv.org/abs/2406.15486>`__ 基于自适应结构稀疏注意力的长上下文LLM推理近无损加速

::

    Mon, 17 Jun 2024 11:05:15 GMT
    Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Xiuhong Li, Guanyu Feng, Xin Lv, Huanqi Cao, Xiao Chuanfu, Xingcheng Zhang, Dahua Lin, Chao Yang

Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$ compared with FlashAttention.

------------

`[2406.15504] Dr.E Bridges Graphs with Large Language Models through Words <https://arxiv.org/abs/2406.15504>`__ Dr.E通过单词将图与大型语言模型联系起来

::

    Wed, 19 Jun 2024 16:43:56 GMT
    Zipeng Liu, Likang Wu, Ming He, Zhong Guan, Hongke Zhao, Nan Feng

Significant efforts have been directed toward integrating powerful Large Language Models (LLMs) with diverse modalities, particularly focusing on the fusion of vision, language, and audio data. However, the graph-structured data, inherently rich in structural and domain-specific knowledge, have not yet been gracefully adapted to LLMs. Existing methods either describe the graph with raw text, suffering the loss of graph structural information, or feed Graph Neural Network (GNN) embeddings directly into LLM at the cost of losing semantic representation. To bridge this gap, we introduce an innovative, end-to-end modality-aligning framework, equipped with a pretrained Dual-Residual Vector Quantized-Variational AutoEncoder (Dr.E). This framework is specifically designed to facilitate token-level alignment with LLMs, enabling an effective translation of the intrinsic `language' of graphs into comprehensible natural language. Our experimental evaluations on standard GNN node classification tasks demonstrate competitive performance against other state-of-the-art approaches. Additionally, our framework ensures interpretability, efficiency, and robustness, with its effectiveness further validated under both fine-tuning and few-shot settings. This study marks the first successful endeavor to achieve token-level alignment between GNNs and LLMs.

------------

`[2406.15524] Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization <https://arxiv.org/abs/2406.15524>`__ 重新思考修剪大型语言模型:重构误差最小化的好处和缺陷

::

    Fri, 21 Jun 2024 05:13:34 GMT
    Sungbin Shin, Wonpyo Park, Jaeho Lee, Namhoon Lee

This work suggests fundamentally rethinking the current practice of pruning large language models (LLMs). The way it is done is by divide and conquer: split the model into submodels, sequentially prune them, and reconstruct predictions of the dense counterparts on small calibration data one at a time; the final model is obtained simply by putting the resulting sparse submodels together. While this approach enables pruning under memory constraints, it generates high reconstruction errors. In this work, we first present an array of reconstruction techniques that can significantly reduce this error by more than $90\%$. Unwittingly, however, we discover that minimizing reconstruction error is not always ideal and can overfit the given calibration data, resulting in rather increased language perplexity and poor performance at downstream tasks. We find out that a strategy of self-generating calibration data can mitigate this trade-off between reconstruction and generalization, suggesting new directions in the presence of both benefits and pitfalls of reconstruction for pruning LLMs.

------------

`[2406.15583] Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods <https://arxiv.org/abs/2406.15583>`__ 

::

    Fri, 21 Jun 2024 18:31:49 GMT
    Kathleen C. Fraser, Hillary Dawkins, Svetlana Kiritchenko

Large language models (LLMs) have advanced to a point that even humans have difficulty discerning whether a text was generated by another human, or by a computer. However, knowing whether a text was produced by human or artificial intelligence (AI) is important to determining its trustworthiness, and has applications in many domains including detecting fraud and academic dishonesty, as well as combating the spread of misinformation and political propaganda. The task of AI-generated text (AIGT) detection is therefore both very challenging, and highly critical. In this survey, we summarize state-of-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task. Synthesizing the research findings, we aim to provide insight into the salient factors that combine to determine how "detectable" AIGT text is under different scenarios, and to make practical recommendations for future work towards this significant technical and societal challenge.

------------

`[2406.15593] News Deja Vu: Connecting Past and Present with Semantic Search <https://arxiv.org/abs/2406.15593>`__ 新闻似曾相识:用语义搜索连接过去和现在

::

    Fri, 21 Jun 2024 18:50:57 GMT
    Brevin Franklin, Emily Silcock, Abhishek Arora, Tom Bryan, Melissa Dell

Social scientists and the general public often analyze contemporary events by drawing parallels with the past, a process complicated by the vast, noisy, and unstructured nature of historical texts. For example, hundreds of millions of page scans from historical newspapers have been noisily transcribed.
Traditional sparse methods for searching for relevant material in these vast corpora, e.g., with keywords, can be brittle given complex vocabularies and OCR noise. This study introduces News Deja Vu, a novel semantic search tool that leverages transformer large language models and a bi-encoder approach to identify historical news articles that are most similar to modern news queries.
News Deja Vu first recognizes and masks entities, in order to focus on broader parallels rather than the specific named entities being discussed. Then, a contrastively trained, lightweight bi-encoder retrieves historical articles that are most similar semantically to a modern query, illustrating how phenomena that might seem unique to the present have varied historical precedents. Aimed at social scientists, the user-friendly News Deja Vu package is designed to be accessible for those who lack extensive familiarity with deep learning. It works with large text datasets, and we show how it can be deployed to a massive scale corpus of historical, open-source news articles. While human expertise remains important for drawing deeper insights, News Deja Vu provides a powerful tool for exploring parallels in how people have perceived past and present.

------------

`[2406.15673] Large Language Models have Intrinsic Self-Correction Ability <https://arxiv.org/abs/2406.15673>`__ 大型语言模型具有内在的自校正能力

::

    Fri, 21 Jun 2024 22:29:40 GMT
    Dancheng Liu, Amir Nassereldine, Ziming Yang, Chenhui Xu, Yuting Hu, Jiajie Li, Utkarsh Kumar, Changjae Lee, Jinjun Xiong

Large language models (LLMs) have attracted significant attention for their remarkable abilities in various natural language processing tasks, but they suffer from hallucinations that will cause performance degradation. One promising solution to improve the LLMs' performance is to ask LLMs to revise their answer after generation, a technique known as self-correction. Among the two types of self-correction, intrinsic self-correction is considered a promising direction because it does not utilize external knowledge. However, recent works doubt the validity of LLM's ability to conduct intrinsic self-correction. In this paper, we present a novel perspective on the intrinsic self-correction capabilities of LLMs through theoretical analyses and empirical experiments. In addition, we identify two critical factors for successful self-correction: zero temperature and fair prompts. Leveraging these factors, we demonstrate that intrinsic self-correction ability is exhibited across multiple existing LLMs. Our findings offer insights into the fundamental theories underlying the self-correction behavior of LLMs and remark on the importance of unbiased prompts and zero temperature settings in harnessing their full potential.

------------

`[2406.15708] Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization <https://arxiv.org/abs/2406.15708>`__ 教得更好还是表现得更聪明?关于自动提示优化中的指令和范例

::

    Sat, 22 Jun 2024 02:07:10 GMT
    Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Sercan O. Arik

Large language models have demonstrated remarkable capabilities, but their performance is heavily reliant on effective prompt engineering. Automatic prompt optimization (APO) methods are designed to automate this and can be broadly categorized into those targeting instructions (instruction optimization, IO) vs. those targeting exemplars (exemplar selection, ES).
Despite their shared objective, these have evolved rather independently, with IO recently receiving more research attention. This paper seeks to bridge this gap by comprehensively comparing the performance of representative IO and ES techniques, both isolation and combination, on a diverse set of challenging tasks. Our findings reveal that intelligently reusing model-generated input-output pairs obtained from evaluating prompts on the validation set as exemplars consistently improves performance over IO methods but is currently under-investigated. We also find that despite the recent focus on IO, how we select exemplars can outweigh how we optimize instructions, with ES strategies as simple as random search outperforming state-of-the-art IO methods with seed instructions without any optimization. Moreover, we observe synergy between ES and IO, with optimal combinations surpassing individual contributions. We conclude that studying exemplar selection as a standalone method and its optimal combination with instruction optimization remains a crucial aspect of APO and deserves greater consideration in future research, even in the era of highly capable instruction-following models.

------------

`[2406.15718] Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models <https://arxiv.org/abs/2406.15718>`__ 超越回合制游戏:实现与双工模型的实时对话

::

    Sat, 22 Jun 2024 03:20:10 GMT
    Xinrong Zhang, Yingfa Chen, Shengding Hu, Xu Han, Zihang Xu, Yuanwei Xu, Weilin Zhao, Maosong Sun, Zhiyuan Liu

As large language models (LLMs) increasingly permeate daily lives, there is a growing demand for real-time interactions that mirror human conversations.
Traditional turn-based chat systems driven by LLMs prevent users from verbally interacting with the system while it is generating responses. To overcome these limitations, we adapt existing LLMs to \textit{duplex models} so that these LLMs can listen for users while generating output and dynamically adjust themselves to provide users with instant feedback. % such as in response to interruptions. Specifically, we divide the queries and responses of conversations into several time slices and then adopt a time-division-multiplexing (TDM) encoding-decoding strategy to pseudo-simultaneously process these slices. Furthermore, to make LLMs proficient enough to handle real-time conversations, we build a fine-tuning dataset consisting of alternating time slices of queries and responses as well as covering typical feedback types in instantaneous interactions. Our experiments show that although the queries and responses of conversations are segmented into incomplete slices for processing, LLMs can preserve their original performance on standard benchmarks with a few fine-tuning steps on our dataset. Automatic and human evaluation indicate that duplex models make user-AI interactions more natural and human-like, and greatly improve user satisfaction compared to vanilla LLMs. Our duplex model and dataset will be released.

------------

`[2406.15720] Scaling Laws for Fact Memorization of Large Language Models <https://arxiv.org/abs/2406.15720>`__ 大型语言模型事实记忆的缩放定律

::

    Sat, 22 Jun 2024 03:32:09 GMT
    Xingyu Lu, Xiaonan Li, Qinyuan Cheng, Kai Ding, Xuanjing Huang, Xipeng Qiu

Fact knowledge memorization is crucial for Large Language Models (LLM) to generate factual and reliable responses. However, the behaviors of LLM fact memorization remain under-explored. In this paper, we analyze the scaling laws for LLM's fact knowledge and LLMs' behaviors of memorizing different types of facts. We find that LLMs' fact knowledge capacity has a linear and negative exponential law relationship with model size and training epochs, respectively.
Estimated by the built scaling law, memorizing the whole Wikidata's facts requires training an LLM with 1000B non-embed parameters for 100 epochs, suggesting that using LLMs to memorize all public facts is almost implausible for a general pre-training setting. Meanwhile, we find that LLMs can generalize on unseen fact knowledge and its scaling law is similar to general pre-training. Additionally, we analyze the compatibility and preference of LLMs' fact memorization. For compatibility, we find LLMs struggle with memorizing redundant facts in a unified way. Only when correlated facts have the same direction and structure, the LLM can compatibly memorize them. This shows the inefficiency of LLM memorization for redundant facts. For preference, the LLM pays more attention to memorizing more frequent and difficult facts, and the subsequent facts can overwrite prior facts' memorization, which significantly hinders low-frequency facts memorization. Our findings reveal the capacity and characteristics of LLMs' fact knowledge learning, which provide directions for LLMs' fact knowledge augmentation.

------------

`[2406.15734] RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs <https://arxiv.org/abs/2406.15734>`__ RankAdaptor:面向结构剪枝llm的分层动态低秩自适应

::

    Sat, 22 Jun 2024 04:52:58 GMT
    Changhai Zhou, Shijie Han, Shiyang Zhang, Shichao Weng, Zekai Liu, Cheng Jin

The efficient compression of large language models (LLMs) is becoming increasingly popular. However, recovering the accuracy of compressed LLMs is still a major challenge. Structural pruning with standard Low-Rank Adaptation (LoRA) is a common technique in current LLM compression. In structural pruning, the model architecture is modified unevenly, resulting in suboptimal performance in various downstream tasks via standard LoRA with fixed rank. To address this problem, we introduce RankAdaptor, an efficient fine-tuning method with hierarchical dynamic rank scheduling for pruned LLMs. An end-to-end automatic optimization flow is developed that utilizes a lightweight performance model to determine the different ranks during fine-tuning.
Comprehensive experiments on popular benchmarks show that RankAdaptor consistently outperforms standard LoRA with structural pruning over different pruning settings. Without increasing the trainable parameters, RankAdaptor further reduces the accuracy performance gap between the recovery of the pruned model and the original model compared to standard LoRA.

------------

`[2406.15741] Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level <https://arxiv.org/abs/2406.15741>`__ Ladder:一个与模型无关的框架，将基于llm的机器翻译提升到一个新的水平

::

    Sat, 22 Jun 2024 05:33:35 GMT
    Zhaopeng Feng, Ruizhe Chen, Yan Zhang, Zijie Meng, Zuozhu Liu

General-purpose Large Language Models (LLMs) like GPT-4 have achieved remarkable advancements in machine translation (MT) by leveraging extensive web content. On the other hand, translation-specific LLMs are built by pre-training on domain-specific monolingual corpora and fine-tuning with human-annotated translation data. Despite the superior performance, these methods either demand an unprecedented scale of computing and data or substantial human editing and annotation efforts. In this paper, we develop Ladder, a novel model-agnostic and cost-effective tool to refine the performance of general LLMs for MT.
Ladder is trained on pseudo-refinement triplets which can be easily obtained from existing LLMs without additional human cost. During training, we propose a hierarchical fine-tuning strategy with an easy-to-hard schema, improving Ladder's refining performance progressively. The trained Ladder can be seamlessly integrated with any general-purpose LLMs to boost their translation performance. By utilizing Gemma-2B/7B as the backbone, Ladder-2B can elevate raw translations to the level of top-tier open-source models (e.g., refining BigTranslate-13B with +6.91 BLEU and +3.52 COMET for XX-En), and Ladder-7B can further enhance model performance to be on par with the state-of-the-art GPT-4.
Extensive ablation and analysis corroborate the effectiveness of Ladder in diverse settings. Our code is available at https://github.com/fzp0424/Ladder

------------

`[2406.15781] DABL: Detecting Semantic Anomalies in Business Processes Using Large Language Models <https://arxiv.org/abs/2406.15781>`__ DABL:使用大型语言模型检测业务流程中的语义异常

::

    Sat, 22 Jun 2024 08:20:19 GMT
    Wei Guan, Jian Cao, Jianqi Gao, Haiyan Zhao, Shiyou Qian

Detecting anomalies in business processes is crucial for ensuring operational success. While many existing methods rely on statistical frequency to detect anomalies, it's important to note that infrequent behavior doesn't necessarily imply undesirability. To address this challenge, detecting anomalies from a semantic viewpoint proves to be a more effective approach. However, current semantic anomaly detection methods treat a trace (i.e., process instance) as multiple event pairs, disrupting long-distance dependencies. In this paper, we introduce DABL, a novel approach for detecting semantic anomalies in business processes using large language models (LLMs). We collect 143,137 real-world process models from various domains. By generating normal traces through the playout of these process models and simulating both ordering and exclusion anomalies, we fine-tune Llama 2 using the resulting log. Through extensive experiments, we demonstrate that DABL surpasses existing state-of-the-art semantic anomaly detection methods in terms of both generalization ability and learning of given processes. Users can directly apply DABL to detect semantic anomalies in their own datasets without the need for additional training.
Furthermore, DABL offers the capability to interpret the causes of anomalies in natural language, providing valuable insights into the detected anomalies.

------------

`[2406.15796] Rethinking Entity-level Unlearning for Large Language Models <https://arxiv.org/abs/2406.15796>`__ 大型语言模型实体级遗忘的再思考

::

    Sat, 22 Jun 2024 09:40:07 GMT
    Weitao Ma, Xiaocheng Feng, Weihong Zhong, Lei Huang, Yangfan Ye, Bing Qin

Large language model unlearning has gained increasing attention due to its potential to mitigate security and privacy concerns. Current research predominantly focuses on Instance-level unlearning, specifically aiming at forgetting predefined instances of sensitive content. However, a notable gap still exists in exploring the deletion of complete entity-related information, which is crucial in many real-world scenarios, such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, where the entity-related knowledge within the target model is supposed to be entirely erased. Given the challenge of practically accessing all entity-related knowledge within a model, we begin by simulating entity-level unlearning scenarios through fine-tuning models to introduce pseudo entities. Following this, we develop baseline methods inspired by trending unlearning techniques and conduct a detailed comparison of their effectiveness in this task.
Extensive experiments reveal that current unlearning algorithms struggle to achieve effective entity-level unlearning. Additionally, our analyses further indicate that entity-related knowledge injected through fine-tuning is more susceptible than original entities from pre-training during unlearning, highlighting the necessity for more thorough pseudo-entity injection methods to make them closer to pre-trained knowledge.

------------

`[2406.15809] LaMSUM: A Novel Framework for Extractive Summarization of User Generated Content using LLMs <https://arxiv.org/abs/2406.15809>`__ 

::

    Sat, 22 Jun 2024 10:25:55 GMT
    Garima Chhikara, Anurag Sharma, V. Gurucharan, Kripabandhu Ghosh, Abhijnan Chakraborty

Large Language Models (LLMs) have demonstrated impressive performance across a wide range of NLP tasks, including summarization. Inherently LLMs produce abstractive summaries, and the task of achieving extractive summaries through LLMs still remains largely unexplored. To bridge this gap, in this work, we propose a novel framework LaMSUM to generate extractive summaries through LLMs for large user-generated text by leveraging voting algorithms. Our evaluation on three popular open-source LLMs (Llama 3, Mixtral and Gemini) reveal that the LaMSUM outperforms state-of-the-art extractive summarization methods. We further attempt to provide the rationale behind the output summary produced by LLMs. Overall, this is one of the early attempts to achieve extractive summarization for large user-generated text by utilizing LLMs, and likely to generate further interest in the community.

------------

`[2406.15883] SimSMoE: Solving Representational Collapse via Similarity Measure <https://arxiv.org/abs/2406.15883>`__ SimSMoE:基于相似性度量的表征坍缩求解

::

    Sat, 22 Jun 2024 16:10:45 GMT
    Giang Do, Hung Le, Truyen Tran

Sparse mixture of experts (SMoE) have emerged as an effective approach for scaling large language models while keeping a constant computational cost.
Regardless of several notable successes of SMoE, effective training such architecture remains elusive due to the representation collapse problem, which in turn harms model performance and causes parameter redundancy. In this work, we present Similarity-based Sparse Mixture of Experts (SimSMoE), a novel similarity of neural network algorithm, that guarantees a solution to address the representation collapse issue between experts given a fixed FLOPs budget.
We conduct extensive empirical evaluations on three large language models for both Pre-training and Fine-tuning tasks to illustrate the efficacy, robustness, and scalability of our method. The results demonstrate that SimSMoE significantly enhances existing routing policy and outperforms other SMoE training methods in performance for the tasks.

------------

`[2406.15888] Real-time Speech Summarization for Medical Conversations <https://arxiv.org/abs/2406.15888>`__ 医学对话的实时语音摘要

::

    Sat, 22 Jun 2024 16:37:51 GMT
    Khai Le-Duc, Khai-Nguyen Nguyen, Long Vo-Dang, Truong-Son Hy

In doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization.
Finally, we present baseline results of state-of-the-art models on VietMed-Sum.
All code, data (English-translated and Vietnamese) and models are available online: https://github.com/leduckhai/MultiMed

------------

`[2406.15891] The Unlikely Duel: Evaluating Creative Writing in LLMs through a Unique Scenario <https://arxiv.org/abs/2406.15891>`__ 不太可能的决斗:通过独特的场景评估llm的创意写作

::

    Sat, 22 Jun 2024 17:01:59 GMT
    Carlos G\'omez-Rodr\'iguez and Paul Williams

This is a summary of the paper "A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing", which was published in Findings of EMNLP 2023. We evaluate a range of recent state-of-the-art, instruction-tuned large language models (LLMs) on an English creative writing task, and compare them to human writers. For this purpose, we use a specifically-tailored prompt (based on an epic combat between Ignatius J. Reilly, main character of John Kennedy Toole's "A Confederacy of Dunces", and a pterodactyl) to minimize the risk of training data leakage and force the models to be creative rather than reusing existing stories. The same prompt is presented to LLMs and human writers, and evaluation is performed by humans using a detailed rubric including various aspects like fluency, style, originality or humor. Results show that some state-of-the-art commercial LLMs match or slightly outperform our human writers in most of the evaluated dimensions. Open-source LLMs lag behind. Humans keep a close lead in originality, and only the top three LLMs can handle humor at human-like levels.

------------

`[2406.15927] Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs <https://arxiv.org/abs/2406.15927>`__ 语义熵探测:llm中鲁棒且廉价的幻觉检测

::

    Sat, 22 Jun 2024 19:46:06 GMT
    Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, Yarin Gal

We propose semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs.
Recent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, we propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. We show that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. Our results across models and tasks suggest that model hidden states capture SE, and our ablation studies give further insights into the token positions and model layers for which this is the case.

------------

`[2406.15938] RuleR: Improving LLM Controllability by Rule-based Data Recycling <https://arxiv.org/abs/2406.15938>`__ RuleR:通过基于规则的数据回收提高LLM可控性

::

    Sat, 22 Jun 2024 20:57:12 GMT
    Ming Li, Han Chen, Chenguang Wang, Dang Nguyen, Dianqi Li, Tianyi Zhou

Large language models (LLMs) still lack delicate controllability over their responses, which is critical to enhancing their performance and the user experience. However, curating supervised fine-tuning (SFT) datasets to improve LLM controllability usually relies on human experts or proprietary LLMs, which requires additional costs. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a data augmentation method incorporating multiple constraints into the original data samples according to predefined rules, which creates new training tasks to consolidate the controllability of LLMs. Instead of creating new data from scratch, RuleR ``recycles'' existing data by simply applying rule-based edits to their responses and appending the rule-instructions in their original instructions. Experimental results demonstrate RuleR's effectiveness in improving LLM controllability while maintaining general instruction-following capabilities. The code will be released on https://github.com/MingLiiii/RuleR.

------------

`[2406.15948] Teaching LLMs to Abstain across Languages via Multilingual Feedback <https://arxiv.org/abs/2406.15948>`__ 通过多语言反馈教llm跨语言弃权

::

    Sat, 22 Jun 2024 21:59:12 GMT
    Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Orevaoghene Ahia, Shuyue Stella Li, Vidhisha Balachandran, Sunayana Sitaram, Yulia Tsvetkov

Multilingual LLMs often have knowledge disparities across languages, with larger gaps in under-resourced languages. Teaching LLMs to abstain in the face of knowledge gaps is thus a promising strategy to mitigate hallucinations in multilingual settings. However, previous studies on LLM abstention primarily focus on English; we find that directly applying existing solutions beyond English results in up to 20.5% performance gaps between high and low-resource languages, potentially due to LLMs' drop in calibration and reasoning beyond a few resource-rich languages. To this end, we propose strategies to enhance LLM abstention by learning from multilingual feedback, where LLMs self-reflect on proposed answers in one language by generating multiple feedback items in related languages: we show that this helps identifying the knowledge gaps across diverse languages, cultures, and communities. Extensive experiments demonstrate that our multilingual feedback approach outperforms various strong baselines, achieving up to 9.2% improvement for low-resource languages across three black-box and open models on three datasets, featuring open-book, closed-book, and commonsense QA. Further analysis reveals that multilingual feedback is both an effective and a more equitable abstain strategy to serve diverse language speakers, and cultural factors have great impact on language selection and LLM abstention behavior, highlighting future directions for multilingual and multi-cultural reliable language modeling.

------------

`[2406.15951] Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration <https://arxiv.org/abs/2406.15951>`__ 

::

    Sat, 22 Jun 2024 22:07:40 GMT
    Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian Fisher, Chan Young Park, Yejin Choi, Yulia Tsvetkov

While existing alignment paradigms have been integral in developing large language models (LLMs), LLMs often learn an averaged human preference and struggle to model diverse preferences across cultures, demographics, and communities. We propose Modular Pluralism, a modular framework based on multi-LLM collaboration for pluralistic alignment: it "plugs into" a base LLM a pool of smaller but specialized community LMs, where models collaborate in distinct modes to flexibility support three modes of pluralism: Overton, steerable, and distributional. Modular Pluralism is uniquely compatible with black-box LLMs and offers the modular control of adding new community LMs for previously underrepresented communities. We evaluate Modular Pluralism with six tasks and four datasets featuring questions/instructions with value-laden and perspective-informed responses. Extensive experiments demonstrate that Modular Pluralism advances the three pluralism objectives across six black-box and open-source LLMs. Further analysis reveals that LLMs are generally faithful to the inputs from smaller community LLMs, allowing seamless patching by adding a new community LM to better cover previously underrepresented communities.

------------

`[2406.15966] Evaluating the Effectiveness of the Foundational Models for Q&A Classification in Mental Health care <https://arxiv.org/abs/2406.15966>`__ 评估精神卫生保健问答分类基础模型的有效性

::

    Sun, 23 Jun 2024 00:11:07 GMT
    Hassan Alhuzali and Ashwag Alasmari

Pre-trained Language Models (PLMs) have the potential to transform mental health support by providing accessible and culturally sensitive resources.
However, despite this potential, their effectiveness in mental health care and specifically for the Arabic language has not been extensively explored. To bridge this gap, this study evaluates the effectiveness of foundational models for classification of Questions and Answers (Q&A) in the domain of mental health care. We leverage the MentalQA dataset, an Arabic collection featuring Q&A interactions related to mental health. In this study, we conducted experiments using four different types of learning approaches: traditional feature extraction, PLMs as feature extractors, Fine-tuning PLMs and prompting large language models (GPT-3.5 and GPT-4) in zero-shot and few-shot learning settings. While traditional feature extractors combined with Support Vector Machines (SVM) showed promising performance, PLMs exhibited even better results due to their ability to capture semantic meaning. For example, MARBERT achieved the highest performance with a Jaccard Score of 0.80 for question classification and a Jaccard Score of 0.86 for answer classification. We further conducted an in-depth analysis including examining the effects of fine-tuning versus non-fine-tuning, the impact of varying data size, and conducting error analysis. Our analysis demonstrates that fine-tuning proved to be beneficial for enhancing the performance of PLMs, and the size of the training data played a crucial role in achieving high performance. We also explored prompting, where few-shot learning with GPT-3.5 yielded promising results. There was an improvement of 12% for question and classification and 45% for answer classification. Based on our findings, it can be concluded that PLMs and prompt-based approaches hold promise for mental health support in Arabic.

------------

`[2406.15968] ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods <https://arxiv.org/abs/2406.15968>`__ 回顾:基于相对条件对数似然的隶属推断

::

    Sun, 23 Jun 2024 00:23:13 GMT
    Roy Xie, Junlin Wang, Ruomin Huang, Minxing Zhang, Rong Ge, Jian Pei, Neil Zhenqiang Gong, Bhuwan Dhingra

The rapid scaling of large language models (LLMs) has raised concerns about the transparency and fair use of the pretraining data used for training them.
Detecting such content is challenging due to the scale of the data and limited exposure of each instance during training. We propose ReCaLL (Relative Conditional Log-Likelihood), a novel membership inference attack (MIA) to detect LLMs' pretraining data by leveraging their conditional language modeling capabilities. ReCaLL examines the relative change in conditional log-likelihoods when prefixing target data points with non-member context. Our empirical findings show that conditioning member data on non-member prefixes induces a larger decrease in log-likelihood compared to non-member data. We conduct comprehensive experiments and show that ReCaLL achieves state-of-the-art performance on the WikiMIA dataset, even with random and synthetic prefixes, and can be further improved using an ensemble approach.
Moreover, we conduct an in-depth analysis of LLMs' behavior with different membership contexts, providing insights into how LLMs leverage membership information for effective inference at both the sequence and token level.

------------

`[2406.15981] Serial Position Effects of Large Language Models <https://arxiv.org/abs/2406.15981>`__ 大型语言模型的序列位置效应

::

    Sun, 23 Jun 2024 02:02:52 GMT
    Xiaobo Guo and Soroush Vosoughi

Large Language Models (LLMs) have shown remarkable capabilities in zero-shot learning applications, generating responses to queries using only pre-training information without the need for additional fine-tuning. This represents a significant departure from traditional machine learning approaches. Previous research has indicated that LLMs may exhibit serial position effects, such as primacy and recency biases, which are well-documented cognitive biases in human psychology. Our extensive testing across various tasks and models confirms the widespread occurrence of these effects, although their intensity varies. We also discovered that while carefully designed prompts can somewhat mitigate these biases, their effectiveness is inconsistent. These findings underscore the significance of serial position effects during the inference process, particularly in scenarios where there are no ground truth labels, highlighting the need for greater focus on addressing these effects in LLM applications.

------------

`[2406.15996] Memorizing Documents with Guidance in Large Language Models <https://arxiv.org/abs/2406.15996>`__ 在大型语言模型中使用指南记忆文档

::

    Sun, 23 Jun 2024 03:12:03 GMT
    Bumjin Park and Jaesik Choi

Training data plays a pivotal role in AI models. Large language models (LLMs) are trained with massive amounts of documents, and their parameters hold document-related contents. Recently, several studies identified content-specific locations in LLMs by examining the parameters. Instead of the post hoc interpretation, we propose another approach. We propose document-wise memory architecture to track document memories in training. The proposed architecture maps document representations to memory entries, which softly mask memories in the forward process of LLMs. Additionally, we propose document guidance loss, which increases the likelihood of text with document memories and reduces the likelihood of the text with the memories of other documents.
Experimental results on Wikitext-103-v1 with Pythia-1B show that the proposed methods provide different memory entries for documents and high recall of document-related content in generation with trained document-wise memories.

------------

`[2406.16008] Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization <https://arxiv.org/abs/2406.16008>`__ 中间发现:校准位置注意力偏差可提高长上下文利用率

::

    Sun, 23 Jun 2024 04:35:42 GMT
    Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long T. Le, Abhishek Kumar, James Glass, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister

Large language models (LLMs), even when specifically trained to process long input contexts, struggle to capture relevant information located in the middle of their input. This phenomenon has been known as the lost-in-the-middle problem. In this work, we make three contributions. First, we set out to understand the factors that cause this phenomenon. In doing so, we establish a connection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs exhibit a U-shaped attention bias where the tokens at the beginning and at the end of its input receive higher attention, regardless of their relevance.
Second, we mitigate this positional bias through a calibration mechanism, found-in-the-middle, that allows the model to attend to contexts faithfully according to their relevance, even though when they are in the middle. Third, we show found-in-the-middle not only achieves better performance in locating relevant information within a long context, but also eventually leads to improved retrieval-augmented generation (RAG) performance across various tasks, outperforming existing methods by up to 15 percentage points. These findings open up future directions in understanding LLM attention bias and its potential consequences.

------------

`[2406.16033] Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models <https://arxiv.org/abs/2406.16033>`__ 

::

    Sun, 23 Jun 2024 06:54:47 GMT
    Tianyi Men, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao

Planning, as the core module of agents, is crucial in various fields such as embodied agents, web navigation, and tool using. With the development of large language models (LLMs), some researchers treat large language models as intelligent agents to stimulate and evaluate their planning capabilities.
However, the planning mechanism is still unclear. In this work, we focus on exploring the look-ahead planning mechanism in large language models from the perspectives of information flow and internal representations. First, we study how planning is done internally by analyzing the multi-layer perception (MLP) and multi-head self-attention (MHSA) components at the last token. We find that the output of MHSA in the middle layers at the last token can directly decode the decision to some extent. Based on this discovery, we further trace the source of MHSA by information flow, and we reveal that MHSA mainly extracts information from spans of the goal states and recent steps. According to information flow, we continue to study what information is encoded within it.
Specifically, we explore whether future decisions have been encoded in advance in the representation of flow. We demonstrate that the middle and upper layers encode a few short-term future decisions to some extent when planning is successful. Overall, our research analyzes the look-ahead planning mechanisms of LLMs, facilitating future research on LLMs performing planning tasks.

------------

`[2406.16069] FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models <https://arxiv.org/abs/2406.16069>`__ FastMem:提示符的快速记忆提高大型语言模型的上下文感知

::

    Sun, 23 Jun 2024 10:36:35 GMT
    Junyi Zhu, Shuochen Liu, Yu Yu, Bo Tang, Yibo Yan, Zhiyu Li, Feiyu Xiong, Tong Xu, Matthew B. Blaschko

Large language models (LLMs) excel in generating coherent text, but they often struggle with context awareness, leading to inaccuracies in tasks requiring faithful adherence to provided information. We introduce FastMem, a novel method designed to enhance instruction fine-tuned LLMs' context awareness through fast memorization of the prompt. FastMem maximizes the likelihood of the prompt before inference by fine-tuning only the last Feed-Forward Network (FFN) module. This targeted approach ensures efficient optimization without overfitting, significantly improving the model's ability to comprehend and accurately follow the context. Our experiments demonstrate substantial gains in reading comprehension, text summarization and adherence to output structures.
For instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAP dataset from 59.1% to 71.6%, and reduces the output structure failure rate of Qwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlight FastMem's potential to offer a robust solution to enhance the reliability and accuracy of LLMs in various applications. Our code is available at: https://github.com/IAAR-Shanghai/FastMem

------------

`[2406.16135] Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models <https://arxiv.org/abs/2406.16135>`__ 多语言大型语言模型的跨语言能力和知识障碍

::

    Sun, 23 Jun 2024 15:15:17 GMT
    Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chulin Xie, Chiyuan Zhang

Large language models (LLMs) are typically multilingual due to pretraining on diverse multilingual corpora. But can these models relate corresponding concepts across languages, effectively being crosslingual? This study evaluates six state-of-the-art LLMs on inherently crosslingual tasks. We observe that while these models show promising surface-level crosslingual abilities on machine translation and embedding space analyses, they struggle with deeper crosslingual knowledge transfer, revealing a crosslingual knowledge barrier in both general (MMLU benchmark) and domain-specific (Harry Potter quiz) contexts.
We observe that simple inference-time mitigation methods offer only limited improvement. On the other hand, we propose fine-tuning of LLMs on mixed-language data, which effectively reduces these gaps, even when using out-of-domain datasets like WikiText. Our findings suggest the need for explicit optimization to unlock the full crosslingual potential of LLMs. Our code is publicly available at https://github.com/google-research/crosslingual-knowledge-barriers.

------------

`[2406.16144] Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step <https://arxiv.org/abs/2406.16144>`__ 探针链:检验辊套分步的必要性和准确性

::

    Sun, 23 Jun 2024 15:50:22 GMT
    Zezhong Wang, Xingshan Zeng, Weiwen Liu, Yufei Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong

Current research found the issue of Early Answering in large language models (LLMs), where the models already have an answer before generating the Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary dependency between the predicted answer and the reasoning process.
Consequently, two important questions arise: (1) Is CoT still necessary if the model already has an answer? (2) Can the correctness of the answer serve as valid evidence for the correctness of CoT? To address these questions, we propose a method, namely Chain-of-Probe (CoP), to probe changes in the mind during the model's reasoning. The probing results show that in a significant number of question-answer cases, CoT appears to be unnecessary, and this necessity correlates with the simplicity of the task, defined by reasoning steps required. Furthermore, by analyzing patterns in mind change, we examine the correctness of the model's reasoning. Our validation reveals that many responses, although correct in their final answer, contain errors in their reasoning process. To this end, we propose a strategic approach based on CoP to prioritize answers with correct reasoning among multiple candidates, thereby bolstering the reliability of the model's reasoning.

------------

`[2406.16152] Towards Region-aware Bias Evaluation Metrics <https://arxiv.org/abs/2406.16152>`__ 区域感知偏差评估指标研究

::

    Sun, 23 Jun 2024 16:26:27 GMT
    Angana Borah, Aparna Garimella, Rada Mihalcea

When exposed to human-generated data, language models are known to learn and amplify societal biases. While previous works introduced benchmarks that can be used to assess the bias in these models, they rely on assumptions that may not be universally true. For instance, a gender bias dimension commonly used by these metrics is that of family--career, but this may not be the only common bias in certain regions of the world. In this paper, we identify topical differences in gender bias across different regions and propose a region-aware bottom-up approach for bias assessment. Our proposed approach uses gender-aligned topics for a given region and identifies gender bias dimensions in the form of topic pairs that are likely to capture gender societal biases.
Several of our proposed bias topic pairs are on par with human perception of gender biases in these regions in comparison to the existing ones, and we also identify new pairs that are more aligned than the existing ones. In addition, we use our region-aware bias topic pairs in a Word Embedding Association Test (WEAT)-based evaluation metric to test for gender biases across different regions in different data domains. We also find that LLMs have a higher alignment to bias pairs for highly-represented regions showing the importance of region-aware bias evaluation metric.

------------

`[2406.16203] LLMs' Classification Performance is Overclaimed <https://arxiv.org/abs/2406.16203>`__ 

::

    Sun, 23 Jun 2024 19:49:10 GMT
    Hanzi Xu, Renze Lou, Jiangshu Du, Vahid Mahzoon, Elmira Talebianaraki, Zhuoan Zhou, Elizabeth Garrison, Slobodan Vucetic, Wenpeng Yin

In many classification tasks designed for AI or human to solve, gold labels are typically included within the label space by default, often posed as "which of the following is correct?" This standard setup has traditionally highlighted the strong performance of advanced AI, particularly top-performing Large Language Models (LLMs), in routine classification tasks. However, when the gold label is intentionally excluded from the label space, it becomes evident that LLMs still attempt to select from the available label candidates, even when none are correct. This raises a pivotal question: Do LLMs truly demonstrate their intelligence in understanding the essence of classification tasks? In this study, we evaluate both closed-source and open-source LLMs across representative classification tasks, arguing that the perceived performance of LLMs is overstated due to their inability to exhibit the expected comprehension of the task. This paper makes a threefold contribution: i) To our knowledge, this is the first work to identify the limitations of LLMs in classification tasks when gold labels are absent. We define this task as Classify-w/o-Gold and propose it as a new testbed for LLMs. ii) We introduce a benchmark, Know-No, comprising two existing classification tasks and one new task, to evaluate Classify-w/o-Gold. iii) This work defines and advocates for a new evaluation metric, OmniAccuracy, which assesses LLMs' performance in classification tasks both when gold labels are present and absent.

------------

`[2406.16229] Multi-Objective Linguistic Control of Large Language Models <https://arxiv.org/abs/2406.16229>`__ 大型语言模型的多目标语言控制

::

    Sun, 23 Jun 2024 21:56:48 GMT
    Dang Nguyen, Jiuhai Chen, Tianyi Zhou

Large language models (LLMs), despite their breakthroughs on many challenging benchmark tasks, lean to generate verbose responses and lack the controllability of output complexity, which is usually preferred by human users in practice. In this paper, we study how to precisely control multiple linguistic complexities of LLM output by finetuning using off-the-shelf data.
To this end, we propose multi-control tuning (MCTune), which includes multiple linguistic complexity values of ground-truth responses as controls in the input for instruction tuning. We finetune LLaMA2-7B on Alpaca-GPT4 and WizardLM datasets. Evaluations on widely used benchmarks demonstrate that our method does not only improve LLMs' multi-complexity controllability substantially but also retains or even enhances the quality of the responses as a side benefit.

------------

`[2406.16235] Preference Tuning For Toxicity Mitigation Generalizes Across Languages <https://arxiv.org/abs/2406.16235>`__ 缓解毒性的偏好调整适用于各种语言

::

    Sun, 23 Jun 2024 22:53:47 GMT
    Xiaochen Li, Zheng-Xin Yong, Stephen H. Bach

Detoxifying multilingual Large Language Models (LLMs) has become crucial due to their increasing global use. In this work, we explore zero-shot cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike previous studies that show limited cross-lingual generalization for other safety tasks, we demonstrate that Direct Preference Optimization (DPO) training with only English data can significantly reduce toxicity in multilingual open-ended generations. For example, the probability of mGPT-1.3B generating toxic continuations drops from 46.8% to 3.9% across 17 different languages after training. Our results also extend to other multilingual LLMs, such as BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal intervention and activation analysis, we identified the dual multilinguality property of MLP layers in LLMs, which explains the cross-lingual generalization of DPO. Finally, we show that bilingual sentence retrieval can predict the cross-lingual transferability of DPO preference tuning.

------------

`[2406.16253] LLMs assist NLP Researchers: Critique Paper (Meta-)Reviewing <https://arxiv.org/abs/2406.16253>`__ LLMs协助NLP研究人员:评论论文(元)评审

::

    Mon, 24 Jun 2024 01:30:22 GMT
    Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Ranran Zhang, Vipul Gupta, Yinghui Li, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Jiayang Cheng, Zhaowei Wang, Ying Su, Raj Sanjay Shah, Ruohao Guo, Jing Gu, Haoran Li, Kangda Wei, Zihao Wang, Lu Cheng, Surangika Ranathunga, Meng Fang, Jie Fu, Fei Liu, Ruihong Huang, Eduardo Blanco, Yixin Cao, Rui Zhang, Philip S. Yu, Wenpeng Yin

This work is motivated by two key trends. On one hand, large language models (LLMs) have shown remarkable versatility in various generative tasks such as writing, drawing, and question answering, significantly reducing the time required for many routine tasks. On the other hand, researchers, whose work is not only time-consuming but also highly expertise-demanding, face increasing challenges as they have to spend more time reading, writing, and reviewing papers. This raises the question: how can LLMs potentially assist researchers in alleviating their heavy workload? This study focuses on the topic of LLMs assist NLP Researchers, particularly examining the effectiveness of LLM in assisting paper (meta-)reviewing and its recognizability. To address this, we constructed the ReviewCritique dataset, which includes two types of information: (i) NLP papers (initial submissions rather than camera-ready) with both human-written and LLM-generated reviews, and (ii) each review comes with "deficiency" labels and corresponding explanations for individual segments, annotated by experts. Using ReviewCritique, this study explores two threads of research questions: (i) "LLMs as Reviewers", how do reviews generated by LLMs compare with those written by humans in terms of quality and distinguishability? (ii) "LLMs as Metareviewers", how effectively can LLMs identify potential issues, such as Deficient or unprofessional review segments, within individual paper reviews? To our knowledge, this is the first work to provide such a comprehensive analysis.

------------

`[2406.16264] One Thousand and One Pairs: A "novel" challenge for long-context language models <https://arxiv.org/abs/2406.16264>`__ 《一千零一对:长上下文语言模型的“新”挑战》

::

    Mon, 24 Jun 2024 02:03:57 GMT
    Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, Mohit Iyyer

Synthetic long-context LLM benchmarks (e.g., "needle-in-the-haystack") test only surface-level retrieval capabilities, but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models.

------------

`[2406.16275] Investigating the Influence of Prompt-Specific Shortcuts in AI Generated Text Detection <https://arxiv.org/abs/2406.16275>`__ 研究AI生成文本检测中提示特定快捷方式的影响

::

    Mon, 24 Jun 2024 02:50:09 GMT
    Choonghyun Park, Hyuhng Joon Kim, Junyeob Kim, Youna Kim, Taeuk Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-goo Lee and Kang Min Yoo

AI Generated Text (AIGT) detectors are developed with texts from humans and LLMs of common tasks. Despite the diversity of plausible prompt choices, these datasets are generally constructed with a limited number of prompts. The lack of prompt variation can introduce prompt-specific shortcut features that exist in data collected with the chosen prompt, but do not generalize to others. In this paper, we analyze the impact of such shortcuts in AIGT detection. We propose Feedback-based Adversarial Instruction List Optimization (FAILOpt), an attack that searches for instructions deceptive to AIGT detectors exploiting prompt-specific shortcuts. FAILOpt effectively drops the detection performance of the target detector, comparable to other attacks based on adversarial in-context examples. We also utilize our method to enhance the robustness of the detector by mitigating the shortcuts. Based on the findings, we further train the classifier with the dataset augmented by FAILOpt prompt. The augmented classifier exhibits improvements across generation models, tasks, and attacks. Our code will be available at https://github.com/zxcvvxcz/FAILOpt.

------------

`[2406.16288] PlagBench: Exploring the Duality of Large Language Models in Plagiarism Generation and Detection <https://arxiv.org/abs/2406.16288>`__ PlagBench:探索大型语言模型在抄袭生成和检测中的二重性

::

    Mon, 24 Jun 2024 03:29:53 GMT
    Jooyoung Lee, Toshini Agrawal, Adaku Uchendu, Thai Le, Jinghui Chen, Dongwon Lee

Recent literature has highlighted potential risks to academic integrity associated with large language models (LLMs), as they can memorize parts of training instances and reproduce them in the generated texts without proper attribution. In addition, given their capabilities in generating high-quality texts, plagiarists can exploit LLMs to generate realistic paraphrases or summaries indistinguishable from original work. In response to possible malicious use of LLMs in plagiarism, we introduce PlagBench, a comprehensive dataset consisting of 46.5K synthetic plagiarism cases generated using three instruction-tuned LLMs across three writing domains. The quality of PlagBench is ensured through fine-grained automatic evaluation for each type of plagiarism, complemented by human annotation. We then leverage our proposed dataset to evaluate the plagiarism detection performance of five modern LLMs and three specialized plagiarism checkers. Our findings reveal that GPT-3.5 tends to generates paraphrases and summaries of higher quality compared to Llama2 and GPT-4. Despite LLMs' weak performance in summary plagiarism identification, they can surpass current commercial plagiarism detectors.
Overall, our results highlight the potential of LLMs to serve as robust plagiarism detection tools.

------------

`[2406.16294] LangSuitE: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments <https://arxiv.org/abs/2406.16294>`__ LangSuitE:具身文本环境中大型语言模型的规划、控制和交互

::

    Mon, 24 Jun 2024 03:36:29 GMT
    Zixia Jia, Mengmeng Wang, Baichen Tong, Song-Chun Zhu, Zilong Zheng

Recent advances in Large Language Models (LLMs) have shown inspiring achievements in constructing autonomous agents that rely on language descriptions as inputs. However, it remains unclear how well LLMs can function as few-shot or zero-shot embodied agents in dynamic interactive environments.
To address this gap, we introduce LangSuitE, a versatile and simulation-free testbed featuring 6 representative embodied tasks in textual embodied worlds.
Compared with previous LLM-based testbeds, LangSuitE (i) offers adaptability to diverse environments without multiple simulation engines, (ii) evaluates agents' capacity to develop ``internalized world knowledge'' with embodied observations, and (iii) allows easy customization of communication and action strategies. To address the embodiment challenge, we devise a novel chain-of-thought (CoT) schema, EmMem, which summarizes embodied states w.r.t.
history information. Comprehensive benchmark results illustrate challenges and insights of embodied planning. LangSuitE represents a significant step toward building embodied generalists in the context of language models.

------------

`[2406.16299] Compensate Quantization Errors: Make Weights Hierarchical to Compensate Each Other <https://arxiv.org/abs/2406.16299>`__ 

::

    Mon, 24 Jun 2024 03:52:52 GMT
    Yifei Gao, Jie Ou, Lei Wang, Yuting Xiao, Zhiyuan Xiang, Ruiting Dai, Jun Cheng

Emergent Large Language Models (LLMs) use their extraordinary performance and powerful deduction capacity to discern from traditional language models.
However, the expenses of computational resources and storage for these LLMs are stunning, quantization then arises as a trending conversation. To address accuracy decay caused by quantization, two streams of works in post-training quantization methods stand out. One uses other weights to compensate existing quantization error, while the other transfers the quantization difficulty to other parts in the model. Combining both merits, we introduce Learnable Singular value Increment (LSI) as an advanced solution. LSI uses Singular Value Decomposition to extract singular values of the weights and make them learnable to help weights compensate each other conditioned on activation. Incorporating LSI with existing techniques, we achieve state-of-the-art performance in diverse quantization settings, no matter in weight-only, weight-activation or extremely low bit scenarios. By unleashing the potential of LSI, efficient finetuning on quantized model is no longer a prohibitive problem.

------------

`[2406.16330] Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging <https://arxiv.org/abs/2406.16330>`__ 通过合并修剪:通过基于流形对齐的层合并压缩llm

::

    Mon, 24 Jun 2024 05:57:55 GMT
    Deyuan Liu, Zhanyue Qin, Hairu Wang, Zhao Yang, Zecheng Wang, Fangying Rong, Qingbin Liu, Yanchao Hao, Xi Chen, Cunhang Fan, Zhao Lv, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui

While large language models (LLMs) excel in many domains, their complexity and scale challenge deployment in resource-limited environments. Current compression techniques, such as parameter pruning, often fail to effectively utilize the knowledge from pruned parameters. To address these challenges, we propose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA), a novel approach that uses manifold learning and the Normalized Pairwise Information Bottleneck (NPIB) measure to merge similar layers, reducing model size while preserving essential performance. We evaluate MKA on multiple benchmark datasets and various LLMs. Our findings show that MKA not only preserves model performance but also achieves substantial compression ratios, outperforming traditional pruning methods. Moreover, when coupled with quantization, MKA delivers even greater compression. Specifically, on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75% with a minimal performance decrease of only 2.82\%. The proposed MKA method offers a resource-efficient and performance-preserving model compression technique for LLMs.

------------

`[2406.16341] EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records <https://arxiv.org/abs/2406.16341>`__ EHRCon:用于检查电子健康记录中非结构化笔记和结构化表之间一致性的数据集

::

    Mon, 24 Jun 2024 06:26:50 GMT
    Yeonsu Kwon, Jiho Kim, Gyubok Lee, Seongsu Bae, Daeun Kyung, Wonchul Cha, Tom Pollard, Alistair Johnson, Edward Choi

Electronic Health Records (EHRs) are integral for storing comprehensive patient medical records, combining structured data (e.g., medications) with detailed clinical notes (e.g., physician notes). These elements are essential for straightforward data retrieval and provide deep, contextual insights into patient care. However, they often suffer from discrepancies due to unintuitive EHR system designs and human errors, posing serious risks to patient safety. To address this, we developed EHRCon, a new dataset and task specifically designed to ensure data consistency between structured tables and unstructured notes in EHRs. EHRCon was crafted in collaboration with healthcare professionals using the MIMIC-III EHR dataset, and includes manual annotations of 3,943 entities across 105 clinical notes checked against database entries for consistency.
EHRCon has two versions, one using the original MIMIC-III schema, and another using the OMOP CDM schema, in order to increase its applicability and generalizability. Furthermore, leveraging the capabilities of large language models, we introduce CheckEHR, a novel framework for verifying the consistency between clinical notes and database tables. CheckEHR utilizes an eight-stage process and shows promising results in both few-shot and zero-shot settings.
The code is available at https://github.com/dustn1259/EHRCon.

------------

`[2406.16356] Evaluation of Instruction-Following Ability for Large Language Models on Story-Ending Generation <https://arxiv.org/abs/2406.16356>`__ 大型语言模型在故事结尾生成中的指令跟随能力评估

::

    Mon, 24 Jun 2024 06:53:36 GMT
    Rem Hida, Junki Ohmura, Toshiyuki Sekiya

Instruction-tuned Large Language Models (LLMs) have achieved remarkable performance across various benchmark tasks. While providing instructions to LLMs for guiding their generations is user-friendly, assessing their instruction-following capabilities is still unclarified due to a lack of evaluation metrics. In this paper, we focus on evaluating the instruction-following ability of LLMs in the context of story-ending generation, which requires diverse and context-specific instructions. We propose an automatic evaluation pipeline that utilizes a machine reading comprehension (MRC) model to determine whether the generated story-ending reflects instruction. Our findings demonstrate that our proposed metric aligns with human evaluation. Furthermore, our experiments confirm that recent open-source LLMs can achieve instruction-following performance close to GPT-3.5, as assessed through automatic evaluation.

------------

`[2406.16377] On the Transformations across Reward Model, Parameter Update, and In-Context Prompt <https://arxiv.org/abs/2406.16377>`__ 关于奖励模型、参数更新和上下文提示的转换

::

    Mon, 24 Jun 2024 07:42:32 GMT
    Deng Cai and Huayang Li and Tingchen Fu and Siheng Li and Weiwen Xu and Shuaiyi Li and Bowen Cao and Zhisong Zhang and Xinting Huang and Leyang Cui and Yan Wang and Lemao Liu and Taro Watanabe and Shuming Shi

Despite the general capabilities of pre-trained large language models (LLMs), they still need further adaptation to better serve practical applications. In this paper, we demonstrate the interchangeability of three popular and distinct adaptation tools: parameter updating, reward modeling, and in-context prompting. This interchangeability establishes a triangular framework with six transformation directions, each of which facilitates a variety of applications.
Our work offers a holistic view that unifies numerous existing studies and suggests potential research directions. We envision our work as a useful roadmap for future research on LLMs.

------------

`[2406.16382] UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models <https://arxiv.org/abs/2406.16382>`__ 大型语言模型序列决策能力评估UNO Arena

::

    Mon, 24 Jun 2024 07:47:34 GMT
    Zhanyue Qin, Haochuan Wang, Deyuan Liu, Ziyang Song, Cunhang Fan, Zhao Lv, Jinlin Wu, Zhen Lei, Zhiying Tu, Dianhui Chu, Xiaoyan Yu, Dianbo Sui

Sequential decision-making refers to algorithms that take into account the dynamics of the environment, where early decisions affect subsequent decisions.
With large language models (LLMs) demonstrating powerful capabilities between tasks, we can't help but ask: Can Current LLMs Effectively Make Sequential Decisions? In order to answer this question, we propose the UNO Arena based on the card game UNO to evaluate the sequential decision-making capability of LLMs and explain in detail why we choose UNO. In UNO Arena, We evaluate the sequential decision-making capability of LLMs dynamically with novel metrics based Monte Carlo methods. We set up random players, DQN-based reinforcement learning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison testing. Furthermore, in order to improve the sequential decision-making capability of LLMs, we propose the TUTRI player, which can involves having LLMs reflect their own actions wtih the summary of game history and the game strategy. Numerous experiments demonstrate that the TUTRI player achieves a notable breakthrough in the performance of sequential decision-making compared to the vanilla LLM player.

------------

`[2406.16416] Multilingual Knowledge Editing with Language-Agnostic Factual Neurons <https://arxiv.org/abs/2406.16416>`__ 基于语言无关事实神经元的多语言知识编辑

::

    Mon, 24 Jun 2024 08:06:56 GMT
    Xue zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Yufeng Chen, Jinan Xu, Jie Zhou

Multilingual knowledge editing (MKE) aims to simultaneously revise factual knowledge across multilingual languages within large language models (LLMs).
However, most existing MKE methods just adapt existing monolingual editing methods to multilingual scenarios, overlooking the deep semantic connections of the same factual knowledge between different languages, thereby limiting edit performance. To address this issue, we first investigate how LLMs represent multilingual factual knowledge and discover that the same factual knowledge in different languages generally activates a shared set of neurons, which we call language-agnostic factual neurons. These neurons represent the semantic connections between multilingual knowledge and are mainly located in certain layers. Inspired by this finding, we propose a new MKE method by locating and modifying Language-Agnostic Factual Neurons (LAFN) to simultaneously edit multilingual knowledge. Specifically, we first generate a set of paraphrases for each multilingual knowledge to be edited to precisely locate the corresponding language-agnostic factual neurons. Then we optimize the update values for modifying these located neurons to achieve simultaneous modification of the same factual knowledge in multiple languages. Experimental results on Bi-ZsRE and MzsRE benchmarks demonstrate that our method outperforms existing MKE methods and achieves remarkable edit performance, indicating the importance of considering the semantic connections among multilingual knowledge.

------------

`[2406.16441] UniCoder: Scaling Code Large Language Model via Universal Code <https://arxiv.org/abs/2406.16441>`__ 

::

    Mon, 24 Jun 2024 08:32:48 GMT
    Tao Sun, Linzheng Chai, Jian Yang, Yuwei Yin, Hongcheng Guo, Jiaheng Liu, Bing Wang, Liqun Yang, Zhoujun Li

Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks. When applying LLMs for code generation, recent works mainly focus on directing the models to articulate intermediate natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and then output code with the natural language or other structured intermediate steps. However, such output is not suitable for code translation or generation tasks since the standard CoT has different logical structures and forms of expression with the code. In this work, we introduce the universal code (UniCode) as the intermediate representation. It is a description of algorithm steps using a mix of conventions of programming languages, such as assignment operator, conditional operator, and loop. Hence, we collect an instruction dataset UniCoder-Instruct to train our model UniCoder on multi-task learning objectives. UniCoder-Instruct comprises natural-language questions, code solutions, and the corresponding universal code. The alignment between the intermediate universal code representation and the final code solution significantly improves the quality of the generated code. The experimental results demonstrate that UniCoder with the universal code significantly outperforms the previous prompting methods by a large margin, showcasing the effectiveness of the structural clues in pseudo-code.

------------

`[2406.16508] Large Vocabulary Size Improves Large Language Models <https://arxiv.org/abs/2406.16508>`__ 大词汇量改进大型语言模型

::

    Mon, 24 Jun 2024 10:27:07 GMT
    Sho Takase, Ryokan Ri, Shun Kiyono, Takuya Kato

This paper empirically investigates the relationship between subword vocabulary size and the performance of large language models (LLMs) to provide insights on how to define the vocabulary size. Experimental results show that larger vocabulary sizes lead to better performance in LLMs. Moreover, we consider a continual training scenario where a pre-trained language model is trained on a different target language. We introduce a simple method to use a new vocabulary instead of the pre-defined one. We show that using the new vocabulary outperforms the model with the vocabulary used in pre-training.

------------

`[2406.16528] Evaluating the Ability of Large Language Models to Reason about Cardinal Directions <https://arxiv.org/abs/2406.16528>`__ 大型语言模型对基本方向推理的能力评估

::

    Mon, 24 Jun 2024 11:07:01 GMT
    Anthony G Cohn and Robert E Blackwell

We investigate the abilities of a representative set of Large language Models (LLMs) to reason about cardinal directions (CDs). To do so, we create two datasets: the first, co-created with ChatGPT, focuses largely on recall of world knowledge about CDs; the second is generated from a set of templates, comprehensively testing an LLM's ability to determine the correct CD given a particular scenario. The templates allow for a number of degrees of variation such as means of locomotion of the agent involved, and whether set in the first , second or third person. Even with a temperature setting of zero, Our experiments show that although LLMs are able to perform well in the simpler dataset, in the second more complex dataset no LLM is able to reliably determine the correct CD, even with a temperature setting of zero.

------------

`[2406.16536] C-LLM: Learn to Check Chinese Spelling Errors Character by Character <https://arxiv.org/abs/2406.16536>`__ 学习逐字检查中文拼写错误

::

    Mon, 24 Jun 2024 11:16:31 GMT
    Kunting Li, Yong Hu, Liang He, Fandong Meng, Jie Zhou

Chinese Spell Checking (CSC) aims to detect and correct spelling errors in sentences. Despite Large Language Models (LLMs) exhibit robust capabilities and are widely applied in various tasks, their performance on CSC is often unsatisfactory. We find that LLMs fail to meet the Chinese character-level constraints of the CSC task, namely equal length and phonetic similarity, leading to a performance bottleneck. Further analysis reveal that this issue stems from the granularity of tokenization, as current mixed character-word tokenization struggles to satisfy these character-level constraints. To address this issue, we propose C-LLM, a Large Language Model-based Chinese Spell Checking method that learns to check errors Character by Character.
Character-level tokenization enables the model to learn character-level alignment, effectively mitigating issues related to character-level constraints. Furthermore, CSC is simplified to replication-dominated and substitution-supplemented tasks. Experiments on two CSC benchmarks demonstrate that C-LLM achieves an average improvement of 10% over existing methods.
Specifically, it shows a 2.1% improvement in general scenarios and a significant 12% improvement in vertical domain scenarios, establishing state-of-the-art performance. The source code can be accessed at https://github.com/ktlKTL/C-LLM.

------------

`[2406.16554] LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training <https://arxiv.org/abs/2406.16554>`__ LLaMA- moe:通过持续的预训练建立来自LLaMA的专家混合

::

    Mon, 24 Jun 2024 11:43:07 GMT
    Tong Zhu,Xiaoye Qu,Daize Dong,Jiacheng Ruan,Jingqi Tong,Conghui He,Yu Cheng

Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, training MoE from scratch in a large-scale setting still suffers from data-hungry and instability problems. Motivated by this limit, we investigate building MoE models from existing dense large language models. Specifically, based on the well-known LLaMA-2 7B model, we obtain an MoE model by: (1) Expert Construction, which partitions the parameters of original Feed-Forward Networks (FFNs) into multiple experts; (2) Continual Pre-training, which further trains the transformed MoE model and additional gate networks. In this paper, we comprehensively explore different methods for expert construction and various data sampling strategies for continual pre-training. After these stages, our LLaMA-MoE models could maintain language abilities and route the input tokens to specific experts with part of the parameters activated. Empirically, by training 200B tokens, LLaMA-MoE-3.5B models significantly outperform dense models that contain similar activation parameters. The source codes and models are available at https://github.com/pjlab-sys4nlp/llama-moe .

------------

`[2406.16567] Data Augmentation of Multi-turn Psychological Dialogue via Knowledge-driven Progressive Thought Prompting <https://arxiv.org/abs/2406.16567>`__ 基于知识驱动渐进式思维提示的多轮心理对话数据增强

::

    Mon, 24 Jun 2024 12:02:56 GMT
    Jiyue Jiang, Liheng Chen, Sheng Wang, Lingpeng Kong, Yu Li, and Chuan Wu

Existing dialogue data augmentation (DA) techniques predominantly focus on augmenting utterance-level dialogues, which makes it difficult to take dialogue contextual information into account. The advent of large language models (LLMs) has simplified the implementation of multi-turn dialogues. Due to absence of professional understanding and knowledge, it remains challenging to deliver satisfactory performance in low-resource domain, like psychological dialogue dialogue. DA involves creating new training or prompting data based on the existing data, which help the model better understand and generate psychology-related responses. In this paper, we aim to address the issue of multi-turn dialogue data augmentation for boosted performance in the psychology domain. We propose a knowledge-driven progressive thought prompting method to guide LLM to generate multi-turn psychology-related dialogue. This method integrates a progressive thought generator, a psychology knowledge generator, and a multi-turn dialogue generator. The thought generated by the progressive thought generator serves as a prompt to prevent the generated dialogue from having significant semantic deviations, while the psychology knowledge generator produces psychological knowledge to serve as the dialogue history for the LLM, guiding the dialogue generator to create multi-turn psychological dialogue. To ensure the precision of multi-turn psychological dialogue generation by LLM, a meticulous professional evaluation is required. Extensive experiments conducted on three datasets related to psychological dialogue verify the effectiveness of the proposed method.

------------

`[2406.16655] Large Language Models Are Cross-Lingual Knowledge-Free Reasoners <https://arxiv.org/abs/2406.16655>`__ 

::

    Mon, 24 Jun 2024 14:03:04 GMT
    Peng Hu, Sizhe Liu, Changjiang Gao, Xin Huang, Xue Han, Junlan Feng, Chao Deng, and Shujian Huang

Large Language Models have demonstrated impressive reasoning capabilities across multiple languages. However, the relationship between capabilities in different languages is less explored. In this work, we decompose the process of reasoning tasks into two separated parts: knowledge retrieval and knowledge-free reasoning, and analyze the cross-lingual transferability of them. With adapted and constructed knowledge-free reasoning datasets, we show that the knowledge-free reasoning capability can be nearly perfectly transferred across various source-target language directions despite the secondary impact of resource in some specific target languages, while cross-lingual knowledge retrieval significantly hinders the transfer. Moreover, by analyzing the hidden states and feed-forward network neuron activation during the reasoning tasks, we show that higher similarity of hidden representations and larger overlap of activated neurons could explain the better cross-lingual transferability of knowledge-free reasoning than knowledge retrieval. Thus, we hypothesize that knowledge-free reasoning embeds in some language-shared mechanism, while knowledge is stored separately in different languages.

------------

`[2406.16690] Scaling Laws for Linear Complexity Language Models <https://arxiv.org/abs/2406.16690>`__ 线性复杂性语言模型的缩放律

::

    Mon, 24 Jun 2024 14:51:31 GMT
    Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, Yiran Zhong

The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, we examine the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay.
We also include LLaMA as a baseline architecture for softmax attention for comparison. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention.

------------

`[2406.16694] Task Oriented In-Domain Data Augmentation <https://arxiv.org/abs/2406.16694>`__ 面向任务的域内数据增强

::

    Mon, 24 Jun 2024 14:58:11 GMT
    Xiao Liang, Xinyu Hu, Simiao Zuo, Yeyun Gong, Qiang Lou, Yi Liu, Shao-Lun Huang, Jian Jiao

Large Language Models (LLMs) have shown superior performance in various applications and fields. To achieve better performance on specialized domains such as law and advertisement, LLMs are often continue pre-trained on in-domain data. However, existing approaches suffer from two major issues. First, in-domain data are scarce compared with general domain-agnostic data. Second, data used for continual pre-training are not task-aware, such that they may not be helpful to downstream applications. We propose TRAIT, a task-oriented in-domain data augmentation framework. Our framework is divided into two parts: in-domain data selection and task-oriented synthetic passage generation. The data selection strategy identifies and selects a large amount of in-domain data from general corpora, and thus significantly enriches domain knowledge in the continual pre-training data. The synthetic passages contain guidance on how to use domain knowledge to answer questions about downstream tasks. By training on such passages, the model aligns with the need of downstream applications. We adapt LLMs to two domains: advertisement and math. On average, TRAIT improves LLM performance by 8% in the advertisement domain and 7.5% in the math domain.

------------

`[2406.16714] AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models <https://arxiv.org/abs/2406.16714>`__ AutoDetect:面向大型语言模型自动缺陷检测的统一框架

::

    Mon, 24 Jun 2024 15:16:45 GMT
    Jiale Cheng, Yida Lu, Xiaotao Gu, Pei Ke, Xiao Liu, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang

Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction-following or coding tasks. As these unexpected errors could lead to severe consequences in practical deployments, it is crucial to investigate the limitations within LLMs systematically. Traditional benchmarking approaches cannot thoroughly pinpoint specific model deficiencies, while manual inspections are costly and not scalable. In this paper, we introduce a unified framework, AutoDetect, to automatically expose weaknesses in LLMs across various tasks. Inspired by the educational assessment process that measures students' learning outcomes, AutoDetect consists of three LLM-powered agents: Examiner, Questioner, and Assessor. The collaboration among these three agents is designed to realize comprehensive and in-depth weakness identification. Our framework demonstrates significant success in uncovering flaws, with an identification success rate exceeding 30% in prominent models such as ChatGPT and Claude. More importantly, these identified weaknesses can guide specific model improvements, proving more effective than untargeted data augmentation methods like Self-Instruct. Our approach has led to substantial enhancements in popular LLMs, including the Llama series and Mistral-7b, boosting their performance by over 10% across several benchmarks. Code and data are publicly available at https://github.com/thu-coai/AutoDetect.

------------

`[2406.16767] The GPT-WritingPrompts Dataset: A Comparative Analysis of Character Portrayal in Short Stories <https://arxiv.org/abs/2406.16767>`__ 

::

    Mon, 24 Jun 2024 16:24:18 GMT
    Xi Yu Huang and Krishnapriya Vishnubhotla and Frank Rudzicz

The improved generative capabilities of large language models have made them a powerful tool for creative writing and storytelling. It is therefore important to quantitatively understand the nature of generated stories, and how they differ from human storytelling. We augment the Reddit WritingPrompts dataset with short stories generated by GPT-3.5, given the same prompts. We quantify and compare the emotional and descriptive features of storytelling from both generative processes, human and machine, along a set of six dimensions. We find that generated stories differ significantly from human stories along all six dimensions, and that human and machine generations display similar biases when grouped according to the narrative point-of-view and gender of the main protagonist. We release our dataset and code at https://github.com/KristinHuangg/gpt-writing-prompts.

------------

`[2406.16777] Blending LLMs into Cascaded Speech Translation: KIT's Offline Speech Translation System for IWSLT 2024 <https://arxiv.org/abs/2406.16777>`__ 将llm融合到级联语音翻译中:KIT的IWSLT 2024离线语音翻译系统

::

    Mon, 24 Jun 2024 16:38:17 GMT
    Sai Koneru, Thai-Binh Nguyen, Ngoc-Quan Pham, Danni Liu, Zhaolin Li, Alexander Waibel, Jan Niehues

Large Language Models (LLMs) are currently under exploration for various tasks, including Automatic Speech Recognition (ASR), Machine Translation (MT), and even End-to-End Speech Translation (ST). In this paper, we present KIT's offline submission in the constrained + LLM track by incorporating recently proposed techniques that can be added to any cascaded speech translation.
Specifically, we integrate Mistral-7B\footnote{mistralai/Mistral-7B-Instruct-v0.1} into our system to enhance it in two ways. Firstly, we refine the ASR outputs by utilizing the N-best lists generated by our system and fine-tuning the LLM to predict the transcript accurately. Secondly, we refine the MT outputs at the document level by fine-tuning the LLM, leveraging both ASR and MT predictions to improve translation quality. We find that integrating the LLM into the ASR and MT systems results in an absolute improvement of $0.3\%$ in Word Error Rate and $0.65\%$ in COMET for tst2019 test set. In challenging test sets with overlapping speakers and background noise, we find that integrating LLM is not beneficial due to poor ASR performance. Here, we use ASR with chunked long-form decoding to improve context usage that may be unavailable when transcribing with Voice Activity Detection segmentation alone.

------------

`[2406.16779] It Is Not About What You Say, It Is About How You Say It: A Surprisingly Simple Approach for Improving Reading Comprehension <https://arxiv.org/abs/2406.16779>`__ 关键不在于你说了什么，而在于你怎么说:这是一种提高阅读理解能力的非常简单的方法

::

    Mon, 24 Jun 2024 16:43:11 GMT
    Sagi Shaier, Lawrence E Hunter, Katharina von der Wense

Natural language processing has seen rapid progress over the past decade. Due to the speed of developments, some practices get established without proper evaluation. Considering one such case and focusing on reading comprehension, we ask our first research question: 1) How does the order of inputs -- i.e., question and context -- affect model performance? Additionally, given recent advancements in input emphasis, we ask a second research question: 2) Does emphasizing either the question, the context, or both enhance performance? Experimenting with 9 large language models across 3 datasets, we find that presenting the context before the question improves model performance, with an accuracy increase of up to $31\%$. Furthermore, emphasizing the context yields superior results compared to question emphasis, and in general, emphasizing parts of the input is particularly effective for addressing questions that models lack the parametric knowledge to answer. Experimenting with both prompt-based and attention-based emphasis methods, we additionally find that the best method is surprisingly simple: it only requires concatenating a few tokens to the input and results in an accuracy improvement of up to $36\%$, allowing smaller models to outperform their significantly larger counterparts.

------------

`[2406.16783] M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models <https://arxiv.org/abs/2406.16783>`__ M2Lingual:增强大型语言模型多语言多回合指令对齐

::

    Mon, 24 Jun 2024 16:45:13 GMT
    Rishabh Maheshwary and Vikas Yadav and Hoang Nguyen and Khyati Mahajan and Sathwik Tejaswi Madhusudhan

Instruction finetuning (IFT) is critical for aligning Large Language Models (LLMs) to follow instructions. Numerous effective IFT datasets have been proposed in the recent past, but most focus on high resource languages such as English. In this work, we propose a fully synthetic, novel taxonomy (Evol) guided Multilingual, Multi-turn instruction finetuning dataset, called M2Lingual, to better align LLMs on a diverse set of languages and tasks.
M2Lingual contains a total of 182K IFT pairs that are built upon diverse seeds, covering 70 languages, 17 NLP tasks and general instruction-response pairs.
LLMs finetuned with M2Lingual substantially outperform the majority of existing multilingual IFT datasets. Importantly, LLMs trained with M2Lingual consistently achieve competitive results across a wide variety of evaluation benchmarks compared to existing multilingual IFT datasets. Specifically, LLMs finetuned with M2Lingual achieve strong performance on our translated multilingual, multi-turn evaluation benchmark as well as a wide variety of multilingual tasks. Thus we contribute, and the 2 step Evol taxonomy used for its creation. M2Lingual repository - https://huggingface.co/datasets/ServiceNow-AI/M2Lingual

------------

`[2406.16797] Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs <https://arxiv.org/abs/2406.16797>`__ 彩票适配:降低llm中的破坏性干扰

::

    Mon, 24 Jun 2024 16:58:23 GMT
    Ashwinee Panda, Berivan Isik, Xiangyu Qi, Sanmi Koyejo, Tsachy Weissman, Prateek Mittal

Existing methods for adapting large language models (LLMs) to new tasks are not suited to multi-task adaptation because they modify all the model weights -- causing destructive interference between tasks. The resulting effects, such as catastrophic forgetting of earlier tasks, make it challenging to obtain good performance on multiple tasks at the same time. To mitigate this, we propose Lottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies and optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide range of challenging tasks such as instruction following, reasoning, math, and summarization. LoTA obtains better performance than full fine-tuning and low-rank adaptation (LoRA), and maintains good performance even after training on other tasks -- thus, avoiding catastrophic forgetting. By extracting and fine-tuning over \emph{lottery tickets} (or \emph{sparse task vectors}), LoTA also enables model merging over highly dissimilar tasks.

------------

`[2406.16801] RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale <https://arxiv.org/abs/2406.16801>`__ RES-Q:评估存储库规模的大型语言模型系统的代码编辑

::

    Mon, 24 Jun 2024 17:08:17 GMT
    Beck LaBash, August Rosedale, Alex Reents, Colin Wiel

The instruction-following ability of Large Language Models (LLMs) has cultivated a class of LLM-based systems capable of approaching complex tasks such as making edits to large code repositories. Due to the high sensitivity and unpredictability of LLM behavior in response to changes in prompting, robust evaluation tools are needed to drive future iteration of these systems.
We propose RES-Q, a natural language instruction-based benchmark for evaluating $\textbf{R}$epository $\textbf{E}$diting $\textbf{S}$ystems, which consists of 100 repository editing tasks derived from real GitHub commits. Given an edit instruction and a code repository, RES-Q evaluates an LLM system's ability to gather information and construct an edit that satisfies the criteria set by the instruction. We argue that evaluating LLMs in this way addresses issues with traditional benchmarks and provides a more holistic assessment of a model's abilities. We evaluate various state-of-the-art LLMs as language agents in a repository-editing system built on Qurrent OS, our language agent development software. Despite their 1% pass@1 performance difference on HumanEval, we find Claude Sonnet 3.5 outperforms GPT-4o by 12% pass@1 on RES-Q, indicating RES-Q's capacity to differentiate model capability as traditional benchmarks approach saturation. We further investigate token efficiency, performance relationships with existing benchmarks, and interesting disparities between closed and open-source LLMs. Code and dataset are available at https://github.com/Qurrent-AI/RES-Q.

------------

`[2406.16833] USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$onversations <https://arxiv.org/abs/2406.16833>`__ USDC: $\underline{U}$ser $\underline{S}$tance和$\underline{D}$ogmatism组成的数据集

::

    Mon, 24 Jun 2024 17:41:53 GMT
    Mounika Marreddy, Subba Reddy Oota, Venkata Charan Chinni, Manish Gupta, Lucie Flek

Identifying user's opinions and stances in long conversation threads on various topics can be extremely critical for enhanced personalization, market research, political campaigns, customer service, conflict resolution, targeted advertising, and content moderation. Hence, training language models to automate this task is critical. However, to train such models, gathering manual annotations has multiple challenges: 1) It is time-consuming and costly; 2) Conversation threads could be very long, increasing chances of noisy annotations; and 3) Interpreting instances where a user changes their opinion within a conversation is difficult because often such transitions are subtle and not expressed explicitly. Inspired by the recent success of large language models (LLMs) for complex natural language processing (NLP) tasks, we leverage Mistral Large and GPT-4 to automate the human annotation process on the following two tasks while also providing reasoning: i) User Stance classification, which involves labeling a user's stance of a post in a conversation on a five-point scale; ii) User Dogmatism classification, which deals with labeling a user's overall opinion in the conversation on a four-point scale. The majority voting on zero-shot, one-shot, and few-shot annotations from these two LLMs on 764 multi-user Reddit conversations helps us curate the USDC dataset. USDC is then used to finetune and instruction-tune multiple deployable small language models for the 5-class stance and 4-class dogmatism classification tasks. We make the code and dataset publicly available [https://anonymous.4open.science/r/USDC-0F7F].

------------

`[2406.16858] EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees <https://arxiv.org/abs/2406.16858>`__ EAGLE-2:基于动态草案树的语言模型快速推理

::

    Mon, 24 Jun 2024 17:59:11 GMT
    Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang

Inference with modern Large Language Models (LLMs) is expensive and time-consuming, and speculative sampling has proven to be an effective solution. Most speculative sampling methods such as EAGLE use a static draft tree, implicitly assuming that the acceptance rate of draft tokens depends only on their position. Interestingly, we found that the acceptance rate of draft tokens is also context-dependent. In this paper, building upon EAGLE, we propose EAGLE-2, which introduces a new technique of context-aware dynamic draft tree into drafting modeling. This improvement leverages the fact that the draft model of EAGLE is well-calibrated: the confidence scores from the draft model approximate acceptance rates with small errors. We conducted extensive evaluations on three series of LLMs and six tasks, with EAGLE-2 achieving speedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also ensures that the distribution of the generated text remains unchanged, making it a lossless acceleration algorithm.

------------

`[2406.15534] Geneverse: A collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research <https://arxiv.org/abs/2406.15534>`__ Geneverse:用于基因组和蛋白质组学研究的开源多模态大型语言模型集合

::

    Fri, 21 Jun 2024 14:19:10 GMT
    Tianyu Liu, Yijia Xiao, Xiao Luo, Hua Xu, W. Jim Zheng, Hongyu Zhao

The applications of large language models (LLMs) are promising for biomedical and healthcare research. Despite the availability of open-source LLMs trained using a wide range of biomedical data, current research on the applications of LLMs to genomics and proteomics is still limited. To fill this gap, we propose a collection of finetuned LLMs and multimodal LLMs (MLLMs), known as Geneverse, for three novel tasks in genomic and proteomic research. The models in Geneverse are trained and evaluated based on domain-specific datasets, and we use advanced parameter-efficient finetuning techniques to achieve the model adaptation for tasks including the generation of descriptions for gene functions, protein function inference from its structure, and marker gene selection from spatial transcriptomic data. We demonstrate that adapted LLMs and MLLMs perform well for these tasks and may outperform closed-source large-scale models based on our evaluations focusing on both truthfulness and structural correctness. All of the training strategies and base models we used are freely accessible.

------------

`[2406.15568] Robust Reinforcement Learning from Corrupted Human Feedback <https://arxiv.org/abs/2406.15568>`__ 基于错误人类反馈的鲁棒强化学习

::

    Fri, 21 Jun 2024 18:06:30 GMT
    Alexander Bukharin, Ilgee Hong, Haoming Jiang, Qingru Zhang, Zixuan Zhang, Tuo Zhao

Reinforcement learning from human feedback (RLHF) provides a principled framework for aligning AI systems with human preference data. For various reasons, e.g., personal bias, context ambiguity, lack of training, etc, human annotators may give incorrect or inconsistent preference labels. To tackle this challenge, we propose a robust RLHF approach -- $R^3M$, which models the potentially corrupted preference label as sparse outliers. Accordingly, we formulate the robust reward learning as an $\ell_1$-regularized maximum likelihood estimation problem. Computationally, we develop an efficient alternating optimization algorithm, which only incurs negligible computational overhead compared with the standard RLHF approach. Theoretically, we prove that under proper regularity conditions, $R^3M$ can consistently learn the underlying reward and identify outliers, provided that the number of outlier labels scales sublinearly with the preference sample size. Furthermore, we remark that $R^3M$ is versatile and can be extended to various preference optimization methods, including direct preference optimization (DPO). Our experiments on robotic control and natural language generation with large language models (LLMs) show that $R^3M$ improves robustness of the reward against several types of perturbations to the preference data.

------------

`[2406.15763] AllMatch: Exploiting All Unlabeled Data for Semi-Supervised Learning <https://arxiv.org/abs/2406.15763>`__ AllMatch:利用所有未标记数据进行半监督学习

::

    Sat, 22 Jun 2024 06:59:52 GMT
    Zhiyu Wu, Jinshi Cui

Existing semi-supervised learning algorithms adopt pseudo-labeling and consistency regulation techniques to introduce supervision signals for unlabeled samples. To overcome the inherent limitation of threshold-based pseudo-labeling, prior studies have attempted to align the confidence threshold with the evolving learning status of the model, which is estimated through the predictions made on the unlabeled data. In this paper, we further reveal that classifier weights can reflect the differentiated learning status across categories and consequently propose a class-specific adaptive threshold mechanism. Additionally, considering that even the optimal threshold scheme cannot resolve the problem of discarding unlabeled samples, a binary classification consistency regulation approach is designed to distinguish candidate classes from negative options for all unlabeled samples. By combining the above strategies, we present a novel SSL algorithm named AllMatch, which achieves improved pseudo-label accuracy and a 100\% utilization ratio for the unlabeled data. We extensively evaluate our approach on multiple benchmarks, encompassing both balanced and imbalanced settings. The results demonstrate that AllMatch consistently outperforms existing state-of-the-art methods.

------------

`[2406.15765] Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration <https://arxiv.org/abs/2406.15765>`__ 

::

    Sat, 22 Jun 2024 07:00:43 GMT
    Zhongzhi Yu, Zheng Wang, Yonggan Fu, Huihong Shi, Khalid Shaikh, Yingyan Celine Lin

Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established, remains limited. Inspired by recent studies that explore the presence of attention sink in the initial token, which receives disproportionately large attention scores despite their lack of semantic importance, this work delves deeper into this phenomenon. We aim to provide a more profound understanding of the existence of attention sinks within LLMs and to uncover ways to enhance the achievable accuracy of LLMs by directly optimizing the attention distributions, without the need for weight finetuning.
Specifically, this work begins with comprehensive visualizations of the attention distributions in LLMs during inference across various inputs and tasks. Based on these visualizations, to the best of our knowledge, we are the first to discover that (1) attention sinks occur not only at the start of sequences but also within later tokens of the input, and (2) not all attention sinks have a positive impact on the achievable accuracy of LLMs. Building upon our findings, we propose a training-free Attention Calibration Technique (ACT) that automatically optimizes the attention distributions on the fly during inference in an input-adaptive manner. Extensive experiments validate that ACT consistently enhances the accuracy of various LLMs across different applications. Specifically, ACT achieves an average improvement of up to 7.30% in accuracy across different datasets when applied to Llama-30B. Our code is available at https://github.com/GATECH-EIC/ACT.

------------

`[2406.15786] What Matters in Transformers? Not All Attention is Needed <https://arxiv.org/abs/2406.15786>`__ transformer中什么是重要的?并非所有的注意力都是需要的

::

    Sat, 22 Jun 2024 08:41:48 GMT
    Shwai He, Guoheng Sun, Zheyu Shen, Ang Li

Scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks. However, this scaling also introduces redundant structures, posing challenges for real-world deployment.
Despite some recognition of redundancy in LLMs, the variability of redundancy across different structures, such as MLP and Attention layers, is under-explored. In this work, we investigate the varying redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. This metric operates on the premise that redundant structures produce outputs highly similar to their inputs.
Surprisingly, while attention layers are essential for transformers and distinguish them from other mainstream architectures, we found that a large proportion of attention layers exhibit excessively high similarity and can be safely pruned without degrading performance, leading to reduced memory and computation costs. Additionally, we further propose a method that jointly drops Attention and MLP layers, achieving improved performance and dropping ratios.
Extensive experiments demonstrate the effectiveness of our methods, e.g., Llama-3-70B maintains comparable performance even after pruning half of the attention layers. Our findings provide valuable insights for future network architecture design. The code will be released at: \url{https://github.com/Shwai-He/LLM-Drop}.

------------

`[2406.15890] Language Alignment via Nash-learning and Adaptive feedback <https://arxiv.org/abs/2406.15890>`__ 基于纳什学习和自适应反馈的语言对齐

::

    Sat, 22 Jun 2024 16:55:21 GMT
    Ari Azarafrooz, Farshid Faal

Recent research has shown the potential of Nash Learning via Human Feedback for large language model alignment by incorporating the notion of a preference model in a minimax game setup. We take this idea further by casting the alignment as a mirror descent algorithm against the adaptive feedback of an improved opponent, thereby removing the need for learning a preference model or the existence of an annotated dataset altogether. The resulting algorithm, which we refer to as Language Alignment via Nash-learning and Adaptive feedback (LANA), is capable of self-alignment without the need for a human-annotated preference dataset. We support this statement with various experiments and mathematical discussion.

------------

`[2406.16252] Graph-Augmented LLMs for Personalized Health Insights: A Case Study in Sleep Analysis <https://arxiv.org/abs/2406.16252>`__ 面向个性化健康见解的图增强llm:睡眠分析案例研究

::

    Mon, 24 Jun 2024 01:22:54 GMT
    Ajan Subramanian, Zhongqi Yang, Iman Azimi and Amir M. Rahmani

Health monitoring systems have revolutionized modern healthcare by enabling the continuous capture of physiological and behavioral data, essential for preventive measures and early health intervention. While integrating this data with Large Language Models (LLMs) has shown promise in delivering interactive health advice, traditional methods like Retrieval-Augmented Generation (RAG) and fine-tuning often fail to fully utilize the complex, multi-dimensional, and temporally relevant data from wearable devices. These conventional approaches typically provide limited actionable and personalized health insights due to their inadequate capacity to dynamically integrate and interpret diverse health data streams. In response, this paper introduces a graph-augmented LLM framework designed to significantly enhance the personalization and clarity of health insights. Utilizing a hierarchical graph structure, the framework captures inter and intra-patient relationships, enriching LLM prompts with dynamic feature importance scores derived from a Random Forest Model. The effectiveness of this approach is demonstrated through a sleep analysis case study involving 20 college students during the COVID-19 lockdown, highlighting the potential of our model to generate actionable and personalized health insights efficiently. We leverage another LLM to evaluate the insights for relevance, comprehensiveness, actionability, and personalization, addressing the critical need for models that process and interpret complex health data effectively. Our findings show that augmenting prompts with our framework yields significant improvements in all 4 criteria. Through our framework, we can elicit well-crafted, more thoughtful responses tailored to a specific patient.

------------

`[2406.16254] Confidence Regulation Neurons in Language Models <https://arxiv.org/abs/2406.16254>`__ 语言模型中的置信度调节神经元

::

    Mon, 24 Jun 2024 01:31:03 GMT
    Alessandro Stolfo, Ben Wu, Wes Gurnee, Yonatan Belinkov, Xingyi Song, Mrinmaya Sachan, Neel Nanda

Despite their widespread use, the mechanisms by which large language models (LLMs) represent and regulate uncertainty in next-token predictions remain largely unexplored. This study investigates two critical components believed to influence this uncertainty: the recently discovered entropy neurons and a new set of components that we term token frequency neurons. Entropy neurons are characterized by an unusually high weight norm and influence the final layer normalization (LayerNorm) scale to effectively scale down the logits. Our work shows that entropy neurons operate by writing onto an unembedding null space, allowing them to impact the residual stream norm with minimal direct effect on the logits themselves. We observe the presence of entropy neurons across a range of models, up to 7 billion parameters. On the other hand, token frequency neurons, which we discover and describe here for the first time, boost or suppress each token's logit proportionally to its log frequency, thereby shifting the output distribution towards or away from the unigram distribution.
Finally, we present a detailed case study where entropy neurons actively manage confidence in the setting of induction, i.e. detecting and continuing repeated subsequences.

------------

`[2406.16308] Anomaly Detection of Tabular Data Using LLMs <https://arxiv.org/abs/2406.16308>`__ 基于LLMs的表格数据异常检测

::

    Mon, 24 Jun 2024 04:17:03 GMT
    Aodong Li, Yunhan Zhao, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, Stephan Mandt

Large language models (LLMs) have shown their potential in long-context understanding and mathematical reasoning. In this paper, we study the problem of using LLMs to detect tabular anomalies and show that pre-trained LLMs are zero-shot batch-level anomaly detectors. That is, without extra distribution-specific model fitting, they can discover hidden outliers in a batch of data, demonstrating their ability to identify low-density data regions. For LLMs that are not well aligned with anomaly detection and frequently output factual errors, we apply simple yet effective data-generating processes to simulate synthetic batch-level anomaly detection datasets and propose an end-to-end fine-tuning strategy to bring out the potential of LLMs in detecting real anomalies. Experiments on a large anomaly detection benchmark (ODDS) showcase i) GPT-4 has on-par performance with the state-of-the-art transductive learning-based anomaly detection methods and ii) the efficacy of our synthetic dataset and fine-tuning strategy in aligning LLMs to this task.

------------

`[2406.16349] AnnotatedTables: A Large Tabular Dataset with Language Model Annotations <https://arxiv.org/abs/2406.16349>`__ 

::

    Mon, 24 Jun 2024 06:44:14 GMT
    Yaojie Hu, Ilias Fountalis, Jin Tian, Nikolaos Vasiloglou

Tabular data is ubiquitous in real-world applications and abundant on the web, yet its annotation has traditionally required human labor, posing a significant scalability bottleneck for tabular machine learning. Our methodology can successfully annotate a large amount of tabular data and can be flexibly steered to generate various types of annotations based on specific research objectives, as we demonstrate with SQL annotation and input-target column annotation as examples. As a result, we release AnnotatedTables, a collection of 32,119 databases with LLM-generated annotations. The dataset includes 405,616 valid SQL programs, making it the largest SQL dataset with associated tabular data that supports query execution. To further demonstrate the value of our methodology and dataset, we perform two follow-up research studies. 1) We investigate whether LLMs can translate SQL programs to Rel programs, a database language previously unknown to LLMs, while obtaining the same execution results. Using our Incremental Prompt Engineering methods based on execution feedback, we show that LLMs can produce adequate translations with few-shot learning. 2) We evaluate the performance of TabPFN, a recent neural tabular classifier trained on Bayesian priors, on 2,720 tables with input-target columns identified and annotated by LLMs. On average, TabPFN performs on par with the baseline AutoML method, though the relative performance can vary significantly from one data table to another, making both models viable for practical applications depending on the situation. Our findings underscore the potential of LLMs in automating the annotation of large volumes of diverse tabular data.

------------

`[2406.16635] ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models <https://arxiv.org/abs/2406.16635>`__ ShadowLLM:基于预测器的大型语言模型上下文稀疏性

::

    Mon, 24 Jun 2024 13:41:08 GMT
    Yash Akhauri, Ahmed F AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander M Rush, Safeen Huda, Mohamed S Abdelfattah

The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy. Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We developed a novel predictor called ShadowLLM, which can shadow the LLM behavior and enforce better sparsity patterns, resulting in over 15% improvement in end-to-end accuracy without increasing latency compared to previous methods.
ShadowLLM achieves up to a 20\% speed-up over the state-of-the-art DejaVu framework. These enhancements are validated on models with up to 30 billion parameters. Our code is available at \href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.

------------

`[2406.16738] Inducing Group Fairness in LLM-Based Decisions <https://arxiv.org/abs/2406.16738>`__ 基于llm决策的群体公平性诱导

::

    Mon, 24 Jun 2024 15:45:20 GMT
    James Atwood, Preethi Lahoti, Ananth Balashankar, Flavien Prost, Ahmad Beirami

Prompting Large Language Models (LLMs) has created new and interesting means for classifying textual data. While evaluating and remediating group fairness is a well-studied problem in classifier fairness literature, some classical approaches (e.g., regularization) do not carry over, and some new opportunities arise (e.g., prompt-based remediation). We measure fairness of LLM-based classifiers on a toxicity classification task, and empirically show that prompt-based classifiers may lead to unfair decisions. We introduce several remediation techniques and benchmark their fairness and performance trade-offs.
We hope our work encourages more research on group fairness in LLM-based classifiers.

------------

`[2406.16745] Bandits with Preference Feedback: A Stackelberg Game Perspective <https://arxiv.org/abs/2406.16745>`__ 基于偏好反馈的土匪:Stackelberg游戏视角

::

    Mon, 24 Jun 2024 15:53:11 GMT
    Barna P\'asztor, Parnian Kassraie, Andreas Krause

Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for fine-tuning large language models. The problem is well understood in simplified settings with linear target functions or over finite small domains that limit practical interest. Taking the next step, we consider infinite domains and nonlinear (kernelized) rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm. We propose MAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards.
MAXMINLCB consistently outperforms existing algorithms and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our novel preference-based confidence sequences for kernelized logistic estimators.

------------

`[2406.16748] OCALM: Object-Centric Assessment with Language Models <https://arxiv.org/abs/2406.16748>`__ OCALM:以对象为中心的语言模型评估

::

    Mon, 24 Jun 2024 15:57:48 GMT
    Timo Kaufmann, Jannis Bl\"uml, Antonia W\"ust, Quentin Delfosse, Kristian Kersting, Eyke H\"ullermeier

Properly defining a reward signal to efficiently train a reinforcement learning (RL) agent is a challenging task. Designing balanced objective functions from which a desired behavior can emerge requires expert knowledge, especially for complex environments. Learning rewards from human feedback or using large language models (LLMs) to directly provide rewards are promising alternatives, allowing non-experts to specify goals for the agent. However, black-box reward models make it difficult to debug the reward. In this work, we propose Object-Centric Assessment with Language Models (OCALM) to derive inherently interpretable reward functions for RL agents from natural language task descriptions. OCALM uses the extensive world-knowledge of LLMs while leveraging the object-centric nature common to many environments to derive reward functions focused on relational concepts, providing RL agents with the ability to derive policies from task descriptions.

------------

`[2406.16810] PISTOL: Dataset Compilation Pipeline for Structural Unlearning of LLMs <https://arxiv.org/abs/2406.16810>`__ 手枪:用于llm结构遗忘的数据集编译管道

::

    Mon, 24 Jun 2024 17:22:36 GMT
    Xinchi Qiu, William F. Shen, Yihong Chen, Nicola Cancedda, Pontus Stenetorp, Nicholas D. Lane

Recently, machine unlearning, which seeks to erase specific data stored in the pre-trained or fine-tuned models, has emerged as a crucial protective measure for LLMs. However, unlearning approaches for LLMs that have been considered thus far have focused on the removal of independent data points and have not taken into account that the stored facts are logically connected to one another and form an implicit knowledge graph. To facilitate the development of structural unlearning methods, which are essential for the practical application of unlearning, we propose PISTOL, a pipeline for compiling multi-scenario datasets for benchmarking structural LLM unlearning.
Additionally, leveraging sample datasets synthesized using PISTOL, we conducted benchmarks with four distinct unlearning methods on both Llama2-7B and Mistral-7B models. This analysis helps to illustrate the prevailing challenges in effectively and robustly removing highly inter-connected data, batched data, or data skewed towards a specific domain. It also highlights the choice of pre-trained model can impact unlearning performance. This work not only advances our understandings on the limitation of current LLMs unlearning methods and proposes future research directions, but also provides a replicable framework for ongoing exploration and validation in the field.

------------

`[2406.15360] Generative AI Adoption in Classroom in Context of Technology Acceptance Model (TAM) and the Innovation Diffusion Theory (IDT) <https://arxiv.org/abs/2406.15360>`__ 技术接受模型(TAM)和创新扩散理论(IDT)背景下的生成性人工智能在课堂中的应用

::

    Fri, 29 Mar 2024 22:41:51 GMT
    Aashish Ghimire and John Edwards

The burgeoning development of generative artificial intelligence (GenAI) and the widespread adoption of large language models (LLMs) in educational settings have sparked considerable debate regarding their efficacy and acceptability.Despite the potential benefits, the assimilation of these cutting-edge technologies among educators exhibits a broad spectrum of attitudes, from enthusiastic advocacy to profound skepticism.This study aims to dissect the underlying factors influencing educators' perceptions and acceptance of GenAI and LLMs.We conducted a survey among educators and analyzed the data through the frameworks of the Technology Acceptance Model (TAM) and Innovation Diffusion Theory (IDT). Our investigation reveals a strong positive correlation between the perceived usefulness of GenAI tools and their acceptance, underscoring the importance of demonstrating tangible benefits to educators. Additionally, the perceived ease of use emerged as a significant factor, though to a lesser extent, influencing acceptance. Our findings also show that the knowledge and acceptance of these tools is not uniform, suggesting that targeted strategies are required to address the specific needs and concerns of each adopter category to facilitate broader integration of AI tools.in education.

------------

`[2406.15508] What Teaches Robots to Walk, Teaches Them to Trade too -- Regime Adaptive Execution using Informed Data and LLMs <https://arxiv.org/abs/2406.15508>`__ 什么教机器人走路，教他们交易-使用知情数据和法学硕士制度自适应执行

::

    Thu, 20 Jun 2024 00:17:28 GMT
    Raeid Saqur

Machine learning techniques applied to the problem of financial market forecasting struggle with dynamic regime switching, or underlying correlation and covariance shifts in true (hidden) market variables. Drawing inspiration from the success of reinforcement learning in robotics, particularly in agile locomotion adaptation of quadruped robots to unseen terrains, we introduce an innovative approach that leverages world knowledge of pretrained LLMs (aka.
'privileged information' in robotics) and dynamically adapts them using intrinsic, natural market rewards using LLM alignment technique we dub as "Reinforcement Learning from Market Feedback" (**RLMF**). Strong empirical results demonstrate the efficacy of our method in adapting to regime shifts in financial markets, a challenge that has long plagued predictive models in this domain. The proposed algorithmic framework outperforms best-performing SOTA LLM models on the existing (FLARE) benchmark stock-movement (SM) tasks by more than 15\% improved accuracy. On the recently proposed NIFTY SM task, our adaptive policy outperforms the SOTA best performing trillion parameter models like GPT-4. The paper details the dual-phase, teacher-student architecture and implementation of our model, the empirical results obtained, and an analysis of the role of language embeddings in terms of Information Gain.

------------

`[2406.15609] Automated radiotherapy treatment planning guided by GPT-4Vision <https://arxiv.org/abs/2406.15609>`__ 

::

    Fri, 21 Jun 2024 19:23:03 GMT
    Sheng Liu, Oscar Pastor-Serrano, Yizheng Chen, Matthew Gopaulchan, Weixing Liang, Mark Buyyounouski, Erqi Pollom, Quynh-Thu Le, Michael Gensheimer, Peng Dong, Yong Yang, James Zou, Lei Xing

Radiotherapy treatment planning is a time-consuming and potentially subjective process that requires the iterative adjustment of model parameters to balance multiple conflicting objectives. Recent advancements in large foundation models offer promising avenues for addressing the challenges in planning and clinical decision-making. This study introduces GPT-RadPlan, a fully automated treatment planning framework that harnesses prior radiation oncology knowledge encoded in multi-modal large language models, such as GPT-4Vision (GPT-4V) from OpenAI. GPT-RadPlan is made aware of planning protocols as context and acts as an expert human planner, capable of guiding a treatment planning process. Via in-context learning, we incorporate clinical protocols for various disease sites as prompts to enable GPT-4V to acquire treatment planning domain knowledge. The resulting GPT-RadPlan agent is integrated into our in-house inverse treatment planning system through an API.
The efficacy of the automated planning system is showcased using multiple prostate and head & neck cancer cases, where we compared GPT-RadPlan results to clinical plans. In all cases, GPT-RadPlan either outperformed or matched the clinical plans, demonstrating superior target coverage and organ-at-risk sparing. Consistently satisfying the dosimetric objectives in the clinical protocol, GPT-RadPlan represents the first multimodal large language model agent that mimics the behaviors of human planners in radiation oncology clinics, achieving remarkable results in automating the treatment planning process without the need for additional training.

------------

`[2406.15676] Inferring Pluggable Types with Machine Learning <https://arxiv.org/abs/2406.15676>`__ 用机器学习推断可插拔类型

::

    Fri, 21 Jun 2024 22:32:42 GMT
    Kazi Amanul Islam Siddiqui, Martin Kellogg

Pluggable type systems allow programmers to extend the type system of a programming language to enforce semantic properties defined by the programmer.
Pluggable type systems are difficult to deploy in legacy codebases because they require programmers to write type annotations manually. This paper investigates how to use machine learning to infer type qualifiers automatically. We propose a novel representation, NaP-AST, that encodes minimal dataflow hints for the effective inference of type qualifiers. We evaluate several model architectures for inferring type qualifiers, including Graph Transformer Network, Graph Convolutional Network and Large Language Model. We further validated these models by applying them to 12 open-source programs from a prior evaluation of the NullAway pluggable typechecker, lowering warnings in all but one unannotated project. We discovered that GTN shows the best performance, with a recall of .89 and precision of 0.6. Furthermore, we conduct a study to estimate the number of Java classes needed for good performance of the trained model.
For our feasibility study, performance improved around 16k classes, and deteriorated due to overfitting around 22k classes.

------------

`[2406.16093] Towards Natural Language-Driven Assembly Using Foundation Models <https://arxiv.org/abs/2406.16093>`__ 基于基础模型的自然语言驱动汇编研究

::

    Sun, 23 Jun 2024 12:14:37 GMT
    Omkar Joglekar, Tal Lancewicki, Shir Kozlovsky, Vladimir Tchuiev, Zohar Feldman, Dotan Di Castro

Large Language Models (LLMs) and strong vision models have enabled rapid research and development in the field of Vision-Language-Action models that enable robotic control. The main objective of these methods is to develop a generalist policy that can control robots with various embodiments. However, in industrial robotic applications such as automated assembly and disassembly, some tasks, such as insertion, demand greater accuracy and involve intricate factors like contact engagement, friction handling, and refined motor skills.
Implementing these skills using a generalist policy is challenging because these policies might integrate further sensory data, including force or torque measurements, for enhanced precision. In our method, we present a global control policy based on LLMs that can transfer the control policy to a finite set of skills that are specifically trained to perform high-precision tasks through dynamic context switching. The integration of LLMs into this framework underscores their significance in not only interpreting and processing language inputs but also in enriching the control mechanisms for diverse and intricate robotic operations.

------------

`[2406.16224] From Text to Test: AI-Generated Control Software for Materials Science Instruments <https://arxiv.org/abs/2406.16224>`__ 从文本到测试:材料科学仪器ai生成控制软件

::

    Sun, 23 Jun 2024 21:32:57 GMT
    Davi M F\'ebba, Kingsley Egbo, William A. Callahan, Andriy Zakutayev

Large language models (LLMs) are transforming the landscape of chemistry and materials science. Recent examples of LLM-accelerated experimental research include virtual assistants for parsing synthesis recipes from the literature, or using the extracted knowledge to guide synthesis and characterization.
Despite these advancements, their application is constrained to labs with automated instruments and control software, leaving much of materials science reliant on manual processes. Here, we demonstrate the rapid deployment of a Python-based control module for a Keithley 2400 electrical source measure unit using ChatGPT-4. Through iterative refinement, we achieved effective instrument management with minimal human intervention. Additionally, a user-friendly graphical user interface (GUI) was created, effectively linking all instrument controls to interactive screen elements. Finally, we integrated this AI-crafted instrument control software with a high-performance stochastic optimization algorithm to facilitate rapid and automated extraction of electronic device parameters related to semiconductor charge transport mechanisms from current-voltage (IV) measurement data. This integration resulted in a comprehensive open-source toolkit for semiconductor device characterization and analysis using IV curve measurements. We demonstrate the application of these tools by acquiring, analyzing, and parameterizing IV data from a Pt/Cr<sub>2</sub>O<sub>3</sub>/\b{eta}-Ga<sub>2</sub>O<sub>3</sub> heterojunction diode, a novel stack for high-power and high-temperature electronic devices. This approach underscores the powerful synergy between LLMs and the development of instruments for scientific inquiry, showcasing a path for further acceleration in materials science.

------------

`[2406.16333] Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models <https://arxiv.org/abs/2406.16333>`__ 提示一致性图像生成(PCIG):集成LLMs、知识图谱和可控扩散模型的统一框架

::

    Mon, 24 Jun 2024 06:12:16 GMT
    Yichen Sun, Zhixuan Chu, Zhan Qin, Kui Ren

The rapid advancement of Text-to-Image(T2I) generative models has enabled the synthesis of high-quality images guided by textual descriptions. Despite this significant progress, these models are often susceptible in generating contents that contradict the input text, which poses a challenge to their reliability and practical deployment. To address this problem, we introduce a novel diffusion-based framework to significantly enhance the alignment of generated images with their corresponding descriptions, addressing the inconsistency between visual output and textual input. Our framework is built upon a comprehensive analysis of inconsistency phenomena, categorizing them based on their manifestation in the image. Leveraging a state-of-the-art large language module, we first extract objects and construct a knowledge graph to predict the locations of these objects in potentially generated images. We then integrate a state-of-the-art controllable image generation model with a visual text generation module to generate an image that is consistent with the original prompt, guided by the predicted object locations. Through extensive experiments on an advanced multimodal hallucination benchmark, we demonstrate the efficacy of our approach in accurately generating the images without the inconsistency with the original prompt. The code can be accessed via https://github.com/TruthAI-Lab/PCIG.

------------

`[2406.16346] Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific Training Tasks <https://arxiv.org/abs/2406.16346>`__ 

::

    Mon, 24 Jun 2024 06:39:02 GMT
    Daniel Wen and Nafisa Hussain

Large language models (LLMs) and large visual language models (LVLMs) have been at the forefront of the artificial intelligence field, particularly for tasks like text generation, video captioning, and question-answering.
Typically, it is more applicable to train these models on broader knowledge bases or datasets to increase generalizability, learn relationships between topics, and recognize patterns. Instead, we propose to provide instructional datasets specific to the task of each modality within a distinct domain and then fine-tune the parameters of the model using LORA. With our approach, we can eliminate all noise irrelevant to the given task while also ensuring that the model generates with enhanced precision. For this work, we use Video-LLaVA to generate recipes given cooking videos without transcripts. Video-LLaVA's multimodal architecture allows us to provide cooking images to its image encoder, cooking videos to its video encoder, and general cooking questions to its text encoder. Thus, we aim to remove all noise unrelated to cooking while improving our model's capabilities to generate specific ingredient lists and detailed instructions. As a result, our approach to fine-tuning Video-LLaVA leads to gains over the baseline Video-LLaVA by 2% on the YouCook2 dataset.
While this may seem like a marginal increase, our model trains on an image instruction dataset 2.5% the size of Video-LLaVA's and a video instruction dataset 23.76% of Video-LLaVA's.

------------

`[2406.16386] Automatically Generating UI Code from Screenshot: A Divide-and-Conquer-Based Approach <https://arxiv.org/abs/2406.16386>`__ 从截图自动生成UI代码:一种基于分而治之的方法

::

    Mon, 24 Jun 2024 07:58:36 GMT
    Yuxuan Wan, Chaozheng Wang, Yi Dong, Wenxuan Wang, Shuqing Li, Yintong Huo, Michael R. Lyu

Websites are critical in today's digital world, with over 1.11 billion currently active and approximately 252,000 new sites launched daily. Converting website layout design into functional UI code is a time-consuming yet indispensable step of website development. Manual methods of converting visual designs into functional code present significant challenges, especially for non-experts. To explore automatic design-to-code solutions, we first conduct a motivating study on GPT-4o and identify three types of issues in generating UI code: element omission, element distortion, and element misarrangement. We further reveal that a focus on smaller visual segments can help multimodal large language models (MLLMs) mitigate these failures in the generation process. In this paper, we propose DCGen, a divide-and-conquer-based approach to automate the translation of webpage design to UI code. DCGen starts by dividing screenshots into manageable segments, generating descriptions for each segment, and then reassembling them into complete UI code for the entire screenshot. We conduct extensive testing with a dataset comprised of real-world websites and various MLLMs and demonstrate that DCGen achieves up to a 14% improvement in visual similarity over competing methods. To the best of our knowledge, DCGen is the first segment-aware prompt-based approach for generating UI code directly from screenshots.

------------

`[2406.15963] Effectiveness of ChatGPT in explaining complex medical reports to patients <https://arxiv.org/abs/2406.15963>`__ ChatGPT在向患者解释复杂医疗报告中的有效性

::

    Sun, 23 Jun 2024 00:04:07 GMT
    Mengxuan Sun, Ehud Reiter, Anne E Kiltie, George Ramsay, Lisa Duncan, Peter Murchie, Rosalind Adam

Electronic health records contain detailed information about the medical condition of patients, but they are difficult for patients to understand even if they have access to them. We explore whether ChatGPT (GPT 4) can help explain multidisciplinary team (MDT) reports to colorectal and prostate cancer patients. These reports are written in dense medical language and assume clinical knowledge, so they are a good test of the ability of ChatGPT to explain complex medical reports to patients. We asked clinicians and lay people (not patients) to review explanations and responses of ChatGPT. We also ran three focus groups (including cancer patients, caregivers, computer scientists, and clinicians) to discuss output of ChatGPT. Our studies highlighted issues with inaccurate information, inappropriate language, limited personalization, AI distrust, and challenges integrating large language models (LLMs) into clinical workflow. These issues will need to be resolved before LLMs can be used to explain complex personal medical information to patients.

------------

`[2406.16332] DemoRank: Selecting Effective Demonstrations for Large Language Models in Ranking Task <https://arxiv.org/abs/2406.16332>`__ 士气低落:在排名任务中为大型语言模型选择有效的演示

::

    Mon, 24 Jun 2024 06:10:13 GMT
    Wenhan Liu, Yutao Zhu, and Zhicheng Dou

Recently, there has been increasing interest in applying large language models (LLMs) as zero-shot passage rankers. However, few studies have explored how to select appropriate in-context demonstrations for the passage ranking task, which is the focus of this paper. Previous studies mainly apply a demonstration retriever to retrieve demonstrations and use top-$k$ demonstrations for in-context learning (ICL). Although effective, this approach overlooks the dependencies between demonstrations, leading to inferior performance of few-shot ICL in the passage ranking task. In this paper, we formulate the demonstration selection as a \textit{retrieve-then-rerank} process and introduce the DemoRank framework. In this framework, we first use LLM feedback to train a demonstration retriever and construct a novel dependency-aware training samples to train a demonstration reranker to improve few-shot ICL. The construction of such training samples not only considers demonstration dependencies but also performs in an efficient way. Extensive experiments demonstrate DemoRank's effectiveness in in-domain scenarios and strong generalization to out-of-domain scenarios. Our codes are available at~\url{https://github.com/8421BCD/DemoRank}.

------------

`[2406.16562] EvalAlign: Evaluating Text-to-Image Models through Precision Alignment of Multimodal Large Models with Supervised Fine-Tuning to Human Annotations <https://arxiv.org/abs/2406.16562>`__ EvalAlign:通过对人工标注进行监督微调的多模态大型模型的精确对齐来评估文本到图像模型

::

    Mon, 24 Jun 2024 11:56:15 GMT
    Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, Mengping Yang,Cheng Zhang, Hao Li

The recent advancements in text-to-image generative models have been remarkable. Yet, the field suffers from a lack of evaluation metrics that accurately reflect the performance of these models, particularly lacking fine-grained metrics that can guide the optimization of the models. In this paper, we propose EvalAlign, a metric characterized by its accuracy, stability, and fine granularity. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) pre-trained on extensive datasets. We develop evaluation protocols that focus on two key dimensions: image faithfulness and text-image alignment. Each protocol comprises a set of detailed, fine-grained instructions linked to specific scoring options, enabling precise manual scoring of the generated images. We Supervised Fine-Tune (SFT) the MLLM to align closely with human evaluative judgments, resulting in a robust evaluation model. Our comprehensive tests across 24 text-to-image generation models demonstrate that EvalAlign not only provides superior metric stability but also aligns more closely with human preferences than existing metrics, confirming its effectiveness and utility in model assessment.

------------

`[2406.15540] Specify What? Enhancing Neural Specification Synthesis by Symbolic Methods <https://arxiv.org/abs/2406.15540>`__ 指定什么?用符号方法增强神经规范合成

::

    Fri, 21 Jun 2024 17:39:57 GMT
    George Granberry, Wolfgang Ahrendt, Moa Johansson

We investigate how combinations of Large Language Models (LLMs) and symbolic analyses can be used to synthesise specifications of C programs. The LLM prompts are augmented with outputs from two formal methods tools in the Frama-C ecosystem, Pathcrawler and EVA, to produce C program annotations in the specification language ACSL. We demonstrate how the addition of symbolic analysis to the workflow impacts the quality of annotations: information about input/output examples from Pathcrawler produce more context-aware annotations, while the inclusion of EVA reports yields annotations more attuned to runtime errors. In addition, we show that the method infers rather the programs intent than its behaviour, by generating specifications for buggy programs and observing robustness of the result against bugs.

------------

`[2309.10253] GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts <https://arxiv.org/abs/2309.10253>`__ GPTFUZZER: Red将大型语言模型与自动生成的越狱提示结合起来

::

    replaced with revised version Mon, 24 Jun 2024 15:34:17 GMT
    Submission history From: Jiahao Yu [view email]
    [v1] Tue, 19 Sep 2023 02:19:48 UTC (9,081 KB)
    [v2] Wed, 4 Oct 2023 06:15:12 UTC (3,672 KB)
    [v3] Mon, 24 Jun 2024 15:34:17 UTC (3,302 KB)
    [v4] Thu, 27 Jun 2024 16:01:27 UTC (3,302 KB)
    Jiahao Yu, Xingwei Lin, Zheng Yu, Xinyu Xing

Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging.
In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack.
We evaluate GPTFuzz against various commercial and open-source LLMs, including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our results indicate that GPTFuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz achieves over 90% attack success rates against ChatGPT and Llama-2 models, even with suboptimal initial seed templates. We anticipate that GPTFuzz will be instrumental for researchers and practitioners in examining LLM robustness and will encourage further exploration into enhancing LLM safety.

------------

`[2402.05359] Prompting with Divide-and-Conquer Program Makes Large Language Models Discerning to Hallucination and Deception <https://arxiv.org/abs/2402.05359>`__ 分而治之的程序激励使大型语言模型能够分辨幻觉和欺骗

::

    replaced with revised version Mon, 24 Jun 2024 08:49:29 GMT
    Submission history From: Yizhou Zhang [view email]
    [v1] Thu, 8 Feb 2024 02:37:30 UTC (585 KB)
    [v2] Mon, 11 Mar 2024 23:15:10 UTC (585 KB)
    [v3] Thu, 14 Mar 2024 21:12:42 UTC (585 KB)
    [v4] Fri, 5 Apr 2024 23:22:07 UTC (586 KB)
    [v5] Mon, 24 Jun 2024 08:49:29 UTC (581 KB)
    [v6] Tue, 2 Jul 2024 18:18:18 UTC (581 KB)
    Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu

Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, simple instructional prompts suffer from inaccurate responses. Existing works show that more complicated prompting strategies, such as Chain-of-Thoughts and Least-to-Most, can unlock LLM's powerful capacity in diverse areas. Recent researches reveal that simple divide-and-conquer prompting strategy, i.e. simply dividing the input sequence to multiple sub-inputs, can also substantially improve LLM's performance in some specific tasks such as misinformation detection. In this paper, we aim at examining the utility of divide-and-conquer prompting strategy and answer on which kind of tasks this strategy gets advantages. Specifically, we provide a theoretic analysis to divide-and-conquer prompting strategy and help us identify the specific tasks where DaC prompting can bring performance boost with theoretic guarantee. We then present two cases (large integer arithmetic and fact verification) where experimental results aligns with our theoretic analysis.

------------

`[2406.12227] Interpretable Catastrophic Forgetting of Large Language Model Fine-tuning via Instruction Vector <https://arxiv.org/abs/2406.12227>`__ 基于指令向量微调的大型语言模型可解释灾难性遗忘

::

    replaced with revised version Mon, 24 Jun 2024 09:29:28 GMT
    Submission history From: Gangwei Jiang [view email]
    [v1] Tue, 18 Jun 2024 03:05:08 UTC (9,378 KB)
    [v2] Mon, 24 Jun 2024 09:29:28 UTC (732 KB)
    Gangwei Jiang, Caigao Jiang, Zhaoyi Li, Siqiao Xue, Jun Zhou, Linqi Song, Defu Lian, Ying Wei

Fine-tuning large language models (LLMs) can cause them to lose their general capabilities. However, the intrinsic mechanisms behind such forgetting remain unexplored. In this paper, we begin by examining this phenomenon by focusing on knowledge understanding and instruction following, with the latter identified as the main contributor to forgetting during fine-tuning. Consequently, we propose the Instruction Vector (IV) framework to capture model representations highly related to specific instruction-following capabilities, thereby making it possible to understand model-intrinsic forgetting. Through the analysis of IV dynamics pre and post-training, we suggest that fine-tuning mostly adds specialized reasoning patterns instead of erasing previous skills, which may appear as forgetting. Building on this insight, we develop IV-guided training, which aims to preserve original computation graph, thereby mitigating catastrophic forgetting. Empirical tests on three benchmarks confirm the efficacy of this new approach, supporting the relationship between IVs and forgetting. Our code will be made available soon.

------------

`[2406.13558] Enhancing Travel Choice Modeling with Large Language Models: A Prompt-Learning Approach <https://arxiv.org/abs/2406.13558>`__ 利用大型语言模型增强旅游选择建模:一种快速学习方法

::

    replaced with revised version Sat, 22 Jun 2024 14:44:34 GMT
    Submission history From: Hanlin Tian [view email]
    [v1] Wed, 19 Jun 2024 13:46:08 UTC (1,533 KB)
    [v2] Sat, 22 Jun 2024 14:44:34 UTC (1 KB) (withdrawn)
    Xuehao Zhai, Hanlin Tian, Lintong Li, Tianyu Zhao

Travel choice analysis is crucial for understanding individual travel behavior to develop appropriate transport policies and recommendation systems in Intelligent Transportation Systems (ITS). Despite extensive research, this domain faces two critical challenges: a) modeling with limited survey data, and b) simultaneously achieving high model explainability and accuracy. In this paper, we introduce a novel prompt-learning-based Large Language Model(LLM) framework that significantly improves prediction accuracy and provides explicit explanations for individual predictions. This framework involves three main steps: transforming input variables into textual form; building of demonstrations similar to the object, and applying these to a well-trained LLM. We tested the framework's efficacy using two widely used choice datasets: London Passenger Mode Choice (LPMC) and Optima-Mode collected in Switzerland. The results indicate that the LLM significantly outperforms state-of-the-art deep learning methods and discrete choice models in predicting people's choices. Additionally, we present a case of explanation illustrating how the LLM framework generates understandable and explicit explanations at the individual level.

------------

`[2406.14343] iWISDM: Assessing instruction following in multimodal models at scale <https://arxiv.org/abs/2406.14343>`__ iWISDM:评估大规模多模态模型的指令跟随

::

    replaced with revised version Sun, 23 Jun 2024 01:51:53 GMT
    Submission history From: Xiaoxuan Lei [view email]
    [v1] Thu, 20 Jun 2024 14:09:54 UTC (14,117 KB)
    [v2] Sun, 23 Jun 2024 01:51:53 UTC (14,118 KB)
    [v3] Tue, 25 Jun 2024 15:12:01 UTC (14,118 KB)
    [v4] Wed, 3 Jul 2024 21:44:23 UTC (14,118 KB)
    Xiaoxuan Lei and Lucas Gomez and Hao Yuan Bai and Pouya Bashivan

The ability to perform complex tasks from detailed instructions is a key to many remarkable achievements of our species. As humans, we are not only capable of performing a wide variety of tasks but also very complex ones that may entail hundreds or thousands of steps to complete. Large language models and their more recent multimodal counterparts that integrate textual and visual inputs have achieved unprecedented success in performing complex tasks. Yet, most existing benchmarks are largely confined to single-modality inputs (either text or vision), narrowing the scope of multimodal assessments, particularly for instruction-following in multimodal contexts. To bridge this gap, we introduce the instructed-Virtual VISual Decision Making (iWISDM) environment engineered to generate a limitless array of vision-language tasks of varying complexity. Using iWISDM, we compiled three distinct benchmarks of instruction following visual tasks across varying complexity levels and evaluated several newly developed multimodal models on these benchmarks. Our findings establish iWISDM as a robust benchmark for assessing the instructional adherence of both existing and emergent multimodal models and highlight a large gap between these models' ability to precisely follow instructions with that of humans.The code of iWISDM is available on GitHub at this https URL.

------------

`[2406.14903] GIEBench: Towards Holistic Evaluation of Group Identity-based Empathy for Large Language Models <https://arxiv.org/abs/2406.14903>`__ GIEBench:基于群体身份的大型语言模型共情的整体评估

::

    replaced with revised version Mon, 24 Jun 2024 14:57:18 GMT
    Submission history From: Leyan Wang [view email]
    [v1] Fri, 21 Jun 2024 06:50:42 UTC (687 KB)
    [v2] Mon, 24 Jun 2024 14:57:18 UTC (1,367 KB)
    Leyan Wang, Yonggang Jin, Tianhao Shen, Tianyu Zheng, Xinrun Du, Chenchen Zhang, Wenhao Huang, Jiaheng Liu, Shi Wang, Ge Zhang, Liuyu Xiang, Zhaofeng He

As large language models (LLMs) continue to develop and gain widespread application, the ability of LLMs to exhibit empathy towards diverse group identities and understand their perspectives is increasingly recognized as critical. Most existing benchmarks for empathy evaluation of LLMs focus primarily on universal human emotions, such as sadness and pain, often overlooking the context of individuals' group identities. To address this gap, we introduce GIEBench, a comprehensive benchmark that includes 11 identity dimensions, covering 97 group identities with a total of 999 single-choice questions related to specific group identities. GIEBench is designed to evaluate the empathy of LLMs when presented with specific group identities such as gender, age, occupation, and race, emphasizing their ability to respond from the standpoint of the identified group. This supports the ongoing development of empathetic LLM applications tailored to users with different identities. Our evaluation of 23 LLMs revealed that while these LLMs understand different identity standpoints, they fail to consistently exhibit equal empathy across these identities without explicit instructions to adopt those perspectives. This highlights the need for improved alignment of LLMs with diverse values to better accommodate the multifaceted nature of human identities. Our datasets are available at this https URL.

------------

`[2308.07269] EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models <https://arxiv.org/abs/2308.07269>`__ EasyEdit:面向大型语言模型的易用知识编辑框架

::

    replaced with revised version Mon, 24 Jun 2024 02:17:57 GMT
    Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen

Categories

------------

`[2309.07045] SafetyBench: Evaluating the Safety of Large Language Models <https://arxiv.org/abs/2309.07045>`__ 

::

    replaced with revised version Mon, 24 Jun 2024 04:04:21 GMT
    Submission history From: Zhexin Zhang [view email]
    [v1] Wed, 13 Sep 2023 15:56:50 UTC (3,913 KB)
    [v2] Mon, 24 Jun 2024 04:04:21 UTC (4,846 KB)
    Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang

With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We also demonstrate that the measured safety understanding abilities in SafetyBench are correlated with safety generation abilities. Data and evaluation guidelines are available at \url{this https URL}{this https URL}. Submission entrance and leaderboard are available at \url{this https URL}{this https URL}.

------------

`[2309.10952] LMDX: Language Model-based Document Information Extraction and Localization <https://arxiv.org/abs/2309.10952>`__ LMDX:基于语言模型的文档信息抽取与定位

::

    replaced with revised version Fri, 21 Jun 2024 21:55:07 GMT
    Submission history From: Vincent Perot [view email]
    [v1] Tue, 19 Sep 2023 22:32:56 UTC (480 KB)
    [v2] Fri, 21 Jun 2024 21:55:07 UTC (845 KB)
    Vincent Perot, Kai Kang, Florian Luisier, Guolong Su, Xiaoyu Sun, Ramya Sree Boppana, Zilong Wang, Zifeng Wang, Jiaqi Mu, Hao Zhang, Chen-Yu Lee, Nan Hua

Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art and exhibiting emergent capabilities across various tasks. However, their application in extracting information from visually rich documents, which is at the core of many document processing workflows and involving the extraction of key entities from semi-structured documents, has not yet been successful. The main obstacles to adopting LLMs for this task include the absence of layout encoding within LLMs, which is critical for high quality extraction, and the lack of a grounding mechanism to localize the predicted entities within the document. In this paper, we introduce Language Model-based Document Information Extraction and Localization (LMDX), a methodology to reframe the document information extraction task for a LLM. LMDX enables extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and localizing the entities within the document. Finally, we apply LMDX to the PaLM 2-S and Gemini Pro LLMs and evaluate it on VRDU and CORD benchmarks, setting a new state-of-the-art and showing how LMDX enables the creation of high quality, data-efficient parsers.

------------

`[2402.03049] EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models <https://arxiv.org/abs/2402.03049>`__ EasyInstruct:面向大型语言模型的易用指令处理框架

::

    replaced with revised version Mon, 24 Jun 2024 02:10:23 GMT
    Submission history From: Ningyu Zhang [view email]
    [v1] Mon, 5 Feb 2024 14:33:56 UTC (2,686 KB)
    [v2] Tue, 6 Feb 2024 02:51:23 UTC (2,686 KB)
    [v3] Thu, 21 Mar 2024 15:33:34 UTC (2,687 KB)
    [v4] Mon, 24 Jun 2024 02:10:23 UTC (2,690 KB)
    Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen

In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at this https URL, along with an online demo app and a demo video for quick-start, calling for broader research centered on instruction data and synthetic data.

------------

`[2402.04678] FaithLM: Towards Faithful Explanations for Large Language Models <https://arxiv.org/abs/2402.04678>`__ FaithLM:大型语言模型的忠实解释

::

    replaced with revised version Sun, 23 Jun 2024 01:13:25 GMT
    Submission history From: Yu-Neng Chuang [view email]
    [v1] Wed, 7 Feb 2024 09:09:14 UTC (491 KB)
    [v2] Sun, 23 Jun 2024 01:13:25 UTC (1,427 KB)
    [v3] Wed, 26 Jun 2024 07:43:11 UTC (1,427 KB)
    Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Ruixiang Tang, Shaochen Zhong, Fan Yang, Mengnan Du, Xuanting Cai, and Xia Hu

Large Language Models (LLMs) have become proficient in addressing complex tasks by leveraging their extensive internal knowledge and reasoning capabilities. However, the black-box nature of these models complicates the task of explaining their decision-making processes. While recent advancements demonstrate the potential of leveraging LLMs to self-explain their predictions through natural language (NL) explanations, their explanations may not accurately reflect the LLMs' decision-making process due to a lack of fidelity optimization on the derived explanations. Measuring the fidelity of NL explanations is a challenging issue, as it is difficult to manipulate the input context to mask the semantics of these explanations. To this end, we introduce FaithLM to explain the decision of LLMs with NL explanations. Specifically, FaithLM designs a method for evaluating the fidelity of NL explanations by incorporating the contrary explanations to the query process. Moreover, FaithLM conducts an iterative process to improve the fidelity of derived explanations. Experiment results on three datasets from multiple domains demonstrate that FaithLM can significantly improve the fidelity of derived explanations, which also provides a better alignment with the ground-truth explanations.

------------

`[2402.12842] PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning <https://arxiv.org/abs/2402.12842>`__ PromptKD:通过提示调优提取对学生友好的生成语言模型知识

::

    replaced with revised version Mon, 24 Jun 2024 05:40:38 GMT
    Submission history From: Gyeongman Kim [view email]
    [v1] Tue, 20 Feb 2024 09:10:08 UTC (7,761 KB)
    [v2] Mon, 24 Jun 2024 05:40:38 UTC (7,779 KB)
    Gyeongman Kim, Doohyuk Jang, Eunho Yang

Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Extensive experiments on instruction-following datasets show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher's parameters as prompts. Further analysis suggests that distilling student-friendly knowledge alleviates exposure bias effectively throughout the entire training process, leading to performance enhancements.

------------

`[2402.14208] LLM-Assisted Content Conditional Debiasing for Fair Text Embedding <https://arxiv.org/abs/2402.14208>`__ llm辅助的公平文本嵌入内容条件去偏

::

    replaced with revised version Mon, 24 Jun 2024 04:49:16 GMT
    Submission history From: Wenlong Deng [view email]
    [v1] Thu, 22 Feb 2024 01:20:51 UTC (2,191 KB)
    [v2] Fri, 23 Feb 2024 08:19:09 UTC (2,188 KB)
    [v3] Mon, 24 Jun 2024 04:49:16 UTC (4,931 KB)
    Wenlong Deng, Blair Chen, Beidi Zhao, Chiyu Zhang, Xiaoxiao Li, Christos Thrampoulidis

Mitigating biases in machine learning models has become an increasing concern in Natural Language Processing (NLP), particularly in developing fair text embeddings, which are crucial yet challenging for real-world applications like search engines. In response, this paper proposes a novel method for learning fair text embeddings. First, we define a novel content-conditional equal distance (CCED) fairness for text embeddings, ensuring content-conditional independence between sensitive attributes and text embeddings. Building on CCED, we introduce a content-conditional debiasing (CCD) loss to ensure that embeddings of texts with different sensitive attributes but identical content maintain the same distance from the embedding of their corresponding neutral text. Additionally, we tackle the issue of insufficient training data by using Large Language Models (LLMs) with instructions to fairly augment texts into different sensitive groups. Our extensive evaluations show that our approach effectively enhances fairness while maintaining the utility of embeddings. Furthermore, our augmented dataset, combined with the CCED metric, serves as an new benchmark for evaluating fairness.

------------

`[2403.03628] GPTopic: Dynamic and Interactive Topic Representations <https://arxiv.org/abs/2403.03628>`__ GPTopic:动态交互式主题表示

::

    replaced with revised version Sat, 22 Jun 2024 09:00:29 GMT
    Submission history From: Arik Reuter [view email]
    [v1] Wed, 6 Mar 2024 11:34:20 UTC (7,916 KB)
    [v2] Sat, 22 Jun 2024 09:00:29 UTC (7,916 KB)
    Arik Reuter, Anton Thielmann, Christoph Weisser, Sebastian Fischer, Benjamin S\"afken

Topic modeling seems to be almost synonymous with generating lists of top words to represent topics within large text corpora. However, deducing a topic from such list of individual terms can require substantial expertise and experience, making topic modelling less accessible to people unfamiliar with the particularities and pitfalls of top-word interpretation. A topic representation limited to top-words might further fall short of offering a comprehensive and easily accessible characterization of the various aspects, facets and nuances a topic might have. To address these challenges, we introduce GPTopic, a software package that leverages Large Language Models (LLMs) to create dynamic, interactive topic representations. GPTopic provides an intuitive chat interface for users to explore, analyze, and refine topics interactively, making topic modeling more accessible and comprehensive. The corresponding code is available here: this https URL.

------------

`[2403.04814] Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks <https://arxiv.org/abs/2403.04814>`__ LLMs在语法感知代码中间填充任务上的评估

::

    replaced with revised version Sun, 23 Jun 2024 01:17:48 GMT
    Submission history From: Linyuan Gong [view email]
    [v1] Thu, 7 Mar 2024 05:05:56 UTC (245 KB)
    [v2] Wed, 10 Apr 2024 20:26:31 UTC (303 KB)
    [v3] Sun, 23 Jun 2024 01:17:48 UTC (304 KB)
    Linyuan Gong, Sida Wang, Mostafa Elhoushi, Alvin Cheung

We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at this https URL, and the leaderboard is available at this https URL.

------------

`[2403.13737] EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation <https://arxiv.org/abs/2403.13737>`__ EthioLLM:带任务评估的埃塞俄比亚语多语言大型语言模型

::

    replaced with revised version Sun, 23 Jun 2024 12:06:27 GMT
    Atnafu Lambebo Tonja, Israel Abebe Azime, Tadesse Destaw Belay, Mesay Gemeda Yigezu, Moges Ahmed Mehamed, Abinew Ali Ayele, Ebrahim Chekol Jibril, Michael Melese Woldeyohannis, Olga Kolesnikova, Philipp Slusallek, Dietrich Klakow, Shengwu Xiong, Seid Muhie Yimam

Categories

------------

`[2403.15250] Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach <https://arxiv.org/abs/2403.15250>`__ LLMs大规模评估结果的综合再评估:多层面统计方法

::

    replaced with revised version Mon, 24 Jun 2024 07:49:25 GMT
    Submission history From: Kun Sun [view email]
    [v1] Fri, 22 Mar 2024 14:47:35 UTC (4,452 KB)
    [v2] Mon, 24 Jun 2024 07:49:25 UTC (6,812 KB)
    Kun Sun, Rong Wang, and Anders S{\o}gaard

Amidst the rapid evolution of LLMs, the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of LLMs. However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points. Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens. Our study embarks on a thorough re-examination of these LLMs, targeting the inadequacies in current evaluation methods. With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology. This includes the application of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering a robust and transparent approach to deciphering LLM performance data. Contrary to prevailing findings, our results challenge assumptions about emergent abilities and the influence of given training types and architectures in LLMs. These findings furnish new perspectives on the characteristics, intrinsic nature, and developmental trajectories of LLMs. By providing straightforward and reliable methods to scrutinize and reassess LLM performance data, this study contributes a nuanced perspective on LLM efficiency and potentials.

------------

`[2403.16056] Qibo: A Large Language Model for Traditional Chinese Medicine <https://arxiv.org/abs/2403.16056>`__ Qibo:一个面向中医的大型语言模型

::

    replaced with revised version Sat, 22 Jun 2024 05:43:53 GMT
    Submission history From: Heyi Zhang [view email]
    [v1] Sun, 24 Mar 2024 07:48:05 UTC (874 KB)
    [v2] Mon, 17 Jun 2024 18:52:04 UTC (1,368 KB)
    [v3] Sat, 22 Jun 2024 05:43:53 UTC (1,360 KB)
    Heyi Zhang and Xin Wang and Zhaopeng Meng and Zhe Chen and Pengwei Zhuang and Yongzhe Jia and Dawei Xu and Wenbin Guo

Large Language Models (LLMs) has made significant progress in a number of professional fields, including medicine, law, and finance. However, in traditional Chinese medicine (TCM), there are challenges such as the essential differences between theory and modern medicine, the lack of specialized corpus resources, and the fact that relying only on supervised fine-tuning may lead to overconfident predictions. To address these challenges, we propose a two-stage training approach that combines continuous pre-training and supervised fine-tuning. A notable contribution of our study is the processing of a 2GB corpus dedicated to TCM, constructing pre-training and instruction fine-tuning datasets for TCM, respectively. In addition, we have developed Qibo-Benchmark, a tool that evaluates the performance of LLM in the TCM on multiple dimensions, including subjective, objective, and three TCM NLP tasks. The medical LLM trained with our pipeline, named $\textbf{Qibo}$, exhibits significant performance boosts. Compared to the baselines, the average subjective win rate is 63%, the average objective accuracy improved by 23% to 58%, and the Rouge-L scores for the three TCM NLP tasks are 0.72, 0.61, and 0.55. Finally, we propose a pipline to apply Qibo to TCM consultation and demonstrate the model performance through the case study.

------------

`[2403.16512] LLMs Are Few-Shot In-Context Low-Resource Language Learners <https://arxiv.org/abs/2403.16512>`__ llm是少样本的上下文低资源语言学习者

::

    replaced with revised version Mon, 24 Jun 2024 12:41:52 GMT
    Submission history From: Samuel Cahyawijaya [view email]
    [v1] Mon, 25 Mar 2024 07:55:29 UTC (10,055 KB)
    [v2] Wed, 27 Mar 2024 06:25:10 UTC (10,057 KB)
    [v3] Sat, 18 May 2024 08:51:10 UTC (10,065 KB)
    [v4] Mon, 24 Jun 2024 12:41:52 UTC (10,067 KB)
    [v5] Tue, 25 Jun 2024 11:54:23 UTC (10,067 KB)
    Samuel Cahyawijaya, Holy Lovenia, Pascale Fung

In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-context information on enhancing the low-resource understanding quality of LLMs through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted low-resource and the high-resource language that the model is proficient in. Our work highlights the importance of advancing ICL research, particularly for low-resource languages. Our code is publicly released at this https URL

------------

`[2404.01054] Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment <https://arxiv.org/abs/2404.01054>`__ 基于正则化Best-of-N采样的语言模型对齐奖励黑客行为

::

    replaced with revised version Mon, 24 Jun 2024 02:31:06 GMT
    Submission history From: Yuu Jinnai [view email]
    [v1] Mon, 1 Apr 2024 11:26:50 UTC (457 KB)
    [v2] Fri, 5 Apr 2024 02:47:41 UTC (640 KB)
    [v3] Mon, 24 Jun 2024 02:31:06 UTC (924 KB)
    Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe

Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate RBoN on the AlpacaFarm and Anthropic's hh-rlhf datasets and find that it outperforms BoN. As an application of RBoN, we use RBoN to generate a pairwise preference learning dataset. Experimental results show that a DPO model trained on a dataset generated with RBoN outperforms a DPO model generated with vanilla BoN. Our code is available at this https URL

------------

`[2404.04067] CLUE: A Clinical Language Understanding Evaluation for LLMs <https://arxiv.org/abs/2404.04067>`__ 线索:llm的临床语言理解评估

::

    replaced with revised version Mon, 24 Jun 2024 12:32:41 GMT
    Submission history From: Amin Dada [view email]
    [v1] Fri, 5 Apr 2024 12:51:37 UTC (2,004 KB)
    [v2] Thu, 11 Apr 2024 13:10:30 UTC (1,231 KB)
    [v3] Mon, 24 Jun 2024 12:32:41 UTC (1,243 KB)
    Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Kora\c{s}, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek

Large Language Models (LLMs) are expected to significantly contribute to patient care, diagnostics, and administrative processes. Emerging biomedical LLMs aim to address healthcare-specific challenges, including privacy demands and computational constraints. Assessing the models' suitability for this sensitive application area is of the utmost importance. However, evaluation has primarily been limited to non-clinical tasks, which do not reflect the complexity of practical clinical applications. To fill this gap, we present the Clinical Language Understanding Evaluation (CLUE), a benchmark tailored to evaluate LLMs on clinical tasks. CLUE includes six tasks to test the practical applicability of LLMs in complex healthcare settings. Our evaluation includes a total of $25$ LLMs. In contrast to previous evaluations, CLUE shows a decrease in performance for nine out of twelve biomedical models. Our benchmark represents a step towards a standardized approach to evaluating and developing LLMs in healthcare to align future model development with the real-world needs of clinical application. We open-source all evaluation scripts and datasets for future research at this https URL.

------------

`[2405.14092] Large Language Models Can Self-Correct with Minimal Effort <https://arxiv.org/abs/2405.14092>`__ 

::

    replaced with revised version Sun, 23 Jun 2024 18:30:26 GMT
    Submission history From: Zhenyu Wu [view email]
    [v1] Thu, 23 May 2024 01:43:45 UTC (8,215 KB)
    [v2] Sun, 23 Jun 2024 18:30:26 UTC (8,227 KB)
    Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, Meng Jiang

Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields $+6.8$ exact match on four open-domain question answering datasets, $+14.1$ accuracy on three arithmetic reasoning datasets, and $+9.6$ accuracy on a commonsense reasoning dataset, compared to Self-Correct.

------------

`[2405.15067] Promoting Constructive Deliberation: Reframing for Receptiveness <https://arxiv.org/abs/2405.15067>`__ 促进建设性审议:接受性重构

::

    replaced with revised version Fri, 21 Jun 2024 21:03:27 GMT
    Submission history From: Gauri Kambhatla [view email]
    [v1] Thu, 23 May 2024 21:35:22 UTC (7,968 KB)
    [v2] Fri, 21 Jun 2024 21:03:27 UTC (8,041 KB)
    Gauri Kambhatla, Matthew Lease, Ashwin Rajadesingan

To promote constructive discussion of controversial topics online, we propose automatic reframing of disagreeing responses to signal receptiveness to a preceding comment. Drawing on research from psychology, communications, and linguistics, we identify six strategies for reframing. We automatically reframe replies to comments according to each strategy, using a Reddit dataset. Through human-centered experiments, we find that the replies generated with our framework are perceived to be significantly more receptive than the original replies and a generic receptiveness baseline. We illustrate how transforming receptiveness, a particular social science construct, into a computational framework, can make LLM generations more aligned with human perceptions. We analyze and discuss the implications of our results, and highlight how a tool based on our framework might be used for more teachable and creative content moderation.

------------

`[2405.16884] Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching <https://arxiv.org/abs/2405.16884>`__ 匹配、比较还是选择?用于实体匹配的大型语言模型研究

::

    replaced with revised version Sun, 23 Jun 2024 13:42:02 GMT
    Submission history From: Tianshu Wang [view email]
    [v1] Mon, 27 May 2024 07:05:27 UTC (357 KB)
    [v2] Sun, 23 Jun 2024 13:42:02 UTC (213 KB)
    Tianshu Wang, Xiaoyang Chen, Hongyu Lin, Xuanang Chen, Xianpei Han, Hao Wang, Zhenyu Zeng, Le Sun

Entity matching (EM) is a critical step in entity resolution (ER). Recently, entity matching based on large language models (LLMs) has shown great promise. However, current LLM-based entity matching approaches typically follow a binary matching paradigm that ignores the global consistency between record relationships. In this paper, we investigate various methodologies for LLM-based entity matching that incorporate record interactions from different perspectives. Specifically, we comprehensively compare three representative strategies: matching, comparing, and selecting, and analyze their respective advantages and challenges in diverse scenarios. Based on our findings, we further design a compound entity matching framework (ComEM) that leverages the composition of multiple strategies and LLMs. ComEM benefits from the advantages of different sides and achieves improvements in both effectiveness and efficiency. Experimental results on 8 ER datasets and 9 LLMs verify the superiority of incorporating record interactions through the selecting strategy, as well as the further cost-effectiveness brought by ComEM.

------------

`[2406.00020] Harmful Speech Detection by Language Models Exhibits Gender-Queer Dialect Bias <https://arxiv.org/abs/2406.00020>`__ 语言模型的有害语音检测表现出性别歧视的方言偏见

::

    replaced with revised version Fri, 21 Jun 2024 18:01:06 GMT
    Submission history From: Rebecca Dorn [view email]
    [v1] Thu, 23 May 2024 18:07:28 UTC (10,349 KB)
    [v2] Fri, 21 Jun 2024 18:01:06 UTC (10,350 KB)
    Rebecca Dorn, Lee Kezar, Fred Morstatter, Kristina Lerman

Content moderation on social media platforms shapes the dynamics of online discourse, influencing whose voices are amplified and whose are suppressed. Recent studies have raised concerns about the fairness of content moderation practices, particularly for aggressively flagging posts from transgender and non-binary individuals as toxic. In this study, we investigate the presence of bias in harmful speech classification of gender-queer dialect online, focusing specifically on the treatment of reclaimed slurs. We introduce a novel dataset, QueerReclaimLex, based on 109 curated templates exemplifying non-derogatory uses of LGBTQ+ slurs. Dataset instances are scored by gender-queer annotators for potential harm depending on additional context about speaker identity. We systematically evaluate the performance of five off-the-shelf language models in assessing the harm of these texts and explore the effectiveness of chain-of-thought prompting to teach large language models (LLMs) to leverage author identity context. We reveal a tendency for these models to inaccurately flag texts authored by gender-queer individuals as harmful. Strikingly, across all LLMs the performance is poorest for texts that show signs of being written by individuals targeted by the featured slur (F1 <= 0.24). We highlight an urgent need for fairness and inclusivity in content moderation systems. By uncovering these biases, this work aims to inform the development of more equitable content moderation practices and contribute to the creation of inclusive online spaces for all users.

------------

`[2406.03689] Evaluating the World Model Implicit in a Generative Model <https://arxiv.org/abs/2406.03689>`__ 评估生成模型中隐含的世界模型

::

    replaced with revised version Sat, 22 Jun 2024 18:23:08 GMT
    Submission history From: Keyon Vafa [view email]
    [v1] Thu, 6 Jun 2024 02:20:31 UTC (10,082 KB)
    [v2] Sat, 22 Jun 2024 18:23:08 UTC (10,218 KB)
    Keyon Vafa, Justin Y. Chen, Jon Kleinberg, Sendhil Mullainathan, Ashesh Rambachan

Recent work suggests that large language models may implicitly learn world models. How should we assess this possibility? We formalize this question for the case where the underlying reality is governed by a deterministic finite automaton. This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry. We propose new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory. We illustrate their utility in three domains: game playing, logic puzzles, and navigation. In all domains, the generative models we consider do well on existing diagnostics for assessing world models, but our evaluation metrics reveal their world models to be far less coherent than they appear. Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead it to fail badly. Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; our results suggest new ways to assess how close a given model is to that goal.

------------

`[2406.12746] Rationale-based Ensemble of Multiple QA Strategies for Zero-shot Knowledge-based VQA <https://arxiv.org/abs/2406.12746>`__ 基于理论基础的多QA策略集成的零样本知识库VQA

::

    replaced with revised version Sun, 23 Jun 2024 03:06:42 GMT
    Submission history From: Miaoyu Li [view email]
    [v1] Tue, 18 Jun 2024 16:06:38 UTC (1,443 KB)
    [v2] Wed, 19 Jun 2024 02:02:13 UTC (1,443 KB)
    [v3] Sun, 23 Jun 2024 03:06:42 UTC (1,443 KB)
    Miaoyu Li, Haoxin Li, Zilin Du, and Boyang Li

Knowledge-based Visual Qustion-answering (K-VQA) necessitates the use of background knowledge beyond what is depicted in the image. Current zero-shot K-VQA methods usually translate an image to a single type of textual decision context and use a text-based model to answer the question based on it, which conflicts with the fact that K-VQA questions often require the combination of multiple question-answering strategies. In light of this, we propose Rationale-based Ensemble of Answer Context Tactics (REACT) to achieve a dynamic ensemble of multiple question-answering tactics, comprising Answer Candidate Generation (ACG) and Rationale-based Strategy Fusion (RSF). In ACG, we generate three distinctive decision contexts to provide different strategies for each question, resulting in the generation of three answer candidates. RSF generates automatic and mechanistic rationales from decision contexts for each candidate, allowing the model to select the correct answer from all candidates. We conduct comprehensive experiments on the OK-VQA and A-OKVQA datasets, and our method significantly outperforms state-of-the-art LLM-based baselines on all datasets.

------------

`[2406.13131] When Parts are Greater Than Sums: Individual LLM Components Can Outperform Full Models <https://arxiv.org/abs/2406.13131>`__ 当部分大于和时:单个LLM组件的表现可以超过整个模型

::

    replaced with revised version Mon, 24 Jun 2024 15:13:13 GMT
    Submission history From: Ting-Yun Chang [view email]
    [v1] Wed, 19 Jun 2024 00:48:44 UTC (9,784 KB)
    [v2] Mon, 24 Jun 2024 15:13:13 UTC (9,886 KB)
    Ting-Yun Chang, Jesse Thomason, Robin Jia

This paper studies in-context learning (ICL) by decomposing the output of large language models into the individual contributions of attention heads and MLPs (components). We observe curious components: good-performing ones that individually do well on a classification task, even when the model performs poorly; bad-performing ones that do much worse than chance; and label-biased components that always predict the same label. We find that component accuracies are well-correlated across different demonstration sets and perturbations of prompt templates, even when the full-model accuracy varies greatly. Based on our findings, we propose component reweighting, which learns to linearly re-scale the component activations from a few labeled examples. Given 24 labeled examples, our method improves by an average of 6.0% accuracy points over 24-shot ICL across 8 tasks on Llama-2-7B. Overall, this paper both enriches our understanding of ICL and provides a practical method for improvement by examining model internals.

------------

`[2406.13632] Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations <https://arxiv.org/abs/2406.13632>`__ 少镜头能在长语境中发挥作用吗?循环利用上下文来生成演示

::

    replaced with revised version Sun, 23 Jun 2024 07:19:22 GMT
    Submission history From: Arie Cattan [view email]
    [v1] Wed, 19 Jun 2024 15:28:29 UTC (9,015 KB)
    [v2] Sun, 23 Jun 2024 07:19:22 UTC (9,016 KB)
    Arie Cattan, Alon Jacovi, Alex Fabrikant, Jonathan Herzig, Roee Aharoni, Hannah Rashkin, Dror Marcus, Avinatan Hassidim, Yossi Matias, Idan Szpektor, Avi Caciularu

Despite recent advancements in Large Language Models (LLMs), their performance on tasks involving long contexts remains sub-optimal. In-Context Learning (ICL) with few-shot examples may be an appealing solution to enhance LLM performance in this scenario; However, naively adding ICL examples with long context introduces challenges, including substantial token overhead added for each few-shot example and context mismatch between the demonstrations and the target query. In this work, we propose to automatically generate few-shot examples for long context QA tasks by recycling contexts. Specifically, given a long input context (1-3k tokens) and a query, we generate additional query-output pairs from the given context as few-shot examples, while introducing the context only once. This ensures that the demonstrations are leveraging the same context as the target query while only adding a small number of tokens to the prompt. We further enhance each demonstration by instructing the model to explicitly identify the relevant paragraphs before the answer, which improves performance while providing fine-grained attribution to the answer source. We apply our method on multiple LLMs and obtain substantial improvements (+23\% on average across models) on various QA datasets with long context, especially when the answer lies within the middle of the context. Surprisingly, despite introducing only single-hop ICL examples, LLMs also successfully generalize to multi-hop long-context QA using our approach.

------------

`[2406.14952] ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models <https://arxiv.org/abs/2406.14952>`__ ESC-Eval:大型语言模型中情感支持对话的评估

::

    replaced with revised version Mon, 24 Jun 2024 12:24:52 GMT
    Submission history From: Haiquan Zhao [view email]
    [v1] Fri, 21 Jun 2024 08:03:33 UTC (2,560 KB)
    [v2] Mon, 24 Jun 2024 12:24:52 UTC (2,560 KB)
    Haiquan Zhao and Lingyu Li and Shisong Chen and Shuqi Kong and Jiaan Wang and Kexin Huang and Tianle Gu and Yixu Wang and Dandan Liang and Zhixu Li and Yan Teng and Yanghua Xiao and Yingchun Wang

Emotion Support Conversation (ESC) is a crucial application, which aims to reduce human stress, offer emotional guidance, and ultimately enhance human mental and physical well-being. With the advancement of Large Language Models (LLMs), many researchers have employed LLMs as the ESC models. However, the evaluation of these LLM-based ESCs remains uncertain. Inspired by the awesome development of role-playing agents, we propose an ESC Evaluation framework (ESC-Eval), which uses a role-playing agent to interact with ESC models, followed by a manual evaluation of the interactive dialogues. In detail, we first re-organize 2,801 role-playing cards from seven existing datasets to define the roles of the role-playing agent. Second, we train a specific role-playing model called ESC-Role which behaves more like a confused person than GPT-4. Third, through ESC-Role and organized role cards, we systematically conduct experiments using 14 LLMs as the ESC models, including general AI-assistant LLMs (ChatGPT) and ESC-oriented LLMs (ExTES-Llama). We conduct comprehensive human annotations on interactive multi-turn dialogues of different ESC models. The results show that ESC-oriented LLMs exhibit superior ESC abilities compared to general AI-assistant LLMs, but there is still a gap behind human performance. Moreover, to automate the scoring process for future ESC models, we developed ESC-RANK, which trained on the annotated data, achieving a scoring performance surpassing 35 points of GPT-4. Our data and code are available at this https URL.

------------

`[2002.01987] Function approximation by neural nets in the mean-field regime: Entropic regularization and controlled McKean-Vlasov dynamics <https://arxiv.org/abs/2002.01987>`__ 平均场状态下神经网络函数逼近:熵正则化和受控McKean-Vlasov动力学

::

    replaced with revised version Sat, 22 Jun 2024 16:57:27 GMT
    Submission history From: Maxim Raginsky [view email]
    [v1] Wed, 5 Feb 2020 20:50:33 UTC (28 KB)
    [v2] Mon, 10 Feb 2020 07:11:07 UTC (28 KB)
    [v3] Mon, 23 Mar 2020 21:47:47 UTC (28 KB)
    [v4] Sat, 22 Jun 2024 16:57:27 UTC (28 KB)
    Belinda Tzen and Maxim Raginsky

We consider the problem of function approximation by two-layer neural nets with random weights that are "nearly Gaussian" in the sense of Kullback-Leibler divergence. Our setting is the mean-field limit, where the finite population of neurons in the hidden layer is replaced by a continuous ensemble. We show that the problem can be phrased as global minimization of a free energy functional on the space of (finite-length) paths over probability measures on the weights. This functional trades off the $L^2$ approximation risk of the terminal measure against the KL divergence of the path with respect to an isotropic Brownian motion prior. We characterize the unique global minimizer and examine the dynamics in the space of probability measures over weights that can achieve it. In particular, we show that the optimal path-space measure corresponds to the Föllmer drift, the solution to a McKean-Vlasov optimal control problem closely related to the classic Schrödinger bridge problem. While the Föllmer drift cannot in general be obtained in closed form, thus limiting its potential algorithmic utility, we illustrate the viability of the mean-field Langevin diffusion as a finite-time approximation under various conditions on entropic regularization. Specifically, we show that it closely tracks the Föllmer drift when the regularization is such that the minimizing density is log-concave.

------------

`[2310.02905] Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers <https://arxiv.org/abs/2310.02905>`__ 用你的直觉:使用神经匪结合transformer对llm进行指令优化

::

    replaced with revised version Sun, 23 Jun 2024 23:59:53 GMT
    Submission history From: Zhongxiang Dai [view email]
    [v1] Mon, 2 Oct 2023 02:01:16 UTC (1,300 KB)
    [v2] Fri, 31 May 2024 16:27:53 UTC (1,729 KB)
    [v3] Sun, 23 Jun 2024 23:59:53 UTC (1,730 KB)
    Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low

Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algorithm which replaces the GP in BO by an NN surrogate to optimize instructions for black-box LLMs. More importantly, the neural bandit algorithm allows us to naturally couple the NN surrogate with the hidden representation learned by a pre-trained transformer (i.e., an open-source LLM), which significantly boosts its performance. These motivate us to propose our INSTruction optimization usIng Neural bandits Coupled with Transformers (INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use extensive experiments to show that INSTINCT consistently outperforms baselines in different tasks, e.g., various instruction induction tasks and the task of improving zero-shot chain-of-thought instructions. Our code is available at this https URL.

------------

`[2402.02834] Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods <https://arxiv.org/abs/2402.02834>`__ 缩短的LLaMA:大型语言模型的深度剪枝与再训练方法比较

::

    replaced with revised version Sun, 23 Jun 2024 08:45:33 GMT
    Submission history From: Bo-Kyeong Kim Ph.D. [view email]
    [v1] Mon, 5 Feb 2024 09:44:49 UTC (1,144 KB)
    [v2] Sun, 23 Jun 2024 08:45:33 UTC (1,723 KB)
    Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, Hyoung-Kyu Song

Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that simple depth pruning can effectively compress LLMs while achieving comparable or superior performance to recent width pruning studies. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. In retraining pruned models for quality recovery, continued pretraining on a large corpus markedly outperforms LoRA-based tuning, particularly at severe pruning ratios. We hope this work can help build compact yet capable LLMs. Code and models can be found at: this https URL

------------

`[2402.03244] Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills <https://arxiv.org/abs/2402.03244>`__ 技能集优化:通过可转移技能强化语言模型行为

::

    replaced with revised version Sat, 22 Jun 2024 18:58:09 GMT
    Submission history From: Kolby Nottingham [view email]
    [v1] Mon, 5 Feb 2024 17:59:00 UTC (360 KB)
    [v2] Sat, 22 Jun 2024 18:58:09 UTC (386 KB)
    Kolby Nottingham, Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Sameer Singh, Peter Clark, Roy Fox

Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%.

------------

`[2402.09834] All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining <https://arxiv.org/abs/2402.09834>`__ All in One和One for All:一种简单有效的跨域图预训练方法

::

    replaced with revised version Sat, 22 Jun 2024 13:29:36 GMT
    Submission history From: Haihong Zhao [view email]
    [v1] Thu, 15 Feb 2024 09:55:39 UTC (226 KB)
    [v2] Sat, 22 Jun 2024 13:29:36 UTC (227 KB)
    Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, and Jia Li

Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.

------------

`[2402.11235] ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs <https://arxiv.org/abs/2402.11235>`__ ZeroG:图中跨数据集零样本迁移性研究

::

    replaced with revised version Mon, 24 Jun 2024 03:34:02 GMT
    Submission history From: Yuhan Li [view email]
    [v1] Sat, 17 Feb 2024 09:52:43 UTC (980 KB)
    [v2] Mon, 24 Jun 2024 03:34:02 UTC (927 KB)
    Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li

With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to zero-shot transferability in graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a prompt-based subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using prompting nodes and neighborhood aggregation, respectively. We further adopt a lightweight fine-tuning strategy that reduces the risk of overfitting and maintains the zero-shot learning efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset zero-shot transferability, opening pathways for the development of graph foundation models. Codes and data are available at this https URL.

------------

`[2402.11518] Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network <https://arxiv.org/abs/2402.11518>`__ 大规模语言模型驱动的异构信息网络元结构发现

::

    replaced with revised version Sat, 22 Jun 2024 04:03:30 GMT
    Submission history From: Lin Chen [view email]
    [v1] Sun, 18 Feb 2024 09:21:12 UTC (7,307 KB)
    [v2] Sat, 22 Jun 2024 04:03:30 UTC (2,415 KB)
    Lin Chen, Fengli Xu, Nian Li, Zhenyu Han, Meng Wang, Yong Li, Pan Hui

Heterogeneous information networks (HIN) have gained increasing popularity in recent years for capturing complex relations between diverse types of nodes. Meta-structures are proposed as a useful tool to identify the important patterns in HINs, but hand-crafted meta-structures pose significant challenges for scaling up, drawing wide research attention towards developing automatic search algorithms. Previous efforts primarily focused on searching for meta-structures with good empirical performance, overlooking the importance of human comprehensibility and generalizability. To address this challenge, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose ReStruct, a meta-structure search framework that integrates LLM reasoning into the evolutionary procedure. ReStruct uses a grammar translator to encode the meta-structures into natural language sentences, and leverages the reasoning power of LLMs to evaluate their semantic feasibility. Besides, ReStruct also employs performance-oriented evolutionary operations. These two competing forces allow ReStruct to jointly optimize the semantic explainability and empirical performance of meta-structures. Furthermore, ReStruct contains a differential LLM explainer to generate and refine natural language explanations for the discovered meta-structures by reasoning through the search history. Experiments on eight representative HIN datasets demonstrate that ReStruct achieves state-of-the-art performance in both recommendation and node classification tasks. Moreover, a survey study involving 73 graduate students shows that the discovered meta-structures and generated explanations by ReStruct are substantially more comprehensible. Our code and questionnaire are available at this https URL.

------------

`[2402.11838] UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction <https://arxiv.org/abs/2402.11838>`__ UniST:一种快速赋能的城市时空预测通用模型

::

    replaced with revised version Mon, 24 Jun 2024 02:51:27 GMT
    Submission history From: Yuan Yuan [view email]
    [v1] Mon, 19 Feb 2024 05:04:11 UTC (3,122 KB)
    [v2] Mon, 20 May 2024 13:18:47 UTC (3,123 KB)
    [v3] Thu, 23 May 2024 13:01:23 UTC (3,122 KB)
    [v4] Mon, 24 Jun 2024 02:51:27 UTC (4,127 KB)
    [v5] Mon, 1 Jul 2024 02:51:58 UTC (4,126 KB)
    Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li

Urban spatio-temporal prediction is crucial for informed decision-making, such as traffic management, resource optimization, and emergence response. Despite remarkable breakthroughs in pretrained natural language models that enable one model to handle diverse tasks, a universal solution for spatio-temporal prediction remains challenging Existing prediction approaches are typically tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive domain-specific training data. In this study, we introduce UniST, a universal model designed for general urban spatio-temporal prediction across a wide range of scenarios. Inspired by large language models, UniST achieves success through: (i) utilizing diverse spatio-temporal data from different scenarios, (ii) effective pre-training to capture complex spatio-temporal dynamics, (iii) knowledge-guided prompts to enhance generalization capabilities. These designs together unlock the potential of building a universal model for various scenarios Extensive experiments on more than 20 spatio-temporal scenarios demonstrate UniST's efficacy in advancing state-of-the-art performance, especially in few-shot and zero-shot prediction. The datasets and code implementation are released on this https URL.

------------

`[2402.17879] Automated Statistical Model Discovery with Language Models <https://arxiv.org/abs/2402.17879>`__ 基于语言模型的统计模型自动发现

::

    replaced with revised version Sat, 22 Jun 2024 05:08:30 GMT
    Submission history From: Michael Li [view email]
    [v1] Tue, 27 Feb 2024 20:33:22 UTC (1,058 KB)
    [v2] Sat, 22 Jun 2024 05:08:30 UTC (1,280 KB)
    Michael Y. Li, Emily B. Fox, Noah D. Goodman

Statistical model discovery is a challenging search over a vast space of models subject to domain-specific constraints. Efficiently searching over this space requires expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, which are key restrictions of previous systems. We evaluate our method in three settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving expert models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method identifies models on par with human expert designed models and extends classic models in interpretable ways. Our results highlight the promise of LM-driven model discovery.

------------

`[2402.19361] Watermark Stealing in Large Language Models <https://arxiv.org/abs/2402.19361>`__ 大型语言模型中的水印窃取

::

    replaced with revised version Mon, 24 Jun 2024 14:48:29 GMT
    Submission history From: Nikola JovanoviÄ‡ [view email]
    [v1] Thu, 29 Feb 2024 17:12:39 UTC (569 KB)
    [v2] Mon, 24 Jun 2024 14:48:29 UTC (408 KB)
    Nikola Jovanovi\'c, Robin Staab, Martin Vechev

LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as hypothesized in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at this https URL.

------------

`[2403.07379] Hallmarks of Optimization Trajectories in Neural Networks: Directional Exploration and Redundancy <https://arxiv.org/abs/2403.07379>`__ 

::

    replaced with revised version Mon, 24 Jun 2024 04:53:34 GMT
    Submission history From: Sidak Pal Singh [view email]
    [v1] Tue, 12 Mar 2024 07:32:47 UTC (15,321 KB)
    [v2] Mon, 24 Jun 2024 04:53:34 UTC (36,807 KB)
    Sidak Pal Singh, Bobby He, Thomas Hofmann, Bernhard Sch\"olkopf

We propose a fresh take on understanding the mechanisms of neural networks by analyzing the rich directional structure of optimization trajectories, represented by their pointwise parameters. Towards this end, we introduce some natural notions of the complexity of optimization trajectories, both qualitative and quantitative, which hallmark the directional nature of optimization in neural networks: when is there redundancy, and when exploration. We use them to reveal the inherent nuance and interplay involved between various optimization choices, such as momentum and weight decay. Further, the trajectory perspective helps us see the effect of scale on regularizing the directional nature of trajectories, and as a by-product, we also observe an intriguing heterogeneity of Q,K,V dynamics in the middle attention layers in LLMs and which is homogenized by scale. Importantly, we put the significant directional redundancy observed to the test by demonstrating that training only scalar batchnorm parameters some while into training matches the performance of training the entire network, which thus exhibits the potential of hybrid optimization schemes that are geared towards efficiency.

------------

`[2405.15861] Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization <https://arxiv.org/abs/2405.15861>`__ 基于零阶优化实现联邦学习中的无维通信

::

    replaced with revised version Mon, 24 Jun 2024 04:52:25 GMT
    Submission history From: Zhe Li [view email]
    [v1] Fri, 24 May 2024 18:07:05 UTC (2,373 KB)
    [v2] Mon, 24 Jun 2024 04:52:25 UTC (2,373 KB)
    Zhe Li, Bicheng Ying, Zidong Liu, Haibo Yang

Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. However, the substantial communication costs associated with FL pose a significant challenge to its efficiency. Specifically, in each communication round, the communication costs scale linearly with the model's dimension, which presents a formidable obstacle, especially in large model scenarios. Despite various communication efficient strategies, the intrinsic dimension-dependent communication cost remains a major bottleneck for current FL implementations. In this paper, we introduce a novel dimension-free communication strategy for FL, leveraging zero-order optimization techniques. We propose a new algorithm, FedDisco, which facilitates the transmission of only a constant number of scalar values between clients and the server in each communication round, thereby reducing the communication cost from $\mathscr{O}(d)$ to $\mathscr{O}(1)$, where $d$ is the dimension of the model parameters. Theoretically, in non-convex functions, we prove that our algorithm achieves state-of-the-art rates, which show a linear speedup of the number of clients and local steps under standard assumptions and dimension-free rate for low effective rank scenarios. Empirical evaluations through classic deep learning training and large language model fine-tuning substantiate significant reductions in communication overhead compared to traditional FL approaches. Our code is available at this https URL.

------------

`[2311.03366] Functional Overlap Reranking for Neural Code Generation <https://arxiv.org/abs/2311.03366>`__ 基于功能重叠重排序的神经代码生成

::

    replaced with revised version Sat, 22 Jun 2024 17:42:32 GMT
    Submission history From: Nghi D. Q. Bui [view email]
    [v1] Mon, 16 Oct 2023 22:20:31 UTC (1,144 KB)
    [v2] Mon, 8 Apr 2024 21:54:14 UTC (705 KB)
    [v3] Sat, 22 Jun 2024 17:42:32 UTC (854 KB)
    Hung Quoc To, Minh Huynh Nguyen, Nghi D. Q. Bui

Code Large Language Models (CodeLLMs) have ushered in a new era in code generation advancements. However, selecting the best code solutions from all possible CodeLLM outputs remains a challenge. Previous methods often overlooked the intricate functional similarities and interactions between solution clusters. We introduce SRank, a novel reranking strategy for selecting the best solutions from code generation, focusing on modeling the relationships between clusters of solutions. By quantifying the functional overlap between solution clusters, our approach provides a better ranking strategy for code solutions. Empirical results show that our method achieves remarkable results on the pass@1 score. For instance, on the Human-Eval benchmark, we achieve 69.66% in pass@1 with Codex002, 75.31% with WizardCoder, 53.99% with StarCoder, and 60.55% with CodeGen, surpassing state-of-the-art code generation reranking methods such as CodeT and Coder-Reviewer on the same CodeLLM by a significant margin (approximately 6.1% improvement on average). Even in scenarios with a limited number of sampled solutions and test cases, our approach demonstrates robustness and superiority, marking a new benchmark in code generation reranking. Our implementation can be found at this https URL.

------------

`[2311.10947] RecExplainer: Aligning Large Language Models for Explaining Recommendation Models <https://arxiv.org/abs/2311.10947>`__ RecExplainer:对齐大型语言模型以解释推荐模型

::

    replaced with revised version Sat, 22 Jun 2024 08:34:47 GMT
    Submission history From: Yuxuan Lei [view email]
    [v1] Sat, 18 Nov 2023 03:05:43 UTC (3,899 KB)
    [v2] Sat, 22 Jun 2024 08:34:47 UTC (3,233 KB)
    Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie

Recommender systems are widely used in online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often function as a black box, making them less transparent and reliable for both users and developers. Recently, large language models (LLMs) have demonstrated remarkable intelligence in understanding, reasoning, and instruction following. This paper presents the initial exploration of using LLMs as surrogate models to explaining black-box recommender models. The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models. By leveraging LLMs' own extensive world knowledge and multi-step reasoning abilities, these aligned LLMs can serve as advanced surrogates, capable of reasoning about observations. Moreover, employing natural language as an interface allows for the creation of customizable explanations that can be adapted to individual user preferences. To facilitate an effective alignment, we introduce three methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to mimic the target model's behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model's behavior; hybrid alignment combines both language and latent spaces. Comprehensive experiments conducted on three public datasets show that our approach yields promising results in understanding and mimicking target models, producing high-quality, high-fidelity, and distinct explanations. Our code is available at this https URL.

------------

`[2311.14381] Potential Societal Biases of ChatGPT in Higher Education: A Scoping Review <https://arxiv.org/abs/2311.14381>`__ 高等教育ChatGPT的潜在社会偏见:一项范围综述

::

    replaced with revised version Sat, 22 Jun 2024 11:37:05 GMT
    Submission history From: Fei Cheng [view email]
    [v1] Fri, 24 Nov 2023 10:00:23 UTC (379 KB)
    [v2] Sat, 22 Jun 2024 11:37:05 UTC (397 KB)
    Ming Li, Ariunaa Enkhtur, Beverley Anne Yamamoto, Fei Cheng

Purpose:Generative Artificial Intelligence (GAI) models, such as ChatGPT, may inherit or amplify societal biases due to their training on extensive datasets. With the increasing usage of GAI by students, faculty, and staff in higher education institutions (HEIs), it is urgent to examine the ethical issues and potential biases associated with these technologies. Design/Approach/Methods:This scoping review aims to elucidate how biases related to GAI in HEIs have been researched and discussed in recent academic publications. We categorized the potential societal biases that GAI might cause in the field of higher education. Our review includes articles written in English, Chinese, and Japanese across four main databases, focusing on GAI usage in higher education and bias. Findings:Our findings reveal that while there is meaningful scholarly discussion around bias and discrimination concerning LLMs in the AI field, most articles addressing higher education approach the issue superficially. Few articles identify specific types of bias under different circumstances, and there is a notable lack of empirical research. Most papers in our review focus primarily on educational and research fields related to medicine and engineering, with some addressing English education. However, there is almost no discussion regarding the humanities and social sciences. Additionally, a significant portion of the current discourse is in English and primarily addresses English-speaking contexts. Originality/Value:To the best of our knowledge, our study is the first to summarize the potential societal biases in higher education. This review highlights the need for more in-depth studies and empirical work to understand the specific biases that GAI might introduce or amplify in educational settings, guiding the development of more ethical AI applications in higher education.

------------

`[2402.17012] Pandora's White-Box: Precise Training Data Detection and Extraction in Large Language Models <https://arxiv.org/abs/2402.17012>`__ 潘多拉的白盒:大型语言模型中的精确训练数据检测和提取

::

    replaced with revised version Mon, 24 Jun 2024 16:18:45 GMT
    Submission history From: Seth Neel [view email]
    [v1] Mon, 26 Feb 2024 20:41:50 UTC (2,727 KB)
    [v2] Tue, 28 May 2024 20:42:01 UTC (15,045 KB)
    [v3] Mon, 24 Jun 2024 16:18:45 UTC (29,016 KB)
    Jeffrey G. Wang, Jason Wang, Marvin Li, Seth Neel

In this paper we develop state-of-the-art privacy attacks against Large Language Models (LLMs), where an adversary with some access to the model tries to learn something about the underlying training data. Our headline results are new membership inference attacks (MIAs) against pretrained LLMs that perform hundreds of times better than baseline attacks, and a pipeline showing that over 50% (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, pretraining and fine-tuning data, and both MIAs and training data extraction. For pretraining data, we propose two new MIAs: a supervised neural network classifier that predicts training data membership on the basis of (dimensionality-reduced) model gradients, as well as a variant of this attack that only requires logit access to the model by leveraging recent model-stealing work on LLMs. To our knowledge this is the first MIA that explicitly incorporates model-stealing information. Both attacks outperform existing black-box baselines, and our supervised attack closes the gap between MIA attack success against LLMs and the strongest known attacks for other machine learning models. In fine-tuning, we find that a simple attack based on the ratio of the loss between the base and fine-tuned models is able to achieve near-perfect MIA performance; we then leverage our MIA to extract a large fraction of the fine-tuning dataset from fine-tuned Pythia and Llama models. Our code is available at this http URL.

------------

`[2406.07595] VulDetectBench: Evaluating the Deep Capability of Vulnerability Detection with Large Language Models <https://arxiv.org/abs/2406.07595>`__ VulDetectBench:基于大型语言模型的漏洞检测深度能力评估

::

    replaced with revised version Mon, 24 Jun 2024 09:02:57 GMT
    Submission history From: Yu Liu [view email]
    [v1] Tue, 11 Jun 2024 13:42:57 UTC (1,791 KB)
    [v2] Fri, 14 Jun 2024 04:36:42 UTC (1,791 KB)
    [v3] Mon, 24 Jun 2024 09:02:57 UTC (2,283 KB)
    Yu Liu, Lang Gao, Mingxin Yang, Yu Xie, Ping Chen, Xiaojin Zhang, Wei Chen

Large Language Models (LLMs) have training corpora containing large amounts of program code, greatly improving the model's code comprehension and generation capabilities. However, sound comprehensive research on detecting program vulnerabilities, a more specific task related to code, and evaluating the performance of LLMs in this more specialized scenario is still lacking. To address common challenges in vulnerability analysis, our study introduces a new benchmark, VulDetectBench, specifically designed to assess the vulnerability detection capabilities of LLMs. The benchmark comprehensively evaluates LLM's ability to identify, classify, and locate vulnerabilities through five tasks of increasing difficulty. We evaluate the performance of 17 models (both open- and closed-source) and find that while existing models can achieve over 80% accuracy on tasks related to vulnerability identification and classification, they still fall short on specific, more detailed vulnerability analysis tasks, with less than 30% accuracy, making it difficult to provide valuable auxiliary information for professional vulnerability mining. Our benchmark effectively evaluates the capabilities of various LLMs at different levels in the specific task of vulnerability detection, providing a foundation for future research and improvements in this critical area of code security. VulDetectBench is publicly available at this https URL.

------------

`[2406.11548] AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation <https://arxiv.org/abs/2406.11548>`__ AIC MLLM:面向鲁棒机器人操纵的自主交互修正MLLM

::

    replaced with revised version Sun, 23 Jun 2024 12:58:01 GMT
    Submission history From: Chuyan Xiong [view email]
    [v1] Mon, 17 Jun 2024 13:44:53 UTC (3,384 KB)
    [v2] Sun, 23 Jun 2024 12:58:01 UTC (3,384 KB)
    Chuyan Xiong, Chengyu Shen, Xiaoqi Li, Kaichen Zhou, Jiaming Liu, Ruiping Wang, Hao Dong

The ability to reflect on and correct failures is crucial for robotic systems to interact stably with real-life objects.Observing the generalization and reasoning capabilities of Multimodal Large Language Models (MLLMs), previous approaches have aimed to utilize these models to enhance robotic systems accordingly.However, these methods typically focus on high-level planning corrections using an additional MLLM, with limited utilization of failed samples to correct low-level contact poses. To address this gap, we propose an Autonomous Interactive Correction (AIC) MLLM, which makes use of previous low-level interaction experiences to correct SE(3) pose predictions. Specifically, AIC MLLM is initially fine-tuned to acquire both pose prediction and feedback prompt comprehension abilities.We carefully design two types of prompt instructions through interactions with objects: 1) visual masks to highlight unmovable parts for position correction, and 2)textual descriptions to indicate potential directions for rotation correction.During inference, a Feedback Information Extraction module is introduced to recognize the failure cause, allowing AIC MLLM to adaptively correct the pose prediction using the corresponding prompts. To further enhance manipulation stability, we devise a Test Time Adaptation strategy that enables AIC MLLM to better adapt to the current scene configuration.Finally, extensive experiments are conducted in both simulated and real-world environments to evaluate the proposed method. The results demonstrate that our AIC MLLM can efficiently correct failure samples by leveraging interaction experience prompts.Real-world demonstration can be found at this https URL

------------

`[2401.03003] AST-T5: Structure-Aware Pretraining for Code Generation and Understanding <https://arxiv.org/abs/2401.03003>`__ AST-T5:面向代码生成和理解的结构感知预训练

::

    replaced with revised version Sun, 23 Jun 2024 01:24:33 GMT
    Submission history From: Linyuan Gong [view email]
    [v1] Fri, 5 Jan 2024 06:51:08 UTC (387 KB)
    [v2] Fri, 9 Feb 2024 05:15:07 UTC (430 KB)
    [v3] Thu, 7 Mar 2024 05:03:11 UTC (430 KB)
    [v4] Sun, 23 Jun 2024 01:24:33 UTC (236 KB)
    Linyuan Gong, Mostafa Elhoushi, Alvin Cheung

Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our code and model are publicly available at this https URL.

------------

`[2403.19181] Make Large Language Model a Better Ranker <https://arxiv.org/abs/2403.19181>`__ 使大型语言模型成为更好的排序器

::

    replaced with revised version Mon, 24 Jun 2024 13:22:22 GMT
    Submission history From: Wen-Shuo Chao [view email]
    [v1] Thu, 28 Mar 2024 07:22:16 UTC (384 KB)
    [v2] Mon, 24 Jun 2024 13:22:22 UTC (378 KB)
    Wenshuo Chao, Zhi Zheng, Hengshu Zhu, Hao Liu

Large Language Models (LLMs) demonstrate robust capabilities across various fields, leading to a paradigm shift in LLM-enhanced Recommender System (RS). Research to date focuses on point-wise and pair-wise recommendation paradigms, which are inefficient for LLM-based recommenders due to high computational costs. However, existing list-wise approaches also fall short in ranking tasks due to misalignment between ranking objectives and next-token prediction. Moreover, these LLM-based methods struggle to effectively address the order relation among candidates, particularly given the scale of ratings. To address these challenges, this paper introduces the large language model framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks. Specifically, ALRO employs explicit feedback in a listwise manner by introducing soft lambda loss, a customized adaptation of lambda loss designed for optimizing order relations. This mechanism provides more accurate optimization goals, enhancing the ranking process. Additionally, ALRO incorporates a permutation-sensitive learning mechanism that addresses position bias, a prevalent issue in generative models, without imposing additional computational burdens during inference. Our evaluative studies reveal that ALRO outperforms both existing embedding-based recommendation methods and LLM-based recommendation baselines.

------------

`[2401.16445] OMPGPT: A Generative Pre-trained Transformer Model for OpenMP <https://arxiv.org/abs/2401.16445>`__ OMPGPT:面向OpenMP的生成式预训练Transformer模型

::

    replaced with revised version Sat, 22 Jun 2024 01:28:07 GMT
    Submission history From: Le Chen [view email]
    [v1] Sun, 28 Jan 2024 06:06:59 UTC (1,127 KB)
    [v2] Mon, 13 May 2024 16:17:38 UTC (1,441 KB)
    [v3] Sat, 22 Jun 2024 01:28:07 UTC (1,357 KB)
    Le Chen, Arijit Bhattacharjee, Nesreen Ahmed, Niranjan Hasabnis, Gal Oren, Vy Vo, Ali Jannesari

Large language models (LLMs)such as ChatGPT have significantly advanced the field of Natural Language Processing (NLP). This trend led to the development of code-based large language models such as StarCoder, WizardCoder, and CodeLlama, which are trained extensively on vast repositories of code and programming languages. While the generic abilities of these code LLMs are useful for many programmers in tasks like code generation, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific model a smarter choice. This paper presents OMPGPT, a novel domain-specific model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we leverage prompt engineering techniques from the NLP domain to create Chain-of-OMP, an innovative strategy designed to enhance OMPGPT's effectiveness. Our extensive evaluations demonstrate that OMPGPT outperforms existing large language models specialized in OpenMP tasks and maintains a notably smaller size, aligning it more closely with the typical hardware constraints of HPC environments. We consider our contribution as a pivotal bridge, connecting the advantage of language models with the specific demands of HPC tasks.

------------

-----------
Index (156)
-----------

`[2406.15481] CSRT: Evaluation and Analysis of LLMs using Code-Switching Red-Teaming Dataset <https://arxiv.org/abs/2406.15481>`__ CSRT:基于代码切换Red-Teaming数据集的LLMs评估与分析

`[2406.15513] PKU-SafeRLHF: A Safety Alignment Preference Dataset for Llama Family Models <https://arxiv.org/abs/2406.15513>`__ PKU-SafeRLHF:羊驼家族模型安全对齐偏好数据集

`[2406.16455] Guardrails for avoiding harmful medical product recommendations and off-label promotion in generative AI models <https://arxiv.org/abs/2406.16455>`__ 生成式AI模型中避免有害医疗产品推荐和标签外推广的护栏

`[2406.16486] Towards Comprehensive Preference Data Collection for Reward Modeling <https://arxiv.org/abs/2406.16486>`__ 面向奖励模型的综合偏好数据收集

`[2406.15359] Constructing Multilingual Visual-Text Datasets Revealing Visual Multilingual Ability of Vision Language Models <https://arxiv.org/abs/2406.15359>`__ 揭示视觉语言模型视觉多语言能力的多语言视觉-文本数据集构建

`[2406.15444] Investigating the Robustness of LLMs on Math Word Problems <https://arxiv.org/abs/2406.15444>`__ 研究llm在数学应用题上的鲁棒性

`[2406.15465] RadEx: A Framework for Structured Information Extraction from Radiology Reports based on Large Language Models <https://arxiv.org/abs/2406.15465>`__ RadEx:基于大型语言模型的放射学报告结构化信息提取框架

`[2406.15470] Mental Disorder Classification via Temporal Representation of Text <https://arxiv.org/abs/2406.15470>`__ 基于文本时间表示的精神障碍分类

`[2406.15473] Intertwining CP and NLP: The Generation of Unreasonably Constrained Sentences <https://arxiv.org/abs/2406.15473>`__ 交织CP和NLP:不合理约束句子的生成

`[2406.15477] CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics <https://arxiv.org/abs/2406.15477>`__ CrisisSense-LLM:面向灾难信息学多标签社交媒体文本分类的指令微调大型语言模型

`[2406.15479] Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging <https://arxiv.org/abs/2406.15479>`__

`[2406.15480] On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion <https://arxiv.org/abs/2406.15480>`__ 在巨人的肩膀上:毫不费力的弱到强由动态Logits融合

`[2406.15483] Duplicate Detection with GenAI <https://arxiv.org/abs/2406.15483>`__ 使用GenAI进行重复检测

`[2406.15486] Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention <https://arxiv.org/abs/2406.15486>`__ 基于自适应结构稀疏注意力的长上下文LLM推理近无损加速

`[2406.15504] Dr.E Bridges Graphs with Large Language Models through Words <https://arxiv.org/abs/2406.15504>`__ Dr.E通过单词将图与大型语言模型联系起来

`[2406.15524] Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization <https://arxiv.org/abs/2406.15524>`__ 重新思考修剪大型语言模型:重构误差最小化的好处和缺陷

`[2406.15583] Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods <https://arxiv.org/abs/2406.15583>`__

`[2406.15593] News Deja Vu: Connecting Past and Present with Semantic Search <https://arxiv.org/abs/2406.15593>`__ 新闻似曾相识:用语义搜索连接过去和现在

`[2406.15673] Large Language Models have Intrinsic Self-Correction Ability <https://arxiv.org/abs/2406.15673>`__ 大型语言模型具有内在的自校正能力

`[2406.15708] Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization <https://arxiv.org/abs/2406.15708>`__ 教得更好还是表现得更聪明?关于自动提示优化中的指令和范例

`[2406.15718] Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models <https://arxiv.org/abs/2406.15718>`__ 超越回合制游戏:实现与双工模型的实时对话

`[2406.15720] Scaling Laws for Fact Memorization of Large Language Models <https://arxiv.org/abs/2406.15720>`__ 大型语言模型事实记忆的缩放定律

`[2406.15734] RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs <https://arxiv.org/abs/2406.15734>`__ RankAdaptor:面向结构剪枝llm的分层动态低秩自适应

`[2406.15741] Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level <https://arxiv.org/abs/2406.15741>`__ Ladder:一个与模型无关的框架，将基于llm的机器翻译提升到一个新的水平

`[2406.15781] DABL: Detecting Semantic Anomalies in Business Processes Using Large Language Models <https://arxiv.org/abs/2406.15781>`__ DABL:使用大型语言模型检测业务流程中的语义异常

`[2406.15796] Rethinking Entity-level Unlearning for Large Language Models <https://arxiv.org/abs/2406.15796>`__ 大型语言模型实体级遗忘的再思考

`[2406.15809] LaMSUM: A Novel Framework for Extractive Summarization of User Generated Content using LLMs <https://arxiv.org/abs/2406.15809>`__

`[2406.15883] SimSMoE: Solving Representational Collapse via Similarity Measure <https://arxiv.org/abs/2406.15883>`__ SimSMoE:基于相似性度量的表征坍缩求解

`[2406.15888] Real-time Speech Summarization for Medical Conversations <https://arxiv.org/abs/2406.15888>`__ 医学对话的实时语音摘要

`[2406.15891] The Unlikely Duel: Evaluating Creative Writing in LLMs through a Unique Scenario <https://arxiv.org/abs/2406.15891>`__ 不太可能的决斗:通过独特的场景评估llm的创意写作

`[2406.15927] Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs <https://arxiv.org/abs/2406.15927>`__ 语义熵探测:llm中鲁棒且廉价的幻觉检测

`[2406.15938] RuleR: Improving LLM Controllability by Rule-based Data Recycling <https://arxiv.org/abs/2406.15938>`__ RuleR:通过基于规则的数据回收提高LLM可控性

`[2406.15948] Teaching LLMs to Abstain across Languages via Multilingual Feedback <https://arxiv.org/abs/2406.15948>`__ 通过多语言反馈教llm跨语言弃权

`[2406.15951] Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration <https://arxiv.org/abs/2406.15951>`__

`[2406.15966] Evaluating the Effectiveness of the Foundational Models for Q&A Classification in Mental Health care <https://arxiv.org/abs/2406.15966>`__ 评估精神卫生保健问答分类基础模型的有效性

`[2406.15968] ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods <https://arxiv.org/abs/2406.15968>`__ 回顾:基于相对条件对数似然的隶属推断

`[2406.15981] Serial Position Effects of Large Language Models <https://arxiv.org/abs/2406.15981>`__ 大型语言模型的序列位置效应

`[2406.15996] Memorizing Documents with Guidance in Large Language Models <https://arxiv.org/abs/2406.15996>`__ 在大型语言模型中使用指南记忆文档

`[2406.16008] Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization <https://arxiv.org/abs/2406.16008>`__ 中间发现:校准位置注意力偏差可提高长上下文利用率

`[2406.16033] Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models <https://arxiv.org/abs/2406.16033>`__

`[2406.16069] FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models <https://arxiv.org/abs/2406.16069>`__ FastMem:提示符的快速记忆提高大型语言模型的上下文感知

`[2406.16135] Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models <https://arxiv.org/abs/2406.16135>`__ 多语言大型语言模型的跨语言能力和知识障碍

`[2406.16144] Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step <https://arxiv.org/abs/2406.16144>`__ 探针链:检验辊套分步的必要性和准确性

`[2406.16152] Towards Region-aware Bias Evaluation Metrics <https://arxiv.org/abs/2406.16152>`__ 区域感知偏差评估指标研究

`[2406.16203] LLMs' Classification Performance is Overclaimed <https://arxiv.org/abs/2406.16203>`__

`[2406.16229] Multi-Objective Linguistic Control of Large Language Models <https://arxiv.org/abs/2406.16229>`__ 大型语言模型的多目标语言控制

`[2406.16235] Preference Tuning For Toxicity Mitigation Generalizes Across Languages <https://arxiv.org/abs/2406.16235>`__ 缓解毒性的偏好调整适用于各种语言

`[2406.16253] LLMs assist NLP Researchers: Critique Paper (Meta-)Reviewing <https://arxiv.org/abs/2406.16253>`__ LLMs协助NLP研究人员:评论论文(元)评审

`[2406.16264] One Thousand and One Pairs: A "novel" challenge for long-context language models <https://arxiv.org/abs/2406.16264>`__ 《一千零一对:长上下文语言模型的“新”挑战》

`[2406.16275] Investigating the Influence of Prompt-Specific Shortcuts in AI Generated Text Detection <https://arxiv.org/abs/2406.16275>`__ 研究AI生成文本检测中提示特定快捷方式的影响

`[2406.16288] PlagBench: Exploring the Duality of Large Language Models in Plagiarism Generation and Detection <https://arxiv.org/abs/2406.16288>`__ PlagBench:探索大型语言模型在抄袭生成和检测中的二重性

`[2406.16294] LangSuitE: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments <https://arxiv.org/abs/2406.16294>`__ LangSuitE:具身文本环境中大型语言模型的规划、控制和交互

`[2406.16299] Compensate Quantization Errors: Make Weights Hierarchical to Compensate Each Other <https://arxiv.org/abs/2406.16299>`__

`[2406.16330] Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging <https://arxiv.org/abs/2406.16330>`__ 通过合并修剪:通过基于流形对齐的层合并压缩llm

`[2406.16341] EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records <https://arxiv.org/abs/2406.16341>`__ EHRCon:用于检查电子健康记录中非结构化笔记和结构化表之间一致性的数据集

`[2406.16356] Evaluation of Instruction-Following Ability for Large Language Models on Story-Ending Generation <https://arxiv.org/abs/2406.16356>`__ 大型语言模型在故事结尾生成中的指令跟随能力评估

`[2406.16377] On the Transformations across Reward Model, Parameter Update, and In-Context Prompt <https://arxiv.org/abs/2406.16377>`__ 关于奖励模型、参数更新和上下文提示的转换

`[2406.16382] UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models <https://arxiv.org/abs/2406.16382>`__ 大型语言模型序列决策能力评估UNO Arena

`[2406.16416] Multilingual Knowledge Editing with Language-Agnostic Factual Neurons <https://arxiv.org/abs/2406.16416>`__ 基于语言无关事实神经元的多语言知识编辑

`[2406.16441] UniCoder: Scaling Code Large Language Model via Universal Code <https://arxiv.org/abs/2406.16441>`__

`[2406.16508] Large Vocabulary Size Improves Large Language Models <https://arxiv.org/abs/2406.16508>`__ 大词汇量改进大型语言模型

`[2406.16528] Evaluating the Ability of Large Language Models to Reason about Cardinal Directions <https://arxiv.org/abs/2406.16528>`__ 大型语言模型对基本方向推理的能力评估

`[2406.16536] C-LLM: Learn to Check Chinese Spelling Errors Character by Character <https://arxiv.org/abs/2406.16536>`__ 学习逐字检查中文拼写错误

`[2406.16554] LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training <https://arxiv.org/abs/2406.16554>`__ LLaMA- moe:通过持续的预训练建立来自LLaMA的专家混合

`[2406.16567] Data Augmentation of Multi-turn Psychological Dialogue via Knowledge-driven Progressive Thought Prompting <https://arxiv.org/abs/2406.16567>`__ 基于知识驱动渐进式思维提示的多轮心理对话数据增强

`[2406.16655] Large Language Models Are Cross-Lingual Knowledge-Free Reasoners <https://arxiv.org/abs/2406.16655>`__

`[2406.16690] Scaling Laws for Linear Complexity Language Models <https://arxiv.org/abs/2406.16690>`__ 线性复杂性语言模型的缩放律

`[2406.16694] Task Oriented In-Domain Data Augmentation <https://arxiv.org/abs/2406.16694>`__ 面向任务的域内数据增强

`[2406.16714] AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models <https://arxiv.org/abs/2406.16714>`__ AutoDetect:面向大型语言模型自动缺陷检测的统一框架

`[2406.16767] The GPT-WritingPrompts Dataset: A Comparative Analysis of Character Portrayal in Short Stories <https://arxiv.org/abs/2406.16767>`__

`[2406.16777] Blending LLMs into Cascaded Speech Translation: KIT's Offline Speech Translation System for IWSLT 2024 <https://arxiv.org/abs/2406.16777>`__ 将llm融合到级联语音翻译中:KIT的IWSLT 2024离线语音翻译系统

`[2406.16779] It Is Not About What You Say, It Is About How You Say It: A Surprisingly Simple Approach for Improving Reading Comprehension <https://arxiv.org/abs/2406.16779>`__ 关键不在于你说了什么，而在于你怎么说:这是一种提高阅读理解能力的非常简单的方法

`[2406.16783] M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models <https://arxiv.org/abs/2406.16783>`__ M2Lingual:增强大型语言模型多语言多回合指令对齐

`[2406.16797] Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs <https://arxiv.org/abs/2406.16797>`__ 彩票适配:降低llm中的破坏性干扰

`[2406.16801] RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale <https://arxiv.org/abs/2406.16801>`__ RES-Q:评估存储库规模的大型语言模型系统的代码编辑

`[2406.16833] USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$onversations <https://arxiv.org/abs/2406.16833>`__ USDC: $\underline{U}$ser $\underline{S}$tance和$\underline{D}$ogmatism组成的数据集

`[2406.16858] EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees <https://arxiv.org/abs/2406.16858>`__ EAGLE-2:基于动态草案树的语言模型快速推理

`[2406.15534] Geneverse: A collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research <https://arxiv.org/abs/2406.15534>`__ Geneverse:用于基因组和蛋白质组学研究的开源多模态大型语言模型集合

`[2406.15568] Robust Reinforcement Learning from Corrupted Human Feedback <https://arxiv.org/abs/2406.15568>`__ 基于错误人类反馈的鲁棒强化学习

`[2406.15763] AllMatch: Exploiting All Unlabeled Data for Semi-Supervised Learning <https://arxiv.org/abs/2406.15763>`__ AllMatch:利用所有未标记数据进行半监督学习

`[2406.15765] Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration <https://arxiv.org/abs/2406.15765>`__

`[2406.15786] What Matters in Transformers? Not All Attention is Needed <https://arxiv.org/abs/2406.15786>`__ transformer中什么是重要的?并非所有的注意力都是需要的

`[2406.15890] Language Alignment via Nash-learning and Adaptive feedback <https://arxiv.org/abs/2406.15890>`__ 基于纳什学习和自适应反馈的语言对齐

`[2406.16252] Graph-Augmented LLMs for Personalized Health Insights: A Case Study in Sleep Analysis <https://arxiv.org/abs/2406.16252>`__ 面向个性化健康见解的图增强llm:睡眠分析案例研究

`[2406.16254] Confidence Regulation Neurons in Language Models <https://arxiv.org/abs/2406.16254>`__ 语言模型中的置信度调节神经元

`[2406.16308] Anomaly Detection of Tabular Data Using LLMs <https://arxiv.org/abs/2406.16308>`__ 基于LLMs的表格数据异常检测

`[2406.16349] AnnotatedTables: A Large Tabular Dataset with Language Model Annotations <https://arxiv.org/abs/2406.16349>`__

`[2406.16635] ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models <https://arxiv.org/abs/2406.16635>`__ ShadowLLM:基于预测器的大型语言模型上下文稀疏性

`[2406.16738] Inducing Group Fairness in LLM-Based Decisions <https://arxiv.org/abs/2406.16738>`__ 基于llm决策的群体公平性诱导

`[2406.16745] Bandits with Preference Feedback: A Stackelberg Game Perspective <https://arxiv.org/abs/2406.16745>`__ 基于偏好反馈的土匪:Stackelberg游戏视角

`[2406.16748] OCALM: Object-Centric Assessment with Language Models <https://arxiv.org/abs/2406.16748>`__ OCALM:以对象为中心的语言模型评估

`[2406.16810] PISTOL: Dataset Compilation Pipeline for Structural Unlearning of LLMs <https://arxiv.org/abs/2406.16810>`__ 手枪:用于llm结构遗忘的数据集编译管道

`[2406.15360] Generative AI Adoption in Classroom in Context of Technology Acceptance Model (TAM) and the Innovation Diffusion Theory (IDT) <https://arxiv.org/abs/2406.15360>`__ 技术接受模型(TAM)和创新扩散理论(IDT)背景下的生成性人工智能在课堂中的应用

`[2406.15508] What Teaches Robots to Walk, Teaches Them to Trade too -- Regime Adaptive Execution using Informed Data and LLMs <https://arxiv.org/abs/2406.15508>`__ 什么教机器人走路，教他们交易-使用知情数据和法学硕士制度自适应执行

`[2406.15609] Automated radiotherapy treatment planning guided by GPT-4Vision <https://arxiv.org/abs/2406.15609>`__

`[2406.15676] Inferring Pluggable Types with Machine Learning <https://arxiv.org/abs/2406.15676>`__ 用机器学习推断可插拔类型

`[2406.16093] Towards Natural Language-Driven Assembly Using Foundation Models <https://arxiv.org/abs/2406.16093>`__ 基于基础模型的自然语言驱动汇编研究

`[2406.16224] From Text to Test: AI-Generated Control Software for Materials Science Instruments <https://arxiv.org/abs/2406.16224>`__ 从文本到测试:材料科学仪器ai生成控制软件

`[2406.16333] Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models <https://arxiv.org/abs/2406.16333>`__ 提示一致性图像生成(PCIG):集成LLMs、知识图谱和可控扩散模型的统一框架

`[2406.16346] Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific Training Tasks <https://arxiv.org/abs/2406.16346>`__

`[2406.16386] Automatically Generating UI Code from Screenshot: A Divide-and-Conquer-Based Approach <https://arxiv.org/abs/2406.16386>`__ 从截图自动生成UI代码:一种基于分而治之的方法

`[2406.15963] Effectiveness of ChatGPT in explaining complex medical reports to patients <https://arxiv.org/abs/2406.15963>`__ ChatGPT在向患者解释复杂医疗报告中的有效性

`[2406.16332] DemoRank: Selecting Effective Demonstrations for Large Language Models in Ranking Task <https://arxiv.org/abs/2406.16332>`__ 士气低落:在排名任务中为大型语言模型选择有效的演示

`[2406.16562] EvalAlign: Evaluating Text-to-Image Models through Precision Alignment of Multimodal Large Models with Supervised Fine-Tuning to Human Annotations <https://arxiv.org/abs/2406.16562>`__ EvalAlign:通过对人工标注进行监督微调的多模态大型模型的精确对齐来评估文本到图像模型

`[2406.15540] Specify What? Enhancing Neural Specification Synthesis by Symbolic Methods <https://arxiv.org/abs/2406.15540>`__ 指定什么?用符号方法增强神经规范合成

`[2309.10253] GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts <https://arxiv.org/abs/2309.10253>`__ GPTFUZZER: Red将大型语言模型与自动生成的越狱提示结合起来

`[2402.05359] Prompting with Divide-and-Conquer Program Makes Large Language Models Discerning to Hallucination and Deception <https://arxiv.org/abs/2402.05359>`__ 分而治之的程序激励使大型语言模型能够分辨幻觉和欺骗

`[2406.12227] Interpretable Catastrophic Forgetting of Large Language Model Fine-tuning via Instruction Vector <https://arxiv.org/abs/2406.12227>`__ 基于指令向量微调的大型语言模型可解释灾难性遗忘

`[2406.13558] Enhancing Travel Choice Modeling with Large Language Models: A Prompt-Learning Approach <https://arxiv.org/abs/2406.13558>`__ 利用大型语言模型增强旅游选择建模:一种快速学习方法

`[2406.14343] iWISDM: Assessing instruction following in multimodal models at scale <https://arxiv.org/abs/2406.14343>`__ iWISDM:评估大规模多模态模型的指令跟随

`[2406.14903] GIEBench: Towards Holistic Evaluation of Group Identity-based Empathy for Large Language Models <https://arxiv.org/abs/2406.14903>`__ GIEBench:基于群体身份的大型语言模型共情的整体评估

`[2308.07269] EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models <https://arxiv.org/abs/2308.07269>`__ EasyEdit:面向大型语言模型的易用知识编辑框架

`[2309.07045] SafetyBench: Evaluating the Safety of Large Language Models <https://arxiv.org/abs/2309.07045>`__

`[2309.10952] LMDX: Language Model-based Document Information Extraction and Localization <https://arxiv.org/abs/2309.10952>`__ LMDX:基于语言模型的文档信息抽取与定位

`[2402.03049] EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models <https://arxiv.org/abs/2402.03049>`__ EasyInstruct:面向大型语言模型的易用指令处理框架

`[2402.04678] FaithLM: Towards Faithful Explanations for Large Language Models <https://arxiv.org/abs/2402.04678>`__ FaithLM:大型语言模型的忠实解释

`[2402.12842] PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning <https://arxiv.org/abs/2402.12842>`__ PromptKD:通过提示调优提取对学生友好的生成语言模型知识

`[2402.14208] LLM-Assisted Content Conditional Debiasing for Fair Text Embedding <https://arxiv.org/abs/2402.14208>`__ llm辅助的公平文本嵌入内容条件去偏

`[2403.03628] GPTopic: Dynamic and Interactive Topic Representations <https://arxiv.org/abs/2403.03628>`__ GPTopic:动态交互式主题表示

`[2403.04814] Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks <https://arxiv.org/abs/2403.04814>`__ LLMs在语法感知代码中间填充任务上的评估

`[2403.13737] EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation <https://arxiv.org/abs/2403.13737>`__ EthioLLM:带任务评估的埃塞俄比亚语多语言大型语言模型

`[2403.15250] Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach <https://arxiv.org/abs/2403.15250>`__ LLMs大规模评估结果的综合再评估:多层面统计方法

`[2403.16056] Qibo: A Large Language Model for Traditional Chinese Medicine <https://arxiv.org/abs/2403.16056>`__ Qibo:一个面向中医的大型语言模型

`[2403.16512] LLMs Are Few-Shot In-Context Low-Resource Language Learners <https://arxiv.org/abs/2403.16512>`__ llm是少样本的上下文低资源语言学习者

`[2404.01054] Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment <https://arxiv.org/abs/2404.01054>`__ 基于正则化Best-of-N采样的语言模型对齐奖励黑客行为

`[2404.04067] CLUE: A Clinical Language Understanding Evaluation for LLMs <https://arxiv.org/abs/2404.04067>`__ 线索:llm的临床语言理解评估

`[2405.14092] Large Language Models Can Self-Correct with Minimal Effort <https://arxiv.org/abs/2405.14092>`__

`[2405.15067] Promoting Constructive Deliberation: Reframing for Receptiveness <https://arxiv.org/abs/2405.15067>`__ 促进建设性审议:接受性重构

`[2405.16884] Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching <https://arxiv.org/abs/2405.16884>`__ 匹配、比较还是选择?用于实体匹配的大型语言模型研究

`[2406.00020] Harmful Speech Detection by Language Models Exhibits Gender-Queer Dialect Bias <https://arxiv.org/abs/2406.00020>`__ 语言模型的有害语音检测表现出性别歧视的方言偏见

`[2406.03689] Evaluating the World Model Implicit in a Generative Model <https://arxiv.org/abs/2406.03689>`__ 评估生成模型中隐含的世界模型

`[2406.12746] Rationale-based Ensemble of Multiple QA Strategies for Zero-shot Knowledge-based VQA <https://arxiv.org/abs/2406.12746>`__ 基于理论基础的多QA策略集成的零样本知识库VQA

`[2406.13131] When Parts are Greater Than Sums: Individual LLM Components Can Outperform Full Models <https://arxiv.org/abs/2406.13131>`__ 当部分大于和时:单个LLM组件的表现可以超过整个模型

`[2406.13632] Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations <https://arxiv.org/abs/2406.13632>`__ 少镜头能在长语境中发挥作用吗?循环利用上下文来生成演示

`[2406.14952] ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models <https://arxiv.org/abs/2406.14952>`__ ESC-Eval:大型语言模型中情感支持对话的评估

`[2002.01987] Function approximation by neural nets in the mean-field regime: Entropic regularization and controlled McKean-Vlasov dynamics <https://arxiv.org/abs/2002.01987>`__ 平均场状态下神经网络函数逼近:熵正则化和受控McKean-Vlasov动力学

`[2310.02905] Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers <https://arxiv.org/abs/2310.02905>`__ 用你的直觉:使用神经匪结合transformer对llm进行指令优化

`[2402.02834] Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods <https://arxiv.org/abs/2402.02834>`__ 缩短的LLaMA:大型语言模型的深度剪枝与再训练方法比较

`[2402.03244] Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills <https://arxiv.org/abs/2402.03244>`__ 技能集优化:通过可转移技能强化语言模型行为

`[2402.09834] All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining <https://arxiv.org/abs/2402.09834>`__ All in One和One for All:一种简单有效的跨域图预训练方法

`[2402.11235] ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs <https://arxiv.org/abs/2402.11235>`__ ZeroG:图中跨数据集零样本迁移性研究

`[2402.11518] Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network <https://arxiv.org/abs/2402.11518>`__ 大规模语言模型驱动的异构信息网络元结构发现

`[2402.11838] UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction <https://arxiv.org/abs/2402.11838>`__ UniST:一种快速赋能的城市时空预测通用模型

`[2402.17879] Automated Statistical Model Discovery with Language Models <https://arxiv.org/abs/2402.17879>`__ 基于语言模型的统计模型自动发现

`[2402.19361] Watermark Stealing in Large Language Models <https://arxiv.org/abs/2402.19361>`__ 大型语言模型中的水印窃取

`[2403.07379] Hallmarks of Optimization Trajectories in Neural Networks: Directional Exploration and Redundancy <https://arxiv.org/abs/2403.07379>`__

`[2405.15861] Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization <https://arxiv.org/abs/2405.15861>`__ 基于零阶优化实现联邦学习中的无维通信

`[2311.03366] Functional Overlap Reranking for Neural Code Generation <https://arxiv.org/abs/2311.03366>`__ 基于功能重叠重排序的神经代码生成

`[2311.10947] RecExplainer: Aligning Large Language Models for Explaining Recommendation Models <https://arxiv.org/abs/2311.10947>`__ RecExplainer:对齐大型语言模型以解释推荐模型

`[2311.14381] Potential Societal Biases of ChatGPT in Higher Education: A Scoping Review <https://arxiv.org/abs/2311.14381>`__ 高等教育ChatGPT的潜在社会偏见:一项范围综述

`[2402.17012] Pandora's White-Box: Precise Training Data Detection and Extraction in Large Language Models <https://arxiv.org/abs/2402.17012>`__ 潘多拉的白盒:大型语言模型中的精确训练数据检测和提取

`[2406.07595] VulDetectBench: Evaluating the Deep Capability of Vulnerability Detection with Large Language Models <https://arxiv.org/abs/2406.07595>`__ VulDetectBench:基于大型语言模型的漏洞检测深度能力评估

`[2406.11548] AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation <https://arxiv.org/abs/2406.11548>`__ AIC MLLM:面向鲁棒机器人操纵的自主交互修正MLLM

`[2401.03003] AST-T5: Structure-Aware Pretraining for Code Generation and Understanding <https://arxiv.org/abs/2401.03003>`__ AST-T5:面向代码生成和理解的结构感知预训练

`[2403.19181] Make Large Language Model a Better Ranker <https://arxiv.org/abs/2403.19181>`__ 使大型语言模型成为更好的排序器

`[2401.16445] OMPGPT: A Generative Pre-trained Transformer Model for OpenMP <https://arxiv.org/abs/2401.16445>`__ OMPGPT:面向OpenMP的生成式预训练Transformer模型

