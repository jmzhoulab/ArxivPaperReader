240611
========

----------
Survey (5)
----------

`[2406.05804] A Survey on LLM-Based Agentic Workflows and LLM-Profiled Components <https://arxiv.org/abs/2406.05804>`__ 基于llm的智能体工作流和llm组件综述

::

    Sun, 9 Jun 2024 14:42:55 GMT
    Xinzhe Li

Recent advancements in Large Language Models (LLMs) have catalyzed the development of sophisticated agentic workflows, offering improvements over traditional single-path, Chain-of-Thought (CoT) prompting techniques. This survey summarize the common workflows, with the particular focus on LLM-Profiled Components (LMPCs) and ignorance of non-LLM components. The reason behind such exploration is to facilitate a clearer understanding of LLM roles and see how reusabile of the LMPCs.

------------

`[2406.06391] Towards Lifelong Learning of Large Language Models: A Survey <https://arxiv.org/abs/2406.06391>`__ 大型语言模型终身学习研究综述

::

    Mon, 10 Jun 2024 15:46:25 GMT
    Junhao Zheng, Shengjie Qiu, Chengming Shi, Qianli Ma

As the applications of large language models (LLMs) expand across diverse fields, the ability of these models to adapt to ongoing changes in data, tasks, and user preferences becomes crucial. Traditional training methods, relying on static datasets, are increasingly inadequate for coping with the dynamic nature of real-world information. Lifelong learning, also known as continual or incremental learning, addresses this challenge by enabling LLMs to learn continuously and adaptively over their operational lifetime, integrating new knowledge while retaining previously learned information and preventing catastrophic forgetting. This survey delves into the sophisticated landscape of lifelong learning, categorizing strategies into two primary groups: Internal Knowledge and External Knowledge. Internal Knowledge includes continual pretraining and continual finetuning, each enhancing the adaptability of LLMs in various scenarios. External Knowledge encompasses retrieval-based and tool-based lifelong learning, leveraging external data sources and computational tools to extend the model's capabilities without modifying core parameters. The key contributions of our survey are: (1) Introducing a novel taxonomy categorizing the extensive literature of lifelong learning into 12 scenarios; (2) Identifying common techniques across all lifelong learning scenarios and classifying existing literature into various technique groups within each scenario; (3) Highlighting emerging techniques such as model expansion and data selection, which were less explored in the pre-LLM era.
Through a detailed examination of these groups and their respective categories, this survey aims to enhance the adaptability, reliability, and overall performance of LLMs in real-world applications.

------------

`[2405.19334] LLMs Meet Multimodal Generation and Editing: A Survey <https://arxiv.org/abs/2405.19334>`__ llm满足多模态生成和编辑:综述

::

    replaced with revised version Sun, 9 Jun 2024 11:34:12 GMT
    Submission history From: Jingye Chen [view email]
    [v1] Wed, 29 May 2024 17:59:20 UTC (31,737 KB)
    [v2] Sun, 9 Jun 2024 11:34:12 UTC (31,739 KB)
    Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, Jifeng Dai, Yong Zhang, Wei Xue, Qifeng Liu, Yike Guo, Qifeng Chen

With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on multimodal understanding. This survey elaborates on multimodal generation and editing across various domains, comprising image, video, 3D, and audio. Specifically, we summarize the notable advancements with milestone works in these fields and categorize these studies into LLM-based and CLIP/T5-based methods. Then, we summarize the various roles of LLMs in multimodal generation and exhaustively investigate the critical technical components behind these methods and the multimodal datasets utilized in these studies. Additionally, we dig into tool-augmented multimodal agents that can leverage existing generative models for human-computer interaction. Lastly, we discuss the advancements in the generative AI safety field, investigate emerging applications, and discuss future prospects. Our work provides a systematic and insightful overview of multimodal generation and processing, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at this https URL

------------

`[2402.17944] Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding -- A Survey <https://arxiv.org/abs/2402.17944>`__ 表格数据上的大型语言模型(llm):预测、生成和理解——综述

::

    replaced with revised version Mon, 10 Jun 2024 17:41:32 GMT
    Submission history From: Weijie Xu [view email]
    [v1] Tue, 27 Feb 2024 23:59:01 UTC (263 KB)
    [v2] Fri, 1 Mar 2024 00:14:42 UTC (552 KB)
    [v3] Mon, 10 Jun 2024 17:41:32 UTC (1,287 KB)
    Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, Christos Faloutsos

Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.

------------

`[2404.14294] A Survey on Efficient Inference for Large Language Models <https://arxiv.org/abs/2404.14294>`__ 大型语言模型高效推理综述

::

    replaced with revised version Sat, 8 Jun 2024 04:49:51 GMT
    Submission history From: Zixuan Zhou [view email]
    [v1] Mon, 22 Apr 2024 15:53:08 UTC (630 KB)
    [v2] Sat, 8 Jun 2024 04:49:51 UTC (633 KB)
    Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, Yu Wang

Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.

------------

--------------
Benchmark (21)
--------------

`[2406.05343] M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark <https://arxiv.org/abs/2406.05343>`__ M3GIA:基于认知启发的多语言多模态通用智能能力基准

::

    Sat, 8 Jun 2024 04:07:09 GMT
    Wei Song (1 and 2 and 3), Yadong Li (2), Jianhua Xu (2), Guowei Wu (3), Lingfeng Ming (2), Kexin Yi (2), Weihua Luo (2), Houyi Li (2), Yi Du (4), Fangda Guo (5), Kaicheng Yu (1) ((1) AutoLab, Westlake University, (2) AI Business, Alibaba Group, (3) Zhejiang University, (4) Key Laboratory of Behavioral Science, Institute of Psychology, CAS, (5) Key Laboratory of AI Safety, Institute of Computing Technology, CAS)

As recent multi-modality large language models (MLLMs) have shown formidable proficiency on various complex tasks, there has been increasing attention on debating whether these models could eventually mirror human intelligence.
However, existing benchmarks mainly focus on evaluating solely on task performance, such as the accuracy of identifying the attribute of an object.
Combining well-developed cognitive science to understand the intelligence of MLLMs beyond superficial achievements remains largely unexplored. To this end, we introduce the first cognitive-driven multi-lingual and multi-modal benchmark to evaluate the general intelligence ability of MLLMs, dubbed M3GIA.
Specifically, we identify five key cognitive factors based on the well-recognized Cattell-Horn-Carrol (CHC) model of intelligence and propose a novel evaluation metric. In addition, since most MLLMs are trained to perform in different languages, a natural question arises: is language a key factor influencing the cognitive ability of MLLMs? As such, we go beyond English to encompass other languages based on their popularity, including Chinese, French, Spanish, Portuguese and Korean, to construct our M3GIA. We make sure all the data relevant to the cultural backgrounds are collected from their native context to avoid English-centric bias. We collected a significant corpus of data from human participants, revealing that the most advanced MLLM reaches the lower boundary of human intelligence in English. Yet, there remains a pronounced disparity in the other five languages assessed. We also reveals an interesting winner takes all phenomenon that are aligned with the discovery in cognitive studies. Our benchmark will be open-sourced, with the aspiration of facilitating the enhancement of cognitive capabilities in MLLMs.

------------

`[2406.05506] Towards a Benchmark for Causal Business Process Reasoning with LLMs <https://arxiv.org/abs/2406.05506>`__ 基于llm的因果业务流程推理基准

::

    Sat, 8 Jun 2024 16:10:53 GMT
    Fabiana Fournier, Lior Limonad, Inna Skarbovsky

Large Language Models (LLMs) are increasingly used for boosting organizational efficiency and automating tasks. While not originally designed for complex cognitive processes, recent efforts have further extended to employ LLMs in activities such as reasoning, planning, and decision-making. In business processes, such abilities could be invaluable for leveraging on the massive corpora LLMs have been trained on for gaining deep understanding of such processes. In this work, we plant the seeds for the development of a benchmark to assess the ability of LLMs to reason about causal and process perspectives of business operations. We refer to this view as Causally-augmented Business Processes (BP^C). The core of the benchmark comprises a set of BP^C related situations, a set of questions about these situations, and a set of deductive rules employed to systematically resolve the ground truth answers to these questions. Also with the power of LLMs, the seed is then instantiated into a larger-scale set of domain-specific situations and questions. Reasoning on BP^C is of crucial importance for process interventions and process improvement. Our benchmark could be used in one of two possible modalities: testing the performance of any target LLM and training an LLM to advance its capability to reason about BP^C.

------------

`[2406.05194] LLMs Are Not Intelligent Thinkers: Introducing Mathematical Topic Tree Benchmark for Comprehensive Evaluation of LLMs <https://arxiv.org/abs/2406.05194>`__ llm不是智能思考者:引入数学主题树基准，用于全面评估llm

::

    Fri, 7 Jun 2024 18:21:26 GMT
    Arash Gholami Davoodi, Seyed Pouyan Mousavi Davoudi, Pouya Pezeshkpour

Large language models (LLMs) demonstrate impressive capabilities in mathematical reasoning. However, despite these achievements, current evaluations are mostly limited to specific mathematical topics, and it remains unclear whether LLMs are genuinely engaging in reasoning. To address these gaps, we present the Mathematical Topics Tree (MaTT) benchmark, a challenging and structured benchmark that offers 1,958 questions across a wide array of mathematical subjects, each paired with a detailed hierarchical chain of topics. Upon assessing different LLMs using the MaTT benchmark, we find that the most advanced model, GPT-4, achieved a mere 54\% accuracy in a multiple-choice scenario. Interestingly, even when employing Chain-of-Thought prompting, we observe mostly no notable improvement. Moreover, LLMs accuracy dramatically reduced by up to 24.2 percentage point when the questions were presented without providing choices. Further detailed analysis of the LLMs' performance across a range of topics showed significant discrepancy even for closely related subtopics within the same general mathematical area. In an effort to pinpoint the reasons behind LLMs performances, we conducted a manual evaluation of the completeness and correctness of the explanations generated by GPT-4 when choices were available. Surprisingly, we find that in only 53.3\% of the instances where the model provided a correct answer, the accompanying explanations were deemed complete and accurate, i.e., the model engaged in genuine reasoning.

------------

`[2406.05654] DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation <https://arxiv.org/abs/2406.05654>`__ 

::

    Sun, 9 Jun 2024 05:33:51 GMT
    Shuting Wang, Jiongnan Liu Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang, Yutao Zhu, Zhicheng Dou

Retrieval-Augmented Generation (RAG) offers a promising solution to address various limitations of Large Language Models (LLMs), such as hallucination and difficulties in keeping up with real-time updates. This approach is particularly critical in expert and domain-specific applications where LLMs struggle to cover expert knowledge. Therefore, evaluating RAG models in such scenarios is crucial, yet current studies often rely on general knowledge sources like Wikipedia to assess the models' abilities in solving common-sense problems. In this paper, we evaluated LLMs by RAG settings in a domain-specific context, college enrollment. We identified six required abilities for RAG models, including the ability in conversational RAG, analyzing structural information, faithfulness to external knowledge, denoising, solving time-sensitive problems, and understanding multi-document interactions. Each ability has an associated dataset with shared corpora to evaluate the RAG models' performance. We evaluated popular LLMs such as Llama, Baichuan, ChatGLM, and GPT models. Experimental results indicate that existing closed-book LLMs struggle with domain-specific questions, highlighting the need for RAG models to solve expert problems. Moreover, there is room for RAG models to improve their abilities in comprehending conversational history, analyzing structural information, denoising, processing multi-document interactions, and faithfulness in expert knowledge. We expect future studies could solve these problems better.

------------

`[2406.05862] II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models <https://arxiv.org/abs/2406.05862>`__ 

::

    Sun, 9 Jun 2024 17:25:47 GMT
    Ziqiang Liu, Feiteng Fang, Xi Feng, Xinrun Du, Chenhao Zhang, Zekun Wang, Yuelin Bai, Qixuan Zhao, Liyang Fan, Chengguang Gan, Hongquan Lin, Jiaming Li, Yuansheng Ni, Haihong Wu, Yaswanth Narsupalli, Zhigang Zheng, Chengming Li, Xiping Hu, Ruifeng Xu, Xiaojun Chen, Min Yang, Jiaheng Liu, Ruibo Liu, Wenhao Huang, Ge Zhang, Shiwen Ni

The rapid advancements in the development of multimodal large language models (MLLMs) have consistently led to new breakthroughs on various benchmarks. In response, numerous challenging and comprehensive benchmarks have been proposed to more accurately assess the capabilities of MLLMs. However, there is a dearth of exploration of the higher-order perceptual capabilities of MLLMs. To fill this gap, we propose the Image Implication understanding Benchmark, II-Bench, which aims to evaluate the model's higher-order perception of images. Through extensive experiments on II-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs attains 74.8%, whereas human accuracy averages 90%, peaking at an impressive 98%. Subsequently, MLLMs perform worse on abstract and complex images, suggesting limitations in their ability to understand high-level semantics and capture image details. Finally, it is observed that most models exhibit enhanced accuracy when image sentiment polarity hints are incorporated into the prompts. This observation underscores a notable deficiency in their inherent understanding of image sentiment. We believe that II-Bench will inspire the community to develop the next generation of MLLMs, advancing the journey towards expert artificial general intelligence (AGI). II-Bench is publicly available at https://huggingface.co/datasets/m-a-p/II-Bench.

------------

`[2406.06196] LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages <https://arxiv.org/abs/2406.06196>`__ LINGOLY:资源稀缺和灭绝语言的奥林匹克语言推理基准

::

    Mon, 10 Jun 2024 11:50:29 GMT
    Andrew M. Bean, Simi Hellsten, Harry Mayne, Jabez Magomere, Ethan A. Chi, Ryan Chi, Scott A. Hale, Hannah Rose Kirk

In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions.
The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 35.3% accuracy, 21.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.

------------

`[2406.06331] MedExQA: Medical Question Answering Benchmark with Multiple Explanations <https://arxiv.org/abs/2406.06331>`__ MedExQA:具有多种解释的医疗问答基准

::

    Mon, 10 Jun 2024 14:47:04 GMT
    Yunsoo Kim, Jinge Wu, Yusuf Abdulle, Honghan Wu

This paper introduces MedExQA, a novel benchmark in medical question-answering, to evaluate large language models' (LLMs) understanding of medical knowledge through explanations. By constructing datasets across five distinct medical specialties that are underrepresented in current datasets and further incorporating multiple explanations for each question-answer pair, we address a major gap in current medical QA benchmarks which is the absence of comprehensive assessments of LLMs' ability to generate nuanced medical explanations. Our work highlights the importance of explainability in medical LLMs, proposes an effective methodology for evaluating models beyond classification accuracy, and sheds light on one specific domain, speech language pathology, where current LLMs including GPT4 lack good understanding.
Our results show generation evaluation with multiple explanations aligns better with human assessment, highlighting an opportunity for a more robust automated comprehension assessment for LLMs. To diversify open-source medical LLMs (currently mostly based on Llama2), this work also proposes a new medical model, MedPhi-2, based on Phi-2 (2.7B). The model outperformed medical LLMs based on Llama2-70B in generating explanations, showing its effectiveness in the resource-constrained medical domain. We will share our benchmark datasets and the trained model.

------------

`[2406.06357] MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific Workflows <https://arxiv.org/abs/2406.06357>`__ MASSW:人工智能辅助科学工作流的新数据集和基准任务

::

    Mon, 10 Jun 2024 15:19:09 GMT
    Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, Qiaozhu Mei

Scientific innovation relies on detailed workflows, which include critical steps such as analyzing literature, generating ideas, validating these ideas, interpreting results, and inspiring follow-up research. However, scientific publications that document these workflows are extensive and unstructured. This makes it difficult for both human researchers and AI systems to effectively navigate and explore the space of scientific innovation. To address this issue, we introduce MASSW, a comprehensive text dataset on Multi-Aspect Summarization of Scientific Workflows. MASSW includes more than 152,000 peer-reviewed publications from 17 leading computer science conferences spanning the past 50 years. Using Large Language Models (LLMs), we automatically extract five core aspects from these publications -- context, key idea, method, outcome, and projected impact -- which correspond to five key steps in the research workflow. These structured summaries facilitate a variety of downstream tasks and analyses. The quality of the LLM-extracted summaries is validated by comparing them with human annotations. We demonstrate the utility of MASSW through multiple novel machine-learning tasks that can be benchmarked using this new dataset, which make various types of predictions and recommendations along the scientific workflow. MASSW holds significant potential for researchers to create and benchmark new AI methods for optimizing scientific workflows and fostering scientific innovation in the field. Our dataset is openly available at \url{https://github.com/xingjian-zhang/massw}.

------------

`[2406.05540] A Fine-tuning Dataset and Benchmark for Large Language Models for Protein Understanding <https://arxiv.org/abs/2406.05540>`__ 蛋白质理解大型语言模型的微调数据集和基准

::

    Sat, 8 Jun 2024 18:11:30 GMT
    Yiqing Shen, Zan Chen, Michail Mamalakis, Luhan He, Haiyang Xia, Tianbin Li, Yanzhou Su, Junjun He, Yu Guang Wang

The parallels between protein sequences and natural language in their sequential structures have inspired the application of large language models (LLMs) to protein understanding. Despite the success of LLMs in NLP, their effectiveness in comprehending protein sequences remains an open question, largely due to the absence of datasets linking protein sequences to descriptive text. Researchers have then attempted to adapt LLMs for protein understanding by integrating a protein sequence encoder with a pre-trained LLM. However, this adaptation raises a fundamental question: "Can LLMs, originally designed for NLP, effectively comprehend protein sequences as a form of language?" Current datasets fall short in addressing this question due to the lack of a direct correlation between protein sequences and corresponding text descriptions, limiting the ability to train and evaluate LLMs for protein understanding effectively. To bridge this gap, we introduce ProteinLMDataset, a dataset specifically designed for further self-supervised pretraining and supervised fine-tuning (SFT) of LLMs to enhance their capability for protein sequence comprehension. Specifically, ProteinLMDataset includes 17.46 billion tokens for pretraining and 893,000 instructions for SFT. Additionally, we present ProteinLMBench, the first benchmark dataset consisting of 944 manually verified multiple-choice questions for assessing the protein understanding capabilities of LLMs. ProteinLMBench incorporates protein-related details and sequences in multiple languages, establishing a new standard for evaluating LLMs' abilities in protein comprehension. The large language model InternLM2-7B, pretrained and fine-tuned on the ProteinLMDataset, outperforms GPT-4 on ProteinLMBench, achieving the highest accuracy score. The dataset and the benchmark are available at https://huggingface.co/datasets/tsynbio/ProteinLMBench.

------------

`[2406.05590] NYU CTF Dataset: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security <https://arxiv.org/abs/2406.05590>`__ NYU CTF Dataset:一个可扩展的开源基准数据集，用于评估llm的攻击性安全性

::

    Sat, 8 Jun 2024 22:21:42 GMT
    Minghao Shao, Sofija Jancheska, Meet Udeshi, Brendan Dolan-Gavitt, Haoran Xi, Kimberly Milner, Boyuan Chen, Max Yin, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique

Large Language Models (LLMs) are being deployed across various domains today.
However, their capacity to solve Capture the Flag (CTF) challenges in cybersecurity has not been thoroughly evaluated. To address this, we develop a novel method to assess LLMs in solving CTF challenges by creating a scalable, open-source benchmark database specifically designed for these applications.
This database includes metadata for LLM testing and adaptive learning, compiling a diverse range of CTF challenges from popular competitions.
Utilizing the advanced function calling capabilities of LLMs, we build a fully automated system with an enhanced workflow and support for external tool calls.
Our benchmark dataset and automated framework allow us to evaluate the performance of five LLMs, encompassing both black-box and open-source models.
This work lays the foundation for future research into improving the efficiency of LLMs in interactive cybersecurity tasks and automated task planning. By providing a specialized dataset, our project offers an ideal platform for developing, testing, and refining LLM-based approaches to vulnerability detection and resolution. Evaluating LLMs on these challenges and comparing with human performance yields insights into their potential for AI-driven cybersecurity solutions to perform real-world threat management. We make our dataset open source to public https://github.com/NYU-LLM-CTF/LLM_CTF_Database along with our playground automated framework https://github.com/NYU-LLM-CTF/llm_ctf_automation.

------------

`[2406.05967] CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark <https://arxiv.org/abs/2406.05967>`__ CVQA:文化多样化的多语言视觉问答基准

::

    Mon, 10 Jun 2024 01:59:00 GMT
    David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, Hern\'an Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando D'Haro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago Cabrera, Mario Rodr\'iguez-Cantelar, et al. (32 additional authors not shown)

Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric.
While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA, a new Culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 28 countries on four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.

------------

`[2406.06425] Multivariate Stochastic Dominance via Optimal Transport and Applications to Models Benchmarking <https://arxiv.org/abs/2406.06425>`__ 基于最优传输的多元随机支配及其在模型基准测试中的应用

::

    Mon, 10 Jun 2024 16:14:50 GMT
    Gabriel Rioux and Apoorva Nitsure and Mattia Rigotti and Kristjan Greenewald and Youssef Mroueh

Stochastic dominance is an important concept in probability theory, econometrics and social choice theory for robustly modeling agents' preferences between random outcomes. While many works have been dedicated to the univariate case, little has been done in the multivariate scenario, wherein an agent has to decide between different multivariate outcomes. By exploiting a characterization of multivariate first stochastic dominance in terms of couplings, we introduce a statistic that assesses multivariate almost stochastic dominance under the framework of Optimal Transport with a smooth cost. Further, we introduce an entropic regularization of this statistic, and establish a central limit theorem (CLT) and consistency of the bootstrap procedure for the empirical statistic. Armed with this CLT, we propose a hypothesis testing framework as well as an efficient implementation using the Sinkhorn algorithm. We showcase our method in comparing and benchmarking Large Language Models that are evaluated on multiple metrics. Our multivariate stochastic dominance test allows us to capture the dependencies between the metrics in order to make an informed and statistically significant decision on the relative performance of the models.

------------

`[2403.15879] TrustSQL: Benchmarking Text-to-SQL Reliability with Penalty-Based Scoring <https://arxiv.org/abs/2403.15879>`__ TrustSQL:基于评分的文本到sql可靠性基准测试

::

    replaced with revised version Sat, 8 Jun 2024 16:56:45 GMT
    Submission history From: Gyubok Lee [view email]
    [v1] Sat, 23 Mar 2024 16:12:52 UTC (82 KB)
    [v2] Tue, 16 Apr 2024 15:33:39 UTC (86 KB)
    [v3] Sat, 8 Jun 2024 16:56:45 UTC (90 KB)
    Gyubok Lee, Woosog Chay, Seonhee Cho, Edward Choi

Text-to-SQL enables users to interact with databases using natural language, simplifying the retrieval and synthesis of information. Despite the remarkable success of large language models (LLMs) in translating natural language questions into SQL queries, widespread deployment remains limited due to two primary challenges. First, the effective use of text-to-SQL models depends on users' understanding of the model's capabilities-the scope of questions the model can correctly answer. Second, the absence of abstention mechanisms can lead to incorrect SQL generation going unnoticed, thereby undermining trust in the model's output. To enable wider deployment, it is crucial to address these challenges in model design and enhance model evaluation to build trust in the model's output. To this end, we introduce TrustSQL, a novel comprehensive benchmark designed to evaluate text-to-SQL reliability-defined as a model's ability to correctly handle any type of input question by generating correct SQL queries for feasible questions and abstaining from generating infeasible ones (e.g., due to schema incompatibility or functionalities beyond SQL). We evaluate existing methods using a novel penalty-based scoring metric with two modeling approaches: (1) pipeline-based methods combining SQL generators with infeasible question detectors and SQL error detectors for abstention; and (2) unified methods using a single model for the entire task. Our experimental results reveal that achieving high scores under severe penalties requires significant effort and provide a new perspective on developing text-to-SQL models for safer deployment.

------------

`[2305.14463] ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment <https://arxiv.org/abs/2305.14463>`__ readme++:面向多领域可读性评估的多语言语言模型基准测试

::

    replaced with revised version Sat, 8 Jun 2024 15:54:54 GMT
    Submission history From: Tarek Naous [view email]
    [v1] Tue, 23 May 2023 18:37:30 UTC (2,566 KB)
    [v2] Wed, 15 Nov 2023 15:50:31 UTC (3,055 KB)
    [v3] Sat, 8 Jun 2024 15:54:54 UTC (2,737 KB)
    Tarek Naous, Michael J. Ryan, Anton Lavrouk, Mohit Chandra, Wei Xu

We present a comprehensive evaluation of large language models for multilingual readability assessment. Existing evaluation resources lack domain and language diversity, limiting the ability for cross-domain and cross-lingual analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset with human annotations of 9757 sentences in Arabic, English, French, Hindi, and Russian, collected from 112 different data sources. This benchmark will encourage research on developing robust multilingual readability assessment methods. Using ReadMe++, we benchmark multilingual and monolingual language models in the supervised, unsupervised, and few-shot prompting settings. The domain and language diversity in ReadMe++ enable us to test more effective few-shot prompting, and identify shortcomings in state-of-the-art unsupervised methods. Our experiments also reveal exciting results of superior domain generalization and enhanced cross-lingual transfer capabilities by models trained on ReadMe++. We will make our data publicly available and release a python package tool for multilingual sentence readability prediction using our trained models at: this https URL

------------

`[2309.03564] Supervised Learning and Large Language Model Benchmarks on Mental Health Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media <https://arxiv.org/abs/2309.03564>`__ 

::

    replaced with revised version Sun, 9 Jun 2024 12:49:52 GMT
    Submission history From: Guanghui Fu [view email]
    [v1] Thu, 7 Sep 2023 08:50:46 UTC (44 KB)
    [v2] Wed, 1 Nov 2023 10:15:34 UTC (304 KB)
    [v3] Sun, 9 Jun 2024 12:49:52 UTC (263 KB)
    Hongzhi Qi, Qing Zhao, Jianqiang Li, Changwei Song, Wei Zhai, Dan Luo, Shuo Liu, Yi Jing Yu, Fan Wang, Huijing Zou, Bing Xiang Yang, Guanghui Fu

On social media, users often express their personal feelings, which may exhibit cognitive distortions or even suicidal tendencies on certain specific topics. Early recognition of these signs is critical for effective psychological intervention. In this paper, we introduce two novel datasets from Chinese social media: SOS-HL-1K for suicidal risk classification and SocialCD-3K for cognitive distortions detection. The SOS-HL-1K dataset contained 1,249 posts and SocialCD-3K dataset was a multi-label classification dataset that containing 3,407 posts. We propose a comprehensive evaluation using two supervised learning methods and eight large language models (LLMs) on the proposed datasets. From the prompt engineering perspective, we experimented with two types of prompt strategies, including four zero-shot and five few-shot strategies. We also evaluated the performance of the LLMs after fine-tuning on the proposed tasks. The experimental results show that there is still a huge gap between LLMs relying only on prompt engineering and supervised learning. In the suicide classification task, this gap is 6.95% points in F1-score, while in the cognitive distortion task, the gap is even more pronounced, reaching 31.53% points in F1-score. However, after fine-tuning, this difference is significantly reduced. In the suicide and cognitive distortion classification tasks, the gap decreases to 4.31% and 3.14%, respectively. This research highlights the potential of LLMs in psychological contexts, but supervised learning remains necessary for more challenging tasks. All datasets and code are made available.

------------

`[2402.05547] Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset <https://arxiv.org/abs/2402.05547>`__ 大型语言模型在交流式医疗指导上的基准测试:一个新的系统和数据集

::

    replaced with revised version Sat, 8 Jun 2024 16:36:56 GMT
    Submission history From: Hengguan Huang [view email]
    [v1] Thu, 8 Feb 2024 10:32:06 UTC (614 KB)
    [v2] Sat, 8 Jun 2024 16:36:56 UTC (620 KB)
    Hengguan Huang, Songtao Wang, Hongfu Liu, Hao Wang and Ye Wang

Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce "ChatCoach", a human-AI cooperative framework designed to assist medical learners in practicing their communication skills during patient consultations. ChatCoach (Our data and code are available online: this https URL itself from conventional dialogue systems by offering a simulated environment where medical learners can practice dialogues with a patient agent, while a coach agent provides immediate, structured feedback. This is facilitated by our proposed Generalized Chain-of-Thought (GCoT) approach, which fosters the generation of structured feedback and enhances the utilization of external knowledge sources. Additionally, we have developed a dataset specifically for evaluating Large Language Models (LLMs) within the ChatCoach framework on communicative medical coaching tasks. Our empirical results validate the effectiveness of ChatCoach.

------------

`[2402.11100] When LLMs Meet Cunning Texts: A Fallacy Understanding Benchmark for Large Language Models <https://arxiv.org/abs/2402.11100>`__ 当llm遇到狡猾的文本:大型语言模型的谬论理解基准

::

    replaced with revised version Sun, 9 Jun 2024 17:55:05 GMT
    Submission history From: Yinghui Li [view email]
    [v1] Fri, 16 Feb 2024 22:12:53 UTC (342 KB)
    [v2] Sun, 9 Jun 2024 17:55:05 UTC (644 KB)
    Yinghui Li, Qingyu Zhou, Yuanzhen Luo, Shirong Ma, Yangning Li, Hai-Tao Zheng, Xuming Hu, Philip S. Yu

Recently, Large Language Models (LLMs) make remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning texts that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning texts that FLUB focuses on mainly consist of the tricky, humorous, and misleading texts collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights are achieved in our extensive experiments and detailed analyses. We hope that our benchmark can encourage the community to improve LLMs' ability to understand fallacies. Our data and codes are available at this https URL.

------------

`[2402.14973] Introducing GenCeption for Multimodal LLM Benchmarking: You May Bypass Annotations <https://arxiv.org/abs/2402.14973>`__ 多模态LLM基准测试的GenCeption介绍:您可以绕过注释

::

    replaced with revised version Sun, 9 Jun 2024 21:10:34 GMT
    Submission history From: Lele Cao [view email]
    [v1] Thu, 22 Feb 2024 21:22:04 UTC (12,335 KB)
    [v2] Sun, 9 Jun 2024 21:10:34 UTC (19,273 KB)
    Lele Cao, Valentin Buchner, Zineb Senane and Fangkai Yang

Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.

------------

`[2402.17644] Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data <https://arxiv.org/abs/2402.17644>`__ llm是否能够进行基于数据的统计和因果推理?用数据对高级定量推理进行基准测试

::

    replaced with revised version Sun, 9 Jun 2024 13:54:09 GMT
    Submission history From: Xiao Liu [view email]
    [v1] Tue, 27 Feb 2024 16:15:03 UTC (8,094 KB)
    [v2] Sun, 9 Jun 2024 13:54:09 UTC (8,096 KB)
    Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng

Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has much room for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously. Code and data are in this https URL.

------------

`[2310.07132] Risk Aware Benchmarking of Large Language Models <https://arxiv.org/abs/2310.07132>`__ 风险感知的大型语言模型基准测试

::

    replaced with revised version Sun, 9 Jun 2024 18:26:34 GMT
    Submission history From: Youssef Mroueh [view email]
    [v1] Wed, 11 Oct 2023 02:08:37 UTC (1,521 KB)
    [v2] Tue, 9 Jan 2024 14:38:20 UTC (2,804 KB)
    [v3] Sun, 9 Jun 2024 18:26:34 UTC (3,888 KB)
    Apoorva Nitsure, Youssef Mroueh, Mattia Rigotti, Kristjan Greenewald, Brian Belgodere, Mikhail Yurochkin, Jiri Navratil, Igor Melnyk, and Jerret Ross

We propose a distributional framework for benchmarking socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a metrics portfolio for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit theorems instantiated in practice via a bootstrap variance estimate. We use our framework to compare various large language models regarding risks related to drifting from instructions and outputting toxic content.

------------

`[2405.06634] Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA Benchmark <https://arxiv.org/abs/2405.06634>`__ 多模态llm与基本可视网络分析的斗争:VNA基准

::

    replaced with revised version Mon, 10 Jun 2024 15:28:16 GMT
    Submission history From: Evan Williams [view email]
    [v1] Fri, 10 May 2024 17:51:35 UTC (1,635 KB)
    [v2] Mon, 10 Jun 2024 15:28:16 UTC (1,635 KB)
    Evan M. Williams and Kathleen M. Carley

We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual Network Analysis (VNA) tasks on small-scale graphs. We evaluate the Vision Language Models (VLMs) on 5 tasks related to three foundational network science concepts: identifying nodes of maximal degree on a rendered graph, identifying whether signed triads are balanced or unbalanced, and counting components. The tasks are structured to be easy for a human who understands the underlying graph theoretic concepts, and can all be solved by counting the appropriate elements in graphs. We find that while GPT-4 consistently outperforms LLaVa, both models struggle with every visual network analysis task we propose. We publicly release the first benchmark for the evaluation of VLMs on foundational VNA tasks.

------------

---------------
Accelerate (13)
---------------

`[2406.05250] LLM-Enhanced Bayesian Optimization for Efficient Analog Layout Constraint Generation <https://arxiv.org/abs/2406.05250>`__ 高效模拟布局约束生成的llm增强贝叶斯优化

::

    Fri, 7 Jun 2024 20:22:36 GMT
    Guojin Chen, Keren Zhu, Seunggeun Kim, Hanqing Zhu, Yao Lai, Bei Yu, David Z. Pan

Analog layout synthesis faces significant challenges due to its dependence on manual processes, considerable time requirements, and performance instability.
Current Bayesian Optimization (BO)-based techniques for analog layout synthesis, despite their potential for automation, suffer from slow convergence and extensive data needs, limiting their practical application. This paper presents the \texttt{LLANA} framework, a novel approach that leverages Large Language Models (LLMs) to enhance BO by exploiting the few-shot learning abilities of LLMs for more efficient generation of analog design-dependent parameter constraints. Experimental results demonstrate that \texttt{LLANA} not only achieves performance comparable to state-of-the-art (SOTA) BO methods but also enables a more effective exploration of the analog circuit design space, thanks to LLM's superior contextual understanding and learning efficiency. The code is available at \url{https://github.com/dekura/LLANA}.

------------

`[2406.05673] Flow of Reasoning: Efficient Training of LLM Policy with Divergent Thinking <https://arxiv.org/abs/2406.05673>`__ 

::

    Sun, 9 Jun 2024 07:06:58 GMT
    Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, Lianhui Qin

Divergent thinking, the cognitive process of generating diverse solutions, is a hallmark of human creativity and problem-solving. For machines, sampling diverse solution trajectories in complex reasoning problems is crucial for robust outcomes, data augmentation, and enhanced model generalization. Large language models (LLMs) often struggle with generating high-quality, diverse reasoning. While supervised fine-tuning helps with quality, it requires extensive supervision data to capture the full diversity of solutions.
Alternatively, reinforcement learning methods like PPO aim to find limited highest-reward solutions while neglecting the solution diversity, akin to convergent thinking. To address these limitations, we propose Flow of Reasoning (FoR) -- an efficient LLM training approach enabling diverse reasoning with minimal data. FoR formulates multi-step LLM reasoning as a Markovian flow from an initial state to terminal states. The formulation allows to adapt principled GFlowNet approaches to train the LLM as a policy, which is able to sample multiple reasoning paths with probabilities proportional to the unnormalized reward. Empirical results show that, with limited training data (e.g., 15 examples), FoR can discover diverse high-quality solutions that excel greatly beyond current state-of-the-art methods across three tasks, including embodied reasoning (BlocksWorld), math puzzle solving (Game24), and logical reasoning (PrOntoQA). Code is available at https://github.com/Yu-Fangxu/FoR.

------------

`[2406.05369] Venn Diagram Prompting : Accelerating Comprehension with Scaffolding Effect <https://arxiv.org/abs/2406.05369>`__ 维恩图提示:用脚手架效应加速理解

::

    Sat, 8 Jun 2024 06:27:26 GMT
    Sakshi Mahendru, Tejul Pandit

We introduce Venn Diagram (VD) Prompting, an innovative prompting technique which allows Large Language Models (LLMs) to combine and synthesize information across complex, diverse and long-context documents in knowledge-intensive question-answering tasks. Generating answers from multiple documents involves numerous steps to extract relevant and unique information and amalgamate it into a cohesive response. To improve the quality of the final answer, multiple LLM calls or pretrained models are used to perform different tasks such as summarization, reorganization and customization. The approach covered in the paper focuses on replacing the multi-step strategy via a single LLM call using VD prompting. Our proposed technique also aims to eliminate the inherent position bias in the LLMs, enhancing consistency in answers by removing sensitivity to the sequence of input information. It overcomes the challenge of inconsistency traditionally associated with varying input sequences. We also explore the practical applications of the VD prompt based on our examination of the prompt's outcomes. In the experiments performed on four public benchmark question-answering datasets, VD prompting continually matches or surpasses the performance of a meticulously crafted instruction prompt which adheres to optimal guidelines and practices.

------------

`[2406.06110] Recurrent Context Compression: Efficiently Expanding the Context Window of LLM <https://arxiv.org/abs/2406.06110>`__ 循环上下文压缩:有效扩展LLM的上下文窗口

::

    Mon, 10 Jun 2024 08:50:59 GMT
    Chensen Huang, Guibo Zhu, Xuepeng Wang, Yifei Luo, Guojing Ge, Haoran Chen, Dong Yi, Jinqiao Wang

To extend the context length of Transformer-based large language models (LLMs) and improve comprehension capabilities, we often face limitations due to computational resources and bounded memory storage capacity. This work introduces a method called Recurrent Context Compression (RCC), designed to efficiently expand the context window length of LLMs within constrained storage space. We also investigate the issue of poor model responses when both instructions and context are compressed in downstream tasks, and propose an instruction reconstruction method to mitigate this problem. We validated the effectiveness of our approach on multiple tasks, achieving a compression rate of up to 32x on text reconstruction tasks with a BLEU4 score close to 0.95, and nearly 100\% accuracy on a passkey retrieval task with a sequence length of 1M.
Finally, our method demonstrated competitive performance in long-text question-answering tasks compared to non-compressed methods, while significantly saving storage resources in long-text inference tasks. Our code, models, and demo are available at https://github.com/WUHU-G/RCC_Transformer

------------

`[2406.05881] LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning <https://arxiv.org/abs/2406.05881>`__ 

::

    Sun, 9 Jun 2024 18:40:24 GMT
    Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri

Developing interactive systems that leverage natural language instructions to solve complex robotic control tasks has been a long-desired goal in the robotics community. Large Language Models (LLMs) have demonstrated exceptional abilities in handling complex tasks, including logical reasoning, in-context learning, and code generation. However, predicting low-level robotic actions using LLMs poses significant challenges. Additionally, the complexity of such tasks usually demands the acquisition of policies to execute diverse subtasks and combine them to attain the ultimate objective. Hierarchical Reinforcement Learning (HRL) is an elegant approach for solving such tasks, which provides the intuitive benefits of temporal abstraction and improved exploration.
However, HRL faces the recurring issue of non-stationarity due to unstable lower primitive behaviour. In this work, we propose LGR2, a novel HRL framework that leverages language instructions to generate a stationary reward function for the higher-level policy. Since the language-guided reward is unaffected by the lower primitive behaviour, LGR2 mitigates non-stationarity and is thus an elegant method for leveraging language instructions to solve robotic control tasks. To analyze the efficacy of our approach, we perform empirical analysis and demonstrate that LGR2 effectively alleviates non-stationarity in HRL. Our approach attains success rates exceeding 70$\%$ in challenging, sparse-reward robotic navigation and manipulation environments where the baselines fail to achieve any significant progress. Additionally, we conduct real-world robotic manipulation experiments and demonstrate that CRISP shows impressive generalization in real-world scenarios.

------------

`[2406.05981] ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization <https://arxiv.org/abs/2406.05981>`__ ShiftAddLLM:基于训练后无乘法重参数化的预训练llm加速

::

    Mon, 10 Jun 2024 02:47:55 GMT
    Haoran You, Yipin Guo, Yichao Fu, Wei Zhou, Huihong Shi, Xiaofan Zhang, Souvik Kundu, Amir Yazdanbakhsh, Yingyan Lin

Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. To address this, we propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM.
Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, we present a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3 and 2 bits, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at https://github.com/GATECH-EIC/ShiftAddLLM.

------------

`[2406.06246] Data-Efficient Learning with Neural Programs <https://arxiv.org/abs/2406.06246>`__ 基于神经程序的数据高效学习

::

    Mon, 10 Jun 2024 13:23:00 GMT
    Alaia Solko-Breslin, Seewon Choi, Ziyang Li, Neelay Velingker, Rajeev Alur, Mayur Naik, Eric Wong

Many computational tasks can be naturally expressed as a composition of a DNN followed by a program written in a traditional programming language or an API call to an LLM. We call such composites "neural programs" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite. When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components. We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components. For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as GPT-4 and also consider benchmarks from the neurosymolic learning literature. Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks. For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner.

------------

`[2310.07177] Online Speculative Decoding <https://arxiv.org/abs/2310.07177>`__ 在线推测解码

::

    replaced with revised version Mon, 10 Jun 2024 01:36:31 GMT
    Submission history From: Xiaoxuan Liu [view email]
    [v1] Wed, 11 Oct 2023 04:03:42 UTC (908 KB)
    [v2] Tue, 17 Oct 2023 18:02:19 UTC (908 KB)
    [v3] Fri, 7 Jun 2024 00:14:47 UTC (2,243 KB)
    [v4] Mon, 10 Jun 2024 01:36:31 UTC (2,243 KB)
    Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Alvin Cheung, Zhijie Deng, Ion Stoica, Hao Zhang

Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding to address this challenge. The main idea is to continuously update the (multiple) draft model(s) on observed user query data. Adapting to query distribution mitigates the shifts between the training distribution of the draft model and the query distribution, enabling the draft model to more accurately predict the target model's outputs. We develop a prototype of online speculative decoding based on knowledge distillation and evaluate it using both synthetic and real query data. The results show a substantial increase in the token acceptance rate by 0.1 to 0.65, bringing 1.42x to 2.17x latency reduction. Our code is available at this https URL.

------------

`[2401.10471] DeepEdit: Knowledge Editing as Decoding with Constraints <https://arxiv.org/abs/2401.10471>`__ DeepEdit:基于约束解码的知识编辑

::

    replaced with revised version Sat, 8 Jun 2024 03:47:03 GMT
    Submission history From: Yiwei Wang [view email]
    [v1] Fri, 19 Jan 2024 03:48:27 UTC (1,341 KB)
    [v2] Mon, 1 Apr 2024 16:12:50 UTC (1,732 KB)
    [v3] Sat, 8 Jun 2024 03:47:03 UTC (1,729 KB)
    Yiwei Wang, Muhao Chen, Nanyun Peng, Kai-Wei Chang

Answering multi-hop questions involving new knowledge is a challenging task in evaluating large language models' (LLMs) knowledge editing (KE) methods. This task is rather difficult because the LLMs' hallucinations on new knowledge would harm the logical coherence of LLMs' multi-hop reasoning and lead to incorrect answers. To address this issue, we design decoding constraints to "regulate" LLMs' reasoning, enhancing logical coherence when incorporating new knowledge. We incorporate the constraints into a new KE framework: DEEPEDIT (Depth-first Search-based Constrained Decoding for Knowledge Editing), which enhances LLMs to generate coherent reasoning chains with new knowledge through a depth-first search. Our search selects the most important knowledge that satisfies our constraints as the reasoning step to efficiently increase the reasoning depth. In addition to DEEPEDIT, we propose two new KE benchmarks: MQUAKE-2002 and MQUAKE-HARD, which provide more precise and challenging assessments of KE approaches. Qualitatively, DEEPEDIT enables LLMs to produce succinct and coherent reasoning chains involving new knowledge. Quantitatively, it yields significant improvements on multiple KE benchmarks.

------------

`[2402.17433] Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder <https://arxiv.org/abs/2402.17433>`__ 通过预训练对比脑电信号-文本掩码自编码器的可迁移表示增强脑电信号-文本解码

::

    replaced with revised version Mon, 10 Jun 2024 09:51:50 GMT
    Submission history From: Jiaqi Wang [view email]
    [v1] Tue, 27 Feb 2024 11:45:21 UTC (1,496 KB)
    [v2] Wed, 28 Feb 2024 03:34:00 UTC (1,496 KB)
    [v3] Mon, 10 Jun 2024 09:51:50 UTC (1,531 KB)
    Jiaqi Wang, Zhenxi Song, Zhengyu Ma, Xipeng Qiu, Min Zhang, Zhiguo Zhang

Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modules alongside the EEG stream from CET-MAE and further enables an LLM (specifically BART) to decode text from EEG sequences. Comprehensive experiments conducted on the popular text-evoked EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms the state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%, respectively. These results indicate significant advancements in the field and underscores the proposed framework's potential to enable more powerful and widespread BCI applications.

------------

`[2404.14294] A Survey on Efficient Inference for Large Language Models <https://arxiv.org/abs/2404.14294>`__ 大型语言模型高效推理综述

::

    replaced with revised version Sat, 8 Jun 2024 04:49:51 GMT
    Submission history From: Zixuan Zhou [view email]
    [v1] Mon, 22 Apr 2024 15:53:08 UTC (630 KB)
    [v2] Sat, 8 Jun 2024 04:49:51 UTC (633 KB)
    Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, Yu Wang

Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.

------------

`[2405.05894] Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons <https://arxiv.org/abs/2405.05894>`__ 高效的LLM比较评估:成对比较的专家框架产品

::

    replaced with revised version Sun, 9 Jun 2024 17:56:11 GMT
    Submission history From: Adian Liusie [view email]
    [v1] Thu, 9 May 2024 16:45:27 UTC (1,269 KB)
    [v2] Sun, 9 Jun 2024 17:56:11 UTC (763 KB)
    Adian Liusie, Vatsal Raina, Yassir Fathullah, Mark Gales

LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks, aligning with human judgements especially when applied in a comparative assessment fashion. However, when using pairwise comparisons to rank a set of candidates the computational costs scale quadratically with the number of candidates, which can have practical limitations. This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment. Here individual comparisons are considered experts that provide information on a pair's score difference. The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed. When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking. Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate as well to human judgements as the predictions when all comparisons are used. We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment. When N is large, with as few as 2% of comparisons the PoE solution can achieve similar performance to when all comparisons are used.

------------

`[2405.17202] Efficient multi-prompt evaluation of LLMs <https://arxiv.org/abs/2405.17202>`__ 高效的LLMs多提示评估

::

    replaced with revised version Fri, 7 Jun 2024 18:24:13 GMT
    Submission history From: Felipe Maia Polo [view email]
    [v1] Mon, 27 May 2024 14:24:47 UTC (525 KB)
    [v2] Fri, 7 Jun 2024 18:24:13 UTC (551 KB)
    Felipe Maia Polo, Ronald Xu, Lucas Weber, M\'irian Silva, Onkar Bhardwaj, Leshem Choshen, Allysson Flavio Melo de Oliveira, Yuekai Sun, Mikhail Yurochkin

Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry. For example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Our code and data can be found at this https URL.

------------

-----------------------
In-Context Learning (1)
-----------------------

`[2310.10707] Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning <https://arxiv.org/abs/2310.10707>`__ 演示就是你所需要的:使用上下文学习推进攻击性内容的解释

::

    replaced with revised version Sun, 9 Jun 2024 18:22:33 GMT
    Submission history From: Anirudh Som [view email]
    [v1] Mon, 16 Oct 2023 16:18:55 UTC (1,046 KB)
    [v2] Sun, 9 Jun 2024 18:22:33 UTC (1,680 KB)
    Anirudh Som, Karan Sikka, Helen Gent, Ajay Divakaran, Andreas Kathol, Dimitra Vergyri

Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also often retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as - number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase (CAPP) dataset, comprising of dialogue-style rude utterances, polite paraphrases, and additional dialogue context. We evaluate our approach using four closed source and one open source LLM. Our results reveal that ICL is comparable to supervised methods in generation quality, while being qualitatively better by 25% on human evaluation and attaining lower toxicity by 76%. Also, ICL-based paraphrasers only show a slight reduction in performance even with just 10% training data.

------------

--------------
Reasoning (16)
--------------

`[2406.05506] Towards a Benchmark for Causal Business Process Reasoning with LLMs <https://arxiv.org/abs/2406.05506>`__ 基于llm的因果业务流程推理基准

::

    Sat, 8 Jun 2024 16:10:53 GMT
    Fabiana Fournier, Lior Limonad, Inna Skarbovsky

Large Language Models (LLMs) are increasingly used for boosting organizational efficiency and automating tasks. While not originally designed for complex cognitive processes, recent efforts have further extended to employ LLMs in activities such as reasoning, planning, and decision-making. In business processes, such abilities could be invaluable for leveraging on the massive corpora LLMs have been trained on for gaining deep understanding of such processes. In this work, we plant the seeds for the development of a benchmark to assess the ability of LLMs to reason about causal and process perspectives of business operations. We refer to this view as Causally-augmented Business Processes (BP^C). The core of the benchmark comprises a set of BP^C related situations, a set of questions about these situations, and a set of deductive rules employed to systematically resolve the ground truth answers to these questions. Also with the power of LLMs, the seed is then instantiated into a larger-scale set of domain-specific situations and questions. Reasoning on BP^C is of crucial importance for process interventions and process improvement. Our benchmark could be used in one of two possible modalities: testing the performance of any target LLM and training an LLM to advance its capability to reason about BP^C.

------------

`[2406.05673] Flow of Reasoning: Efficient Training of LLM Policy with Divergent Thinking <https://arxiv.org/abs/2406.05673>`__ 

::

    Sun, 9 Jun 2024 07:06:58 GMT
    Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, Lianhui Qin

Divergent thinking, the cognitive process of generating diverse solutions, is a hallmark of human creativity and problem-solving. For machines, sampling diverse solution trajectories in complex reasoning problems is crucial for robust outcomes, data augmentation, and enhanced model generalization. Large language models (LLMs) often struggle with generating high-quality, diverse reasoning. While supervised fine-tuning helps with quality, it requires extensive supervision data to capture the full diversity of solutions.
Alternatively, reinforcement learning methods like PPO aim to find limited highest-reward solutions while neglecting the solution diversity, akin to convergent thinking. To address these limitations, we propose Flow of Reasoning (FoR) -- an efficient LLM training approach enabling diverse reasoning with minimal data. FoR formulates multi-step LLM reasoning as a Markovian flow from an initial state to terminal states. The formulation allows to adapt principled GFlowNet approaches to train the LLM as a policy, which is able to sample multiple reasoning paths with probabilities proportional to the unnormalized reward. Empirical results show that, with limited training data (e.g., 15 examples), FoR can discover diverse high-quality solutions that excel greatly beyond current state-of-the-art methods across three tasks, including embodied reasoning (BlocksWorld), math puzzle solving (Game24), and logical reasoning (PrOntoQA). Code is available at https://github.com/Yu-Fangxu/FoR.

------------

`[2406.05659] Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses <https://arxiv.org/abs/2406.05659>`__ llm表现出类似人类的推理吗?评估llm中开放式反应的心智理论

::

    Sun, 9 Jun 2024 05:57:59 GMT
    Maryam Amirizaniani, Elias Martin, Maryna Sivachenko, Afra Mashhadi, Chirag Shah

Theory of Mind (ToM) reasoning entails recognizing that other individuals possess their own intentions, emotions, and thoughts, which is vital for guiding one's own thought processes. Although large language models (LLMs) excel in tasks such as summarization, question answering, and translation, they still face challenges with ToM reasoning, especially in open-ended questions.
Despite advancements, the extent to which LLMs truly understand ToM reasoning and how closely it aligns with human ToM reasoning remains inadequately explored in open-ended scenarios. Motivated by this gap, we assess the abilities of LLMs to perceive and integrate human intentions and emotions into their ToM reasoning processes within open-ended questions. Our study utilizes posts from Reddit's ChangeMyView platform, which demands nuanced social reasoning to craft persuasive responses. Our analysis, comparing semantic similarity and lexical overlap metrics between responses generated by humans and LLMs, reveals clear disparities in ToM reasoning capabilities in open-ended questions, with even the most advanced models showing notable limitations. To enhance LLM capabilities, we implement a prompt tuning method that incorporates human intentions and emotions, resulting in improvements in ToM reasoning performance. However, despite these improvements, the enhancement still falls short of fully achieving human-like reasoning. This research highlights the deficiencies in LLMs' social reasoning and demonstrates how integrating human intentions and emotions can boost their effectiveness.

------------

`[2406.06196] LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages <https://arxiv.org/abs/2406.06196>`__ LINGOLY:资源稀缺和灭绝语言的奥林匹克语言推理基准

::

    Mon, 10 Jun 2024 11:50:29 GMT
    Andrew M. Bean, Simi Hellsten, Harry Mayne, Jabez Magomere, Ethan A. Chi, Ryan Chi, Scott A. Hale, Hannah Rose Kirk

In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions.
The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 35.3% accuracy, 21.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.

------------

`[2406.06461] Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies <https://arxiv.org/abs/2406.06461>`__ 代币经济中的推理:LLM推理策略的预算感知评估

::

    Mon, 10 Jun 2024 16:55:08 GMT
    Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, Ben Athiwaratkun

A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don't surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.

------------

`[2406.05963] Solution for SMART-101 Challenge of CVPR Multi-modal Algorithmic Reasoning Task 2024 <https://arxiv.org/abs/2406.05963>`__ CVPR多模态算法推理任务2024的SMART-101挑战解决方案

::

    Mon, 10 Jun 2024 01:45:55 GMT
    Jinwoo Ahn, Junhyeok Park, Min-Jun Kim, Kang-Hyeon Kim, So-Yeong Sohn, Yun-Ji Lee, Du-Seong Chang, Yu-Jung Heo, Eun-Sol Kim

In this paper, the solution of HYU MLLAB KT Team to the Multimodal Algorithmic Reasoning Task: SMART-101 CVPR 2024 Challenge is presented. Beyond conventional visual question-answering problems, the SMART-101 challenge aims to achieve human-level multimodal understanding by tackling complex visio-linguistic puzzles designed for children in the 6-8 age group. To solve this problem, we suggest two main ideas. First, to utilize the reasoning ability of a large-scale language model (LLM), the given visual cues (images) are grounded in the text modality. For this purpose, we generate highly detailed text captions that describe the context of the image and use these captions as input for the LLM. Second, due to the nature of puzzle images, which often contain various geometric visual patterns, we utilize an object detection algorithm to ensure these patterns are not overlooked in the captioning process. We employed the SAM algorithm, which can detect various-size objects, to capture the visual features of these geometric patterns and used this information as input for the LLM. Under the puzzle split configuration, we achieved an option selection accuracy Oacc of 29.5 on the test set and a weighted option selection accuracy (WOSA) of 27.1 on the challenge set.

------------

`[2402.11804] LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs <https://arxiv.org/abs/2402.11804>`__ LLM作为提示器:任意知识图谱上的低资源归纳推理

::

    replaced with revised version Sat, 8 Jun 2024 12:16:22 GMT
    Submission history From: Kai Wang [view email]
    [v1] Mon, 19 Feb 2024 03:21:19 UTC (2,276 KB)
    [v2] Sat, 8 Jun 2024 12:16:22 UTC (3,061 KB)
    Kai Wang, Yuwei Xu, Zhiyong Wu, Siqiang Luo

Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-resource KG datasets and find that ProLINK outperforms previous methods in three-shot, one-shot, and zero-shot reasoning tasks, exhibiting average performance improvements by 20%, 45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong robustness for various LLM promptings as well as full-shot scenarios.

------------

`[2309.16938] "I'd Like to Have an Argument, Please": Argumentative Reasoning in Large Language Models <https://arxiv.org/abs/2309.16938>`__ “我想要一个论点”:大型语言模型中的论证推理

::

    replaced with revised version Mon, 10 Jun 2024 12:39:29 GMT
    Submission history From: Adrian de Wynter [view email]
    [v1] Fri, 29 Sep 2023 02:41:38 UTC (1,640 KB)
    [v2] Mon, 10 Jun 2024 12:39:29 UTC (1,783 KB)
    Adrian de Wynter and Tangming Yuan

We evaluate two large language models (LLMs) ability to perform argumentative reasoning. We experiment with argument mining (AM) and argument pair extraction (APE), and evaluate the LLMs' ability to recognize arguments under progressively more abstract input and output (I/O) representations (e.g., arbitrary label sets, graphs, etc.). Unlike the well-known evaluation of prompt phrasings, abstraction evaluation retains the prompt's phrasing but tests reasoning capabilities. We find that scoring-wise the LLMs match or surpass the SOTA in AM and APE, and under certain I/O abstractions LLMs perform well, even beating chain-of-thought--we call this symbolic prompting. However, statistical analysis on the LLMs outputs when subject to small, yet still human-readable, alterations in the I/O representations (e.g., asking for BIO tags as opposed to line numbers) showed that the models are not performing reasoning. This suggests that LLM applications to some tasks, such as data labelling and paper reviewing, must be done with care.

------------

`[2311.07532] It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning <https://arxiv.org/abs/2311.07532>`__ 犯错并不容易:大型语言模型在消除推理过程中举步维艰

::

    replaced with revised version Fri, 7 Jun 2024 23:01:20 GMT
    Submission history From: Nishant Balepur [view email]
    [v1] Mon, 13 Nov 2023 18:18:22 UTC (7,802 KB)
    [v2] Mon, 19 Feb 2024 16:46:06 UTC (8,579 KB)
    [v3] Fri, 7 Jun 2024 23:01:20 UTC (8,904 KB)
    Nishant Balepur, Shramay Palta, Rachel Rudinger

Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.

------------

`[2401.06961] CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities <https://arxiv.org/abs/2401.06961>`__ CHAMP:用于llm数学推理能力细粒度分析的竞赛级数据集

::

    replaced with revised version Sun, 9 Jun 2024 01:47:26 GMT
    Submission history From: Yilun Zhou [view email]
    [v1] Sat, 13 Jan 2024 03:18:16 UTC (1,153 KB)
    [v2] Sun, 9 Jun 2024 01:47:26 UTC (786 KB)
    Yujun Mao, Yoon Kim, Yilun Zhou

Recent large language models (LLMs) have shown indications of mathematical reasoning ability on challenging competition-level problems, especially with self-generated verbalizations of intermediate reasoning steps (i.e., chain-of-thought prompting). However, current evaluations mainly focus on the end-to-end final answer correctness, and it is unclear whether LLMs can make use of helpful side information such as problem-specific hints. In this paper, we propose a challenging benchmark dataset for enabling such analyses. The Concept and Hint-Annotated Math Problems (CHAMP) consists of high school math competition problems, annotated with concepts, or general math facts, and hints, or problem-specific tricks. These annotations allow us to explore the effects of additional information, such as relevant hints, misleading concepts, or related problems. This benchmark is difficult, with the best model only scoring 58.1% in standard settings. With concepts and hints, performance sometimes improves, indicating that some models can make use of such side information. Furthermore, we annotate model-generated solutions for their correctness. Using this corpus, we find that models often arrive at the correct final answer through wrong reasoning steps. In addition, we test whether models are able to verify these solutions, and find that most models struggle.

------------

`[2402.01620] MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models <https://arxiv.org/abs/2402.01620>`__ MAGDi:多智能体交互图的结构化蒸馏改进了较小语言模型的推理

::

    replaced with revised version Fri, 7 Jun 2024 18:13:16 GMT
    Submission history From: Swarnadeep Saha [view email]
    [v1] Fri, 2 Feb 2024 18:35:14 UTC (1,316 KB)
    [v2] Fri, 7 Jun 2024 18:13:16 UTC (1,320 KB)
    Justin Chih-Yao Chen, Swarnadeep Saha, Elias Stengel-Eskin, Mohit Bansal

Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods that distill from a single teacher and multiple teachers. Moreover, MAGDi also demonstrates an order of magnitude higher efficiency over its teachers. We conduct extensive analyses to show that MAGDi (1) enhances the generalizability to out-of-domain tasks, (2) scales positively with the size and strength of the base student model, and (3) obtains larger improvements (via our multi-teacher training) when applying self-consistency -- an inference technique that relies on model diversity.

------------

`[2402.12348] GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations <https://arxiv.org/abs/2402.12348>`__ GTBench:通过博弈论评估揭示llm的战略推理局限性

::

    replaced with revised version Mon, 10 Jun 2024 17:14:09 GMT
    Submission history From: Jinhao Duan [view email]
    [v1] Mon, 19 Feb 2024 18:23:36 UTC (6,630 KB)
    [v2] Mon, 10 Jun 2024 17:14:09 UTC (6,722 KB)
    Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, Kaidi Xu

As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and (2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Most open-source LLMs, e.g., CodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than commercial LLMs, e.g., GPT-4, in complex games, yet the recently released Llama-3-70b-Instruct makes up for this shortcoming. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. We further characterize the game-theoretic properties of LLMs, such as equilibrium and Pareto Efficiency in repeated games. Detailed error profiles are provided for a better understanding of LLMs' behavior. We hope our research provides standardized protocols and serves as a foundation to spur further explorations in the strategic reasoning of LLMs.

------------

`[2402.17644] Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data <https://arxiv.org/abs/2402.17644>`__ llm是否能够进行基于数据的统计和因果推理?用数据对高级定量推理进行基准测试

::

    replaced with revised version Sun, 9 Jun 2024 13:54:09 GMT
    Submission history From: Xiao Liu [view email]
    [v1] Tue, 27 Feb 2024 16:15:03 UTC (8,094 KB)
    [v2] Sun, 9 Jun 2024 13:54:09 UTC (8,096 KB)
    Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng

Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has much room for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously. Code and data are in this https URL.

------------

`[2402.18374] VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models <https://arxiv.org/abs/2402.18374>`__ VerifiNER:基于大型语言模型基于知识推理的验证增强NER

::

    replaced with revised version Sat, 8 Jun 2024 13:01:36 GMT
    Submission history From: Kwangwook Seo [view email]
    [v1] Wed, 28 Feb 2024 14:49:05 UTC (1,511 KB)
    [v2] Sat, 8 Jun 2024 13:01:36 UTC (1,529 KB)
    Seoyeon Kim, Kwangwook Seo, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee

Recent approaches in domain-specific named entity recognition (NER), such as biomedical NER, have shown remarkable advances. However, they still lack of faithfulness, producing erroneous predictions. We assume that knowledge of entities can be useful in verifying the correctness of the predictions. Despite the usefulness of knowledge, resolving such errors with knowledge is nontrivial, since the knowledge itself does not directly indicate the ground-truth label. To this end, we propose VerifiNER, a post-hoc verification framework that identifies errors from existing NER methods using knowledge and revises them into more faithful predictions. Our framework leverages the reasoning abilities of large language models to adequately ground on knowledge and the contextual information in the verification process. We validate effectiveness of VerifiNER through extensive experiments on biomedical datasets. The results suggest that VerifiNER can successfully verify errors from existing models as a model-agnostic approach. Further analyses on out-of-domain and low-resource settings show the usefulness of VerifiNER on real-world applications.

------------

`[2403.06609] Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds <https://arxiv.org/abs/2403.06609>`__ 基于知识种子的大型语言模型指导临床推理

::

    replaced with revised version Sat, 8 Jun 2024 04:14:46 GMT
    Submission history From: Jiageng Wu [view email]
    [v1] Mon, 11 Mar 2024 10:53:20 UTC (364 KB)
    [v2] Sat, 8 Jun 2024 04:14:46 UTC (225 KB)
    Jiageng WU, Xian Wu, Jie Yang

Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients' diseases, and deciding on appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians. In this study, we introduce a novel framework, In-Context Padding (ICP), designed to enhance LLMs with medical knowledge. Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs. Experiments on two clinical question datasets demonstrate that ICP significantly improves the clinical reasoning ability of LLMs.

------------

`[2406.02746] RATT: A Thought Structure for Coherent and Correct LLM Reasoning <https://arxiv.org/abs/2406.02746>`__ RATT:一个连贯和正确的LLM推理的思想结构

::

    replaced with revised version Sun, 9 Jun 2024 05:37:26 GMT
    Submission history From: Jinghan Zhang [view email]
    [v1] Tue, 4 Jun 2024 20:02:52 UTC (3,077 KB)
    [v2] Sun, 9 Jun 2024 05:37:26 UTC (3,077 KB)
    Jinghan Zhang, Xiting Wang, Weijieying Ren, Lu Jiang, Dongjie Wang, Kunpeng Liu

Large Language Models (LLMs) gain substantial reasoning and decision-making capabilities from thought structures. However, existing methods such as Tree of Thought and Retrieval Augmented Thoughts often fall short in complex tasks due to the limitations of insufficient local retrieval of factual knowledge and inadequate global selection of strategies. These limitations make it challenging for these methods to balance factual accuracy and comprehensive logical optimization effectively. To address these limitations, we introduce the Retrieval Augmented Thought Tree (RATT), a novel thought structure that considers both overall logical soundness and factual correctness at each step of the thinking process. Specifically, at every point of a thought branch, RATT performs planning and lookahead to explore and evaluate multiple potential reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented Generation (RAG) with LLM's ability to assess overall strategy. Through this combination of factual knowledge and strategic feasibility, the RATT adjusts and integrates the thought tree structure to search for the most promising branches within the search space. This thought structure significantly enhances the model's coherence in logical inference and efficiency in decision-making, and thus increases the limit of the capacity of LLM to generate reliable inferences and decisions based on thought structures. A broad range of experiments on different types of tasks showcases that the RATT structure significantly outperforms existing methods in factual correctness and logical coherence.

------------

-----------
ToolUse (1)
-----------

`[2406.06451] Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course <https://arxiv.org/abs/2406.06451>`__ 社会塑造理论的启示:大型语言模型在本科程序设计课程中的挪用

::

    Mon, 10 Jun 2024 16:40:14 GMT
    Aadarsh Padiyath, Xinying Hou, Amy Pang, Diego Viramontes Vargas, Xingjian Gu, Tamara Nelson-Fromm, Zihan Wu, Mark Guzdial, Barbara Ericson

The capability of large language models (LLMs) to generate, debug, and explain code has sparked the interest of researchers and educators in undergraduate programming, with many anticipating their transformative potential in programming education. However, decisions about why and how to use LLMs in programming education may involve more than just the assessment of an LLM's technical capabilities. Using the social shaping of technology theory as a guiding framework, our study explores how students' social perceptions influence their own LLM usage. We then examine the correlation of self-reported LLM usage with students' self-efficacy and midterm performances in an undergraduate programming course. Triangulating data from an anonymous end-of-course student survey (n = 158), a mid-course self-efficacy survey (n=158), student interviews (n = 10), self-reported LLM usage on homework, and midterm performances, we discovered that students' use of LLMs was associated with their expectations for their future careers and their perceptions of peer usage. Additionally, early self-reported LLM usage in our context correlated with lower self-efficacy and lower midterm scores, while students' perceived over-reliance on LLMs, rather than their usage itself, correlated with decreased self-efficacy later in the course.

------------

------------------------
Retrieval-Augmented (10)
------------------------

`[2406.05654] DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation <https://arxiv.org/abs/2406.05654>`__ 

::

    Sun, 9 Jun 2024 05:33:51 GMT
    Shuting Wang, Jiongnan Liu Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang, Yutao Zhu, Zhicheng Dou

Retrieval-Augmented Generation (RAG) offers a promising solution to address various limitations of Large Language Models (LLMs), such as hallucination and difficulties in keeping up with real-time updates. This approach is particularly critical in expert and domain-specific applications where LLMs struggle to cover expert knowledge. Therefore, evaluating RAG models in such scenarios is crucial, yet current studies often rely on general knowledge sources like Wikipedia to assess the models' abilities in solving common-sense problems. In this paper, we evaluated LLMs by RAG settings in a domain-specific context, college enrollment. We identified six required abilities for RAG models, including the ability in conversational RAG, analyzing structural information, faithfulness to external knowledge, denoising, solving time-sensitive problems, and understanding multi-document interactions. Each ability has an associated dataset with shared corpora to evaluate the RAG models' performance. We evaluated popular LLMs such as Llama, Baichuan, ChatGLM, and GPT models. Experimental results indicate that existing closed-book LLMs struggle with domain-specific questions, highlighting the need for RAG models to solve expert problems. Moreover, there is room for RAG models to improve their abilities in comprehending conversational history, analyzing structural information, denoising, processing multi-document interactions, and faithfulness in expert knowledge. We expect future studies could solve these problems better.

------------

`[2406.05733] MrRank: Improving Question Answering Retrieval System through Multi-Result Ranking Model <https://arxiv.org/abs/2406.05733>`__ MrRank:利用多结果排序模型改进问答检索系统

::

    Sun, 9 Jun 2024 11:00:01 GMT
    Danupat Khamnuansin, Tawunrat Chalothorn, Ekapol Chuangsuwanich

Large Language Models (LLMs) often struggle with hallucinations and outdated information. To address this, Information Retrieval (IR) systems can be employed to augment LLMs with up-to-date knowledge. However, existing IR techniques contain deficiencies, posing a performance bottleneck. Given the extensive array of IR systems, combining diverse approaches presents a viable strategy. Nevertheless, prior attempts have yielded restricted efficacy. In this work, we propose an approach that leverages learning-to-rank techniques to combine heterogeneous IR systems. We demonstrate the method on two Retrieval Question Answering (ReQA) tasks. Our empirical findings exhibit a significant performance enhancement, outperforming previous approaches and achieving state-of-the-art results on ReQA SQuAD.

------------

`[2406.05794] RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation <https://arxiv.org/abs/2406.05794>`__ RE-RAG:用检索增强生成中的相关估计器提高开放域QA性能和可解释性

::

    Sun, 9 Jun 2024 14:11:19 GMT
    Kiseung Kim, Jay-Yoon Lee

Retrieval-augmented generation (RAG) frame work is showing state-of-the-art performance on open-domain question answering tasks by referencing external knowledge. However, the RAG system faces challenges with performance degradation when it is fed contexts of low relevance or when the relative relevance among the input contexts is inaccurately assessed. In this work, we propose a RE-RAG framework that injects an explicit context relevance estimator (RE) into the RAG system. RE-RAG re-evaluates the retrieved contexts with the proposed context RE and passes the more relevant contexts along with their measure importance to the generator. To train context RE, we propose an unsupervised learning method, which does not utilize any labeled document ranking data to train the context RE. To examine the efficacy of RE-RAG, we examine its performance on Natural Questions and TriviaQA datasets. RE-RAG achieves on-par performance compared to the FiD variants while utilizing fewer contexts (0.25x). We show that the proposed context RE, which was trained with the T5 model, is also applicable to RAG with LLMs(ChatGPT) by improving the performance on NQ (+6.4EM) and TQA (+2.8EM), respecitvely. Lastly, we display that RE can add interpretability to RAG framework as RE score highly correlates with the RE-RAG accuracy. Consequently, RE can be utilized to filter out unanswerable scenarios where context does not contain answers with 38.9%-51.3% accuracy just by examining a set of retrieved contexts.

------------

`[2406.06124] Enhancing Long-Term Memory using Hierarchical Aggregate Tree for Retrieval Augmented Generation <https://arxiv.org/abs/2406.06124>`__ 基于层次聚集树的检索增强生成增强长期记忆

::

    Mon, 10 Jun 2024 09:29:08 GMT
    Aadharsh Aadhithya A, Sachin Kumar S, Soman K.P

Large language models have limited context capacity, hindering reasoning over long conversations. We propose the Hierarchical Aggregate Tree memory structure to recursively aggregate relevant dialogue context through conditional tree traversals. HAT encapsulates information from children nodes, enabling broad coverage with depth control. We formulate finding best context as optimal tree traversal. Experiments show HAT improves dialog coherence and summary quality over baseline contexts, demonstrating the techniques effectiveness for multi turn reasoning without exponential parameter growth. This memory augmentation enables more consistent, grounded longform conversations from LLMs

------------

`[2406.06399] Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue <https://arxiv.org/abs/2406.06399>`__ 

::

    Mon, 10 Jun 2024 15:52:49 GMT
    Simone Alghisi, Massimo Rizzoli, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi

We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue. Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain).
However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics. In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.
We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type. We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent evaluation and explainability criteria for automatic metrics and human evaluation protocols. Our analysis shows that there is no universal best-technique for adapting large language models as the efficacy of each technique depends on both the base LLM and the specific type of dialogue. Last but not least, the assessment of the best adaptation technique should include human evaluation to avoid false expectations and outcomes derived from automatic metrics.

------------

`[2406.06458] Evaluating the Retrieval Component in LLM-Based Question Answering Systems <https://arxiv.org/abs/2406.06458>`__ 评估基于llm的问答系统中的检索部分

::

    Mon, 10 Jun 2024 16:46:22 GMT
    Ashkan Alinejad, Krtin Kumar, Ali Vahdat

Question answering systems (QA) utilizing Large Language Models (LLMs) heavily depend on the retrieval component to provide them with domain-specific information and reduce the risk of generating inaccurate responses or hallucinations. Although the evaluation of retrievers dates back to the early research in Information Retrieval, assessing their performance within LLM-based chatbots remains a challenge.
This study proposes a straightforward baseline for evaluating retrievers in Retrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate that this evaluation framework provides a better image of how the retriever performs and is more aligned with the overall performance of the QA system.
Although conventional metrics such as precision, recall, and F1 score may not fully capture LLMs' capabilities - as they can yield accurate responses despite imperfect retrievers - our method considers LLMs' strengths to ignore irrelevant contexts, as well as potential errors and hallucinations in their responses.

------------

`[2406.05814] Unified Text-to-Image Generation and Retrieval <https://arxiv.org/abs/2406.05814>`__ 统一的文本到图像生成和检索

::

    Sun, 9 Jun 2024 15:00:28 GMT
    Leigang Qu, Haochuan Li, Tan Wang, Wenjie Wang, Yongqi Li, Liqiang Nie, Tat-Seng Chua

How humans can efficiently and effectively acquire images has always been a perennial question. A typical solution is text-to-image retrieval from an existing database given the text query; however, the limited database typically lacks creativity. By contrast, recent breakthroughs in text-to-image generation have made it possible to produce fancy and diverse visual content, but it faces challenges in synthesizing knowledge-intensive images. In this work, we rethink the relationship between text-to-image generation and retrieval and propose a unified framework in the context of Multimodal Large Language Models (MLLMs).
Specifically, we first explore the intrinsic discriminative abilities of MLLMs and introduce a generative retrieval method to perform retrieval in a training-free manner. Subsequently, we unify generation and retrieval in an autoregressive generation way and propose an autonomous decision module to choose the best-matched one between generated and retrieved images as the response to the text query. Additionally, we construct a benchmark called TIGeR-Bench, including creative and knowledge-intensive domains, to standardize the evaluation of unified text-to-image generation and retrieval. Extensive experimental results on TIGeR-Bench and two retrieval benchmarks, i.e., Flickr30K and MS-COCO, demonstrate the superiority and effectiveness of our proposed method.

------------

`[2406.05870] Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents <https://arxiv.org/abs/2406.05870>`__ 机器对抗RAG:用拦截器文档干扰检索增强生成

::

    Sun, 9 Jun 2024 17:55:55 GMT
    Avital Shafran, Roei Schuster, Vitaly Shmatikov

Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevant documents from a knowledge database, then generating an answer by applying an LLM to the retrieved documents.
We demonstrate that RAG systems that operate on databases with potentially untrusted content are vulnerable to a new class of denial-of-service attacks we call jamming. An adversary can add a single ``blocker'' document to the database that will be retrieved in response to a specific query and, furthermore, result in the RAG system not answering the query - ostensibly because it lacks the information or because the answer is unsafe.
We describe and analyze several methods for generating blocker documents, including a new method based on black-box optimization that does not require the adversary to know the embedding or LLM used by the target RAG system, nor access to an auxiliary LLM to generate blocker documents. We measure the efficacy of the considered methods against several LLMs and embeddings, and demonstrate that the existing safety metrics for LLMs do not capture their vulnerability to jamming. We then discuss defenses against blocker documents.

------------

`[2403.15729] Towards a RAG-based Summarization Agent for the Electron-Ion Collider <https://arxiv.org/abs/2403.15729>`__ 基于碎片的电子离子对撞机摘要Agent研究

::

    replaced with revised version Sat, 8 Jun 2024 01:15:05 GMT
    Submission history From: Karthik Suresh [view email]
    [v1] Sat, 23 Mar 2024 05:32:46 UTC (4,267 KB)
    [v2] Tue, 26 Mar 2024 02:42:08 UTC (4,267 KB)
    [v3] Sat, 8 Jun 2024 01:15:05 UTC (4,267 KB)
    Karthik Suresh, Neeltje Kackar, Luke Schleck, Cristiano Fanelli

The complexity and sheer volume of information encompassing documents, papers, data, and other resources from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to assess the effectiveness of responses. Furthermore, we describe the concept of prompt template-based instruction-tuning which provides flexibility and accuracy in summarization. Importantly, the implementation relies on LangChain, which serves as the foundation of our entire workflow. This integration ensures efficiency and scalability, facilitating smooth deployment and accessibility for various user groups within the Electron Ion Collider (EIC) community. This innovative AI-driven framework not only simplifies the understanding of vast datasets but also encourages collaborative participation, thereby empowering researchers. As a demonstration, a web application has been developed to explain each stage of the RAG Agent development in detail.

------------

`[2405.19670] One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2405.19670>`__ 一个代币就能帮上忙!为检索增强的大型语言模型学习可扩展和可插拔的虚拟token

::

    replaced with revised version Sat, 8 Jun 2024 07:14:13 GMT
    Submission history From: Yutao Zhu [view email]
    [v1] Thu, 30 May 2024 03:44:54 UTC (264 KB)
    [v2] Fri, 31 May 2024 02:56:56 UTC (264 KB)
    [v3] Sat, 8 Jun 2024 07:14:13 UTC (313 KB)
    Yutao Zhu, Zhaoheng Huang, Zhicheng Dou, Ji-Rong Wen

Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across nine question-answering tasks demonstrate the superiority of our approach.

------------

---------
Agent (9)
---------

`[2406.05804] A Survey on LLM-Based Agentic Workflows and LLM-Profiled Components <https://arxiv.org/abs/2406.05804>`__ 基于llm的智能体工作流和llm组件综述

::

    Sun, 9 Jun 2024 14:42:55 GMT
    Xinzhe Li

Recent advancements in Large Language Models (LLMs) have catalyzed the development of sophisticated agentic workflows, offering improvements over traditional single-path, Chain-of-Thought (CoT) prompting techniques. This survey summarize the common workflows, with the particular focus on LLM-Profiled Components (LMPCs) and ignorance of non-LLM components. The reason behind such exploration is to facilitate a clearer understanding of LLM roles and see how reusabile of the LMPCs.

------------

`[2406.06464] Transforming Wearable Data into Health Insights using Large Language Model Agents <https://arxiv.org/abs/2406.06464>`__ 利用大型语言模型代理将可穿戴数据转化为健康见解

::

    Mon, 10 Jun 2024 17:00:54 GMT
    Mike A. Merrill, Akshay Paruchuri, Naghmeh Rezaei, Geza Kovacs, Javier Perez, Yun Liu, Erik Schenck, Nova Hammerquist, Jake Sunshine, Shyam Tailor, Kumar Ayush, Hao-Wei Su, Qian He, Cory McLean, Mark Malhotra, Shwetak Patel, Jiening Zhan, Tim Althoff, Daniel McDuff, Xin Liu

Despite the proliferation of wearable health trackers and the importance of sleep and exercise to health, deriving actionable personalized insights from wearable data remains a challenge because doing so requires non-trivial open-ended analysis of these data. The recent rise of large language model (LLM) agents, which can use tools to reason about and interact with the world, presents a promising opportunity to enable such personalized analysis at scale.
Yet, the application of LLM agents in analyzing personal health is still largely untapped. In this paper, we introduce the Personal Health Insights Agent (PHIA), an agent system that leverages state-of-the-art code generation and information retrieval tools to analyze and interpret behavioral health data from wearables. We curate two benchmark question-answering datasets of over 4000 health insights questions. Based on 650 hours of human and expert evaluation we find that PHIA can accurately address over 84% of factual numerical questions and more than 83% of crowd-sourced open-ended questions.
This work has implications for advancing behavioral health across the population, potentially enabling individuals to interpret their own wearable data, and paving the way for a new era of accessible, personalized wellness regimens that are informed by data-driven insights.

------------

`[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue <https://arxiv.org/abs/2406.05925>`__ 你好了!llm驱动的个性化代理，用于长期对话

::

    Sun, 9 Jun 2024 21:58:32 GMT
    Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, Tat-Seng Chua

Open-domain dialogue systems have seen remarkable advancements with the development of large language models (LLMs). Nonetheless, most existing dialogue systems predominantly focus on brief single-session interactions, neglecting the real-world demands for long-term companionship and personalized interactions with chatbots. Crucial to addressing this real-world need are event summary and persona management, which enable reasoning for appropriate long-term dialogue responses. Recent progress in the human-like cognitive and reasoning capabilities of LLMs suggests that LLM-based agents could significantly enhance automated perception, decision-making, and problem-solving. In response to this potential, we introduce a model-agnostic framework, the Long-term Dialogue Agent (LD-Agent), which incorporates three independently tunable modules dedicated to event perception, persona extraction, and response generation. For the event memory module, long and short-term memory banks are employed to separately focus on historical and ongoing sessions, while a topic-based retrieval mechanism is introduced to enhance the accuracy of memory retrieval. Furthermore, the persona module conducts dynamic persona modeling for both users and agents. The integration of retrieved memories and extracted personas is subsequently fed into the generator to induce appropriate responses. The effectiveness, generality, and cross-domain capabilities of LD-Agent are empirically demonstrated across various illustrative benchmarks, models, and tasks. The code is released at https://github.com/leolee99/LD-Agent.

------------

`[2406.05872] STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models <https://arxiv.org/abs/2406.05872>`__ STARLING:基于大型语言模型的文本强化学习智能体的自监督训练

::

    Sun, 9 Jun 2024 18:07:47 GMT
    Shreyas Basavatia, Keerthiram Murugesan, Shivam Ratnakar

Interactive fiction games have emerged as an important application to improve the generalization capabilities of language-based reinforcement learning (RL) agents. Existing environments for interactive fiction games are domain-specific or time-consuming to generate and do not train the RL agents to master a specific set of skills. In this work, we introduce an interactive environment for self-supervised RL, STARLING, for text-based games that bootstraps the text-based RL agents with automatically generated games (based on the seed set of game ideas) to boost the performance and generalization capabilities to reach a goal of the target environment. These games let the agent hone their skills on a predefined set of tasks. We create and test an environment with 100 games, generated using this automated framework that uses large language models (GPT-3) and an interactive fiction game engine (based on Inform7) to provide the user with the ability to generate more games under minimal human supervision. Experimental results based on both the human participants and baseline text-based RL agents reveal that current state-of-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can. These results enforce STARLING's potential to serve as a sandbox environment for further research in self-supervised text-based RL.

------------

`[2402.11359] Offline Training of Language Model Agents with Functions as Learnable Weights <https://arxiv.org/abs/2402.11359>`__ 以函数作为可学习权重的语言模型智能体的离线训练

::

    replaced with revised version Fri, 7 Jun 2024 23:49:18 GMT
    Submission history From: Shaokun Zhang [view email]
    [v1] Sat, 17 Feb 2024 18:31:21 UTC (816 KB)
    [v2] Fri, 3 May 2024 06:26:33 UTC (1,009 KB)
    [v3] Fri, 7 Jun 2024 23:49:18 UTC (822 KB)
    Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, Qingyun Wu

Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with two strategies, roll-back, and early-stop, to streamline the training process. With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative LLM agents in various downstream tasks. We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability.

------------

`[2403.20097] ITCMA: A Generative Agent Based on a Computational Consciousness Structure <https://arxiv.org/abs/2403.20097>`__ ITCMA:一种基于计算意识结构的生成Agent

::

    replaced with revised version Sat, 8 Jun 2024 13:04:40 GMT
    Submission history From: Hanzhong Zhang [view email]
    [v1] Fri, 29 Mar 2024 10:23:18 UTC (2,981 KB)
    [v2] Sat, 8 Jun 2024 13:04:40 UTC (3,850 KB)
    Hanzhong Zhang, Jibin Yin, Haoyang Wang, Ziwei Xiang

Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior. This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure to simulate the process of human consciousness. We further propose the ITCM-based Agent (ITCMA), which supports action generation and reasoning in open-world settings, and can independently complete tasks. ITCMA enhances LLMs' ability to understand implicit instructions and apply common-sense knowledge by considering agents' interaction and reasoning with the environment. Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even untrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher than SOTA, indicating its superiority over traditional intelligent agents in utility and generalization. In real-world tasks with quadruped robots, the untrained ITCMA achieves an 85% task completion rate, which is close to its performance in the unseen set, demonstrating its comparable utility and universality in real-world settings.

------------

`[2402.01620] MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models <https://arxiv.org/abs/2402.01620>`__ MAGDi:多智能体交互图的结构化蒸馏改进了较小语言模型的推理

::

    replaced with revised version Fri, 7 Jun 2024 18:13:16 GMT
    Submission history From: Swarnadeep Saha [view email]
    [v1] Fri, 2 Feb 2024 18:35:14 UTC (1,316 KB)
    [v2] Fri, 7 Jun 2024 18:13:16 UTC (1,320 KB)
    Justin Chih-Yao Chen, Swarnadeep Saha, Elias Stengel-Eskin, Mohit Bansal

Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods that distill from a single teacher and multiple teachers. Moreover, MAGDi also demonstrates an order of magnitude higher efficiency over its teachers. We conduct extensive analyses to show that MAGDi (1) enhances the generalizability to out-of-domain tasks, (2) scales positively with the size and strength of the base student model, and (3) obtains larger improvements (via our multi-teacher training) when applying self-consistency -- an inference technique that relies on model diversity.

------------

`[2403.15729] Towards a RAG-based Summarization Agent for the Electron-Ion Collider <https://arxiv.org/abs/2403.15729>`__ 基于碎片的电子离子对撞机摘要Agent研究

::

    replaced with revised version Sat, 8 Jun 2024 01:15:05 GMT
    Submission history From: Karthik Suresh [view email]
    [v1] Sat, 23 Mar 2024 05:32:46 UTC (4,267 KB)
    [v2] Tue, 26 Mar 2024 02:42:08 UTC (4,267 KB)
    [v3] Sat, 8 Jun 2024 01:15:05 UTC (4,267 KB)
    Karthik Suresh, Neeltje Kackar, Luke Schleck, Cristiano Fanelli

The complexity and sheer volume of information encompassing documents, papers, data, and other resources from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to assess the effectiveness of responses. Furthermore, we describe the concept of prompt template-based instruction-tuning which provides flexibility and accuracy in summarization. Importantly, the implementation relies on LangChain, which serves as the foundation of our entire workflow. This integration ensures efficiency and scalability, facilitating smooth deployment and accessibility for various user groups within the Electron Ion Collider (EIC) community. This innovative AI-driven framework not only simplifies the understanding of vast datasets but also encourages collaborative participation, thereby empowering researchers. As a demonstration, a web application has been developed to explain each stage of the RAG Agent development in detail.

------------

`[2404.15269] Aligning LLM Agents by Learning Latent Preference from User Edits <https://arxiv.org/abs/2404.15269>`__ 

::

    replaced with revised version Sun, 9 Jun 2024 21:45:09 GMT
    Submission history From: Ge Gao [view email]
    [v1] Tue, 23 Apr 2024 17:57:47 UTC (2,536 KB)
    [v2] Sun, 9 Jun 2024 21:45:09 UTC (2,551 KB)
    Ge Gao, Alexey Taymanov, Eduardo Salinas, Paul Mineiro, Dipendra Misra

We study interactive learning of LLM-based language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data. The inferred user preference descriptions are used to define prompts for generating responses in the future. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex, subtle, and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages the LLM to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, and use a GPT-4 simulated user for evaluation. On both tasks, CIPHER outperforms several baselines by achieving the lowest edit distance cost while only having a small overhead in LLM query cost. Our analysis reports that user preferences learned by CIPHER show significant similarity to the ground truth latent preferences.

------------

-----------
Other (146)
-----------

`[2406.05410] MLLM-SR: Conversational Symbolic Regression base Multi-Modal Large Language Models <https://arxiv.org/abs/2406.05410>`__ MLLM-SR:基于对话符号回归的多模态大型语言模型

::

    Sat, 8 Jun 2024 09:17:54 GMT
    Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jingyi Liu, Wenqiang Li, Shu Wei, Yusong Deng

Formulas are the language of communication between humans and nature. It is an important research topic of artificial intelligence to find expressions from observed data to reflect the relationship between each variable in the data, which is called a symbolic regression problem. The existing symbolic regression methods directly generate expressions according to the given observation data, and we cannot require the algorithm to generate expressions that meet specific requirements according to the known prior knowledge. For example, the expression needs to contain $\sin$ or be symmetric, and so on. Even if it can, it often requires very complex operations, which is very inconvenient. In this paper, based on multi-modal large language models, we propose MLLM-SR, a conversational symbolic regression method that can generate expressions that meet the requirements simply by describing the requirements with natural language instructions. By experimenting on the Nguyen dataset, we can demonstrate that MLLM-SR leads the state-of-the-art baselines in fitting performance. More notably, we experimentally demonstrate that MLLM-SR can well understand the prior knowledge we add to the natural language instructions.
Moreover, the addition of prior knowledge can effectively guide MLLM-SR to generate correct expressions.

------------

`[2406.05534] Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing <https://arxiv.org/abs/2406.05534>`__ 在线DPO:快慢追逐的在线直接偏好优化

::

    Sat, 8 Jun 2024 17:30:54 GMT
    Biqing Qi, Pengfei Li, Fangyuan Li, Junqi Gao, Kaiyan Zhang, Bowen Zhou

Direct Preference Optimization (DPO) improves the alignment of large language models (LLMs) with human values by training directly on human preference datasets, eliminating the need for reward models. However, due to the presence of cross-domain human preferences, direct continual training can lead to catastrophic forgetting, limiting DPO's performance and efficiency. Inspired by intraspecific competition driving species evolution, we propose a Online Fast-Slow chasing DPO (OFS-DPO) for preference alignment, simulating competition through fast and slow chasing among models to facilitate rapid adaptation. Specifically, we first derive the regret upper bound for online learning, validating our motivation with a min-max optimization pattern. Based on this, we introduce two identical modules using Low-rank Adaptive (LoRA) with different optimization speeds to simulate intraspecific competition, and propose a new regularization term to guide their learning. To further mitigate catastrophic forgetting in cross-domain scenarios, we extend the OFS-DPO with LoRA modules combination strategy, resulting in the Cross domain Online Fast-Slow chasing DPO (COFS-DPO). This method leverages linear combinations of fast modules parameters from different task domains, fully utilizing historical information to achive continual value alignment. Experimental results show that OFS-DPO outperforms DPO in in-domain alignment, while COFS-DPO excels in cross-domain continual learning scenarios.

------------

`[2406.05954] Aligning Large Language Models with Representation Editing: A Control Perspective <https://arxiv.org/abs/2406.05954>`__ 基于表示编辑的大型语言模型对齐:一个控制视角

::

    Mon, 10 Jun 2024 01:21:31 GMT
    Lingkai Kong, Haorui Wang, Wenhao Mu, Yuanqi Du, Yuchen Zhuang, Yifei Zhou, Yue Song, Rongzhi Zhang, Kai Wang, Chao Zhang

Aligning large language models (LLMs) with human objectives is crucial for real-world applications. However, fine-tuning LLMs for alignment often suffers from unstable training and requires substantial computing resources. Test-time alignment techniques, such as prompting and guided decoding, do not modify the underlying model, and their performance remains dependent on the original model's capabilities. To address these challenges, we propose aligning LLMs through representation editing. The core of our method is to view a pre-trained autoregressive LLM as a discrete-time stochastic dynamical system. To achieve alignment for specific objectives, we introduce external control signals into the state space of this language dynamical system. We train a value function directly on the hidden states according to the Bellman equation, enabling gradient-based optimization to obtain the optimal control signals at test time.
Our experiments demonstrate that our method outperforms existing test-time alignment techniques while requiring significantly fewer resources compared to fine-tuning methods.

------------

`[2406.05972] Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context <https://arxiv.org/abs/2406.05972>`__ 不确定背景下llm决策行为评估框架

::

    Mon, 10 Jun 2024 02:14:19 GMT
    Jingru Jia, Zehua Yuan, Junhao Pan, Paul McNamara, Deming Chen

When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: risk preference, probability weighting, and loss aversion. Given the widespread use of large language models (LLMs) in decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases. Several empirical studies have investigated the rationality and social behavior performance of LLMs, yet their internal decision-making tendencies and capabilities remain inadequately understood.
This paper proposes a framework, grounded in behavioral economics, to evaluate the decision-making behaviors of LLMs. Through a multiple-choice-list experiment, we estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities. However, there are significant variations in the degree to which these behaviors are expressed across different LLMs. We also explore their behavior when embedded with socio-demographic features, uncovering significant disparities. For instance, when modeled with attributes of sexual minority groups or physical disabilities, Claude-3-Opus displays increased risk aversion, leading to more conservative choices. These findings underscore the need for careful consideration of the ethical implications and potential biases in deploying LLMs in decision-making scenarios. Therefore, this study advocates for developing standards and guidelines to ensure that LLMs operate within ethical boundaries while enhancing their utility in complex decision-making environments.

------------

`[2406.06455] A Large Language Model Pipeline for Breast Cancer Oncology <https://arxiv.org/abs/2406.06455>`__ 乳腺癌肿瘤的大型语言模型管道

::

    Mon, 10 Jun 2024 16:44:48 GMT
    Tristen Pool and Dennis Trujillo

Large language models (LLMs) have demonstrated potential in the innovation of many disciplines. However, how they can best be developed for oncology remains underdeveloped. State-of-the-art OpenAI models were fine-tuned on a clinical dataset and clinical guidelines text corpus for two important cancer treatment factors, adjuvant radiation therapy and chemotherapy, using a novel Langchain prompt engineering pipeline. A high accuracy (0.85+) was achieved in the classification of adjuvant radiation therapy and chemotherapy for breast cancer patients. Furthermore, a confidence interval was formed from observational data on the quality of treatment from human oncologists to estimate the proportion of scenarios in which the model must outperform the original oncologist in its treatment prediction to be a better solution overall as 8.2% to 13.3%. Due to indeterminacy in the outcomes of cancer treatment decisions, future investigation, potentially a clinical trial, would be required to determine if this threshold was met by the models. Nevertheless, with 85% of U.S. cancer patients receiving treatment at local community facilities, these kinds of models could play an important part in expanding access to quality care with outcomes that lie, at minimum, close to a human oncologist.

------------

`[2406.06474] Towards a Personal Health Large Language Model <https://arxiv.org/abs/2406.06474>`__ 面向个人健康的大型语言模型

::

    Mon, 10 Jun 2024 17:16:49 GMT
    Justin Cosentino, Anastasiya Belyaeva, Xin Liu, Nicholas A. Furlotte, Zhun Yang, Chace Lee, Erik Schenck, Yojan Patel, Jian Cui, Logan Douglas Schneider, Robby Bryant, Ryan G. Gomes, Allen Jiang, Roy Lee, Yun Liu, Javier Perez, Jameson K. Rogers, Cathy Speed, Shyam Tailor, Megan Walker, Jeffrey Yu, Tim Althoff, Conor Heneghan, John Hernandez, Mark Malhotra, Leor Stern, Yossi Matias, Greg S. Corrado, Shwetak Patel, Shravya Shetty, Jiening Zhan, Shruthi Prabhakara, Daniel McDuff, Cory Y. McLean

In health, most large language model (LLM) research has focused on clinical tasks. However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring.
Here we present Personal Health Large Language Model (PH-LLM), fine-tuned from Gemini for understanding and reasoning over numerical time-series personal health data. We created and curated three datasets that test 1) production of personalized insights and recommendations from sleep patterns, physical activity, and physiological responses, 2) expert domain knowledge, and 3) prediction of self-reported sleep outcomes. For the first task we designed 857 case studies in collaboration with domain experts to assess real-world scenarios in sleep and fitness. Through comprehensive evaluation of domain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not statistically different from expert performance in fitness and, while experts remain superior for sleep, fine-tuning PH-LLM provided significant improvements in using relevant domain knowledge and personalizing information for sleep insights. We evaluated PH-LLM domain knowledge using multiple choice sleep medicine and fitness examinations. PH-LLM achieved 79% on sleep and 88% on fitness, exceeding average scores from a sample of human experts. Finally, we trained PH-LLM to predict self-reported sleep quality outcomes from textual and multimodal encoding representations of wearable data, and demonstrate that multimodal encoding is required to match performance of specialized discriminative models. Although further development and evaluation are necessary in the safety-critical personal health domain, these results demonstrate both the broad knowledge and capabilities of Gemini models and the benefit of contextualizing physiological data for personal health applications as done with PH-LLM.

------------

`[2406.05213] On Subjective Uncertainty Quantification and Calibration in Natural Language Generation <https://arxiv.org/abs/2406.05213>`__ 自然语言生成中主观不确定性的量化与校准

::

    Fri, 7 Jun 2024 18:54:40 GMT
    Ziyu Wang, Chris Holmes

Applications of large language models often involve the generation of free-form responses, in which case uncertainty quantification becomes challenging. This is due to the need to identify task-specific uncertainties (e.g., about the semantics) which appears difficult to define in general cases.
This work addresses these challenges from a perspective of Bayesian decision theory, starting from the assumption that our utility is characterized by a similarity measure that compares a generated response with a hypothetical true response. We discuss how this assumption enables principled quantification of the model's subjective uncertainty and its calibration. We further derive a measure for epistemic uncertainty, based on a missing data perspective and its characterization as an excess risk. The proposed measures can be applied to black-box language models. We demonstrate the proposed methods on question answering and machine translation tasks, where they extract broadly meaningful uncertainty estimates from GPT and Gemini models and quantify their calibration.

------------

`[2406.05232] Improving Logits-based Detector without Logits from Black-box LLMs <https://arxiv.org/abs/2406.05232>`__ 

::

    Fri, 7 Jun 2024 19:38:05 GMT
    Cong Zeng, Shengkun Tang, Xianjun Yang, Yuanzhou Chen, Yiyou Sun, zhiqiang xu, Yao Li, Haifeng Chen, Wei Cheng, Dongkuan Xu

The advent of Large Language Models (LLMs) has revolutionized text generation, producing outputs that closely mimic human writing. This blurring of lines between machine- and human-written text presents new challenges in distinguishing one from the other a task further complicated by the frequent updates and closed nature of leading proprietary LLMs. Traditional logits-based detection methods leverage surrogate models for identifying LLM-generated content when the exact logits are unavailable from black-box LLMs. However, these methods grapple with the misalignment between the distributions of the surrogate and the often undisclosed target models, leading to performance degradation, particularly with the introduction of new, closed-source models.
Furthermore, while current methodologies are generally effective when the source model is identified, they falter in scenarios where the model version remains unknown, or the test set comprises outputs from various source models.
To address these limitations, we present Distribution-Aligned LLMs Detection (DALD), an innovative framework that redefines the state-of-the-art performance in black-box text detection even without logits from source LLMs. DALD is designed to align the surrogate model's distribution with that of unknown target LLMs, ensuring enhanced detection capability and resilience against rapid model iterations with minimal training investment. By leveraging corpus samples from publicly accessible outputs of advanced models such as ChatGPT, GPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize with unknown source model distributions effectively.

------------

`[2406.05255] Generative Explore-Exploit: Training-free Optimization of Generative Recommender Systems using LLM Optimizers <https://arxiv.org/abs/2406.05255>`__ 生成式探索-利用:使用LLM优化器的生成式推荐系统的无训练优化

::

    Fri, 7 Jun 2024 20:41:59 GMT
    L\"utfi Kerem Senel, Besnik Fetahu, Davis Yoshida, Zhiyu Chen, Giuseppe Castellucci, Nikhita Vedula, Jason Choi, Shervin Malmasi

Recommender systems are widely used to suggest engaging content, and Large Language Models (LLMs) have given rise to generative recommenders. Such systems can directly generate items, including for open-set tasks like question suggestion. While the world knowledge of LLMs enable good recommendations, improving the generated content through user feedback is challenging as continuously fine-tuning LLMs is prohibitively expensive. We present a training-free approach for optimizing generative recommenders by connecting user feedback loops to LLM-based optimizers. We propose a generative explore-exploit method that can not only exploit generated items with known high engagement, but also actively explore and discover hidden population preferences to improve recommendation quality. We evaluate our approach on question generation in two domains (e-commerce and general knowledge), and model user feedback with Click Through Rate (CTR). Experiments show our LLM-based explore-exploit approach can iteratively improve recommendations, and consistently increase CTR. Ablation analysis shows that generative exploration is key to learning user preferences, avoiding the pitfalls of greedy exploit-only approaches. A human evaluation strongly supports our quantitative findings.

------------

`[2406.05322] Teaching-Assistant-in-the-Loop: Improving Knowledge Distillation from Imperfect Teacher Models in Low-Budget Scenarios <https://arxiv.org/abs/2406.05322>`__ 教学辅助在环:改进从低预算场景中不完美教师模型提取的知识

::

    Sat, 8 Jun 2024 02:17:43 GMT
    Yuhang Zhou, Wei Ai

There is increasing interest in distilling task-specific knowledge from large language models (LLM) to smaller student models. Nonetheless, LLM distillation presents a dual challenge: 1) there is a high cost associated with querying the teacher LLM, such as GPT-4, for gathering an ample number of demonstrations; 2) the teacher LLM might provide imperfect outputs with a negative impact on the student's learning process. To enhance sample efficiency within resource-constrained, imperfect teacher scenarios, we propose a three-component framework leveraging three signal types. The first signal is the student's self-consistency (consistency of student multiple outputs), which is a proxy of the student's confidence. Specifically, we introduce a ``teaching assistant'' (TA) model to assess the uncertainty of both the student's and the teacher's outputs via confidence scoring, which serves as another two signals for student training. Furthermore, we propose a two-stage training schema to first warm up the student with a small proportion of data to better utilize student's signal.
Experiments have shown the superiority of our proposed framework for four complex reasoning tasks. On average, our proposed two-stage framework brings a relative improvement of up to 20.79% compared to fine-tuning without any signals across datasets.

------------

`[2406.05328] Hidden Question Representations Tell Non-Factuality Within and Across Large Language Models <https://arxiv.org/abs/2406.05328>`__ 隐藏问题表示告诉大型语言模型内部和跨语言模型的非事实性

::

    Sat, 8 Jun 2024 02:59:52 GMT
    Yanling Wang, Haoyang Li, Hao Zou, Jing Zhang, Xinlei He, Qi Li, Ke Xu

Despite the remarkable advance of large language models (LLMs), the prevalence of non-factual responses remains a common issue. This work studies non-factuality prediction (NFP), which predicts whether an LLM will generate non-factual responses to a question before the generation process. Previous efforts on NFP usually rely on extensive computation. In this work, we conduct extensive analysis to explore the capabilities of using a lightweight probe to elicit ``whether an LLM knows'' from the hidden representations of questions.
Additionally, we discover that the non-factuality probe employs similar patterns for NFP across multiple LLMs. Motivated by the intriguing finding, we conduct effective transfer learning for cross-LLM NFP and propose a question-aligned strategy to ensure the efficacy of mini-batch based training.

------------

`[2406.05344] MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention <https://arxiv.org/abs/2406.05344>`__ MemeGuard:基于LLM和vlm的框架，通过模因干预推进内容审核

::

    Sat, 8 Jun 2024 04:09:20 GMT
    Prince Jha, Raghav Jain, Konika Mandal, Aman Chadha, Sriparna Saha, Pushpak Bhattacharyya

In the digital world, memes present a unique challenge for content moderation due to their potential to spread harmful content. Although detection methods have improved, proactive solutions such as intervention are still limited, with current research focusing mostly on text-based content, neglecting the widespread influence of multimodal content like memes. Addressing this gap, we present \textit{MemeGuard}, a comprehensive framework leveraging Large Language Models (LLMs) and Visual Language Models (VLMs) for meme intervention.
\textit{MemeGuard} harnesses a specially fine-tuned VLM, \textit{VLMeme}, for meme interpretation, and a multimodal knowledge selection and ranking mechanism (\textit{MKS}) for distilling relevant knowledge. This knowledge is then employed by a general-purpose LLM to generate contextually appropriate interventions. Another key contribution of this work is the \textit{\textbf{I}ntervening} \textit{\textbf{C}yberbullying in \textbf{M}ultimodal \textbf{M}emes (ICMM)} dataset, a high-quality, labeled dataset featuring toxic memes and their corresponding human-annotated interventions. We leverage \textit{ICMM} to test \textit{MemeGuard}, demonstrating its proficiency in generating relevant and effective responses to toxic memes.

------------

`[2406.05360] Flexible and Adaptable Summarization via Expertise Separation <https://arxiv.org/abs/2406.05360>`__ 基于专业知识分离的灵活自适应摘要

::

    Sat, 8 Jun 2024 05:31:19 GMT
    Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qingqing Zhu, Rui Yan, Xin Gao, Xiangliang Zhang

A proficient summarization model should exhibit both flexibility -- the capacity to handle a range of in-domain summarization tasks, and adaptability -- the competence to acquire new knowledge and adjust to unseen out-of-domain tasks. Unlike large language models (LLMs) that achieve this through parameter scaling, we propose a more parameter-efficient approach in this study. Our motivation rests on the principle that the general summarization ability to capture salient information can be shared across different tasks, while the domain-specific summarization abilities need to be distinct and tailored.
Concretely, we propose MoeSumm, a Mixture-of-Expert Summarization architecture, which utilizes a main expert for gaining the general summarization capability and deputy experts that selectively collaborate to meet specific summarization task requirements. We further propose a max-margin loss to stimulate the separation of these abilities. Our model's distinct separation of general and domain-specific summarization abilities grants it with notable flexibility and adaptability, all while maintaining parameter efficiency. MoeSumm achieves flexibility by managing summarization across multiple domains with a single model, utilizing a shared main expert and selected deputy experts. It exhibits adaptability by tailoring deputy experts to cater to out-of-domain few-shot and zero-shot scenarios. Experimental results on 11 datasets show the superiority of our model compared with recent baselines and LLMs. We also provide statistical and visual evidence of the distinct separation of the two abilities in MoeSumm (https://github.com/iriscxy/MoE_Summ).

------------

`[2406.05361] Write Summary Step-by-Step: A Pilot Study of Stepwise Summarization <https://arxiv.org/abs/2406.05361>`__ 分步写摘要:对分步总结的初步研究

::

    Sat, 8 Jun 2024 05:37:26 GMT
    Xiuying Chen, Shen Gao, Mingzhe Li, Qingqing Zhu, Xin Gao, Xiangliang Zhang

Nowadays, neural text generation has made tremendous progress in abstractive summarization tasks. However, most of the existing summarization models take in the whole document all at once, which sometimes cannot meet the needs in practice. Practically, social text streams such as news events and tweets keep growing from time to time, and can only be fed to the summarization system step by step. Hence, in this paper, we propose the task of Stepwise Summarization, which aims to generate a new appended summary each time a new document is proposed. The appended summary should not only summarize the newly added content but also be coherent with the previous summary, to form an up-to-date complete summary. To tackle this challenge, we design an adversarial learning model, named Stepwise Summary Generator (SSG). First, SSG selectively processes the new document under the guidance of the previous summary, obtaining polished document representation. Next, SSG generates the summary considering both the previous summary and the document. Finally, a convolutional-based discriminator is employed to determine whether the newly generated summary is coherent with the previous summary. For the experiment, we extend the traditional two-step update summarization setting to a multi-step stepwise setting, and re-propose a large-scale stepwise summarization dataset based on a public story generation dataset. Extensive experiments on this dataset show that SSG achieves state-of-the-art performance in terms of both automatic metrics and human evaluations. Ablation studies demonstrate the effectiveness of each module in our framework. We also discuss the benefits and limitations of recent large language models on this task.

------------

`[2406.05374] Planning Like Human: A Dual-process Framework for Dialogue Planning <https://arxiv.org/abs/2406.05374>`__ 像人一样规划:对话规划的双过程框架

::

    Sat, 8 Jun 2024 06:52:47 GMT
    Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Ming Liu, Zerui Chen, Bing Qin

In proactive dialogue, the challenge lies not just in generating responses but in steering conversations toward predetermined goals, a task where Large Language Models (LLMs) typically struggle due to their reactive nature.
Traditional approaches to enhance dialogue planning in LLMs, ranging from elaborate prompt engineering to the integration of policy networks, either face efficiency issues or deliver suboptimal performance. Inspired by the dualprocess theory in psychology, which identifies two distinct modes of thinking - intuitive (fast) and analytical (slow), we propose the Dual-Process Dialogue Planning (DPDP) framework. DPDP embodies this theory through two complementary planning systems: an instinctive policy model for familiar contexts and a deliberative Monte Carlo Tree Search (MCTS) mechanism for complex, novel scenarios. This dual strategy is further coupled with a novel two-stage training regimen: offline Reinforcement Learning for robust initial policy model formation followed by MCTS-enhanced on-the-fly learning, which ensures a dynamic balance between efficiency and strategic depth. Our empirical evaluations across diverse dialogue tasks affirm DPDP's superiority in achieving both high-quality dialogues and operational efficiency, outpacing existing methods.

------------

`[2406.05392] Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas <https://arxiv.org/abs/2406.05392>`__ 

::

    Sat, 8 Jun 2024 07:55:01 GMT
    Chengyuan Deng, Yiqun Duan, Xin Jin, Heng Chang, Yijun Tian, Han Liu, Henry Peng Zou, Yiqiao Jin, Yijia Xiao, Yichen Wang, Shenghao Wu, Zongxing Xie, Kuofeng Gao, Sihong He, Jun Zhuang, Lu Cheng, Haohan Wang

Large Language Models (LLMs) have achieved unparalleled success across diverse language modeling tasks in recent years. However, this progress has also intensified ethical concerns, impacting the deployment of LLMs in everyday contexts. This paper provides a comprehensive survey of ethical challenges associated with LLMs, from longstanding issues such as copyright infringement, systematic bias, and data privacy, to emerging problems like truthfulness and social norms. We critically analyze existing research aimed at understanding, examining, and mitigating these ethical risks. Our survey underscores integrating ethical standards and societal values into the development of LLMs, thereby guiding the development of responsible and ethically aligned language models.

------------

`[2406.05460] Fighting Against the Repetitive Training and Sample Dependency Problem in Few-shot Named Entity Recognition <https://arxiv.org/abs/2406.05460>`__ 解决小样本命名实体识别中的重复训练和样本依赖问题

::

    Sat, 8 Jun 2024 12:36:30 GMT
    Chang Tian, Wenpeng Yin, Dan Li, Marie-Francine Moens

Few-shot named entity recognition (NER) systems recognize entities using a few labeled training examples. The general pipeline consists of a span detector to identify entity spans in text and an entity-type classifier to assign types to entities. Current span detectors rely on extensive manual labeling to guide training. Almost every span detector requires initial training on basic span features followed by adaptation to task-specific features. This process leads to repetitive training of the basic span features among span detectors.
Additionally, metric-based entity-type classifiers, such as prototypical networks, typically employ a specific metric that gauges the distance between the query sample and entity-type referents, ultimately assigning the most probable entity type to the query sample. However, these classifiers encounter the sample dependency problem, primarily stemming from the limited samples available for each entity-type referent. To address these challenges, we proposed an improved few-shot NER pipeline. First, we introduce a steppingstone span detector that is pre-trained on open-domain Wikipedia data. It can be used to initialize the pipeline span detector to reduce the repetitive training of basic features. Second, we leverage a large language model (LLM) to set reliable entity-type referents, eliminating reliance on few-shot samples of each type. Our model exhibits superior performance with fewer training steps and human-labeled data compared with baselines, as demonstrated through extensive experiments on various datasets. Particularly in fine-grained few-shot NER settings, our model outperforms strong baselines, including ChatGPT. We will publicly release the code, datasets, LLM outputs, and model checkpoints.

------------

`[2406.05494] Investigating and Addressing Hallucinations of LLMs in Tasks Involving Negation <https://arxiv.org/abs/2406.05494>`__ 在涉及否定的任务中调查和解决llm的幻觉

::

    Sat, 8 Jun 2024 15:20:56 GMT
    Neeraj Varshney, Satyam Raj, Venkatesh Mishra, Agneet Chatterjee, Ritika Sarkar, Amir Saeidi, Chitta Baral

Large Language Models (LLMs) have achieved remarkable performance across a wide variety of natural language tasks. However, they have been shown to suffer from a critical limitation pertinent to 'hallucination' in their output. Recent research has focused on investigating and addressing this problem for a variety of tasks such as biography generation, question answering, abstractive summarization, and dialogue generation. However, the crucial aspect pertaining to 'negation' has remained considerably underexplored. Negation is important because it adds depth and nuance to the understanding of language and is also crucial for logical reasoning and inference. In this work, we address the above limitation and particularly focus on studying the impact of negation in LLM hallucinations. Specifically, we study four tasks with negation: 'false premise completion', 'constrained fact generation', 'multiple choice question answering', and 'fact generation'. We show that open-source state-of-the-art LLMs such as LLaMA-2-chat, Vicuna, and Orca-2 hallucinate considerably on all these tasks involving negation which underlines a critical shortcoming of these models. Addressing this problem, we further study numerous strategies to mitigate these hallucinations and demonstrate their impact.

------------

`[2406.05559] ThatiAR: Subjectivity Detection in Arabic News Sentences <https://arxiv.org/abs/2406.05559>`__ 阿拉伯语新闻句子主观性检测

::

    Sat, 8 Jun 2024 19:24:17 GMT
    Reem Suwaileh, Maram Hasanain, Fatema Hubail, Wajdi Zaghouani, Firoj Alam

Detecting subjectivity in news sentences is crucial for identifying media bias, enhancing credibility, and combating misinformation by flagging opinion-based content. It provides insights into public sentiment, empowers readers to make informed decisions, and encourages critical thinking. While research has developed methods and systems for this purpose, most efforts have focused on English and other high-resourced languages. In this study, we present the first large dataset for subjectivity detection in Arabic, consisting of ~3.6K manually annotated sentences, and GPT-4o based explanation.
In addition, we included instructions (both in English and Arabic) to facilitate LLM based fine-tuning. We provide an in-depth analysis of the dataset, annotation process, and extensive benchmark results, including PLMs and LLMs. Our analysis of the annotation process highlights that annotators were strongly influenced by their political, cultural, and religious backgrounds, especially at the beginning of the annotation process. The experimental results suggest that LLMs with in-context learning provide better performance. We aim to release the dataset and resources for the community.

------------

`[2406.05569] Do LLMs Recognize me, When I is not me: Assessment of LLMs Understanding of Turkish Indexical Pronouns in Indexical Shift Contexts <https://arxiv.org/abs/2406.05569>`__ 法学硕士认识我吗，当我不是我:评估法学硕士对索引转换语境中土耳其索引代词的理解

::

    Sat, 8 Jun 2024 20:30:53 GMT
    Metehan O\u{g}uz, Yusuf Umut Ciftci, Yavuz Faruk Bakman

Large language models (LLMs) have shown impressive capabilities in tasks such as machine translation, text summarization, question answering, and solving complex mathematical problems. However, their primary training on data-rich languages like English limits their performance in low-resource languages. This study addresses this gap by focusing on the Indexical Shift problem in Turkish.
The Indexical Shift problem involves resolving pronouns in indexical shift contexts, a grammatical challenge not present in high-resource languages like English. We present the first study examining indexical shift in any language, releasing a Turkish dataset specifically designed for this purpose. Our Indexical Shift Dataset consists of 156 multiple-choice questions, each annotated with necessary linguistic details, to evaluate LLMs in a few-shot setting. We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset. Our analysis reveals that even advanced models like GPT-4 struggle with the grammatical nuances of indexical shift in Turkish, achieving only moderate performance.
These findings underscore the need for focused research on the grammatical challenges posed by low-resource languages. We released the dataset and code \href{https://anonymous.4open.science/r/indexical_shift_llm-E1B4} {here}.

------------

`[2406.05587] Creativity Has Left the Chat: The Price of Debiasing Language Models <https://arxiv.org/abs/2406.05587>`__ 创造性已经离开了话题:去偏见语言模型的代价

::

    Sat, 8 Jun 2024 22:14:51 GMT
    Behnam Mohammadi

Large Language Models (LLMs) have revolutionized natural language processing but can exhibit biases and may generate toxic content. While alignment techniques like Reinforcement Learning from Human Feedback (RLHF) reduce these issues, their impact on creativity, defined as syntactic and semantic diversity, remains unexplored. We investigate the unintended consequences of RLHF on the creativity of LLMs through three experiments focusing on the Llama-2 series. Our findings reveal that aligned models exhibit lower entropy in token predictions, form distinct clusters in the embedding space, and gravitate towards "attractor states", indicating limited output diversity. Our findings have significant implications for marketers who rely on LLMs for creative tasks such as copywriting, ad creation, and customer persona generation. The trade-off between consistency and creativity in aligned models should be carefully considered when selecting the appropriate model for a given application. We also discuss the importance of prompt engineering in harnessing the creative potential of base models.

------------

`[2406.05588] CERET: Cost-Effective Extrinsic Refinement for Text Generation <https://arxiv.org/abs/2406.05588>`__ CERET:具有成本效益的文本生成外部优化

::

    Sat, 8 Jun 2024 22:17:52 GMT
    Jason Cai, Hang Su, Monica Sunkara, Igor Shalyminov, Saab Mansour

Large Language Models (LLMs) are powerful models for generation tasks, but they may not generate good quality outputs in their first attempt. Apart from model fine-tuning, existing approaches to improve prediction accuracy and quality typically involve LLM self-improvement / self-reflection that incorporate feedback from models themselves. Despite their effectiveness, these methods are hindered by their high computational cost and lack of scalability.
In this work, we propose CERET, a method for refining text generations by considering semantic stability, entailment and inter-sample uncertainty measures. Experimental results show that CERET outperforms Self-consistency and Self-rerank baselines consistently under various task setups, by ~1.6% in Rouge-1 for abstractive summarization and ~3.5% in hit rate for question answering. Compared to LLM Self-rerank method, our approach only requires 9.4% of its latency and is more cost-effective.

------------

`[2406.05606] GrowOVER: How Can LLMs Adapt to Growing Real-World Knowledge? <https://arxiv.org/abs/2406.05606>`__ GrowOVER: llm如何适应不断增长的现实世界知识?

::

    Sun, 9 Jun 2024 01:16:04 GMT
    Dayoon Ko, Jinyoung Kim, Hahyeon Choi and Gunhee Kim

In the real world, knowledge is constantly evolving, which can render existing knowledge-based datasets outdated. This unreliability highlights the critical need for continuous updates to ensure both accuracy and relevance in knowledge-intensive tasks. To address this, we propose GrowOVER-QA and GrowOVER-Dialogue, dynamic open-domain QA and dialogue benchmarks that undergo a continuous cycle of updates, keeping pace with the rapid evolution of knowledge. Our research indicates that retrieval-augmented language models (RaLMs) struggle with knowledge that has not been trained on or recently updated. Consequently, we introduce a novel retrieval-interactive language model framework, where the language model evaluates and reflects on its answers for further re-retrieval. Our exhaustive experiments demonstrate that our training-free framework significantly improves upon existing methods, performing comparably to or even surpassing continuously trained language models.

------------

`[2406.05644] How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States <https://arxiv.org/abs/2406.05644>`__ 对齐和越狱是如何工作的:通过中间隐藏状态解释LLM的安全性

::

    Sun, 9 Jun 2024 05:04:37 GMT
    Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Yongbin Li

Large language models (LLMs) rely on safety alignment to avoid responding to malicious user inputs. Unfortunately, jailbreak can circumvent safety guardrails, resulting in LLMs generating harmful content and raising concerns about LLM safety. Due to language models with intensive parameters often regarded as black boxes, the mechanisms of alignment and jailbreak are challenging to elucidate. In this paper, we employ weak classifiers to explain LLM safety through the intermediate hidden states. We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers. Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations. Jailbreak disturbs the transformation of early unethical classification into negative emotions. We conduct experiments on models from 7B to 70B across various model families to prove our conclusion. Overall, our paper indicates the intrinsical mechanism of LLM safety and how jailbreaks circumvent safety guardrails, offering a new perspective on LLM safety and reducing concerns.

------------

`[2406.05678] SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models <https://arxiv.org/abs/2406.05678>`__ SinkLoRA:增强的长上下文大型语言模型的效率和聊天能力

::

    Sun, 9 Jun 2024 07:23:34 GMT
    Hengyu Zhang

Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\(^2\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of "sink attention tokens", achieving 92\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \url{https://github.com/Dexter-GT-86/SinkLoRA}.

------------

`[2406.05688] Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions <https://arxiv.org/abs/2406.05688>`__ 同行评议是基于角色互动的多轮长语境对话

::

    Sun, 9 Jun 2024 08:24:17 GMT
    Cheng Tan, Dongxin Lyu, Siyuan Li, Zhangyang Gao, Jingxuan Wei, Siqi Ma, Zicheng Liu, Stan Z. Li

Large Language Models (LLMs) have demonstrated wide-ranging applications across various fields and have shown significant potential in the academic peer-review process. However, existing applications are primarily limited to static review generation based on submitted papers, which fail to capture the dynamic and iterative nature of real-world peer reviews. In this paper, we reformulate the peer-review process as a multi-turn, long-context dialogue, incorporating distinct roles for authors, reviewers, and decision makers. We construct a comprehensive dataset containing over 26,841 papers with 92,017 reviews collected from multiple sources, including the top-tier conference and prestigious journal. This dataset is meticulously designed to facilitate the applications of LLMs for multi-turn dialogues, effectively simulating the complete peer-review process. Furthermore, we propose a series of metrics to evaluate the performance of LLMs for each role under this reformulated peer-review setting, ensuring fair and comprehensive evaluations. We believe this work provides a promising perspective on enhancing the LLM-driven peer-review process by incorporating dynamic, role-based interactions. It aligns closely with the iterative and interactive nature of real-world academic peer review, offering a robust foundation for future research and development in this area. We open-source the dataset at https://github.com/chengtan9907/ReviewMT.

------------

`[2406.05690] MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation <https://arxiv.org/abs/2406.05690>`__ MoPS:面向开放式自动故事生成的模块化故事前提合成

::

    Sun, 9 Jun 2024 08:31:14 GMT
    Yan Ma, Yu Qiao, Pengfei Liu

A story premise succinctly defines a story's main idea, foundation, and trajectory. It serves as the initial trigger in automatic story generation.
Existing sources of story premises are limited by a lack of diversity, uneven quality, and high costs that make them difficult to scale. In response, we introduce Modular Story Premise Synthesis (MoPS) which breaks down story premises into modules like background and persona for automated design and generation. MoPS consists of three phases: (1) Precollect a consistent set of candidates for each module to form a nested dictionary. (2) Extract a key path from the nested dictionary as the premise design. (3) Instruct an LLM to integrate the design into a coherent premise sentence. Thorough evaluations demonstrate that our synthesized premises excel in diversity, fascination, completeness, and originality compared to those induced from large language models and captured from public story datasets. Similarly, the extended novels and scripts generated from our premises also exhibit higher quality. In supplementary materials, we provide the MoPS code suite, along with 7.6k generated premises and 1k extended stories. Code: https://github.com/GAIR-NLP/MoPS.

------------

`[2406.05798] Hidden Holes: topological aspects of language models <https://arxiv.org/abs/2406.05798>`__ 隐洞:语言模型的拓扑方面

::

    Sun, 9 Jun 2024 14:25:09 GMT
    Stephen Fitz, Peter Romero, Jiyan Jonas Schneider

We explore the topology of representation manifolds arising in autoregressive neural language models trained on raw text data. In order to study their properties, we introduce tools from computational algebraic topology, which we use as a basis for a measure of topological complexity, that we call perforation.
Using this measure, we study the evolution of topological structure in GPT based large language models across depth and time during training. We then compare these to gated recurrent models, and show that the latter exhibit more topological complexity, with a distinct pattern of changes common to all natural languages but absent from synthetically generated data. The paper presents a detailed analysis of the representation manifolds derived by these models based on studying the shapes of vector clouds induced by them as they are conditioned on sentences from corpora of natural language text.
The methods developed in this paper are novel in the field and based on mathematical apparatus that might be unfamiliar to the target audience. To help with that we introduce the minimum necessary theory, and provide additional visualizations in the appendices.
The main contribution of the paper is a striking observation about the topological structure of the transformer as compared to LSTM based neural architectures. It suggests that further research into mathematical properties of these neural networks is necessary to understand the operation of large transformer language models. We hope this work inspires further explorations in this direction within the NLP community.

------------

`[2406.05812] Seventeenth-Century Spanish American Notary Records for Fine-Tuning Spanish Large Language Models <https://arxiv.org/abs/2406.05812>`__ 17世纪西班牙裔美国人公证人对西班牙大型语言模型进行微调的记录

::

    Sun, 9 Jun 2024 14:54:22 GMT
    Shraboni Sarker, Ahmad Tamim Hamad, Hulayyil Alshammari, Viviana Grieco, Praveen Rao

Large language models have gained tremendous popularity in domains such as e-commerce, finance, healthcare, and education. Fine-tuning is a common approach to customize an LLM on a domain-specific dataset for a desired downstream task. In this paper, we present a valuable resource for fine-tuning LLMs developed for the Spanish language to perform a variety of tasks such as classification, masked language modeling, clustering, and others. Our resource is a collection of handwritten notary records from the seventeenth century obtained from the National Archives of Argentina. This collection contains a combination of original images and transcribed text (and metadata) of 160+ pages that were handwritten by two notaries, namely, Estenban Agreda de Vergara and Nicolas de Valdivia y Brisuela nearly 400 years ago. Through empirical evaluation, we demonstrate that our collection can be used to fine-tune Spanish LLMs for tasks such as classification and masked language modeling, and can outperform pre-trained Spanish models and ChatGPT-3.5/ChatGPT-4o. Our resource will be an invaluable resource for historical text analysis and is publicly available on GitHub.

------------

`[2406.05845] MedREQAL: Examining Medical Knowledge Recall of Large Language Models via Question Answering <https://arxiv.org/abs/2406.05845>`__ MedREQAL:基于问答的大型语言模型医学知识召回研究

::

    Sun, 9 Jun 2024 16:33:28 GMT
    Juraj Vladika, Phillip Schneider, Florian Matthes

In recent years, Large Language Models (LLMs) have demonstrated an impressive ability to encode knowledge during pre-training on large text corpora. They can leverage this knowledge for downstream tasks like question answering (QA), even in complex areas involving health topics. Considering their high potential for facilitating clinical work in the future, understanding the quality of encoded medical knowledge and its recall in LLMs is an important step forward. In this study, we examine the capability of LLMs to exhibit medical knowledge recall by constructing a novel dataset derived from systematic reviews -- studies synthesizing evidence-based answers for specific medical questions. Through experiments on the new MedREQAL dataset, comprising question-answer pairs extracted from rigorous systematic reviews, we assess six LLMs, such as GPT and Mixtral, analyzing their classification and generation performance. Our experimental insights into LLM performance on the novel biomedical QA dataset reveal the still challenging nature of this task.

------------

`[2406.05876] Zero-Shot End-To-End Spoken Question Answering In Medical Domain <https://arxiv.org/abs/2406.05876>`__ 医疗领域零样本端到端口语问答

::

    Sun, 9 Jun 2024 18:13:36 GMT
    Yanis Labrak, Adel Moumen, Richard Dufour and Mickael Rouvier

In the rapidly evolving landscape of spoken question-answering (SQA), the integration of large language models (LLMs) has emerged as a transformative development. Conventional approaches often entail the use of separate models for question audio transcription and answer selection, resulting in significant resource utilization and error accumulation. To tackle these challenges, we explore the effectiveness of end-to-end (E2E) methodologies for SQA in the medical domain. Our study introduces a novel zero-shot SQA approach, compared to traditional cascade systems. Through a comprehensive evaluation conducted on a new open benchmark of 8 medical tasks and 48 hours of synthetic audio, we demonstrate that our approach requires up to 14.7 times fewer resources than a combined 1.3B parameters LLM with a 1.55B parameters ASR model while improving average accuracy by 0.5\%. These findings underscore the potential of E2E methodologies for SQA in resource-constrained contexts.

------------

`[2406.05885] Are Large Language Models Actually Good at Text Style Transfer? <https://arxiv.org/abs/2406.05885>`__ 大型语言模型真的擅长文本风格迁移吗?

::

    Sun, 9 Jun 2024 18:45:41 GMT
    Sourabrata Mukherjee, Atul Kr. Ojha, Ond\v{r}ej Du\v{s}ek

We analyze the performance of large language models (LLMs) on Text Style Transfer (TST), specifically focusing on sentiment transfer and text detoxification across three languages: English, Hindi, and Bengali. Text Style Transfer involves modifying the linguistic style of a text while preserving its core content. We evaluate the capabilities of pre-trained LLMs using zero-shot and few-shot prompting as well as parameter-efficient finetuning on publicly available datasets. Our evaluation using automatic metrics, GPT-4 and human evaluations reveals that while some prompted LLMs perform well in English, their performance in on other languages (Hindi, Bengali) remains average.
However, finetuning significantly improves results compared to zero-shot and few-shot prompting, making them comparable to previous state-of-the-art. This underscores the necessity of dedicated datasets and specialized models for effective TST.

------------

`[2406.05888] Feriji: A French-Zarma Parallel Corpus, Glossary & Translator <https://arxiv.org/abs/2406.05888>`__ Feriji:法语- zarma平行语料库、词汇表和翻译

::

    Sun, 9 Jun 2024 19:08:33 GMT
    Mamadou K. Keita, Elysabhete Amadou Ibrahim, Habibatou Abdoulaye Alfari, Christopher Homan

Machine translation (MT) is a rapidly expanding field that has experienced significant advancements in recent years with the development of models capable of translating multiple languages with remarkable accuracy. However, the representation of African languages in this field still needs to improve due to linguistic complexities and limited resources. This applies to the Zarma language, a dialect of Songhay (of the Nilo-Saharan language family) spoken by over 5 million people across Niger and neighboring countries \cite{lewis2016ethnologue}. This paper introduces Feriji, the first robust French-Zarma parallel corpus and glossary designed for MT. The corpus, containing 61,085 sentences in Zarma and 42,789 in French, and a glossary of 4,062 words represent a significant step in addressing the need for more resources for Zarma. We fine-tune three large language models on our dataset, obtaining a BLEU score of 30.06 on the best-performing model. We further evaluate the models on human judgments of fluency, comprehension, and readability and the importance and impact of the corpus and models. Our contributions help to bridge a significant language gap and promote an essential and overlooked indigenous African language.

------------

`[2406.05918] Why Don't Prompt-Based Fairness Metrics Correlate? <https://arxiv.org/abs/2406.05918>`__ 为什么基于提示的公平性指标没有关联?

::

    Sun, 9 Jun 2024 21:12:15 GMT
    Abdelrahman Zayed, Goncalo Mordido, Ioana Baldini, Sarath Chandar

The widespread use of large language models has brought up essential questions about the potential biases these models might learn. This led to the development of several metrics aimed at evaluating and mitigating these biases.
In this paper, we first demonstrate that prompt-based fairness metrics exhibit poor agreement, as measured by correlation, raising important questions about the reliability of fairness assessment using prompts. Then, we outline six relevant reasons why such a low correlation is observed across existing metrics. Based on these insights, we propose a method called Correlated Fairness Output (CAIRO) to enhance the correlation between fairness metrics.
CAIRO augments the original prompts of a given fairness metric by using several pre-trained language models and then selects the combination of the augmented prompts that achieves the highest correlation across metrics. We show a significant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and 0.98 across metrics for gender and religion biases, respectively. Our code is available at https://github.com/chandar-lab/CAIRO.

------------

`[2406.06027] HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs <https://arxiv.org/abs/2406.06027>`__ HOLMES:基于llm的多跳问答超关系知识图谱

::

    Mon, 10 Jun 2024 05:22:49 GMT
    Pranoy Panda, Ankush Agarwal, Chaitanya Devaguptapu, Manohar Kaul, Prathosh A P

Given unstructured text, Large Language Models (LLMs) are adept at answering simple (single-hop) questions. However, as the complexity of the questions increase, the performance of LLMs degrade. We believe this is due to the overhead associated with understanding the complex question followed by filtering and aggregating unstructured information in the raw text. Recent methods try to reduce this burden by integrating structured knowledge triples into the raw text, aiming to provide a structured overview that simplifies information processing. However, this simplistic approach is query-agnostic and the extracted facts are ambiguous as they lack context. To address these drawbacks and to enable LLMs to answer complex (multi-hop) questions with ease, we propose to use a knowledge graph (KG) that is context-aware and is distilled to contain query-relevant information. The use of our compressed distilled KG as input to the LLM results in our method utilizing up to $67\%$ fewer tokens to represent the query relevant information present in the supporting documents, compared to the state-of-the-art (SoTA) method. Our experiments show consistent improvements over the SoTA across several metrics (EM, F1, BERTScore, and Human Eval) on two popular benchmark datasets (HotpotQA and MuSiQue).

------------

`[2406.06056] Synth-SBDH: A Synthetic Dataset of Social and Behavioral Determinants of Health for Clinical Text <https://arxiv.org/abs/2406.06056>`__ Synth-SBDH:面向临床文本的健康社会和行为决定因素合成数据集

::

    Mon, 10 Jun 2024 07:03:36 GMT
    Avijit Mitra, Emily Druhl, Raelene Goodwin, Hong Yu

Social and behavioral determinants of health (SBDH) play a crucial role in health outcomes and are frequently documented in clinical text. Automatically extracting SBDH information from clinical text relies on publicly available good-quality datasets. However, existing SBDH datasets exhibit substantial limitations in their availability and coverage. In this study, we introduce Synth-SBDH, a novel synthetic dataset with detailed SBDH annotations, encompassing status, temporal information, and rationale across 15 SBDH categories. We showcase the utility of Synth-SBDH on three tasks using real-world clinical datasets from two distinct hospital settings, highlighting its versatility, generalizability, and distillation capabilities. Models trained on Synth-SBDH consistently outperform counterparts with no Synth-SBDH training, achieving up to 62.5% macro-F improvements. Additionally, Synth-SBDH proves effective for rare SBDH categories and under-resource constraints. Human evaluation demonstrates a Human-LLM alignment of 71.06% and uncovers areas for future refinements.

------------

`[2406.06125] Verifiable Generation with Subsentence-Level Fine-Grained Citations <https://arxiv.org/abs/2406.06125>`__ 

::

    Mon, 10 Jun 2024 09:32:37 GMT
    Shuyang Cao and Lu Wang

Verifiable generation requires large language models (LLMs) to cite source documents supporting their outputs, thereby improve output transparency and trustworthiness. Yet, previous work mainly targets the generation of sentence-level citations, lacking specificity about which parts of a sentence are backed by the cited sources. This work studies verifiable generation with subsentence-level fine-grained citations for more precise location of generated content supported by the cited sources. We first present a dataset, SCiFi, comprising 10K Wikipedia paragraphs with subsentence-level citations. Each paragraph is paired with a set of candidate source documents for citation and a query that triggers the generation of the paragraph content. On SCiFi, we evaluate the performance of state-of-the-art LLMs and strategies for processing long documents designed for these models. Our experiment results reveals key factors that could enhance the quality of citations, including the expansion of the source documents' context accessible to the models and the implementation of specialized model tuning.

------------

`[2406.06140] Can I understand what I create? Self-Knowledge Evaluation of Large Language Models <https://arxiv.org/abs/2406.06140>`__ 我能理解我创造了什么吗?大型语言模型的自我认识评估

::

    Mon, 10 Jun 2024 09:53:54 GMT
    Zhiquan Tan, Lai Wei, Jindong Wang, Xing Xie, Weiran Huang

Large language models (LLMs) have achieved remarkable progress in linguistic tasks, necessitating robust evaluation frameworks to understand their capabilities and limitations. Inspired by Feynman's principle of understanding through creation, we introduce a self-knowledge evaluation framework that is easy to implement, evaluating models on their ability to comprehend and respond to self-generated questions. Our findings, based on testing multiple models across diverse tasks, reveal significant gaps in the model's self-knowledge ability. Further analysis indicates these gaps may be due to misalignment with human attention mechanisms. Additionally, fine-tuning on self-generated math task may enhance the model's math performance, highlighting the potential of the framework for efficient and insightful model evaluation and may also contribute to the improvement of LLMs.

------------

`[2406.06144] Language Models Resist Alignment <https://arxiv.org/abs/2406.06144>`__ 语言模型抵抗对齐

::

    Mon, 10 Jun 2024 10:03:16 GMT
    Jiaming Ji, Kaile Wang, Tianyi Qiu, Boyuan Chen, Jiayi Zhou, Changye Li, Hantao Lou, Yaodong Yang

Large language models (LLMs) may exhibit undesirable behaviors. Recent efforts have focused on aligning these models to prevent harmful generation.
Despite these efforts, studies have shown that even a well-conducted alignment process can be easily circumvented, whether intentionally or accidentally. Do alignment fine-tuning have robust effects on models, or are merely superficial? In this work, we answer this question through both theoretical and empirical means. Empirically, we demonstrate the elasticity of post-alignment models, i.e., the tendency to revert to the behavior distribution formed during the pre-training phase upon further fine-tuning. Using compression theory, we formally derive that such fine-tuning process \textit{disproportionately} undermines alignment compared to pre-training, potentially by orders of magnitude. We conduct experimental validations to confirm the presence of elasticity across models of varying types and sizes. Specifically, we find that model performance declines rapidly before reverting to the pre-training distribution, after which the rate of decline drops significantly. We further reveal that elasticity positively correlates with increased model size and the expansion of pre-training data. Our discovery signifies the importance of taming the inherent elasticity of LLMs, thereby overcoming the resistance of LLMs to alignment finetuning.

------------

`[2406.06316] Tx-LLM: A Large Language Model for Therapeutics <https://arxiv.org/abs/2406.06316>`__ Tx-LLM:治疗学大型语言模型

::

    Mon, 10 Jun 2024 14:33:02 GMT
    Juan Manuel Zambrano Chaves, Eric Wang, Tao Tu, Eeshit Dhaval Vaishnav, Byron Lee, S. Sara Mahdavi, Christopher Semturs, David Fleet, Vivek Natarajan, Shekoofeh Azizi

Developing therapeutics is a lengthy and expensive process that requires the satisfaction of many different criteria, and AI models capable of expediting the process would be invaluable. However, the majority of current AI approaches address only a narrowly defined set of tasks, often circumscribed within a particular domain. To bridge this gap, we introduce Tx-LLM, a generalist large language model (LLM) fine-tuned from PaLM-2 which encodes knowledge about diverse therapeutic modalities. Tx-LLM is trained using a collection of 709 datasets that target 66 tasks spanning various stages of the drug discovery pipeline. Using a single set of weights, Tx-LLM simultaneously processes a wide variety of chemical or biological entities(small molecules, proteins, nucleic acids, cell lines, diseases) interleaved with free-text, allowing it to predict a broad range of associated properties, achieving competitive with state-of-the-art (SOTA) performance on 43 out of 66 tasks and exceeding SOTA on 22. Among these, Tx-LLM is particularly powerful and exceeds best-in-class performance on average for tasks combining molecular SMILES representations with text such as cell line names or disease names, likely due to context learned during pretraining. We observe evidence of positive transfer between tasks with diverse drug types (e.g.,tasks involving small molecules and tasks involving proteins), and we study the impact of model size, domain finetuning, and prompting strategies on performance. We believe Tx-LLM represents an important step towards LLMs encoding biochemical knowledge and could have a future role as an end-to-end tool across the drug discovery development pipeline.

------------

`[2406.06326] Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching <https://arxiv.org/abs/2406.06326>`__ 自调整:指导llm通过自我教学有效地获取新知识

::

    Mon, 10 Jun 2024 14:42:20 GMT
    Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Yipeng Zhang, Haitao Mi, Helen Meng

Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world.
To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection.
Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on Llama2 family models reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge.

------------

`[2406.06369] Annotation alignment: Comparing LLM and human annotations of conversational safety <https://arxiv.org/abs/2406.06369>`__ 注释对齐:比较LLM和会话安全性的人工注释

::

    Mon, 10 Jun 2024 15:30:13 GMT
    Rajiv Movva, Pang Wei Koh, Emma Pierson

To what extent to do LLMs align with human perceptions of safety? We study this question via *annotation alignment*, the extent to which LLMs and humans agree when annotating the safety of user-chatbot conversations. We leverage the recent DICES dataset (Aroyo et al., 2023), in which 350 conversations are each rated for safety by 112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson correlation of $r = 0.59$ with the average annotator rating, higher than the median annotator's correlation with the average ($r=0.51$). We show that larger datasets are needed to resolve whether GPT-4 exhibits disparities in how well it correlates with demographic groups. Also, there is substantial idiosyncratic variation in correlation *within* groups, suggesting that race & gender do not fully capture differences in alignment. Finally, we find that GPT-4 cannot predict when one demographic group finds a conversation more unsafe than another.

------------

`[2406.06435] Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain <https://arxiv.org/abs/2406.06435>`__ 语言模型是可对齐的决策者:医疗分诊领域的数据集和应用

::

    Mon, 10 Jun 2024 16:25:23 GMT
    Brian Hu, Bill Ray, Alice Leung, Amy Summerville, David Joy, Christopher Funk, Arslan Basharat

In difficult decision-making scenarios, it is common to have conflicting opinions among expert human decision-makers as there may not be a single right answer. Such decisions may be guided by different attributes that can be used to characterize an individual's decision. We introduce a novel dataset for medical triage decision-making, labeled with a set of decision-maker attributes (DMAs). This dataset consists of 62 scenarios, covering six different DMAs, including ethical principles such as fairness and moral desert. We present a novel software framework for human-aligned decision-making by utilizing these DMAs, paving the way for trustworthy AI with better guardrails. Specifically, we demonstrate how large language models (LLMs) can serve as ethical decision-makers, and how their decisions can be aligned to different DMAs using zero-shot prompting. Our experiments focus on different open-source models with varying sizes and training techniques, such as Falcon, Mistral, and Llama 2.
Finally, we also introduce a new form of weighted self-consistency that improves the overall quantified performance. Our results provide new research directions in the use of LLMs as alignable decision-makers. The dataset and open-source software are publicly available at: https://github.com/ITM-Kitware/llm-alignable-dm.

------------

`[2406.06485] Can Language Models Serve as Text-Based World Simulators? <https://arxiv.org/abs/2406.06485>`__ 语言模型能充当基于文本的世界模拟器吗?

::

    Mon, 10 Jun 2024 17:24:44 GMT
    Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre C\^ot\'e, Peter Clark, Peter Jansen

Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM's capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.

------------

`[2406.05183] The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More <https://arxiv.org/abs/2406.05183>`__ 因子分解诅咒:你预测的token是反转诅咒的基础，还有更多

::

    Fri, 7 Jun 2024 18:00:37 GMT
    Ouail Kitouni, Niklas Nolte, Diane Bouchacourt, Adina Williams, Mike Rabbat, Mark Ibrahim

Today's best language models still struggle with hallucinations: factually incorrect generations, which impede their ability to reliably retrieve information seen during training. The reversal curse, where models cannot recall information when probed in a different order than was encountered during training, exemplifies this in information retrieval. We reframe the reversal curse as a factorization curse - a failure of models to learn the same joint distribution under different factorizations. Through a series of controlled experiments with increasing levels of realism including WikiReversal, a setting we introduce to closely simulate a knowledge intensive finetuning task, we find that the factorization curse is an inherent failure of the next-token prediction objective used in popular large language models. Moreover, we demonstrate reliable information retrieval cannot be solved with scale, reversed tokens, or even naive bidirectional-attention training. Consequently, various approaches to finetuning on specialized data would necessarily provide mixed results on downstream tasks, unless the model has already seen the right sequence of tokens. Across five tasks of varying levels of complexity, our results uncover a promising path forward: factorization-agnostic objectives can significantly mitigate the reversal curse and hint at improved knowledge storage and planning capabilities.

------------

`[2406.05223] CorDA: Context-Oriented Decomposition Adaptation of Large Language Models <https://arxiv.org/abs/2406.05223>`__ CorDA:面向上下文的大型语言模型分解自适应

::

    Fri, 7 Jun 2024 19:10:35 GMT
    Yibo Yang, Xiaojie Li, Zhongzhu Zhou, Shuaiwen Leon Song, Jianlong Wu, Liqiang Nie, Bernard Ghanem

Current parameter-efficient fine-tuning (PEFT) methods build adapters without considering the context of downstream task to learn, or the context of important knowledge to maintain. As a result, there is often a performance gap compared to full-parameter finetuning, and meanwhile the finetuned model suffers from catastrophic forgetting of the pre-trained world knowledge. In this paper, we propose CorDA, a Context-oriented Decomposition Adaptation method that builds learnable adapters from weight decomposition oriented by the context of downstream task or world knowledge. Concretely, we collect a few data samples, and perform singular value decomposition for each linear layer of a pre-trained LLM multiplied by the covariance matrix of the input activation using these samples. By doing so, the context of the representative samples is captured through deciding the factorizing orientation. Our method enables two options, the knowledge-preserved adaptation and the instruction-previewed adaptation. For the former, we use question-answering samples to obtain the covariance matrices, and use the decomposed components with the smallest $r$ singular values to initialize a learnable adapter, with the others frozen such that the world knowledge is better preserved. For the latter, we use the instruction data from the finetuning task, such as math or coding, to orientate the decomposition and train the largest $r$ components that capture the main characteristics of the task to learn. We conduct extensive experiments on Math, Code, and Instruction Following tasks. Our knowledge-preserved adaptation not only achieves better performance than LoRA on finetuning tasks, but also mitigates the forgetting of world knowledge. Our instruction-previewed adaptation is able to further enhance the finetuning performance, surpassing full-parameter finetuning and the state-of-the-art PEFT methods.

------------

`[2406.05317] LoCoCo: Dropping In Convolutions for Long Context Compression <https://arxiv.org/abs/2406.05317>`__ LoCoCo:减少卷积以进行长上下文压缩

::

    Sat, 8 Jun 2024 01:35:11 GMT
    Ruisi Cai, Yuandong Tian, Zhangyang Wang, Beidi Chen

This paper tackles the memory hurdle of processing long context sequences in Large Language Models (LLMs), by presenting a novel approach, Dropping In Convolutions for Long Context Compression (LoCoCo). LoCoCo employs only a fixed-size Key-Value (KV) cache, and can enhance efficiency in both inference and fine-tuning stages. Diverging from prior methods that selectively drop KV pairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information and ensure accurate attention modeling. This token integration is achieved through injecting one-dimensional convolutional kernels that dynamically calculate mixing weights for each KV cache slot. Designed for broad compatibility with existing LLM frameworks, LoCoCo allows for straightforward "drop-in" integration without needing architectural modifications, while incurring minimal tuning overhead. Experiments demonstrate that LoCoCo maintains consistently outstanding performance across various context lengths and can achieve a high context compression rate during both inference and fine-tuning phases. During inference, we successfully compressed up to 3482 tokens into a 128-size KV cache, while retaining comparable performance to the full sequence - an accuracy improvement of up to 0.2791 compared to baselines at the same cache size. During post-training tuning, we also effectively extended the context length from 4K to 32K using a KV cache of fixed size 512, achieving performance similar to fine-tuning with entire sequences.

------------

`[2406.05516] Verbalized Probabilistic Graphical Modeling with Large Language Models <https://arxiv.org/abs/2406.05516>`__ 基于大型语言模型的语言化概率图模型

::

    Sat, 8 Jun 2024 16:35:31 GMT
    Hengguan Huang, Xing Shen, Songtao Wang, Dianbo Liu, Hao Wang

Faced with complex problems, the human brain demonstrates a remarkable capacity to transcend sensory input and form latent understandings of perceived world patterns. However, this cognitive capacity is not explicitly considered or encoded in current large language models (LLMs). As a result, LLMs often struggle to capture latent structures and model uncertainty in complex compositional reasoning tasks. This work introduces a novel Bayesian prompting approach that facilitates training-free Bayesian inference with LLMs by using a verbalized Probabilistic Graphical Model (PGM). While traditional Bayesian approaches typically depend on extensive data and predetermined mathematical structures for learning latent factors and dependencies, our approach efficiently reasons latent variables and their probabilistic dependencies by prompting LLMs to adhere to Bayesian principles. We evaluated our model on several compositional reasoning tasks, both close-ended and open-ended. Our results indicate that the model effectively enhances confidence elicitation and text generation quality, demonstrating its potential to improve AI language understanding systems, especially in modeling uncertainty.

------------

`[2406.05882] Distributional Preference Alignment of LLMs via Optimal Transport <https://arxiv.org/abs/2406.05882>`__ 基于最优传输的llm分布偏好对齐

::

    Sun, 9 Jun 2024 18:41:05 GMT
    Igor Melnyk, Youssef Mroueh, Brian Belgodere, Mattia Rigotti, Apoorva Nitsure, Mikhail Yurochkin, Kristjan Greenewald, Jiri Navratil, Jerret Ross

Current LLM alignment techniques use pairwise human preferences at a sample level, and as such, they do not imply an alignment on the distributional level.
We propose in this paper Alignment via Optimal Transport (AOT), a novel method for distributional preference alignment of LLMs. AOT aligns LLMs on unpaired preference data by making the reward distribution of the positive samples stochastically dominant in the first order on the distribution of negative samples. We introduce a convex relaxation of this first-order stochastic dominance and cast it as an optimal transport problem with a smooth and convex cost. Thanks to the one-dimensional nature of the resulting optimal transport problem and the convexity of the cost, it has a closed-form solution via sorting on empirical measures. We fine-tune LLMs with this AOT objective, which enables alignment by penalizing the violation of the stochastic dominance of the reward distribution of the positive samples on the reward distribution of the negative samples. We analyze the sample complexity of AOT by considering the dual of the OT problem and show that it converges at the parametric rate.
Empirically, we show on a diverse set of alignment datasets and LLMs that AOT leads to state-of-the-art models in the 7B family of models when evaluated with Open LLM Benchmarks and AlpacaEval.

------------

`[2406.05883] Information Theoretic Guarantees For Policy Alignment In Large Language Models <https://arxiv.org/abs/2406.05883>`__ 大型语言模型中策略对齐的信息论保证

::

    Sun, 9 Jun 2024 18:41:50 GMT
    Youssef Mroueh

Policy alignment of large language models refers to constrained policy optimization, where the policy is optimized to maximize a reward while staying close to a reference policy with respect to an $f$-divergence such as the $\mathsf{KL}$ divergence. The best of $n$ alignment policy selects a sample from the reference policy that has the maximum reward among $n$ independent samples. For both cases (policy alignment and best of $n$), recent works showed empirically that the reward improvement of the aligned policy on the reference one scales like $\sqrt{\mathsf{KL}}$, with an explicit bound in $n$ on the $\mathsf{KL}$ for the best of $n$ policy. We show in this paper that the $\sqrt{\mathsf{KL}}$ information theoretic upper bound holds if the reward under the reference policy has sub-gaussian tails. Moreover, we prove for the best of $n$ policy, that the $\mathsf{KL}$ upper bound can be obtained for any $f$-divergence via a reduction to exponential order statistics owing to the R\'enyi representation of order statistics, and a data processing inequality.
If additional information is known on the tails of the aligned policy we show that tighter control on the reward improvement can be obtained via the R\'enyi divergence. Finally we demonstrate how these upper bounds transfer from proxy rewards to golden rewards which results in a decrease in the golden reward improvement due to overestimation and approximation errors of the proxy reward.

------------

`[2406.05900] Large Language Models Memorize Sensor Datasets! Implications on Human Activity Recognition Research <https://arxiv.org/abs/2406.05900>`__ 大型语言模型记住传感器数据集!对人类活动识别研究的启示

::

    Sun, 9 Jun 2024 19:38:27 GMT
    Harish Haresamudram, Hrudhai Rajasekhar, Nikhil Murlidhar Shanbhogue, Thomas Ploetz

The astonishing success of Large Language Models (LLMs) in Natural Language Processing (NLP) has spurred their use in many application domains beyond text analysis, including wearable sensor-based Human Activity Recognition (HAR). In such scenarios, often sensor data are directly fed into an LLM along with text instructions for the model to perform activity classification. Seemingly remarkable results have been reported for such LLM-based HAR systems when they are evaluated on standard benchmarks from the field. Yet, we argue, care has to be taken when evaluating LLM-based HAR systems in such a traditional way. Most contemporary LLMs are trained on virtually the entire (accessible) internet -- potentially including standard HAR datasets. With that, it is not unlikely that LLMs actually had access to the test data used in such benchmark experiments.The resulting contamination of training data would render these experimental evaluations meaningless. In this paper we investigate whether LLMs indeed have had access to standard HAR datasets during training. We apply memorization tests to LLMs, which involves instructing the models to extend given snippets of data. When comparing the LLM-generated output to the original data we found a non-negligible amount of matches which suggests that the LLM under investigation seems to indeed have seen wearable sensor data from the benchmark datasets during training. For the Daphnet dataset in particular, GPT-4 is able to reproduce blocks of sensor readings. We report on our investigations and discuss potential implications on HAR research, especially with regards to reporting results on experimental evaluation

------------

`[2406.05955] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters <https://arxiv.org/abs/2406.05955>`__ Turbo Sparse:以最小激活参数实现LLM SOTA性能

::

    Mon, 10 Jun 2024 01:21:59 GMT
    Yixin Song, Haotong Xie, Zhengyan Zhang, Bo Wen, Li Ma, Zeyu Mi, and Haibo Chen

Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance. However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity. Simply replacing these functions with ReLU fails to achieve sufficient sparsity. Moreover, inadequate training data can further increase the risk of performance degradation. To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification. Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to further boost efficiency. By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance. Evaluation results demonstrate that this sparsity achieves a 2-5x decoding speedup. Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second.
Our models are available at \url{https://huggingface.co/PowerInfer}

------------

`[2406.06282] PowerInfer-2: Fast Large Language Model Inference on a Smartphone <https://arxiv.org/abs/2406.06282>`__ powerinfer2:智能手机上的快速大型语言模型推理

::

    Mon, 10 Jun 2024 14:01:21 GMT
    Zhenliang Xue, Yixin Song, Zeyu Mi, Le Chen, Yubin Xia, Haibo Chen

This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM.
For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2.

------------

`[2406.06385] Low-Rank Quantization-Aware Training for LLMs <https://arxiv.org/abs/2406.06385>`__ llm的低秩量化感知训练

::

    Mon, 10 Jun 2024 15:44:22 GMT
    Yelysei Bondarenko, Riccardo Del Chiaro, Markus Nagel

Large language models (LLMs) are omnipresent, however their practical deployment is challenging due to their ever increasing computational and memory demands. Quantization is one of the most effective ways to make them more compute and memory efficient. Quantization-aware training (QAT) methods, generally produce the best quantized performance, however it comes at the cost of potentially long training time and excessive memory usage, making it impractical when applying for LLMs. Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a lightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several components to save memory without sacrificing predictive performance: (a) low-rank auxiliary weights that are aware of the quantization grid; (b) a downcasting operator using fixed-point or double-packed integers and (c) checkpointing. Unlike most related work, our method (i) is inference-efficient, leading to no additional overhead compared to traditional PTQ; (ii) can be seen as a general extended pretraining framework, meaning that the resulting model can still be utilized for any downstream task afterwards; (iii) can be applied across a wide range of quantization settings, such as different choices quantization granularity, activation quantization, and seamlessly combined with many PTQ techniques. We apply LR-QAT to the LLaMA-2/3 and Mistral model families and validate its effectiveness on several downstream tasks. Our method outperforms common post-training quantization (PTQ) approaches and reaches the same model performance as full-model QAT at the fraction of its memory usage.
Specifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of memory.

------------

`[2406.06443] LLM Dataset Inference: Did you train on my dataset? <https://arxiv.org/abs/2406.06443>`__ 

::

    Mon, 10 Jun 2024 16:34:43 GMT
    Pratyush Maini and Hengrui Jia and Nicolas Papernot and Adam Dziedzic

The proliferation of large language models (LLMs) in the real world has come with a rise in copyright cases against companies for training their models on unlicensed data from the internet. Recent works have presented methods to identify if individual text sequences were members of the model's training data, known as membership inference attacks (MIAs). We demonstrate that the apparent success of these MIAs is confounded by selecting non-members (text sequences not used for training) belonging to a different distribution from the members (e.g., temporally shifted recent Wikipedia articles compared with ones used to train the model). This distribution shift makes membership inference appear successful. However, most MIA methods perform no better than random guessing when discriminating between members and non-members from the same distribution (e.g., in this case, the same period of time). Even when MIAs work, we find that different MIAs succeed at inferring membership of samples from different distributions. Instead, we propose a new dataset inference method to accurately identify the datasets used to train large language models.
This paradigm sits realistically in the modern-day copyright landscape, where authors claim that an LLM is trained over multiple documents (such as a book) written by them, rather than one particular paragraph. While dataset inference shares many of the challenges of membership inference, we solve it by selectively combining the MIAs that provide positive signal for a given distribution, and aggregating them to perform a statistical test on a given dataset. Our approach successfully distinguishes the train and test sets of different subsets of the Pile with statistically significant p-values < 0.1, without any false positives.

------------

`[2406.06487] When is Multicalibration Post-Processing Necessary? <https://arxiv.org/abs/2406.06487>`__ 什么时候需要多校准后处理?

::

    Mon, 10 Jun 2024 17:26:39 GMT
    Dutch Hansen, Siddartha Devic, Preetum Nakkiran, Vatsal Sharan

Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates. Multicalibration is a related notion -- originating in algorithmic fairness -- which requires predictors to be simultaneously calibrated over a potentially complex and overlapping collection of protected subpopulations (such as groups defined by ethnicity, race, or income). We conduct the first comprehensive study evaluating the usefulness of multicalibration post-processing across a broad set of tabular, image, and language datasets for models spanning from simple decision trees to 90 million parameter fine-tuned LLMs. Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration post-processing can help inherently uncalibrated models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly. More generally, we also distill many independent observations which may be useful for practical and effective applications of multicalibration post-processing in real-world contexts.

------------

`[2406.05249] A Language Model-Guided Framework for Mining Time Series with Distributional Shifts <https://arxiv.org/abs/2406.05249>`__ 一种语言模型指导的分布偏移时间序列挖掘框架

::

    Fri, 7 Jun 2024 20:21:07 GMT
    Haibei Zhu, Yousef El-Laham, Elizabeth Fons, Svitlana Vyetrenko

Effective utilization of time series data is often constrained by the scarcity of data quantity that reflects complex dynamics, especially under the condition of distributional shifts. Existing datasets may not encompass the full range of statistical properties required for robust and comprehensive analysis. And privacy concerns can further limit their accessibility in domains such as finance and healthcare. This paper presents an approach that utilizes large language models and data source interfaces to explore and collect time series datasets. While obtained from external sources, the collected data share critical statistical properties with primary time series datasets, making it possible to model and adapt to various scenarios. This method enlarges the data quantity when the original data is limited or lacks essential properties. It suggests that collected datasets can effectively supplement existing datasets, especially involving changes in data distribution. We demonstrate the effectiveness of the collected datasets through practical examples and show how time series forecasting foundation models fine-tuned on these datasets achieve comparable performance to those models without fine-tuning.

------------

`[2406.05314] Relational Proxy Loss for Audio-Text based Keyword Spotting <https://arxiv.org/abs/2406.05314>`__ 基于音频-文本关键词识别的关系代理损失

::

    Sat, 8 Jun 2024 01:21:17 GMT
    Youngmoon Jung, Seungjin Lee, Joon-Young Yang, Jaeyoung Roh, Chang Woo Han, Hoon-Young Cho

In recent years, there has been an increasing focus on user convenience, leading to increased interest in text-based keyword enrollment systems for keyword spotting (KWS). Since the system utilizes text input during the enrollment phase and audio input during actual usage, we call this task audio-text based KWS. To enable this task, both acoustic and text encoders are typically trained using deep metric learning loss functions, such as triplet- and proxy-based losses. This study aims to improve existing methods by leveraging the structural relations within acoustic embeddings and within text embeddings. Unlike previous studies that only compare acoustic and text embeddings on a point-to-point basis, our approach focuses on the relational structures within the embedding space by introducing the concept of Relational Proxy Loss (RPL). By incorporating RPL, we demonstrated improved performance on the Wall Street Journal (WSJ) corpus.

------------

`[2406.05498] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner <https://arxiv.org/abs/2406.05498>`__ 自我防御:llm可以用一种实用的方式防御越狱

::

    Sat, 8 Jun 2024 15:45:31 GMT
    Xunguang Wang and Daoyuan Wu and Zhenlan Ji and Zongjie Li and Pingchuan Ma and Shuai Wang and Yingjiu Li and Yang Liu and Ning Liu and Juergen Rahmel

Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs) and has evolved into four major categories: optimization-based attacks such as Greedy Coordinate Gradient (GCG), jailbreak template-based attacks such as "Do-Anything-Now", advanced indirect attacks like DrAttack, and multilingual jailbreaks. However, delivering a practical jailbreak defense is challenging because it needs to not only handle all the above jailbreak attacks but also incur negligible delay to user prompts, as well as be compatible with both open-source and closed-source LLMs.
Inspired by how the traditional security concept of shadow stacks defends against memory overflow attacks, this paper introduces a generic LLM jailbreak defense framework called SelfDefend, which establishes a shadow LLM defense instance to concurrently protect the target LLM instance in the normal stack and collaborate with it for checkpoint-based access control. The effectiveness of SelfDefend builds upon our observation that existing LLMs (both target and defense LLMs) have the capability to identify harmful prompts or intentions in user queries, which we empirically validate using the commonly used GPT-3.5/4 models across all major jailbreak attacks. Our measurements show that SelfDefend enables GPT-3.5 to suppress the attack success rate (ASR) by 8.97-95.74% (average: 60%) and GPT-4 by even 36.36-100% (average: 83%), while incurring negligible effects on normal queries. To further improve the defense's robustness and minimize costs, we employ a data distillation approach to tune dedicated open-source defense models. These models outperform four SOTA defenses and match the performance of GPT-4-based SelfDefend, with significantly lower extra delays. We also empirically show that the tuned models are robust to targeted GCG and prompt injection attacks.

------------

`[2406.05543] VP-LLM: Text-Driven 3D Volume Completion with Large Language Models through Patchification <https://arxiv.org/abs/2406.05543>`__ VP-LLM:基于补丁的大型语言模型文本驱动3D体补全

::

    Sat, 8 Jun 2024 18:17:09 GMT
    Jianmeng Liu, Yichen Liu, Yuyao Zhang, Zeyuan Meng, Yu-Wing Tai, Chi-Keung Tang

Recent conditional 3D completion works have mainly relied on CLIP or BERT to encode textual information, which cannot support complex instruction.
Meanwhile, large language models (LLMs) have shown great potential in multi-modal understanding and generation tasks. Inspired by the recent advancements of LLM, we present Volume Patch LLM (VP-LLM), which leverages LLMs to perform conditional 3D completion in a single-forward pass. To integrate a 3D model into the LLM tokenization configuration, the incomplete 3D object is first divided into small patches that can be encoded independently. These encoded patches are then fed into an LLM along with the text prompt, instructing the LLM to capture the relations between these patches as well as injecting semantic meanings into the 3D object. Our results demonstrate a strong ability of LLMs to interpret complex text instructions and understand 3D objects, surpassing state-of-the-art diffusion-based 3D completion models in generation quality.

------------

`[2406.05572] Trust the PRoC3S: Solving Long-Horizon Robotics Problems with LLMs and Constraint Satisfaction <https://arxiv.org/abs/2406.05572>`__ 相信PRoC3S:用llm和约束满足解决长视距机器人问题

::

    Sat, 8 Jun 2024 20:56:14 GMT
    Aidan Curtis, Nishanth Kumar, Jing Cao, Tom\'as Lozano-P\'erez, Leslie Pack Kaelbling

Recent developments in pretrained large language models (LLMs) applied to robotics have demonstrated their capacity for sequencing a set of discrete skills to achieve open-ended goals in simple robotic tasks. In this paper, we examine the topic of LLM planning for a set of continuously parameterized skills whose execution must avoid violations of a set of kinematic, geometric, and physical constraints. We prompt the LLM to output code for a function with open parameters, which, together with environmental constraints, can be viewed as a Continuous Constraint Satisfaction Problem (CCSP). This CCSP can be solved through sampling or optimization to find a skill sequence and continuous parameter settings that achieve the goal while avoiding constraint violations.
Additionally, we consider cases where the LLM proposes unsatisfiable CCSPs, such as those that are kinematically infeasible, dynamically unstable, or lead to collisions, and re-prompt the LLM to form a new CCSP accordingly.
Experiments across three different simulated 3D domains demonstrate that our proposed strategy, PRoC3S, is capable of solving a wide range of complex manipulation tasks with realistic constraints on continuous parameters much more efficiently and effectively than existing baselines.

------------

`[2406.05603] A Knowledge-Component-Based Methodology for Evaluating AI Assistants <https://arxiv.org/abs/2406.05603>`__ 

::

    Sun, 9 Jun 2024 00:58:39 GMT
    Laryn Qi, J.D. Zamfirescu-Pereira, Taehan Kim, Bj\"orn Hartmann, John DeNero, Narges Norouzi

We evaluate an automatic hint generator for CS1 programming assignments powered by GPT-4, a large language model. This system provides natural language guidance about how students can improve their incorrect solutions to short programming exercises. A hint can be requested each time a student fails a test case. Our evaluation addresses three Research Questions: RQ1: Do the hints help students improve their code? RQ2: How effectively do the hints capture problems in student code? RQ3: Are the issues that students resolve the same as the issues addressed in the hints? To address these research questions quantitatively, we identified a set of fine-grained knowledge components and determined which ones apply to each exercise, incorrect solution, and generated hint. Comparing data from two large CS1 offerings, we found that access to the hints helps students to address problems with their code more quickly, that hints are able to consistently capture the most pressing errors in students' code, and that hints that address a few issues at once rather than a single bug are more likely to lead to direct student progress.

------------

`[2406.05839] MaLa-ASR: Multimedia-Assisted LLM-Based ASR <https://arxiv.org/abs/2406.05839>`__ MaLa-ASR:基于llm的多媒体辅助ASR

::

    Sun, 9 Jun 2024 16:00:00 GMT
    Guanrou Yang, Ziyang Ma, Fan Yu, Zhifu Gao, Shiliang Zhang, Xie Chen

As more and more information-rich data like video become available, utilizing multi-modal auxiliary information to enhance audio tasks has sparked widespread research interest. The recent surge in research on LLM-based audio models provides fresh perspectives for tackling audio tasks. Given that LLM can flexibly ingest multiple inputs, we propose MaLa-ASR, an LLM-based ASR model that can integrate textual keywords extracted from presentation slides to improve recognition of conference content. MaLa-ASR yields average WERs of 9.4% and 11.7% on the L95 and S95 subsets of the SlideSpeech corpus, representing a significant relative WER drop of 27.9% and 44.7% over the baseline model reported in SlideSpeech. MaLa-ASR underscores LLM's strong performance in speech tasks and the capability to integrate auxiliary information conveniently. By adding keywords to the input prompt, the biased word error rate (B-WER) reduces relatively by 46.0% and 44.2%, establishing a new SOTA on this dataset.

------------

`[2406.05946] Safety Alignment Should Be Made More Than Just a Few Tokens Deep <https://arxiv.org/abs/2406.05946>`__ 安全对齐应该不仅仅是深入几个标记

::

    Mon, 10 Jun 2024 00:35:23 GMT
    Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma, Subhrajit Roy, Ahmad Beirami, Prateek Mittal, Peter Henderson

The safety alignment of current Large Language Models (LLMs) is vulnerable.
Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue.
We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.

------------

`[2406.05948] Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models <https://arxiv.org/abs/2406.05948>`__ 审查链:检测大型语言模型的后门攻击

::

    Mon, 10 Jun 2024 00:53:25 GMT
    Xi Li, Yusen Zhang, Renze Lou, Chen Wu, Jiaqi Wang

Backdoor attacks present significant threats to Large Language Models (LLMs), particularly with the rise of third-party services that offer API integration and prompt engineering. Untrustworthy third parties can plant backdoors into LLMs and pose risks to users by embedding malicious instructions into user queries. The backdoor-compromised LLM will generate malicious output when and input is embedded with a specific trigger predetermined by an attacker.
Traditional defense strategies, which primarily involve model parameter fine-tuning and gradient calculation, are inadequate for LLMs due to their extensive computational and clean data requirements. In this paper, we propose a novel solution, Chain-of-Scrutiny (CoS), to address these challenges.
Backdoor attacks fundamentally create a shortcut from the trigger to the target output, thus lack reasoning support. Accordingly, CoS guides the LLMs to generate detailed reasoning steps for the input, then scrutinizes the reasoning process to ensure consistency with the final answer. Any inconsistency may indicate an attack. CoS only requires black-box access to LLM, offering a practical defense, particularly for API-accessible LLMs. It is user-friendly, enabling users to conduct the defense themselves. Driven by natural language, the entire defense process is transparent to users. We validate the effectiveness of CoS through extensive experiments across various tasks and LLMs. Additionally, experiments results shows CoS proves more beneficial for more powerful LLMs.

------------

`[2406.06400] An Empirical Design Justice Approach to Identifying Ethical Considerations in the Intersection of Large Language Models and Social Robotics <https://arxiv.org/abs/2406.06400>`__ 用经验设计正义方法在大型语言模型和社会机器人的交叉领域识别伦理考虑

::

    Mon, 10 Jun 2024 15:53:50 GMT
    Alva Markelius

The integration of Large Language Models (LLMs) in social robotics presents a unique set of ethical challenges and social impacts. This research is set out to identify ethical considerations that arise in the design and development of these two technologies in combination. Using LLMs for social robotics may provide benefits, such as enabling natural language open-domain dialogues.
However, the intersection of these two technologies also gives rise to ethical concerns related to misinformation, non-verbal cues, emotional disruption, and biases. The robot's physical social embodiment adds complexity, as ethical hazards associated with LLM-based Social AI, such as hallucinations and misinformation, can be exacerbated due to the effects of physical embodiment on social perception and communication. To address these challenges, this study employs an empirical design justice-based methodology, focusing on identifying socio-technical ethical considerations through a qualitative co-design and interaction study. The purpose of the study is to identify ethical considerations relevant to the process of co-design of, and interaction with a humanoid social robot as the interface of a LLM, and to evaluate how a design justice methodology can be used in the context of designing LLMs-based social robotics. The findings reveal a mapping of ethical considerations arising in four conceptual dimensions: interaction, co-design, terms of service and relationship and evaluates how a design justice approach can be used empirically in the intersection of LLMs and social robotics.

------------

`[2406.06465] AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction <https://arxiv.org/abs/2406.06465>`__ 辅助:用于指令引导视频预测的图像-视频扩散模型

::

    Mon, 10 Jun 2024 17:02:08 GMT
    Zhen Xing and Qi Dai and Zejia Weng and Zuxuan Wu and Yu-Gang Jiang

Text-guided video prediction (TVP) involves predicting the motion of future frames from the initial frame according to an instruction, which has wide applications in virtual reality, robotics, and content creation. Previous TVP methods make significant breakthroughs by adapting Stable Diffusion for this task. However, they struggle with frame consistency and temporal stability primarily due to the limited scale of video datasets. We observe that pretrained Image2Video diffusion models possess good priors for video dynamics but they lack textual control. Hence, transferring Image2Video models to leverage their video dynamic priors while injecting instruction control to generate controllable videos is both a meaningful and challenging task. To achieve this, we introduce the Multi-Modal Large Language Model (MLLM) to predict future video states based on initial frames and text instructions. More specifically, we design a dual query transformer (DQFormer) architecture, which integrates the instructions and frames into the conditional embeddings for future frame prediction. Additionally, we develop Long-Short Term Temporal Adapters and Spatial Adapters that can quickly transfer general video diffusion models to specific scenarios with minimal training costs. Experimental results show that our method significantly outperforms state-of-the-art techniques on four datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and UCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and SSv2 respectively, demonstrating its effectiveness in various domains. More examples can be found at our website https://chenhsing.github.io/AID.

------------

`[2406.05013] CHIQ: Contextual History Enhancement for Improving Query Rewriting in Conversational Search <https://arxiv.org/abs/2406.05013>`__ 

::

    Fri, 7 Jun 2024 15:23:53 GMT
    Fengran Mo, Abbas Ghaddar, Kelong Mao, Mehdi Rezagholizadeh, Boxing Chen, Qun Liu, Jian-Yun Nie

In this paper, we study how open-source large language models (LLMs) can be effectively deployed for improving query rewriting in conversational search, especially for ambiguous queries. We introduce CHIQ, a two-step method that leverages the capabilities of LLMs to resolve ambiguities in the conversation history before query rewriting. This approach contrasts with prior studies that predominantly use closed-source LLMs to directly generate search queries from conversation history. We demonstrate on five well-established benchmarks that CHIQ leads to state-of-the-art results across most settings, showing highly competitive performances with systems leveraging closed-source LLMs. Our study provides a first step towards leveraging open-source LLMs in conversational search, as a competitive alternative to the prevailing reliance on commercial LLMs. Data, models, and source code will be publicly available upon acceptance at https://github.com/fengranMark/CHIQ.

------------

`[2406.05651] A Superalignment Framework in Autonomous Driving with Large Language Models <https://arxiv.org/abs/2406.05651>`__ 基于大型语言模型的自动驾驶超对齐框架

::

    Sun, 9 Jun 2024 05:26:38 GMT
    Xiangrui Kong, Thomas Braunl, Marco Fahmi, Yue Wang

Over the last year, significant advancements have been made in the realms of large language models (LLMs) and multi-modal large language models (MLLMs), particularly in their application to autonomous driving. These models have showcased remarkable abilities in processing and interacting with complex information. In autonomous driving, LLMs and MLLMs are extensively used, requiring access to sensitive vehicle data such as precise locations, images, and road conditions. These data are transmitted to an LLM-based inference cloud for advanced analysis. However, concerns arise regarding data security, as the protection against data and privacy breaches primarily depends on the LLM's inherent security measures, without additional scrutiny or evaluation of the LLM's inference outputs. Despite its importance, the security aspect of LLMs in autonomous driving remains underexplored. Addressing this gap, our research introduces a novel security framework for autonomous vehicles, utilizing a multi-agent LLM approach. This framework is designed to safeguard sensitive information associated with autonomous vehicles from potential leaks, while also ensuring that LLM outputs adhere to driving regulations and align with human values. It includes mechanisms to filter out irrelevant queries and verify the safety and reliability of LLM outputs. Utilizing this framework, we evaluated the security, privacy, and cost aspects of eleven large language model-driven autonomous driving cues. Additionally, we performed QA tests on these driving prompts, which successfully demonstrated the framework's efficacy.

------------

`[2406.05968] Prompting Large Language Models with Audio for General-Purpose Speech Summarization <https://arxiv.org/abs/2406.05968>`__ 用音频激励大型语言模型进行通用语音摘要

::

    Mon, 10 Jun 2024 02:04:28 GMT
    Wonjune Kang, Deb Roy

In this work, we introduce a framework for speech summarization that leverages the processing and reasoning capabilities of large language models (LLMs). We propose an end-to-end system that combines an instruction-tuned LLM with an audio encoder that converts speech into token representations that the LLM can interpret. Using a dataset with paired speech-text data, the overall system is trained to generate consistent responses to prompts with the same semantic information regardless of the input modality. The resulting framework allows the LLM to process speech inputs in the same way as text, enabling speech summarization by simply prompting the LLM. Unlike prior approaches, our method is able to summarize spoken content from any arbitrary domain, and it can produce summaries in different styles by varying the LLM prompting strategy. Experiments demonstrate that our approach outperforms a cascade baseline of speech recognition followed by LLM text processing.

------------

`[2406.06025] RepoQA: Evaluating Long Context Code Understanding <https://arxiv.org/abs/2406.06025>`__ RepoQA:评估长上下文代码的理解能力

::

    Mon, 10 Jun 2024 05:15:30 GMT
    Jiawei Liu, Jia Le Tian, Vijay Daita, Yuxiang Wei, Yifeng Ding, Yuhan Katherine Wang, Jun Yang, Lingming Zhang

Recent advances have been improving the context windows of Large Language Models (LLMs). To quantify the real long-context capabilities of LLMs, evaluators such as the popular Needle in a Haystack have been developed to test LLMs over a large chunk of raw texts. While effective, current evaluations overlook the insight of how LLMs work with long-context code, i.e., repositories. To this end, we initiate the RepoQA benchmark to evaluate LLMs on long-context code understanding. Traditional needle testers ask LLMs to directly retrieve the answer from the context without necessary deep understanding. In RepoQA, we built our initial task, namely Searching Needle Function (SNF), which exercises LLMs to search functions given their natural-language description, i.e., LLMs cannot find the desired function if they cannot understand the description and code. RepoQA is multilingual and comprehensive: it includes 500 code search tasks gathered from 50 popular repositories across 5 modern programming languages. By evaluating 26 general and code-specific LLMs on RepoQA, we show (i) there is still a small gap between the best open and proprietary models; (ii) different models are good at different languages; and (iii) models may understand code better without comments.

------------

`[2406.06382] Diffusion-RPO: Aligning Diffusion Models through Relative Preference Optimization <https://arxiv.org/abs/2406.06382>`__ Diffusion- rpo:基于相对偏好优化的扩散模型对齐

::

    Mon, 10 Jun 2024 15:42:03 GMT
    Yi Gu, Zhendong Wang, Yueqin Yin, Yujia Xie, Mingyuan Zhou

Aligning large language models with human preferences has emerged as a critical focus in language modeling research. Yet, integrating preference learning into Text-to-Image (T2I) generative models is still relatively uncharted territory. The Diffusion-DPO technique made initial strides by employing pairwise preference learning in diffusion models tailored for specific text prompts. We introduce Diffusion-RPO, a new method designed to align diffusion-based T2I models with human preferences more effectively. This approach leverages both prompt-image pairs with identical prompts and those with semantically related content across various modalities. Furthermore, we have developed a new evaluation metric, style alignment, aimed at overcoming the challenges of high costs, low reproducibility, and limited interpretability prevalent in current evaluations of human preference alignment. Our findings demonstrate that Diffusion-RPO outperforms established methods such as Supervised Fine-Tuning and Diffusion-DPO in tuning Stable Diffusion versions 1.5 and XL-1.0, achieving superior results in both automated evaluations of human preferences and style alignment. Our code is available at https://github.com/yigu1008/Diffusion-RPO

------------

`[2406.05335] Critical Phase Transition in a Large Language Model <https://arxiv.org/abs/2406.05335>`__ 大型语言模型中的关键相变

::

    Sat, 8 Jun 2024 03:37:05 GMT
    Kai Nakaishi, Yoshihiko Nishikawa, Koji Hukushima

The performance of large language models (LLMs) strongly depends on the \textit{temperature} parameter. Empirically, at very low temperatures, LLMs generate sentences with clear repetitive structures, while at very high temperatures, generated sentences are often incomprehensible. In this study, using GPT-2, we numerically demonstrate that the difference between the two regimes is not just a smooth change but a phase transition with singular, divergent statistical quantities. Our extensive analysis shows that critical behaviors, such as a power-law decay of correlation in a text, emerge in the LLM at the transition temperature as well as in a natural language dataset. We also discuss that several statistical quantities characterizing the criticality should be useful to evaluate the performance of LLMs.

------------

`[2406.05596] Aligning Human Knowledge with Visual Concepts Towards Explainable Medical Image Classification <https://arxiv.org/abs/2406.05596>`__ 面向可解释医学图像分类的人类知识与视觉概念对齐

::

    Sat, 8 Jun 2024 23:23:28 GMT
    Yunhe Gao, Difei Gu, Mu Zhou, Dimitris Metaxas

Although explainability is essential in the clinical diagnosis, most deep learning models still function as black boxes without elucidating their decision-making process. In this study, we investigate the explainable model development that can mimic the decision-making process of human experts by fusing the domain knowledge of explicit diagnostic criteria. We introduce a simple yet effective framework, Explicd, towards Explainable language-informed criteria-based diagnosis. Explicd initiates its process by querying domain knowledge from either large language models (LLMs) or human experts to establish diagnostic criteria across various concept axes (e.g., color, shape, texture, or specific patterns of diseases). By leveraging a pretrained vision-language model, Explicd injects these criteria into the embedding space as knowledge anchors, thereby facilitating the learning of corresponding visual concepts within medical images. The final diagnostic outcome is determined based on the similarity scores between the encoded visual concepts and the textual criteria embeddings. Through extensive evaluation of five medical image classification benchmarks, Explicd has demonstrated its inherent explainability and extends to improve classification performance compared to traditional black-box models.

------------

`[2406.05709] TR2MTL: LLM based framework for Metric Temporal Logic Formalization of Traffic Rules <https://arxiv.org/abs/2406.05709>`__ TR2MTL:基于LLM的交通规则度量时序逻辑形式化框架

::

    Sun, 9 Jun 2024 09:55:04 GMT
    Kumar Manas, Stefan Zwicklbauer and Adrian Paschke

Traffic rules formalization is crucial for verifying the compliance and safety of autonomous vehicles (AVs). However, manual translation of natural language traffic rules as formal specification requires domain knowledge and logic expertise, which limits its adaptation. This paper introduces TR2MTL, a framework that employs large language models (LLMs) to automatically translate traffic rules (TR) into metric temporal logic (MTL). It is envisioned as a human-in-loop system for AV rule formalization. It utilizes a chain-of-thought in-context learning approach to guide the LLM in step-by-step translation and generating valid and grammatically correct MTL formulas. It can be extended to various forms of temporal logic and rules. We evaluated the framework on a challenging dataset of traffic rules we created from various sources and compared it against LLMs using different in-context learning methods. Results show that TR2MTL is domain-agnostic, achieving high accuracy and generalization capability even with a small dataset. Moreover, the method effectively predicts formulas with varying degrees of logical and semantic structure in unstructured traffic rules.

------------

`[2406.05741] Digital Business Model Analysis Using a Large Language Model <https://arxiv.org/abs/2406.05741>`__ 

::

    Sun, 9 Jun 2024 11:16:11 GMT
    Masahiro Watanabe and Naoshi Uchihira

Digital transformation (DX) has recently become a pressing issue for many companies as the latest digital technologies, such as artificial intelligence and the Internet of Things, can be easily utilized. However, devising new business models is not easy for compa-nies, though they can improve their operations through digital technologies. Thus, business model design support methods are needed by people who lack digital tech-nology expertise. In contrast, large language models (LLMs) represented by ChatGPT and natural language processing utilizing LLMs have been developed revolutionarily. A business model design support system that utilizes these technologies has great potential. However, research on this area is scant. Accordingly, this study proposes an LLM-based method for comparing and analyzing similar companies from different business do-mains as a first step toward business model design support utilizing LLMs. This method can support idea generation in digital business model design.

------------

`[2406.05892] Security Vulnerability Detection with Multitask Self-Instructed Fine-Tuning of Large Language Models <https://arxiv.org/abs/2406.05892>`__ 基于大型语言模型多任务自指导微调的安全漏洞检测

::

    Sun, 9 Jun 2024 19:18:05 GMT
    Aidan Z.H. Yang, Haoye Tian, He Ye, Ruben Martins, Claire Le Goues

Software security vulnerabilities allow attackers to perform malicious activities to disrupt software operations. Recent Transformer-based language models have significantly advanced vulnerability detection, surpassing the capabilities of static analysis based deep learning models. However, language models trained solely on code tokens do not capture either the explanation of vulnerability type or the data flow structure information of code, both of which are crucial for vulnerability detection. We propose a novel technique that integrates a multitask sequence-to-sequence LLM with pro-gram control flow graphs encoded as a graph neural network to achieve sequence-to-classification vulnerability detection. We introduce MSIVD, multitask self-instructed fine-tuning for vulnerability detection, inspired by chain-of-thought prompting and LLM self-instruction. Our experiments demonstrate that MSIVD achieves superior performance, outperforming the highest LLM-based vulnerability detector baseline (LineVul), with a F1 score of 0.92 on the BigVul dataset, and 0.48 on the PreciseBugs dataset. By training LLMs and GNNs simultaneously using a combination of code and explanatory metrics of a vulnerable program, MSIVD represents a promising direction for advancing LLM-based vulnerability detection that generalizes to unseen data. Based on our findings, we further discuss the necessity for new labelled security vulnerability datasets, as recent LLMs have seen or memorized prior datasets' held-out evaluation data.

------------

`[2401.08189] PRewrite: Prompt Rewriting with Reinforcement Learning <https://arxiv.org/abs/2401.08189>`__ PRewrite:使用强化学习提示重写

::

    replaced with revised version Mon, 10 Jun 2024 13:46:22 GMT
    Submission history From: Spurthi Amba Hombaiah [view email]
    [v1] Tue, 16 Jan 2024 08:04:50 UTC (175 KB)
    [v2] Fri, 16 Feb 2024 19:22:19 UTC (226 KB)
    [v3] Tue, 20 Feb 2024 14:26:06 UTC (226 KB)
    [v4] Mon, 10 Jun 2024 13:46:22 UTC (227 KB)
    Weize Kong and Spurthi Amba Hombaiah and Mingyang Zhang and Qiaozhu Mei and Michael Bendersky

Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion that can be time consuming, ineffective, and sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?
To address these problems, we investigate automated prompt engineering in this paper. Specifically, we propose PRewrite, an automated method to rewrite an under-optimized prompt to a more effective prompt. We instantiate the prompt rewriter using a LLM. The rewriter LLM is trained using reinforcement learning to optimize the performance on a given downstream task. We conduct experiments on diverse benchmark datasets, which demonstrates the effectiveness of PRewrite.

------------

`[2402.02392] DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models <https://arxiv.org/abs/2402.02392>`__ DeLLMa:基于大型语言模型的不确定性决策框架

::

    replaced with revised version Sun, 9 Jun 2024 05:04:13 GMT
    Submission history From: Ollie Liu [view email]
    [v1] Sun, 4 Feb 2024 08:11:45 UTC (3,950 KB)
    [v2] Sun, 9 Jun 2024 05:04:13 UTC (5,407 KB)
    Ollie Liu, Deqing Fu, Dani Yogatama, Willie Neiswanger

The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of decision-making under uncertainty. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide a rational and human-auditable decision-making process. We validate our framework on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods.

------------

`[2403.08802] Governance of Generative Artificial Intelligence for Companies <https://arxiv.org/abs/2403.08802>`__ 企业生成性人工智能的治理

::

    replaced with revised version Sun, 9 Jun 2024 19:48:05 GMT
    Submission history From: Johannes Schneider [view email]
    [v1] Mon, 5 Feb 2024 14:20:19 UTC (482 KB)
    [v2] Sun, 9 Jun 2024 19:48:05 UTC (495 KB)
    Johannes Schneider, Rene Abraham, Christian Meske

Generative Artificial Intelligence (GenAI), specifically large language models like ChatGPT, has swiftly entered organizations without adequate governance, posing both opportunities and risks. Despite extensive debates on GenAI's transformative nature and regulatory measures, limited research addresses organizational governance, encompassing technical and business perspectives. Our review paper fills this gap by surveying recent works with the purpose of developing a framework for GenAI governance within companies. This framework outlines the scope, objectives, and governance mechanisms tailored to harness business opportunities as well as mitigate risks associated with GenAI integration. Our research contributes a focused approach to GenAI governance, offering practical insights for companies navigating the challenges of GenAI adoption and highlighting research gaps.

------------

`[2403.17787] Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications <https://arxiv.org/abs/2403.17787>`__ 评估prompt工程的大型多模态模型与微调视觉transformer在基于图像的安全应用中的效果

::

    replaced with revised version Mon, 10 Jun 2024 10:07:24 GMT
    Submission history From: Fouad Trad [view email]
    [v1] Tue, 26 Mar 2024 15:20:49 UTC (2,366 KB)
    [v2] Mon, 10 Jun 2024 10:07:24 UTC (2,618 KB)
    Fouad Trad and Ali Chehab

The success of Large Language Models (LLMs) has led to a parallel rise in the development of Large Multimodal Models (LMMs), which have begun to transform a variety of applications. These sophisticated multimodal models are designed to interpret and analyze complex data by integrating multiple modalities such as text and images, thereby opening new avenues for a range of applications. This paper investigates the applicability and effectiveness of prompt-engineered LMMs that process both images and text, including models such as LLaVA, BakLLaVA, Moondream, Gemini-pro-vision, and GPT-4o, compared to fine-tuned Vision Transformer (ViT) models in addressing critical security challenges. We focus on two distinct security tasks: 1) a visually evident task of detecting simple triggers, such as small pixel variations in images that could be exploited to access potential backdoors in the models, and 2) a visually non-evident task of malware classification through visual representations. In the visually evident task, some LMMs, such as Gemini-pro-vision and GPT-4o, have demonstrated the potential to achieve good performance with careful prompt engineering, with GPT-4o achieving the highest accuracy and F1-score of 91.9\% and 91\%, respectively. However, the fine-tuned ViT models exhibit perfect performance in this task due to its simplicity. For the visually non-evident task, the results highlight a significant divergence in performance, with ViT models achieving F1-scores of 97.11\% in predicting 25 malware classes and 97.61\% in predicting 5 malware families, whereas LMMs showed suboptimal performance despite iterative prompt improvements. This study not only showcases the strengths and limitations of prompt-engineered LMMs in cybersecurity applications but also emphasizes the unmatched efficacy of fine-tuned ViT models for precise and dependable tasks.

------------

`[2305.12307] OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing <https://arxiv.org/abs/2305.12307>`__ OntoType:本体引导的预训练语言模型辅助的细粒度实体分类

::

    replaced with revised version Mon, 10 Jun 2024 01:12:48 GMT
    Submission history From: Tanay Komarlu [view email]
    [v1] Sun, 21 May 2023 00:32:37 UTC (1,458 KB)
    [v2] Mon, 10 Jun 2024 01:12:48 UTC (1,607 KB)
    Tanay Komarlu, Minhao Jiang, Xuan Wang, Jiawei Han

Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, fine-grained semantic types, is a basic but important task for knowledge extraction from unstructured text. FET has been studied extensively in natural language processing and typically relies on human-annotated corpora for training, which is costly and difficult to scale. Recent studies explore the utilization of pre-trained language models (PLMs) as a knowledge base to generate rich and context-aware weak supervision for FET. However, a PLM still requires direction and guidance to serve as a knowledge base as they often generate a mixture of rough and fine-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel annotation-free, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot fine-grained entity typing methods as well as a typical LLM method, ChatGPT. Our error analysis shows that refinement of the existing ontology structures will further improve fine-grained entity typing.

------------

`[2306.11879] Open-Domain Text Evaluation via Contrastive Distribution Methods <https://arxiv.org/abs/2306.11879>`__ 基于对比分布方法的开放域文本评价

::

    replaced with revised version Mon, 10 Jun 2024 00:44:32 GMT
    Submission history From: Sidi Lu [view email]
    [v1] Tue, 20 Jun 2023 20:37:54 UTC (2,259 KB)
    [v2] Fri, 3 May 2024 23:21:45 UTC (2,375 KB)
    [v3] Thu, 6 Jun 2024 21:24:17 UTC (2,375 KB)
    [v4] Mon, 10 Jun 2024 00:44:32 UTC (2,354 KB)
    Sidi Lu and Hongyi Liu and Asli Celikyilmaz and Tianlu Wang and Nanyun Peng

Recent advancements in open-domain text generation, driven by the power of large pre-trained language models (LLMs), have demonstrated remarkable performance. However, assessing these models' generation quality remains a challenge. In this paper, we introduce a novel method for evaluating open-domain text generation called Contrastive Distribution Methods (CDM). Leveraging the connection between increasing model parameters and enhanced LLM performance, CDM creates a mapping from the _contrast_ of two probabilistic distributions -- one known to be superior to the other -- to quality measures. We investigate CDM for open-domain text generation evaluation under two paradigms: 1) _Generative_ CDM, which harnesses the contrast of two language models' distributions to generate synthetic examples for training discriminator-based metrics; 2) _Discriminative_ CDM, which directly uses distribution disparities between two language models for evaluation. Our experiments on coherence evaluation for multi-turn dialogue and commonsense evaluation for controllable generation demonstrate CDM's superior correlate with human judgment than existing automatic evaluation metrics, highlighting the strong performance and generalizability of our approach.

------------

`[2307.12114] A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks <https://arxiv.org/abs/2307.12114>`__ 应用于临床和生物医学任务的指令微调大型语言模型的零样本和少样本研究

::

    replaced with revised version Sun, 9 Jun 2024 15:06:57 GMT
    Submission history From: Yanis Labrak [view email]
    [v1] Sat, 22 Jul 2023 15:58:17 UTC (1,423 KB)
    [v2] Sun, 28 Apr 2024 16:17:43 UTC (895 KB)
    [v3] Sun, 9 Jun 2024 15:06:57 UTC (895 KB)
    Yanis Labrak, Mickael Rouvier, Richard Dufour

We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.

------------

`[2309.17234] Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation <https://arxiv.org/abs/2309.17234>`__ 合作、竞争与恶意:llm利益相关者互动谈判

::

    replaced with revised version Mon, 10 Jun 2024 14:43:34 GMT
    Submission history From: Sahar Abdelnabi [view email]
    [v1] Fri, 29 Sep 2023 13:33:06 UTC (5,662 KB)
    [v2] Mon, 10 Jun 2024 14:43:34 UTC (7,776 KB)
    Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Sch\"onherr, Mario Fritz

There is an growing interest in using Large Language Models (LLMs) in multi-agent systems to tackle interactive real-world tasks that require effective collaboration and assessing complex situations. Yet, we still have a limited understanding of LLMs' communication and decision-making abilities in multi-agent setups. The fundamental task of negotiation spans many key features of communication, such as cooperation, competition, and manipulation potentials. Thus, we propose using scorable negotiation to evaluate LLMs. We create a testbed of complex multi-agent, multi-issue, and semantically rich negotiation games. To reach an agreement, agents must have strong arithmetic, inference, exploration, and planning capabilities while integrating them in a dynamic and multi-turn setup. We propose multiple metrics to rigorously quantify agents' performance and alignment with the assigned role. We provide procedures to create new games and increase games' difficulty to have an evolving benchmark. Importantly, we evaluate critical safety aspects such as the interaction dynamics between agents influenced by greedy and adversarial players. Our benchmark is highly challenging; GPT-3.5 and small models mostly fail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform.

------------

`[2310.14558] AlpaCare:Instruction-tuned Large Language Models for Medical Application <https://arxiv.org/abs/2310.14558>`__ AlpaCare:面向医学应用的指令调优大型语言模型

::

    replaced with revised version Mon, 10 Jun 2024 17:52:31 GMT
    Submission history From: Xinlu Zhang [view email]
    [v1] Mon, 23 Oct 2023 04:22:50 UTC (2,905 KB)
    [v2] Wed, 3 Apr 2024 21:36:08 UTC (4,928 KB)
    [v3] Mon, 13 May 2024 21:49:17 UTC (4,928 KB)
    [v4] Mon, 10 Jun 2024 17:52:31 UTC (4,862 KB)
    Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, Linda Ruth Petzold

Instruction-finetuning (IFT) has become crucial in aligning Large Language Models (LLMs) with diverse human needs and has shown great potential in medical applications. However, previous studies mainly fine-tune LLMs on biomedical datasets with limited diversity, which often rely on benchmarks or narrow task scopes, and hence significantly limit the effectiveness on their medical instruction-following ability and generalizability. To bridge this gap, we propose creating a diverse, machine-generated medical IFT dataset, MedInstruct-52k, using GPT-4 and ChatGPT with a high-quality expert-curated seed set. We then fine-tune LLaMA-series models on the dataset to develop AlpaCare. Despite using a smaller domain-specific dataset than previous medical LLMs, AlpaCare not only demonstrates superior performance on medical applications, with up to 38.1% absolute gain over best baselines in medical free-form instruction evaluations, but also achieves 6.7% absolute gains averaged over multiple general domain benchmarks. Human evaluation further shows that AlpaCare consistently outperforms best baselines in terms of both correctness and helpfulness. We offer public access to our data, model, and codebase in this https URL.

------------

`[2311.06503] Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering <https://arxiv.org/abs/2311.06503>`__ 特定领域问答中llm的知识偏好对齐

::

    replaced with revised version Mon, 10 Jun 2024 09:06:10 GMT
    Submission history From: Yichi Zhang [view email]
    [v1] Sat, 11 Nov 2023 07:56:40 UTC (8,687 KB)
    [v2] Thu, 18 Apr 2024 10:02:47 UTC (6,636 KB)
    [v3] Mon, 10 Jun 2024 09:06:10 UTC (6,636 KB)
    Yichi Zhang, Zhuo Chen, Yin Fang, Yanxi Lu, Fangming Li, Wen Zhang, Huajun Chen

Deploying large language models (LLMs) to real scenarios for domain-specific question answering (QA) is a key thrust for LLM applications, which poses numerous challenges, especially in ensuring that responses are both accommodating to user requirements and appropriately leveraging domain-specific knowledge bases. They are the two major difficulties for LLM application as vanilla fine-tuning falls short of addressing. Combining these requirements, we conceive of them as the requirement for the model's preference to be harmoniously aligned with humans'. Thus, we introduce Knowledgeable Preference AlignmenT (KnowPAT), which constructs two kinds of preference sets to tackle the two issues. Besides, we design a new alignment objective to align the LLM preference with different human preferences uniformly, aiming to optimize LLM performance in real-world, domain-specific QA settings. Adequate experiments and comprehensive comparisons with 15 baseline methods illustrate that our KnowPAT is a superior pipeline for real-scenario domain-specific QA with LLMs.

------------

`[2311.07430] Controlled Text Generation for Black-box Language Models via Score-based Progressive Editor <https://arxiv.org/abs/2311.07430>`__ 基于分数渐进式编辑器的黑盒语言模型受控文本生成

::

    replaced with revised version Sat, 8 Jun 2024 04:35:07 GMT
    Submission history From: Sangwon Yu [view email]
    [v1] Mon, 13 Nov 2023 16:03:23 UTC (252 KB)
    [v2] Sat, 8 Jun 2024 04:35:07 UTC (442 KB)
    Sangwon Yu, Changmin Lee, Hojin Lee, Sungroh Yoon

Controlled text generation is very important for the practical use of language models because it ensures that the produced text includes only the desired attributes from a specific domain or dataset. Existing methods, however, are inapplicable to black-box models or suffer a significant trade-off between controlling the generated text and maintaining its fluency. This paper introduces the Score-based Progressive Editor (ScoPE), a novel approach designed to overcome these issues. ScoPE modifies the context at the token level during the generation process of a backbone language model. This modification guides the subsequent text to naturally include the target attributes. To facilitate this process, ScoPE employs a training objective that maximizes a target score, thoroughly considering both the ability to guide the text and its fluency. Experimental results on diverse controlled generation tasks demonstrate that ScoPE can effectively regulate the attributes of the generated text while fully utilizing the capability of the backbone large language models. Our codes are available at \url{this https URL}.

------------

`[2312.12141] Neuron-Level Knowledge Attribution in Large Language Models <https://arxiv.org/abs/2312.12141>`__ 大型语言模型中的神经元级知识归因

::

    replaced with revised version Sun, 9 Jun 2024 20:03:02 GMT
    Submission history From: Zeping Yu [view email]
    [v1] Tue, 19 Dec 2023 13:23:18 UTC (904 KB)
    [v2] Tue, 30 Jan 2024 12:19:09 UTC (2,065 KB)
    [v3] Sun, 9 Jun 2024 20:03:02 UTC (8,425 KB)
    Zeping Yu, Sophia Ananiadou

Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons for different outputs. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify "value neurons" directly contributing to the final prediction, we introduce a static method for identifying "query neurons" which activate these "value neurons". Finally, we apply our methods to analyze the localization of six distinct types of knowledge across both attention and feed-forward network (FFN) layers. Our method and analysis are helpful for understanding the mechanisms of knowledge storage and set the stage for future research in knowledge editing. We will release our data and code on github.

------------

`[2401.03735] Language Models Know the Value of Numbers <https://arxiv.org/abs/2401.03735>`__ 语言模型知道数字的值

::

    replaced with revised version Sun, 9 Jun 2024 12:42:01 GMT
    Submission history From: Fangwei Zhu [view email]
    [v1] Mon, 8 Jan 2024 08:54:22 UTC (937 KB)
    [v2] Sun, 4 Feb 2024 05:26:41 UTC (275 KB)
    [v3] Sun, 9 Jun 2024 12:42:01 UTC (433 KB)
    Fangwei Zhu, Damai Dai, Zhifang Sui

Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under-explored. In this paper, we study a fundamental question: whether language models know the value of numbers, a basic element in math. To study the question, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of encoded number values in LLMs on different layers, and these values can be extracted via linear probes. Further experiments show that LLMs store their calculation results in a similar manner, and we can intervene the output via simple vector additions, proving the causal connection between encoded numbers and language model outputs. Our research provides evidence that LLMs know the value of numbers, thus offering insights for better exploring, designing, and utilizing numeric information in LLMs.

------------

`[2401.06468] Adapting Large Language Models for Document-Level Machine Translation <https://arxiv.org/abs/2401.06468>`__ 面向文档级机器翻译的大型语言模型适配

::

    replaced with revised version Sun, 9 Jun 2024 13:13:03 GMT
    Submission history From: Minghao Wu [view email]
    [v1] Fri, 12 Jan 2024 09:29:13 UTC (7,819 KB)
    [v2] Thu, 15 Feb 2024 09:35:37 UTC (7,824 KB)
    [v3] Sun, 9 Jun 2024 13:13:03 UTC (8,413 KB)
    Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George Foster, Gholamreza Haffari

Large language models (LLMs) have significantly advanced various natural language processing (NLP) tasks. Recent research indicates that moderately-sized LLMs often outperform larger ones after task-specific fine-tuning. This study focuses on adapting LLMs for document-level machine translation (DocMT) for specific language pairs. We first investigate the impact of prompt strategies on translation performance and then conduct extensive experiments using two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our results show that specialized models can sometimes surpass GPT-4 in translation performance but still face issues like off-target translation due to error propagation in decoding. We provide an in-depth analysis of these LLMs tailored for DocMT, examining translation errors, discourse phenomena, training strategies, the scaling law of parallel documents, recent test set evaluations, and zero-shot crosslingual transfer. Our findings highlight the strengths and limitations of LLM-based DocMT models and provide a foundation for future research.

------------

`[2401.07453] Model Editing at Scale leads to Gradual and Catastrophic Forgetting <https://arxiv.org/abs/2401.07453>`__ 大规模模型编辑会导致渐进和灾难性的遗忘

::

    replaced with revised version Mon, 10 Jun 2024 17:50:14 GMT
    Submission history From: Akshat Gupta [view email]
    [v1] Mon, 15 Jan 2024 03:57:15 UTC (1,497 KB)
    [v2] Tue, 20 Feb 2024 06:24:03 UTC (16,734 KB)
    [v3] Wed, 22 May 2024 17:46:22 UTC (16,738 KB)
    [v4] Mon, 10 Jun 2024 17:50:14 UTC (16,738 KB)
    Akshat Gupta, Anurag Rao, Gopala Anumanchipalli

Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgetting phase. Both gradual and catastrophic forgetting limit the usefulness of model editing methods at scale -- the former making model editing less effective as multiple edits are made to the model while the latter caps the scalability of such model editing methods. Our analysis also highlights other key limitations of ROME and MEMIT at scale. With our work, we push for the development and evaluation of model editing methods keeping scalability in mind.

------------

`[2401.13927] Adaptive Text Watermark for Large Language Models <https://arxiv.org/abs/2401.13927>`__ 面向大型语言模型的自适应文本水印

::

    replaced with revised version Sun, 9 Jun 2024 03:52:21 GMT
    Submission history From: Yepeng Liu [view email]
    [v1] Thu, 25 Jan 2024 03:57:12 UTC (4,276 KB)
    [v2] Sun, 9 Jun 2024 03:52:21 UTC (4,719 KB)
    Yepeng Liu, Yuheng Bu

The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive watermarking strategy to address this problem. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured using an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits in proportion based on the semantic embedding of previously generated text using a well designed semantic mapping model. Our experiments involving various LLMs demonstrate that our approach achieves comparable robustness performance to existing watermark methods. Additionally, the text generated by our method has perplexity comparable to that of \emph{un-watermarked} LLMs while maintaining security even under various attacks.

------------

`[2402.00530] Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning <https://arxiv.org/abs/2402.00530>`__ 超过滤:用于快速指令调整的弱到强数据过滤

::

    replaced with revised version Fri, 7 Jun 2024 20:28:36 GMT
    Submission history From: Ming Li [view email]
    [v1] Thu, 1 Feb 2024 11:57:53 UTC (2,226 KB)
    [v2] Fri, 7 Jun 2024 20:28:36 UTC (2,230 KB)
    Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, Tianyi Zhou

Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of our approach.

------------

`[2402.05602] AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers <https://arxiv.org/abs/2402.05602>`__ 

::

    replaced with revised version Mon, 10 Jun 2024 09:58:55 GMT
    Submission history From: Reduan Achtibat [view email]
    [v1] Thu, 8 Feb 2024 12:01:24 UTC (14,457 KB)
    [v2] Mon, 10 Jun 2024 09:58:55 UTC (6,894 KB)
    Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Aakriti Jain, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek

Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a single backward pass. Through extensive evaluations against existing methods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an LRP library at this https URL.

------------

`[2402.05699] Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation <https://arxiv.org/abs/2402.05699>`__ 基于单语社会场景仿真的大型语言模型自对齐

::

    replaced with revised version Sat, 8 Jun 2024 06:13:55 GMT
    Submission history From: Rui Ye [view email]
    [v1] Thu, 8 Feb 2024 14:21:03 UTC (1,167 KB)
    [v2] Thu, 29 Feb 2024 08:46:47 UTC (1,167 KB)
    [v3] Sat, 8 Jun 2024 06:13:55 UTC (2,385 KB)
    Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang, Siheng Chen

Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 benchmarks. As evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning with human values. See our project page at this https URL.

------------

`[2402.10110] Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning <https://arxiv.org/abs/2402.10110>`__ 选择性反射调优:学生选择的LLM指令调优数据回收

::

    replaced with revised version Fri, 7 Jun 2024 20:23:21 GMT
    Submission history From: Ming Li [view email]
    [v1] Thu, 15 Feb 2024 17:06:21 UTC (1,309 KB)
    [v2] Fri, 7 Jun 2024 20:23:21 UTC (1,311 KB)
    Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, Tianyi Zhou

Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without collecting brand-new data. We apply our method to Alpaca and WizardLM data and achieve much stronger and top-tier 7B and 13B LLMs.

------------

`[2402.10373] BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains <https://arxiv.org/abs/2402.10373>`__ BioMistral:面向医学领域的开源预训练大型语言模型集合

::

    replaced with revised version Sun, 9 Jun 2024 15:19:09 GMT
    Submission history From: Yanis Labrak [view email]
    [v1] Thu, 15 Feb 2024 23:39:04 UTC (7,101 KB)
    [v2] Sun, 9 Jun 2024 15:19:09 UTC (7,101 KB)
    Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, Richard Dufour

Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical LLMs, we automatically translated and evaluated this benchmark into 7 other languages. This marks the first large-scale multilingual evaluation of LLMs in the medical domain. Datasets, multilingual evaluation benchmarks, scripts, and all the models obtained during our experiments are freely released.

------------

`[2402.10614] Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements <https://arxiv.org/abs/2402.10614>`__ 法学硕士能代表不同的人吗?通过辩论对法学硕士进行调优，生成可控的争议语句

::

    replaced with revised version Fri, 7 Jun 2024 20:19:09 GMT
    Submission history From: Ming Li [view email]
    [v1] Fri, 16 Feb 2024 12:00:34 UTC (2,032 KB)
    [v2] Fri, 7 Jun 2024 20:19:09 UTC (2,035 KB)
    Ming Li, Jiuhai Chen, Lichang Chen, Tianyi Zhou

Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate & tuning (DEBATUNE) pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATUNE, we curate the largest dataset of debate topics so far, which covers 710 controversial topics and corresponding arguments for each topic. Evaluations by the GPT-4 judge with a novel controversy controllability metric show that LLMs' capability of generating diverse perspectives is significantly improved by DEBATUNE. Moreover, such controllability can be generalized to unseen topics, generating high-quality statements supporting controversial arguments.

------------

`[2402.11683] One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation <https://arxiv.org/abs/2402.11683>`__ 一个决定一切的提示:意见总结评估的LLMs

::

    replaced with revised version Sun, 9 Jun 2024 06:10:22 GMT
    Submission history From: Tejpalsingh Siledar [view email]
    [v1] Sun, 18 Feb 2024 19:13:52 UTC (9,632 KB)
    [v2] Sun, 9 Jun 2024 06:10:22 UTC (9,882 KB)
    Tejpalsingh Siledar, Swaroop Nath, Sankara Sri Raghava Ravindra Muddu, Rupasai Rangaraju, Swaprava Nath, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Sudhanshu Shekhar Singh, Muthusamy Chelliah, Nikesh Garera

Evaluation of opinion summaries using conventional reference-based metrics rarely provides a holistic evaluation and has been shown to have a relatively low correlation with human judgments. Recent studies suggest using Large Language Models (LLMs) as reference-free metrics for NLG evaluation, however, they remain unexplored for opinion summary evaluation. Moreover, limited opinion summary evaluation datasets inhibit progress. To address this, we release the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a dimension-independent prompt, and Op-Prompts, a dimension-dependent set of prompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries achieving an average Spearman correlation of 0.70 with humans, outperforming all previous approaches. To the best of our knowledge, we are the first to investigate LLMs as evaluators on both closed-source and open-source models in the opinion summarization domain.

------------

`[2402.11756] MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs <https://arxiv.org/abs/2402.11756>`__ MARS:生成式llm不确定性估计的意义感知响应评分

::

    replaced with revised version Sat, 8 Jun 2024 20:40:55 GMT
    Submission history From: Yavuz Faruk Bakman [view email]
    [v1] Mon, 19 Feb 2024 01:04:22 UTC (9,830 KB)
    [v2] Tue, 20 Feb 2024 02:12:09 UTC (9,830 KB)
    [v3] Sat, 8 Jun 2024 20:40:55 UTC (9,833 KB)
    Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, Salman Avestimehr

Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical QA dataset. Code can be found this https URL.

------------

`[2402.12195] Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion <https://arxiv.org/abs/2402.12195>`__ 浏览和集中:通过先验- llm上下文融合理解多模态内容

::

    replaced with revised version Sat, 8 Jun 2024 03:04:30 GMT
    Submission history From: Ziyue Wang [view email]
    [v1] Mon, 19 Feb 2024 14:59:07 UTC (10,925 KB)
    [v2] Sat, 8 Jun 2024 03:04:30 UTC (10,927 KB)
    Ziyue Wang, Chi Chen, Yiqi Zhu, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, Yang Liu

With the bloom of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) that incorporate LLMs with pre-trained vision models have recently demonstrated impressive performance across diverse vision-language tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth multimodal context fusion prior to feeding the features into LLMs. This paradigm initially "browses" through the inputs for essential insights, and then revisits the inputs to "concentrate" on crucial details, guided by these insights, to achieve a more comprehensive understanding of the multimodal inputs. Additionally, we develop training strategies specifically to enhance the understanding of multi-image inputs. Our method markedly boosts the performance on 7 multi-image scenarios, contributing to increments on average accuracy by 2.13% and 7.60% against strong MLLMs baselines with 3B and 11B LLMs, respectively.

------------

`[2402.12483] Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question? <https://arxiv.org/abs/2402.12483>`__ 工件或绑架:llm如何回答没有问题的多项选择题?

::

    replaced with revised version Fri, 7 Jun 2024 23:11:14 GMT
    Submission history From: Nishant Balepur [view email]
    [v1] Mon, 19 Feb 2024 19:38:58 UTC (9,370 KB)
    [v2] Fri, 7 Jun 2024 23:11:14 UTC (9,371 KB)
    Nishant Balepur, Abhilasha Ravichander, Rachel Rudinger

Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices. In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference. Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices. Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question. Inferring the original question is an impressive reasoning strategy, but it cannot fully explain the high choices-only accuracy of LLMs in MCQA. Thus, while LLMs are not fully incapable of reasoning in MCQA, we still advocate for the use of stronger baselines in MCQA benchmarks, the design of robust MCQA datasets for fair evaluations, and further efforts to explain LLM decision-making.

------------

`[2402.14860] Ranking Large Language Models without Ground Truth <https://arxiv.org/abs/2402.14860>`__ 没有基本事实的大型语言模型排名

::

    replaced with revised version Mon, 10 Jun 2024 16:25:30 GMT
    Submission history From: Amit Dhurandhar [view email]
    [v1] Wed, 21 Feb 2024 00:49:43 UTC (2,792 KB)
    [v2] Wed, 6 Mar 2024 20:10:11 UTC (2,843 KB)
    [v3] Wed, 5 Jun 2024 15:56:49 UTC (2,871 KB)
    [v4] Mon, 10 Jun 2024 16:25:30 UTC (2,871 KB)
    Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly and Karthikeyan Natesan Ramamurthy

Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover close to true rankings without reference data. This points to a viable low-resource mechanism for practical use.

------------

`[2403.00071] Resonance RoPE: Improving Context Length Generalization of Large Language Models <https://arxiv.org/abs/2403.00071>`__ 共振绳:改进大型语言模型的上下文长度泛化

::

    replaced with revised version Mon, 10 Jun 2024 13:30:34 GMT
    Submission history From: Suyuchen Wang [view email]
    [v1] Thu, 29 Feb 2024 19:02:03 UTC (845 KB)
    [v2] Mon, 10 Jun 2024 13:30:34 UTC (846 KB)
    Suyuchen Wang, Ivan Kobyzev, Peng Lu, Mehdi Rezagholizadeh, Bang Liu

This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position better and more robustly. Our extensive LLM experiments also show superior performance after applying Resonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on both upstream language modeling tasks and a variety of downstream long-text applications.

------------

`[2403.01509] Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics <https://arxiv.org/abs/2403.01509>`__ 奇妙的语义以及在哪里可以找到它们:研究生成式llm的哪些层反映了词汇语义

::

    replaced with revised version Sun, 9 Jun 2024 13:07:50 GMT
    Submission history From: Zhu Liu [view email]
    [v1] Sun, 3 Mar 2024 13:14:47 UTC (200 KB)
    [v2] Sun, 9 Jun 2024 13:07:50 UTC (120 KB)
    Zhu Liu, Cunliang Kong, Ying Liu and Maosong Sun

Large language models have achieved remarkable success in general language understanding tasks. However, as a family of generative methods with the objective of next token prediction, the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures. In this paper, we specifically investigate the bottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by probing its hidden states at the end of each layer using a contextualized word identification task. Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction. This is in contrast to models with discriminative objectives, such as mask language modeling, where the higher layers obtain better lexical semantics. The conclusion is further supported by the monotonic increase in performance via the hidden states for the last meaningless symbols, such as punctuation, in the prompting strategy. Our codes are available at this https URL.

------------

`[2403.04460] Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset <https://arxiv.org/abs/2403.04460>`__ Pearl:评论驱动的人物角色知识基础会话推荐数据集

::

    replaced with revised version Sat, 8 Jun 2024 17:40:14 GMT
    Submission history From: Minju Kim [view email]
    [v1] Thu, 7 Mar 2024 12:57:16 UTC (3,500 KB)
    [v2] Fri, 8 Mar 2024 04:54:31 UTC (3,839 KB)
    [v3] Fri, 5 Apr 2024 11:11:01 UTC (4,078 KB)
    [v4] Sat, 8 Jun 2024 17:40:14 UTC (6,854 KB)
    Minjin Kim, Minju Kim, Hana Kim, Beong-woo Kwak, Soyeon Chun, Hyunseo Kim, SeongKu Kang, Youngjae Yu, Jinyoung Yeo, Dongha Lee

Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relevant to the dialogue context than those in prior datasets.

------------

`[2403.06448] Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models <https://arxiv.org/abs/2403.06448>`__ 基于大型语言模型内部状态的无监督实时幻觉检测

::

    replaced with revised version Mon, 10 Jun 2024 05:48:30 GMT
    Submission history From: Weihang Su [view email]
    [v1] Mon, 11 Mar 2024 05:51:03 UTC (8,027 KB)
    [v2] Mon, 10 Jun 2024 05:48:30 UTC (8,028 KB)
    Weihang Su, Changyue Wang, Qingyao Ai, Yiran HU, Zhijing Wu, Yujia Zhou, Yiqun Liu

Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection.

------------

`[2403.07865] CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion <https://arxiv.org/abs/2403.07865>`__ 代码攻击:通过代码补全揭示大型语言模型的安全泛化挑战

::

    replaced with revised version Sun, 9 Jun 2024 15:04:34 GMT
    Submission history From: Qibing Ren [view email]
    [v1] Tue, 12 Mar 2024 17:55:38 UTC (8,284 KB)
    [v2] Thu, 14 Mar 2024 16:57:37 UTC (8,284 KB)
    [v3] Sun, 7 Apr 2024 15:39:24 UTC (8,288 KB)
    [v4] Sun, 9 Jun 2024 15:04:34 UTC (8,289 KB)
    Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, Lizhuang Ma

The rapid advancement of Large Language Models (LLMs) has brought about remarkable generative capabilities but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a new and universal safety vulnerability of these models against code input: CodeAttack bypasses the safety guardrails of all models more than 80\% of the time. We find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures. Furthermore, we give our hypotheses about the success of CodeAttack: the misaligned bias acquired by LLMs during code training, prioritizing code completion over avoiding the potential safety risk. Finally, we analyze potential mitigation measures. These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs.

------------

`[2403.11103] ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models <https://arxiv.org/abs/2403.11103>`__ ProgGen:用自反式大型语言模型逐步生成命名实体识别数据集

::

    replaced with revised version Sun, 9 Jun 2024 04:48:35 GMT
    Submission history From: Yuzhao Heng [view email]
    [v1] Sun, 17 Mar 2024 06:12:43 UTC (8,954 KB)
    [v2] Sun, 9 Jun 2024 04:48:35 UTC (8,979 KB)
    Yuzhao Heng, Chunyuan Deng, Yitong Li, Yue Yu, Yinghao Li, Rongzhi Zhang, Chao Zhang

Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being more cost-effective than existing alternatives.

------------

`[2403.13485] An Entropy-based Text Watermarking Detection Method <https://arxiv.org/abs/2403.13485>`__ 一种基于信息熵的文本水印检测方法

::

    replaced with revised version Sun, 9 Jun 2024 06:51:57 GMT
    Submission history From: Yijian Lu [view email]
    [v1] Wed, 20 Mar 2024 10:40:01 UTC (529 KB)
    [v2] Tue, 2 Apr 2024 12:31:27 UTC (530 KB)
    [v3] Tue, 4 Jun 2024 10:00:18 UTC (538 KB)
    [v4] Sun, 9 Jun 2024 06:51:57 UTC (742 KB)
    Yijian Lu, Aiwei Liu, Dianzhi Yu, Jingjing Li, Irwin King

Text watermarking algorithms for large language models (LLMs) can effectively identify machine-generated texts by embedding and detecting hidden features in the text. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we opine that the influence of token entropy should be fully considered in the watermark detection process, $i.e.$, the weight of each token during watermark detection should be customized according to its entropy, rather than setting the weights of all tokens to the same value as in previous methods. Specifically, we propose \textbf{E}ntropy-based Text \textbf{W}atermarking \textbf{D}etection (\textbf{EWD}) that gives higher-entropy tokens higher influence weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free and fully automated. From the experiments, we demonstrate that our EWD can achieve better detection performance in low-entropy scenarios, and our method is also general and can be applied to texts with different entropy distributions. Our code and data is available\footnote{\url{this https URL}}. Additionally, our algorithm could be accessed through MarkLLM \cite{pan2024markllm}\footnote{\url{this https URL}}.

------------

`[2404.10198] ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence <https://arxiv.org/abs/2404.10198>`__ 冲突:量化LLM内部先验和外部证据之间的拉锯战

::

    replaced with revised version Mon, 10 Jun 2024 04:44:57 GMT
    Submission history From: Eric Wu [view email]
    [v1] Tue, 16 Apr 2024 00:43:03 UTC (7,878 KB)
    [v2] Mon, 10 Jun 2024 04:44:57 UTC (7,528 KB)
    Kevin Wu and Eric Wu and James Zou

Retrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over 60% of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs -- namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect.

------------

`[2404.17287] When to Trust LLMs: Aligning Confidence with Response Quality <https://arxiv.org/abs/2404.17287>`__ 何时信任llm:将信任与响应质量相结合

::

    replaced with revised version Sun, 9 Jun 2024 04:54:46 GMT
    Submission history From: Shuchang Tao [view email]
    [v1] Fri, 26 Apr 2024 09:42:46 UTC (3,690 KB)
    [v2] Sun, 9 Jun 2024 04:54:46 UTC (5,457 KB)
    Shuchang Tao, Liuyi Yao, Hanxing Ding, Yuexiang Xie, Qi Cao, Fei Sun, Jinyang Gao, Huawei Shen, Bolin Ding

Despite the success of large language models (LLMs) in natural language generation, much evidence shows that LLMs may produce incorrect or nonsensical text. This limitation highlights the importance of discerning when to trust LLMs, especially in safety-critical domains. Existing methods often express reliability by confidence level, however, their effectiveness is limited by the lack of objective guidance. To address this, we propose CONfidence-Quality-ORDer-preserving alignment approach (CONQORD), which leverages reinforcement learning guided by a tailored dual-component reward function. This function integrates quality reward and order-preserving alignment reward functions. Specifically, the order-preserving reward incentivizes the model to verbalize greater confidence for responses of higher quality to align the order of confidence and quality. Experiments demonstrate that CONQORD significantly improves the alignment performance between confidence and response accuracy, without causing over-cautious. Furthermore, the aligned confidence provided by CONQORD informs when to trust LLMs, and acts as a determinant for initiating the retrieval process of external knowledge. Aligning confidence with response quality ensures more transparent and reliable responses, providing better trustworthiness.

------------

`[2404.18624] Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations? <https://arxiv.org/abs/2404.18624>`__ 视觉和语言解码器是否平等地使用图像和文本?他们的解释有多自洽?

::

    replaced with revised version Mon, 10 Jun 2024 10:43:20 GMT
    Submission history From: Letitia Parcalabescu [view email]
    [v1] Mon, 29 Apr 2024 11:52:20 UTC (14,925 KB)
    [v2] Mon, 10 Jun 2024 10:43:20 UTC (15,119 KB)
    Letitia Parcalabescu and Anette Frank

Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to predictions, they can also produce explanations, either in post-hoc or CoT settings. However, it is not clear how much they use the vision and text modalities when generating predictions or explanations. In this work, we investigate if VLMs rely on modalities differently when they produce explanations as opposed to providing answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing unimodal tests and measures to VLM decoders. We find that VLMs are less self-consistent than LLMs. Text contributions in VL decoders are more important than image contributions in all examined tasks. Moreover, the contributions of images are significantly stronger for explanation generation compared to answer generation. This difference is even larger in CoT compared to post-hoc explanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which before only covered VL encoders. We find that VL decoders still struggle with most phenomena tested by VALSE.

------------

`[2405.00715] Adapting Open-Source Large Language Models for Cost-Effective, Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning <https://arxiv.org/abs/2405.00715>`__ 采用基于策略强化学习的开源大型语言模型，用于具有成本效益的专家级临床记录生成

::

    replaced with revised version Mon, 10 Jun 2024 01:09:03 GMT
    Submission history From: Hanyin Wang [view email]
    [v1] Thu, 25 Apr 2024 15:34:53 UTC (3,060 KB)
    [v2] Wed, 5 Jun 2024 04:03:17 UTC (2,847 KB)
    [v3] Fri, 7 Jun 2024 00:59:52 UTC (2,847 KB)
    [v4] Mon, 10 Jun 2024 01:09:03 UTC (2,847 KB)
    Hanyin Wang, Chufan Gao, Bolun Liu, Qiping Xu, Guleid Hussein, Mohamad El Labban, Kingsley Iheasirim, Hariprasad Korsapati, Chuck Outcalt, Jimeng Sun

Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have demonstrated promising capabilities in clinical text summarization tasks. However, due to patient data privacy concerns and computational costs, many healthcare providers prefer using small, locally-hosted models over external generic LLMs. This study presents a comprehensive domain- and task-specific adaptation process for the open-source LLaMA-2 13 billion parameter model, enabling it to generate high-quality clinical notes from outpatient patient-doctor dialogues. Our process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced a new approach, DistillDirect, for performing on-policy reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting model, LLaMA-Clinic, can generate clinical notes comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as "acceptable" or higher across all three criteria: real-world readiness, completeness, and accuracy. In the more challenging "Assessment and Plan" section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than physician-authored notes (4.1/5). Our cost analysis for inference shows that our LLaMA-Clinic model achieves a 3.75-fold cost reduction compared to an external generic LLM service. Additionally, we highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format, rather than relying on LLMs to determine this for clinical practice. We have made our newly created synthetic clinic dialogue-note dataset and the physician feedback dataset publicly available to foster future research.

------------

`[2405.16282] Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models <https://arxiv.org/abs/2405.16282>`__ 幕后的信心:大型语言模型的信心概率对齐研究

::

    replaced with revised version Fri, 7 Jun 2024 22:48:31 GMT
    Submission history From: Ali Emami Dr. [view email]
    [v1] Sat, 25 May 2024 15:42:04 UTC (9,154 KB)
    [v2] Wed, 29 May 2024 13:05:16 UTC (9,152 KB)
    [v3] Mon, 3 Jun 2024 16:41:53 UTC (9,152 KB)
    [v4] Fri, 7 Jun 2024 22:48:31 UTC (2,558 KB)
    Abhishek Kumar, Robert Morabito, Sanzhar Umbet, Jad Kabbara, and Ali Emami

As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM's internal confidence, quantified by token probabilities, to the confidence conveyed in the model's response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model's confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI's GPT-4 showed the strongest confidence-probability alignment, with an average Spearman's $\hat{\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness.

------------

`[2405.16433] CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling <https://arxiv.org/abs/2405.16433>`__ CPsyCoun:基于报告的中文心理咨询多轮对话重构与评估框架

::

    replaced with revised version Mon, 10 Jun 2024 11:43:48 GMT
    Submission history From: Chenhao Zhang [view email]
    [v1] Sun, 26 May 2024 05:18:00 UTC (5,722 KB)
    [v2] Tue, 4 Jun 2024 16:25:25 UTC (5,722 KB)
    [v3] Mon, 10 Jun 2024 11:43:48 UTC (5,722 KB)
    Chenhao Zhang, Renhao Li, Minghuan Tan, Min Yang, Jingwei Zhu, Di Yang, Jiahao Zhao, Guancheng Ye, Chengming Li, Xiping Hu

Using large language models (LLMs) to assist psychological counseling is a significant but challenging task at present. Attempts have been made on improving empathetic conversations or acting as effective assistants in the treatment with LLMs. However, the existing datasets lack consulting knowledge, resulting in LLMs lacking professional consulting competence. Moreover, how to automatically evaluate multi-turn dialogues within the counseling process remains an understudied area. To bridge the gap, we propose CPsyCoun, a report-based multi-turn dialogue reconstruction and evaluation framework for Chinese psychological counseling. To fully exploit psychological counseling reports, a two-phase approach is devised to construct high-quality dialogues while a comprehensive evaluation benchmark is developed for the effective automatic evaluation of multi-turn psychological consultations. Competitive experimental results demonstrate the effectiveness of our proposed framework in psychological counseling. We open-source the datasets and model for future research at this https URL

------------

`[2405.20805] Multilingual Text Style Transfer: Datasets & Models for Indian Languages <https://arxiv.org/abs/2405.20805>`__ 多语言文本风格迁移:印度语言的数据集和模型

::

    replaced with revised version Sun, 9 Jun 2024 18:46:48 GMT
    Submission history From: Sourabrata Mukherjee [view email]
    [v1] Fri, 31 May 2024 14:05:27 UTC (2,201 KB)
    [v2] Sun, 9 Jun 2024 18:46:48 UTC (2,366 KB)
    Sourabrata Mukherjee, Atul Kr. Ojha, Akanksha Bansal, Deepak Alok, John P. McCrae, Ond\v{r}ej Du\v{s}ek

Text style transfer (TST) involves altering the linguistic style of a text while preserving its core content. This paper focuses on sentiment transfer, a vital TST subtask (Mukherjee et al., 2022a), across a spectrum of Indian languages: Hindi, Magahi, Malayalam, Marathi, Punjabi, Odia, Telugu, and Urdu, expanding upon previous work on English-Bangla sentiment transfer (Mukherjee et al., 2023). We introduce dedicated datasets of 1,000 positive and 1,000 negative style-parallel sentences for each of these eight languages. We then evaluate the performance of various benchmark models categorized into parallel, non-parallel, cross-lingual, and shared learning approaches, including the Llama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the significance of parallel data in TST and demonstrate the effectiveness of the Masked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel techniques. Moreover, cross-lingual and joint multilingual learning methods show promise, offering insights into selecting optimal models tailored to the specific language and task requirements. To the best of our knowledge, this work represents the first comprehensive exploration of the TST task as sentiment transfer across a diverse set of languages.

------------

`[2406.02528] Scalable MatMul-free Language Modeling <https://arxiv.org/abs/2406.02528>`__ 可扩展的无matmull语言建模

::

    replaced with revised version Mon, 10 Jun 2024 14:55:29 GMT
    Submission history From: Rui-Jie Zhu [view email]
    [v1] Tue, 4 Jun 2024 17:50:34 UTC (1,050 KB)
    [v2] Mon, 10 Jun 2024 14:55:29 UTC (1,051 KB)
    Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, Jason K. Eshraghian

Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at this https URL.

------------

`[2406.03897] HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew <https://arxiv.org/abs/2406.03897>`__ HeSum:希伯来语文本摘要的新数据集

::

    replaced with revised version Mon, 10 Jun 2024 05:45:25 GMT
    Submission history From: Tzuf Paz-Argaman [view email]
    [v1] Thu, 6 Jun 2024 09:36:14 UTC (53 KB)
    [v2] Mon, 10 Jun 2024 05:45:25 UTC (53 KB)
    Tzuf Paz-Argaman, Itai Mondshine, Asaf Achi Mordechai, and Reut Tsarfaty

While large language models (LLMs) excel in various natural language tasks in English, their performance in lower-resourced languages like Hebrew, especially for generative tasks such as abstractive summarization, remains unclear. The high morphological richness in Hebrew adds further challenges due to the ambiguity in sentence comprehension and the complexities in meaning construction. In this paper, we address this resource and evaluation gap by introducing HeSum, a novel benchmark specifically designed for abstractive text summarization in Modern Hebrew. HeSum consists of 10,000 article-summary pairs sourced from Hebrew news websites written by professionals. Linguistic analysis confirms HeSum's high abstractness and unique morphological challenges. We show that HeSum presents distinct difficulties for contemporary state-of-the-art LLMs, establishing it as a valuable testbed for generative language technology in Hebrew, and MRLs generative challenges in general.

------------

`[2406.04216] What Do Language Models Learn in Context? The Structured Task Hypothesis <https://arxiv.org/abs/2406.04216>`__ 语言模型在上下文中学习什么?结构化任务假说

::

    replaced with revised version Sat, 8 Jun 2024 11:59:08 GMT
    Submission history From: Yifan Hou [view email]
    [v1] Thu, 6 Jun 2024 16:15:34 UTC (269 KB)
    [v2] Sat, 8 Jun 2024 11:59:08 UTC (269 KB)
    Jiaoda Li, Yifan Hou, Mrinmaya Sachan, Ryan Cotterell

Large language models (LLMs) exhibit an intriguing ability to learn a novel task from in-context examples presented in a demonstration, termed in-context learning (ICL). Understandably, a swath of research has been dedicated to uncovering the theories underpinning ICL. One popular hypothesis explains ICL by task selection. LLMs identify the task based on the demonstration and generalize it to the prompt. Another popular hypothesis is that ICL is a form of meta-learning, i.e., the models learn a learning algorithm at pre-training time and apply it to the demonstration. Finally, a third hypothesis argues that LLMs use the demonstration to select a composition of tasks learned during pre-training to perform ICL. In this paper, we empirically explore these three hypotheses that explain LLMs' ability to learn in context with a suite of experiments derived from common text classification tasks. We invalidate the first two hypotheses with counterexamples and provide evidence in support of the last hypothesis. Our results suggest an LLM could learn a novel task in context via composing tasks learned during pre-training.

------------

`[2312.11875] Sparse is Enough in Fine-tuning Pre-trained Large Language Models <https://arxiv.org/abs/2312.11875>`__ 在微调预训练大型语言模型时，稀疏性就足够了

::

    replaced with revised version Sat, 8 Jun 2024 03:29:17 GMT
    Submission history From: Weixi Song [view email]
    [v1] Tue, 19 Dec 2023 06:06:30 UTC (915 KB)
    [v2] Thu, 2 May 2024 16:25:46 UTC (1,315 KB)
    [v3] Sat, 8 Jun 2024 03:29:17 UTC (1,318 KB)
    Weixi Song, Zuchao Li, Lefei Zhang, Hai Zhao, Bo Du

With the prevalence of pre-training-fine-tuning paradigm, how to efficiently adapt the pre-trained model to the downstream tasks has been an intriguing issue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for low-cost adaptation. Although PEFT has demonstrated effectiveness and been widely applied, the underlying principles are still unclear. In this paper, we adopt the PAC-Bayesian generalization error bound, viewing pre-training as a shift of prior distribution which leads to a tighter bound for generalization error. We validate this shift from the perspectives of oscillations in the loss landscape and the quasi-sparsity in gradient distribution. Based on this, we propose a gradient-based sparse fine-tuning algorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its effectiveness on a range of tasks including the GLUE Benchmark and Instruction-tuning. The code is accessible at this https URL.

------------

`[2401.06118] Extreme Compression of Large Language Models via Additive Quantization <https://arxiv.org/abs/2401.06118>`__ 基于加性量化的大型语言模型极端压缩

::

    replaced with revised version Sat, 8 Jun 2024 10:55:52 GMT
    Submission history From: Vage Egiazarian [view email]
    [v1] Thu, 11 Jan 2024 18:54:44 UTC (2,320 KB)
    [v2] Tue, 6 Feb 2024 18:55:25 UTC (2,993 KB)
    [v3] Sat, 8 Jun 2024 10:55:52 UTC (3,998 KB)
    Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, Dan Alistarh

The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of ``extreme'' LLM compression -- defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter -- from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic Additive Quantization (AQ) approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint.

------------

`[2402.03142] Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models <https://arxiv.org/abs/2402.03142>`__ Less is KEN:用于大型语言模型的通用简单非参数剪枝算法

::

    replaced with revised version Sun, 9 Jun 2024 10:32:03 GMT
    Submission history From: Michele Mastromattei [view email]
    [v1] Mon, 5 Feb 2024 16:11:43 UTC (27,113 KB)
    [v2] Sun, 9 Jun 2024 10:32:03 UTC (28,595 KB)
    Michele Mastromattei, Fabio Massimo Zanzotto

Neural network pruning has become increasingly crucial due to the complexity of these models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on demanding calculations, rendering them impractical for real-world applications. This paper introduces KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformers by selectively preserving the most significant parameters while restoring others to their pre-training state. This strategy preserves model performance while enabling storage of only the optimized subnetwork, leading to substantial memory savings. Extensive evaluations across seven different LLMs demonstrate that KEN achieves equal or better performance than their original unpruned versions, with a minimum parameter reduction of 25%. Furthermore, in-depth comparisons with established pruning and PEFT algorithms confirm KEN effectiveness. We further introduce KEN$_{viz}$, an explainable tool that visualizes the optimized model composition achieved by KEN from different points of view.

------------

`[2402.06255] Fight Back Against Jailbreaking via Prompt Adversarial Tuning <https://arxiv.org/abs/2402.06255>`__ 通过提示对抗性调整来反击越狱

::

    replaced with revised version Sun, 9 Jun 2024 16:18:46 GMT
    Submission history From: Yichuan Mo [view email]
    [v1] Fri, 9 Feb 2024 09:09:39 UTC (211 KB)
    [v2] Sun, 9 Jun 2024 16:18:46 UTC (2,342 KB)
    Yichuan Mo, Yuji Wang, Zeming Wei, Yisen Wang

While Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to jailbreak attacks. Several primary defense strategies have been proposed to protect LLMs from producing harmful information, mostly with a particular focus on harmful content filtering or heuristical defensive prompt designs. However, how to achieve intrinsic robustness through the prompts remains an open problem. In this paper, motivated by adversarial training paradigms for achieving reliable robustness, we propose an approach named Prompt Adversarial Tuning (PAT) that trains a prompt control attached to the user prompt as a guard prefix. To achieve our defense goal whilst maintaining natural performance, we optimize the control prompt with both adversarial and benign prompts. Comprehensive experiments show that our method is effective against both black-box and white-box attacks, reducing the success rate of advanced attacks to nearly 0 while maintaining the model's utility on the benign task. The proposed defense strategy incurs only negligible computational overhead, charting a new perspective for future explorations in LLM security. Our code is available at this https URL.

------------

`[2402.14760] Generalizing Reward Modeling for Out-of-Distribution Preference Learning <https://arxiv.org/abs/2402.14760>`__ 分布外偏好学习的泛化奖励模型

::

    replaced with revised version Sat, 8 Jun 2024 16:10:45 GMT
    Submission history From: Chen Jia [view email]
    [v1] Thu, 22 Feb 2024 18:20:33 UTC (1,441 KB)
    [v2] Sat, 8 Jun 2024 16:10:45 UTC (2,461 KB)
    Chen Jia

Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model for PL. We theoretically demonstrate the convergence rate of the bilevel optimization algorithm under reasonable assumptions. Additionally, we conduct experiments on two text generation tasks across 20 held-out domains and outperform a variety of strong baselines across various evaluation metrics.

------------

`[2403.13213] From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards <https://arxiv.org/abs/2403.13213>`__ 从代表性危害到服务质量危害:Llama 2安全保障案例研究

::

    replaced with revised version Sat, 8 Jun 2024 01:58:20 GMT
    Submission history From: Khaoula Chehbouni [view email]
    [v1] Wed, 20 Mar 2024 00:22:38 UTC (1,069 KB)
    [v2] Thu, 21 Mar 2024 02:27:57 UTC (1,069 KB)
    [v3] Sat, 8 Jun 2024 01:58:20 UTC (7,934 KB)
    Khaoula Chehbouni, Megha Roshan, Emmanuel Ma, Futian Andrew Wei, Afaf Taik, Jackie CK Cheung, Golnoosh Farnadi

Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluating models on already mitigated biases. Using the case of Llama 2 as an example, we illustrate how LLMs' safety responses can still encode harmful assumptions. To do so, we create a set of non-toxic prompts, which we then use to evaluate Llama models. Through our new taxonomy of LLMs responses to users, we observe that the safety/helpfulness trade-offs are more pronounced for certain demographic groups which can lead to quality-of-service harms for marginalized populations.

------------

`[2405.15362] Pipeline Parallelism with Controllable Memory <https://arxiv.org/abs/2405.15362>`__ 具有可控内存的流水线并行

::

    replaced with revised version Mon, 10 Jun 2024 11:24:06 GMT
    Submission history From: Penghui Qi [view email]
    [v1] Fri, 24 May 2024 08:54:36 UTC (1,466 KB)
    [v2] Wed, 5 Jun 2024 08:19:02 UTC (1,467 KB)
    [v3] Mon, 10 Jun 2024 11:24:06 UTC (1,153 KB)
    Penghui Qi, Xinyi Wan, Nyamdavaa Amar, Min Lin

Pipeline parallelism has been widely explored, but most existing schedules lack a systematic methodology. In this paper, we propose a framework to decompose pipeline schedules as repeating a building block and we show that the lifespan of the building block decides the peak activation memory of the pipeline schedule. Guided by the observations, we find that almost all existing pipeline schedules, to the best of our knowledge, are memory inefficient. To address this, we introduce a family of memory efficient building blocks with controllable activation memory, which can reduce the peak activation memory to 1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable throughput. We can also achieve almost zero pipeline bubbles while maintaining the same activation memory as 1F1B. Our evaluations demonstrate that in pure pipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in terms of throughput. When employing a grid search over hybrid parallelism hyperparameters in practical scenarios, our proposed methods demonstrate a 16% throughput improvement over the 1F1B baseline for large language models.

------------

`[2406.01539] Physics-informed deep learning and compressive collocation for high-dimensional diffusion-reaction equations: practical existence theory and numerics <https://arxiv.org/abs/2406.01539>`__ 基于物理学的深度学习与压缩配置的高维扩散-反应方程:实践存在理论与数值

::

    replaced with revised version Mon, 10 Jun 2024 17:22:08 GMT
    Submission history From: Simone Brugiapaglia [view email]
    [v1] Mon, 3 Jun 2024 17:16:11 UTC (3,224 KB)
    [v2] Mon, 10 Jun 2024 17:22:08 UTC (3,231 KB)
    Simone Brugiapaglia, Nick Dexter, Samir Karam and Weiqi Wang

On the forefront of scientific computing, Deep Learning (DL), i.e., machine learning with Deep Neural Networks (DNNs), has emerged a powerful new tool for solving Partial Differential Equations (PDEs). It has been observed that DNNs are particularly well suited to weakening the effect of the curse of dimensionality, a term coined by Richard E. Bellman in the late `50s to describe challenges such as the exponential dependence of the sample complexity, i.e., the number of samples required to solve an approximation problem, on the dimension of the ambient space. However, although DNNs have been used to solve PDEs since the `90s, the literature underpinning their mathematical efficiency in terms of numerical analysis (i.e., stability, accuracy, and sample complexity), is only recently beginning to emerge. In this paper, we leverage recent advancements in function approximation using sparsity-based techniques and random sampling to develop and analyze an efficient high-dimensional PDE solver based on DL. We show, both theoretically and numerically, that it can compete with a novel stable and accurate compressive spectral collocation method. In particular, we demonstrate a new practical existence theorem, which establishes the existence of a class of trainable DNNs with suitable bounds on the network architecture and a sufficient condition on the sample complexity, with logarithmic or, at worst, linear scaling in dimension, such that the resulting networks stably and accurately approximate a diffusion-reaction PDE with high probability.

------------

`[2406.02616] Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A Model-Based Reinforcement Learning Approach <https://arxiv.org/abs/2406.02616>`__ 边缘计算中无线LLM推理的自适应层分裂:基于模型的强化学习方法

::

    replaced with revised version Sat, 8 Jun 2024 08:41:32 GMT
    Submission history From: Yuxuan Chen [view email]
    [v1] Mon, 3 Jun 2024 09:41:42 UTC (2,382 KB)
    [v2] Thu, 6 Jun 2024 09:07:59 UTC (2,393 KB)
    [v3] Sat, 8 Jun 2024 08:41:32 UTC (2,393 KB)
    Yuxuan Chen, Rongpeng Li, Xiaoxue Yu, Zhifeng Zhao, and Honggang Zhang

Optimizing the deployment of large language models (LLMs) in edge computing environments is critical for enhancing privacy and computational efficiency. Toward efficient wireless LLM inference in edge computing, this study comprehensively analyzes the impact of different splitting points in mainstream open-source LLMs. On this basis, this study introduces a framework taking inspiration from model-based reinforcement learning (MBRL) to determine the optimal splitting point across the edge and user equipment (UE). By incorporating a reward surrogate model, our approach significantly reduces the computational cost of frequent performance evaluations. Extensive simulations demonstrate that this method effectively balances inference performance and computational load under varying network conditions, providing a robust solution for LLM deployment in decentralized settings.

------------

`[2406.02806] Randomized Geometric Algebra Methods for Convex Neural Networks <https://arxiv.org/abs/2406.02806>`__ 凸神经网络的随机几何代数方法

::

    replaced with revised version Sat, 8 Jun 2024 20:35:12 GMT
    Submission history From: Yifei Wang [view email]
    [v1] Tue, 4 Jun 2024 22:22:39 UTC (4,479 KB)
    [v2] Sat, 8 Jun 2024 20:35:12 UTC (4,483 KB)
    Yifei Wang, Sungyoon Kim, Paul Chu, Indu Subramaniam, Mert Pilanci

We introduce randomized algorithms to Clifford's Geometric Algebra, generalizing randomized linear algebra to hypercomplex vector spaces. This novel approach has many implications in machine learning, including training neural networks to global optimality via convex optimization. Additionally, we consider fine-tuning large language model (LLM) embeddings as a key application area, exploring the intersection of geometric algebra and modern AI techniques. In particular, we conduct a comparative analysis of the robustness of transfer learning via embeddings, such as OpenAI GPT models and BERT, using traditional methods versus our novel approach based on convex optimization. We test our convex optimization transfer learning method across a variety of case studies, employing different embeddings (GPT-4 and BERT embeddings) and different text classification datasets (IMDb, Amazon Polarity Dataset, and GLUE) with a range of hyperparameter settings. Our results demonstrate that convex optimization and geometric algebra not only enhances the performance of LLMs but also offers a more stable and reliable method of transfer learning via embeddings.

------------

`[2312.00027] Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections <https://arxiv.org/abs/2312.00027>`__ 通过后门注入对大型语言模型进行隐形和持续的不对齐

::

    replaced with revised version Sun, 9 Jun 2024 03:27:40 GMT
    Submission history From: Yuanpu Cao [view email]
    [v1] Wed, 15 Nov 2023 23:52:05 UTC (7,764 KB)
    [v2] Sun, 9 Jun 2024 03:27:40 UTC (7,825 KB)
    Yuanpu Cao, Bochuan Cao, Jinghui Chen

Recent developments in Large Language Models (LLMs) have manifested significant advancements. To facilitate safeguards against malicious exploitation, a body of research has concentrated on aligning LLMs with human preferences and inhibiting their generation of inappropriate content. Unfortunately, such alignments are often vulnerable: fine-tuning with a minimal amount of harmful data can easily unalign the target LLM. While being effective, such fine-tuning-based unalignment approaches also have their own limitations: (1) non-stealthiness, after fine-tuning, safety audits or red-teaming can easily expose the potential weaknesses of the unaligned models, thereby precluding their release/use. (2) non-persistence, the unaligned LLMs can be easily repaired through re-alignment, i.e., fine-tuning again with aligned data points. In this work, we show that it is possible to conduct stealthy and persistent unalignment on large language models via backdoor injections. We also provide a novel understanding on the relationship between the backdoor persistence and the activation pattern and further provide guidelines for potential trigger design. Through extensive experiments, we demonstrate that our proposed stealthy and persistent unalignment can successfully pass the safety evaluation while maintaining strong persistence against re-alignment defense.

------------

`[2401.13201] MLLMReID: Multimodal Large Language Model-based Person Re-identification <https://arxiv.org/abs/2401.13201>`__ MLLMReID:基于多模态大型语言模型的行人再识别

::

    replaced with revised version Mon, 10 Jun 2024 10:21:19 GMT
    Submission history From: Shan Yang [view email]
    [v1] Wed, 24 Jan 2024 03:07:26 UTC (5,786 KB)
    [v2] Wed, 3 Apr 2024 03:52:44 UTC (8,513 KB)
    [v3] Mon, 10 Jun 2024 10:21:19 UTC (8,378 KB)
    Shan Yang, Yongfei Zhang

Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of ReID (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) When fine-tuning the visual encoder of a MLLM, it is not trained synchronously with the ReID task. As a result, the effectiveness of the visual encoder fine-tuning cannot be directly reflected in the performance of the ReID task. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instruction, a simple approach that leverages the essence ability of LLMs to continue writing, avoiding complex and diverse instruction design. Secondly, we propose a multi-task learning-based synchronization module to ensure that the visual encoder of the MLLM is trained synchronously with the ReID task. The experimental results demonstrate the superiority of our method.

------------

`[2401.16310] Security Code Review by Large Language Models <https://arxiv.org/abs/2401.16310>`__ 

::

    replaced with revised version Sat, 8 Jun 2024 15:28:13 GMT
    Submission history From: Peng Liang [view email]
    [v1] Mon, 29 Jan 2024 17:13:44 UTC (153 KB)
    [v2] Sat, 8 Jun 2024 15:28:13 UTC (427 KB)
    Jiaxin Yu, Peng Liang, Yujia Fu, Amjed Tahir, Mojtaba Shahin, Chong Wang, Yangxiao Cai

Security code review, as a time-consuming and labour-intensive process, typically requires integration with automated security defect detection tools to ensure code security. Despite the emergence of numerous security analysis tools, those tools face challenges in terms of their poor generalization, high false positive rates, and coarse detection granularity. A recent development with Large Language Models (LLMs) has made them a promising candidate to support security code review. To this end, we conducted the first empirical study to understand the capabilities of LLMs in security code review, delving into the performance, quality problems, and influential factors of LLMs to detect security defects in code reviews. Specifically, we compared the performance of 6 LLMs under five different prompts with the state-of-the-art static analysis tools to detect and analyze security defects. For the best-performing LLM, we conducted a linguistic analysis to explore quality problems in its responses, as well as a regression analysis to investigate the factors influencing its performance. The results are that: (1) existing pre-trained LLMs have limited capability in detecting security defects during code review but significantly outperform the state-of-the-art static analysis tools. (2) GPT-4 performs best among all LLMs when provided with a CWE list for reference. (3) GPT-4 makes few factual errors but frequently generates unnecessary content or responses that are not compliant with the task requirements given in the prompts. (4) GPT-4 is more adept at identifying security defects in code files with fewer tokens, containing functional logic and written by developers with less involvement in the project.

------------

`[2402.09508] Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls <https://arxiv.org/abs/2402.09508>`__ 编排，修复和完善:通过基于内容的控件操纵长期音乐音频生成和编辑

::

    replaced with revised version Mon, 10 Jun 2024 14:08:17 GMT
    Submission history From: Liwei Lin [view email]
    [v1] Wed, 14 Feb 2024 19:00:01 UTC (7,084 KB)
    [v2] Mon, 10 Jun 2024 14:08:17 UTC (7,394 KB)
    Liwei Lin, Gus Xia, Yixiao Zhang, Junyan Jiang

Controllable music generation plays a vital role in human-AI music co-creation. While Large Language Models (LLMs) have shown promise in generating high-quality music, their focus on autoregressive generation limits their utility in music editing tasks. To address this gap, we propose a novel approach leveraging a parameter-efficient heterogeneous adapter combined with a masking training scheme. This approach enables autoregressive language models to seamlessly address music inpainting tasks. Additionally, our method integrates frame-level content-based controls, facilitating track-conditioned music refinement and score-conditioned music arrangement. We apply this method to fine-tune MusicGen, a leading autoregressive music generation model. Our experiments demonstrate promising results across multiple music editing tasks, offering more flexible controls for future AI-driven music editing tools. The source codes and a demo page showcasing our work are available at this https URL.

------------

`[2402.18104] Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction <https://arxiv.org/abs/2402.18104>`__ 让它们问与答:通过伪装和重构在少量查询中越狱大型语言模型

::

    replaced with revised version Mon, 10 Jun 2024 11:20:43 GMT
    Submission history From: Tong Liu [view email]
    [v1] Wed, 28 Feb 2024 06:50:14 UTC (4,185 KB)
    [v2] Mon, 10 Jun 2024 11:20:43 UTC (4,527 KB)
    Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, Kai Chen

In recent years, large language models (LLMs) have demonstrated notable success across various tasks, but the trustworthiness of LLMs is still an open problem. One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs. In this work, we pioneer a theoretical foundation in LLMs security by identifying bias vulnerabilities within the safety fine-tuning and design a black-box jailbreak method named DRA (Disguise and Reconstruction Attack), which conceals harmful instructions through disguise and prompts the model to reconstruct the original harmful instruction within its completion. We evaluate DRA across various open-source and closed-source models, showcasing state-of-the-art jailbreak success rates and attack efficiency. Notably, DRA boasts a 91.1% attack success rate on OpenAI GPT-4 chatbot.

------------

`[2403.12388] Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models <https://arxiv.org/abs/2403.12388>`__ 具有大型语言模型的会话系统的可解释用户满意度估计

::

    replaced with revised version Sun, 9 Jun 2024 00:58:25 GMT
    Submission history From: Ying-Chun Lin [view email]
    [v1] Tue, 19 Mar 2024 02:57:07 UTC (2,798 KB)
    [v2] Sun, 9 Jun 2024 00:58:25 UTC (3,503 KB)
    Ying-Chun Lin, Jennifer Neville, Jack W. Stokes, Longqi Yang, Tara Safavi, Mengting Wan, Scott Counts, Siddharth Suri, Reid Andersen, Xiaofeng Xu, Deepak Gupta, Sujay Kumar Jauhar, Xia Song, Georg Buscher, Saurabh Tiwary, Brent Hecht, Jaime Teevan

Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. The resulting method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.

------------

`[2404.00166] Uncovering Bias in Large Vision-Language Models with Counterfactuals <https://arxiv.org/abs/2404.00166>`__ 用反事实揭示大型视觉-语言模型中的偏见

::

    replaced with revised version Fri, 7 Jun 2024 23:29:19 GMT
    Submission history From: Phillip Howard [view email]
    [v1] Fri, 29 Mar 2024 21:45:53 UTC (19,855 KB)
    [v2] Fri, 7 Jun 2024 23:29:19 UTC (17,457 KB)
    Phillip Howard, Anahita Bhiwandiwalla, Kathleen C. Fraser, Svetlana Kiritchenko

With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images. Specifically, we present LVLMs with identical open-ended text prompts while conditioning on images from different counterfactual sets, where each set contains images which are largely identical in their depiction of a common subject (e.g., a doctor), but vary only in terms of intersectional social attributes (e.g., race and gender). We comprehensively evaluate the text produced by different LVLMs under this counterfactual generation setting and find that social attributes such as race, gender, and physical characteristics depicted in input images can significantly influence toxicity and the generation of competency-associated words.

------------

`[2404.02905] Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction <https://arxiv.org/abs/2404.02905>`__ 视觉自回归建模:基于下一尺度预测的可扩展图像生成

::

    replaced with revised version Mon, 10 Jun 2024 17:59:07 GMT
    Submission history From: Keyu Tian [view email]
    [v1] Wed, 3 Apr 2024 17:59:53 UTC (5,611 KB)
    [v2] Mon, 10 Jun 2024 17:59:07 UTC (6,050 KB)
    Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang

We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes GPT-like AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.

------------

`[2404.11960] Generating Diverse Criteria On-the-Fly to Improve Point-wise LLM Rankers <https://arxiv.org/abs/2404.11960>`__ 实时生成不同的标准以提高逐点LLM排名

::

    replaced with revised version Sat, 8 Jun 2024 14:09:22 GMT
    Submission history From: Fang Guo [view email]
    [v1] Thu, 18 Apr 2024 07:42:46 UTC (6,131 KB)
    [v2] Sat, 8 Jun 2024 14:09:22 UTC (5,596 KB)
    Fang Guo, Wenyu Li, Honglei Zhuang, Yun Luo, Yafu Li, Qi Zhu, Le Yan, Yue Zhang

The most recent pointwise Large Language Model (LLM) rankers have achieved remarkable ranking results. However, these rankers are hindered by two major drawbacks: (1) they fail to follow a standardized comparison guidance during the ranking process, and (2) they struggle with comprehensive considerations when dealing with complicated passages. To address these shortcomings, we propose to build a ranker that generates ranking scores based on a set of criteria from various perspectives. These criteria are intended to direct each perspective in providing a distinct yet synergistic evaluation. Our research, which examines eight datasets from the BEIR benchmark demonstrates that incorporating this multi-perspective criteria ensemble approach markedly enhanced the performance of pointwise LLM rankers.

------------

`[2405.07798] FreeVA: Offline MLLM as Training-Free Video Assistant <https://arxiv.org/abs/2405.07798>`__ FreeVA:离线MLLM作为免培训视频助手

::

    replaced with revised version Mon, 10 Jun 2024 13:55:21 GMT
    Submission history From: Wenhao Wu [view email]
    [v1] Mon, 13 May 2024 14:42:13 UTC (836 KB)
    [v2] Mon, 10 Jun 2024 13:55:21 UTC (4,521 KB)
    Wenhao Wu

This paper undertakes an empirical study to revisit the latest advancements in Multimodal Large Language Models (MLLMs): Video Assistant. This study, namely FreeVA, aims to extend existing image-based MLLM to the video domain in a training-free manner. The study provides an essential, yet must-know baseline, and reveals several surprising findings: 1) FreeVA, leveraging only offline image-based MLLM without additional training, excels in zero-shot video question-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), even surpassing state-of-the-art methods that involve video instruction tuning. 2) While mainstream video-based MLLMs typically initialize with an image-based MLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the study indicates that utilizing the widely adopted VideoInstruct-100K for video instruction tuning doesn't actually lead to better performance compared to not training at all. 3) The commonly used evaluation metrics in existing works are significantly influenced by changes in the GPT API version over time. If ignored, this could affect the fairness and uniformity of comparisons between different methods and impact the analysis and judgment of researchers in the field. The advancement of MLLMs is currently thriving, drawing numerous researchers into the field. We aim for this work to serve as a plug-and-play, simple yet effective baseline, encouraging the direct evaluation of existing MLLMs in video domain while also standardizing the field of video conversational models to a certain extent. Also, we encourage researchers to reconsider: Have current video MLLM methods truly acquired knowledge beyond image MLLM? Code is available at this https URL

------------

`[2405.16363] LLMs for User Interest Exploration in Large-scale Recommendation Systems <https://arxiv.org/abs/2405.16363>`__ 用于大规模推荐系统中用户兴趣探索的llm

::

    replaced with revised version Fri, 7 Jun 2024 18:06:20 GMT
    Submission history From: Jianling Wang [view email]
    [v1] Sat, 25 May 2024 21:57:36 UTC (2,534 KB)
    [v2] Fri, 7 Jun 2024 18:06:20 UTC (1,937 KB)
    Jianling Wang, Haokai Lu, Yifan Liu, He Ma, Yueqi Wang, Yang Gu, Shuzhou Zhang, Ningren Han, Shuchao Bi, Lexi Baugher, Ed Chi and Minmin Chen

Traditional recommendation systems are subject to a strong feedback loop by learning from and reinforcing past user-item interactions, which in turn limits the discovery of novel user interests. To address this, we introduce a hybrid hierarchical framework combining Large Language Models (LLMs) and classic recommendation models for user interest exploration. The framework controls the interfacing between the LLMs and the classic recommendation models through "interest clusters", the granularity of which can be explicitly determined by algorithm designers. It recommends the next novel interests by first representing "interest clusters" using language, and employs a fine-tuned LLM to generate novel interest descriptions that are strictly within these predefined clusters. At the low level, it grounds these generated interests to an item-level policy by restricting classic recommendation models, in this case a transformer-based sequence recommender to return items that fall within the novel clusters generated at the high level. We showcase the efficacy of this approach on an industrial-scale commercial platform serving billions of users. Live experiments show a significant increase in both exploration of novel interests and overall user enjoyment of the platform.

------------

`[2406.04337] Coherent Zero-Shot Visual Instruction Generation <https://arxiv.org/abs/2406.04337>`__ 连贯零样本视觉指令生成

::

    replaced with revised version Sat, 8 Jun 2024 12:07:32 GMT
    Submission history From: Quynh Phung [view email]
    [v1] Thu, 6 Jun 2024 17:59:44 UTC (35,206 KB)
    [v2] Sat, 8 Jun 2024 12:07:32 UTC (16,316 KB)
    Quynh Phung, Songwei Ge, Jia-Bin Huang

Despite the advances in text-to-image synthesis, particularly with diffusion models, generating visual instructions that require consistent representation and smooth state transitions of objects across sequential steps remains a formidable challenge. This paper introduces a simple, training-free framework to tackle the issues, capitalizing on the advancements in diffusion models and large language models (LLMs). Our approach systematically integrates text comprehension and image generation to ensure visual instructions are visually appealing and maintain consistency and accuracy throughout the instruction sequence. We validate the effectiveness by testing multi-step instructions and comparing the text alignment and consistency with several baselines. Our experiments show that our approach can visualize coherent and visually pleasing instructions

------------

`[2402.19200] PRSA: PRompt Stealing Attacks against Large Language Models <https://arxiv.org/abs/2402.19200>`__ PRSA:针对大型语言模型的提示窃取攻击

::

    replaced with revised version Sat, 8 Jun 2024 03:43:12 GMT
    Submission history From: Yong Yang [view email]
    [v1] Thu, 29 Feb 2024 14:30:28 UTC (8,404 KB)
    [v2] Sat, 8 Jun 2024 03:43:12 UTC (9,409 KB)
    Yong Yang, Changjiang Li, Yi Jiang, Xi Chen, Haoyu Wang, Xuhong Zhang, Zonghui Wang, Shouling Ji

In recent years, "prompt as a service" has greatly enhanced the utility of large language models (LLMs) by enabling them to perform various downstream tasks efficiently without fine-tuning. This has also increased the commercial value of prompts. However, the potential risk of leakage in these commercialized prompts remains largely underexplored. In this paper, we introduce a novel attack framework, PRSA, designed for prompt stealing attacks against LLMs. The main idea of PRSA is to infer the intent behind a prompt by analyzing its input-output content, enabling the generation of a surrogate prompt that replicates the original's functionality. Specifically, PRSA mainly consists of two key phases: prompt mutation and prompt pruning. In the mutation phase, we propose a prompt attention algorithm based on output difference. The algorithm facilitates the generation of effective surrogate prompts by learning key factors that influence the accurate inference of prompt intent. During the pruning phase, we employ a two-step related word identification strategy to detect and mask words that are highly related to the input, thus improving the generalizability of the surrogate prompts. We verify the actual threat of PRSA through evaluation in both real-world settings, non-interactive and interactive prompt services. The results strongly confirm the PRSA's effectiveness and generalizability. We have reported these findings to prompt service providers and actively collaborate with them to implement defensive measures.

------------

`[2406.00799] Are you still on track!? Catching LLM Task Drift with Activations <https://arxiv.org/abs/2406.00799>`__ 你还在正轨上吗?用激活捕捉LLM任务漂移

::

    replaced with revised version Mon, 10 Jun 2024 15:39:56 GMT
    Submission history From: Sahar Abdelnabi [view email]
    [v1] Sun, 2 Jun 2024 16:53:21 UTC (3,274 KB)
    [v2] Mon, 10 Jun 2024 15:39:56 UTC (3,275 KB)
    Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, Andrew Paverd

Large Language Models (LLMs) are routinely used in retrieval-augmented applications to orchestrate tasks and process inputs from users and other sources. These inputs, even in a single LLM interaction, can come from a variety of sources, of varying trustworthiness and provenance. This opens the door to prompt injection attacks, where the LLM receives and acts upon instructions from supposedly data-only sources, thus deviating from the user's original instructions. We define this as task drift, and we propose to catch it by scanning and analyzing the LLM's activations. We compare the LLM's activations before and after processing the external input in order to detect whether this input caused instruction drift. We develop two probing methods and find that simply using a linear classifier can detect drift with near perfect ROC AUC on an out-of-distribution test set. We show that this approach generalizes surprisingly well to unseen task domains, such as prompt injections, jailbreaks, and malicious instructions, without being trained on any of these attacks. Our setup does not require any modification of the LLM (e.g., fine-tuning) or any text generation, thus maximizing deployability and cost efficiency and avoiding reliance on unreliable model output. To foster future research on activation-based task inspection, decoding, and interpretability, we will release our large-scale TaskTracker toolkit, comprising a dataset of over 500K instances, representations from 4 SoTA language models, and inspection tools.

------------

`[2406.03230] Defending Large Language Models Against Attacks With Residual Stream Activation Analysis <https://arxiv.org/abs/2406.03230>`__ 基于残差流激活分析的大型语言模型防御攻击

::

    replaced with revised version Fri, 7 Jun 2024 22:27:00 GMT
    Submission history From: Amelia Kawasaki [view email]
    [v1] Wed, 5 Jun 2024 13:06:33 UTC (21 KB)
    [v2] Fri, 7 Jun 2024 22:27:00 UTC (21 KB)
    Amelia Kawasaki, Andrew Davis, Houssam Abbas

The widespread adoption of Large Language Models (LLMs), exemplified by OpenAI's ChatGPT, brings to the forefront the imperative to defend against adversarial threats on these models. These attacks, which manipulate an LLM's output by introducing malicious inputs, undermine the model's integrity and the trust users place in its outputs. In response to this challenge, our paper presents an innovative defensive strategy, given white box access to an LLM, that harnesses residual activation analysis between transformer layers of the LLM. We apply a novel methodology for analyzing distinctive activation patterns in the residual streams for attack prompt classification. We curate multiple datasets to demonstrate how this method of classification has high accuracy across multiple types of attack scenarios, including our newly-created attack dataset. Furthermore, we enhance the model's resilience by integrating safety fine-tuning techniques for LLMs in order to measure its effect on our capability to detect attacks. The results underscore the effectiveness of our approach in enhancing the detection and mitigation of adversarial inputs, advancing the security framework within which LLMs operate.

------------

-----------
Index (146)
-----------

`[2406.05410] MLLM-SR: Conversational Symbolic Regression base Multi-Modal Large Language Models <https://arxiv.org/abs/2406.05410>`__ MLLM-SR:基于对话符号回归的多模态大型语言模型

`[2406.05534] Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing <https://arxiv.org/abs/2406.05534>`__ 在线DPO:快慢追逐的在线直接偏好优化

`[2406.05954] Aligning Large Language Models with Representation Editing: A Control Perspective <https://arxiv.org/abs/2406.05954>`__ 基于表示编辑的大型语言模型对齐:一个控制视角

`[2406.05972] Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context <https://arxiv.org/abs/2406.05972>`__ 不确定背景下llm决策行为评估框架

`[2406.06455] A Large Language Model Pipeline for Breast Cancer Oncology <https://arxiv.org/abs/2406.06455>`__ 乳腺癌肿瘤的大型语言模型管道

`[2406.06474] Towards a Personal Health Large Language Model <https://arxiv.org/abs/2406.06474>`__ 面向个人健康的大型语言模型

`[2406.05213] On Subjective Uncertainty Quantification and Calibration in Natural Language Generation <https://arxiv.org/abs/2406.05213>`__ 自然语言生成中主观不确定性的量化与校准

`[2406.05232] Improving Logits-based Detector without Logits from Black-box LLMs <https://arxiv.org/abs/2406.05232>`__

`[2406.05255] Generative Explore-Exploit: Training-free Optimization of Generative Recommender Systems using LLM Optimizers <https://arxiv.org/abs/2406.05255>`__ 生成式探索-利用:使用LLM优化器的生成式推荐系统的无训练优化

`[2406.05322] Teaching-Assistant-in-the-Loop: Improving Knowledge Distillation from Imperfect Teacher Models in Low-Budget Scenarios <https://arxiv.org/abs/2406.05322>`__ 教学辅助在环:改进从低预算场景中不完美教师模型提取的知识

`[2406.05328] Hidden Question Representations Tell Non-Factuality Within and Across Large Language Models <https://arxiv.org/abs/2406.05328>`__ 隐藏问题表示告诉大型语言模型内部和跨语言模型的非事实性

`[2406.05344] MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention <https://arxiv.org/abs/2406.05344>`__ MemeGuard:基于LLM和vlm的框架，通过模因干预推进内容审核

`[2406.05360] Flexible and Adaptable Summarization via Expertise Separation <https://arxiv.org/abs/2406.05360>`__ 基于专业知识分离的灵活自适应摘要

`[2406.05361] Write Summary Step-by-Step: A Pilot Study of Stepwise Summarization <https://arxiv.org/abs/2406.05361>`__ 分步写摘要:对分步总结的初步研究

`[2406.05374] Planning Like Human: A Dual-process Framework for Dialogue Planning <https://arxiv.org/abs/2406.05374>`__ 像人一样规划:对话规划的双过程框架

`[2406.05392] Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas <https://arxiv.org/abs/2406.05392>`__

`[2406.05460] Fighting Against the Repetitive Training and Sample Dependency Problem in Few-shot Named Entity Recognition <https://arxiv.org/abs/2406.05460>`__ 解决小样本命名实体识别中的重复训练和样本依赖问题

`[2406.05494] Investigating and Addressing Hallucinations of LLMs in Tasks Involving Negation <https://arxiv.org/abs/2406.05494>`__ 在涉及否定的任务中调查和解决llm的幻觉

`[2406.05559] ThatiAR: Subjectivity Detection in Arabic News Sentences <https://arxiv.org/abs/2406.05559>`__ 阿拉伯语新闻句子主观性检测

`[2406.05569] Do LLMs Recognize me, When I is not me: Assessment of LLMs Understanding of Turkish Indexical Pronouns in Indexical Shift Contexts <https://arxiv.org/abs/2406.05569>`__ 法学硕士认识我吗，当我不是我:评估法学硕士对索引转换语境中土耳其索引代词的理解

`[2406.05587] Creativity Has Left the Chat: The Price of Debiasing Language Models <https://arxiv.org/abs/2406.05587>`__ 创造性已经离开了话题:去偏见语言模型的代价

`[2406.05588] CERET: Cost-Effective Extrinsic Refinement for Text Generation <https://arxiv.org/abs/2406.05588>`__ CERET:具有成本效益的文本生成外部优化

`[2406.05606] GrowOVER: How Can LLMs Adapt to Growing Real-World Knowledge? <https://arxiv.org/abs/2406.05606>`__ GrowOVER: llm如何适应不断增长的现实世界知识?

`[2406.05644] How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States <https://arxiv.org/abs/2406.05644>`__ 对齐和越狱是如何工作的:通过中间隐藏状态解释LLM的安全性

`[2406.05678] SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models <https://arxiv.org/abs/2406.05678>`__ SinkLoRA:增强的长上下文大型语言模型的效率和聊天能力

`[2406.05688] Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions <https://arxiv.org/abs/2406.05688>`__ 同行评议是基于角色互动的多轮长语境对话

`[2406.05690] MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation <https://arxiv.org/abs/2406.05690>`__ MoPS:面向开放式自动故事生成的模块化故事前提合成

`[2406.05798] Hidden Holes: topological aspects of language models <https://arxiv.org/abs/2406.05798>`__ 隐洞:语言模型的拓扑方面

`[2406.05812] Seventeenth-Century Spanish American Notary Records for Fine-Tuning Spanish Large Language Models <https://arxiv.org/abs/2406.05812>`__ 17世纪西班牙裔美国人公证人对西班牙大型语言模型进行微调的记录

`[2406.05845] MedREQAL: Examining Medical Knowledge Recall of Large Language Models via Question Answering <https://arxiv.org/abs/2406.05845>`__ MedREQAL:基于问答的大型语言模型医学知识召回研究

`[2406.05876] Zero-Shot End-To-End Spoken Question Answering In Medical Domain <https://arxiv.org/abs/2406.05876>`__ 医疗领域零样本端到端口语问答

`[2406.05885] Are Large Language Models Actually Good at Text Style Transfer? <https://arxiv.org/abs/2406.05885>`__ 大型语言模型真的擅长文本风格迁移吗?

`[2406.05888] Feriji: A French-Zarma Parallel Corpus, Glossary & Translator <https://arxiv.org/abs/2406.05888>`__ Feriji:法语- zarma平行语料库、词汇表和翻译

`[2406.05918] Why Don't Prompt-Based Fairness Metrics Correlate? <https://arxiv.org/abs/2406.05918>`__ 为什么基于提示的公平性指标没有关联?

`[2406.06027] HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs <https://arxiv.org/abs/2406.06027>`__ HOLMES:基于llm的多跳问答超关系知识图谱

`[2406.06056] Synth-SBDH: A Synthetic Dataset of Social and Behavioral Determinants of Health for Clinical Text <https://arxiv.org/abs/2406.06056>`__ Synth-SBDH:面向临床文本的健康社会和行为决定因素合成数据集

`[2406.06125] Verifiable Generation with Subsentence-Level Fine-Grained Citations <https://arxiv.org/abs/2406.06125>`__

`[2406.06140] Can I understand what I create? Self-Knowledge Evaluation of Large Language Models <https://arxiv.org/abs/2406.06140>`__ 我能理解我创造了什么吗?大型语言模型的自我认识评估

`[2406.06144] Language Models Resist Alignment <https://arxiv.org/abs/2406.06144>`__ 语言模型抵抗对齐

`[2406.06316] Tx-LLM: A Large Language Model for Therapeutics <https://arxiv.org/abs/2406.06316>`__ Tx-LLM:治疗学大型语言模型

`[2406.06326] Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching <https://arxiv.org/abs/2406.06326>`__ 自调整:指导llm通过自我教学有效地获取新知识

`[2406.06369] Annotation alignment: Comparing LLM and human annotations of conversational safety <https://arxiv.org/abs/2406.06369>`__ 注释对齐:比较LLM和会话安全性的人工注释

`[2406.06435] Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain <https://arxiv.org/abs/2406.06435>`__ 语言模型是可对齐的决策者:医疗分诊领域的数据集和应用

`[2406.06485] Can Language Models Serve as Text-Based World Simulators? <https://arxiv.org/abs/2406.06485>`__ 语言模型能充当基于文本的世界模拟器吗?

`[2406.05183] The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More <https://arxiv.org/abs/2406.05183>`__ 因子分解诅咒:你预测的token是反转诅咒的基础，还有更多

`[2406.05223] CorDA: Context-Oriented Decomposition Adaptation of Large Language Models <https://arxiv.org/abs/2406.05223>`__ CorDA:面向上下文的大型语言模型分解自适应

`[2406.05317] LoCoCo: Dropping In Convolutions for Long Context Compression <https://arxiv.org/abs/2406.05317>`__ LoCoCo:减少卷积以进行长上下文压缩

`[2406.05516] Verbalized Probabilistic Graphical Modeling with Large Language Models <https://arxiv.org/abs/2406.05516>`__ 基于大型语言模型的语言化概率图模型

`[2406.05882] Distributional Preference Alignment of LLMs via Optimal Transport <https://arxiv.org/abs/2406.05882>`__ 基于最优传输的llm分布偏好对齐

`[2406.05883] Information Theoretic Guarantees For Policy Alignment In Large Language Models <https://arxiv.org/abs/2406.05883>`__ 大型语言模型中策略对齐的信息论保证

`[2406.05900] Large Language Models Memorize Sensor Datasets! Implications on Human Activity Recognition Research <https://arxiv.org/abs/2406.05900>`__ 大型语言模型记住传感器数据集!对人类活动识别研究的启示

`[2406.05955] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters <https://arxiv.org/abs/2406.05955>`__ Turbo Sparse:以最小激活参数实现LLM SOTA性能

`[2406.06282] PowerInfer-2: Fast Large Language Model Inference on a Smartphone <https://arxiv.org/abs/2406.06282>`__ powerinfer2:智能手机上的快速大型语言模型推理

`[2406.06385] Low-Rank Quantization-Aware Training for LLMs <https://arxiv.org/abs/2406.06385>`__ llm的低秩量化感知训练

`[2406.06443] LLM Dataset Inference: Did you train on my dataset? <https://arxiv.org/abs/2406.06443>`__

`[2406.06487] When is Multicalibration Post-Processing Necessary? <https://arxiv.org/abs/2406.06487>`__ 什么时候需要多校准后处理?

`[2406.05249] A Language Model-Guided Framework for Mining Time Series with Distributional Shifts <https://arxiv.org/abs/2406.05249>`__ 一种语言模型指导的分布偏移时间序列挖掘框架

`[2406.05314] Relational Proxy Loss for Audio-Text based Keyword Spotting <https://arxiv.org/abs/2406.05314>`__ 基于音频-文本关键词识别的关系代理损失

`[2406.05498] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner <https://arxiv.org/abs/2406.05498>`__ 自我防御:llm可以用一种实用的方式防御越狱

`[2406.05543] VP-LLM: Text-Driven 3D Volume Completion with Large Language Models through Patchification <https://arxiv.org/abs/2406.05543>`__ VP-LLM:基于补丁的大型语言模型文本驱动3D体补全

`[2406.05572] Trust the PRoC3S: Solving Long-Horizon Robotics Problems with LLMs and Constraint Satisfaction <https://arxiv.org/abs/2406.05572>`__ 相信PRoC3S:用llm和约束满足解决长视距机器人问题

`[2406.05603] A Knowledge-Component-Based Methodology for Evaluating AI Assistants <https://arxiv.org/abs/2406.05603>`__

`[2406.05839] MaLa-ASR: Multimedia-Assisted LLM-Based ASR <https://arxiv.org/abs/2406.05839>`__ MaLa-ASR:基于llm的多媒体辅助ASR

`[2406.05946] Safety Alignment Should Be Made More Than Just a Few Tokens Deep <https://arxiv.org/abs/2406.05946>`__ 安全对齐应该不仅仅是深入几个标记

`[2406.05948] Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models <https://arxiv.org/abs/2406.05948>`__ 审查链:检测大型语言模型的后门攻击

`[2406.06400] An Empirical Design Justice Approach to Identifying Ethical Considerations in the Intersection of Large Language Models and Social Robotics <https://arxiv.org/abs/2406.06400>`__ 用经验设计正义方法在大型语言模型和社会机器人的交叉领域识别伦理考虑

`[2406.06465] AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction <https://arxiv.org/abs/2406.06465>`__ 辅助:用于指令引导视频预测的图像-视频扩散模型

`[2406.05013] CHIQ: Contextual History Enhancement for Improving Query Rewriting in Conversational Search <https://arxiv.org/abs/2406.05013>`__

`[2406.05651] A Superalignment Framework in Autonomous Driving with Large Language Models <https://arxiv.org/abs/2406.05651>`__ 基于大型语言模型的自动驾驶超对齐框架

`[2406.05968] Prompting Large Language Models with Audio for General-Purpose Speech Summarization <https://arxiv.org/abs/2406.05968>`__ 用音频激励大型语言模型进行通用语音摘要

`[2406.06025] RepoQA: Evaluating Long Context Code Understanding <https://arxiv.org/abs/2406.06025>`__ RepoQA:评估长上下文代码的理解能力

`[2406.06382] Diffusion-RPO: Aligning Diffusion Models through Relative Preference Optimization <https://arxiv.org/abs/2406.06382>`__ Diffusion- rpo:基于相对偏好优化的扩散模型对齐

`[2406.05335] Critical Phase Transition in a Large Language Model <https://arxiv.org/abs/2406.05335>`__ 大型语言模型中的关键相变

`[2406.05596] Aligning Human Knowledge with Visual Concepts Towards Explainable Medical Image Classification <https://arxiv.org/abs/2406.05596>`__ 面向可解释医学图像分类的人类知识与视觉概念对齐

`[2406.05709] TR2MTL: LLM based framework for Metric Temporal Logic Formalization of Traffic Rules <https://arxiv.org/abs/2406.05709>`__ TR2MTL:基于LLM的交通规则度量时序逻辑形式化框架

`[2406.05741] Digital Business Model Analysis Using a Large Language Model <https://arxiv.org/abs/2406.05741>`__

`[2406.05892] Security Vulnerability Detection with Multitask Self-Instructed Fine-Tuning of Large Language Models <https://arxiv.org/abs/2406.05892>`__ 基于大型语言模型多任务自指导微调的安全漏洞检测

`[2401.08189] PRewrite: Prompt Rewriting with Reinforcement Learning <https://arxiv.org/abs/2401.08189>`__ PRewrite:使用强化学习提示重写

`[2402.02392] DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models <https://arxiv.org/abs/2402.02392>`__ DeLLMa:基于大型语言模型的不确定性决策框架

`[2403.08802] Governance of Generative Artificial Intelligence for Companies <https://arxiv.org/abs/2403.08802>`__ 企业生成性人工智能的治理

`[2403.17787] Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications <https://arxiv.org/abs/2403.17787>`__ 评估prompt工程的大型多模态模型与微调视觉transformer在基于图像的安全应用中的效果

`[2305.12307] OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing <https://arxiv.org/abs/2305.12307>`__ OntoType:本体引导的预训练语言模型辅助的细粒度实体分类

`[2306.11879] Open-Domain Text Evaluation via Contrastive Distribution Methods <https://arxiv.org/abs/2306.11879>`__ 基于对比分布方法的开放域文本评价

`[2307.12114] A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks <https://arxiv.org/abs/2307.12114>`__ 应用于临床和生物医学任务的指令微调大型语言模型的零样本和少样本研究

`[2309.17234] Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation <https://arxiv.org/abs/2309.17234>`__ 合作、竞争与恶意:llm利益相关者互动谈判

`[2310.14558] AlpaCare:Instruction-tuned Large Language Models for Medical Application <https://arxiv.org/abs/2310.14558>`__ AlpaCare:面向医学应用的指令调优大型语言模型

`[2311.06503] Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering <https://arxiv.org/abs/2311.06503>`__ 特定领域问答中llm的知识偏好对齐

`[2311.07430] Controlled Text Generation for Black-box Language Models via Score-based Progressive Editor <https://arxiv.org/abs/2311.07430>`__ 基于分数渐进式编辑器的黑盒语言模型受控文本生成

`[2312.12141] Neuron-Level Knowledge Attribution in Large Language Models <https://arxiv.org/abs/2312.12141>`__ 大型语言模型中的神经元级知识归因

`[2401.03735] Language Models Know the Value of Numbers <https://arxiv.org/abs/2401.03735>`__ 语言模型知道数字的值

`[2401.06468] Adapting Large Language Models for Document-Level Machine Translation <https://arxiv.org/abs/2401.06468>`__ 面向文档级机器翻译的大型语言模型适配

`[2401.07453] Model Editing at Scale leads to Gradual and Catastrophic Forgetting <https://arxiv.org/abs/2401.07453>`__ 大规模模型编辑会导致渐进和灾难性的遗忘

`[2401.13927] Adaptive Text Watermark for Large Language Models <https://arxiv.org/abs/2401.13927>`__ 面向大型语言模型的自适应文本水印

`[2402.00530] Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning <https://arxiv.org/abs/2402.00530>`__ 超过滤:用于快速指令调整的弱到强数据过滤

`[2402.05602] AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers <https://arxiv.org/abs/2402.05602>`__

`[2402.05699] Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation <https://arxiv.org/abs/2402.05699>`__ 基于单语社会场景仿真的大型语言模型自对齐

`[2402.10110] Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning <https://arxiv.org/abs/2402.10110>`__ 选择性反射调优:学生选择的LLM指令调优数据回收

`[2402.10373] BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains <https://arxiv.org/abs/2402.10373>`__ BioMistral:面向医学领域的开源预训练大型语言模型集合

`[2402.10614] Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements <https://arxiv.org/abs/2402.10614>`__ 法学硕士能代表不同的人吗?通过辩论对法学硕士进行调优，生成可控的争议语句

`[2402.11683] One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation <https://arxiv.org/abs/2402.11683>`__ 一个决定一切的提示:意见总结评估的LLMs

`[2402.11756] MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs <https://arxiv.org/abs/2402.11756>`__ MARS:生成式llm不确定性估计的意义感知响应评分

`[2402.12195] Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion <https://arxiv.org/abs/2402.12195>`__ 浏览和集中:通过先验- llm上下文融合理解多模态内容

`[2402.12483] Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question? <https://arxiv.org/abs/2402.12483>`__ 工件或绑架:llm如何回答没有问题的多项选择题?

`[2402.14860] Ranking Large Language Models without Ground Truth <https://arxiv.org/abs/2402.14860>`__ 没有基本事实的大型语言模型排名

`[2403.00071] Resonance RoPE: Improving Context Length Generalization of Large Language Models <https://arxiv.org/abs/2403.00071>`__ 共振绳:改进大型语言模型的上下文长度泛化

`[2403.01509] Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics <https://arxiv.org/abs/2403.01509>`__ 奇妙的语义以及在哪里可以找到它们:研究生成式llm的哪些层反映了词汇语义

`[2403.04460] Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset <https://arxiv.org/abs/2403.04460>`__ Pearl:评论驱动的人物角色知识基础会话推荐数据集

`[2403.06448] Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models <https://arxiv.org/abs/2403.06448>`__ 基于大型语言模型内部状态的无监督实时幻觉检测

`[2403.07865] CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion <https://arxiv.org/abs/2403.07865>`__ 代码攻击:通过代码补全揭示大型语言模型的安全泛化挑战

`[2403.11103] ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models <https://arxiv.org/abs/2403.11103>`__ ProgGen:用自反式大型语言模型逐步生成命名实体识别数据集

`[2403.13485] An Entropy-based Text Watermarking Detection Method <https://arxiv.org/abs/2403.13485>`__ 一种基于信息熵的文本水印检测方法

`[2404.10198] ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence <https://arxiv.org/abs/2404.10198>`__ 冲突:量化LLM内部先验和外部证据之间的拉锯战

`[2404.17287] When to Trust LLMs: Aligning Confidence with Response Quality <https://arxiv.org/abs/2404.17287>`__ 何时信任llm:将信任与响应质量相结合

`[2404.18624] Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations? <https://arxiv.org/abs/2404.18624>`__ 视觉和语言解码器是否平等地使用图像和文本?他们的解释有多自洽?

`[2405.00715] Adapting Open-Source Large Language Models for Cost-Effective, Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning <https://arxiv.org/abs/2405.00715>`__ 采用基于策略强化学习的开源大型语言模型，用于具有成本效益的专家级临床记录生成

`[2405.16282] Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models <https://arxiv.org/abs/2405.16282>`__ 幕后的信心:大型语言模型的信心概率对齐研究

`[2405.16433] CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling <https://arxiv.org/abs/2405.16433>`__ CPsyCoun:基于报告的中文心理咨询多轮对话重构与评估框架

`[2405.20805] Multilingual Text Style Transfer: Datasets & Models for Indian Languages <https://arxiv.org/abs/2405.20805>`__ 多语言文本风格迁移:印度语言的数据集和模型

`[2406.02528] Scalable MatMul-free Language Modeling <https://arxiv.org/abs/2406.02528>`__ 可扩展的无matmull语言建模

`[2406.03897] HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew <https://arxiv.org/abs/2406.03897>`__ HeSum:希伯来语文本摘要的新数据集

`[2406.04216] What Do Language Models Learn in Context? The Structured Task Hypothesis <https://arxiv.org/abs/2406.04216>`__ 语言模型在上下文中学习什么?结构化任务假说

`[2312.11875] Sparse is Enough in Fine-tuning Pre-trained Large Language Models <https://arxiv.org/abs/2312.11875>`__ 在微调预训练大型语言模型时，稀疏性就足够了

`[2401.06118] Extreme Compression of Large Language Models via Additive Quantization <https://arxiv.org/abs/2401.06118>`__ 基于加性量化的大型语言模型极端压缩

`[2402.03142] Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models <https://arxiv.org/abs/2402.03142>`__ Less is KEN:用于大型语言模型的通用简单非参数剪枝算法

`[2402.06255] Fight Back Against Jailbreaking via Prompt Adversarial Tuning <https://arxiv.org/abs/2402.06255>`__ 通过提示对抗性调整来反击越狱

`[2402.14760] Generalizing Reward Modeling for Out-of-Distribution Preference Learning <https://arxiv.org/abs/2402.14760>`__ 分布外偏好学习的泛化奖励模型

`[2403.13213] From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards <https://arxiv.org/abs/2403.13213>`__ 从代表性危害到服务质量危害:Llama 2安全保障案例研究

`[2405.15362] Pipeline Parallelism with Controllable Memory <https://arxiv.org/abs/2405.15362>`__ 具有可控内存的流水线并行

`[2406.01539] Physics-informed deep learning and compressive collocation for high-dimensional diffusion-reaction equations: practical existence theory and numerics <https://arxiv.org/abs/2406.01539>`__ 基于物理学的深度学习与压缩配置的高维扩散-反应方程:实践存在理论与数值

`[2406.02616] Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A Model-Based Reinforcement Learning Approach <https://arxiv.org/abs/2406.02616>`__ 边缘计算中无线LLM推理的自适应层分裂:基于模型的强化学习方法

`[2406.02806] Randomized Geometric Algebra Methods for Convex Neural Networks <https://arxiv.org/abs/2406.02806>`__ 凸神经网络的随机几何代数方法

`[2312.00027] Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections <https://arxiv.org/abs/2312.00027>`__ 通过后门注入对大型语言模型进行隐形和持续的不对齐

`[2401.13201] MLLMReID: Multimodal Large Language Model-based Person Re-identification <https://arxiv.org/abs/2401.13201>`__ MLLMReID:基于多模态大型语言模型的行人再识别

`[2401.16310] Security Code Review by Large Language Models <https://arxiv.org/abs/2401.16310>`__

`[2402.09508] Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls <https://arxiv.org/abs/2402.09508>`__ 编排，修复和完善:通过基于内容的控件操纵长期音乐音频生成和编辑

`[2402.18104] Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction <https://arxiv.org/abs/2402.18104>`__ 让它们问与答:通过伪装和重构在少量查询中越狱大型语言模型

`[2403.12388] Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models <https://arxiv.org/abs/2403.12388>`__ 具有大型语言模型的会话系统的可解释用户满意度估计

`[2404.00166] Uncovering Bias in Large Vision-Language Models with Counterfactuals <https://arxiv.org/abs/2404.00166>`__ 用反事实揭示大型视觉-语言模型中的偏见

`[2404.02905] Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction <https://arxiv.org/abs/2404.02905>`__ 视觉自回归建模:基于下一尺度预测的可扩展图像生成

`[2404.11960] Generating Diverse Criteria On-the-Fly to Improve Point-wise LLM Rankers <https://arxiv.org/abs/2404.11960>`__ 实时生成不同的标准以提高逐点LLM排名

`[2405.07798] FreeVA: Offline MLLM as Training-Free Video Assistant <https://arxiv.org/abs/2405.07798>`__ FreeVA:离线MLLM作为免培训视频助手

`[2405.16363] LLMs for User Interest Exploration in Large-scale Recommendation Systems <https://arxiv.org/abs/2405.16363>`__ 用于大规模推荐系统中用户兴趣探索的llm

`[2406.04337] Coherent Zero-Shot Visual Instruction Generation <https://arxiv.org/abs/2406.04337>`__ 连贯零样本视觉指令生成

`[2402.19200] PRSA: PRompt Stealing Attacks against Large Language Models <https://arxiv.org/abs/2402.19200>`__ PRSA:针对大型语言模型的提示窃取攻击

`[2406.00799] Are you still on track!? Catching LLM Task Drift with Activations <https://arxiv.org/abs/2406.00799>`__ 你还在正轨上吗?用激活捕捉LLM任务漂移

`[2406.03230] Defending Large Language Models Against Attacks With Residual Stream Activation Analysis <https://arxiv.org/abs/2406.03230>`__ 基于残差流激活分析的大型语言模型防御攻击

