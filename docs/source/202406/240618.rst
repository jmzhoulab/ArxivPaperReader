240618
========

----------
Survey (9)
----------

`[2406.10294] RelevAI-Reviewer: A Benchmark on AI Reviewers for Survey Paper Relevance <https://arxiv.org/abs/2406.10294>`__ 

::

    Thu, 13 Jun 2024 06:42:32 GMT
    Paulo Henrique Couto, Quang Phuoc Ho, Nageeta Kumari, Benedictus Kent Rachmat (TAU, LISN), Thanh Gia Hieu Khuong (TAU, LISN), Ihsan Ullah, Lisheng Sun-Hosoya (TAU, LISN)

Recent advancements in Artificial Intelligence (AI), particularly the widespread adoption of Large Language Models (LLMs), have significantly enhanced text analysis capabilities. This technological evolution offers considerable promise for automating the review of scientific papers, a task traditionally managed through peer review by fellow researchers. Despite its critical role in maintaining research quality, the conventional peer-review process is often slow and subject to biases, potentially impeding the swift propagation of scientific knowledge. In this paper, we propose RelevAI-Reviewer, an automatic system that conceptualizes the task of survey paper review as a classification problem, aimed at assessing the relevance of a paper in relation to a specified prompt, analogous to a "call for papers". To address this, we introduce a novel dataset comprised of 25,164 instances. Each instance contains one prompt and four candidate papers, each varying in relevance to the prompt. The objective is to develop a machine learning (ML) model capable of determining the relevance of each paper and identifying the most pertinent one. We explore various baseline approaches, including traditional ML classifiers like Support Vector Machine (SVM) and advanced language models such as BERT. Preliminary findings indicate that the BERT-based end-to-end classifier surpasses other conventional ML methods in performance.
We present this problem as a public challenge to foster engagement and interest in this area of research.

------------

`[2406.10303] A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations <https://arxiv.org/abs/2406.10303>`__ 从通用到医疗应用的大型语言模型综述:数据集、方法和评估

::

    Fri, 14 Jun 2024 02:42:20 GMT
    Jinqiang Wang, Huansheng Ning, Yi Peng, Qikai Wei, Daniel Tesfai, Wenwei Mao, Tao Zhu, Runhe Huang

Large Language Models (LLMs) have demonstrated surprising performance across various natural language processing tasks. Recently, medical LLMs enhanced with domain-specific knowledge have exhibited excellent capabilities in medical consultation and diagnosis. These models can smoothly simulate doctor-patient dialogues and provide professional medical advice. Most medical LLMs are developed through continued training of open-source general LLMs, which require significantly fewer computational resources than training LLMs from scratch.
Additionally, this approach offers better protection of patient privacy compared to API-based solutions. This survey systematically explores how to train medical LLMs based on general LLMs. It covers: (a) how to acquire training corpus and construct customized medical training sets, (b) how to choose a appropriate training paradigm, (c) how to choose a suitable evaluation benchmark, and (d) existing challenges and promising future research directions are discussed. This survey can provide guidance for the development of LLMs focused on various medical applications, such as medical education, diagnostic planning, and clinical assistants.

------------

`[2406.10833] A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery <https://arxiv.org/abs/2406.10833>`__ 科学大型语言模型及其在科学发现中的应用综述

::

    Sun, 16 Jun 2024 08:03:24 GMT
    Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han

In many scientific fields, large language models (LLMs) have revolutionized the way with which text and other modalities of data (e.g., molecules and proteins) are dealt, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one to two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we comprehensively survey over 250 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality. Moreover, we investigate how LLMs have been deployed to benefit scientific discovery. Resources related to this survey are available at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.

------------

`[2406.11191] A Survey on Human Preference Learning for Large Language Models <https://arxiv.org/abs/2406.11191>`__ 

::

    Mon, 17 Jun 2024 03:52:51 GMT
    Ruili Jiang, Kehai Chen, Xuefeng Bai, Zhixuan He, Juntao Li, Muyun Yang, Tiejun Zhao, Liqiang Nie, Min Zhang

The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs.

------------

`[2406.11289] A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models <https://arxiv.org/abs/2406.11289>`__ 

::

    Mon, 17 Jun 2024 07:52:32 GMT
    Haopeng Zhang, Philip S. Yu, Jiawei Zhang

Text summarization research has undergone several significant transformations with the advent of deep neural networks, pre-trained language models (PLMs), and recent large language models (LLMs). This survey thus provides a comprehensive review of the research progress and evolution in text summarization through the lens of these paradigm shifts. It is organized into two main parts: (1) a detailed overview of datasets, evaluation metrics, and summarization methods before the LLM era, encompassing traditional statistical methods, deep learning approaches, and PLM fine-tuning techniques, and (2) the first detailed examination of recent advancements in benchmarking, modeling, and evaluating summarization in the LLM era. By synthesizing existing literature and presenting a cohesive overview, this survey also discusses research trends, open challenges, and proposes promising research directions in summarization, aiming to guide researchers through the evolving landscape of summarization research.

------------

`[2406.10729] A Comprehensive Survey of Foundation Models in Medicine <https://arxiv.org/abs/2406.10729>`__ 

::

    Sat, 15 Jun 2024 20:04:06 GMT
    Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang

Foundation models (FMs) are large-scale deep-learning models trained on extensive datasets using self-supervised techniques. These models serve as a base for various downstream tasks, including healthcare. FMs have been adopted with great success across various domains within healthcare, including natural language processing (NLP), computer vision, graph learning, biology, and omics.
Existing healthcare-based surveys have not yet included all of these domains.
Therefore, this survey provides a comprehensive overview of FMs in healthcare.
We focus on the history, learning strategies, flagship models, applications, and challenges of FMs. We explore how FMs such as the BERT and GPT families are reshaping various healthcare domains, including clinical large language models, medical image analysis, and omics data. Furthermore, we provide a detailed taxonomy of healthcare applications facilitated by FMs, such as clinical NLP, medical computer vision, graph learning, and other biology-related tasks.
Despite the promising opportunities FMs provide, they also have several associated challenges, which are explained in detail. We also outline potential future directions to provide researchers and practitioners with insights into the potential and limitations of FMs in healthcare to advance their deployment and mitigate associated risks.

------------

`[2406.10252] AutoSurvey: Large Language Models Can Automatically Write Surveys <https://arxiv.org/abs/2406.10252>`__ 

::

    Mon, 10 Jun 2024 12:56:06 GMT
    Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang

This paper introduces AutoSurvey, a speedy and well-organized methodology for automating the creation of comprehensive literature surveys in rapidly evolving fields like artificial intelligence. Traditional survey paper creation faces challenges due to the vast volume and complexity of information, prompting the need for efficient survey methods. While large language models (LLMs) offer promise in automating this process, challenges such as context window limitations, parametric knowledge constraints, and the lack of evaluation benchmarks remain. AutoSurvey addresses these challenges through a systematic approach that involves initial retrieval and outline generation, subsection drafting by specialized LLMs, integration and refinement, and rigorous evaluation and iteration. Our contributions include a comprehensive solution to the survey problem, a reliable evaluation method, and experimental validation demonstrating AutoSurvey's effectiveness.

------------

`[2406.05804] A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components <https://arxiv.org/abs/2406.05804>`__ 

::

    replaced with revised version Sun, 16 Jun 2024 00:59:27 GMT
    Submission history From: Xinzhe Li [view email]
    [v1] Sun, 9 Jun 2024 14:42:55 UTC (9,288 KB)
    [v2] Sun, 16 Jun 2024 00:59:27 UTC (9,290 KB)
    Xinzhe Li

Recent advancements in Large Language Models (LLMs) have catalyzed the development of sophisticated frameworks for developing LLM-based agents. However, the complexity of these frameworks r poses a hurdle for nuanced differentiation at a granular level, a critical aspect for enabling efficient implementations across different frameworks and fostering future research. Hence, the primary purpose of this survey is to facilitate a cohesive understanding of diverse recently proposed frameworks by identifying common workflows and reusable LLM-Profiled Components (LMPCs).

------------

`[2405.06211] A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2405.06211>`__ RAG Meeting llm综述:面向检索增强的大型语言模型

::

    replaced with revised version Mon, 17 Jun 2024 08:56:38 GMT
    Submission history From: Yujuan Ding [view email]
    [v1] Fri, 10 May 2024 02:48:45 UTC (823 KB)
    [v2] Fri, 14 Jun 2024 13:07:27 UTC (1,484 KB)
    [v3] Mon, 17 Jun 2024 08:56:38 UTC (1,487 KB)
    Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li

As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations, such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: architectures, training strategies, and applications. As the preliminary knowledge, we briefly introduce the foundations and recent advances of LLMs. Then, to illustrate the practical significance of RAG for LLMs, we systematically review mainstream relevant work by their architectures, training strategies, and application areas, detailing specifically the challenges of each and the corresponding capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at this https URL

------------

--------------
Benchmark (29)
--------------

`[2406.10291] ResearchArena: Benchmarking LLMs' Ability to Collect and Organize Information as Research Agents <https://arxiv.org/abs/2406.10291>`__ ResearchArena:对llm作为研究代理收集和组织信息的能力进行基准测试

::

    Thu, 13 Jun 2024 03:26:30 GMT
    Hao Kang, Chenyan Xiong

Large language models (LLMs) have exhibited remarkable performance across various tasks in natural language processing. Nevertheless, challenges still arise when these tasks demand domain-specific expertise and advanced analytical skills, such as conducting research surveys on a designated topic. In this research, we develop ResearchArena, a benchmark that measures LLM agents' ability to conduct academic surveys, an initial step of academic research process. Specifically, we deconstructs the surveying process into three stages 1) information discovery: locating relevant papers, 2) information selection: assessing papers' importance to the topic, and 3) information organization: organizing papers into meaningful structures. In particular, we establish an offline environment comprising 12.0M full-text academic papers and 7.9K survey papers, which evaluates agents' ability to locate supporting materials for composing the survey on a topic, rank the located papers based on their impact, and organize these into a hierarchical knowledge mind-map. With this benchmark, we conduct preliminary evaluations of existing techniques and find that all LLM-based methods under-performing when compared to basic keyword-based retrieval techniques, highlighting substantial opportunities for future research.

------------

`[2406.10292] Automatically Labeling $200B Life-Saving Datasets: A Large Clinical Trial Outcome Benchmark <https://arxiv.org/abs/2406.10292>`__ 自动标记2000亿美元的救生数据集:一个大型临床试验结果基准

::

    Thu, 13 Jun 2024 04:23:35 GMT
    Chufan Gao, Jathurshan Pradeepkumar, Trisha Das, Shivashankar Thati, Jimeng Sun

The global cost of drug discovery and development exceeds $200 billion annually. The main results of drug discovery and development are the outcomes of clinical trials, which directly influence the regulatory approval of new drug candidates and ultimately affect patient outcomes. Despite their significance, large-scale, high-quality clinical trial outcome data are not readily available to the public. Suppose a large clinical trial outcome dataset is provided; machine learning researchers can potentially develop accurate prediction models using past trials and outcome labels, which could help prioritize and optimize therapeutic programs, ultimately benefiting patients.
This paper introduces Clinical Trial Outcome (CTO) dataset, the largest trial outcome dataset with around 479K clinical trials, aggregating outcomes from multiple sources of weakly supervised labels, minimizing the noise from individual sources, and eliminating the need for human annotation. These sources include large language model (LLM) decisions on trial-related documents, news headline sentiments, stock prices of trial sponsors, trial linkages across phases, and other signals such as patient dropout rates and adverse events. CTO's labels show unprecedented agreement with supervised clinical trial outcome labels from test split of the supervised TOP dataset, with a 91 F1.

------------

`[2406.10290] MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases <https://arxiv.org/abs/2406.10290>`__ MobileAIBench:针对设备上的用例对llm和lms进行基准测试

::

    Wed, 12 Jun 2024 22:58:12 GMT
    Rithesh Murthy, Liangwei Yang, Juntao Tan, Tulika Manoj Awalgaonkar, Yilun Zhou, Shelby Heinecke, Sachin Desai, Jason Wu, Ran Xu, Sarah Tan, Jianguo Zhang, Zhiwei Liu, Shirley Kokane, Zuxin Liu, Ming Zhu, Huan Wang, Caiming Xiong, Silvio Savarese

The deployment of Large Language Models (LLMs) and Large Multimodal Models (LMMs) on mobile devices has gained significant attention due to the benefits of enhanced privacy, stability, and personalization. However, the hardware constraints of mobile devices necessitate the use of models with fewer parameters and model compression techniques like quantization. Currently, there is limited understanding of quantization's impact on various task performances, including LLM tasks, LMM tasks, and, critically, trust and safety. There is a lack of adequate tools for systematically testing these models on mobile devices. To address these gaps, we introduce MobileAIBench, a comprehensive benchmarking framework for evaluating mobile-optimized LLMs and LMMs.
MobileAIBench assesses models across different sizes, quantization levels, and tasks, measuring latency and resource consumption on real devices. Our two-part open-source framework includes a library for running evaluations on desktops and an iOS app for on-device latency and hardware utilization measurements. Our thorough analysis aims to accelerate mobile AI research and deployment by providing insights into the performance and feasibility of deploying LLMs and LMMs on mobile platforms.

------------

`[2406.10294] RelevAI-Reviewer: A Benchmark on AI Reviewers for Survey Paper Relevance <https://arxiv.org/abs/2406.10294>`__ 

::

    Thu, 13 Jun 2024 06:42:32 GMT
    Paulo Henrique Couto, Quang Phuoc Ho, Nageeta Kumari, Benedictus Kent Rachmat (TAU, LISN), Thanh Gia Hieu Khuong (TAU, LISN), Ihsan Ullah, Lisheng Sun-Hosoya (TAU, LISN)

Recent advancements in Artificial Intelligence (AI), particularly the widespread adoption of Large Language Models (LLMs), have significantly enhanced text analysis capabilities. This technological evolution offers considerable promise for automating the review of scientific papers, a task traditionally managed through peer review by fellow researchers. Despite its critical role in maintaining research quality, the conventional peer-review process is often slow and subject to biases, potentially impeding the swift propagation of scientific knowledge. In this paper, we propose RelevAI-Reviewer, an automatic system that conceptualizes the task of survey paper review as a classification problem, aimed at assessing the relevance of a paper in relation to a specified prompt, analogous to a "call for papers". To address this, we introduce a novel dataset comprised of 25,164 instances. Each instance contains one prompt and four candidate papers, each varying in relevance to the prompt. The objective is to develop a machine learning (ML) model capable of determining the relevance of each paper and identifying the most pertinent one. We explore various baseline approaches, including traditional ML classifiers like Support Vector Machine (SVM) and advanced language models such as BERT. Preliminary findings indicate that the BERT-based end-to-end classifier surpasses other conventional ML methods in performance.
We present this problem as a public challenge to foster engagement and interest in this area of research.

------------

`[2406.10311] CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large Language Models <https://arxiv.org/abs/2406.10311>`__ CHiSafetyBench:面向大型语言模型的中文层次化安全基准

::

    Fri, 14 Jun 2024 06:47:40 GMT
    Wenjing Zhang, Xuejiao Lei, Zhaoxiang Liu, Meijuan An, Bikun Yang, KaiKai Zhao, Kai Wang, and Shiguo Lian

With the profound development of large language models(LLMs), their safety concerns have garnered increasing attention. However, there is a scarcity of Chinese safety benchmarks for LLMs, and the existing safety taxonomies are inadequate, lacking comprehensive safety detection capabilities in authentic Chinese scenarios. In this work, we introduce CHiSafetyBench, a dedicated safety benchmark for evaluating LLMs' capabilities in identifying risky content and refusing answering risky questions in Chinese contexts. CHiSafetyBench incorporates a dataset that covers a hierarchical Chinese safety taxonomy consisting of 5 risk areas and 31 categories. This dataset comprises two types of tasks: multiple-choice questions and question-answering, evaluating LLMs from the perspectives of risk content identification and the ability to refuse answering risky questions respectively. Utilizing this benchmark, we validate the feasibility of automatic evaluation as a substitute for human evaluation and conduct comprehensive automatic safety assessments on mainstream Chinese LLMs. Our experiments reveal the varying performance of different models across various safety domains, indicating that all models possess considerable potential for improvement in Chinese safety capabilities. Our dataset is publicly available at https://github.com/UnicomAI/DataSet/tree/main/TestData/Safety.

------------

`[2406.10421] SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading <https://arxiv.org/abs/2406.10421>`__ SciEx:基于人工专家评分和自动评分的科学考试大型语言模型基准测试

::

    Fri, 14 Jun 2024 21:52:21 GMT
    Tu Anh Dinh, Carlos Mullov, Leonard B\"armann, Zhaolin Li, Danni Liu, Simon Rei{\ss}, Jueun Lee, Nathan Lerzer, Fabian Ternava, Jianfeng Gao, Alexander Waibel, Tamim Asfour, Michael Beigl, Rainer Stiefelhagen, Carsten Dachsbacher, Klemens B\"ohm, Jan Niehues

With the rapid development of Large Language Models (LLMs), it is crucial to have benchmarks which can evaluate the ability of LLMs on different domains.
One common use of LLMs is performing tasks on scientific topics, such as writing algorithms, querying databases or giving mathematical proofs. Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx - a benchmark consisting of university computer science exam questions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1) multilingual, containing both English and German exams, and (2) multi-modal, containing questions that involve images, and (3) contains various types of freeform questions with different difficulty levels, due to the nature of university exams. We evaluate the performance of various state-of-the-art LLMs on our new benchmark. Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance. Therefore, we provide human expert grading of the LLM outputs on SciEx. We show that the free-form exams in SciEx remain challenging for the current LLMs, where the best LLM only achieves 59.4\% exam grade on average. We also provide detailed comparisons between LLM performance and student performance on SciEx. To enable future evaluation of new LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.
Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading.

------------

`[2406.10621] StructBench: An Autogenerated Benchmark for Evaluating Large Language Model's Ability in Structure-Rich Text Understanding <https://arxiv.org/abs/2406.10621>`__ StructBench:评估大型语言模型对结构丰富文本理解能力的自动生成基准

::

    Sat, 15 Jun 2024 12:48:00 GMT
    Zhouhong Gu, Haoning Ye, Zeyang Zhou, Hongwei Feng, Yanghua Xiao

Given the substantial volumes of structured data held by many companies, enabling Large Language Models (LLMs) to directly understand structured text in non-structured forms could significantly enhance their capabilities across various business scenarios. To this end, we propose evaluation data generation method for assessing LLM's ability in understanding the structure-rich text, which generates structured data of controllable complexity based on manually crafted question templates and generation rules. Building on this generation method, we introduce StructBench, a benchmark comprising 6,032 questions across 8 different structured languages and 29 specific tasks. Furthermore, considering human proficiency in rule-based tasks, we also present StructBench-Hard, which includes 3,016 questions designed to further examine the gap between LLMs and human performance. Results indicate that the best-performing LLM currently achieve an accuracy of 65.0\% on StructBench-Hard, while human accuracy reaches up to 95.7\%. Moreover, while fine-tuning using StructBench can enhance existing LLMs' understanding of all structured languages, it does not necessarily improve performance across all task types. The benchmark and generation codes are open sourced in https://github.com/MikeGu721/StructBench

------------

`[2406.10890] RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models <https://arxiv.org/abs/2406.10890>`__ RWKU:大型语言模型的真实世界知识遗忘基准测试

::

    Sun, 16 Jun 2024 10:47:21 GMT
    Zhuoran Jin, Pengfei Cao, Chenhao Wang, Zhitao He, Hongbang Yuan, Jiachun Li, Yubo Chen, Kang Liu, Jun Zhao

Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible.
(2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model's capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.

------------

`[2406.11020] RUPBench: Benchmarking Reasoning Under Perturbations for Robustness Evaluation in Large Language Models <https://arxiv.org/abs/2406.11020>`__ RUPBench:大型语言模型鲁棒性评估扰动下推理基准测试

::

    Sun, 16 Jun 2024 17:26:44 GMT
    Yuqing Wang, Yun Zhao

With the increasing use of large language models (LLMs), ensuring reliable performance in diverse, real-world environments is essential. Despite their remarkable achievements, LLMs often struggle with adversarial inputs, significantly impacting their effectiveness in practical applications. To systematically understand the robustness of LLMs, we present RUPBench, a comprehensive benchmark designed to evaluate LLM robustness across diverse reasoning tasks. Our benchmark incorporates 15 reasoning datasets, categorized into commonsense, arithmetic, logical, and knowledge-intensive reasoning, and introduces nine types of textual perturbations at lexical, syntactic, and semantic levels. By examining the performance of state-of-the-art LLMs such as GPT-4o, Llama3, Phi-3, and Gemma on both original and perturbed datasets, we provide a detailed analysis of their robustness and error patterns. Our findings highlight that larger models tend to exhibit greater robustness to perturbations. Additionally, common error types are identified through manual inspection, revealing specific challenges faced by LLMs in different reasoning contexts. This work provides insights into areas where LLMs need further improvement to handle diverse and noisy inputs effectively.

------------

`[2406.11328] Are Large Language Models True Healthcare Jacks-of-All-Trades? Benchmarking Across Health Professions Beyond Physician Exams <https://arxiv.org/abs/2406.11328>`__ 大型语言模型是真正的医疗保健万事通吗?超越医师考试的卫生专业基准测试

::

    Mon, 17 Jun 2024 08:40:36 GMT
    Zheheng Luo, Chenhan Yuan, Qianqian Xie, Sophia Ananiadou

Recent advancements in Large Language Models (LLMs) have demonstrated their potential in delivering accurate answers to questions about world knowledge.
Despite this, existing benchmarks for evaluating LLMs in healthcare predominantly focus on medical doctors, leaving other critical healthcare professions underrepresented. To fill this research gap, we introduce the Examinations for Medical Personnel in Chinese (EMPEC), a pioneering large-scale healthcare knowledge benchmark in traditional Chinese. EMPEC consists of 157,803 exam questions across 124 subjects and 20 healthcare professions, including underrepresented occupations like Optometrists and Audiologists. Each question is tagged with its release time and source, ensuring relevance and authenticity. We conducted extensive experiments on 17 LLMs, including proprietary, open-source models, general domain models and medical specific models, evaluating their performance under various settings. Our findings reveal that while leading models like GPT-4 achieve over 75\% accuracy, they still struggle with specialized fields and alternative medicine. Surprisingly, general-purpose LLMs outperformed medical-specific models, and incorporating EMPEC's training data significantly enhanced performance. Additionally, the results on questions released after the models' training cutoff date were consistent with overall performance trends, suggesting that the models' performance on the test set can predict their effectiveness in addressing unseen healthcare-related queries. The transition from traditional to simplified Chinese characters had a negligible impact on model performance, indicating robust linguistic versatility. Our study underscores the importance of expanding benchmarks to cover a broader range of healthcare professions to better assess the applicability of LLMs in real-world healthcare scenarios.

------------

`[2406.11634] The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance <https://arxiv.org/abs/2406.11634>`__ 基准率对LLM基准性能的影响:基准性能中应试策略的消歧

::

    Mon, 17 Jun 2024 15:14:10 GMT
    Kyle Moore, Jesse Roberts, Thao Pham, Oseremhen Ewaleifoh, Doug Fisher

Cloze testing is a common method for measuring the behavior of large language models on a number of benchmark tasks. Using the MMLU dataset, we show that the base-rate probability (BRP) differences across answer tokens are significant and affect task performance ie. guess A if uncertain. We find that counterfactual prompting does sufficiently mitigate the BRP effect. The BRP effect is found to have a similar effect to test taking strategies employed by humans leading to the conflation of task performance and test-taking ability.
We propose the Nvr-X-MMLU task, a variation of MMLU, which helps to disambiguate test-taking ability from task performance and reports the latter.

------------

`[2406.11670] Benchmarking of LLM Detection: Comparing Two Competing Approaches <https://arxiv.org/abs/2406.11670>`__ 

::

    Mon, 17 Jun 2024 15:51:46 GMT
    Thorsten Pr\"ohl, Erik Putzier, R\"udiger Zarnekow

This article gives an overview of the field of LLM text recognition.
Different approaches and implemented detectors for the recognition of LLM-generated text are presented. In addition to discussing the implementations, the article focuses on benchmarking the detectors. Although there are numerous software products for the recognition of LLM-generated text, with a focus on ChatGPT-like LLMs, the quality of the recognition (recognition rate) is not clear. Furthermore, while it can be seen that scientific contributions presenting their novel approaches strive for some kind of comparison with other approaches, the construction and independence of the evaluation dataset is often not comprehensible. As a result, discrepancies in the performance evaluation of LLM detectors are often visible due to the different benchmarking datasets. This article describes the creation of an evaluation dataset and uses this dataset to investigate the different detectors. The selected detectors are benchmarked against each other.

------------

`[2406.11811] RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content <https://arxiv.org/abs/2406.11811>`__ RepLiQA:用于在未见过的参考内容上对llm进行基准测试的问答数据集

::

    Mon, 17 Jun 2024 17:52:54 GMT
    Joao Monteiro, Pierre-Andre Noel, Etienne Marcotte, Sai Rajeswar, Valentina Zantedeschi, David Vazquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian

Large Language Models (LLMs) are trained on vast amounts of data, most of which is automatically scraped from the internet. This data includes encyclopedic documents that harbor a vast amount of general knowledge (e.g., Wikipedia) but also potentially overlap with benchmark datasets used for evaluating LLMs. Consequently, evaluating models on test splits that might have leaked into the training set is prone to misleading conclusions. To foster sound evaluation of language models, we introduce a new test dataset named RepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a collection of five splits of test sets, four of which have not been released to the internet or exposed to LLM APIs prior to this publication. Each sample in RepLiQA comprises (1) a reference document crafted by a human annotator and depicting an imaginary scenario (e.g., a news article) absent from the internet; (2) a question about the document's topic; (3) a ground-truth answer derived directly from the information in the document; and (4) the paragraph extracted from the reference document containing the answer. As such, accurate answers can only be generated if a model can find relevant content within the provided document. We run a large-scale benchmark comprising several state-of-the-art LLMs to uncover differences in performance across models of various types and sizes in a context-conditional language modeling setting.
Released splits of RepLiQA can be found here: https://huggingface.co/datasets/ServiceNow/repliqa.

------------

`[2406.10522] Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning <https://arxiv.org/abs/2406.10522>`__ AI中的幽默:卡通字幕的大规模众包偏好和基准

::

    Sat, 15 Jun 2024 06:26:25 GMT
    Jifan Zhang, Lalit Jain, Yang Guo, Jiayi Chen, Kuan Lok Zhou, Siddharth Suresh, Andrew Wagenmaker, Scott Sievert, Timothy Rogers, Kevin Jamieson, Robert Mankoff, Robert Nowak

We present a novel multimodal preference dataset for creative tasks, consisting of over 250 million human ratings on more than 2.2 million captions, collected through crowdsourcing rating data for The New Yorker's weekly cartoon caption contest over the past eight years. This unique dataset supports the development and evaluation of multimodal large language models and preference-based fine-tuning algorithms for humorous caption generation. We propose novel benchmarks for judging the quality of model-generated captions, utilizing both GPT4 and human judgments to establish ranking-based evaluation strategies. Our experimental results highlight the limitations of current fine-tuning methods, such as RLHF and DPO, when applied to creative tasks.
Furthermore, we demonstrate that even state-of-the-art models like GPT4 and Claude currently underperform top human contestants in generating humorous captions. As we conclude this extensive data collection effort, we release the entire preference dataset to the research community, fostering further advancements in AI humor generation and evaluation.

------------

`[2406.11230] Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models <https://arxiv.org/abs/2406.11230>`__ 多模态大海中针:多模态大型语言模型的长上下文能力基准测试

::

    Mon, 17 Jun 2024 05:54:06 GMT
    Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, Hao Wang

Multimodal Large Language Models (MLLMs) have shown significant promise in various applications, leading to broad interest from researchers and practitioners alike. However, a comprehensive evaluation of their long-context capabilities remains underexplored. To address these gaps, we introduce the MultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to assess the long-context capabilities of MLLMs. Besides multi-image input, we employ image stitching to further increase the input context length, and develop a protocol to automatically generate labels for sub-image level retrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their capability to locate a target sub-image (needle) within a set of images (haystack) based on textual instructions and descriptions of image contents.
This setup necessitates an advanced understanding of extensive visual contexts and effective information retrieval within long-context image inputs. With this benchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and open-source models. The findings reveal that GPT-4o consistently surpasses other models in long-context scenarios, but suffers from hallucination problems in negative samples, i.e., when needles are not in the haystacks. Our comprehensive long-context evaluation of MLLMs also sheds lights on the considerable performance gap between API-based and open-source models. All the code, data, and instructions required to reproduce the main results are available at https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.

------------

`[2406.11547] GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations <https://arxiv.org/abs/2406.11547>`__ gecbench:性别控制的文本数据集和解释中量化偏见的基准

::

    Mon, 17 Jun 2024 13:44:37 GMT
    Rick Wilming, Artur Dox, Hjalmar Schulz, Marta Oliveira, Benedict Clark, Stefan Haufe

Large pre-trained language models have become popular for many applications and form an important backbone of many downstream tasks in natural language processing (NLP). Applying 'explainable artificial intelligence' (XAI) techniques to enrich such models' outputs is considered crucial for assuring their quality and shedding light on their inner workings. However, large language models are trained on a plethora of data containing a variety of biases, such as gender biases, affecting model weights and, potentially, behavior. Currently, it is unclear to what extent such biases also impact model explanations in possibly unfavorable ways. We create a gender-controlled text dataset, GECO, in which otherwise identical sentences appear in male and female forms. This gives rise to ground-truth 'world explanations' for gender classification tasks, enabling the objective evaluation of the correctness of XAI methods. We also provide GECOBench, a rigorous quantitative evaluation framework benchmarking popular XAI methods, applying them to pre-trained language models fine-tuned to different degrees. This allows us to investigate how pre-training induces undesirable bias in model explanations and to what extent fine-tuning can mitigate such explanation bias. We show a clear dependency between explanation performance and the number of fine-tuned layers, where XAI methods are observed to particularly benefit from fine-tuning or complete retraining of embedding layers. Remarkably, this relationship holds for models achieving similar classification performance on the same task. With that, we highlight the utility of the proposed gender-controlled dataset and novel benchmarking approach for research and development of novel XAI methods.
All code including dataset generation, model training, evaluation and visualization is available at: https://github.com/braindatalab/gecobench

------------

`[2406.11612] Long Code Arena: a Set of Benchmarks for Long-Context Code Models <https://arxiv.org/abs/2406.11612>`__ 长代码竞技场:长上下文代码模型的一组基准

::

    Mon, 17 Jun 2024 14:58:29 GMT
    Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, Timofey Bryksin

Nowadays, the fields of code and natural language processing are evolving rapidly. In particular, models become better at processing long context windows - supported context sizes have increased by orders of magnitude over the last few years. However, there is a shortage of benchmarks for code processing that go beyond a single file of context, while the most popular ones are limited to a single method. With this work, we aim to close this gap by introducing Long Code Arena, a suite of six benchmarks for code processing tasks that require project-wide context. These tasks cover different aspects of code processing: library-based code generation, CI builds repair, project-level code completion, commit message generation, bug localization, and module summarization. For each task, we provide a manually verified dataset for testing, an evaluation suite, and open-source baseline solutions based on popular LLMs to showcase the usage of the dataset and to simplify adoption by other researchers. We publish the benchmark page on HuggingFace Spaces with the leaderboard, links to HuggingFace Hub for all the datasets, and link to the GitHub repository with baselines: https://huggingface.co/spaces/JetBrains-Research/long-code-arena.

------------

`[2312.11865] Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach <https://arxiv.org/abs/2312.11865>`__ 大型语言模型玩星际争霸2:基准和摘要链方法

::

    replaced with revised version Mon, 17 Jun 2024 09:04:43 GMT
    Submission history From: Weiyu Ma [view email]
    [v1] Tue, 19 Dec 2023 05:27:16 UTC (38,368 KB)
    [v2] Mon, 17 Jun 2024 09:04:43 UTC (12,766 KB)
    Weiyu Ma, Qirui Mi, Xue Yan, Yuqiao Wu, Runji Lin, Haifeng Zhang, Jun Wang

StarCraft II is a challenging benchmark for AI agents due to the necessity of both precise micro level operations and strategic macro awareness. Previous works, such as Alphastar and SCC, achieve impressive performance on tackling StarCraft II , however, still exhibit deficiencies in long term strategic planning and strategy interpretability. Emerging large language model (LLM) agents, such as Voyage and MetaGPT, presents the immense potential in solving intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs on StarCraft II, a highly complex RTS this http URL conveniently take full advantage of LLMs` reasoning abilities, we first develop textual StratCraft II environment, called TextStarCraft II, which LLM agent can interact. Secondly, we propose a Chain of Summarization method, including single frame summarization for processing raw observations and multi frame summarization for analyzing game information, providing command recommendations, and generating strategic decisions. Our experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs`s mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in game performance of LLM agents, encompassing aspects like win rate and the impact of Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the relevant knowledge and complex planning abilities needed to address StarCraft II scenarios; 2. Human experts consider the performance of LLM agents to be close to that of an average player who has played StarCraft II for eight years; 3. LLM agents are capable of defeating the built in AI at the Harder(Lv5) difficulty level. We have open sourced the code and released demo videos of LLM agent playing StarCraft II.

------------

`[2311.09861] ConceptPsy:A Benchmark Suite with Conceptual Comprehensiveness in Psychology <https://arxiv.org/abs/2311.09861>`__ conceptsy:心理学中具有概念全面性的基准套件

::

    replaced with revised version Sun, 16 Jun 2024 11:33:03 GMT
    Submission history From: Junlei Zhang [view email]
    [v1] Thu, 16 Nov 2023 12:43:18 UTC (1,432 KB)
    [v2] Fri, 17 Nov 2023 03:17:05 UTC (1,432 KB)
    [v3] Thu, 13 Jun 2024 13:56:20 UTC (9,461 KB)
    [v4] Sun, 16 Jun 2024 11:33:03 UTC (8,303 KB)
    Junlei Zhang, Hongliang He, Nirui Song, Zhanchao Zhou, Shuyuan He, Shuai Zhang, Huachuan Qiu, Anqi Li, Yong Dai, Lizhi Ma, Zhenzhong Lan

The critical field of psychology necessitates a comprehensive benchmark to enhance the evaluation and development of domain-specific Large Language Models (LLMs). Existing MMLU-type benchmarks, such as C-EVAL and CMMLU, include psychology-related subjects, but their limited number of questions and lack of systematic concept sampling strategies mean they cannot cover the concepts required in psychology. Consequently, despite their broad subject coverage, these benchmarks lack the necessary depth in the psychology domain, making them inadequate as psychology-specific evaluation suite. To address this issue, this paper presents ConceptPsy, designed to evaluate Chinese complex reasoning and knowledge abilities in psychology. ConceptPsy includes 12 core subjects and 1383 manually collected concepts. Specifically, we prompt GPT-4 to generate questions for each concept using carefully designed diverse prompts and hire professional psychologists to review these questions. To help to understand the fine-grained performances and enhance the weaknesses, we annotate each question with a chapter label and provide chapter-wise accuracy. Based on ConceptPsy, we evaluate a broad range of LLMs. We observe that, although some LLMs achieve similar accuracies on overall performances, they exhibit significant performance variations across different psychology concepts, even when they are models from the same series. We hope our work can facilitate the development of LLMs in the field of psychology.

------------

`[2312.17115] How Far Are LLMs from Believable AI? A Benchmark for Evaluating the Believability of Human Behavior Simulation <https://arxiv.org/abs/2312.17115>`__ llm离可信的AI有多远?评估人类行为模拟可信性的基准

::

    replaced with revised version Sat, 15 Jun 2024 14:08:30 GMT
    Submission history From: Yang Xiao [view email]
    [v1] Thu, 28 Dec 2023 16:51:11 UTC (2,741 KB)
    [v2] Sat, 15 Jun 2024 14:08:30 UTC (3,622 KB)
    Yang Xiao, Yi Cheng, Jinlan Fu, Jiashuo Wang, Wenjie Li, Pengfei Liu

In recent years, AI has demonstrated remarkable capabilities in simulating human behaviors, particularly those implemented with large language models (LLMs). However, due to the lack of systematic evaluation of LLMs' simulated behaviors, the believability of LLMs among humans remains ambiguous, i.e., it is unclear which behaviors of LLMs are convincingly human-like and which need further improvements. In this work, we design SimulateBench to evaluate the believability of LLMs when simulating human behaviors. In specific, we evaluate the believability of LLMs based on two critical dimensions: 1) consistency: the extent to which LLMs can behave consistently with the given information of a human to simulate; and 2) robustness: the ability of LLMs' simulated behaviors to remain robust when faced with perturbations. SimulateBench includes 65 character profiles and a total of 8,400 questions to examine LLMs' simulated behaviors. Based on SimulateBench, we evaluate the performances of 10 widely used LLMs when simulating characters. The experimental results reveal that current LLMs struggle to align their behaviors with assigned characters and are vulnerable to perturbations in certain factors.

------------

`[2402.11432] Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning <https://arxiv.org/abs/2402.11432>`__ 欺骗检测能更深入吗?用于欺骗推理的数据集、评估和基准

::

    replaced with revised version Mon, 17 Jun 2024 02:57:09 GMT
    Submission history From: Zheng Lian [view email]
    [v1] Sun, 18 Feb 2024 02:52:54 UTC (704 KB)
    [v2] Mon, 17 Jun 2024 02:57:09 UTC (707 KB)
    Kang Chen, Zheng Lian, Haiyang Sun, Bin Liu, Jianhua Tao

Deception detection has attracted increasing attention due to its importance in real-world scenarios. Its main goal is to detect deceptive behaviors from multimodal clues such as gestures, facial expressions, prosody, etc. However, these bases are usually subjective and related to personal habits. Therefore, we extend deception detection to deception reasoning, further providing objective evidence to support subjective judgment. Specifically, we provide potential lies and basic facts and then analyze why this sentence may be a lie by combining factual inconsistencies and intent behind them. Compared with deception detection, this task is more applicable to real-world scenarios. For example, in interrogation, the police should judge whether a person is lying based on solid evidence. This paper presents our initial attempts at this task, including constructing a dataset and defining evaluation metrics. Meanwhile, this task can serve as a benchmark for evaluating the complex reasoning capability of large language models. Code and data will be made publicly available.

------------

`[2403.00896] DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models <https://arxiv.org/abs/2403.00896>`__ DiaHalu:大型语言模型对话级幻觉评估基准

::

    replaced with revised version Mon, 17 Jun 2024 15:11:20 GMT
    Submission history From: Kedi Chen [view email]
    [v1] Fri, 1 Mar 2024 15:38:55 UTC (440 KB)
    [v2] Mon, 17 Jun 2024 15:11:20 UTC (463 KB)
    Kedi Chen and Qin Chen and Jie Zhou and Yishen He and Liang He

Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-machine interaction scenarios. Finally, professional scholars annotate all the samples in the dataset. DiaHalu covers four common multi-turn dialogue domains and five hallucination subtypes, extended from factuality and faithfulness hallucination. Experiments through some well-known LLMs and detection methods on the dataset show that DiaHalu is a challenging benchmark, holding significant value for further research.

------------

`[2403.01976] SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis <https://arxiv.org/abs/2403.01976>`__ sciassessment: LLM科学文献分析能力的基准测试

::

    replaced with revised version Sat, 15 Jun 2024 15:45:47 GMT
    Submission history From: Hengxing Cai [view email]
    [v1] Mon, 4 Mar 2024 12:19:28 UTC (4,202 KB)
    [v2] Fri, 15 Mar 2024 13:27:31 UTC (8,174 KB)
    [v3] Sat, 15 Jun 2024 15:45:47 UTC (7,855 KB)
    Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Mingjun Xu, Jin Huang, Fang Xi, Jiaxi Zhuang, Yuqi Yin, Yaqi Li, Changhong Chen, Zheng Cheng, Zifeng Zhao, Linfeng Zhang, Guolin Ke

Recent breakthroughs in Large Language Models (LLMs) have revolutionized natural language understanding and generation, sparking significant interest in applying them to scientific literature analysis. However, existing benchmarks fail to adequately evaluate the proficiency of LLMs in this domain, particularly in scenarios requiring higher-level abilities beyond mere memorization and the handling of multimodal data. In response to this gap, we introduce SciAssess, a benchmark specifically designed for the comprehensive evaluation of LLMs in scientific literature analysis. SciAssess aims to thoroughly assess the efficacy of LLMs by focusing on their capabilities in Memorization (L1), Comprehension (L2), and Analysis \& Reasoning (L3). It encompasses a variety of tasks drawn from diverse scientific fields, including fundamental science, alloy materials, biomedicine, drug discovery, and organic materials. To ensure the reliability of SciAssess, rigorous quality control measures have been implemented, ensuring accuracy, anonymization, and compliance with copyright standards. SciAssess evaluates 11 LLMs, including GPT, Claude, and Gemini, highlighting their strengths and areas for improvement. This evaluation supports the ongoing development of LLM applications in the analysis of scientific literature. SciAssess and its resources are available at \url{this https URL}.

------------

`[2403.12766] NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens <https://arxiv.org/abs/2403.12766>`__ 

::

    replaced with revised version Mon, 17 Jun 2024 13:53:15 GMT
    Submission history From: Cunxiang Wang [view email]
    [v1] Mon, 18 Mar 2024 17:32:32 UTC (12,471 KB)
    [v2] Mon, 17 Jun 2024 13:53:15 UTC (9,026 KB)
    Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Xiangkun Hu, Zheng Zhang, Qian Wang, Yue Zhang

The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with an average length more than 200,000 tokens. The results underscore the necessity for further advancements in LLMs to improve their long-context comprehension.

------------

`[2404.04671] PhyloLM : Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks <https://arxiv.org/abs/2404.04671>`__ PhyloLM:推断大型语言模型的发展史并预测其在基准测试中的表现

::

    replaced with revised version Sun, 16 Jun 2024 14:39:20 GMT
    Submission history From: Nicolas Yax [view email]
    [v1] Sat, 6 Apr 2024 16:16:30 UTC (4,036 KB)
    [v2] Thu, 23 May 2024 16:03:29 UTC (11,764 KB)
    [v3] Sun, 16 Jun 2024 14:39:20 UTC (11,764 KB)
    Nicolas Yax, Pierre-Yves Oudeyer, Stefano Palminteri

This paper introduces PhyloLM, a method adapting phylogenetic algorithms to Large Language Models (LLMs) to explore whether and how they relate to each other and to predict their performance characteristics. Our method calculates a phylogenetic distance metrics based on the similarity of LLMs' output. The resulting metric is then used to construct dendrograms, which satisfactorily capture known relationships across a set of 111 open-source and 45 closed models. Furthermore, our phylogenetic distance predicts performance in standard benchmarks, thus demonstrating its functional validity and paving the way for a time and cost-effective estimation of LLM capabilities. To sum up, by translating population genetic concepts to machine learning, we propose and validate a tool to evaluate LLM development, relationships and capabilities, even in the absence of transparent training information.

------------

`[2406.05654] DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation <https://arxiv.org/abs/2406.05654>`__ 

::

    replaced with revised version Mon, 17 Jun 2024 03:01:39 GMT
    Submission history From: Shuting Wang [view email]
    [v1] Sun, 9 Jun 2024 05:33:51 UTC (609 KB)
    [v2] Mon, 17 Jun 2024 03:01:39 UTC (609 KB)
    Shuting Wang, Jiongnan Liu, Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang, Yutao Zhu, Zhicheng Dou

Retrieval-Augmented Generation (RAG) offers a promising solution to address various limitations of Large Language Models (LLMs), such as hallucination and difficulties in keeping up with real-time updates. This approach is particularly critical in expert and domain-specific applications where LLMs struggle to cover expert knowledge. Therefore, evaluating RAG models in such scenarios is crucial, yet current studies often rely on general knowledge sources like Wikipedia to assess the models' abilities in solving common-sense problems. In this paper, we evaluated LLMs by RAG settings in a domain-specific context, college enrollment. We identified six required abilities for RAG models, including the ability in conversational RAG, analyzing structural information, faithfulness to external knowledge, denoising, solving time-sensitive problems, and understanding multi-document interactions. Each ability has an associated dataset with shared corpora to evaluate the RAG models' performance. We evaluated popular LLMs such as Llama, Baichuan, ChatGLM, and GPT models. Experimental results indicate that existing closed-book LLMs struggle with domain-specific questions, highlighting the need for RAG models to solve expert problems. Moreover, there is room for RAG models to improve their abilities in comprehending conversational history, analyzing structural information, denoising, processing multi-document interactions, and faithfulness in expert knowledge. We expect future studies could solve these problems better.

------------

`[2406.06647] How Efficient is LLM-Generated Code? A Rigorous & High-Standard Benchmark <https://arxiv.org/abs/2406.06647>`__ llm生成的代码的效率如何?一个严格且高标准的基准

::

    replaced with revised version Sun, 16 Jun 2024 19:34:04 GMT
    Submission history From: Ruizhong Qiu [view email]
    [v1] Mon, 10 Jun 2024 04:19:20 UTC (97 KB)
    [v2] Sun, 16 Jun 2024 19:34:04 UTC (97 KB)
    Ruizhong Qiu, Weiliang Will Zeng, Hanghang Tong, James Ezick, Christopher Lott

The emergence of large language models (LLMs) has significantly pushed the frontiers of program synthesis. Advancement of LLM-based program synthesis calls for a thorough evaluation of LLM-generated code. Most evaluation frameworks focus on the (functional) correctness of generated code; efficiency, as an important measure of code quality, has been overlooked in existing evaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a rigorous and high-standard benchmark for evaluating the capability of LLMs in generating efficient code. Firstly, we propose a new efficiency metric called eff@k, which generalizes the pass@k metric from correctness to efficiency and appropriately handles right-censored execution time. Furthermore, we derive an unbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we also provide a numerically stable implementation for the new estimator. Secondly, to set a high-standard for efficiency evaluation, we employ a human expert to design best algorithms and implementations as our reference solutions of efficiency, many of which are much more efficient than existing canonical solutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous evaluation, we employ a human expert to curate strong test case generators to filter out wrong code and differentiate suboptimal algorithms. An extensive study across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still fall short of generating expert-level efficient code. Using two subsets of our problem set, we demonstrate that such deficiency is because current LLMs struggle in designing advanced algorithms and are barely aware of implementation optimization. Our benchmark is publicly available at this https URL .

------------

`[2405.21075] Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis <https://arxiv.org/abs/2405.21075>`__ Video- mme:视频分析中第一个多模态llm的综合评估基准

::

    replaced with revised version Sun, 16 Jun 2024 15:49:12 GMT
    Submission history From: Chaoyou Fu [view email]
    [v1] Fri, 31 May 2024 17:59:47 UTC (3,403 KB)
    [v2] Sun, 16 Jun 2024 15:49:12 UTC (3,406 KB)
    Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, Xing Sun

In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However, the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs in processing sequential visual data is still insufficiently explored, highlighting the absence of a comprehensive, high-quality assessment of their performance. In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. 900 videos with a total of 254 hours are manually selected and annotated by repeatedly viewing all the video content, resulting in 2,700 question-answer pairs. With Video-MME, we extensively evaluate various state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as open-source image models like InternVL-Chat-V1.5 and video models like LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data. Project Page: this https URL

------------

`[2404.01318] JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models <https://arxiv.org/abs/2404.01318>`__ 

::

    replaced with revised version Sun, 16 Jun 2024 15:58:44 GMT
    Submission history From: Maksym Andriushchenko [view email]
    [v1] Thu, 28 Mar 2024 02:44:02 UTC (1,333 KB)
    [v2] Tue, 23 Apr 2024 16:41:42 UTC (1,335 KB)
    [v3] Sun, 16 Jun 2024 15:58:44 UTC (218 KB)
    Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, Eric Wong

Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors -- both original and sourced from prior work -- which align with OpenAI's usage policies; (3) a standardized evaluation framework at this https URL that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at this https URL that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.

------------

---------------
Accelerate (23)
---------------

`[2406.10382] Efficient Prompting for LLM-based Generative Internet of Things <https://arxiv.org/abs/2406.10382>`__ 基于llm的生成式物联网的有效提示

::

    Fri, 14 Jun 2024 19:24:00 GMT
    Bin Xiao, Burak Kantarci, Jiawen Kang, Dusit Niyato, Mohsen Guizani

Large language models (LLMs) have demonstrated remarkable capacities on various tasks, and integrating the capacities of LLMs into the Internet of Things (IoT) applications has drawn much research attention recently. Due to security concerns, many institutions avoid accessing state-of-the-art commercial LLM services, requiring the deployment and utilization of open-source LLMs in a local network setting. However, open-source LLMs usually have more limitations regarding their performance, such as their arithmetic calculation and reasoning capacities, and practical systems of applying LLMs to IoT have yet to be well-explored. Therefore, we propose a text-based generative IoT (GIoT) system deployed in the local network setting in this study. To alleviate the limitations of LLMs and provide service with competitive performance, we apply prompt engineering methods to enhance the capacities of the open-source LLMs, design a Prompt Management Module and a Post-processing Module to manage the tailored prompts for different tasks and process the results generated by the LLMs. To demonstrate the effectiveness of the proposed system, we discuss a challenging Table Question Answering (Table-QA) task as a case study of the proposed system, as tabular data is usually more challenging than plain text because of their complex structures, heterogeneous data types and sometimes huge sizes. We conduct comprehensive experiments on two popular Table-QA datasets, and the results show that our proposal can achieve competitive performance compared with state-of-the-art LLMs, demonstrating that the proposed LLM-based GIoT system can provide competitive performance with tailored prompting methods and is easily extensible to new tasks without training.

------------

`[2406.10393] EWEK-QA: Enhanced Web and Efficient Knowledge Graph Retrieval for Citation-based Question Answering Systems <https://arxiv.org/abs/2406.10393>`__ EWEK-QA:面向引文问答系统的增强Web和高效知识图谱检索

::

    Fri, 14 Jun 2024 19:40:38 GMT
    Mohammad Dehghan, Mohammad Ali Alomrani, Sunyam Bagga, David Alfonso-Hermelo, Khalil Bibi, Abbas Ghaddar, Yingxue Zhang, Xiaoguang Li, Jianye Hao, Qun Liu, Jimmy Lin, Boxing Chen, Prasanna Parthasarathi, Mahdi Biparva, Mehdi Rezagholizadeh

The emerging citation-based QA systems are gaining more attention especially in generative AI search applications. The importance of extracted knowledge provided to these systems is vital from both accuracy (completeness of information) and efficiency (extracting the information in a timely manner). In this regard, citation-based QA systems are suffering from two shortcomings.
First, they usually rely only on web as a source of extracted knowledge and adding other external knowledge sources can hamper the efficiency of the system. Second, web-retrieved contents are usually obtained by some simple heuristics such as fixed length or breakpoints which might lead to splitting information into pieces. To mitigate these issues, we propose our enhanced web and efficient knowledge graph (KG) retrieval solution (EWEK-QA) to enrich the content of the extracted knowledge fed to the system. This has been done through designing an adaptive web retriever and incorporating KGs triples in an efficient manner. We demonstrate the effectiveness of EWEK-QA over the open-source state-of-the-art (SoTA) web-based and KG baseline models using a comprehensive set of quantitative and human evaluation experiments. Our model is able to: first, improve the web-retriever baseline in terms of extracting more relevant passages (>20\%), the coverage of answer span (>25\%) and self containment (>35\%); second, obtain and integrate KG triples into its pipeline very efficiently (by avoiding any LLM calls) to outperform the web-only and KG-only SoTA baselines significantly in 7 quantitative QA tasks and our human evaluation.

------------

`[2406.10471] Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts <https://arxiv.org/abs/2406.10471>`__ 个性化部件:通过协作实现高效个性化大型语言模型

::

    Sat, 15 Jun 2024 02:26:18 GMT
    Zhaoxuan Tan, Zheyuan Liu, Meng Jiang

Personalized large language models (LLMs) aim to tailor interactions, content, and recommendations to individual user preferences. While parameter-efficient fine-tuning (PEFT) methods excel in performance and generalization, they are costly and limit communal benefits when used individually. To this end, we introduce Personalized Pieces (Per-Pcs), a framework that allows users to safely share and assemble personalized PEFT efficiently with collaborative efforts. Per-Pcs involves selecting sharers, breaking their PEFT into pieces, and training gates for each piece. These pieces are added to a pool, from which target users can select and assemble personalized PEFT using their history data. This approach preserves privacy and enables fine-grained user modeling without excessive storage and computation demands. Experimental results show Per-Pcs outperforms non-personalized and PEFT retrieval baselines, offering performance comparable to OPPU with significantly lower resource use across six tasks. Further analysis highlights Per-Pcs's robustness concerning sharer count and selection strategy, pieces sharing ratio, and scalability in computation time and storage space. Per-Pcs's modularity promotes safe sharing, making LLM personalization more efficient, effective, and widely accessible through collaborative efforts.

------------

`[2406.10660] DIEKAE: Difference Injection for Efficient Knowledge Augmentation and Editing of Large Language Models <https://arxiv.org/abs/2406.10660>`__ 

::

    Sat, 15 Jun 2024 14:57:39 GMT
    Alessio Galatolo, Meriem Beloucif, Katie Winkle

Pretrained Language Models (PLMs) store extensive knowledge within their weights, enabling them to recall vast amount of information. However, relying on this parametric knowledge brings some limitations such as outdated information or gaps in the training data. This work addresses these problems by distinguish between two separate solutions: knowledge editing and knowledge augmentation. We introduce Difference Injection for Efficient Knowledge Augmentation and Editing (DIEK\AE), a new method that decouples knowledge processing from the PLM (LLaMA2-7B, in particular) by adopting a series of encoders. These encoders handle external knowledge and inject it into the PLM layers, significantly reducing computational costs and improving performance of the PLM. We propose a novel training technique for these encoders that does not require back-propagation through the PLM, thus greatly reducing the memory and time required to train them. Our findings demonstrate how our method is faster and more efficient compared to multiple baselines in knowledge augmentation and editing during both training and inference. We have released our code and data at https://github.com/alessioGalatolo/DIEKAE.

------------

`[2406.10774] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference <https://arxiv.org/abs/2406.10774>`__ Quest:基于查询感知的高效长上下文LLM推理稀疏性

::

    Sun, 16 Jun 2024 01:33:02 GMT
    Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han

As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention.
Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .

------------

`[2406.10785] ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation <https://arxiv.org/abs/2406.10785>`__ ShareLoRA:基于共享低秩自适应的参数高效鲁棒大型语言模型微调

::

    Sun, 16 Jun 2024 02:52:28 GMT
    Yurun Song, Junchen Zhao, Ian G. Harris, Sangeetha Abdu Jyothi

This study introduces an approach to optimize Parameter Efficient Fine Tuning (PEFT) for Pretrained Language Models (PLMs) by implementing a Shared Low Rank Adaptation (ShareLoRA). By strategically deploying ShareLoRA across different layers and adapting it for the Query, Key, and Value components of self-attention layers, we achieve a substantial reduction in the number of training parameters and memory usage. Importantly, ShareLoRA not only maintains model performance but also exhibits robustness in both classification and generation tasks across a variety of models, including RoBERTa, GPT-2, LLaMA and LLaMA2. It demonstrates superior transfer learning capabilities compared to standard LoRA applications and mitigates overfitting by sharing weights across layers. Our findings affirm that ShareLoRA effectively boosts parameter efficiency while ensuring scalable and high-quality performance across different language model architectures.

------------

`[2406.10882] SCAR: Efficient Instruction-Tuning for Large Language Models via Style Consistency-Aware Response Ranking <https://arxiv.org/abs/2406.10882>`__ 

::

    Sun, 16 Jun 2024 10:10:37 GMT
    Zhuang Li, Yuncheng Hua, Thuy-Trang Vu, Haolan Zhan, Lizhen Qu, Gholamreza Haffari

Recent studies have shown that maintaining a consistent response style by human experts and enhancing data quality in training sets can significantly improve the performance of fine-tuned Large Language Models (LLMs) while reducing the number of training examples needed. However, the precise definition of style and the relationship between style, data quality, and LLM performance remains unclear. This research decomposes response style into presentation and composition styles and finds that, among training data of similar quality, those with higher style consistency lead to better LLM performance. Inspired by this, we introduce Style Consistency-Aware Response Ranking (SCAR), which automatically prioritizes instruction-response pairs in the training set based on their response stylistic consistency. By selecting the most style-consistent examples, ranging from the top 25% to 0.7% of the full dataset, the fine-tuned LLMs can match or even surpass the performance of models trained on the entire dataset in coding and open-ended question-answering benchmarks. Code and data are available at https://github.com/zhuang-li/SCAR .

------------

`[2406.11357] $\textit{Refiner}$: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities <https://arxiv.org/abs/2406.11357>`__ $\textit{Refiner}$有效地重构检索内容以提高问答能力

::

    Mon, 17 Jun 2024 09:25:10 GMT
    Zhonghao Li, Xuming Hu, Aiwei Liu, Kening Zheng, Sirui Huang, Hui Xiong

Large Language Models (LLMs) are limited by their parametric knowledge, leading to hallucinations in knowledge-extensive tasks. To address this, Retrieval-Augmented Generation (RAG) incorporates external document chunks to expand LLM knowledge. Furthermore, compressing information from document chunks through extraction or summarization can improve LLM performance. Nonetheless, LLMs still struggle to notice and utilize scattered key information, a problem known as the "lost-in-the-middle" syndrome. Therefore, we typically need to restructure the content for LLM to recognize the key information. We propose $\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that operates in the post-retrieval process of RAG. $\textit{Refiner}$ leverages a single decoder-only LLM to adaptively extract query-relevant contents verbatim along with the necessary context, and section them based on their interconnectedness, thereby highlights information distinction, and aligns downstream LLMs with the original context effectively. Experiments show that a trained $\textit{Refiner}$ (with 7B parameters) exhibits significant gain to downstream LLM in improving answer accuracy, and outperforms other state-of-the-art advanced RAG and concurrent compressing approaches in various single-hop and multi-hop QA tasks. Notably, $\textit{Refiner}$ achieves a 80.5% tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared to the next best solution. $\textit{Refiner}$ is a plug-and-play solution that can be seamlessly integrated with RAG systems, facilitating its application across diverse open-source frameworks.

------------

`[2406.11430] A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression <https://arxiv.org/abs/2406.11430>`__ 一种简单有效的基于$L_2$范数的KV Cache压缩策略

::

    Mon, 17 Jun 2024 11:35:16 GMT
    Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini

The deployment of large language models (LLMs) is often hindered by the extensive memory requirements of the Key-Value (KV) cache, especially as context lengths increase. Existing approaches to reduce the KV cache size involve either fine-tuning the model to learn a compression strategy or leveraging attention scores to reduce the sequence length. We analyse the attention distributions in decoder-only Transformers-based models and observe that attention allocation patterns stay consistent across most layers.
Surprisingly, we find a clear correlation between the $L_2$ and the attention scores over cached KV pairs, where a low $L_2$ of a key embedding usually leads to a high attention score during decoding. This finding indicates that the influence of a KV pair is potentially determined by the key embedding itself before being queried. Based on this observation, we compress the KV cache based on the $L_2$ of key embeddings. Our experimental results show that this simple strategy can reduce the KV cache size by 50% on language modelling and needle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing accuracy.

------------

`[2406.11568] Towards an End-to-End Framework for Invasive Brain Signal Decoding with Large Language Models <https://arxiv.org/abs/2406.11568>`__ 基于大型语言模型的入侵性脑信号解码端到端框架研究

::

    Mon, 17 Jun 2024 14:04:18 GMT
    Sheng Feng, Heyang Liu, Yu Wang, Yanfeng Wang

In this paper, we introduce a groundbreaking end-to-end (E2E) framework for decoding invasive brain signals, marking a significant advancement in the field of speech neuroprosthesis. Our methodology leverages the comprehensive reasoning abilities of large language models (LLMs) to facilitate direct decoding. By fully integrating LLMs, we achieve results comparable to the state-of-the-art cascade models. Our findings underscore the immense potential of E2E frameworks in speech neuroprosthesis, particularly as the technology behind brain-computer interfaces (BCIs) and the availability of relevant datasets continue to evolve. This work not only showcases the efficacy of combining LLMs with E2E decoding for enhancing speech neuroprosthesis but also sets a new direction for future research in BCI applications, underscoring the impact of LLMs in decoding complex neural signals for communication restoration. Code will be made available at https://github.com/FsFrancis15/BrainLLM.

------------

`[2406.11640] Linear Bellman Completeness Suffices for Efficient Online Reinforcement Learning with Few Actions <https://arxiv.org/abs/2406.11640>`__ 

::

    Mon, 17 Jun 2024 15:24:49 GMT
    Noah Golowich and Ankur Moitra

One of the most natural approaches to reinforcement learning (RL) with function approximation is value iteration, which inductively generates approximations to the optimal value function by solving a sequence of regression problems. To ensure the success of value iteration, it is typically assumed that Bellman completeness holds, which ensures that these regression problems are well-specified. We study the problem of learning an optimal policy under Bellman completeness in the online model of RL with linear function approximation. In the linear setting, while statistically efficient algorithms are known under Bellman completeness (e.g., Jiang et al. (2017); Zanette et al.
(2020)), these algorithms all rely on the principle of global optimism which requires solving a nonconvex optimization problem. In particular, it has remained open as to whether computationally efficient algorithms exist. In this paper we give the first polynomial-time algorithm for RL under linear Bellman completeness when the number of actions is any constant.

------------

`[2406.11810] Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics <https://arxiv.org/abs/2406.11810>`__ 确定性动力学线性Bellman完备性下的计算高效RL

::

    Mon, 17 Jun 2024 17:52:38 GMT
    Runzhe Wu, Ayush Sekhari, Akshay Krishnamurthy, Wen Sun

We study computationally and statistically efficient Reinforcement Learning algorithms for the linear Bellman Complete setting, a setting that uses linear function approximation to capture value functions and unifies existing models like linear Markov Decision Processes (MDP) and Linear Quadratic Regulators (LQR). While it is known from the prior works that this setting is statistically tractable, it remained open whether a computationally efficient algorithm exists. Our work provides a computationally efficient algorithm for the linear Bellman complete setting that works for MDPs with large action spaces, random initial states, and random rewards but relies on the underlying dynamics to be deterministic. Our approach is based on randomization: we inject random noise into least square regression problems to perform optimistic value iteration. Our key technical contribution is to carefully design the noise to only act in the null space of the training data to ensure optimism while circumventing a subtle error amplification issue.

------------

`[2406.11087] MemDPT: Differential Privacy for Memory Efficient Language Models <https://arxiv.org/abs/2406.11087>`__ MemDPT:面向内存高效语言模型的差分隐私保护

::

    Sun, 16 Jun 2024 22:11:41 GMT
    Yanming Liu, Xinyue Peng, Jiannan Cao, Yuwei Zhang, Chen Ma, Songhang Deng, Mengchen Fu, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du

Large language models have consistently demonstrated remarkable performance across a wide spectrum of applications. Nonetheless, the deployment of these models can inadvertently expose user privacy to potential risks. The substantial memory demands of these models during training represent a significant resource consumption challenge. The sheer size of these models imposes a considerable burden on memory resources, which is a matter of significant concern in practice. In this paper, we present an innovative training framework MemDPT that not only reduces the memory cost of large language models but also places a strong emphasis on safeguarding user data privacy. MemDPT provides edge network and reverse network designs to accommodate various differential privacy memory-efficient fine-tuning schemes.
Our approach not only achieves $2 \sim 3 \times$ memory optimization but also provides robust privacy protection, ensuring that user data remains secure and confidential. Extensive experiments have demonstrated that MemDPT can effectively provide differential privacy efficient fine-tuning across various task scenarios.

------------

`[2402.06925] A Thorough Examination of Decoding Methods in the Era of LLMs <https://arxiv.org/abs/2402.06925>`__ 对llm时代解码方法的彻底检查

::

    replaced with revised version Mon, 17 Jun 2024 14:34:05 GMT
    Submission history From: Chufan Shi [view email]
    [v1] Sat, 10 Feb 2024 11:14:53 UTC (145 KB)
    [v2] Mon, 17 Jun 2024 14:34:05 UTC (149 KB)
    Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, Wai Lam

Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts.

------------

`[2402.10552] Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models <https://arxiv.org/abs/2402.10552>`__ 会话模拟:基于大型语言模型的高效同声翻译

::

    replaced with revised version Sun, 16 Jun 2024 09:25:13 GMT
    Submission history From: Minghan Wang [view email]
    [v1] Fri, 16 Feb 2024 10:32:16 UTC (7,921 KB)
    [v2] Sun, 16 Jun 2024 09:25:13 UTC (8,047 KB)
    Minghan Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari

Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.

------------

`[2402.10712] An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference <https://arxiv.org/abs/2402.10712>`__ 面向高效语言模型推理的跨语言词汇自适应实证研究

::

    replaced with revised version Mon, 17 Jun 2024 12:00:02 GMT
    Submission history From: Atsuki Yamaguchi [view email]
    [v1] Fri, 16 Feb 2024 14:15:15 UTC (634 KB)
    [v2] Mon, 17 Jun 2024 12:00:02 UTC (879 KB)
    Atsuki Yamaguchi, Aline Villavicencio, Nikolaos Aletras

The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation (CVA) methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of five CVA methods on four generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find that CVA substantially contributes to LLM inference speedups of up to 271.5\%. We also show that adapting LLMs that have been pre-trained on more balanced multilingual data results in downstream performance comparable to the original models.

------------

`[2402.11621] Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Detection <https://arxiv.org/abs/2402.11621>`__ 

::

    replaced with revised version Sat, 15 Jun 2024 23:20:15 GMT
    Submission history From: Valeria Pastorino [view email]
    [v1] Sun, 18 Feb 2024 15:27:48 UTC (71 KB)
    [v2] Fri, 23 Feb 2024 15:43:50 UTC (72 KB)
    [v3] Sat, 15 Jun 2024 23:20:15 UTC (338 KB)
    Valeria Pastorino, Jasivan A. Sivakumar, Nafise Sadat Moosavi

Previous studies on framing have relied on manual analysis or fine-tuning models with limited annotated datasets. However, pre-trained models, with their diverse training backgrounds, offer a promising alternative. This paper presents a comprehensive analysis of GPT-4, GPT-3.5 Turbo, and FLAN-T5 models in detecting framing in news headlines. We evaluated these models in various scenarios: zero-shot, few-shot with in-domain examples, cross-domain examples, and settings where models explain their predictions. Our results show that explainable predictions lead to more reliable outcomes. GPT-4 performed exceptionally well in few-shot settings but often misinterpreted emotional language as framing, highlighting a significant challenge. Additionally, the results suggest that consistent predictions across multiple models could help identify potential annotation inaccuracies in datasets. Finally, we propose a new small dataset for real-world evaluation on headlines from a diverse set of topics.

------------

`[2402.11889] ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding <https://arxiv.org/abs/2402.11889>`__ ROSE没有做到这一点:用反向提示对比解码提高指令调优的大型语言模型的安全性

::

    replaced with revised version Mon, 17 Jun 2024 02:48:21 GMT
    Submission history From: Qihuang Zhong [view email]
    [v1] Mon, 19 Feb 2024 06:58:42 UTC (3,005 KB)
    [v2] Mon, 17 Jun 2024 02:48:21 UTC (3,007 KB)
    Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao

With the development of instruction-tuned large language models (LLMs), improving the safety of LLMs has become more critical. However, the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse prompt contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse prompts. Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8% safety score) upon 5 types of instruction-tuned LLMs, but also benefits the general-purpose ability of LLMs. In-depth analyses explore the underlying mechanism of ROSE, and reveal when and where to use it.

------------

`[2406.02069] PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling <https://arxiv.org/abs/2406.02069>`__ PyramidKV:基于金字塔信息漏斗的动态KV缓存压缩

::

    replaced with revised version Sun, 16 Jun 2024 06:41:08 GMT
    Submission history From: Zefan Cai [view email]
    [v1] Tue, 4 Jun 2024 07:51:30 UTC (4,708 KB)
    [v2] Sun, 16 Jun 2024 06:41:08 UTC (8,227 KB)
    Zefan Cai., Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu and Wen Xiao

In this study, we investigate whether attention-based information flow inside large language models (LLMs) is aggregated through noticeable patterns for long context processing. Our observations reveal that LLMs aggregate information through Pyramidal Information Funneling where attention is scattering widely in lower layers, progressively consolidating within specific contexts, and ultimately focusin on critical tokens (a.k.a massive activation or attention sink) in higher layers. Motivated by these insights, we developed PyramidKV, a novel and effective KV cache compression method. This approach dynamically adjusts the KV cache size across different layers, allocating more cache in lower layers and less in higher ones, diverging from traditional methods that maintain a uniform KV cache size. Our experimental evaluations, utilizing the LongBench benchmark, show that PyramidKV matches the performance of models with a full KV cache while retaining only 12% of the KV cache, thus significantly reducing memory usage. In scenarios emphasizing memory efficiency, where only 0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache compression techniques achieving up to a 20.5 absolute accuracy improvement on TREC.

------------

`[2406.06571] SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling for LLM <https://arxiv.org/abs/2406.06571>`__ SUBLLM:一种新颖高效的LLM Token序列下采样架构

::

    replaced with revised version Mon, 17 Jun 2024 09:23:13 GMT
    Submission history From: Quandong Wang [view email]
    [v1] Mon, 3 Jun 2024 16:43:04 UTC (981 KB)
    [v2] Mon, 17 Jun 2024 09:23:13 UTC (981 KB)
    Quandong Wang, Yuxuan Yuan, Xiaoyu Yang, Ruike Zhang, Kang Zhao, Wei Liu, Jian Luan, Daniel Povey and Bin Wang

While Large Language Models (LLMs) have achieved remarkable success in various fields, the efficiency of training and inference remains a major challenge. To address this issue, we propose SUBLLM, short for Subsampling-Upsampling-Bypass Large Language Model, an innovative architecture that extends the core decoder-only framework by incorporating subsampling, upsampling, and bypass modules. The subsampling modules are responsible for shortening the sequence, while the upsampling modules restore the sequence length, and the bypass modules enhance convergence. In comparison to LLaMA, the proposed SUBLLM exhibits significant enhancements in both training and inference speeds as well as memory usage, while maintaining competitive few-shot performance. During training, SUBLLM increases speeds by 26% and cuts memory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces memory by 1GB per GPU. The training and inference speeds can be enhanced by 34% and 52% respectively when the context window is expanded to 8192. We shall release the source code of the proposed architecture in the published version.

------------

`[2401.10774] Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads <https://arxiv.org/abs/2401.10774>`__ Medusa:具有多个解码头的简单LLM推理加速框架

::

    replaced with revised version Fri, 14 Jun 2024 23:32:32 GMT
    Submission history From: Tianle Cai [view email]
    [v1] Fri, 19 Jan 2024 15:48:40 UTC (1,632 KB)
    [v2] Wed, 5 Jun 2024 22:47:53 UTC (3,113 KB)
    [v3] Fri, 14 Jun 2024 23:32:32 UTC (3,113 KB)
    Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao

Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa substantially reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities.
Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.

------------

`[2406.05881] LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning <https://arxiv.org/abs/2406.05881>`__ LGR2:语言引导奖励重标记加速分层强化学习

::

    replaced with revised version Sun, 16 Jun 2024 10:28:45 GMT
    Submission history From: Utsav Singh [view email]
    [v1] Sun, 9 Jun 2024 18:40:24 UTC (8,896 KB)
    [v2] Sun, 16 Jun 2024 10:28:45 UTC (8,896 KB)
    Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri

Developing interactive systems that leverage natural language instructions to solve complex robotic control tasks has been a long-desired goal in the robotics community. Large Language Models (LLMs) have demonstrated exceptional abilities in handling complex tasks, including logical reasoning, in-context learning, and code generation. However, predicting low-level robotic actions using LLMs poses significant challenges. Additionally, the complexity of such tasks usually demands the acquisition of policies to execute diverse subtasks and combine them to attain the ultimate objective. Hierarchical Reinforcement Learning (HRL) is an elegant approach for solving such tasks, which provides the intuitive benefits of temporal abstraction and improved exploration. However, HRL faces the recurring issue of non-stationarity due to unstable lower primitive behaviour. In this work, we propose LGR2, a novel HRL framework that leverages language instructions to generate a stationary reward function for the higher-level policy. Since the language-guided reward is unaffected by the lower primitive behaviour, LGR2 mitigates non-stationarity and is thus an elegant method for leveraging language instructions to solve robotic control tasks. To analyze the efficacy of our approach, we perform empirical analysis and demonstrate that LGR2 effectively alleviates non-stationarity in HRL. Our approach attains success rates exceeding 70$\%$ in challenging, sparse-reward robotic navigation and manipulation environments where the baselines fail to achieve any significant progress. Additionally, we conduct real-world robotic manipulation experiments and demonstrate that CRISP shows impressive generalization in real-world scenarios.

------------

`[2406.06647] How Efficient is LLM-Generated Code? A Rigorous & High-Standard Benchmark <https://arxiv.org/abs/2406.06647>`__ llm生成的代码的效率如何?一个严格且高标准的基准

::

    replaced with revised version Sun, 16 Jun 2024 19:34:04 GMT
    Submission history From: Ruizhong Qiu [view email]
    [v1] Mon, 10 Jun 2024 04:19:20 UTC (97 KB)
    [v2] Sun, 16 Jun 2024 19:34:04 UTC (97 KB)
    Ruizhong Qiu, Weiliang Will Zeng, Hanghang Tong, James Ezick, Christopher Lott

The emergence of large language models (LLMs) has significantly pushed the frontiers of program synthesis. Advancement of LLM-based program synthesis calls for a thorough evaluation of LLM-generated code. Most evaluation frameworks focus on the (functional) correctness of generated code; efficiency, as an important measure of code quality, has been overlooked in existing evaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a rigorous and high-standard benchmark for evaluating the capability of LLMs in generating efficient code. Firstly, we propose a new efficiency metric called eff@k, which generalizes the pass@k metric from correctness to efficiency and appropriately handles right-censored execution time. Furthermore, we derive an unbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we also provide a numerically stable implementation for the new estimator. Secondly, to set a high-standard for efficiency evaluation, we employ a human expert to design best algorithms and implementations as our reference solutions of efficiency, many of which are much more efficient than existing canonical solutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous evaluation, we employ a human expert to curate strong test case generators to filter out wrong code and differentiate suboptimal algorithms. An extensive study across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still fall short of generating expert-level efficient code. Using two subsets of our problem set, we demonstrate that such deficiency is because current LLMs struggle in designing advanced algorithms and are barely aware of implementation optimization. Our benchmark is publicly available at this https URL .

------------

------------------------
In-Context Learning (15)
------------------------

`[2406.10878] Demonstration Notebook: Finding the Most Suited In-Context Learning Example from Interactions <https://arxiv.org/abs/2406.10878>`__ 演示笔记本:从交互中找到最合适的上下文学习示例

::

    Sun, 16 Jun 2024 10:02:20 GMT
    Yiming Tang and Bin Dong

Large language models (LLMs) benefit greatly from prompt engineering, with in-context learning standing as a pivital technique. While former approaches have provided various ways to construct the demonstrations used for in-context learning, they often ignore the inherent heterogeneity within datasets, applying the same demonstrations to all reasoning questions. We observed that the effectiveness of demonstrations varies depending on the specific question.
This motivates our exploration of using prompt engineering to select appropriate demonstrations. To address the challenge of automatically creating and choosing demonstrations tailored to each question, we propose a novel prompt engineering workflow built around a novel object called the "demonstration notebook." This notebook helps identify the most suitable in-context learning example for a question by gathering and reusing information from the LLM's past interactions. Our experiments show that this approach outperforms all existing methods for automatic demonstration construction and selection (as far as we know), achieving state-of-the-art results on serveral reasoning benchmarks. The method's versatility is further demonstrated by its success in text summarization and prompt compression tasks. Additionally, we contribute a rigorous analysis method to reveal the "demonstrative regime" of a demonstration, providing valuable insights into how demonstrations relate to different question types within a dataset.

------------

`[2406.10908] MICL: Improving In-Context Learning through Multiple-Label Words in Demonstration <https://arxiv.org/abs/2406.10908>`__ MICL:通过演示多标记单词改进上下文学习

::

    Sun, 16 Jun 2024 12:11:46 GMT
    Zhu Zixiao, Feng Zijian, Zhou Hanzhang, Qian Junlang, Mao Kezhi

In-context learning (ICL) enables large language models (LLMs) to perform new tasks by using sample-label pairs as demonstrations. However, variations in demonstrations can lead to significantly different performances. Current research mainly focuses on selecting demonstration samples, preassuming the class name to be the label word when creating sample-label pairs. However, the choice of label words is crucial for ICL performance. In addition, we observe that using a single class name in demonstration may not yield optimal results.
In this paper, we propose to use multiple label words in one sample-label pair to enhance ICL performance. Further, we select and order sample-label pairs based on LLM's output distribution, aiming to optimize the demonstration examples from both the samples' and labels' perspectives. Evaluation results on seven classification datasets show that the use of multiple label words, strategically organized by their selection, order and quantity, improves ICL performance through diverse label information.

------------

`[2406.11093] RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning based on Emotional Information <https://arxiv.org/abs/2406.11093>`__ RAEmoLLM:基于情感信息的上下文学习的检索增强llm跨域假信息检测

::

    Sun, 16 Jun 2024 22:49:11 GMT
    Zhiwei Liu, Kailai Yang, Qianqian Xie, Christine de Kock, Sophia Ananiadou, Eduard Hovy

Misinformation is prevalent in various fields such as education, politics, health, etc., causing significant harm to society. However, current methods for cross-domain misinformation detection rely on time and resources consuming fine-tuning and complex model structures. With the outstanding performance of LLMs, many studies have employed them for misinformation detection.
Unfortunately, they focus on in-domain tasks and do not incorporate significant sentiment and emotion features (which we jointly call affect). In this paper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to address cross-domain misinformation detection using in-context learning based on affective information. It accomplishes this by applying an emotion-aware LLM to construct a retrieval database of affective embeddings. This database is used by our retrieval module to obtain source-domain samples, which are subsequently used for the inference module's in-context few-shot learning to detect target domain misinformation. We evaluate our framework on three misinformation benchmarks. Results show that RAEmoLLM achieves significant improvements compared to the zero-shot method on three datasets, with the highest increases of 20.69%, 23.94%, and 39.11% respectively. This work will be released on https://github.com/lzw108/RAEmoLLM.

------------

`[2406.11629] Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better! <https://arxiv.org/abs/2406.11629>`__ 多镜头语境学习能帮助长语境LLM评判吗?看得多，判断得更好!

::

    Mon, 17 Jun 2024 15:11:58 GMT
    Mingyang Song, Mao Zheng, Xuan Luo

Leveraging Large Language Models (LLMs) as judges for evaluating the performance of LLMs has recently garnered attention. Nonetheless, this type of approach concurrently introduces potential biases from LLMs, raising concerns about the reliability of the evaluation results. To mitigate this issue, we propose and study two versions of many-shot in-context prompts, Reinforced and Unsupervised ICL, for helping GPT-4o-as-a-Judge in single answer grading. Based on the designed prompts, we investigate the impact of scaling the number of in-context examples on the agreement and quality of the evaluation.
Furthermore, we first reveal the symbol bias in GPT-4o-as-a-Judge for pairwise comparison and then propose a simple yet effective approach to mitigate it.
Experimental results show that advanced long-context LLMs, such as GPT-4o, perform better in the many-shot regime than in the zero-shot regime. Meanwhile, the experimental results further verify the effectiveness of the symbol bias mitigation approach.

------------

`[2406.11233] Probing the Decision Boundaries of In-context Learning in Large Language Models <https://arxiv.org/abs/2406.11233>`__ 大型语言模型中上下文学习的决策边界探索

::

    Mon, 17 Jun 2024 06:00:24 GMT
    Siyan Zhao, Tung Nguyen, Aditya Grover

In-context learning is a key paradigm in large language models (LLMs) that enables them to generalize to new tasks and domains by simply prompting these models with a few exemplars without explicit parameter updates. Many attempts have been made to understand in-context learning in LLMs as a function of model scale, pretraining data, and other factors. In this work, we propose a new mechanism to probe and understand in-context learning from the lens of decision boundaries for in-context binary classification. Decision boundaries are straightforward to visualize and provide important information about the qualitative behavior of the inductive biases of standard classifiers. To our surprise, we find that the decision boundaries learned by current LLMs in simple binary classification tasks are often irregular and non-smooth, regardless of linear separability in the underlying task. This paper investigates the factors influencing these decision boundaries and explores methods to enhance their generalizability. We assess various approaches, including training-free and fine-tuning methods for LLMs, the impact of model architecture, and the effectiveness of active prompting techniques for smoothing decision boundaries in a data-efficient manner. Our findings provide a deeper understanding of in-context learning dynamics and offer practical improvements for enhancing robustness and generalizability of in-context learning.

------------

`[2309.08583] ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer <https://arxiv.org/abs/2309.08583>`__ ICLEF:基于专家反馈的可解释风格迁移上下文学习

::

    replaced with revised version Mon, 17 Jun 2024 17:52:54 GMT
    Submission history From: Arkadiy Saakyan [view email]
    [v1] Fri, 15 Sep 2023 17:41:14 UTC (8,319 KB)
    [v2] Mon, 17 Jun 2024 17:52:54 UTC (2,636 KB)
    Arkadiy Saakyan and Smaranda Muresan

While state-of-the-art large language models (LLMs) can excel at adapting text from one style to another, current work does not address the explainability of style transfer models. Recent work has explored generating textual explanations from larger teacher models and distilling them into smaller student models. One challenge with such approach is that LLM outputs may contain errors that require expertise to correct, but gathering and incorporating expert feedback is difficult due to cost and availability. To address this challenge, we propose ICLEF, a novel human-AI collaboration approach to model distillation that incorporates scarce expert human feedback by combining in-context learning and model self-critique. We show that our method leads to generation of high-quality synthetic explainable style transfer datasets for formality (e-GYAFC) and subjective bias (e-WNC). Via automatic and human evaluation, we show that specialized student models fine-tuned on our datasets outperform generalist teacher models on the explainable style transfer task in one-shot settings, and perform competitively compared to few-shot teacher models, highlighting the quality of the data and the role of expert feedback. In an extrinsic task of authorship attribution, we show that explanations generated by smaller models fine-tuned on e-GYAFC are more predictive of authorship than explanations generated by few-shot teacher models.

------------

`[2402.10738] Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning <https://arxiv.org/abs/2402.10738>`__ 循序渐进:以课程学习提升情境学习能力

::

    replaced with revised version Sun, 16 Jun 2024 13:26:10 GMT
    Submission history From: Yinpeng Liu [view email]
    [v1] Fri, 16 Feb 2024 14:55:33 UTC (595 KB)
    [v2] Sun, 16 Jun 2024 13:26:10 UTC (157 KB)
    Yinpeng Liu and Jiawei Liu and Xiang Shi and Qikai Cheng and Yong Huang and Wei Lu

Demonstration ordering, which is an important strategy for in-context learning (ICL), can significantly affects the performance of large language models (LLMs). However, most of the current approaches of ordering require high computational costs to introduce the priori knowledge. In this paper, inspired by the human learning process, we propose a simple but effective demonstration ordering method for ICL, named the few-shot In-Context Curriculum Learning (ICCL). The ICCL implies gradually increasing the complexity of prompt demonstrations during the inference process. The difficulty can be assessed by human experts or LLMs-driven metrics, such as perplexity. Then we design extensive experiments to discuss the effectiveness of the ICCL at both corpus-level and instance-level. Moreover, we also investigate the formation mechanism of LLM's ICCL capability. Experimental results demonstrate that ICCL, developed during the instruction-tuning stage, is effective for representative open-source LLMs. To facilitate further research and applications by other scholars, we make the code publicly available.

------------

`[2403.04233] DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning <https://arxiv.org/abs/2403.04233>`__ DEEP-ICL:定义丰富的上下文语言模型专家

::

    replaced with revised version Sun, 16 Jun 2024 06:44:50 GMT
    Submission history From: Yiming Liang [view email]
    [v1] Thu, 7 Mar 2024 05:26:41 UTC (1,204 KB)
    [v2] Sun, 16 Jun 2024 06:44:50 UTC (1,204 KB)
    Xingwei Qu, Yiming Liang, Yucheng Wang, Tianyu Zheng, Tommy Yue, Lei Ma, Stephen W. Huang, Jiajun Zhang, Yinan Shi, Chenghua Lin, Jie Fu, Ge Zhang

It has long been assumed that the sheer number of parameters in large language models (LLMs) drives in-context learning (ICL) capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations. Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples. We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning. Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by overcoming pretraining sequence length limitations, by supporting unlimited demonstrations. We contend that DEEP-ICL presents a novel alternative for achieving efficient few-shot learning, extending beyond the conventional ICL.

------------

`[2404.14716] Bayesian Example Selection Improves In-Context Learning for Speech, Text, and Visual Modalities <https://arxiv.org/abs/2404.14716>`__ 

::

    replaced with revised version Sun, 16 Jun 2024 08:49:00 GMT
    Submission history From: Siyin Wang [view email]
    [v1] Tue, 23 Apr 2024 03:42:48 UTC (1,187 KB)
    [v2] Sun, 16 Jun 2024 08:49:00 UTC (1,194 KB)
    Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang

Large language models (LLMs) can adapt to new tasks through in-context learning (ICL) based on a few examples presented in dialogue history without any model parameter update. Despite such convenience, the performance of ICL heavily depends on the quality of the in-context examples presented, which makes the in-context example selection approach a critical choice. This paper proposes a novel Bayesian in-Context example Selection method (ByCS) for ICL. Extending the inference probability conditioned on in-context examples based on Bayes' theorem, ByCS focuses on the inverse inference conditioned on test input. Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), in-context examples are selected based on their inverse inference results. Diverse and extensive cross-tasking and cross-modality experiments are performed with speech, text, and image examples. Experimental results show the efficacy and robustness of our ByCS method on various models, tasks and modalities.

------------

`[2405.04960] P-ICL: Point In-Context Learning for Named Entity Recognition with Large Language Models <https://arxiv.org/abs/2405.04960>`__ P-ICL:基于上下文点学习的大型语言模型命名实体识别

::

    replaced with revised version Mon, 17 Jun 2024 09:38:25 GMT
    Submission history From: Guochao Jiang [view email]
    [v1] Wed, 8 May 2024 11:01:21 UTC (512 KB)
    [v2] Mon, 17 Jun 2024 09:38:25 UTC (565 KB)
    Guochao Jiang, Zepeng Ding, Yuchen Shi, Deqing Yang

In recent years, the rise of large language models (LLMs) has made it possible to directly achieve named entity recognition (NER) without any demonstration samples or only using a few samples through in-context learning (ICL). However, standard ICL only helps LLMs understand task instructions, format and input-label mapping, but neglects the particularity of the NER task itself. In this paper, we propose a new prompting framework P-ICL to better achieve NER with LLMs, in which some point entities are leveraged as the auxiliary information to recognize each entity type. With such significant information, the LLM can achieve entity classification more precisely. To obtain optimal point entities for prompting LLMs, we also proposed a point entity selection method based on K-Means clustering. Our extensive experiments on some representative NER benchmarks verify the effectiveness of our proposed strategies in P-ICL and point entity selection.

------------

`[2405.17062] Unifying Demonstration Selection and Compression for In-Context Learning <https://arxiv.org/abs/2405.17062>`__ 上下文学习中演示选择与压缩的统一

::

    replaced with revised version Sat, 15 Jun 2024 21:16:57 GMT
    Submission history From: Jun Gao [view email]
    [v1] Mon, 27 May 2024 11:31:58 UTC (2,325 KB)
    [v2] Sat, 15 Jun 2024 21:16:57 UTC (1,764 KB)
    Jun Gao, Ziqiang Cao, Wenjie Li

In-context learning (ICL) facilitates large language models (LLMs) exhibiting spectacular emergent capabilities in various scenarios. Unfortunately, introducing demonstrations easily makes the prompt length explode, bringing a significant burden to hardware. In addition, random demonstrations usually achieve limited improvements in ICL, necessitating demonstration selection among accessible candidates. Previous studies introduce extra modules to perform demonstration compression or selection independently. In this paper, we propose an ICL framework UniICL, which Unifies demonstration selection and compression, and final response generation via a single frozen LLM. Specifically, UniICL first projects actual demonstrations and inference text inputs into short virtual tokens, respectively. Then, virtual tokens are applied to select suitable demonstrations by measuring semantic similarity within latent space among candidate demonstrations and inference input. Finally, inference text inputs together with selected virtual demonstrations are fed into the same frozen LLM for response generation. Notably, UniICL is a parameter-efficient framework that only contains 17M trainable parameters originating from the projection layer. We conduct experiments and analysis over in- and out-domain datasets of both generative and understanding tasks, encompassing ICL scenarios with plentiful and limited demonstration candidates. Results show that UniICL effectively unifies $12 \times$ compression, demonstration selection, and response generation, efficiently scaling up the baseline from 4-shot to 64-shot ICL in IMDb with 24 GB CUDA allocation

------------

`[2311.09263] Auto-ICL: In-Context Learning without Human Supervision <https://arxiv.org/abs/2311.09263>`__ Auto-ICL:无人工监督的上下文学习

::

    replaced with revised version Mon, 17 Jun 2024 06:53:41 GMT
    Submission history From: Jinghan Yang [view email]
    [v1] Wed, 15 Nov 2023 07:37:28 UTC (916 KB)
    [v2] Mon, 17 Jun 2024 06:53:41 UTC (141 KB)
    Jinghan Yang, Shuming Ma, Furu Wei

With in-context learning ability, the performance of large language models can be significantly boosted when provided with appropriate context. However, existing in-context learning methods mainly rely on human-provided contexts, such as labeled examples and explicit instructions. Writing context by humans is labor-intensive on various tasks and limits the model to tasks manageable by humans. To overcome these limitations, we propose Automatic In-Context Learning framework that enables the model to autonomously generate examples and instructions for problem-solving. With experiments across various models and datasets, results show that model-generated contexts outperform human-annotated contexts, including Few-Shot and Few-Shot-CoT methods, and surpass existing self-generated context methods like Zero-CoT and Auto-CoT.

------------

`[2311.09948] Hijacking Large Language Models via Adversarial In-Context Learning <https://arxiv.org/abs/2311.09948>`__ 基于对抗性上下文学习的大型语言模型劫持

::

    replaced with revised version Sat, 15 Jun 2024 18:54:54 GMT
    Submission history From: Dongxiao Zhu [view email]
    [v1] Thu, 16 Nov 2023 15:01:48 UTC (1,063 KB)
    [v2] Sat, 15 Jun 2024 18:54:54 UTC (2,590 KB)
    Yao Qiang and Xiangyu Zhou and Dongxiao Zhu

In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific downstream tasks by utilizing labeled examples as demonstrations (demos) in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. This work introduces a novel transferable attack against ICL to address these issues, aiming to hijack LLMs to generate the target response or jailbreak. Our hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demos without directly contaminating the user queries. Comprehensive experimental results across different generation and jailbreaking tasks highlight the effectiveness of our hijacking attack, resulting in distracted attention towards adversarial tokens and consequently leading to unwanted target outputs. We also propose a defense strategy against hijacking attacks through the use of extra clean demos, which enhances the robustness of LLMs during ICL. Broadly, this work reveals the significant security vulnerabilities of LLMs and emphasizes the necessity for in-depth studies on their robustness.

------------

`[2402.15607] How Do Nonlinear Transformers Learn and Generalize in In-Context Learning? <https://arxiv.org/abs/2402.15607>`__ 非线性transformer如何在上下文学习中学习和泛化?

::

    replaced with revised version Sun, 16 Jun 2024 04:02:28 GMT
    Submission history From: Hongkang Li [view email]
    [v1] Fri, 23 Feb 2024 21:07:20 UTC (843 KB)
    [v2] Wed, 5 Jun 2024 07:04:56 UTC (593 KB)
    [v3] Sun, 16 Jun 2024 04:02:28 UTC (653 KB)
    Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, Pin-Yu Chen

Transformer-based large language models have displayed impressive in-context learning capabilities, where a pre-trained model can handle new tasks without fine-tuning by simply augmenting the query with some input-output examples from that task. Despite the empirical success, the mechanics of how to train a Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive due to the technical challenges of analyzing the nonconvex training problems resulting from the nonlinear self-attention and nonlinear activation in Transformers. To the best of our knowledge, this paper provides the first theoretical analysis of the training dynamics of Transformers with nonlinear self-attention and nonlinear MLP, together with the ICL generalization capability of the resulting model. Focusing on a group of binary classification tasks, we train Transformers using data from a subset of these tasks and quantify the impact of various factors on the ICL generalization performance on the remaining unseen tasks with and without data distribution shifts. We also analyze how different components in the learned Transformers contribute to the ICL performance. Furthermore, we provide the first theoretical analysis of how model pruning affects ICL performance and prove that proper magnitude-based pruning can have a minimal impact on ICL while reducing inference costs. These theoretical findings are justified through numerical experiments.

------------

`[2310.12477] Exploring In-Context Learning of Textless Speech Language Model for Speech Classification Tasks <https://arxiv.org/abs/2310.12477>`__ 探索用于语音分类任务的无文本语音语言模型的上下文学习

::

    replaced with revised version Sat, 15 Jun 2024 14:13:54 GMT
    Submission history From: Kai-Wei Chang [view email]
    [v1] Thu, 19 Oct 2023 05:31:45 UTC (348 KB)
    [v2] Sat, 15 Jun 2024 14:13:54 UTC (2,382 KB)
    Ming-Hao Hsu, Kai-Wei Chang, Shang-Wen Li, Hung-yi Lee

Ever since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an essential role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to perform various downstream tasks in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study is the first work exploring ICL for speech classification tasks with textless speech LM. We first show that the current speech LM lacks the ICL capability. We then perform warmup training on the speech LM, equipping the LM with demonstration learning capability. This paper explores and proposes the first speech LM capable of performing unseen classification tasks in an ICL manner.

------------

--------------
Reasoning (20)
--------------

`[2406.11160] Move Beyond Triples: Contextual Knowledge Graph Representation and Reasoning <https://arxiv.org/abs/2406.11160>`__ 超越三元组:上下文知识图谱表示和推理

::

    Mon, 17 Jun 2024 02:59:19 GMT
    Chengjin Xu, Muzhi Li, Cehao Yang, Xuhui Jiang, Lumingyuan Tang, Yiyan Qi, Jian Guo

Knowledge Graphs (KGs) are foundational structures in many AI applications, representing entities and their interrelations through triples. However, triple-based KGs lack the contextual information of relational knowledge, like temporal dynamics and provenance details, which are crucial for comprehensive knowledge representation and effective reasoning. Instead, \textbf{Contextual Knowledge Graphs} (CKGs) expand upon the conventional structure by incorporating additional information such as time validity, geographic location, and source provenance. This integration provides a more nuanced and accurate understanding of knowledge, enabling KGs to offer richer insights and support more sophisticated reasoning processes. In this work, we first discuss the inherent limitations of triple-based KGs and introduce the concept of contextual KGs, highlighting their advantages in knowledge representation and reasoning. We then present \textbf{KGR$^3$, a context-enriched KG reasoning paradigm} that leverages large language models (LLMs) to retrieve candidate entities and related contexts, rank them based on the retrieved information, and reason whether sufficient information has been obtained to answer a query.
Our experimental results demonstrate that KGR$^3$ significantly improves performance on KG completion (KGC) and KG question answering (KGQA) tasks, validating the effectiveness of incorporating contextual information on KG representation and reasoning.

------------

`[2406.11161] Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning <https://arxiv.org/abs/2406.11161>`__ Emotion- llama:基于指令调整的多模态情感识别与推理

::

    Mon, 17 Jun 2024 03:01:22 GMT
    Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Jingdong Sun, Kai Wang, Yuxiang Lin, Zheng Lian, Xiaojiang Peng, Alexander Hauptmann

Accurate emotion perception is crucial for various applications, including human-computer interaction, education, and counseling. However, traditional single-modality approaches often fail to capture the complexity of real-world emotional expressions, which are inherently multimodal. Moreover, existing Multimodal Large Language Models (MLLMs) face challenges in integrating audio and recognizing subtle facial micro-expressions. To address this, we introduce the MERR dataset, containing 28,618 coarse-grained and 4,487 fine-grained annotated samples across diverse emotional categories. This dataset enables models to learn from varied scenarios and generalize to real-world applications. Furthermore, we propose Emotion-LLaMA, a model that seamlessly integrates audio, visual, and textual inputs through emotion-specific encoders.
By aligning features into a shared space and employing a modified LLaMA model with instruction tuning, Emotion-LLaMA significantly enhances both emotional recognition and reasoning capabilities. Extensive evaluations show Emotion-LLaMA outperforms other MLLMs, achieving top scores in Clue Overlap (7.83) and Label Overlap (6.25) on EMER, an F1 score of 0.9036 on MER2023 challenge, and the highest UAR (45.59) and WAR (59.37) in zero-shot evaluations on DFEW dataset.

------------

`[2406.10625] On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models <https://arxiv.org/abs/2406.10625>`__ 大型语言模型中忠实思维链推理的困难

::

    Sat, 15 Jun 2024 13:16:44 GMT
    Sree Harsha Tanneru, Dan Ley, Chirag Agarwal, Himabindu Lakkaraju

As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior.
While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.

------------

`[2406.10834] Exposing the Achilles' Heel: Evaluating LLMs Ability to Handle Mistakes in Mathematical Reasoning <https://arxiv.org/abs/2406.10834>`__ 暴露阿喀琉斯之踵:评估llm处理数学推理错误的能力

::

    Sun, 16 Jun 2024 08:06:05 GMT
    Joykirat Singh, Akshay Nambi, Vibhav Vineet

Large Language Models (LLMs) have been applied to Math Word Problems (MWPs) with transformative impacts, revolutionizing how these complex problems are approached and solved in various domains including educational settings.
However, the evaluation of these models often prioritizes final accuracy, overlooking the crucial aspect of reasoning capabilities. This work addresses this gap by focusing on the ability of LLMs to detect and correct reasoning mistakes. We introduce a novel dataset MWP-MISTAKE, incorporating MWPs with both correct and incorrect reasoning steps generated through rule-based methods and smaller language models. Our comprehensive benchmarking reveals significant insights into the strengths and weaknesses of state-of-the-art models, such as GPT-4o, GPT-4, GPT-3.5Turbo, and others. We highlight GPT-$o's superior performance in mistake detection and rectification and the persistent challenges faced by smaller models. Additionally, we identify issues related to data contamination and memorization, impacting the reliability of LLMs in real-world applications. Our findings emphasize the importance of rigorous evaluation of reasoning processes and propose future directions to enhance the generalization and robustness of LLMs in mathematical problem-solving.

------------

`[2406.10858] Step-level Value Preference Optimization for Mathematical Reasoning <https://arxiv.org/abs/2406.10858>`__ 数学推理的阶梯式值偏好优化

::

    Sun, 16 Jun 2024 09:06:17 GMT
    Guoxin Chen, Minpeng Liao, Chengxi Li, Kai Fan

Direct Preference Optimization (DPO) using an implicit reward model has proven to be an effective alternative to reinforcement learning from human feedback (RLHF) for fine-tuning preference aligned large language models (LLMs). However, the overall preference annotations of responses do not fully capture the fine-grained quality of model outputs in complex multi-step reasoning tasks, such as mathematical reasoning. To address this limitation, we introduce a novel algorithm called Step-level Value Preference Optimization (SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically annotate step-level preferences for multi-step reasoning. Furthermore, from the perspective of learning-to-rank, we train an explicit value model to replicate the behavior of the implicit reward model, complementing standard preference optimization. This value model enables the LLM to generate higher reward responses with minimal cost during inference. Experimental results demonstrate that our method achieves state-of-the-art performance on both in-domain and out-of-domain mathematical reasoning benchmarks.

------------

`[2406.10999] Not All Bias is Bad: Balancing Rational Deviations and Cognitive Biases in Large Language Model Reasoning <https://arxiv.org/abs/2406.10999>`__ 并非所有偏见都是坏的:在大型语言模型推理中平衡理性偏差和认知偏见

::

    Sun, 16 Jun 2024 16:25:22 GMT
    Liman Wang, Hanyang Zhong

This paper investigates the nuanced role of biases in the decision-making processes of large language models (LLMs). While conventional research typically aims to eliminate all biases, our study reveals that not all biases are detrimental. By examining rational deviations, involving heuristic shortcuts that enhance decision-making efficiency, we highlight their potential benefits when properly balanced. We introduce the concepts of heuristic moderation and an abstention option, allowing LLMs to abstain from answering when uncertain, thereby reducing error rates and improving decision accuracy.
Using our newly developed BRD (Balance Rational Deviations) dataset, our findings demonstrate that appropriately scaled bias inspection enhances model performance and aligns LLM decision-making more closely with human reasoning.
This balance improves the reliability and trustworthiness of LLMs and suggests new strategies for future enhancements. Our work offers a fresh perspective on leveraging biases constructively to enhance the practical applications of LLMs, from conversational agents to decision support systems and beyond.

------------

`[2406.11012] Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game <https://arxiv.org/abs/2406.11012>`__ Connecting the Dots:使用New York Times Connections文字游戏评估llm的抽象推理能力

::

    Sun, 16 Jun 2024 17:10:32 GMT
    Prisha Samadarshi, Mariam Mustafa, Anushka Kulkarni, Raven Rothkopf, Tuhin Chakrabarty, Smaranda Muresan

The New York Times Connections game has emerged as a popular and challenging pursuit for word puzzle enthusiasts. We collect 200 Connections games to evaluate the performance of state-of-the-art large language models (LLMs) against expert and novice human players. Our results show that even the best-performing LLM, GPT-4o, which has otherwise shown impressive reasoning abilities on a wide variety of benchmarks, can only fully solve 8% of the games. Compared to GPT-4o, novice and expert players perform better, with expert human players significantly outperforming GPT-4o. To deepen our understanding we create a taxonomy of the knowledge types required to successfully categorize words in the Connections game, revealing that LLMs struggle with associative, encyclopedic, and linguistic knowledge. Our findings establish the New York Times Connections game as a challenging benchmark for evaluating abstract reasoning capabilities in humans and AI systems.

------------

`[2406.11020] RUPBench: Benchmarking Reasoning Under Perturbations for Robustness Evaluation in Large Language Models <https://arxiv.org/abs/2406.11020>`__ RUPBench:大型语言模型鲁棒性评估扰动下推理基准测试

::

    Sun, 16 Jun 2024 17:26:44 GMT
    Yuqing Wang, Yun Zhao

With the increasing use of large language models (LLMs), ensuring reliable performance in diverse, real-world environments is essential. Despite their remarkable achievements, LLMs often struggle with adversarial inputs, significantly impacting their effectiveness in practical applications. To systematically understand the robustness of LLMs, we present RUPBench, a comprehensive benchmark designed to evaluate LLM robustness across diverse reasoning tasks. Our benchmark incorporates 15 reasoning datasets, categorized into commonsense, arithmetic, logical, and knowledge-intensive reasoning, and introduces nine types of textual perturbations at lexical, syntactic, and semantic levels. By examining the performance of state-of-the-art LLMs such as GPT-4o, Llama3, Phi-3, and Gemma on both original and perturbed datasets, we provide a detailed analysis of their robustness and error patterns. Our findings highlight that larger models tend to exhibit greater robustness to perturbations. Additionally, common error types are identified through manual inspection, revealing specific challenges faced by LLMs in different reasoning contexts. This work provides insights into areas where LLMs need further improvement to handle diverse and noisy inputs effectively.

------------

`[2406.11402] Evaluating Open Language Models Across Task Types, Application Domains, and Reasoning Types: An In-Depth Experimental Analysis <https://arxiv.org/abs/2406.11402>`__ 跨任务类型、应用领域和推理类型的开放语言模型评估:深入的实验分析

::

    Mon, 17 Jun 2024 10:45:36 GMT
    Neelabh Sinha, Vinija Jain, Aman Chadha

The rapid rise of Language Models (LMs) has expanded their use in several applications. Yet, due to constraints of model size, associated cost, or proprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always feasible. With open, smaller LMs emerging, more applications can leverage their capabilities, but selecting the right LM can be challenging. This work conducts an in-depth experimental analysis of the semantic correctness of outputs of 10 smaller, open LMs across three aspects: task types, application domains and reasoning types, using diverse prompt styles. We demonstrate that most effective models and prompt styles vary depending on the specific requirements.
Our analysis provides a comparative assessment of LMs and prompt styles using a proposed three-tier schema of aspects for their strategic selection based on use-case and other constraints. We also show that if utilized appropriately, these LMs can compete with, and sometimes outperform, SOTA LLMs like DeepSeek-v2, GPT-3.5-Turbo, and GPT-4o.

------------

`[2406.11698] Meta Reasoning for Large Language Models <https://arxiv.org/abs/2406.11698>`__ 大型语言模型的元推理

::

    Mon, 17 Jun 2024 16:14:11 GMT
    Peizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, Furu Wei

We introduce Meta-Reasoning Prompting (MRP), a novel and efficient system prompting method for large language models (LLMs) inspired by human meta-reasoning. Traditional in-context learning-based reasoning techniques, such as Tree-of-Thoughts, show promise but lack consistent state-of-the-art performance across diverse tasks due to their specialized nature. MRP addresses this limitation by guiding LLMs to dynamically select and apply different reasoning methods based on the specific requirements of each task, optimizing both performance and computational efficiency. With MRP, LLM reasoning operates in two phases. Initially, the LLM identifies the most appropriate reasoning method using task input cues and objective descriptions of available methods.
Subsequently, it applies the chosen method to complete the task. This dynamic strategy mirrors human meta-reasoning, allowing the model to excel in a wide range of problem domains. We evaluate the effectiveness of MRP through comprehensive benchmarks. The results demonstrate that MRP achieves or approaches state-of-the-art performance across diverse tasks. MRP represents a significant advancement in enabling LLMs to identify cognitive challenges across problems and leverage benefits across different reasoning approaches, enhancing their ability to handle diverse and complex problem domains efficiently. Every LLM deserves a Meta-Reasoning Prompting to unlock its full potential and ensure adaptability in an ever-evolving landscape of challenges and applications.

------------

`[2406.11784] MDCR: A Dataset for Multi-Document Conditional Reasoning <https://arxiv.org/abs/2406.11784>`__ MDCR:面向多文档条件推理的数据集

::

    Mon, 17 Jun 2024 17:38:43 GMT
    Peter Baile Chen, Yi Zhang, Chunwei Liu, Sejal Gupta, Yoon Kim, Michael Cafarella

The same real-life questions posed to different individuals may lead to different answers based on their unique situations. For instance, whether a student is eligible for a scholarship depends on eligibility conditions, such as major or degree required. ConditionalQA was proposed to evaluate models' capability of reading a document and answering eligibility questions, considering unmentioned conditions. However, it is limited to questions on single documents, neglecting harder cases that may require cross-document reasoning and optimization, for example, "What is the maximum number of scholarships attainable?" Such questions over multiple documents are not only more challenging due to more context having to understand, but also because the model has to (1) explore all possible combinations of unmentioned conditions and (2) understand the relationship between conditions across documents, to reason about the optimal outcome. To evaluate models' capability of answering such questions, we propose a new dataset MDCR, which can reflect real-world challenges and serve as a new test bed for complex conditional reasoning that requires optimization. We evaluate this dataset using the most recent LLMs and demonstrate their limitations in solving this task. We believe this dataset will facilitate future research in answering optimization questions with unknown conditions.

------------

`[2406.11768] GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities <https://arxiv.org/abs/2406.11768>`__ GAMA:具有高级音频理解和复杂推理能力的大型音频语言模型

::

    Mon, 17 Jun 2024 17:31:01 GMT
    Sreyan Ghosh and Sonal Kumar and Ashish Seth and Chandra Kiran Reddy Evuru and Utkarsh Tyagi and S Sakshi and Oriol Nieto and Ramani Duraiswami and Dinesh Manocha

Perceiving and understanding non-speech sounds and non-verbal speech is essential to making decisions that help us interact with our surroundings. In this paper, we propose GAMA, a novel General-purpose Large Audio-Language Model (LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We build GAMA by integrating an LLM with multiple types of audio representations, including features from a custom Audio Q-Former, a multi-layer aggregator that aggregates features from multiple layers of an audio encoder. We fine-tune GAMA on a large-scale audio-language dataset, which augments it with audio understanding capabilities. Next, we propose CompA-R (Instruction-Tuning for Complex Audio Reasoning), a synthetically generated instruction-tuning (IT) dataset with instructions that require the model to perform complex reasoning on the input audio. We instruction-tune GAMA with CompA-R to endow it with complex reasoning abilities, where we further add a soft prompt as input with high-level semantic evidence by leveraging event tags of the input audio.
Finally, we also propose CompA-R-test, a human-labeled evaluation dataset for evaluating the capabilities of LALMs on open-ended audio question-answering that requires complex reasoning. Through automated and expert human evaluations, we show that GAMA outperforms all other LALMs in literature on diverse audio understanding tasks by margins of 1%-84%. Further, GAMA IT-ed on CompA-R proves to be superior in its complex reasoning and instruction following capabilities.

------------

`[2406.10923] Investigating Video Reasoning Capability of Large Language Models with Tropes in Movies <https://arxiv.org/abs/2406.10923>`__ 用电影中的比喻研究大型语言模型的视频推理能力

::

    Sun, 16 Jun 2024 12:58:31 GMT
    Hung-Ting Su, Chun-Tong Chao, Ya-Ching Hsu, Xudong Lin, Yulei Niu, Hung-Yi Lee, Winston H. Hsu

Large Language Models (LLMs) have demonstrated effectiveness not only in language tasks but also in video reasoning. This paper introduces a novel dataset, Tropes in Movies (TiM), designed as a testbed for exploring two critical yet previously overlooked video reasoning skills: (1) Abstract Perception: understanding and tokenizing abstract concepts in videos, and (2) Long-range Compositional Reasoning: planning and integrating intermediate reasoning steps for understanding long-range videos with numerous frames.
Utilizing tropes from movie storytelling, TiM evaluates the reasoning capabilities of state-of-the-art LLM-based approaches. Our experiments show that current methods, including Captioner-Reasoner, Large Multimodal Model Instruction Fine-tuning, and Visual Programming, only marginally outperform a random baseline when tackling the challenges of Abstract Perception and Long-range Compositional Reasoning. To address these deficiencies, we propose Face-Enhanced Viper of Role Interactions (FEVoRI) and Context Query Reduction (ConQueR), which enhance Visual Programming by fostering role interaction awareness and progressively refining movie contexts and trope queries during reasoning processes, significantly improving performance by 15 F1 points.
However, this performance still lags behind human levels (40 vs. 65 F1).
Additionally, we introduce a new protocol to evaluate the necessity of Abstract Perception and Long-range Compositional Reasoning for task resolution. This is done by analyzing the code generated through Visual Programming using an Abstract Syntax Tree (AST), thereby confirming the increased complexity of TiM.
The dataset and code are available at: https://ander1119.github.io/TiM

------------

`[2310.03965] Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models <https://arxiv.org/abs/2310.03965>`__ 思想传播:基于大型语言模型的复杂推理类比方法

::

    replaced with revised version Mon, 17 Jun 2024 05:56:00 GMT
    Submission history From: Junchi Yu [view email]
    [v1] Fri, 6 Oct 2023 01:40:09 UTC (1,639 KB)
    [v2] Mon, 9 Oct 2023 00:35:22 UTC (1,639 KB)
    [v3] Mon, 17 Jun 2024 05:56:00 UTC (1,874 KB)
    Junchi Yu, Ran He, Rex Ying

Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}. To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\% improvement of human preference in Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent Planning.

------------

`[2311.06985] Large Language Models are In-context Teachers for Knowledge Reasoning <https://arxiv.org/abs/2311.06985>`__ 大型语言模型是知识推理的上下文教师

::

    replaced with revised version Mon, 17 Jun 2024 05:57:38 GMT
    Submission history From: Jiachen Zhao [view email]
    [v1] Sun, 12 Nov 2023 23:14:43 UTC (1,218 KB)
    [v2] Mon, 17 Jun 2024 05:57:38 UTC (2,045 KB)
    Jiachen Zhao, Zonghai Yao, Zhichao Yang, Hong Yu

Chain-of-thought (CoT) prompting teaches large language models (LLMs) in context to reason over queries that require more than mere information retrieval. However, human experts are usually required to craft demonstrations for in-context learning (ICL), which is expensive and has high variance. More importantly, how to craft helpful reasoning exemplars for ICL remains unclear. In this work, we investigate whether LLMs can be better in-context teachers for knowledge reasoning. We follow the ``encoding specificity'' hypothesis in human's memory retrieval to assume in-context exemplars at inference should match the encoding context in training data. We are thus motivated to propose Self-Explain to use one LLM's self-elicited explanations as in-context demonstrations for prompting it as they are generalized from the model's training examples. Self-Explain is shown to significantly outperform using human-crafted exemplars and other baselines. We further reveal that for in-context teaching, rationales by distinct teacher LLMs or human experts that more resemble the student LLM's self-explanations are better demonstrations, which supports our encoding specificity hypothesis. We then propose Teach-Back that aligns the teacher LLM with the student to enhance the in-context teaching performance. For example, Teach-Back enables a 7B model to teach the much larger GPT-3.5 in context, surpassing human teachers by around 5% in test accuracy on medical question answering.

------------

`[2402.09614] Probabilistic Reasoning in Generative Large Language Models <https://arxiv.org/abs/2402.09614>`__ 生成式大型语言模型中的概率推理

::

    replaced with revised version Mon, 17 Jun 2024 05:13:33 GMT
    Submission history From: Aliakbar Nafar [view email]
    [v1] Wed, 14 Feb 2024 23:05:44 UTC (17,387 KB)
    [v2] Mon, 17 Jun 2024 05:13:33 UTC (11,570 KB)
    Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi

This paper considers the challenges Large Language Models (LLMs) face when reasoning over text that includes information involving uncertainty explicitly quantified via probability values. This type of reasoning is relevant to a variety of contexts ranging from everyday conversations to medical decision-making. Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning. To deal with this problem, we introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the probabilistic reasoning capabilities of LLMs. We use BLInD to find out the limitations of LLMs for tasks involving probabilistic reasoning. In addition, we present several prompting strategies that map the problem to different formal representations, including Python code, probabilistic algorithms, and probabilistic logical programming. We conclude by providing an evaluation of our methods on BLInD and an adaptation of a causal reasoning question-answering dataset. Our empirical results highlight the effectiveness of our proposed strategies for multiple LLMs.

------------

`[2402.11432] Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning <https://arxiv.org/abs/2402.11432>`__ 欺骗检测能更深入吗?用于欺骗推理的数据集、评估和基准

::

    replaced with revised version Mon, 17 Jun 2024 02:57:09 GMT
    Submission history From: Zheng Lian [view email]
    [v1] Sun, 18 Feb 2024 02:52:54 UTC (704 KB)
    [v2] Mon, 17 Jun 2024 02:57:09 UTC (707 KB)
    Kang Chen, Zheng Lian, Haiyang Sun, Bin Liu, Jianhua Tao

Deception detection has attracted increasing attention due to its importance in real-world scenarios. Its main goal is to detect deceptive behaviors from multimodal clues such as gestures, facial expressions, prosody, etc. However, these bases are usually subjective and related to personal habits. Therefore, we extend deception detection to deception reasoning, further providing objective evidence to support subjective judgment. Specifically, we provide potential lies and basic facts and then analyze why this sentence may be a lie by combining factual inconsistencies and intent behind them. Compared with deception detection, this task is more applicable to real-world scenarios. For example, in interrogation, the police should judge whether a person is lying based on solid evidence. This paper presents our initial attempts at this task, including constructing a dataset and defining evaluation metrics. Meanwhile, this task can serve as a benchmark for evaluating the complex reasoning capability of large language models. Code and data will be made publicly available.

------------

`[2406.06461] Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies <https://arxiv.org/abs/2406.06461>`__ 代币经济中的推理:LLM推理策略的预算感知评估

::

    replaced with revised version Sat, 15 Jun 2024 01:59:02 GMT
    Submission history From: Junlin Wang [view email]
    [v1] Mon, 10 Jun 2024 16:55:08 UTC (4,133 KB)
    [v2] Tue, 11 Jun 2024 02:12:17 UTC (4,129 KB)
    [v3] Sat, 15 Jun 2024 01:59:02 UTC (4,354 KB)
    Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, Ben Athiwaratkun

A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don't surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.

------------

`[2405.16265] MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time <https://arxiv.org/abs/2405.16265>`__ MindStar:在推理时增强预训练llm的数学推理

::

    replaced with revised version Mon, 17 Jun 2024 13:37:39 GMT
    Submission history From: Jikun Kang [view email]
    [v1] Sat, 25 May 2024 15:07:33 UTC (1,366 KB)
    [v2] Mon, 17 Jun 2024 13:37:39 UTC (1,540 KB)
    Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Boxing Chen

Although Large Language Models (LLMs) achieve remarkable performance across various tasks, they often struggle with complex reasoning tasks, such as answering mathematical questions. Recent efforts to address this issue have primarily focused on leveraging mathematical datasets through supervised fine-tuning or self-improvement techniques. However, these methods often depend on high-quality datasets that are difficult to prepare, or they require substantial computational resources for fine-tuning. Inspired by findings that LLMs know how to produce the right answer but struggle to select the correct reasoning path, we propose a purely inference-based searching method -- MindStar (M*). This method formulates reasoning tasks as searching problems and proposes two search ideas to identify the optimal reasoning paths. We evaluate the M* framework on both the GSM8K and MATH datasets, comparing its performance with existing open and closed-source LLMs. Our results demonstrate that M* significantly enhances the reasoning abilities of open-source models, such as Llama-2-13B and Mistral-7B, and achieves comparable performance to GPT-3.5 and Grok-1, but with substantially reduced model size and computational costs.

------------

`[2403.18252] Beyond Embeddings: The Promise of Visual Table in Visual Reasoning <https://arxiv.org/abs/2403.18252>`__ 超越嵌入:视觉推理中视觉表的前景

::

    replaced with revised version Mon, 17 Jun 2024 09:57:09 GMT
    Submission history From: Yiwu Zhong [view email]
    [v1] Wed, 27 Mar 2024 04:49:23 UTC (6,697 KB)
    [v2] Mon, 17 Jun 2024 09:57:09 UTC (4,921 KB)
    Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang

Visual representation learning has been a cornerstone in computer vision, involving typical forms such as visual embeddings, structural symbols, and text-based representations. Despite the success of CLIP-type visual embeddings, they often lack access to world knowledge critical for visual reasoning. In this work, we propose Visual Table, a novel form of visual representation tailored for visual reasoning. Visual tables are constructed as hierarchical descriptions of visual scenes, featuring a scene description and multiple object-centric descriptions covering categories, attributes, and knowledge. Thanks to the structural and textual formats, visual tables offer unique advantages over mere visual embeddings, such as interpretability and controllable editing. Furthermore, they deliver instance-level world knowledge and detailed attributes that are essential for visual reasoning. To create visual tables, we develop a generator trained on the dataset with collected, small-scale annotations. Extensive results on 11 visual reasoning benchmarks demonstrate that the generated visual tables significantly outperform previous structural and text-based representations. Moreover, they consistently enhance state-of-the-art multimodal large language models across diverse benchmarks, showcasing their potential for advancing visual reasoning tasks. Our code is available at this https URL.

------------

-----------
ToolUse (6)
-----------

`[2406.11681] R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models <https://arxiv.org/abs/2406.11681>`__ R-Eval:检索增强大型语言模型领域知识评估统一工具包

::

    Mon, 17 Jun 2024 15:59:49 GMT
    Shangqing Tu, Yuanchun Wang, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, Juanzi Li

Large language models have achieved remarkable success on general NLP tasks, but they may fall short for domain-specific problems. Recently, various Retrieval-Augmented Large Language Models (RALLMs) are proposed to address this shortcoming. However, existing evaluation tools only provide a few baselines and evaluate them on various domains without mining the depth of domain knowledge. In this paper, we address the challenges of evaluating RALLMs by introducing the R-Eval toolkit, a Python toolkit designed to streamline the evaluation of different RAG workflows in conjunction with LLMs. Our toolkit, which supports popular built-in RAG workflows and allows for the incorporation of customized testing data on the specific domain, is designed to be user-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs across three task levels and two representative domains, revealing significant variations in the effectiveness of RALLMs across different tasks and domains.
Our analysis emphasizes the importance of considering both task and domain requirements when choosing a RAG workflow and LLM combination. We are committed to continuously maintaining our platform at https://github.com/THU-KEG/R-Eval to facilitate both the industry and the researchers.

------------

`[2406.11200] AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval <https://arxiv.org/abs/2406.11200>`__ AvaTaR:优化LLM agent的工具辅助知识检索

::

    Mon, 17 Jun 2024 04:20:02 GMT
    Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Vassilis N. Ioannidis, Karthik Subbian, Jure Leskovec, James Zou

Large language model (LLM) agents have demonstrated impressive capability in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing the prompting techniques that make LLM agents able to effectively use external tools and knowledge is a heuristic and laborious task. Here, we introduce AvaTaR, a novel and automatic framework that optimizes an LLM agent to effectively use the provided tools and improve its performance on a given task/domain. During optimization, we design a comparator module to iteratively provide insightful and holistic prompts to the LLM agent via reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information. We find AvaTaR consistently outperforms state-of-the-art approaches across all four challenging tasks and exhibits strong generalization ability when applied to novel cases, achieving an average relative improvement of 14% on the Hit@1 metric. Code and dataset are available at https://github.com/zou-group/avatar.

------------

`[2406.11255] Liberal Entity Matching as a Compound AI Toolchain <https://arxiv.org/abs/2406.11255>`__ 自由实体匹配作为复合AI工具链

::

    Mon, 17 Jun 2024 06:33:34 GMT
    Silvery D. Fu, David Wang, Wen Zhang, Kathleen Ge

Entity matching (EM), the task of identifying whether two descriptions refer to the same entity, is essential in data management. Traditional methods have evolved from rule-based to AI-driven approaches, yet current techniques using large language models (LLMs) often fall short due to their reliance on static knowledge and rigid, predefined prompts. In this paper, we introduce Libem, a compound AI system designed to address these limitations by incorporating a flexible, tool-oriented approach. Libem supports entity matching through dynamic tool use, self-refinement, and optimization, allowing it to adapt and refine its process based on the dataset and performance metrics. Unlike traditional solo-AI EM systems, which often suffer from a lack of modularity that hinders iterative design improvements and system optimization, Libem offers a composable and reusable toolchain. This approach aims to contribute to ongoing discussions and developments in AI-driven data management.

------------

`[2406.10237] Towards commands recommender system in BIM authoring tool using transformers <https://arxiv.org/abs/2406.10237>`__ 基于transformer的BIM创作工具命令推荐系统研究

::

    Sun, 2 Jun 2024 17:47:06 GMT
    Changyu Du, Zihan Deng, Stavros Nousias, Andr\'e Borrmann

The complexity of BIM software presents significant barriers to the widespread adoption of BIM and model-based design within the Architecture, Engineering, and Construction (AEC) sector. End-users frequently express concerns regarding the additional effort required to create a sufficiently detailed BIM model when compared with conventional 2D drafting. This study explores the potential of sequential recommendation systems to accelerate the BIM modeling process. By treating BIM software commands as recommendable items, we introduce a novel end-to-end approach that predicts the next-best command based on user historical interactions. Our framework extensively preprocesses real-world, large-scale BIM log data, utilizes the transformer architectures from the latest large language models as the backbone network, and ultimately results in a prototype that provides real-time command suggestions within the BIM authoring tool Vectorworks. Subsequent experiments validated that our proposed model outperforms the previous study, demonstrating the immense potential of the recommendation system in enhancing design efficiency.

------------

`[2401.15724] RE-GAINS & EnCHANT: Intelligent Tool Manipulation Systems For Enhanced Query Responses <https://arxiv.org/abs/2401.15724>`__ RE-GAINS & EnCHANT:用于增强查询响应的智能工具操作系统

::

    replaced with revised version Mon, 17 Jun 2024 17:59:34 GMT
    Submission history From: Sahil Girhepuje [view email]
    [v1] Sun, 28 Jan 2024 18:26:31 UTC (1,408 KB)
    [v2] Mon, 17 Jun 2024 17:59:34 UTC (1,409 KB)
    Sahil Girhepuje, Siva Sankar Sajeev, Purvam Jain, Arya Sikder, Adithya Rama Varma, Ryan George, Akshay Govind Srinivasan, Mahendra Kurup, Ashmit Sinha, Sudip Mondal

Despite the remarkable success of LLMs, they still suffer from tool invocation and tool chaining due to inadequate input queries and/or tool argument descriptions. We propose two novel frameworks, RE-GAINS and EnCHANT, enabling LLMs to tackle tool manipulation for solving complex user queries by making API calls. EnCHANT is an open-source solution that makes use of an LLM format enforcer, an LLM(OpenChat 3.5) and a retriever(ToolBench's API Retriever). RE-GAINS is based on OpenAI models and embeddings using a special prompt based on the RAP paper. Both solutions cost less than $0.01 per query with minimal latency, therefore showcasing the usefulness of the frameworks.

------------

`[2406.00008] KnowledgeHub: An end-to-end Tool for Assisted Scientific Discovery <https://arxiv.org/abs/2406.00008>`__ KnowledgeHub:一个端到端的科学发现辅助工具

::

    replaced with revised version Mon, 17 Jun 2024 10:23:46 GMT
    Submission history From: Shinnosuke Tanaka [view email]
    [v1] Thu, 16 May 2024 13:17:14 UTC (334 KB)
    [v2] Mon, 17 Jun 2024 10:23:46 UTC (263 KB)
    Shinnosuke Tanaka, James Barry, Vishnudev Kuruvanthodi, Movina Moses, Maxwell J. Giammona, Nathan Herr, Mohab Elkaref, Geeth De Mel

This paper describes the KnowledgeHub tool, a scientific literature Information Extraction (IE) and Question Answering (QA) pipeline. This is achieved by supporting the ingestion of PDF documents that are converted to text and structured representations. An ontology can then be constructed where a user defines the types of entities and relationships they want to capture. A browser-based annotation tool enables annotating the contents of the PDF documents according to the ontology. Named Entity Recognition (NER) and Relation Classification (RC) models can be trained on the resulting annotations and can be used to annotate the unannotated portion of the documents. A knowledge graph is constructed from these entity and relation triples which can be queried to obtain insights from the data. Furthermore, we integrate a suite of Large Language Models (LLMs) that can be used for QA and summarisation that is grounded in the included documents via a retrieval component. KnowledgeHub is a unique tool that supports annotation, IE and QA, which gives the user full insight into the knowledge discovery pipeline.

------------

------------------------
Retrieval-Augmented (20)
------------------------

`[2406.10937] Understanding Understanding: A Pragmatic Framework Motivated by Large Language Models <https://arxiv.org/abs/2406.10937>`__ 理解理解:由大型语言模型驱动的实用框架

::

    Sun, 16 Jun 2024 13:37:08 GMT
    Kevin Leyton-Brown and Yoav Shoham

Motivated by the rapid ascent of Large Language Models (LLMs) and debates about the extent to which they possess human-level qualities, we propose a framework for testing whether any agent (be it a machine or a human) understands a subject matter. In Turing-test fashion, the framework is based solely on the agent's performance, and specifically on how well it answers questions. Elements of the framework include circumscribing the set of questions (the "scope of understanding"), requiring general competence ("passing grade"), avoiding "ridiculous answers", but still allowing wrong and "I don't know" answers to some questions. Reaching certainty about these conditions requires exhaustive testing of the questions which is impossible for nontrivial scopes, but we show how high confidence can be achieved via random sampling and the application of probabilistic confidence bounds. We also show that accompanying answers with explanations can improve the sample complexity required to achieve acceptable bounds, because an explanation of an answer implies the ability to answer many similar questions. According to our framework, current LLMs cannot be said to understand nontrivial domains, but as the framework provides a practical recipe for testing understanding, it thus also constitutes a tool for building AI agents that do understand.

------------

`[2406.10251] The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs <https://arxiv.org/abs/2406.10251>`__ 量化对检索增强生成的影响:小型llm分析

::

    Mon, 10 Jun 2024 08:23:52 GMT
    Mert Yazan, Suzan Verberne, Frederik Situmeang

Post-training quantization reduces the computational demand of Large Language Models (LLMs) but can weaken some of their capabilities. Since LLM abilities emerge with scale, smaller LLMs are more sensitive to quantization. In this paper, we explore how quantization affects smaller LLMs' ability to perform retrieval-augmented generation (RAG), specifically in longer contexts. We chose personalization for evaluation because it is a challenging domain to perform using RAG as it requires long-context reasoning over multiple documents. We compare the original FP16 and the quantized INT4 performance of multiple 7B and 8B LLMs on two tasks while progressively increasing the number of retrieved documents to test how quantized models fare against longer contexts. To better understand the effect of retrieval, we evaluate three retrieval models in our experiments. Our findings reveal that if a 7B LLM performs the task well, quantization does not impair its performance and long-context reasoning capabilities. We conclude that it is possible to utilize RAG with quantized smaller LLMs.

------------

`[2406.10393] EWEK-QA: Enhanced Web and Efficient Knowledge Graph Retrieval for Citation-based Question Answering Systems <https://arxiv.org/abs/2406.10393>`__ EWEK-QA:面向引文问答系统的增强Web和高效知识图谱检索

::

    Fri, 14 Jun 2024 19:40:38 GMT
    Mohammad Dehghan, Mohammad Ali Alomrani, Sunyam Bagga, David Alfonso-Hermelo, Khalil Bibi, Abbas Ghaddar, Yingxue Zhang, Xiaoguang Li, Jianye Hao, Qun Liu, Jimmy Lin, Boxing Chen, Prasanna Parthasarathi, Mahdi Biparva, Mehdi Rezagholizadeh

The emerging citation-based QA systems are gaining more attention especially in generative AI search applications. The importance of extracted knowledge provided to these systems is vital from both accuracy (completeness of information) and efficiency (extracting the information in a timely manner). In this regard, citation-based QA systems are suffering from two shortcomings.
First, they usually rely only on web as a source of extracted knowledge and adding other external knowledge sources can hamper the efficiency of the system. Second, web-retrieved contents are usually obtained by some simple heuristics such as fixed length or breakpoints which might lead to splitting information into pieces. To mitigate these issues, we propose our enhanced web and efficient knowledge graph (KG) retrieval solution (EWEK-QA) to enrich the content of the extracted knowledge fed to the system. This has been done through designing an adaptive web retriever and incorporating KGs triples in an efficient manner. We demonstrate the effectiveness of EWEK-QA over the open-source state-of-the-art (SoTA) web-based and KG baseline models using a comprehensive set of quantitative and human evaluation experiments. Our model is able to: first, improve the web-retriever baseline in terms of extracting more relevant passages (>20\%), the coverage of answer span (>25\%) and self containment (>35\%); second, obtain and integrate KG triples into its pipeline very efficiently (by avoiding any LLM calls) to outperform the web-only and KG-only SoTA baselines significantly in 7 quantitative QA tasks and our human evaluation.

------------

`[2406.11093] RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning based on Emotional Information <https://arxiv.org/abs/2406.11093>`__ RAEmoLLM:基于情感信息的上下文学习的检索增强llm跨域假信息检测

::

    Sun, 16 Jun 2024 22:49:11 GMT
    Zhiwei Liu, Kailai Yang, Qianqian Xie, Christine de Kock, Sophia Ananiadou, Eduard Hovy

Misinformation is prevalent in various fields such as education, politics, health, etc., causing significant harm to society. However, current methods for cross-domain misinformation detection rely on time and resources consuming fine-tuning and complex model structures. With the outstanding performance of LLMs, many studies have employed them for misinformation detection.
Unfortunately, they focus on in-domain tasks and do not incorporate significant sentiment and emotion features (which we jointly call affect). In this paper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to address cross-domain misinformation detection using in-context learning based on affective information. It accomplishes this by applying an emotion-aware LLM to construct a retrieval database of affective embeddings. This database is used by our retrieval module to obtain source-domain samples, which are subsequently used for the inference module's in-context few-shot learning to detect target domain misinformation. We evaluate our framework on three misinformation benchmarks. Results show that RAEmoLLM achieves significant improvements compared to the zero-shot method on three datasets, with the highest increases of 20.69%, 23.94%, and 39.11% respectively. This work will be released on https://github.com/lzw108/RAEmoLLM.

------------

`[2406.11258] Enhancing Biomedical Knowledge Retrieval-Augmented Generation with Self-Rewarding Tree Search and Proximal Policy Optimization <https://arxiv.org/abs/2406.11258>`__ 基于自奖励树搜索和近端策略优化的生物医学知识检索增强生成

::

    Mon, 17 Jun 2024 06:48:31 GMT
    Minda Hu, Licheng Zong, Hongru Wang, Jingyan Zhou, Jingjing Li, Yichen Gao, Kam-Fai Wong, Yu Li, Irwin King

Large Language Models (LLMs) have shown great potential in the biomedical domain with the advancement of retrieval-augmented generation (RAG). However, existing retrieval-augmented approaches face challenges in addressing diverse queries and documents, particularly for medical knowledge queries, resulting in sub-optimal performance. To address these limitations, we propose a novel plug-and-play LLM-based retrieval method called Self-Rewarding Tree Search (SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm.
By combining the reasoning capabilities of LLMs with the effectiveness of tree search, SeRTS boosts the zero-shot performance of retrieving high-quality and informative results for RAG. We further enhance retrieval performance by fine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the trajectories collected by SeRTS as feedback. Controlled experiments using the BioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method significantly improves the performance of the BM25 retriever and surpasses the strong baseline of self-reflection in both efficiency and scalability.
Moreover, SeRTS generates higher-quality feedback for PPO training than self-reflection. Our proposed method effectively adapts LLMs to document retrieval tasks, enhancing their ability to retrieve highly relevant documents for RAG in the context of medical knowledge queries. This work presents a significant step forward in leveraging LLMs for accurate and comprehensive biomedical question answering.

------------

`[2406.11357] $\textit{Refiner}$: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities <https://arxiv.org/abs/2406.11357>`__ $\textit{Refiner}$有效地重构检索内容以提高问答能力

::

    Mon, 17 Jun 2024 09:25:10 GMT
    Zhonghao Li, Xuming Hu, Aiwei Liu, Kening Zheng, Sirui Huang, Hui Xiong

Large Language Models (LLMs) are limited by their parametric knowledge, leading to hallucinations in knowledge-extensive tasks. To address this, Retrieval-Augmented Generation (RAG) incorporates external document chunks to expand LLM knowledge. Furthermore, compressing information from document chunks through extraction or summarization can improve LLM performance. Nonetheless, LLMs still struggle to notice and utilize scattered key information, a problem known as the "lost-in-the-middle" syndrome. Therefore, we typically need to restructure the content for LLM to recognize the key information. We propose $\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that operates in the post-retrieval process of RAG. $\textit{Refiner}$ leverages a single decoder-only LLM to adaptively extract query-relevant contents verbatim along with the necessary context, and section them based on their interconnectedness, thereby highlights information distinction, and aligns downstream LLMs with the original context effectively. Experiments show that a trained $\textit{Refiner}$ (with 7B parameters) exhibits significant gain to downstream LLM in improving answer accuracy, and outperforms other state-of-the-art advanced RAG and concurrent compressing approaches in various single-hop and multi-hop QA tasks. Notably, $\textit{Refiner}$ achieves a 80.5% tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared to the next best solution. $\textit{Refiner}$ is a plug-and-play solution that can be seamlessly integrated with RAG systems, facilitating its application across diverse open-source frameworks.

------------

`[2406.11497] CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation in RAG <https://arxiv.org/abs/2406.11497>`__ CrAM:基于可信度感知的llm注意力修正以对抗RAG中的错误信息

::

    Mon, 17 Jun 2024 13:01:12 GMT
    Boyi Deng, Wenjie Wang, Fengbin Zhu, Qifan Wang, Fuli Feng

Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large Language Models (LLMs) by referencing external documents. However, the misinformation in external documents may mislead LLMs' generation. To address this issue, we explore the task of "credibility-aware RAG", in which LLMs automatically adjust the influence of retrieved documents based on their credibility scores to counteract misinformation. To this end, we introduce a plug-and-play method named $\textbf{Cr}$edibility-aware $\textbf{A}$ttention $\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in LLMs and adjusts their attention scores based on the credibility of the documents, thereby reducing the impact of low-credibility documents.
Experiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and Qwen-7B show that CrAM improves the RAG performance of LLMs against misinformation pollution by over 20%, even surpassing supervised fine-tuning methods.

------------

`[2406.11681] R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models <https://arxiv.org/abs/2406.11681>`__ R-Eval:检索增强大型语言模型领域知识评估统一工具包

::

    Mon, 17 Jun 2024 15:59:49 GMT
    Shangqing Tu, Yuanchun Wang, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, Juanzi Li

Large language models have achieved remarkable success on general NLP tasks, but they may fall short for domain-specific problems. Recently, various Retrieval-Augmented Large Language Models (RALLMs) are proposed to address this shortcoming. However, existing evaluation tools only provide a few baselines and evaluate them on various domains without mining the depth of domain knowledge. In this paper, we address the challenges of evaluating RALLMs by introducing the R-Eval toolkit, a Python toolkit designed to streamline the evaluation of different RAG workflows in conjunction with LLMs. Our toolkit, which supports popular built-in RAG workflows and allows for the incorporation of customized testing data on the specific domain, is designed to be user-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs across three task levels and two representative domains, revealing significant variations in the effectiveness of RALLMs across different tasks and domains.
Our analysis emphasizes the importance of considering both task and domain requirements when choosing a RAG workflow and LLM combination. We are committed to continuously maintaining our platform at https://github.com/THU-KEG/R-Eval to facilitate both the industry and the researchers.

------------

`[2406.11200] AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval <https://arxiv.org/abs/2406.11200>`__ AvaTaR:优化LLM agent的工具辅助知识检索

::

    Mon, 17 Jun 2024 04:20:02 GMT
    Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Vassilis N. Ioannidis, Karthik Subbian, Jure Leskovec, James Zou

Large language model (LLM) agents have demonstrated impressive capability in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing the prompting techniques that make LLM agents able to effectively use external tools and knowledge is a heuristic and laborious task. Here, we introduce AvaTaR, a novel and automatic framework that optimizes an LLM agent to effectively use the provided tools and improve its performance on a given task/domain. During optimization, we design a comparator module to iteratively provide insightful and holistic prompts to the LLM agent via reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information. We find AvaTaR consistently outperforms state-of-the-art approaches across all four challenging tasks and exhibits strong generalization ability when applied to novel cases, achieving an average relative improvement of 14% on the Hit@1 metric. Code and dataset are available at https://github.com/zou-group/avatar.

------------

`[2406.11780] Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs <https://arxiv.org/abs/2406.11780>`__ 分裂、取消学习、合并:在llm中利用数据属性进行更有效的取消学习

::

    Mon, 17 Jun 2024 17:35:52 GMT
    Swanand Ravindra Kadhe, Farhan Ahmed, Dennis Wei, Nathalie Baracaldo, Inkit Padhi

Large language models (LLMs) have shown to pose social and ethical risks such as generating toxic language or facilitating malicious use of hazardous knowledge. Machine unlearning is a promising approach to improve LLM safety by directly removing harmful behaviors and knowledge. In this paper, we propose "SPlit, UNlearn, MerGE" (SPUNGE), a framework that can be used with any unlearning method to amplify its effectiveness. SPUNGE leverages data attributes during unlearning by splitting unlearning data into subsets based on specific attribute values, unlearning each subset separately, and merging the unlearned models. We empirically demonstrate that SPUNGE significantly improves the performance of two recent unlearning methods on state-of-the-art LLMs while maintaining their general capabilities on standard academic benchmarks.

------------

`[2406.11147] Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG <https://arxiv.org/abs/2406.11147>`__ vulg -RAG:利用知识级RAG增强基于llm的漏洞检测

::

    Mon, 17 Jun 2024 02:25:45 GMT
    Xueying Du, Geng Zheng, Kaixin Wang, Jiayi Feng, Wentai Deng, Mingwei Liu, Xin Peng, Tao Ma, Yiling Lou

Vulnerability detection is essential for software quality assurance. In recent years, deep learning models (especially large language models) have shown promise in vulnerability detection. In this work, we propose a novel LLM-based vulnerability detection technique Vul-RAG, which leverages knowledge-level retrieval-augmented generation (RAG) framework to detect vulnerability for the given code in three phases. First, Vul-RAG constructs a vulnerability knowledge base by extracting multi-dimension knowledge via LLMs from existing CVE instances; second, for a given code snippet, Vul-RAG} retrieves the relevant vulnerability knowledge from the constructed knowledge base based on functional semantics; third, Vul-RAG leverages LLMs to check the vulnerability of the given code snippet by reasoning the presence of vulnerability causes and fixing solutions of the retrieved vulnerability knowledge. Our evaluation of Vul-RAG on our constructed benchmark PairVul shows that Vul-RAG substantially outperforms all baselines by 12.96\%/110\% relative improvement in accuracy/pairwise-accuracy. In addition, our user study shows that the vulnerability knowledge generated by Vul-RAG can serve as high-quality explanations which can improve the manual detection accuracy from 0.60 to 0.77.

------------

`[2406.11424] Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability <https://arxiv.org/abs/2406.11424>`__ 评估开源LLMs在企业专用RAG系统中的有效性:性能和可扩展性的比较研究

::

    Mon, 17 Jun 2024 11:22:25 GMT
    Gautam B and Anupam Purwar

This paper presents an analysis of open-source large language models (LLMs) and their application in Retrieval-Augmented Generation (RAG) tasks, specific for enterprise-specific data sets scraped from their websites. With the increasing reliance on LLMs in natural language processing, it is crucial to evaluate their performance, accessibility, and integration within specific organizational contexts. This study examines various open-source LLMs, explores their integration into RAG frameworks using enterprise-specific data, and assesses the performance of different open-source embeddings in enhancing the retrieval and generation process. Our findings indicate that open-source LLMs, combined with effective embedding techniques, can significantly improve the accuracy and efficiency of RAG systems, offering a viable alternative to proprietary solutions for enterprises.

------------

`[2405.06211] A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2405.06211>`__ RAG Meeting llm综述:面向检索增强的大型语言模型

::

    replaced with revised version Mon, 17 Jun 2024 08:56:38 GMT
    Submission history From: Yujuan Ding [view email]
    [v1] Fri, 10 May 2024 02:48:45 UTC (823 KB)
    [v2] Fri, 14 Jun 2024 13:07:27 UTC (1,484 KB)
    [v3] Mon, 17 Jun 2024 08:56:38 UTC (1,487 KB)
    Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li

As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations, such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: architectures, training strategies, and applications. As the preliminary knowledge, we briefly introduce the foundations and recent advances of LLMs. Then, to illustrate the practical significance of RAG for LLMs, we systematically review mainstream relevant work by their architectures, training strategies, and application areas, detailing specifically the challenges of each and the corresponding capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at this https URL

------------

`[2405.18111] ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator <https://arxiv.org/abs/2405.18111>`__ ATM:对抗性调优多智能体系统构建鲁棒的检索增强生成器

::

    replaced with revised version Sun, 16 Jun 2024 12:30:32 GMT
    Submission history From: Junda Zhu [view email]
    [v1] Tue, 28 May 2024 12:18:50 UTC (659 KB)
    [v2] Sun, 16 Jun 2024 12:30:32 UTC (847 KB)
    Junda Zhu, Lingyong Yan, Haibo Shi, Dawei Yin, Lei Sha

Large language models (LLMs) are proven to benefit a lot from retrieval-augmented generation (RAG) in alleviating hallucinations confronted with knowledge-intensive questions. RAG adopts information retrieval techniques to inject external knowledge from semantic-relevant documents as input contexts. However, due to today's Internet being flooded with numerous noisy and fabricating content, it is inevitable that RAG systems are vulnerable to these noises and prone to respond incorrectly. To this end, we propose to optimize the retrieval-augmented Generator with a Adversarial Tuning Multi-agent system (ATM). The ATM steers the Generator to have a robust perspective of useful documents for question answering with the help of an auxiliary Attacker agent. The Generator and the Attacker are tuned adversarially for several iterations. After rounds of multi-agent iterative tuning, the Generator can eventually better discriminate useful documents amongst fabrications. The experimental results verify the effectiveness of ATM and we also observe that the Generator can achieve better performance compared to state-of-the-art baselines.

------------

`[2406.05654] DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation <https://arxiv.org/abs/2406.05654>`__ 

::

    replaced with revised version Mon, 17 Jun 2024 03:01:39 GMT
    Submission history From: Shuting Wang [view email]
    [v1] Sun, 9 Jun 2024 05:33:51 UTC (609 KB)
    [v2] Mon, 17 Jun 2024 03:01:39 UTC (609 KB)
    Shuting Wang, Jiongnan Liu, Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang, Yutao Zhu, Zhicheng Dou

Retrieval-Augmented Generation (RAG) offers a promising solution to address various limitations of Large Language Models (LLMs), such as hallucination and difficulties in keeping up with real-time updates. This approach is particularly critical in expert and domain-specific applications where LLMs struggle to cover expert knowledge. Therefore, evaluating RAG models in such scenarios is crucial, yet current studies often rely on general knowledge sources like Wikipedia to assess the models' abilities in solving common-sense problems. In this paper, we evaluated LLMs by RAG settings in a domain-specific context, college enrollment. We identified six required abilities for RAG models, including the ability in conversational RAG, analyzing structural information, faithfulness to external knowledge, denoising, solving time-sensitive problems, and understanding multi-document interactions. Each ability has an associated dataset with shared corpora to evaluate the RAG models' performance. We evaluated popular LLMs such as Llama, Baichuan, ChatGLM, and GPT models. Experimental results indicate that existing closed-book LLMs struggle with domain-specific questions, highlighting the need for RAG models to solve expert problems. Moreover, there is room for RAG models to improve their abilities in comprehending conversational history, analyzing structural information, denoising, processing multi-document interactions, and faithfulness in expert knowledge. We expect future studies could solve these problems better.

------------

`[2406.05794] RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation <https://arxiv.org/abs/2406.05794>`__ RE-RAG:用检索增强生成中的相关估计器提高开放域QA性能和可解释性

::

    replaced with revised version Sun, 16 Jun 2024 13:28:24 GMT
    Submission history From: Kiseung Kim [view email]
    [v1] Sun, 9 Jun 2024 14:11:19 UTC (8,280 KB)
    [v2] Sun, 16 Jun 2024 13:28:24 UTC (8,136 KB)
    Kiseung Kim, Jay-Yoon Lee

The Retrieval Augmented Generation (RAG) framework utilizes a combination of parametric knowledge and external knowledge to demonstrate state-of-the-art performance on open-domain question answering tasks. However, the RAG framework suffers from performance degradation when the query is accompanied by irrelevant contexts. In this work, we propose the RE-RAG framework, which introduces a relevance estimator (RE) that not only provides relative relevance between contexts as previous rerankers did, but also provides confidence, which can be used to classify whether given context is useful for answering the given question. We propose a weakly supervised method for training the RE simply utilizing question-answer data without any labels for correct contexts. We show that RE trained with a small generator (sLM) can not only improve the sLM fine-tuned together with RE but also improve previously unreferenced large language models (LLMs). Furthermore, we investigate new decoding strategies that utilize the proposed confidence measured by RE such as choosing to let the user know that it is "unanswerable" to answer the question given the retrieved contexts or choosing to rely on LLM's parametric knowledge rather than unrelated contexts.

------------

`[2309.16595] Can LLMs Effectively Leverage Graph Structural Information through Prompts, and Why? <https://arxiv.org/abs/2309.16595>`__ llm能否通过提示有效利用图结构信息，为什么?

::

    replaced with revised version Sat, 15 Jun 2024 09:26:21 GMT
    Submission history From: Jin Huang [view email]
    [v1] Thu, 28 Sep 2023 16:58:37 UTC (206 KB)
    [v2] Fri, 29 Sep 2023 20:59:55 UTC (207 KB)
    [v3] Tue, 27 Feb 2024 00:45:03 UTC (492 KB)
    [v4] Sat, 15 Jun 2024 09:26:21 UTC (590 KB)
    Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma

Large language models (LLMs) are gaining increasing attention for their capability to process graphs with rich text attributes, especially in a zero-shot fashion. Recent studies demonstrate that LLMs obtain decent text classification performance on common text-rich graph benchmarks, and the performance can be improved by appending encoded structural information as natural languages into prompts. We aim to understand why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs. First, we rule out the concern of data leakage by curating a novel leakage-free dataset and conducting a comparative analysis alongside a previously widely-used dataset. Second, as past work usually encodes the ego-graph by describing the graph structure in natural language, we ask the question: do LLMs understand the graph structure in accordance with the intent of the prompt designers? Third, we investigate why LLMs can improve their performance after incorporating structural information. Our exploration of these questions reveals that (i) there is no substantial evidence that the performance of LLMs is significantly attributed to data leakage; (ii) instead of understanding prompts as graph structures as intended by the prompt designers, LLMs tend to process prompts more as contextual paragraphs and (iii) the most efficient elements of the local neighborhood included in the prompt are phrases that are pertinent to the node label, rather than the graph structure.

------------

`[2406.07348] DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation for Question-Answering <https://arxiv.org/abs/2406.07348>`__ DR-RAG:将动态文档相关性应用于问答检索增强生成

::

    replaced with revised version Sun, 16 Jun 2024 04:33:17 GMT
    Submission history From: Wenjie Ou [view email]
    [v1] Tue, 11 Jun 2024 15:15:33 UTC (167 KB)
    [v2] Wed, 12 Jun 2024 01:06:59 UTC (167 KB)
    [v3] Sun, 16 Jun 2024 04:33:17 UTC (183 KB)
    Zijian Hei and Weiling Liu and Wenjie Ou and Juyi Qiao and Junming Jiao and Guowen Song and Ting Tian and Yi Lin

Retrieval-Augmented Generation (RAG) has recently demonstrated the performance of Large Language Models (LLMs) in the knowledge-intensive tasks such as Question-Answering (QA). RAG expands the query context by incorporating external knowledge bases to enhance the response accuracy. However, it would be inefficient to access LLMs multiple times for each query and unreliable to retrieve all the relevant documents by a single query. We have found that even though there is low relevance between some critical documents and query, it is possible to retrieve the remaining documents by combining parts of the documents with the query. To mine the relevance, a two-stage retrieval framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is proposed to improve document retrieval recall and the accuracy of answers while maintaining efficiency. Additionally, a compact classifier is applied to two different selection strategies to determine the contribution of the retrieved documents to answering the query and retrieve the relatively relevant documents. Meanwhile, DR-RAG call the LLMs only once, which significantly improves the efficiency of the experiment. The experimental results on multi-hop QA datasets show that DR-RAG can significantly improve the accuracy of the answers and achieve new progress in QA systems.

------------

`[2307.08303] Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models <https://arxiv.org/abs/2307.08303>`__ 用大型语言模型增强密集检索的软提示调优

::

    replaced with revised version Mon, 17 Jun 2024 04:30:58 GMT
    Submission history From: Zhiyuan Peng [view email]
    [v1] Mon, 17 Jul 2023 07:55:47 UTC (690 KB)
    [v2] Tue, 25 Jul 2023 14:57:05 UTC (675 KB)
    [v3] Tue, 29 Aug 2023 21:52:58 UTC (685 KB)
    [v4] Sat, 10 Feb 2024 22:46:29 UTC (232 KB)
    [v5] Mon, 17 Jun 2024 04:30:58 UTC (237 KB)
    Zhiyuan Peng, Xuyang Wu, Qifan Wang, Yi Fang

Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific dense retrievers. We design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries. To the best of our knowledge, there is no prior work utilizing soft prompt tuning to augment DR models. The experiments demonstrate that SPTAR outperforms the unsupervised baselines BM25 and the recently proposed LLMs-based augmentation method for DR.

------------

`[2406.07113] Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene Graph <https://arxiv.org/abs/2406.07113>`__ 超越简单查询:基于3D场景图的开放词汇表对象检索

::

    replaced with revised version Mon, 17 Jun 2024 13:55:40 GMT
    Submission history From: Sergey Linok [view email]
    [v1] Tue, 11 Jun 2024 09:57:04 UTC (9,276 KB)
    [v2] Mon, 17 Jun 2024 13:55:40 UTC (9,277 KB)
    Sergey Linok, Tatiana Zemskova, Svetlana Ladanova, Roman Titkov, Dmitry Yudin

Locating objects referred to in natural language poses a significant challenge for autonomous agents. Existing CLIP-based open-vocabulary methods successfully perform 3D object retrieval with simple (bare) queries but cannot cope with ambiguous descriptions that demand an understanding of object relations. To tackle this problem, we propose a modular approach called BBQ (Beyond Bare Queries), which constructs 3D scene spatial graph representation with metric edges and utilizes a large language model as a human-to-agent interface through our deductive scene reasoning algorithm. BBQ employs robust DINO-powered associations to form 3D objects, an advanced raycasting algorithm to project them to 2D, and a vision-language model to describe them as graph nodes. On Replica and ScanNet datasets, we show that the designed method accurately constructs 3D object-centric maps. We have demonstrated that their quality takes a leading place for open-vocabulary 3D semantic segmentation against other zero-shot methods. Also, we show that leveraging spatial relations is especially effective for scenes containing multiple entities of the same semantic class. On Sr3D and Nr3D benchmarks, our deductive approach demonstrates a significant improvement, enabling retrieving objects by complex queries compared to other state-of-the-art methods. Considering our design solutions, we achieved a processing speed approximately x3 times faster than the closest analog. This promising performance enables our approach for usage in applied intelligent robotics projects. We make the code publicly available at this http URL.

------------

----------
Agent (18)
----------

`[2406.10291] ResearchArena: Benchmarking LLMs' Ability to Collect and Organize Information as Research Agents <https://arxiv.org/abs/2406.10291>`__ ResearchArena:对llm作为研究代理收集和组织信息的能力进行基准测试

::

    Thu, 13 Jun 2024 03:26:30 GMT
    Hao Kang, Chenyan Xiong

Large language models (LLMs) have exhibited remarkable performance across various tasks in natural language processing. Nevertheless, challenges still arise when these tasks demand domain-specific expertise and advanced analytical skills, such as conducting research surveys on a designated topic. In this research, we develop ResearchArena, a benchmark that measures LLM agents' ability to conduct academic surveys, an initial step of academic research process. Specifically, we deconstructs the surveying process into three stages 1) information discovery: locating relevant papers, 2) information selection: assessing papers' importance to the topic, and 3) information organization: organizing papers into meaningful structures. In particular, we establish an offline environment comprising 12.0M full-text academic papers and 7.9K survey papers, which evaluates agents' ability to locate supporting materials for composing the survey on a topic, rank the located papers based on their impact, and organize these into a hierarchical knowledge mind-map. With this benchmark, we conduct preliminary evaluations of existing techniques and find that all LLM-based methods under-performing when compared to basic keyword-based retrieval techniques, highlighting substantial opportunities for future research.

------------

`[2406.11638] MASAI: Modular Architecture for Software-engineering AI Agents <https://arxiv.org/abs/2406.11638>`__ MASAI:面向软件工程AI智能体的模块化架构

::

    Mon, 17 Jun 2024 15:19:51 GMT
    Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, Nagarajan Natarajan

A common method to solve complex problems in software engineering, is to divide the problem into multiple sub-problems. Inspired by this, we propose a Modular Architecture for Software-engineering AI (MASAI) agents, where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives. Our modular architecture offers several advantages: (1) employing and tuning different problem-solving strategies across sub-agents, (2) enabling sub-agents to gather information from different sources scattered throughout a repository, and (3) avoiding unnecessarily long trajectories which inflate costs and add extraneous context.
MASAI enabled us to achieve the highest performance (28.33% resolution rate) on the popular and highly challenging SWE-bench Lite dataset consisting of 300 GitHub issues from 11 Python repositories. We conduct a comprehensive evaluation of MASAI relative to other agentic methods and analyze the effects of our design decisions and their contribution to the success of MASAI.

------------

`[2406.10478] From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent <https://arxiv.org/abs/2406.10478>`__ 从文字到世界:用交流的LLM Agent将单行提示转换为沉浸式多模态数字故事

::

    Sat, 15 Jun 2024 03:03:43 GMT
    Samuel S. Sohn and Danrui Li and Sen Zhang and Che-Jui Chang and Mubbasir Kapadia

Digital storytelling, essential in entertainment, education, and marketing, faces challenges in production scalability and flexibility. The StoryAgent framework, introduced in this paper, utilizes Large Language Models and generative tools to automate and refine digital storytelling. Employing a top-down story drafting and bottom-up asset generation approach, StoryAgent tackles key issues such as manual intervention, interactive scene orchestration, and narrative consistency. This framework enables efficient production of interactive and consistent narratives across multiple modalities, democratizing content creation and enhancing engagement. Our results demonstrate the framework's capability to produce coherent digital stories without reference videos, marking a significant advancement in automated digital storytelling.

------------

`[2406.11132] RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents <https://arxiv.org/abs/2406.11132>`__ RePrompt:基于自动提示工程的大型语言模型agent规划

::

    Mon, 17 Jun 2024 01:23:11 GMT
    Weizhe Chen, Sven Koenig, Bistra Dilkina

In this past year, large language models (LLMs) have had remarkable success in domains outside the traditional natural language processing, and people are starting to explore the usage of LLMs in more general and close to application domains like code generation, travel planning, and robot controls. Connecting these LLMs with great capacity and external tools, people are building the so-called LLM agents, which are supposed to help people do all kinds of work in everyday life. In all these domains, the prompt to the LLMs has been shown to make a big difference in what the LLM would generate and thus affect the performance of the LLM agents. Therefore, automatic prompt engineering has become an important question for many researchers and users of LLMs. In this paper, we propose a novel method, \textsc{RePrompt}, which does "gradient descent" to optimize the step-by-step instructions in the prompt of the LLM agents based on the chat history obtained from interactions with LLM agents. By optimizing the prompt, the LLM will learn how to plan in specific domains. We have used experiments in PDDL generation and travel planning to show that our method could generally improve the performance for different reasoning tasks when using the updated prompt as the initial prompt.

------------

`[2406.11176] Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement <https://arxiv.org/abs/2406.11176>`__ 注意每一步!通过迭代步骤级过程细化的LLM代理学习

::

    Mon, 17 Jun 2024 03:29:13 GMT
    Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, Sujian Li

Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the Iterative step-level Process Refinement (IPR) framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical findings highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models.

------------

`[2406.11277] Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector <https://arxiv.org/abs/2406.11277>`__ 小经纪人也可以摇滚!赋能小语言模型作为幻觉探测器

::

    Mon, 17 Jun 2024 07:30:05 GMT
    Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Hongzhi Zhang, Fuzheng Zhang, Di Zhang, Kun Gai, Ji-Rong Wen

Hallucination detection is a challenging task for large language models (LLMs), and existing studies heavily rely on powerful closed-source LLMs such as GPT-4. In this paper, we propose an autonomous LLM-based agent framework, called HaluAgent, which enables relatively smaller LLMs (e.g. Baichuan2-Chat 7B) to actively select suitable tools for detecting multiple hallucination types such as text, code, and mathematical expression. In HaluAgent, we integrate the LLM, multi-functional toolbox, and design a fine-grained three-stage detection framework along with memory mechanism. To facilitate the effectiveness of HaluAgent, we leverage existing Chinese and English datasets to synthesize detection trajectories for fine-tuning, which endows HaluAgent with the capability for bilingual hallucination detection. Extensive experiments demonstrate that only using 2K samples for tuning LLMs, HaluAgent can perform hallucination detection on various types of tasks and datasets, achieving performance comparable to or even higher than GPT-4 without tool enhancements on both in-domain and out-of-domain datasets. We release our dataset and code at https://github.com/RUCAIBox/HaluAgent.

------------

`[2406.11555] Input Conditioned Graph Generation for Language Agents <https://arxiv.org/abs/2406.11555>`__ 语言智能体的输入条件图生成

::

    Mon, 17 Jun 2024 13:53:15 GMT
    Lukas Vierling, Jie Fu, Kai Chen

Recent progress in Large Language Models (LLMs) and language agents has demonstrated significant promise for various future applications across multiple disciplines. While traditional approaches to language agents often rely on fixed, handcrafted designs, our research aims to develop both learnable and dynamic agents. Our method uses an existing framework that abstracts language agents as graphs. Within this graph framework, we aim to learn a model that can generate edges for every given input to the language agent. This allows us to generate edges that represent the flow of communication within the graph based on the given input, thereby adjusting the internal communication of a language agent. We learn to generate these edges using a pretrained LLM that is fine-tuned with reinforcement learning. This LLM can be fine-tuned on several datasets simultaneously, and we hypothesize that the model learns to adapt to these different domains during training, achieving good overall performance when encountering data from different domains during deployment. We demonstrate that our approach surpasses the previous static approach by nearly 6% accuracy on a combined dataset of MMLU and CMMLU, and by more than 10% when trained with a sparsity-inducing loss. It also performs superior in additional experiments conducted with the MMLU and Mini Crossword Puzzles datasets. The code is available at https://github.com/lukasVierling/DynamicGPTSwarm.

------------

`[2406.11776] Improving Multi-Agent Debate with Sparse Communication Topology <https://arxiv.org/abs/2406.11776>`__ 稀疏通信拓扑改进多智能体辩论

::

    Mon, 17 Jun 2024 17:33:09 GMT
    Yunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski, Yeqing Li, Eugene Ie

Multi-agent debate has proven effective in improving large language models quality for reasoning and factuality tasks. While various role-playing strategies in multi-agent debates have been explored, in terms of the communication among agents, existing approaches adopt a brute force algorithm -- each agent can communicate with all other agents. In this paper, we systematically investigate the effect of communication connectivity in multi-agent systems. Our experiments on GPT and Mistral models reveal that multi-agent debates leveraging sparse communication topology can achieve comparable or superior performance while significantly reducing computational costs. Furthermore, we extend the multi-agent debate framework to multimodal reasoning and alignment labeling tasks, showcasing its broad applicability and effectiveness. Our findings underscore the importance of communication connectivity on enhancing the efficiency and effectiveness of the "society of minds" approach.

------------

`[2406.10521] MALLM-GAN: Multi-Agent Large Language Model as Generative Adversarial Network for Synthesizing Tabular Data <https://arxiv.org/abs/2406.10521>`__ MALLM-GAN:用于表格数据合成的多智能体大型语言模型生成对抗网络

::

    Sat, 15 Jun 2024 06:26:17 GMT
    Yaobin Ling, Xiaoqian Jiang, Yejin Kim

In the era of big data, access to abundant data is crucial for driving research forward. However, such data is often inaccessible due to privacy concerns or high costs, particularly in healthcare domain. Generating synthetic (tabular) data can address this, but existing models typically require substantial amounts of data to train effectively, contradicting our objective to solve data scarcity. To address this challenge, we propose a novel framework to generate synthetic tabular data, powered by large language models (LLMs) that emulates the architecture of a Generative Adversarial Network (GAN). By incorporating data generation process as contextual information and utilizing LLM as the optimizer, our approach significantly enhance the quality of synthetic data generation in common scenarios with small sample sizes. Our experimental results on public and private datasets demonstrate that our model outperforms several state-of-art models regarding generating higher quality synthetic data for downstream tasks while keeping privacy of the real data.

------------

`[2406.11200] AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval <https://arxiv.org/abs/2406.11200>`__ AvaTaR:优化LLM agent的工具辅助知识检索

::

    Mon, 17 Jun 2024 04:20:02 GMT
    Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Vassilis N. Ioannidis, Karthik Subbian, Jure Leskovec, James Zou

Large language model (LLM) agents have demonstrated impressive capability in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing the prompting techniques that make LLM agents able to effectively use external tools and knowledge is a heuristic and laborious task. Here, we introduce AvaTaR, a novel and automatic framework that optimizes an LLM agent to effectively use the provided tools and improve its performance on a given task/domain. During optimization, we design a comparator module to iteratively provide insightful and holistic prompts to the LLM agent via reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information. We find AvaTaR consistently outperforms state-of-the-art approaches across all four challenging tasks and exhibits strong generalization ability when applied to novel cases, achieving an average relative improvement of 14% on the Hit@1 metric. Code and dataset are available at https://github.com/zou-group/avatar.

------------

`[2406.10819] GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents <https://arxiv.org/abs/2406.10819>`__ GUI-WORLD:面向gui的多模态llm代理数据集

::

    Sun, 16 Jun 2024 06:56:53 GMT
    Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, Tianshuo Zhou, Yue Yu, Chujie Gao, Qihui Zhang, Yi Gui, Zhen Li, Yao Wan, Pan Zhou, Jianfeng Gao, Lichao Sun

Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding code. However, current agents primarily exhibit excellent understanding capabilities in static environments and are predominantly applied in relatively simple domains, such as Web or mobile interfaces. We argue that a robust GUI agent should be capable of perceiving temporal information on the GUI, including dynamic Web content and multi-step tasks. Additionally, it should possess a comprehensive understanding of various GUI scenarios, including desktop software and multi-window interactions. To this end, this paper introduces a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM annotations, extensively covering six GUI scenarios and eight types of GUI-oriented questions in three formats. We evaluate the capabilities of current state-of-the-art MLLMs, including ImageLLMs and VideoLLMs, in understanding various types of GUI content, especially dynamic and sequential content. Our findings reveal that ImageLLMs struggle with dynamic GUI content without manually annotated keyframes or operation history. On the other hand, VideoLLMs fall short in all GUI-oriented tasks given the sparse GUI video dataset. Based on GUI-World, we take the initial step of leveraging a fine-tuned VideoLLM as a GUI agent, demonstrating an improved understanding of various GUI tasks.
However, due to the limitations in the performance of base LLMs, we conclude that using VideoLLMs as GUI agents remains a significant challenge. We believe our work provides valuable insights for future research in dynamic GUI content understanding. The code and dataset are publicly available at our project homepage: https://gui-world.github.io/.

------------

`[2406.05804] A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components <https://arxiv.org/abs/2406.05804>`__ 

::

    replaced with revised version Sun, 16 Jun 2024 00:59:27 GMT
    Submission history From: Xinzhe Li [view email]
    [v1] Sun, 9 Jun 2024 14:42:55 UTC (9,288 KB)
    [v2] Sun, 16 Jun 2024 00:59:27 UTC (9,290 KB)
    Xinzhe Li

Recent advancements in Large Language Models (LLMs) have catalyzed the development of sophisticated frameworks for developing LLM-based agents. However, the complexity of these frameworks r poses a hurdle for nuanced differentiation at a granular level, a critical aspect for enabling efficient implementations across different frameworks and fostering future research. Hence, the primary purpose of this survey is to facilitate a cohesive understanding of diverse recently proposed frameworks by identifying common workflows and reusable LLM-Profiled Components (LMPCs).

------------

`[2312.11242] MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL <https://arxiv.org/abs/2312.11242>`__ MAC-SQL:一个多agent协作的Text-to-SQL框架

::

    replaced with revised version Mon, 17 Jun 2024 02:27:32 GMT
    Submission history From: Bing Wang [view email]
    [v1] Mon, 18 Dec 2023 14:40:20 UTC (8,499 KB)
    [v2] Tue, 26 Dec 2023 03:25:20 UTC (917 KB)
    [v3] Thu, 15 Feb 2024 12:55:55 UTC (8,579 KB)
    [v4] Mon, 17 Jun 2024 02:27:32 UTC (660 KB)
    Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, Linzheng Chai, Zhao Yan, Qian-Wen Zhang, Di Yin, Xing Sun, Zhoujun Li

Recent LLM-based Text-to-SQL methods usually suffer from significant performance degradation on "huge" databases and complex user questions that require multi-step reasoning. Moreover, most existing methods neglect the crucial significance of LLMs utilizing external tools and model collaboration. To address these challenges, we introduce MAC-SQL, a novel LLM-based multi-agent collaborative framework. Our framework comprises a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning, accompanied by two auxiliary agents that utilize external tools or models to acquire smaller sub-databases and refine erroneous SQL queries. The decomposer agent collaborates with auxiliary agents, which are activated as needed and can be expanded to accommodate new features or tools for effective Text-to-SQL parsing. In our framework, We initially leverage GPT-4 as the strong backbone LLM for all agent tasks to determine the upper bound of our framework. We then fine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging Code Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that SQL-Llama achieves a comparable execution accuracy of 43.94, compared to the baseline accuracy of 46.35 for vanilla GPT-4. At the time of writing, MAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the BIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test set (this https URL).

------------

`[2404.09911] ChatShop: Interactive Information Seeking with Language Agents <https://arxiv.org/abs/2404.09911>`__ ChatShop:基于语言代理的交互式信息搜索

::

    replaced with revised version Sun, 16 Jun 2024 15:24:12 GMT
    Submission history From: Sanxing Chen [view email]
    [v1] Mon, 15 Apr 2024 16:35:41 UTC (937 KB)
    [v2] Sun, 16 Jun 2024 15:24:12 UTC (945 KB)
    Sanxing Chen, Sam Wiseman, Bhuwan Dhingra

The desire and ability to seek new information strategically are fundamental to human learning but often overlooked in current language agent evaluation. We analyze a popular web shopping task designed to test language agents' ability to perform strategic exploration and discover that it can be reformulated and solved as a single-turn retrieval task without the need for interactive information seeking. This finding encourages us to rethink realistic constraints on information access that would necessitate strategic information seeking. We then redesign the task to introduce a notion of task ambiguity and the role of a shopper, serving as a dynamic party with whom the agent strategically interacts in an open-ended conversation to make informed decisions. Our experiments demonstrate that the proposed task can effectively evaluate the agent's ability to explore and gradually accumulate information through multi-turn interactions. Additionally, we show that large language model-simulated shoppers serve as a good proxy for real human shoppers, revealing similar error patterns in agents.

------------

`[2404.17662] PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games <https://arxiv.org/abs/2404.17662>`__ 玩家*:增强凶杀悬疑游戏中基于llm的多智能体通信和交互

::

    replaced with revised version Mon, 17 Jun 2024 12:39:47 GMT
    Submission history From: Qinglin Zhu [view email]
    [v1] Fri, 26 Apr 2024 19:07:30 UTC (1,754 KB)
    [v2] Mon, 17 Jun 2024 12:39:47 UTC (2,816 KB)
    Qinglin Zhu, Runcong Zhao, Jinhua Du, Lin Gui, Yulan He

We propose PLAYER*, a novel framework that addresses the limitations of existing agent-based approaches built on Large Language Models (LLMs) in handling complex questions and understanding interpersonal relationships in dynamic environments. PLAYER* enhances path planning in Murder Mystery Games (MMGs) using an anytime sampling-based planner and a questioning-driven search framework. By equipping agents with a set of sensors, PLAYER* eliminates the need for pre-defined questions and enables agents to navigate complex social interactions. We additionally make a contribution by introducing a quantifiable evaluation method using multiple-choice questions and present WellPlay, a dataset containing 1,482 question-answer pairs. Experimental results demonstrate PLAYER*'s superiority over existing multi-agent methods, enhancing the generalisability and adaptability of agents in MMGs and paving the way for more effective multi-agent interactions.

------------

`[2405.18111] ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator <https://arxiv.org/abs/2405.18111>`__ ATM:对抗性调优多智能体系统构建鲁棒的检索增强生成器

::

    replaced with revised version Sun, 16 Jun 2024 12:30:32 GMT
    Submission history From: Junda Zhu [view email]
    [v1] Tue, 28 May 2024 12:18:50 UTC (659 KB)
    [v2] Sun, 16 Jun 2024 12:30:32 UTC (847 KB)
    Junda Zhu, Lingyong Yan, Haibo Shi, Dawei Yin, Lei Sha

Large language models (LLMs) are proven to benefit a lot from retrieval-augmented generation (RAG) in alleviating hallucinations confronted with knowledge-intensive questions. RAG adopts information retrieval techniques to inject external knowledge from semantic-relevant documents as input contexts. However, due to today's Internet being flooded with numerous noisy and fabricating content, it is inevitable that RAG systems are vulnerable to these noises and prone to respond incorrectly. To this end, we propose to optimize the retrieval-augmented Generator with a Adversarial Tuning Multi-agent system (ATM). The ATM steers the Generator to have a robust perspective of useful documents for question answering with the help of an auxiliary Attacker agent. The Generator and the Attacker are tuned adversarially for several iterations. After rounds of multi-agent iterative tuning, the Generator can eventually better discriminate useful documents amongst fabrications. The experimental results verify the effectiveness of ATM and we also observe that the Generator can achieve better performance compared to state-of-the-art baselines.

------------

`[2403.07718] WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks? <https://arxiv.org/abs/2403.07718>`__ 工作领域:Web代理在解决常见知识工作任务方面的能力如何?

::

    replaced with revised version Fri, 14 Jun 2024 19:22:26 GMT
    Submission history From: Maxime Gasse [view email]
    [v1] Tue, 12 Mar 2024 14:58:45 UTC (4,544 KB)
    [v2] Sun, 21 Apr 2024 14:38:00 UTC (4,553 KB)
    [v3] Fri, 14 Jun 2024 19:22:26 UTC (6,309 KB)
    Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, L\'eo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, Alexandre Lacoste

We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 33 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.

------------

`[2402.16333] Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation <https://arxiv.org/abs/2402.16333>`__ 揭示真相促进变革:基于agent的大规模社会运动模拟

::

    replaced with revised version Mon, 17 Jun 2024 05:37:35 GMT
    Submission history From: Xinyi Mou [view email]
    [v1] Mon, 26 Feb 2024 06:28:54 UTC (7,279 KB)
    [v2] Mon, 17 Jun 2024 05:37:35 UTC (1,133 KB)
    Xinyi Mou, Zhongyu Wei, Xuanjing Huang

Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework HiSim for social media user simulation, wherein users are categorized into two types. Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent-based models. We further construct a Twitter-like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets. Experimental results demonstrate the effectiveness and flexibility of our method.

------------

-----------
Other (274)
-----------

`[2406.10249] A Reality check of the benefits of LLM in business <https://arxiv.org/abs/2406.10249>`__ 对LLM在商业中的好处的现实检查

::

    Sun, 9 Jun 2024 02:36:00 GMT
    Ming Cheung

Large language models (LLMs) have achieved remarkable performance in language understanding and generation tasks by leveraging vast amounts of online texts.
Unlike conventional models, LLMs can adapt to new domains through prompt engineering without the need for retraining, making them suitable for various business functions, such as strategic planning, project implementation, and data-driven decision-making. However, their limitations in terms of bias, contextual understanding, and sensitivity to prompts raise concerns about their readiness for real-world applications. This paper thoroughly examines the usefulness and readiness of LLMs for business processes. The limitations and capacities of LLMs are evaluated through experiments conducted on four accessible LLMs using real-world data. The findings have significant implications for organizations seeking to leverage generative AI and provide valuable insights into future research directions. To the best of our knowledge, this represents the first quantified study of LLMs applied to core business operations and challenges.

------------

`[2406.10268] Autograding Mathematical Induction Proofs with Natural Language Processing <https://arxiv.org/abs/2406.10268>`__ 基于自然语言处理的数学归纳法证明自动分级

::

    Tue, 11 Jun 2024 15:30:26 GMT
    Chenyan Zhao, Mariana Silva, and Seth Poulsen

In mathematical proof education, there remains a need for interventions that help students learn to write mathematical proofs. Research has shown that timely feedback can be very helpful to students learning new skills. While for many years natural language processing models have struggled to perform well on tasks related to mathematical texts, recent developments in natural language processing have created the opportunity to complete the task of giving students instant feedback on their mathematical proofs. In this paper, we present a set of training methods and models capable of autograding freeform mathematical proofs by leveraging existing large language models and other machine learning techniques. The models are trained using proof data collected from four different proof by induction problems. We use four different robust large language models to compare their performances, and all achieve satisfactory performances to various degrees. Additionally, we recruit human graders to grade the same proofs as the training data, and find that the best grading model is also more accurate than most human graders. With the development of these grading models, we create and deploy an autograder for proof by induction problems and perform a user study with students. Results from the study shows that students are able to make significant improvements to their proofs using the feedback from the autograder, but students still do not trust the AI autograders as much as they trust human graders. Future work can improve on the autograder feedback and figure out ways to help students trust AI autograders.

------------

`[2406.10479] Unlocking Large Language Model's Planning Capabilities with Maximum Diversity Fine-tuning <https://arxiv.org/abs/2406.10479>`__ 

::

    Sat, 15 Jun 2024 03:06:14 GMT
    Wenjun Li, Changyu Chen, Pradeep Varakantham

Large language models (LLMs) have demonstrated impressive task-solving capabilities, achieved through either prompting techniques or system designs.
However, concerns have arisen regarding their proficiency in planning tasks, as they often struggle to generate valid plans. This paper investigates the impact of fine-tuning on LLMs' planning capabilities. Our findings indicate that LLMs can achieve good performance in planning through substantial (thousands of specific examples) fine-tuning. However, fine-tuning is associated with significant economic and computational costs. To address this challenge, we propose the Maximum Diversity Fine-Tuning (MDFT) strategy to improve the sample efficiency of fine-tuning in the planning domain. Specifically, our algorithm, referred to as MDFT-g, encodes the planning task instances with their graph representations and selects a subset of samples in the vector space that maximizes data diversity. We empirically demonstrate that MDFT-g consistently outperforms existing baselines at various scales across multiple benchmark domains.

------------

`[2406.10504] Task Facet Learning: A Structured Approach to Prompt Optimization <https://arxiv.org/abs/2406.10504>`__ 任务层面学习:一种结构化的提示优化方法

::

    Sat, 15 Jun 2024 04:54:26 GMT
    Gurusha Juneja, Nagarajan Natarajan, Hua Li, Jian Jiao, Amit Sharma

Given a task in the form of a basic description and its training examples, prompt optimization is the problem of synthesizing the given information into a text prompt for a large language model (LLM). Humans solve this problem by also considering the different facets that define a task (e.g., counter-examples, explanations, analogies) and including them in the prompt. However, it is unclear whether existing algorithmic approaches, based on iteratively editing a given prompt or automatically selecting a few in-context examples, can cover the multiple facets required to solve a complex task. In this work, we view prompt optimization as that of learning multiple facets of a task from a set of training examples. We identify and exploit structure in the prompt optimization problem -- first, we find that prompts can be broken down into loosely coupled semantic sections that have a relatively independent effect on the prompt's performance; second, we cluster the input space and use clustered batches so that the optimization procedure can learn the different facets of a task across batches. The resulting algorithm, UniPrompt, consists of a generative model to generate initial candidates for each prompt section; and a feedback mechanism that aggregates suggested edits from multiple mini-batches into a conceptual description for the section. Empirical evaluation on multiple datasets and a real-world task shows that prompts generated using UniPrompt obtain higher accuracy than human-tuned prompts and those from state-of-the-art methods. In particular, our algorithm can generate long, complex prompts that existing methods are unable to generate. Code for UniPrompt will be available at \url{https://aka.ms/uniprompt}.

------------

`[2406.10515] Reactor Mk.1 performances: MMLU, HumanEval and BBH test results <https://arxiv.org/abs/2406.10515>`__ Mk.1反应堆性能:MMLU、HumanEval和BBH测试结果

::

    Sat, 15 Jun 2024 05:52:32 GMT
    TJ Dunham, Henry Syahputra

The paper presents the performance results of Reactor Mk.1, ARCs flagship large language model, through a benchmarking process analysis. The model utilizes the Lychee AI engine and possesses less than 100 billion parameters, resulting in a combination of efficiency and potency. The Reactor Mk.1 outperformed models such as GPT-4o, Claude Opus, and Llama 3, with achieved scores of 92% on the MMLU dataset, 91% on HumanEval dataset, and 88% on BBH dataset. It excels in both managing difficult jobs and reasoning, establishing as a prominent AI solution in the present cutting-edge AI technology.

------------

`[2406.10540] Generating and Evolving Reward Functions for Highway Driving with Large Language Models <https://arxiv.org/abs/2406.10540>`__ 基于大型语言模型的高速公路驾驶奖励函数生成与演化

::

    Sat, 15 Jun 2024 07:50:10 GMT
    Xu Han, Qiannan Yang, Xianda Chen, Xiaowen Chu, Meixin Zhu

Reinforcement Learning (RL) plays a crucial role in advancing autonomous driving technologies by maximizing reward functions to achieve the optimal policy. However, crafting these reward functions has been a complex, manual process in many practices. To reduce this complexity, we introduce a novel framework that integrates Large Language Models (LLMs) with RL to improve reward function design in autonomous driving. This framework utilizes the coding capabilities of LLMs, proven in other areas, to generate and evolve reward functions for highway scenarios. The framework starts with instructing LLMs to create an initial reward function code based on the driving environment and task descriptions. This code is then refined through iterative cycles involving RL training and LLMs' reflection, which benefits from their ability to review and improve the output. We have also developed a specific prompt template to improve LLMs' understanding of complex driving simulations, ensuring the generation of effective and error-free code. Our experiments in a highway driving simulator across three traffic configurations show that our method surpasses expert handcrafted reward functions, achieving a 22% higher average success rate. This not only indicates safer driving but also suggests significant gains in development productivity.

------------

`[2406.10593] QDA-SQL: Questions Enhanced Dialogue Augmentation for Multi-Turn Text-to-SQL <https://arxiv.org/abs/2406.10593>`__ QDA-SQL:面向多轮文本到sql的问题增强对话增强

::

    Sat, 15 Jun 2024 10:54:54 GMT
    Yinggang Sun, Ziming Guo, Haining Yu, Chuanyi Liu, Xiang Li, Bingxuan Wang, Xiangzhan Yu, Tiancheng Zhao

Fine-tuning large language models (LLMs) for specific domain tasks has achieved great success in Text-to-SQL tasks. However, these fine-tuned models often face challenges with multi-turn Text-to-SQL tasks caused by ambiguous or unanswerable questions. It is desired to enhance LLMs to handle multiple types of questions in multi-turn Text-to-SQL tasks. To address this, we propose a novel data augmentation method, called QDA-SQL, which generates multiple types of multi-turn Q\&A pairs by using LLMs. In QDA-SQL, we introduce a novel data augmentation method incorporating validation and correction mechanisms to handle complex multi-turn Text-to-SQL tasks. Experimental results demonstrate that QDA-SQL enables fine-tuned models to exhibit higher performance on SQL statement accuracy and enhances their ability to handle complex, unanswerable questions in multi-turn Text-to-SQL tasks. The generation script and test set are released at https://github.com/mcxiaoxiao/QDA-SQL.

------------

`[2406.10690] Bridging the Gap in Drug Safety Data Analysis: Large Language Models for SQL Query Generation <https://arxiv.org/abs/2406.10690>`__ 弥合药品安全数据分析中的差距:用于SQL查询生成的大型语言模型

::

    Sat, 15 Jun 2024 17:07:31 GMT
    Jeffery L. Painter, Venkateswara Rao Chalamalasetti, Raymond Kassekert and Andrew Bate

Pharmacovigilance (PV) is essential for drug safety, primarily focusing on adverse event monitoring. Traditionally, accessing safety data required database expertise, limiting broader use. This paper introduces a novel application of Large Language Models (LLMs) to democratize database access for non-technical users. Utilizing OpenAI's GPT-4, we developed a chatbot that generates structured query language (SQL) queries from natural language, bridging the gap between domain knowledge and technical requirements. The proposed application aims for more inclusive and efficient data access, enhancing decision making in drug safety. By providing LLMs with plain language summaries of expert knowledge, our approach significantly improves query accuracy over methods relying solely on database schemas. The application of LLMs in this context not only optimizes PV data analysis, ensuring timely and precise drug safety reporting -- a crucial component in adverse drug reaction monitoring -- but also promotes safer pharmacological practices and informed decision making across various data intensive fields.

------------

`[2406.10710] SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task <https://arxiv.org/abs/2406.10710>`__ SyntheT2C:在Text2Cypher任务上为微调大型语言模型生成合成数据

::

    Sat, 15 Jun 2024 18:43:49 GMT
    Ziije Zhong, Linqing Zhong, Zhaoze Sun, Qingyun Jin, Zengchang Qin, Xiaofan Zhang

Integrating Large Language Models (LLMs) with existing Knowledge Graph (KG) databases presents a promising avenue for enhancing LLMs' efficacy and mitigating their "hallucinations". Given that most KGs reside in graph databases accessible solely through specialized query languages (e.g., Cypher), there exists a critical need to bridge the divide between LLMs and KG databases by automating the translation of natural language into Cypher queries (commonly termed the "Text2Cypher" task). Prior efforts tried to bolster LLMs' proficiency in Cypher generation through Supervised Fine-Tuning. However, these explorations are hindered by the lack of annotated datasets of Query-Cypher pairs, resulting from the labor-intensive and domain-specific nature of annotating such datasets. In this study, we propose SyntheT2C, a methodology for constructing a synthetic Query-Cypher pair dataset, comprising two distinct pipelines: (1) LLM-based prompting and (2) template-filling. SyntheT2C facilitates the generation of extensive Query-Cypher pairs with values sampled from an underlying Neo4j graph database. Subsequently, SyntheT2C is applied to two medical databases, culminating in the creation of a synthetic dataset, MedT2C. Comprehensive experiments demonstrate that the MedT2C dataset effectively enhances the performance of backbone LLMs on the Text2Cypher task.
Both the SyntheT2C codebase and the MedT2C dataset will be released soon.

------------

`[2406.10786] Evaluating LLMs with Multiple Problems at once: A New Paradigm for Probing LLM Capabilities <https://arxiv.org/abs/2406.10786>`__ 

::

    Sun, 16 Jun 2024 02:52:32 GMT
    Zhengxiang Wang, Jordan Kodner, Owen Rambow

Current LLM evaluation predominantly performs evaluation with prompts comprising single problems. We propose multi-problem evaluation as an additional approach to study the multiple problem handling capabilities of LLMs. We present a systematic study in this regard by comprehensively examining 7 LLMs on 4 related types of tasks constructed from 6 classification benchmarks. The 4 task types include traditional single-problem tasks, homogeneous multi-problem tasks, and two index selection tasks that embed the multi-problem tasks. We find that LLMs are competent multi-problem solvers: they generally perform (nearly) as well on multi-problem tasks as on single-problem tasks. Furthermore, contrary to common expectation, they often do not suffer from a positional bias with long inputs. This makes multi-problem prompting a simple and cost-efficient prompting method of practical significance. However, our results also strongly indicate that LLMs lack true understanding: they perform significantly worse in the two index selection tasks than in the multi-problem task under various evaluation settings, although they can indeed do index selection in general.

------------

`[2406.10803] HiddenTables & PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies <https://arxiv.org/abs/2406.10803>`__ HiddenTables & PyQTax:用于TableQA的合作游戏和数据集，以确保各种分类的规模和数据隐私

::

    Sun, 16 Jun 2024 04:53:29 GMT
    William Watson, Nicole Cho, Tucker Balch, Manuela Veloso

A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell boundaries, and (3) various limitations stemming from data confidentiality in the process of using external models such as gpt-3.5-turbo. We propose a cooperative game dubbed "HiddenTables" as a potential resolution to this challenge. In essence, "HiddenTables" is played between the code-generating LLM "Solver" and the "Oracle" which evaluates the ability of the LLM agents to solve Table QA tasks.
This game is based on natural language schemas and importantly, ensures the security of the underlying data. We provide evidential experiments on a diverse set of tables that demonstrate an LLM's collective inability to generalize and perform on complex queries, handle compositional dependencies, and align natural language to programmatic commands when concrete table schemas are provided. Unlike encoder-based models, we have pushed the boundaries of "HiddenTables" to not be limited by the number of rows - therefore we exhibit improved efficiency in prompt and completion tokens. Our infrastructure has spawned a new dataset "PyQTax" that spans across 116,671 question-table-answer triplets and provides additional fine-grained breakdowns & labels for varying question taxonomies. Therefore, in tandem with our academic contributions regarding LLMs' deficiency in TableQA tasks, "HiddenTables" is a tactile manifestation of how LLMs can interact with massive datasets while ensuring data security and minimizing generation costs.

------------

`[2406.10847] TorchOpera: A Compound AI System for LLM Safety <https://arxiv.org/abs/2406.10847>`__ TorchOpera:用于LLM安全的复合AI系统

::

    Sun, 16 Jun 2024 08:39:19 GMT
    Shanshan Han, Yuhang Yao, Zijian Hu, Dimitris Stripelis, Zhaozhuo Xu, Chaoyang He

We introduce TorchOpera, a compound AI system for enhancing the safety and quality of prompts and responses for Large Language Models. TorchOpera ensures that all user prompts are safe, contextually grounded, and effectively processed, while enhancing LLM responses to be relevant and high quality.
TorchOpera utilizes the vector database for contextual grounding, rule-based wrappers for flexible modifications, and specialized mechanisms for detecting and adjusting unsafe or incorrect content. We also provide a view of the compound AI system to reduce the computational cost. Extensive experiments show that TorchOpera ensures the safety, reliability, and applicability of LLMs in real-world settings while maintaining the efficiency of LLM responses.

------------

`[2406.10942] Effective Generative AI: The Human-Algorithm Centaur <https://arxiv.org/abs/2406.10942>`__ 有效的生成式人工智能:人类算法的半人马

::

    Sun, 16 Jun 2024 13:44:41 GMT
    Soroush Saghafian, Lihi Idan

Advanced analytics science methods have enabled combining the power of artificial and human intelligence, creating \textit{centaurs} that allow superior decision-making. Centaurs are hybrid human-algorithm AI models that combine both formal analytics and human intuition in a symbiotic manner within their learning and reasoning process. We argue that the future of AI development and use in many domains needs to focus on centaurs as opposed to traditional AI approaches. This paradigm shift from traditional AI methods to centaur-based AI methods raises some fundamental questions: How are centaurs different from traditional human-in-the-loop methods? What are the most effective methods for creating centaurs? When should centaurs be used, and when should the lead be given to traditional AI models? Doesn't the incorporation of human intuition -- which at times can be misleading -- in centaurs' decision-making process degrade its performance compared to traditional AI methods? This work aims to address these fundamental questions, focusing on recent advancements in generative AI, and especially in Large Language Models (LLMs), as a main case study to illustrate centaurs' critical essentiality to future AI endeavors.

------------

`[2406.11301] Optimizing and Testing Instruction-Following: Analyzing the Impact of Fine-Grained Instruction Variants on instruction-tuned LLMs <https://arxiv.org/abs/2406.11301>`__ 

::

    Mon, 17 Jun 2024 08:08:11 GMT
    Jiuding Yang, Weidong Guo, Kaitong Yang, Xiangyang Li, Zhuwei Rao, Yu Xu, Di Niu

The effective alignment of Large Language Models (LLMs) with precise instructions is essential for their application in diverse real-world scenarios. Current methods focus on enhancing the diversity and complexity of training and evaluation samples, yet they fall short in accurately assessing LLMs' ability to follow similar instruction variants. We introduce an effective data augmentation technique that decomposes complex instructions into simpler sub-components, modifies these, and reconstructs them into new variants, thereby preserves the original instruction's context and complexity while introducing variability, which is critical for training and evaluating LLMs' instruction-following precision. We developed the DeMoRecon dataset using this method to both fine-tune and evaluate LLMs. Our findings show that LLMs fine-tuned with DeMoRecon will gain significant performance boost on both ours and commonly used instructions-following benchmarks.

------------

`[2406.11757] STAR: SocioTechnical Approach to Red Teaming Language Models <https://arxiv.org/abs/2406.11757>`__ STAR:红色团队语言模型的社会技术方法

::

    Mon, 17 Jun 2024 17:16:45 GMT
    Laura Weidinger, John Mellor, Bernat Guillen Pegueroles, Nahema Marchal, Ravin Kumar, Kristian Lum, Canfer Akbulut, Mark Diaz, Stevie Bergman, Mikel Rodriguez, Verena Rieser, William Isaac

This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for human red teamers, leading to improved coverage of the risk surface. Parameterised instructions also provide more detailed insights into model failures at no increased cost. Second, STAR improves signal quality by matching demographics to assess harms for specific groups, resulting in more sensitive annotations. STAR further employs a novel step of arbitration to leverage diverse viewpoints and improve label reliability, treating disagreement not as noise but as a valuable contribution to signal quality.

------------

`[2406.10247] QCQA: Quality and Capacity-aware grouped Query Attention <https://arxiv.org/abs/2406.10247>`__ QCQA:质量和能力感知的分组查询注意力

::

    Sat, 8 Jun 2024 07:49:55 GMT
    Vinay Joshi, Prashant Laddha, Shambhavi Sinha, Om Ji Omer, Sreenivas Subramoney

Excessive memory requirements of key and value features (KV-cache) present significant challenges in the autoregressive inference of large language models (LLMs), restricting both the speed and length of text generation. Approaches such as Multi-Query Attention (MQA) and Grouped Query Attention (GQA) mitigate these challenges by grouping query heads and consequently reducing the number of corresponding key and value heads. However, MQA and GQA decrease the KV-cache size requirements at the expense of LLM accuracy (quality of text generation). These methods do not ensure an optimal tradeoff between KV-cache size and text generation quality due to the absence of quality-aware grouping of query heads. To address this issue, we propose Quality and Capacity-Aware Grouped Query Attention (QCQA), which identifies optimal query head groupings using an evolutionary algorithm with a computationally efficient and inexpensive fitness function. We demonstrate that QCQA achieves a significantly better tradeoff between KV-cache capacity and LLM accuracy compared to GQA. For the Llama2 $7\,$B model, QCQA achieves $\mathbf{20}$\% higher accuracy than GQA with similar KV-cache size requirements in the absence of fine-tuning. After fine-tuning both QCQA and GQA, for a similar KV-cache size, QCQA provides $\mathbf{10.55}\,$\% higher accuracy than GQA. Furthermore, QCQA requires $40\,$\% less KV-cache size than GQA to attain similar accuracy. The proposed quality and capacity-aware grouping of query heads can serve as a new paradigm for KV-cache optimization in autoregressive LLM inference.

------------

`[2406.10248] On the Worst Prompt Performance of Large Language Models <https://arxiv.org/abs/2406.10248>`__ 大型语言模型的最差提示性能研究

::

    Sat, 8 Jun 2024 13:40:38 GMT
    Bowen Cao, Deng Cai, Zhisong Zhang, Yuexian Zou, Wai Lam

The performance of large language models (LLMs) is acutely sensitive to the phrasing of prompts, which raises significant concerns about their reliability in real-world scenarios. Existing studies often divide prompts into task-level instructions and case-level inputs and primarily focus on evaluating and improving robustness against variations in tasks-level instructions. However, this setup fails to fully address the diversity of real-world user queries and assumes the existence of task-specific datasets. To address these limitations, we introduce RobustAlpacaEval, a new benchmark that consists of semantically equivalent case-level queries and emphasizes the importance of using the worst prompt performance to gauge the lower bound of model performance. Extensive experiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the Llama, Mistral, and Gemma families uncover substantial variability in model performance; for instance, a difference of 45.48% between the worst and best performance for the Llama-2-70B-chat model, with its worst performance dipping as low as 9.38%. We further illustrate the difficulty in identifying the worst prompt from both model-agnostic and model-dependent perspectives, emphasizing the absence of a shortcut to characterize the worst prompt. We also attempt to enhance the worst prompt performance using existing prompt engineering and prompt consistency methods, but find that their impact is limited. These findings underscore the need to create more resilient LLMs that can maintain high performance across diverse prompts.

------------

`[2406.10254] Towards Signal Processing In Large Language Models <https://arxiv.org/abs/2406.10254>`__ 大型语言模型中的信号处理

::

    Mon, 10 Jun 2024 13:51:52 GMT
    Prateek Verma, Mert Pilanci

This paper introduces the idea of applying signal processing inside a Large Language Model (LLM). With the recent explosion of generative AI, our work can help bridge two fields together, namely the field of signal processing and large language models. We draw parallels between classical Fourier-Transforms and Fourier Transform-like learnable time-frequency representations for every intermediate activation signal of an LLM. Once we decompose every activation signal across tokens into a time-frequency representation, we learn how to filter and reconstruct them, with all components learned from scratch, to predict the next token given the previous context. We show that for GPT-like architectures, our work achieves faster convergence and significantly increases performance by adding a minuscule number of extra parameters when trained for the same epochs. We hope this work paves the way for algorithms exploring signal processing inside the signals found in neural architectures like LLMs and beyond.

------------

`[2406.10260] Flextron: Many-in-One Flexible Large Language Model <https://arxiv.org/abs/2406.10260>`__ Flextron:多合一灵活的大型语言模型

::

    Tue, 11 Jun 2024 01:16:10 GMT
    Ruisi Cai, Saurav Muralidharan, Greg Heinrich, Hongxu Yin, Zhangyang Wang, Jan Kautz, Pavlo Molchanov

Training modern LLMs is extremely resource intensive, and customizing them for various deployment scenarios characterized by limited compute and memory resources through repeated training is impractical. In this paper, we introduce Flextron, a network architecture and post-training model optimization framework supporting flexible model deployment. The Flextron architecture utilizes a nested elastic structure to rapidly adapt to specific user-defined latency and accuracy targets during inference with no additional fine-tuning required. It is also input-adaptive, and can automatically route tokens through its sub-networks for improved performance and efficiency. We present a sample-efficient training method and associated routing algorithms for systematically transforming an existing trained LLM into a Flextron model. We evaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate superior performance over multiple end-to-end trained variants and other state-of-the-art elastic networks, all with a single pretraining run that consumes a mere 7.63% tokens compared to original pretraining.

------------

`[2406.10261] FoodSky: A Food-oriented Large Language Model that Passes the Chef and Dietetic Examination <https://arxiv.org/abs/2406.10261>`__ 

::

    Tue, 11 Jun 2024 01:27:00 GMT
    Pengfei Zhou, Weiqing Min, Chaoran Fu, Ying Jin, Mingyu Huang, Xiangyang Li, Shuhuan Mei and Shuqiang Jiang

Food is foundational to human life, serving not only as a source of nourishment but also as a cornerstone of cultural identity and social interaction. As the complexity of global dietary needs and preferences grows, food intelligence is needed to enable food perception and reasoning for various tasks, ranging from recipe generation and dietary recommendation to diet-disease correlation discovery and understanding. Towards this goal, for powerful capabilities across various domains and tasks in Large Language Models (LLMs), we introduce Food-oriented LLM FoodSky to comprehend food data through perception and reasoning. Considering the complexity and typicality of Chinese cuisine, we first construct one comprehensive Chinese food corpus FoodEarth from various authoritative sources, which can be leveraged by FoodSky to achieve deep understanding of food-related data. We then propose Topic-based Selective State Space Model (TS3M) and the Hierarchical Topic Retrieval Augmented Generation (HTRAG) mechanism to enhance FoodSky in capturing fine-grained food semantics and generating context-aware food-relevant text, respectively. Our extensive evaluations demonstrate that FoodSky significantly outperforms general-purpose LLMs in both chef and dietetic examinations, with an accuracy of 67.2% and 66.4% on the Chinese National Chef Exam and the National Dietetic Exam, respectively. FoodSky not only promises to enhance culinary creativity and promote healthier eating patterns, but also sets a new standard for domain-specific LLMs that address complex real-world issues in the food domain. An online demonstration of FoodSky is available at http://222.92.101.211:8200.

------------

`[2406.10267] Unused information in token probability distribution of generative LLM: improving LLM reading comprehension through calculation of expected values <https://arxiv.org/abs/2406.10267>`__ 生成式LLM token概率分布中的未使用信息:通过计算期望值提高LLM阅读理解

::

    Tue, 11 Jun 2024 09:24:18 GMT
    Krystian Zawistowski

LLM text decoding is key component for perceived LLM quality. We demonstrate two experiments showing that decoding methods could be improved by manipulation of token probabilities. First, we test few LLM on SummEval summary scoring dataset, to measure reading comprehension. We compare scores from greedy decoding to expected values over the next token distribution. We scale logits by large temperature to increase the entropy of scores. This allows strong improvement of performance on SummEval (in terms of correlations to human judgement). We see improvement from 6-8% to 13-28% for 7B Mistral and from 20%-46% to 37%-56% for Mixtral, beating GPT 4 0314 result on two metrics. Part of the gain seems related to positional bias. Secondly, we use probability-based tree sampling algorithm, to examine all most probable generations for given prompt.

------------

`[2406.10269] Markov Constraint as Large Language Model Surrogate <https://arxiv.org/abs/2406.10269>`__ 马尔可夫约束作为大型语言模型代理

::

    Tue, 11 Jun 2024 16:09:53 GMT
    Alexandre Bonlarron, Jean-Charles R\'egin

This paper presents NgramMarkov, a variant of the Markov constraints. It is dedicated to text generation in constraint programming (CP). It involves a set of n-grams (i.e., sequence of n words) associated with probabilities given by a large language model (LLM). It limits the product of the probabilities of the n-gram of a sentence. The propagator of this constraint can be seen as an extension of the ElementaryMarkov constraint propagator, incorporating the LLM distribution instead of the maximum likelihood estimation of n-grams. It uses a gliding threshold, i.e., it rejects n-grams whose local probabilities are too low, to guarantee balanced solutions. It can also be combined with a "look-ahead" approach to remove n-grams that are very unlikely to lead to acceptable sentences for a fixed-length horizon. This idea is based on the MDDMarkovProcess constraint propagator, but without explicitly using an MDD (Multi-Valued Decision Diagram). The experimental results show that the generated text is valued in a similar way to the LLM perplexity function. Using this new constraint dramatically reduces the number of candidate sentences produced, improves computation times, and allows larger corpora or smaller n-grams to be used. A real-world problem has been solved for the first time using 4-grams instead of 5-grams.

------------

`[2406.10273] Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis <https://arxiv.org/abs/2406.10273>`__ Beyond Words:大型语言模型在关键任务风险分析中的可操作性

::

    Tue, 11 Jun 2024 19:20:27 GMT
    Matteo Esposito, Francesco Palagiano, Valentina Lenarduzzi

Context. Risk analysis assesses potential risks in specific scenarios. Risk analysis principles are context-less; the same methodology can be applied to a risk connected to health and information technology security. Risk analysis requires a vast knowledge of national and international regulations and standards and is time and effort-intensive. A large language model can quickly summarize information in less time than a human and can be fine-tuned to specific tasks. Aim. Our empirical study aims to investigate the effectiveness of Retrieval-Augmented Generation and fine-tuned LLM in Risk analysis. To our knowledge, no prior study has explored its capabilities in risk analysis.
Method. We manually curated \totalscenarios unique scenarios leading to \totalsamples representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years. We compared the base GPT-3.5 and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned counterparts. We employ two human experts as competitors of the models and three other three human experts to review the models and the former human expert's analysis. The reviewers analyzed 5,000 scenario analyses.
Results and Conclusions. HEs demonstrated higher accuracy, but LLMs are quicker and more actionable. Moreover, our findings show that RAG-assisted LLMs have the lowest hallucination rates, effectively uncovering hidden risks and complementing human expertise. Thus, the choice of model depends on specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and base models for comprehensiveness and actionability. Therefore, experts can leverage LLMs for an effective complementing companion in risk analysis within a condensed timeframe. They can also save costs by averting unnecessary expenses associated with implementing unwarranted countermeasures.

------------

`[2406.10278] Prompt-Based Length Controlled Generation with Multiple Control Types <https://arxiv.org/abs/2406.10278>`__ 基于提示的长度控制生成具有多种控制类型

::

    Wed, 12 Jun 2024 01:49:54 GMT
    Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang and Qun Liu

Large language models (LLMs) have attracted great attention given their strong performance on a wide range of NLP tasks. In practice, users often expect generated texts to fall within a specific length range, making length controlled generation an important topic, especially for GPT-style models.
Existing length control methods mostly focus on a simple control type of "equal to" a target length. Different from them, we propose a prompt-based method to achieve length controlled generation under different control types with high accuracy. In particular, we adopt reinforcement learning (RL) and sample filtering with the reward signal given by rule-based reward models, which enhances the length control ability of models by rewarding outputs that follow certain control instructions. In addition, we introduce a standard prompt extractor to parse arbitrary users' input into standard control instructions.
Experiments show that our method significantly improves the accuracy of prompt-based length control on popular summarization datasets like CNNDM and NYT under multiple control types. Moreover, both the standard prompt extractor and RL-tuned model show strong generalization to unseen control prompt templates.

------------

`[2406.10288] Mimicking User Data: On Mitigating Fine-Tuning Risks in Closed Large Language Models <https://arxiv.org/abs/2406.10288>`__ 模拟用户数据:降低封闭大型语言模型的微调风险

::

    Wed, 12 Jun 2024 18:33:11 GMT
    Francisco Eiras, Aleksandar Petrov, Phillip H.S. Torr, M. Pawan Kumar, Adel Bibi

Fine-tuning large language models on small, high-quality datasets can enhance their performance on specific downstream tasks. Recent research shows that fine-tuning on benign, instruction-following data can inadvertently undo the safety alignment process and increase a model's propensity to comply with harmful queries. Although critical, understanding and mitigating safety risks in well-defined tasks remains distinct from the instruction-following context due to structural differences in the data. Our work explores the risks associated with fine-tuning closed models - where providers control how user data is utilized in the process - across diverse task-specific data. We demonstrate how malicious actors can subtly manipulate the structure of almost any task-specific dataset to foster significantly more dangerous model behaviors, while maintaining an appearance of innocuity and reasonable downstream task performance. To address this issue, we propose a novel mitigation strategy that mixes in safety data which mimics the task format and prompting style of the user data, showing this is more effective than existing baselines at re-establishing safety alignment while maintaining similar task performance.

------------

`[2406.10295] Robustness of Structured Data Extraction from In-plane Rotated Documents using Multi-Modal Large Language Models (LLM) <https://arxiv.org/abs/2406.10295>`__ 基于多模态大语言模型(LLM)的平面内旋转文档结构化数据提取的鲁棒性

::

    Thu, 13 Jun 2024 08:55:01 GMT
    Anjanava Biswas, Wrick Talukdar

Multi-modal large language models (LLMs) have shown remarkable performance in various natural language processing tasks, including data extraction from documents. However, the accuracy of these models can be significantly affected by document in-plane rotation, also known as skew, a common issue in real-world scenarios for scanned documents. This study investigates the impact of document skew on the data extraction accuracy of three state-of-the-art multi-modal LLMs: Anthropic Claude V3 Sonnet, GPT-4-Turbo, and Llava:v1.6. We focus on extracting specific entities from synthetically generated sample documents with varying degrees of skewness. The results demonstrate that document skew adversely affects the data extraction accuracy of all the tested LLMs, with the severity of the impact varying across models. We identify the safe in-plane rotation angles (SIPRA) for each model and investigate the effects of skew on model hallucinations. Furthermore, we explore existing skew detection and correction mechanisms and discuss their potential limitations. We propose alternative approaches, including developing new multi-modal architectures that are inherently more robust to document skew and incorporating skewing techniques during the pre-training phase of the models. Additionally, we highlight the need for more comprehensive testing on a wider range of document quality and conditions to fully understand the challenges and opportunities associated with using multi-modal LLMs for information extraction in real-world scenarios.

------------

`[2406.10296] CLST: Cold-Start Mitigation in Knowledge Tracing by Aligning a Generative Language Model as a Students' Knowledge Tracer <https://arxiv.org/abs/2406.10296>`__ CLST:通过将生成语言模型对齐为学生知识追踪器来缓解知识追踪中的冷启动

::

    Thu, 13 Jun 2024 09:21:43 GMT
    Heeseok Jung, Jaesang Yoo, Yohaan Yoon, and Yeonju Jang

Knowledge tracing (KT), wherein students' problem-solving histories are used to estimate their current levels of knowledge, has attracted significant interest from researchers. However, most existing KT models were developed with an ID-based paradigm, which exhibits limitations in cold-start performance.
These limitations can be mitigated by leveraging the vast quantities of external knowledge possessed by generative large language models (LLMs). In this study, we propose cold-start mitigation in knowledge tracing by aligning a generative language model as a students' knowledge tracer (CLST) as a framework that utilizes a generative LLM as a knowledge tracer. Upon collecting data from math, social studies, and science subjects, we framed the KT task as a natural language processing task, wherein problem-solving data are expressed in natural language, and fine-tuned the generative LLM using the formatted KT dataset.
Subsequently, we evaluated the performance of the CLST in situations of data scarcity using various baseline models for comparison. The results indicate that the CLST significantly enhanced performance with a dataset of fewer than 100 students in terms of prediction, reliability, and cross-domain generalization.

------------

`[2406.10307] What is the best model? Application-driven Evaluation for Large Language Models <https://arxiv.org/abs/2406.10307>`__ 什么是最好的模型?应用驱动的大型语言模型评估

::

    Fri, 14 Jun 2024 04:52:15 GMT
    Shiguo Lian, Kaikai Zhao, Xinhui Liu, Xuejiao Lei, Bikun Yang, Wenjing Zhang, Kai Wang, Zhaoxiang Liu

General large language models enhanced with supervised fine-tuning and reinforcement learning from human feedback are increasingly popular in academia and industry as they generalize foundation models to various practical tasks in a prompt manner. To assist users in selecting the best model in practical application scenarios, i.e., choosing the model that meets the application requirements while minimizing cost, we introduce A-Eval, an application-driven LLMs evaluation benchmark for general large language models. First, we categorize evaluation tasks into five main categories and 27 sub-categories from a practical application perspective. Next, we construct a dataset comprising 678 question-and-answer pairs through a process of collecting, annotating, and reviewing. Then, we design an objective and effective evaluation method and evaluate a series of LLMs of different scales on A-Eval.
Finally, we reveal interesting laws regarding model scale and task difficulty level and propose a feasible method for selecting the best model. Through A-Eval, we provide clear empirical and engineer guidance for selecting the best model, reducing barriers to selecting and using LLMs and promoting their application and development. Our benchmark is publicly available at https://github.com/UnicomAI/DataSet/tree/main/TestData/GeneralAbility.

------------

`[2406.10323] GenQA: Generating Millions of Instructions from a Handful of Prompts <https://arxiv.org/abs/2406.10323>`__ GenQA:从少量提示中生成数百万条指令

::

    Fri, 14 Jun 2024 17:44:08 GMT
    Jiuhai Chen, Rifaa Qadri, Yuxin Wen, Neel Jain, John Kirchenbauer, Tianyi Zhou, Tom Goldstein

Most public instruction finetuning datasets are relatively small compared to the closed source datasets used to train industry models. To study questions about finetuning at scale, such as curricula and learning rate cooldown schedules, there is a need for industrial-scale datasets. However, this scale necessitates a data generation process that is almost entirely automated. In this work, we study methods for generating large instruction datasets from a single prompt. With little human oversight, we get LLMs to write diverse sets of instruction examples ranging from simple completion tasks to complex multi-turn dialogs across a variety of subject areas. When finetuning a Llama-3 8B base model, our dataset meets or exceeds both WizardLM and Ultrachat on both knowledge-intensive leaderboard tasks as well as conversational evaluations. We release our dataset, the "generator" prompts that created it, and our finetuned model checkpoints.

------------

`[2406.10400] Self-Reflection Outcome is Sensitive to Prompt Construction <https://arxiv.org/abs/2406.10400>`__ 

::

    Fri, 14 Jun 2024 20:07:11 GMT
    Fengyuan Liu, Nouar AlDahoul, Gregory Eady, Yasir Zaki, Bedoor AlShebli, Talal Rahwan

Large language models (LLMs) demonstrate impressive zero-shot and few-shot reasoning capabilities. Some propose that such capabilities can be improved through self-reflection, i.e., letting LLMs reflect on their own output to identify and correct mistakes in the initial responses. However, despite some evidence showing the benefits of self-reflection, recent studies offer mixed results. Here, we aim to reconcile these conflicting findings by first demonstrating that the outcome of self-reflection is sensitive to prompt wording; e.g., LLMs are more likely to conclude that it has made a mistake when explicitly prompted to find mistakes. Consequently, idiosyncrasies in reflection prompts may lead LLMs to change correct responses unnecessarily. We show that most prompts used in the self-reflection literature are prone to this bias. We then propose different ways of constructing prompts that are conservative in identifying mistakes and show that self-reflection using such prompts results in higher accuracy. Our findings highlight the importance of prompt engineering in self-reflection tasks. We release our code at https://github.com/Michael98Liu/mixture-of-prompts.

------------

`[2406.10442] Domain-Specific Shorthand for Generation Based on Context-Free Grammar <https://arxiv.org/abs/2406.10442>`__ 基于上下文无关文法生成的特定领域简写

::

    Fri, 14 Jun 2024 23:26:41 GMT
    Andriy Kanyuka, Elias Mahfoud

The generation of structured data in formats such as JSON, YAML and XML is a critical task in Generative AI (GenAI) applications. These formats, while widely used, contain many redundant constructs that lead to inflated token usage. This inefficiency is particularly evident when employing large language models (LLMs) like GPT-4, where generating extensive structured data incurs increased latency and operational costs. We introduce a domain-specific shorthand (DSS) format, underpinned by a context-free grammar (CFG), and demonstrate its usage to reduce the number of tokens required for structured data generation. The method involves creating a shorthand notation that captures essential elements of the output schema with fewer tokens, ensuring it can be unambiguously converted to and from its verbose form. It employs a CFG to facilitate efficient shorthand generation by the LLM, and to create parsers to translate the shorthand back into standard structured formats. The application of our approach to data visualization with LLMs demonstrates a significant (3x to 5x) reduction in generated tokens, leading to significantly lower latency and cost. This paper outlines the development of the DSS and the accompanying CFG, and the implications of this approach for GenAI applications, presenting a scalable solution to the token inefficiency problem in structured data generation.

------------

`[2406.10459] CancerLLM: A Large Language Model in Cancer Domain <https://arxiv.org/abs/2406.10459>`__ CancerLLM:癌症领域的大型语言模型

::

    Sat, 15 Jun 2024 01:02:48 GMT
    Mingchen Li, Anne Blaes, Steven Johnson, Hongfang Liu, Hua Xu, Rui Zhang

Medical Large Language Models (LLMs) such as ClinicalCamel 70B, Llama3-OpenBioLLM 70B have demonstrated impressive performance on a wide variety of medical NLP task.However, there still lacks a large language model (LLM) specifically designed for cancer domain. Moreover, these LLMs typically have billions of parameters, making them computationally expensive for healthcare systems.Thus, in this study, we propose CancerLLM, a model with 7 billion parameters and a Mistral-style architecture, pre-trained on 2,676,642 clinical notes and 515,524 pathology reports covering 17 cancer types, followed by fine-tuning on three cancer-relevant tasks, including cancer phenotypes extraction, cancer diagnosis generation, and cancer treatment plan generation.
Our evaluation demonstrated that CancerLLM achieves state-of-the-art results compared to other existing LLMs, with an average F1 score improvement of 8.1\%.
Additionally, CancerLLM outperforms other models on two proposed robustness testbeds. This illustrates that CancerLLM can be effectively applied to clinical AI systems, enhancing clinical research and healthcare delivery in the field of cancer.

------------

`[2406.10486] Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender? <https://arxiv.org/abs/2406.10486>`__ 大型语言模型在招聘决策中是否会基于种族、民族和性别进行歧视?

::

    Sat, 15 Jun 2024 03:31:16 GMT
    Haozhe An, Christabel Acquaye, Colin Wang, Zongxia Li, Rachel Rudinger

We examine whether large language models (LLMs) exhibit race- and gender-based name discrimination in hiring decisions, similar to classic findings in the social sciences (Bertrand and Mullainathan, 2004). We design a series of templatic prompts to LLMs to write an email to a named job applicant informing them of a hiring decision. By manipulating the applicant's first name, we measure the effect of perceived race, ethnicity, and gender on the probability that the LLM generates an acceptance or rejection email. We find that the hiring decisions of LLMs in many settings are more likely to favor White applicants over Hispanic applicants. In aggregate, the groups with the highest and lowest acceptance rates respectively are masculine White names and masculine Hispanic names. However, the comparative acceptance rates by group vary under different templatic settings, suggesting that LLMs' race- and gender-sensitivity may be idiosyncratic and prompt-sensitive.

------------

`[2406.10492] Large Language Models as Event Forecasters <https://arxiv.org/abs/2406.10492>`__ 大型语言模型的事件预测

::

    Sat, 15 Jun 2024 04:09:31 GMT
    Libo Zhang and Yue Ning

Key elements of human events are extracted as quadruples that consist of subject, relation, object, and timestamp. This representation can be extended to a quintuple by adding a fifth element: a textual summary that briefly describes the event. These quadruples or quintuples, when organized within a specific domain, form a temporal knowledge graph (TKG). Current learning frameworks focus on a few TKG-related tasks, such as predicting an object given a subject and a relation or forecasting the occurrences of multiple types of events (i.e., relation) in the next time window. They typically rely on complex structural and sequential models like graph neural networks (GNNs) and recurrent neural networks (RNNs) to update intermediate embeddings. However, these methods often neglect the contextual information inherent in each quintuple, which can be effectively captured through concise textual descriptions. In this paper, we investigate how large language models (LLMs) can streamline the design of TKG learning frameworks while maintaining competitive accuracy in prediction and forecasting tasks. We develop multiple prompt templates to frame the object prediction (OP) task as a standard question-answering (QA) task, suitable for instruction fine-tuning with an encoder-decoder generative LLM. For multi-event forecasting (MEF), we design simple yet effective prompt templates for each TKG quintuple. This novel approach removes the need for GNNs and RNNs, instead utilizing an encoder-only LLM to generate fixed intermediate embeddings, which are subsequently processed by a prediction head with a self-attention mechanism to forecast potential future relations. Extensive experiments on multiple real-world datasets using various evaluation metrics validate the effectiveness and robustness of our approach.

------------

`[2406.10505] CroPrompt: Cross-task Interactive Prompting for Zero-shot Spoken Language Understanding <https://arxiv.org/abs/2406.10505>`__ CroPrompt:面向零样本口语理解的跨任务交互式提示

::

    Sat, 15 Jun 2024 04:54:56 GMT
    Libo Qin, Fuxuan Wei, Qiguang Chen, Jingxuan Zhou, Shijue Huang, Jiasheng Si, Wenpeng Lu, Wanxiang Che

Slot filling and intent detection are two highly correlated tasks in spoken language understanding (SLU). Recent SLU research attempts to explore zero-shot prompting techniques in large language models to alleviate the data scarcity problem. Nevertheless, the existing prompting work ignores the cross-task interaction information for SLU, which leads to sub-optimal performance. To solve this problem, we present the pioneering work of Cross-task Interactive Prompting (CroPrompt) for SLU, which enables the model to interactively leverage the information exchange across the correlated tasks in SLU.
Additionally, we further introduce a multi-task self-consistency mechanism to mitigate the error propagation caused by the intent information injection. We conduct extensive experiments on the standard SLU benchmark and the results reveal that CroPrompt consistently outperforms the existing prompting approaches. In addition, the multi-task self-consistency mechanism can effectively ease the error propagation issue, thereby enhancing the performance. We hope this work can inspire more research on cross-task prompting for SLU.

------------

`[2406.10552] Large Language Model Enhanced Clustering for News Event Detection <https://arxiv.org/abs/2406.10552>`__ 

::

    Sat, 15 Jun 2024 08:13:47 GMT
    Adane Nega Tarekegn, Fazle Rabbi, Bj{\o}rnar Tessem

The news landscape is continuously evolving, with an ever-increasing volume of information from around the world. Automated event detection within this vast data repository is essential for monitoring, identifying, and categorizing significant news occurrences across diverse platforms. This paper presents an event detection framework that leverages Large Language Models (LLMs) combined with clustering analysis to detect news events from the Global Database of Events, Language, and Tone (GDELT). The framework enhances event clustering through both pre-event detection tasks (keyword extraction and text embedding) and post-event detection tasks (event summarization and topic labeling). We also evaluate the impact of various textual embeddings on the quality of clustering outcomes, ensuring robust news categorization. Additionally, we introduce a novel Cluster Stability Assessment Index (CSAI) to assess the validity and robustness of clustering results. CSAI utilizes latent feature vectors to provide a new way of measuring clustering quality. Our experiments indicate that combining LLM embeddings with clustering algorithms yields the best results, demonstrating greater robustness in terms of CSAI scores.
Moreover, post-event detection tasks generate meaningful insights, facilitating effective interpretation of event clustering results. Overall, our experimental results indicate that the proposed framework offers valuable insights and could enhance the accuracy and depth of news reporting.

------------

`[2406.10560] Facts-and-Feelings: Capturing both Objectivity and Subjectivity in Table-to-Text Generation <https://arxiv.org/abs/2406.10560>`__ 事实和感觉:在表到文本生成中同时捕获客观性和主观性

::

    Sat, 15 Jun 2024 08:41:44 GMT
    Tathagata Dey and Pushpak Bhattacharyya

Table-to-text generation, a long-standing challenge in natural language generation, has remained unexplored through the lens of subjectivity.
Subjectivity here encompasses the comprehension of information derived from the table that cannot be described solely by objective data. Given the absence of pre-existing datasets, we introduce the Ta2TS dataset with 3849 data instances.
We perform the task of fine-tuning sequence-to-sequence models on the linearized tables and prompting on popular large language models. We analyze the results from a quantitative and qualitative perspective to ensure the capture of subjectivity and factual consistency. The analysis shows the fine-tuned LMs can perform close to the prompted LLMs. Both the models can capture the tabular data, generating texts with 85.15% BERTScore and 26.28% Meteor score. To the best of our knowledge, we provide the first-of-its-kind dataset on tables with multiple genres and subjectivity included and present the first comprehensive analysis and comparison of different LLM performances on this task.

------------

`[2406.10561] We Care: Multimodal Depression Detection and Knowledge Infused Mental Health Therapeutic Response Generation <https://arxiv.org/abs/2406.10561>`__ We Care:多模态抑郁检测和知识注入的心理健康治疗反应生成

::

    Sat, 15 Jun 2024 08:41:46 GMT
    Palash Moon, Pushpak Bhattacharyya

The detection of depression through non-verbal cues has gained significant attention. Previous research predominantly centred on identifying depression within the confines of controlled laboratory environments, often with the supervision of psychologists or counsellors. Unfortunately, datasets generated in such controlled settings may struggle to account for individual behaviours in real-life situations. In response to this limitation, we present the Extended D-vlog dataset, encompassing a collection of 1, 261 YouTube vlogs.
Additionally, the emergence of large language models (LLMs) like GPT3.5, and GPT4 has sparked interest in their potential they can act like mental health professionals. Yet, the readiness of these LLM models to be used in real-life settings is still a concern as they can give wrong responses that can harm the users. We introduce a virtual agent serving as an initial contact for mental health patients, offering Cognitive Behavioral Therapy (CBT)-based responses.
It comprises two core functions: 1. Identifying depression in individuals, and 2. Delivering CBT-based therapeutic responses. Our Mistral model achieved impressive scores of 70.1% and 30.9% for distortion assessment and classification, along with a Bert score of 88.7%. Moreover, utilizing the TVLT model on our Multimodal Extended D-vlog Dataset yielded outstanding results, with an impressive F1-score of 67.8%

------------

`[2406.10594] BlockPruner: Fine-grained Pruning for Large Language Models <https://arxiv.org/abs/2406.10594>`__ BlockPruner:大型语言模型的细粒度剪枝

::

    Sat, 15 Jun 2024 11:03:33 GMT
    Longguang Zhong, Fanqi Wan, Ruijun Chen, Xiaojun Quan, Liangzhi Li

With the rapid growth in the size and complexity of large language models (LLMs), the costs associated with their training and inference have escalated significantly. Research indicates that certain layers in LLMs harbor substantial redundancy, and pruning these layers has minimal impact on the overall performance. While various layer pruning methods have been developed based on this insight, they generally overlook the finer-grained redundancies within the layers themselves. In this paper, we delve deeper into the architecture of LLMs and demonstrate that finer-grained pruning can be achieved by targeting redundancies in multi-head attention (MHA) and multi-layer perceptron (MLP) blocks. We propose a novel, training-free structured pruning approach called BlockPruner. Unlike existing layer pruning methods, BlockPruner segments each Transformer layer into MHA and MLP blocks. It then assesses the importance of these blocks using perplexity measures and applies a heuristic search for iterative pruning. We applied BlockPruner to LLMs of various sizes and architectures and validated its performance across a wide range of downstream tasks. Experimental results show that BlockPruner achieves more granular and effective pruning compared to state-of-the-art baselines.

------------

`[2406.10602] Multilingual Large Language Models and Curse of Multilinguality <https://arxiv.org/abs/2406.10602>`__ 多语言大型语言模型与多语言诅咒

::

    Sat, 15 Jun 2024 11:31:39 GMT
    Daniil Gurgurov, Tanja B\"aumel, Tatiana Anikina

Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models, trained on huge datasets, show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navigates the landscape of multilingual LLMs, providing an introductory overview of their technical aspects. It explains underlying architectures, objective functions, pre-training data sources, and tokenization methods. This work explores the unique features of different model types: encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5, mBART). Additionally, it addresses one of the significant limitations of multilingual LLMs - the curse of multilinguality - and discusses current attempts to overcome it.

------------

`[2406.10630] Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models <https://arxiv.org/abs/2406.10630>`__ 大型语言模型联邦指令调优中的新兴安全攻防

::

    Sat, 15 Jun 2024 13:24:22 GMT
    Rui Ye, Jingyi Chai, Xiangrui Liu, Yaodong Yang, Yanfeng Wang, Siheng Chen

Federated learning (FL) enables multiple parties to collaboratively fine-tune an large language model (LLM) without the need of direct data sharing. Ideally, by training on decentralized data that is aligned with human preferences and safety principles, federated instruction tuning can result in an LLM that could behave in a helpful and safe manner. In this paper, we for the first time reveal the vulnerability of safety alignment in FedIT by proposing a simple, stealthy, yet effective safety attack method. Specifically, the malicious clients could automatically generate attack data without involving manual efforts and attack the FedIT system by training their local LLMs on such attack data. Unfortunately, this proposed safety attack not only can compromise the safety alignment of LLM trained via FedIT, but also can not be effectively defended against by many existing FL defense methods. Targeting this, we further propose a post-hoc defense method, which could rely on a fully automated pipeline: generation of defense data and further fine-tuning of the LLM. Extensive experiments show that our safety attack method can significantly compromise the LLM's safety alignment (e.g., reduce safety rate by 70\%), which can not be effectively defended by existing defense methods (at most 4\% absolute improvement), while our safety defense method can significantly enhance the attacked LLM's safety alignment (at most 69\% absolute improvement).

------------

`[2406.10701] MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding <https://arxiv.org/abs/2406.10701>`__ MIND:从大型视觉-语言模型中提取的面向电子商务购买理解的多模态购物意图

::

    Sat, 15 Jun 2024 17:56:09 GMT
    Baixuan Xu, Weiqi Wang, Haochen Shi, Wenxuan Ding, Huihao Jing, Tianqing Fang, Jiaxin Bai, Long Chen, Yangqiu Song

Improving user experience and providing personalized search results in E-commerce platforms heavily rely on understanding purchase intention. However, existing methods for acquiring large-scale intentions bank on distilling large language models with human annotation for verification. Such an approach tends to generate product-centric intentions, overlook valuable visual information from product images, and incurs high costs for scalability. To address these issues, we introduce MIND, a multimodal framework that allows Large Vision-Language Models (LVLMs) to infer purchase intentions from multimodal product metadata and prioritize human-centric ones. Using Amazon Review data, we apply MIND and create a multimodal intention knowledge base, which contains 1,264,441 million intentions derived from 126,142 co-buy shopping records across 107,215 products. Extensive human evaluations demonstrate the high plausibility and typicality of our obtained intentions and validate the effectiveness of our distillation framework and filtering mechanism. Additional experiments reveal that our obtained intentions significantly enhance large language models in two intention comprehension tasks.

------------

`[2406.10764] GNOME: Generating Negotiations through Open-Domain Mapping of Exchanges <https://arxiv.org/abs/2406.10764>`__ GNOME:通过交换的开放域映射生成谈判

::

    Sun, 16 Jun 2024 00:26:17 GMT
    Darshan Deshpande, Shambhavi Sinha, Anirudh Ravi Kumar, Debaditya Pal, Jonathan May

Language Models have previously shown strong negotiation capabilities in closed domains where the negotiation strategy prediction scope is constrained to a specific setup. In this paper, we first show that these models are not generalizable beyond their original training domain despite their wide-scale pretraining. Following this, we propose an automated framework called GNOME, which processes existing human-annotated, closed-domain datasets using Large Language Models and produces synthetic open-domain dialogues for negotiation.
GNOME improves the generalizability of negotiation systems while reducing the expensive and subjective task of manual data curation. Through our experimental setup, we create a benchmark comparing encoder and decoder models trained on existing datasets against datasets created through GNOME. Our results show that models trained on our dataset not only perform better than previous state of the art models on domain specific strategy prediction, but also generalize better to previously unseen domains.

------------

`[2406.10773] Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles <https://arxiv.org/abs/2406.10773>`__ 用真实世界和生成的新闻文章语料库量化生成媒体偏见

::

    Sun, 16 Jun 2024 01:32:04 GMT
    Filip Trhlik and Pontus Stenetorp

Large language models (LLMs) are increasingly being utilised across a range of tasks and domains, with a burgeoning interest in their application within the field of journalism. This trend raises concerns due to our limited understanding of LLM behaviour in this domain, especially with respect to political bias. Existing studies predominantly focus on LLMs undertaking political questionnaires, which offers only limited insights into their biases and operational nuances. To address this gap, our study establishes a new curated dataset that contains 2,100 human-written articles and utilises their descriptions to generate 56,700 synthetic articles using nine LLMs. This enables us to analyse shifts in properties between human-authored and machine-generated articles, with this study focusing on political bias, detecting it using both supervised models and LLMs. Our findings reveal significant disparities between base and instruction-tuned LLMs, with instruction-tuned models exhibiting consistent political bias. Furthermore, we are able to study how LLMs behave as classifiers, observing their display of political bias even in this role. Overall, for the first time within the journalistic domain, this study outlines a framework and provides a structured dataset for quantifiable experiments, serving as a foundation for further research into LLM political bias and its implications.

------------

`[2406.10794] Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis <https://arxiv.org/abs/2406.10794>`__ LLMs中的越狱攻击理解:表示空间分析

::

    Sun, 16 Jun 2024 03:38:48 GMT
    Yuping Lin, Pengfei He, Han Xu, Yue Xing, Makoto Yamada, Hui Liu, Jiliang Tang

Large language models (LLMs) are susceptible to a type of attack known as jailbreaking, which misleads LLMs to output harmful contents. Although there are diverse jailbreak attack strategies, there is no unified understanding on why some methods succeed and others fail. This paper explores the behavior of harmful and harmless prompts in the LLM's representation space to investigate the intrinsic properties of successful jailbreak attacks. We hypothesize that successful attacks share some similar properties: They are effective in moving the representation of the harmful prompt towards the direction to the harmless prompts. We leverage hidden representations into the objective of existing jailbreak attacks to move the attacks along the acceptance direction, and conduct experiments to validate the above hypothesis using the proposed objective. We hope this study provides new insights into understanding how LLMs understand harmfulness information.

------------

`[2406.10802] KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge Graphs <https://arxiv.org/abs/2406.10802>`__ KGPA:基于跨领域知识图谱的大型语言模型鲁棒性评估

::

    Sun, 16 Jun 2024 04:48:43 GMT
    Aihua Pei (1), Zehua Yang (1), Shunan Zhu (1), Ruoxi Cheng (2), Ju Jia (2), Lina Wang (3) ((1) Waseda University, (2) Southeast University, (3) Wuhan University)

Existing frameworks for assessing robustness of large language models (LLMs) overly depend on specific benchmarks, increasing costs and failing to evaluate performance of LLMs in professional domains due to dataset limitations. This paper proposes a framework that systematically evaluates the robustness of LLMs under adversarial attack scenarios by leveraging knowledge graphs (KGs). Our framework generates original prompts from the triplets of knowledge graphs and creates adversarial prompts by poisoning, assessing the robustness of LLMs through the results of these adversarial attacks. We systematically evaluate the effectiveness of this framework and its modules. Experiments show that adversarial robustness of the ChatGPT family ranks as GPT-4-turbo > GPT-4o > GPT-3.5-turbo, and the robustness of large language models is influenced by the professional domains in which they operate.

------------

`[2406.10811] LLMFactor: Extracting Profitable Factors through Prompts for Explainable Stock Movement Prediction <https://arxiv.org/abs/2406.10811>`__ 

::

    Sun, 16 Jun 2024 06:20:50 GMT
    Meiyun Wang, Kiyoshi Izumi, Hiroki Sakaji

Recently, Large Language Models (LLMs) have attracted significant attention for their exceptional performance across a broad range of tasks, particularly in text analysis. However, the finance sector presents a distinct challenge due to its dependence on time-series data for complex forecasting tasks. In this study, we introduce a novel framework called LLMFactor, which employs Sequential Knowledge-Guided Prompting (SKGP) to identify factors that influence stock movements using LLMs. Unlike previous methods that relied on keyphrases or sentiment analysis, this approach focuses on extracting factors more directly related to stock market dynamics, providing clear explanations for complex temporal changes. Our framework directs the LLMs to create background knowledge through a fill-in-the-blank strategy and then discerns potential factors affecting stock prices from related news. Guided by background knowledge and identified factors, we leverage historical stock prices in textual format to predict stock movement. An extensive evaluation of the LLMFactor framework across four benchmark datasets from both the U.S. and Chinese stock markets demonstrates its superiority over existing state-of-the-art methods and its effectiveness in financial time-series forecasting.

------------

`[2406.10813] Self-Evolution Fine-Tuning for Policy Optimization <https://arxiv.org/abs/2406.10813>`__ 策略优化的自进化微调

::

    Sun, 16 Jun 2024 06:38:02 GMT
    Ruijun Chen, Jiehao Liang, Shiping Gao, Fanqi Wan, Xiaojun Quan

The alignment of large language models (LLMs) is crucial not only for unlocking their potential in specific tasks but also for ensuring that responses meet human expectations and adhere to safety and ethical principles.
Current alignment methodologies face considerable challenges. For instance, supervised fine-tuning (SFT) requires extensive, high-quality annotated samples, while reinforcement learning from human feedback (RLHF) is complex and often unstable. In this paper, we introduce self-evolution fine-tuning (SEFT) for policy optimization, with the aim of eliminating the need for annotated samples while retaining the stability and efficiency of SFT. SEFT first trains an adaptive reviser to elevate low-quality responses while maintaining high-quality ones. The reviser then gradually guides the policy's optimization by fine-tuning it with enhanced responses. One of the prominent features of this method is its ability to leverage unlimited amounts of unannotated data for policy optimization through supervised fine-tuning. Our experiments on AlpacaEval 2.0 and MT-Bench demonstrate the effectiveness of SEFT. We also provide a comprehensive analysis of its advantages over existing alignment techniques.

------------

`[2406.10842] Large Language Models for Automatic Milestone Detection in Group Discussions <https://arxiv.org/abs/2406.10842>`__ 面向群组讨论里程碑自动检测的大型语言模型

::

    Sun, 16 Jun 2024 08:32:22 GMT
    Zhuoxu Duan, Zhengye Yang, Samuel Westby, Christoph Riedl, Brooke Foucault Welles, Richard J. Radke

Large language models like GPT have proven widely successful on natural language understanding tasks based on written text documents. In this paper, we investigate an LLM's performance on recordings of a group oral communication task in which utterances are often truncated or not well-formed. We propose a new group task experiment involving a puzzle with several milestones that can be achieved in any order. We investigate methods for processing transcripts to detect if, when, and by whom a milestone has been completed. We demonstrate that iteratively prompting GPT with transcription chunks outperforms semantic similarity search methods using text embeddings, and further discuss the quality and randomness of GPT responses under different context window sizes.

------------

`[2406.10868] Analyzing Key Neurons in Large Language Models <https://arxiv.org/abs/2406.10868>`__ 大型语言模型中的关键神经元分析

::

    Sun, 16 Jun 2024 09:36:32 GMT
    Lihu Chen, Adam Dejl, Francesca Toni

Large Language Models (LLMs) possess vast amounts of knowledge within their parameters, prompting research into methods for locating and editing this knowledge. Previous investigations have primarily focused on fill-in-the-blank tasks and locating entity-related usually single-token facts) information in relatively small-scale language models. However, several key questions remain unanswered: (1) How can we effectively locate query-relevant neurons in contemporary autoregressive LLMs, such as LLaMA and Mistral? (2) How can we address the challenge of long-form text generation? (3) Are there localized knowledge regions in LLMs? In this study, we introduce Neuron Attribution-Inverse Cluster Attribution (NA-ICA), a novel architecture-agnostic framework capable of identifying key neurons in LLMs. NA-ICA allows for the examination of long-form answers beyond single tokens by employing the proxy task of multi-choice question answering. To evaluate the effectiveness of our detected key neurons, we construct two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that NA-ICA outperforms baseline methods significantly. Moreover, analysis of neuron distributions reveals the presence of visible localized regions, particularly within different domains. Finally, we demonstrate the potential applications of our detected key neurons in knowledge editing and neuron-based prediction.

------------

`[2406.10880] Exploring the Potential of Multimodal LLM with Knowledge-Intensive Multimodal ASR <https://arxiv.org/abs/2406.10880>`__ 利用知识密集型多模态ASR探索多模态LLM的潜力

::

    Sun, 16 Jun 2024 10:04:19 GMT
    Minghan Wang, Yuxia Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari

Recent advancements in multimodal large language models (MLLMs) have made significant progress in integrating information across various modalities, yet real-world applications in educational and scientific domains remain challenging. This paper introduces the Multimodal Scientific ASR (MS-ASR) task, which focuses on transcribing scientific conference videos by leveraging visual information from slides to enhance the accuracy of technical terminologies.
Realized that traditional metrics like WER fall short in assessing performance accurately, prompting the proposal of severity-aware WER (SWER) that considers the content type and severity of ASR errors. We propose the Scientific Vision Augmented ASR (SciVASR) framework as a baseline method, enabling MLLMs to improve transcript quality through post-editing. Evaluations of state-of-the-art MLLMs, including GPT-4o, show a 45% improvement over speech-only baselines, highlighting the importance of multimodal information integration.

------------

`[2406.10881] Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals <https://arxiv.org/abs/2406.10881>`__ 教大型语言模型从自身信号表达知识边界

::

    Sun, 16 Jun 2024 10:07:20 GMT
    Lida Chen, Zujie Liang, Xintao Wang, Jiaqing Liang, Yanghua Xiao, Feng Wei, Jinglei Chen, Zhenghong Hao, Bing Han, Wei Wang

Large language models (LLMs) have achieved great success, but their occasional content fabrication, or hallucination, limits their practical application. Hallucination arises because LLMs struggle to admit ignorance due to inadequate training on knowledge boundaries. We call it a limitation of LLMs that they can not accurately express their knowledge boundary, answering questions they know while admitting ignorance to questions they do not know. In this paper, we aim to teach LLMs to recognize and express their knowledge boundary, so they can reduce hallucinations caused by fabricating when they do not know. We propose CoKE, which first probes LLMs' knowledge boundary via internal confidence given a set of questions, and then leverages the probing results to elicit the expression of the knowledge boundary. Extensive experiments show CoKE helps LLMs express knowledge boundaries, answering known questions while declining unknown ones, significantly improving in-domain and out-of-domain performance.

------------

`[2406.10886] Distilling Opinions at Scale: Incremental Opinion Summarization using XL-OPSUMM <https://arxiv.org/abs/2406.10886>`__ 大规模观点提取:基于XL-OPSUMM的增量式观点摘要

::

    Sun, 16 Jun 2024 10:36:41 GMT
    Sri Raghava Muddu, Rupasai Rangaraju, Tejpalsingh Siledar, Swaroop Nath, Pushpak Bhattacharyya, Swaprava Nath, Suman Banerjee, Amey Patil, Muthusamy Chelliah, Sudhanshu Shekhar Singh, Nikesh Garera

Opinion summarization in e-commerce encapsulates the collective views of numerous users about a product based on their reviews. Typically, a product on an e-commerce platform has thousands of reviews, each review comprising around 10-15 words. While Large Language Models (LLMs) have shown proficiency in summarization tasks, they struggle to handle such a large volume of reviews due to context limitations. To mitigate, we propose a scalable framework called Xl-OpSumm that generates summaries incrementally. However, the existing test set, AMASUM has only 560 reviews per product on average. Due to the lack of a test set with thousands of reviews, we created a new test set called Xl-Flipkart by gathering data from the Flipkart website and generating summaries using GPT-4. Through various automatic evaluations and extensive analysis, we evaluated the framework's efficiency on two datasets, AMASUM and Xl-Flipkart. Experimental results show that our framework, Xl-OpSumm powered by Llama-3-8B-8k, achieves an average ROUGE-1 F1 gain of 4.38% and a ROUGE-L F1 gain of 3.70% over the next best-performing model.

------------

`[2406.10922] Generating Tables from the Parametric Knowledge of Language Models <https://arxiv.org/abs/2406.10922>`__ 从语言模型的参数知识生成表格

::

    Sun, 16 Jun 2024 12:55:55 GMT
    Yevgeni Berkovitch, Oren Glickman, Amit Somech, Tomer Wolfson

We explore generating factual and accurate tables from the parametric knowledge of large language models (LLMs). While LLMs have demonstrated impressive capabilities in recreating knowledge bases and generating free-form text, we focus on generating structured tabular data, which is crucial in domains like finance and healthcare. We examine the table generation abilities of four state-of-the-art LLMs: GPT-3.5, GPT-4, Llama2-13B, and Llama2-70B, using three prompting methods for table generation: (a) full-table, (b) row-by-row; (c) cell-by-cell. For evaluation, we introduce a novel benchmark, WikiTabGen which contains 100 curated Wikipedia tables. Tables are further processed to ensure their factual correctness and manually annotated with short natural language descriptions. Our findings reveal that table generation remains a challenge, with GPT-4 reaching the highest accuracy at 19.6%. Our detailed analysis sheds light on how various table properties, such as size, table popularity, and numerical content, influence generation performance. This work highlights the unique challenges in LLM-based table generation and provides a solid evaluation framework for future research. Our code, prompts and data are all publicly available: https://github.com/analysis-bots/WikiTabGen

------------

`[2406.10950] E-Bench: Towards Evaluating the Ease-of-Use of Large Language Models <https://arxiv.org/abs/2406.10950>`__ E-Bench:大型语言模型易用性评估

::

    Sun, 16 Jun 2024 14:08:30 GMT
    Zhenyu Zhang, Bingguang Hao, Jinpeng Li, Zekai Zhang, Dongyan Zhao

Most large language models (LLMs) are sensitive to prompts, and another synonymous expression or a typo may lead to unexpected results for the model.
Composing an optimal prompt for a specific demand lacks theoretical support and relies entirely on human experimentation, which poses a considerable obstacle to popularizing generative artificial intelligence. However, there is no systematic analysis of the stability of LLMs in resisting prompt perturbations in real-world scenarios. In this work, we propose to evaluate the ease-of-use of LLMs and construct E-Bench, simulating the actual situation of human use from synonymous perturbation (including paraphrasing, simplification, and colloquialism) and typographical perturbation (such as typing). On this basis, we also discuss the combination of these two types of perturbation and analyze the main reasons for performance degradation. Experimental results indicate that with the increase of model size, although the ease-of-use are significantly improved, there is still a long way to go to build a sufficiently user-friendly model.

------------

`[2406.10952] Avoiding Copyright Infringement via Machine Unlearning <https://arxiv.org/abs/2406.10952>`__ 基于机器遗忘的版权侵权避免

::

    Sun, 16 Jun 2024 14:12:37 GMT
    Guangyao Dou, Zheyuan Liu, Qing Lyu, Kaize Ding, Eric Wong

Pre-trained Large Language Models (LLMs) have demonstrated remarkable capabilities but also pose risks by learning and generating copyrighted material, leading to significant legal and ethical concerns. To address these issues, it is critical for model owners to be able to unlearn copyrighted content at various time steps. We explore the setting of sequential unlearning, where copyrighted content is removed over multiple time steps - a scenario that has not been rigorously addressed. To tackle this challenge, we propose Stable Sequential Unlearning (SSU), a novel unlearning framework for LLMs, designed to have a more stable process to remove copyrighted content from LLMs throughout different time steps using task vectors, by incorporating additional random labeling loss and applying gradient-based weight saliency mapping. Experiments demonstrate that SSU finds a good balance between unlearning efficacy and maintaining the model's general knowledge compared to existing baselines.

------------

`[2406.10957] Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence <https://arxiv.org/abs/2406.10957>`__ 下采样KL散度消除直接偏好优化的偏置长度依赖

::

    Sun, 16 Jun 2024 14:24:30 GMT
    Junru Lu and Jiazheng Li and Siyu An and Meng Zhao and Yulan He and Di Yin and Xing Sun

Direct Preference Optimization (DPO) has emerged as a prominent algorithm for the direct and robust alignment of Large Language Models (LLMs) with human preferences, offering a more straightforward alternative to the complex Reinforcement Learning from Human Feedback (RLHF). Despite its promising efficacy, DPO faces a notable drawback: "verbosity", a common over-optimization phenomenon also observed in RLHF. While previous studies mainly attributed verbosity to biased labels within the data, we propose that the issue also stems from an inherent algorithmic length reliance in DPO. Specifically, we suggest that the discrepancy between sequence-level Kullback-Leibler (KL) divergences between chosen and rejected sequences, used in DPO, results in overestimated or underestimated rewards due to varying token lengths.
Empirically, we utilize datasets with different label lengths to demonstrate the presence of biased rewards. We then introduce an effective downsampling approach, named SamPO, to eliminate potential length reliance. Our experimental evaluations, conducted across three LLMs of varying scales and a diverse array of conditional and open-ended benchmarks, highlight the efficacy of SamPO in mitigating verbosity, achieving improvements of 5% to 12% over DPO through debaised rewards. Our codes can be accessed at: https://github.com/LuJunru/SamPO/.

------------

`[2406.10965] DocNet: Semantic Structure in Inductive Bias Detection Models <https://arxiv.org/abs/2406.10965>`__ 

::

    Sun, 16 Jun 2024 14:51:12 GMT
    Jessica Zhu, Iain Cruickshank, Michel Cukier

News will have biases so long as people have opinions. However, as social media becomes the primary entry point for news and partisan gaps increase, it is increasingly important for informed citizens to be able to identify bias.
People will be able to take action to avoid polarizing echo chambers if they know how the news they are consuming is biased. In this paper, we explore an often overlooked aspect of bias detection in documents: the semantic structure of news articles. We present DocNet, a novel, inductive, and low-resource document embedding and bias detection model that outperforms large language models. We also demonstrate that the semantic structure of news articles from opposing partisan sides, as represented in document-level graph embeddings, have significant similarities. These results can be used to advance bias detection in low-resource environments. Our code and data are made available at https://github.com/nlpresearchanon.

------------

`[2406.10977] Toward Optimal LLM Alignments Using Two-Player Games <https://arxiv.org/abs/2406.10977>`__ 基于双人博弈的最佳LLM对齐

::

    Sun, 16 Jun 2024 15:24:50 GMT
    Rui Zheng, Hongyi Guo, Zhihan Liu, Xiaoying Zhang, Yuanshun Yao, Xiaojun Xu, Zhaoran Wang, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang, Hang Li, Yang Liu

The standard Reinforcement Learning from Human Feedback (RLHF) framework primarily focuses on optimizing the performance of large language models using pre-collected prompts. However, collecting prompts that provide comprehensive coverage is both tedious and challenging, and often fails to include scenarios that LLMs need to improve on the most. In this paper, we investigate alignment through the lens of two-agent games, involving iterative interactions between an adversarial and a defensive agent. The adversarial agent's task at each step is to generate prompts that expose the weakness of the defensive agent. In return, the defensive agent seeks to improve its responses to these newly identified prompts it struggled with, based on feedback from the reward model.
We theoretically demonstrate that this iterative reinforcement learning optimization converges to a Nash Equilibrium for the game induced by the agents. Experimental results in safety scenarios demonstrate that learning in such a competitive environment not only fully trains agents but also leads to policies with enhanced generalization capabilities for both adversarial and defensive agents.

------------

`[2406.10985] Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens <https://arxiv.org/abs/2406.10985>`__ 深呼吸:用哨兵标记增强大型语言模型的语言建模

::

    Sun, 16 Jun 2024 15:50:10 GMT
    Weiyao Luo, Suncong Zheng, Heming Xia, Weikang Wang, Yan Lei, Tianyu Liu, Shuang Chen, Zhifang Sui

Large language models (LLMs) have shown promising efficacy across various tasks, becoming powerful tools in numerous aspects of human life. However, Transformer-based LLMs suffer a performance degradation when modeling long-term contexts due to they discard some information to reduce computational overhead.
In this work, we propose a simple yet effective method to enable LLMs to take a deep breath, encouraging them to summarize information contained within discrete text chunks. Specifically, we segment the text into multiple chunks and insert special token <SR> at the end of each chunk. We then modify the attention mask to integrate the chunk's information into the corresponding <SR> token. This facilitates LLMs to interpret information not only from historical individual tokens but also from the <SR> token, aggregating the chunk's semantic information. Experiments on language modeling and out-of-domain downstream tasks validate the superiority of our approach.

------------

`[2406.10991] Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers <https://arxiv.org/abs/2406.10991>`__ 自适应查询重写:根据会话答案的边际概率对重写器进行对齐

::

    Sun, 16 Jun 2024 16:09:05 GMT
    Tianhua Zhang, Kun Li, Hongyin Luo, Xixin Wu, James Glass, Helen Meng

Query rewriting is a crucial technique for passage retrieval in open-domain conversational question answering (CQA). It decontexualizes conversational queries into self-contained questions suitable for off-the-shelf retrievers.
Existing methods attempt to incorporate retriever's preference during the training of rewriting models. However, these approaches typically rely on extensive annotations such as in-domain rewrites and/or relevant passage labels, limiting the models' generalization and adaptation capabilities. In this paper, we introduce AdaQR ($\textbf{Ada}$ptive $\textbf{Q}$uery $\textbf{R}$ewriting), a framework for training query rewriting models with limited rewrite annotations from seed datasets and completely no passage label.
Our approach begins by fine-tuning compact large language models using only ~$10\%$ of rewrite annotations from the seed dataset training split. The models are then utilized to generate rewrite candidates for each query instance. A novel approach is then proposed to assess retriever's preference for these candidates by the probability of answers conditioned on the conversational query by marginalizing the Top-$K$ passages. This serves as the reward for optimizing the rewriter further using Direct Preference Optimization (DPO), a process free of rewrite and retrieval annotations. Experimental results on four open-domain CQA datasets demonstrate that AdaQR not only enhances the in-domain capabilities of the rewriter with limited annotation requirement, but also adapts effectively to out-of-domain datasets.

------------

`[2406.10996] THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation <https://arxiv.org/abs/2406.10996>`__ THEANINE:基于时间线增强响应生成的长时对话记忆管理研究

::

    Sun, 16 Jun 2024 16:17:46 GMT
    Seo Hyun Kim, Kai Tzu-iunn Ong, Taeyoon Kwon, Namyoung Kim, Keummin Ka, SeongHyeon Bae, Yohan Jo, Seung-won Hwang, Dongha Lee, Jinyoung Yeo

Large language models (LLMs) are capable of processing lengthy dialogue histories during prolonged interaction with users without additional memory modules; however, their responses tend to overlook or incorrectly recall information from the past. In this paper, we revisit memory-augmented response generation in the era of LLMs. While prior work focuses on getting rid of outdated memories, we argue that such memories can provide contextual cues that help dialogue systems understand the development of past events and, therefore, benefit response generation. We present Theanine, a framework that augments LLMs' response generation with memory timelines -- series of memories that demonstrate the development and causality of relevant past events. Along with Theanine, we introduce TeaFarm, a counterfactual-driven question-answering pipeline addressing the limitation of G-Eval in long-term conversations.
Supplementary videos of our methods and the TeaBag dataset for TeaFarm evaluation are in https://theanine-693b0.web.app/.

------------

`[2406.11030] FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture <https://arxiv.org/abs/2406.11030>`__ 

::

    Sun, 16 Jun 2024 17:59:32 GMT
    Wenyan Li, Xinyu Zhang, Jiaang Li, Qiwei Peng, Raphael Tang, Li Zhou, Weijia Zhang, Guimin Hu, Yifei Yuan, Anders S{\o}gaard, Daniel Hershcovich, Desmond Elliott

Food is a rich and varied dimension of cultural heritage, crucial to both individuals and social groups. To bridge the gap in the literature on the often-overlooked regional diversity in this domain, we introduce FoodieQA, a manually curated, fine-grained image-text dataset capturing the intricate features of food cultures across various regions in China. We evaluate vision-language Models (VLMs) and large language models (LLMs) on newly collected, unseen food images and corresponding questions. FoodieQA comprises three multiple-choice question-answering tasks where models need to answer questions based on multiple images, a single image, and text-only descriptions, respectively. While LLMs excel at text-based question answering, surpassing human accuracy, the open-sourced VLMs still fall short by 41\% on multi-image and 21\% on single-image VQA tasks, although closed-weights models perform closer to human levels (within 10\%). Our findings highlight that understanding food and its cultural implications remains a challenging and under-explored direction.

------------

`[2406.11036] garak: A Framework for Security Probing Large Language Models <https://arxiv.org/abs/2406.11036>`__ garak:大型语言模型安全探测框架

::

    Sun, 16 Jun 2024 18:18:43 GMT
    Leon Derczynski, Erick Galinkin, Jeffrey Martin, Subho Majumdar, Nanna Inie

As Large Language Models (LLMs) are deployed and integrated into thousands of applications, the need for scalable evaluation of how models respond to adversarial attacks grows rapidly. However, LLM security is a moving target: models produce unpredictable output, are constantly updated, and the potential adversary is highly diverse: anyone with access to the internet and a decent command of natural language. Further, what constitutes a security weak in one context may not be an issue in a different context; one-fits-all guardrails remain theoretical. In this paper, we argue that it is time to rethink what constitutes ``LLM security'', and pursue a holistic approach to LLM security evaluation, where exploration and discovery of issues are central. To this end, this paper introduces garak (Generative AI Red-teaming and Assessment Kit), a framework which can be used to discover and identify vulnerabilities in a target LLM or dialog system. garak probes an LLM in a structured fashion to discover potential vulnerabilities. The outputs of the framework describe a target model's weaknesses, contribute to an informed discussion of what composes vulnerabilities in unique contexts, and can inform alignment and policy discussions for LLM deployment.

------------

`[2406.11044] Evaluating the Performance of Large Language Models via Debates <https://arxiv.org/abs/2406.11044>`__ 通过辩论评估大型语言模型的性能

::

    Sun, 16 Jun 2024 19:02:31 GMT
    Behrad Moniri, Hamed Hassani, Edgar Dobriban

Large Language Models (LLMs) are rapidly evolving and impacting various fields, necessitating the development of effective methods to evaluate and compare their performance. Most current approaches for performance evaluation are either based on fixed, domain-specific questions that lack the flexibility required in many real-world applications where tasks are not always from a single domain, or rely on human input, making them unscalable. We propose an automated benchmarking framework based on debates between LLMs, judged by another LLM. This method assesses not only domain knowledge, but also skills such as problem definition and inconsistency recognition. We evaluate the performance of various state-of-the-art LLMs using the debate framework and achieve rankings that align closely with popular rankings based on human input, eliminating the need for costly human crowdsourcing.

------------

`[2406.11050] A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners <https://arxiv.org/abs/2406.11050>`__ Token偏差一瞥:大型语言模型还不是真正的推理机

::

    Sun, 16 Jun 2024 19:22:53 GMT
    Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J. Su, Camillo J. Taylor, Dan Roth

This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities.

------------

`[2406.11065] Can LLMs Understand the Implication of Emphasized Sentences in Dialogue? <https://arxiv.org/abs/2406.11065>`__ LLMs能否理解对话中强调句的含义?

::

    Sun, 16 Jun 2024 20:41:44 GMT
    Guan-Ting Lin, Hung-yi Lee

Emphasis is a crucial component in human communication, which indicates the speaker's intention and implication beyond pure text in dialogue. While Large Language Models (LLMs) have revolutionized natural language processing, their ability to understand emphasis in dialogue remains unclear. This paper introduces Emphasized-Talk, a benchmark with emphasis-annotated dialogue samples capturing the implications of emphasis. We evaluate various LLMs, both open-source and commercial, to measure their performance in understanding emphasis. Additionally, we propose an automatic evaluation pipeline using GPT-4, which achieves a high correlation with human rating. Our findings reveal that although commercial LLMs generally perform better, there is still significant room for improvement in comprehending emphasized sentences.

------------

`[2406.11073] Exploring the Limitations of Detecting Machine-Generated Text <https://arxiv.org/abs/2406.11073>`__ 探索检测机器生成文本的局限性

::

    Sun, 16 Jun 2024 21:02:02 GMT
    Jad Doughman, Osama Mohammed Afzal, Hawau Olamide Toyin, Shady Shehata, Preslav Nakov, Zeerak Talat

Recent improvements in the quality of the generations by large language models have spurred research into identifying machine-generated text. Systems proposed for the task often achieve high performance. However, humans and machines can produce text in different styles and in different domains, and it remains unclear whether machine generated-text detection models favour particular styles or domains. In this paper, we critically examine the classification performance for detecting machine-generated text by evaluating on texts with varying writing styles. We find that classifiers are highly sensitive to stylistic changes and differences in text complexity, and in some cases degrade entirely to random classifiers. We further find that detection systems are particularly susceptible to misclassify easy-to-read texts while they have high performance for complex texts.

------------

`[2406.11085] Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing <https://arxiv.org/abs/2406.11085>`__ 多源比一源好:在低资源着色中融入外部知识

::

    Sun, 16 Jun 2024 22:01:15 GMT
    Changbing Yang, Garrett Nicolai, Miikka Silfverberg

In this paper, we address the data scarcity problem in automatic data-driven glossing for low-resource languages by coordinating multiple sources of linguistic expertise. We supplement models with translations at both the token and sentence level as well as leverage the extensive linguistic capability of modern LLMs. Our enhancements lead to an average absolute improvement of 5%-points in word-level accuracy over the previous state of the art on a typologically diverse dataset spanning six low-resource languages. The improvements are particularly noticeable for the lowest-resourced language Gitksan, where we achieve a 10%-point improvement. Furthermore, in a simulated ultra-low resource setting for the same six languages, training on fewer than 100 glossed sentences, we establish an average 10%-point improvement in word-level accuracy over the previous state-of-the-art system.

------------

`[2406.11096] The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models <https://arxiv.org/abs/2406.11096>`__ 

::

    Sun, 16 Jun 2024 22:59:18 GMT
    Bolei Ma, Xinpeng Wang, Tiancheng Hu, Anna-Carolina Haensch, Michael A. Hedderich, Barbara Plank, Frauke Kreuter

Recent advances in Large Language Models (LLMs) have sparked wide interest in validating and comprehending the human-like cognitive-behavioral traits LLMs may have. These cognitive-behavioral traits include typically Attitudes, Opinions, Values (AOV). However, measuring AOV embedded within LLMs remains opaque, and different evaluation methods may yield different results. This has led to a lack of clarity on how different studies are related to each other and how they can be interpreted. This paper aims to bridge this gap by providing an overview of recent works on the evaluation of AOV in LLMs. Moreover, we survey related approaches in different stages of the evaluation pipeline in these works. By doing so, we address the potential and challenges with respect to understanding the model, human-AI alignment, and downstream application in social sciences. Finally, we provide practical insights into evaluation methods, model enhancement, and interdisciplinary collaboration, thereby contributing to the evolving landscape of evaluating AOV in LLMs.

------------

`[2406.11097] InstructCMP: Length Control in Sentence Compression through Instruction-based Large Language Models <https://arxiv.org/abs/2406.11097>`__ InstructCMP:基于指令的大型语言模型在句子压缩中的长度控制

::

    Sun, 16 Jun 2024 23:00:47 GMT
    Juseon-Do, Jingun Kwon, Hidetaka Kamigaito, Manabu Okumura

Extractive summarization can produce faithful summaries but often requires additional constraints such as a desired summary length. Traditional sentence compression models do not typically consider the constraints because of their restricted model abilities, which require model modifications for coping with them. To bridge this gap, we propose Instruction-based Compression (InstructCMP), an approach to the sentence compression task that can consider the length constraint through instructions by leveraging the zero-shot task-solving abilities of Large Language Models (LLMs). For this purpose, we created new evaluation datasets by transforming traditional sentence compression datasets into an instruction format. By using the datasets, we first reveal that the current LLMs still face challenges in accurately controlling the length for a compressed text. To address this issue, we propose an approach named "length priming," that incorporates additional length information into the instructions without external resources. While the length priming effectively works in a zero-shot setting, a training dataset with the instructions would further improve the ability of length control. Thus, we additionally created a training dataset in an instruction format to fine-tune the model on it. Experimental results and analysis show that applying the length priming significantly improves performances of InstructCMP in both zero-shot and fine-tuning settings without the need of any model modifications.

------------

`[2406.11102] Grading Massive Open Online Courses Using Large Language Models <https://arxiv.org/abs/2406.11102>`__ 基于大型语言模型的大规模在线开放课程评分

::

    Sun, 16 Jun 2024 23:42:11 GMT
    Shahriar Golchin, Nikhil Garuda, Christopher Impey, Matthew Wenger

Massive open online courses (MOOCs) offer free education globally to anyone with a computer and internet access. Despite this democratization of learning, the massive enrollment in these courses makes it impractical for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, we explore the feasibility of using large language models (LLMs) to replace peer grading in MOOCs. Specifically, we use two LLMs, GPT-4 and GPT-3.5, across three MOOCs: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on the zero-shot chain-of-thought (ZCoT) prompting technique: (1) ZCoT with instructor-provided correct answers, (2) ZCoT with both instructor-provided correct answers and rubrics, and (3) ZCoT with instructor-provided correct answers and LLM-generated rubrics. Tested on 18 settings, our results show that ZCoT, when augmented with instructor-provided correct answers and rubrics, produces grades that are more aligned with those assigned by instructors compared to peer grading. Finally, our findings indicate a promising potential for automated grading systems in MOOCs, especially in subjects with well-defined rubrics, to improve the learning experience for millions of online learners worldwide.

------------

`[2406.11106] From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models <https://arxiv.org/abs/2406.11106>`__ 从意图到技术:大型语言模型文本水印的全面分类与挑战

::

    Mon, 17 Jun 2024 00:09:31 GMT
    Harsh Nishant Lalai, Aashish Anantha Ramakrishnan, Raj Sanjay Shah, Dongwon Lee

With the rapid growth of Large Language Models (LLMs), safeguarding textual content against unauthorized use is crucial. Text watermarking offers a vital solution, protecting both - LLM-generated and plain text sources. This paper presents a unified overview of different perspectives behind designing watermarking techniques, through a comprehensive survey of the research literature. Our work has two key advantages, (1) we analyze research based on the specific intentions behind different watermarking techniques, evaluation datasets used, watermarking addition, and removal methods to construct a cohesive taxonomy. (2) We highlight the gaps and open challenges in text watermarking to promote research in protecting text authorship. This extensive coverage and detailed analysis sets our work apart, offering valuable insights into the evolving landscape of text watermarking in language models.

------------

`[2406.11107] Exploring Safety-Utility Trade-Offs in Personalized Language Models <https://arxiv.org/abs/2406.11107>`__ 探索个性化语言模型的安全性-效用权衡

::

    Mon, 17 Jun 2024 00:17:11 GMT
    Anvesh Rao Vijjini, Somnath Basu Roy Chowdhury, Snigdha Chaturvedi

As large language models (LLMs) become increasingly integrated into daily applications, it is essential to ensure they operate fairly across diverse user demographics. In this work, we show that LLMs suffer from personalization bias, where their performance is impacted when they are personalized to a user's identity. We quantify personalization bias by evaluating the performance of LLMs along two axes - safety and utility. We measure safety by examining how benign LLM responses are to unsafe prompts with and without personalization. We measure utility by evaluating the LLM's performance on various tasks, including general knowledge, mathematical abilities, programming, and reasoning skills.
We find that various LLMs, ranging from open-source models like Llama (Touvron et al., 2023) and Mistral (Jiang et al., 2023) to API-based ones like GPT-3.5 and GPT-4o (Ouyang et al., 2022), exhibit significant variance in performance in terms of safety-utility trade-offs depending on the user's identity.
Finally, we discuss several strategies to mitigate personalization bias using preference tuning and prompt-based defenses.

------------

`[2406.11109] Investigating Annotator Bias in Large Language Models for Hate Speech Detection <https://arxiv.org/abs/2406.11109>`__ 面向仇恨言论检测的大型语言模型标注者偏见研究

::

    Mon, 17 Jun 2024 00:18:31 GMT
    Amit Das, Zheng Zhang, Fatemeh Jamshidi, Vinija Jain, Aman Chadha, Nilanjana Raychawdhary, Mary Sandage, Lauramarie Pope, Gerry Dozier, Cheryl Seals

Data annotation, the practice of assigning descriptive labels to raw data, is pivotal in optimizing the performance of machine learning models. However, it is a resource-intensive process susceptible to biases introduced by annotators.
The emergence of sophisticated Large Language Models (LLMs), like ChatGPT presents a unique opportunity to modernize and streamline this complex procedure. While existing research extensively evaluates the efficacy of LLMs, as annotators, this paper delves into the biases present in LLMs, specifically GPT 3.5 and GPT 4o when annotating hate speech data. Our research contributes to understanding biases in four key categories: gender, race, religion, and disability. Specifically targeting highly vulnerable groups within these categories, we analyze annotator biases. Furthermore, we conduct a comprehensive examination of potential factors contributing to these biases by scrutinizing the annotated data. We introduce our custom hate speech detection dataset, HateSpeechCorpus, to conduct this research. Additionally, we perform the same experiments on the ETHOS (Mollas et al., 2022) dataset also for comparative analysis. This paper serves as a crucial resource, guiding researchers and practitioners in harnessing the potential of LLMs for dataannotation, thereby fostering advancements in this critical field. The HateSpeechCorpus dataset is available here: https://github.com/AmitDasRup123/HateSpeechCorpus

------------

`[2406.11115] Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification <https://arxiv.org/abs/2406.11115>`__ 文本嫁接:文本分类中少数类的近分布弱监督

::

    Mon, 17 Jun 2024 00:23:08 GMT
    Letian Peng, Yi Gu, Chengyu Dong, Zihan Wang, Jingbo Shang

For extremely weak-supervised text classification, pioneer research generates pseudo labels by mining texts similar to the class names from the raw corpus, which may end up with very limited or even no samples for the minority classes.
Recent works have started to generate the relevant texts by prompting LLMs using the class names or definitions; however, there is a high risk that LLMs cannot generate in-distribution (i.e., similar to the corpus where the text classifier will be applied) data, leading to ungeneralizable classifiers. In this paper, we combine the advantages of these two approaches and propose to bridge the gap via a novel framework, \emph{text grafting}, which aims to obtain clean and near-distribution weak supervision for minority classes.
Specifically, we first use LLM-based logits to mine masked templates from the raw corpus, which have a high potential for data synthesis into the target minority class. Then, the templates are filled by state-of-the-art LLMs to synthesize near-distribution texts falling into minority classes. Text grafting shows significant improvement over direct mining or synthesis on minority classes. We also use analysis and case studies to comprehend the property of text grafting.

------------

`[2406.11116] Grammaticality Representation in ChatGPT as Compared to Linguists and Laypeople <https://arxiv.org/abs/2406.11116>`__ 与语言学家和外行人比较ChatGPT中的语法表示

::

    Mon, 17 Jun 2024 00:23:16 GMT
    Zhuang Qiu, Xufeng Duan, Zhenguang G. Cai

Large language models (LLMs) have demonstrated exceptional performance across various linguistic tasks. However, it remains uncertain whether LLMs have developed human-like fine-grained grammatical intuition. This preregistered study (https://osf.io/t5nes) presents the first large-scale investigation of ChatGPT's grammatical intuition, building upon a previous study that collected laypeople's grammatical judgments on 148 linguistic phenomena that linguists judged to be grammatical, ungrammatical, or marginally grammatical (Sprouse, Schutze, & Almeida, 2013). Our primary focus was to compare ChatGPT with both laypeople and linguists in the judgement of these linguistic constructions. In Experiment 1, ChatGPT assigned ratings to sentences based on a given reference sentence. Experiment 2 involved rating sentences on a 7-point scale, and Experiment 3 asked ChatGPT to choose the more grammatical sentence from a pair.
Overall, our findings demonstrate convergence rates ranging from 73% to 95% between ChatGPT and linguists, with an overall point-estimate of 89%.
Significant correlations were also found between ChatGPT and laypeople across all tasks, though the correlation strength varied by task. We attribute these results to the psychometric nature of the judgment tasks and the differences in language processing styles between humans and LLMs.

------------

`[2406.11131] Are Large Language Models a Good Replacement of Taxonomies? <https://arxiv.org/abs/2406.11131>`__ 

::

    Mon, 17 Jun 2024 01:21:50 GMT
    Yushi Sun, Hao Xin, Kai Sun, Yifan Ethan Xu, Xiao Yang, Xin Luna Dong, Nan Tang, Lei Chen

Large language models (LLMs) demonstrate an impressive ability to internalize knowledge and answer natural language questions. Although previous studies validate that LLMs perform well on general knowledge while presenting poor performance on long-tail nuanced knowledge, the community is still doubtful about whether the traditional knowledge graphs should be replaced by LLMs. In this paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made obsolete by LLMs. Intuitively, LLMs should perform well on common taxonomies and at taxonomy levels that are common to people. Unfortunately, there lacks a comprehensive benchmark that evaluates the LLMs over a wide range of taxonomies from common to specialized domains and at levels from root to leaf so that we can draw a confident conclusion. To narrow the research gap, we constructed a novel taxonomy hierarchical structure discovery benchmark named TaxoGlimpse to evaluate the performance of LLMs over taxonomies. TaxoGlimpse covers ten representative taxonomies from common to specialized domains with in-depth experiments of different levels of entities in this taxonomy from root to leaf.
Our comprehensive experiments of eighteen state-of-the-art LLMs under three prompting settings validate that LLMs can still not well capture the knowledge of specialized taxonomies and leaf-level entities.

------------

`[2406.11139] Breaking Boundaries: Investigating the Effects of Model Editing on Cross-linguistic Performance <https://arxiv.org/abs/2406.11139>`__ 突破边界:研究模型编辑对跨语言表现的影响

::

    Mon, 17 Jun 2024 01:54:27 GMT
    Somnath Banerjee, Avik Halder, Rajarshi Mandal, Sayan Layek, Ian Soboroff, Rima Hazra, Animesh Mukherjee

The integration of pretrained language models (PLMs) like BERT and GPT has revolutionized NLP, particularly for English, but it has also created linguistic imbalances. This paper strategically identifies the need for linguistic equity by examining several knowledge editing techniques in multilingual contexts. We evaluate the performance of models such as Mistral, TowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including English, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our research identifies significant discrepancies in normal and merged models concerning cross-lingual consistency. We employ strategies like 'each language for itself' (ELFI) and 'each language for others' (ELFO) to stress-test these models. Our findings demonstrate the potential for LLMs to overcome linguistic barriers, laying the groundwork for future research in achieving linguistic inclusivity in AI technologies.

------------

`[2406.11149] GoldCoin: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory <https://arxiv.org/abs/2406.11149>`__ GoldCoin:基于上下文完整性理论的隐私法大型语言模型

::

    Mon, 17 Jun 2024 02:27:32 GMT
    Wei Fan, Haoran Li, Zheye Deng, Weiqi Wang, Yangqiu Song

Privacy issues arise prominently during the inappropriate transmission of information between entities. Existing research primarily studies privacy by exploring various privacy attacks, defenses, and evaluations within narrowly predefined patterns, while neglecting that privacy is not an isolated, context-free concept limited to traditionally sensitive data (e.g., social security numbers), but intertwined with intricate social contexts that complicate the identification and analysis of potential privacy violations. The advent of Large Language Models (LLMs) offers unprecedented opportunities for incorporating the nuanced scenarios outlined in privacy laws to tackle these complex privacy issues. However, the scarcity of open-source relevant case studies restricts the efficiency of LLMs in aligning with specific legal statutes. To address this challenge, we introduce a novel framework, GoldCoin, designed to efficiently ground LLMs in privacy laws for judicial assessing privacy violations. Our framework leverages the theory of contextual integrity as a bridge, creating numerous synthetic scenarios grounded in relevant privacy statutes (e.g., HIPAA), to assist LLMs in comprehending the complex contexts for identifying privacy risks in the real world. Extensive experimental results demonstrate that GoldCoin markedly enhances LLMs' capabilities in recognizing privacy risks across real court cases, surpassing the baselines on different judicial tasks.

------------

`[2406.11162] How Good are LLMs at Relation Extraction under Low-Resource Scenario? Comprehensive Evaluation <https://arxiv.org/abs/2406.11162>`__ llm在低资源场景下的关系提取效果如何?综合评价

::

    Mon, 17 Jun 2024 03:02:04 GMT
    Dawulie Jinensibieke, Mieradilijiang Maimaiti, Wentao Xiao, Yuanhang Zheng, Xiangbo Wang

Relation Extraction (RE) serves as a crucial technology for transforming unstructured text into structured information, especially within the framework of Knowledge Graph development. Its importance is emphasized by its essential role in various downstream tasks. Besides the conventional RE methods which are based on neural networks and pre-trained language models, large language models (LLMs) are also utilized in the research field of RE. However, on low-resource languages (LRLs), both conventional RE methods and LLM-based methods perform poorly on RE due to the data scarcity issues. To this end, this paper constructs low-resource relation extraction datasets in 10 LRLs in three regions (Central Asia, Southeast Asia and Middle East). The corpora are constructed by translating the original publicly available English RE datasets (NYT10, FewRel and CrossRE) using an effective multilingual machine translation. Then, we use the language perplexity (PPL) to filter out the low-quality data from the translated datasets. Finally, we conduct an empirical study and validate the performance of several open-source LLMs on these generated LRL RE datasets.

------------

`[2406.11177] TIFG: Text-Informed Feature Generation with Large Language Models <https://arxiv.org/abs/2406.11177>`__ TIFG:基于大型语言模型的文本特征生成

::

    Mon, 17 Jun 2024 03:29:14 GMT
    Xinhao Zhang, Jinghan Zhang, Fengran Mo, Yuzhong Chen, Kunpeng Liu

Textual information of data is of vital importance for data mining and feature engineering. However, existing methods focus on learning the data structures and overlook the textual information along with the data.
Consequently, they waste this valuable resource and miss out on the deeper data relationships embedded within the texts. In this paper, we introduce Text-Informed Feature Generation (TIFG), a novel LLM-based text-informed feature generation framework. TIFG utilizes the textual information to generate features by retrieving possible relevant features within external knowledge with Retrieval Augmented Generation (RAG) technology. In this approach, the TIFG can generate new explainable features to enrich the feature space and further mine feature relationships. We design the TIFG to be an automated framework that continuously optimizes the feature generation process, adapts to new data inputs, and improves downstream task performance over iterations. A broad range of experiments in various downstream tasks showcases that our approach can generate high-quality and meaningful features, and is significantly superior to existing methods.

------------

`[2406.11190] Aligning Large Language Models from Self-Reference AI Feedback with one General Principle <https://arxiv.org/abs/2406.11190>`__ 用一个一般原则对齐自参考人工智能反馈中的大型语言模型

::

    Mon, 17 Jun 2024 03:51:46 GMT
    Rong Bao, Rui Zheng, Shihan Dou, Xiao Wang, Enyu Zhou, Bo Wang, Qi Zhang, Liang Ding, Dacheng Tao

In aligning large language models (LLMs), utilizing feedback from existing advanced AI rather than humans is an important method to scale supervisory signals. However, it is highly challenging for AI to understand human intentions and societal values, and provide accurate preference feedback based on these. Current AI feedback methods rely on powerful LLMs, carefully designed specific principles to describe human intentions, and are easily influenced by position bias. To address these issues, we propose a self-reference-based AI feedback framework that enables a 13B Llama2-Chat to provide high-quality feedback under simple and general principles such as ``best for humanity``.
Specifically, we allow the AI to first respond to the user's instructions, then generate criticism of other answers based on its own response as a reference, and finally determine which answer better fits human preferences according to the criticism. Additionally, we use a self-consistency method to further reduce the impact of position bias, and employ semantic perplexity to calculate the preference strength differences between different answers. Experimental results show that our method enables 13B and 70B Llama2-Chat annotators to provide high-quality preference feedback, and the policy models trained based on these preference data achieve significant advantages in benchmark datasets through reinforcement learning.

------------

`[2406.11192] Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets and Languages for Open Named Entity Recognition <https://arxiv.org/abs/2406.11192>`__ 超越边界:为开放命名实体识别学习跨数据集和语言的通用实体分类法

::

    Mon, 17 Jun 2024 03:57:35 GMT
    Yuming Yang, Wantong Zhao, Caishuang Huang, Junjie Ye, Xiao Wang, Huiyuan Zheng, Yang Nan, Yuran Wang, Xueying Xu, Kaixin Huang, Yunke Zhang, Tao Gui, Qi Zhang and Xuanjing Huang

Open Named Entity Recognition (NER), which involves identifying arbitrary types of entities from arbitrary domains, remains challenging for Large Language Models (LLMs). Recent studies suggest that fine-tuning LLMs on extensive NER data can boost their performance. However, training directly on existing datasets faces issues due to inconsistent entity definitions and redundant data, limiting LLMs to dataset-specific learning and hindering out-of-domain generalization. To address this, we present B2NERD, a cohesive and efficient dataset for Open NER, normalized from 54 existing English or Chinese datasets using a two-step approach. First, we detect inconsistent entity definitions across datasets and clarify them by distinguishable label names to construct a universal taxonomy of 400+ entity types. Second, we address redundancy using a data pruning strategy that selects fewer samples with greater category and semantic diversity. Comprehensive evaluation shows that B2NERD significantly improves LLMs' generalization on Open NER. Our B2NER models, trained on B2NERD, outperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3 out-of-domain benchmarks across 15 datasets and 6 languages.

------------

`[2406.11193] MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model <https://arxiv.org/abs/2406.11193>`__ MMNeuron:多模态大型语言模型中神经元级特定领域解释的发现

::

    Mon, 17 Jun 2024 03:59:44 GMT
    Jiahao Huo, Yibo Yan, Boren Hu, Yutao Yue, Xuming Hu

Projecting visual features into word embedding space has become a significant fusion strategy adopted by Multimodal Large Language Models (MLLMs). However, its internal mechanisms have yet to be explored. Inspired by multilingual research, we identify domain-specific neurons in multimodal large language models. Specifically, we investigate the distribution of domain-specific neurons and the mechanism of how MLLMs process features from diverse domains.
Furthermore, we propose a three-stage framework for language model modules in MLLMs when handling projected image features, and verify this hypothesis using logit lens. Extensive experiments indicate that while current MLLMs exhibit Visual Question Answering (VQA) capability, they may not fully utilize domain-specific information. Manipulating domain-specific neurons properly will result in a 10\% change of accuracy at most, shedding light on the development of cross-domain, all-encompassing MLLMs in the future. Our code will be released upon paper notification.

------------

`[2406.11201] Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models <https://arxiv.org/abs/2406.11201>`__ 微调还是微调失败?揭穿大型语言模型的性能神话

::

    Mon, 17 Jun 2024 04:35:17 GMT
    Scott Barnett, Zac Brannelly, Stefanus Kurniawan, Sheng Wong

Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating: "To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case." This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI.
This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.

------------

`[2406.11214] Global Data Constraints: Ethical and Effectiveness Challenges in Large Language Model <https://arxiv.org/abs/2406.11214>`__ 全局数据约束:大型语言模型中的伦理和有效性挑战

::

    Mon, 17 Jun 2024 05:13:25 GMT
    Jin Yang, Zhiqiang Wang, Yanbin Lin, Zunduo Zhao

The efficacy and ethical integrity of large language models (LLMs) are profoundly influenced by the diversity and quality of their training datasets.
However, the global landscape of data accessibility presents significant challenges, particularly in regions with stringent data privacy laws or limited open-source information. This paper examines the multifaceted challenges associated with acquiring high-quality training data for LLMs, focusing on data scarcity, bias, and low-quality content across various linguistic contexts. We highlight the technical and ethical implications of relying on publicly available but potentially biased or irrelevant data sources, which can lead to the generation of biased or hallucinatory content by LLMs. Through a series of evaluations using GPT-4 and GPT-4o, we demonstrate how these data constraints adversely affect model performance and ethical alignment. We propose and validate several mitigation strategies designed to enhance data quality and model robustness, including advanced data filtering techniques and ethical data collection practices. Our findings underscore the need for a proactive approach in developing LLMs that considers both the effectiveness and ethical implications of data constraints, aiming to foster the creation of more reliable and universally applicable AI systems.

------------

`[2406.11234] MiniConGTS: A Near Ultimate Minimalist Contrastive Grid Tagging Scheme for Aspect Sentiment Triplet Extraction <https://arxiv.org/abs/2406.11234>`__ MiniConGTS:面向方面情感三元组抽取的极简对比网格标注方案

::

    Mon, 17 Jun 2024 06:01:11 GMT
    Qiao Sun, Liujia Yang, Minghao Ma, Nanyang Ye, Qinying Gu

Aspect Sentiment Triplet Extraction (ASTE) aims to co-extract the sentiment triplets in a given corpus. Existing approaches within the pretraining-finetuning paradigm tend to either meticulously craft complex tagging schemes and classification heads, or incorporate external semantic augmentation to enhance performance. In this study, we, for the first time, re-evaluate the redundancy in tagging schemes and the internal enhancement in pretrained representations. We propose a method to improve and utilize pretrained representations by integrating a minimalist tagging scheme and a novel token-level contrastive learning strategy. The proposed approach demonstrates comparable or superior performance compared to state-of-the-art techniques while featuring a more compact design and reduced computational overhead. Additionally, we are the first to formally evaluate GPT-4's performance in few-shot learning and Chain-of-Thought scenarios for this task.
The results demonstrate that the pretraining-finetuning paradigm remains highly effective even in the era of large language models.

------------

`[2406.11238] What Kinds of Tokens Benefit from Distant Text? An Analysis on Long Context Language Modeling <https://arxiv.org/abs/2406.11238>`__ 哪些类型的标记可以从远程文本中受益?长上下文语言建模分析

::

    Mon, 17 Jun 2024 06:07:29 GMT
    Yutong Hu and Quzhe Huang and Kangcheng Luo and Yansong Feng

As the context length that large language models can handle continues to increase, these models demonstrate an enhanced ability to utilize distant information for tasks such as language modeling. This capability contrasts with human reading and writing habits, where it is uncommon to remember and use particularly distant information, except in cases of foreshadowing. In this paper, we aim to explore which kinds of words benefit more from long contexts in language models. By analyzing the changes in token probabilities with increasing context length, we find that content words (e.g., nouns, adjectives) and the initial tokens of words benefit the most. Frequent patterns in the context (N-grams) also significantly impact predictions. Additionally, the model's prior knowledge plays a crucial role in influencing predictions, especially for rare tokens. We also observe that language models become more confident with longer contexts, resulting in sharper probability distributions.
This overconfidence may contribute to the increasing probabilities of tokens with distant contextual information. We hope that our analysis will help the community better understand long-text language modeling and contribute to the design of more reliable long-context models.

------------

`[2406.11239] Evading AI-Generated Content Detectors using Homoglyphs <https://arxiv.org/abs/2406.11239>`__ 使用同体文字规避ai生成的内容检测器

::

    Mon, 17 Jun 2024 06:07:32 GMT
    Aldan Creo, Shushanta Pudasaini

The generation of text that is increasingly human-like has been enabled by the advent of large language models (LLMs). As the detection of AI-generated content holds significant importance in the fight against issues such as misinformation and academic cheating, numerous studies have been conducted to develop reliable LLM detectors. While promising results have been demonstrated by such detectors on test data, recent research has revealed that they can be circumvented by employing different techniques. In this article, homoglyph-based ($a \rightarrow {\alpha}$) attacks that can be used to circumvent existing LLM detectors are presented. The efficacy of the attacks is illustrated by analizing how homoglyphs shift the tokenization of the text, and thus its token loglikelihoods. A comprehensive evaluation is conducted to assess the effectiveness of homoglyphs on state-of-the-art LLM detectors, including Binoculars, DetectGPT, OpenAI's detector, and watermarking techniques, on five different datasets. A significant reduction in the efficiency of all the studied configurations of detectors and datasets, down to an accuracy of 0.5 (random guessing), is demonstrated by the proposed approach.
The results show that homoglyph-based attacks can effectively evade existing LLM detectors, and the implications of these findings are discussed along with possible defenses against such attacks.

------------

`[2406.11250] Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs <https://arxiv.org/abs/2406.11250>`__ 

::

    Mon, 17 Jun 2024 06:22:20 GMT
    Muhammad Arslan Manzoor, Yuxia Wang, Minghan Wang, Preslav Nakov

Empathy plays a pivotal role in fostering prosocial behavior, often triggered by the sharing of personal experiences through narratives. However, modeling empathy using NLP approaches remains challenging due to its deep interconnection with human interaction dynamics. Previous approaches, which involve fine-tuning language models (LMs) on human-annotated empathic datasets, have had limited success. In our pursuit of improving empathy understanding in LMs, we propose several strategies, including contrastive learning with masked LMs and supervised fine-tuning with Large Language Models (LLMs). While these methods show improvements over previous methods, the overall results remain unsatisfactory. To better understand this trend, we performed an analysis which reveals a low agreement among annotators. This lack of consensus hinders training and highlights the subjective nature of the task. We also explore the cultural impact on annotations. To study this, we meticulously collected story pairs in Urdu language and find that subjectivity in interpreting empathy among annotators appears to be independent of cultural background. The insights from our systematic exploration of LMs' understanding of empathy suggest that there is considerable room for exploration in both task formulation and modeling.

------------

`[2406.11260] Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection <https://arxiv.org/abs/2406.11260>`__ 基于大规模语言模型的对抗风格增强鲁棒假新闻检测

::

    Mon, 17 Jun 2024 07:00:41 GMT
    Sungwon Park, Sungwon Han, Meeyoung Cha

The spread of fake news negatively impacts individuals and is regarded as a significant social challenge that needs to be addressed. A number of algorithmic and insightful features have been identified for detecting fake news. However, with the recent LLMs and their advanced generation capabilities, many of the detectable features (e.g., style-conversion attacks) can be altered, making it more challenging to distinguish from real news. This study proposes adversarial style augmentation, AdStyle, to train a fake news detector that remains robust against various style-conversion attacks. Our model's key mechanism is the careful use of LLMs to automatically generate a diverse yet coherent range of style-conversion attack prompts. This improves the generation of prompts that are particularly difficult for the detector to handle.
Experiments show that our augmentation strategy improves robustness and detection performance when tested on fake news benchmark datasets.

------------

`[2406.11263] The Fall of ROME: Understanding the Collapse of LLMs in Model Editing <https://arxiv.org/abs/2406.11263>`__ 罗马的衰落:理解模型编辑中llm的崩溃

::

    Mon, 17 Jun 2024 07:08:29 GMT
    Wanli Yang, Fei Sun, Jiajun Tan, Xinyu Ma, Du Su, Dawei Yin, Huawei Shen

Despite significant progress in model editing methods, their application in real-world scenarios remains challenging as they often cause large language models (LLMs) to collapse. Among them, ROME is particularly concerning, as it could disrupt LLMs with only a single edit. In this paper, we study the root causes of such collapse. Through extensive analysis, we identify two primary factors that contribute to the collapse: i) inconsistent handling of prefixed and unprefixed keys in the parameter update equation may result in very small denominators, causing excessively large parameter updates; ii) the subject of collapse cases is usually the first token, whose unprefixed key distribution significantly differs from the prefixed key distribution in autoregressive transformers, causing the aforementioned issue to materialize. To validate our analysis, we propose a simple yet effective approach: uniformly using prefixed keys during editing phase and adding prefixes during the testing phase. The experimental results show that the proposed solution can prevent model collapse while maintaining the effectiveness of the edits.

------------

`[2406.11267] Mitigating Large Language Model Hallucination with Faithful Finetuning <https://arxiv.org/abs/2406.11267>`__ 通过忠实微调缓解大型语言模型幻觉

::

    Mon, 17 Jun 2024 07:16:07 GMT
    Minda Hu, Bowei He, Yufei Wang, Liangyou Li, Chen Ma, Irwin King

Large language models (LLMs) have demonstrated remarkable performance on various natural language processing tasks. However, they are prone to generating fluent yet untruthful responses, known as "hallucinations".
Hallucinations can lead to the spread of misinformation and cause harm in critical applications. Mitigating hallucinations is challenging as they arise from factors such as noisy data, model overconfidence, lack of knowledge, and the generation process itself. Recent efforts have attempted to address this issue through representation editing and decoding algorithms, reducing hallucinations without major structural changes or retraining. However, these approaches either implicitly edit LLMs' behavior in latent space or suppress the tendency to output unfaithful results during decoding instead of explicitly modeling on hallucination. In this work, we introduce Faithful Finetuning (F2), a novel method that explicitly models the process of faithful question answering through carefully designed loss functions during fine-tuning. We conduct extensive experiments on popular datasets and demonstrate that F2 achieves significant improvements over vanilla models and baselines.

------------

`[2406.11275] Self-training Large Language Models through Knowledge Detection <https://arxiv.org/abs/2406.11275>`__ 基于知识检测的大型语言模型自训练

::

    Mon, 17 Jun 2024 07:25:09 GMT
    Wei Jie Yeo, Teddy Ferdinan, Przemyslaw Kazienko, Ranjan Satapathy, Erik Cambria

Large language models (LLMs) often necessitate extensive labeled datasets and training compute to achieve impressive performance across downstream tasks.
This paper explores a self-training paradigm, where the LLM autonomously curates its own labels and selectively trains on unknown data samples identified through a reference-free consistency method. Empirical evaluations demonstrate significant improvements in reducing hallucination in generation across multiple subjects. Furthermore, the selective training framework mitigates catastrophic forgetting in out-of-distribution benchmarks, addressing a critical limitation in training LLMs. Our findings suggest that such an approach can substantially reduce the dependency on large labeled datasets, paving the way for more scalable and cost-effective language model training.

------------

`[2406.11278] Do Not Design, Learn: A Trainable Scoring Function for Uncertainty Estimation in Generative LLMs <https://arxiv.org/abs/2406.11278>`__ 不设计，学习:生成式llm中不确定性估计的可训练评分函数

::

    Mon, 17 Jun 2024 07:30:40 GMT
    Duygu Nur Yaldiz, Yavuz Faruk Bakman, Baturalp Buyukates, Chenyang Tao, Anil Ramakrishna, Dimitrios Dimitriadis, Salman Avestimehr

In this work, we introduce the Learnable Response Scoring Function (LARS) for Uncertainty Estimation (UE) in generative Large Language Models (LLMs). Current scoring functions for probability-based UE, such as length-normalized scoring and semantic contribution-based weighting, are designed to solve specific aspects of the problem but exhibit limitations, including the inability to handle biased probabilities and under-performance in low-resource languages like Turkish. To address these issues, we propose LARS, a scoring function that leverages supervised data to capture complex dependencies between tokens and probabilities, thereby producing more reliable and calibrated response scores in computing the uncertainty of generations. Our extensive experiments across multiple datasets show that LARS substantially outperforms existing scoring functions considering various probability-based UE methods.

------------

`[2406.11341] A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences <https://arxiv.org/abs/2406.11341>`__ 大型语言模型作为软推理器的系统分析:三段论推理的情况

::

    Mon, 17 Jun 2024 08:59:04 GMT
    Leonardo Bertolazzi, Albert Gatt, Raffaella Bernardi

The reasoning abilities of Large Language Models (LLMs) are becoming a central focus of study in NLP. In this paper, we consider the case of syllogistic reasoning, an area of deductive reasoning studied extensively in logic and cognitive psychology. Previous research has shown that pre-trained LLMs exhibit reasoning biases, such as $\textit{content effects}$, avoid answering that $\textit{no conclusion follows}$, display human-like difficulties, and struggle with multi-step reasoning. We contribute to this research line by systematically investigating the effects of chain-of-thought reasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on syllogistic reasoning, considering syllogisms with conclusions that support or violate world knowledge, as well as ones with multiple premises. Crucially, we go beyond the standard focus on accuracy, with an in-depth analysis of the conclusions generated by the models. Our results suggest that the behavior of pre-trained LLMs can be explained by heuristics studied in cognitive science and that both ICL and SFT improve model performance on valid inferences, although only the latter mitigates most reasoning biases without harming model consistency.

------------

`[2406.11345] Full-ECE: A Metric For Token-level Calibration on Large Language Models <https://arxiv.org/abs/2406.11345>`__ Full-ECE:大型语言模型标记级校准指标

::

    Mon, 17 Jun 2024 09:07:58 GMT
    Han Liu, Yupeng Zhang, Bingning Wang, Weipeng Chen, Xiaolin Hu

Deep Neural Networks (DNNs) excel in various domains but face challenges in providing accurate uncertainty estimates, which are crucial for high-stakes applications. Large Language Models (LLMs) have recently emerged as powerful tools, demonstrating exceptional performance in language tasks. However, traditional calibration metrics such as Expected Calibration Error (ECE) and classwise-ECE (cw-ECE) are inadequate for LLMs due to their vast vocabularies, data complexity, and distributional focus. To address this, we propose a novel calibration concept called full calibration and introduce its corresponding metric, Full-ECE. Full-ECE evaluates the entire predicted probability distribution, offering a more accurate and robust measure of calibration for LLMs.

------------

`[2406.11354] Preserving Knowledge in Large Language Model: A Model-Agnostic Self-Decompression Approach <https://arxiv.org/abs/2406.11354>`__ 大型语言模型中的知识保存:一种与模型无关的自解压方法

::

    Mon, 17 Jun 2024 09:17:40 GMT
    Zilun Zhang, Yutao Sun, Tiancheng Zhao, Leigang Sha, Ruochen Xu, Kyusong Lee, Jianwei Yin

Humans can retain old knowledge while learning new information, but Large Language Models (LLMs) often suffer from catastrophic forgetting when post-pretrained or supervised fine-tuned (SFT) on domain-specific data.
Moreover, for Multimodal Large Language Models (MLLMs) which are composed of the LLM base and visual projector (e.g. LLaVA), a significant decline in performance on language benchmarks was observed compared to their single-modality counterparts. To address these challenges, we introduce a novel model-agnostic self-decompression method, Tree Generation (TG), that decompresses knowledge within LLMs into the training corpus. This paper focuses on TG-SFT, which can synthetically generate SFT data for the instruction tuning steps. By incorporating the dumped corpus during SFT for MLLMs, we significantly reduce the forgetting problem.

------------

`[2406.11370] Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments <https://arxiv.org/abs/2406.11370>`__ 更公平的偏好引发改进的人类对齐的大型语言模型判断

::

    Mon, 17 Jun 2024 09:48:53 GMT
    Han Zhou, Xingchen Wan, Yinhong Liu, Nigel Collier, Ivan Vuli\'c, Anna Korhonen

Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.

------------

`[2406.11380] A Realistic Evaluation of LLMs for Quotation Attribution in Literary Texts: A Case Study of LLaMa3 <https://arxiv.org/abs/2406.11380>`__ 文学文本中引用归因的llm现实评估:以LLaMa3为例

::

    Mon, 17 Jun 2024 09:56:46 GMT
    Gaspard Michel and Elena V. Epure and Romain Hennequin and Christophe Cerisara

Large Language Models (LLMs) zero-shot and few-shot performance are subject to memorization and data contamination, complicating the assessment of their validity. In literary tasks, the performance of LLMs is often correlated to the degree of book memorization. In this work, we carry out a realistic evaluation of LLMs for quotation attribution in novels, taking the instruction fined-tuned version of Llama3 as an example. We design a task-specific memorization measure and use it to show that Llama3's ability to perform quotation attribution is positively correlated to the novel degree of memorization. However, Llama3 still performs impressively well on books it has not memorized nor seen. Data and code will be made publicly available.

------------

`[2406.11385] MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic <https://arxiv.org/abs/2406.11385>`__ 

::

    Mon, 17 Jun 2024 10:12:45 GMT
    Yuyan Zhou, Liang Song, Bingning Wang and Weipeng Chen

The advent of large language models (LLMs) like GPT-4 has catalyzed the exploration of multi-task learning (MTL), in which a single model demonstrates proficiency across diverse tasks. Task arithmetic has emerged as a cost-effective approach for MTL. It enables performance enhancement across multiple tasks by adding their corresponding task vectors to a pre-trained model. However, the current lack of a method that can simultaneously achieve optimal performance, computational efficiency, and data privacy limits their application to LLMs. In this paper, we propose \textbf{M}odel \textbf{E}xclusive \textbf{T}ask \textbf{A}rithmetic for merging \textbf{GPT}-scale models, which formalizes the objective of model merging into a multi-task learning framework, aiming to minimize the average loss difference between the merged model and each individual task model. Since data privacy limits the use of multi-task training data, we leverage LLMs' local linearity and task vectors' orthogonality to separate the data term and scaling coefficients term and derive a model-exclusive task arithmetic method. Our proposed MetaGPT is data-agnostic and bypasses the heavy search process, making it cost-effective and easy to implement for LLMs.Extensive experiments demonstrate that MetaGPT leads to improvements in task arithmetic and achieves state-of-the-art performance on multiple tasks.

------------

`[2406.11400] Large Language Models and Knowledge Graphs for Astronomical Entity Disambiguation <https://arxiv.org/abs/2406.11400>`__ 面向天文实体消歧的大型语言模型和知识图谱

::

    Mon, 17 Jun 2024 10:38:03 GMT
    Golnaz Shapurian

This paper presents an experiment conducted during a hackathon, focusing on using large language models (LLMs) and knowledge graph clustering to extract entities and relationships from astronomical text. The study demonstrates an approach to disambiguate entities that can appear in various contexts within the astronomical domain. By collecting excerpts around specific entities and leveraging the GPT-4 language model, relevant entities and relationships are extracted. The extracted information is then used to construct a knowledge graph, which is clustered using the Leiden algorithm. The resulting Leiden communities are utilized to identify the percentage of association of unknown excerpts to each community, thereby enabling disambiguation. The experiment showcases the potential of combining LLMs and knowledge graph clustering techniques for information extraction in astronomical research. The results highlight the effectiveness of the approach in identifying and disambiguating entities, as well as grouping them into meaningful clusters based on their relationships.

------------

`[2406.11410] HARE: HumAn pRiors, a key to small language model Efficiency <https://arxiv.org/abs/2406.11410>`__ HARE:人类先验，小语言模型效率的关键

::

    Mon, 17 Jun 2024 10:56:03 GMT
    Lingyun Zhang, Bin jin, Gaojian Ge, Lunhui Liu, Xuewen Shen, Mingyong Wu, Houqian Zhang, Yongneng Jiang, Shiqi Chen, Shi Pu

Human priors play a crucial role in efficiently utilizing data in deep learning. However, with the development of large language models (LLMs), there is an increasing emphasis on scaling both model size and data volume, which often diminishes the importance of human priors in data construction.
Influenced by these trends, existing Small Language Models (SLMs) mainly rely on web-scraped large-scale training data, neglecting the proper incorporation of human priors. This oversight limits the training efficiency of language models in resource-constrained settings. In this paper, we propose a principle to leverage human priors for data construction. This principle emphasizes achieving high-performance SLMs by training on a concise dataset that accommodates both semantic diversity and data quality consistency, while avoiding benchmark data leakage. Following this principle, we train an SLM named HARE-1.1B. Extensive experiments on large-scale benchmark datasets demonstrate that HARE-1.1B performs favorably against state-of-the-art SLMs, validating the effectiveness of the proposed principle. Additionally, this provides new insights into efficient language model training in resource-constrained environments from the view of human priors.

------------

`[2406.11431] Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization <https://arxiv.org/abs/2406.11431>`__ 超级(官方)对齐:强模型可能在弱到强泛化中欺骗弱模型

::

    Mon, 17 Jun 2024 11:36:39 GMT
    Wenkai Yang, Shiqi Shen, Guangyao Shen, Zhi Gong, Yankai Lin

Superalignment, where humans are weak supervisors of superhuman models, has become an important and widely discussed issue in the current era of rapid development of Large Language Models (LLMs). The recent work preliminarily studies this problem by using weak models to supervise strong models. It discovers that weakly supervised strong students can consistently outperform weak teachers towards the alignment target, leading to a weak-to-strong generalization phenomenon. However, we are concerned that behind such a promising phenomenon, whether there exists an issue of weak-to-strong deception, where strong models may deceive weak models by exhibiting well-aligned in areas known to weak models but producing misaligned behaviors in cases weak models do not know. We then take an initial step towards exploring this security issue in a specific but realistic multi-objective alignment case, where there may be some alignment targets conflicting with each other (e.g., helpfulness v.s. harmlessness). Such a conflict is likely to cause strong models to deceive weak models in one alignment dimension to gain high reward in other alignment dimension. Our experiments on both the reward modeling task and the preference optimization scenario indicate: (1) the weak-to-strong deception exists; (2) the deception phenomenon may intensify as the capability gap between weak and strong models increases. We also discuss potential solutions and find bootstrapping with an intermediate model can mitigate the deception to some extent. Our work highlights the urgent need to pay more attention to the true reliability of superalignment.

------------

`[2406.11455] Adaptive Reinforcement Learning Planning: Harnessing Large Language Models for Complex Information Extraction <https://arxiv.org/abs/2406.11455>`__ 自适应强化学习规划:利用大型语言模型进行复杂信息提取

::

    Mon, 17 Jun 2024 12:11:01 GMT
    Zepeng Ding, Ruiyang Ke, Wenhao Huang, Guochao Jiang, Yanda Li, Deqing Yang, Yanghua Xiao, Jiaqing Liang

Existing research on large language models (LLMs) shows that they can solve information extraction tasks through multi-step planning. However, their extraction behavior on complex sentences and tasks is unstable, emerging issues such as false positives and missing elements. We observe that decomposing complex extraction tasks and extracting them step by step can effectively improve LLMs' performance, and the extraction orders of entities significantly affect the final results of LLMs. This paper proposes a two-stage multi-step method for LLM-based information extraction and adopts the RL framework to execute the multi-step planning. We regard sequential extraction as a Markov decision process, build an LLM-based extraction environment, design a decision module to adaptively provide the optimal order for sequential entity extraction on different sentences, and utilize the DDQN algorithm to train the decision model. We also design the rewards and evaluation metrics suitable for the extraction results of LLMs. We conduct extensive experiments on multiple public datasets to demonstrate the effectiveness of our method in improving the information extraction capabilities of LLMs.

------------

`[2406.11473] Promises, Outlooks and Challenges of Diffusion Language Modeling <https://arxiv.org/abs/2406.11473>`__ 扩散语言建模的前景、展望与挑战

::

    Mon, 17 Jun 2024 12:38:38 GMT
    Justin Deschenaux, Caglar Gulcehre

The modern autoregressive Large Language Models (LLMs) have achieved outstanding performance on NLP benchmarks, and they are deployed in the real world. However, they still suffer from limitations of the autoregressive training paradigm. For example, autoregressive token generation is notably slow and can be prone to \textit{exposure bias}. The diffusion-based language models were proposed as an alternative to autoregressive generation to address some of these limitations. We evaluate the recently proposed Score Entropy Discrete Diffusion (SEDD) approach and show it is a promising alternative to autoregressive generation but it has some short-comings too. We empirically demonstrate the advantages and challenges of SEDD, and observe that SEDD generally matches autoregressive models in perplexity and on benchmarks such as HellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference latency, SEDD can be up to 4.5$\times$ more efficient than GPT-2. While SEDD allows conditioning on tokens at abitrary positions, SEDD appears slightly weaker than GPT-2 for conditional generation given short prompts. Finally, we reproduced the main results from the original SEDD paper.

------------

`[2406.11474] How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment <https://arxiv.org/abs/2406.11474>`__ 上下文对齐能走多远?探索语境内对齐的状态

::

    Mon, 17 Jun 2024 12:38:48 GMT
    Heyan Huang, Yinghao Li, Huashan Sun, Yu Bai, Yang Gao

Recent studies have demonstrated that In-Context Learning (ICL), through the use of specific demonstrations, can align Large Language Models (LLMs) with human preferences known as In-Context Alignment (ICA), indicating that models can comprehend human instructions without requiring parameter adjustments.
However, the exploration of the mechanism and applicability of ICA remains limited. In this paper, we begin by dividing the context text used in ICA into three categories: format, system prompt, and example. Through ablation experiments, we investigate the effectiveness of each part in enabling ICA to function effectively. We then examine how variants in these parts impact the model's alignment performance. Our findings indicate that the example part is crucial for enhancing the model's alignment capabilities, with changes in examples significantly affecting alignment performance. We also conduct a comprehensive evaluation of ICA's zero-shot capabilities in various alignment tasks. The results indicate that compared to parameter fine-tuning methods, ICA demonstrates superior performance in knowledge-based tasks and tool-use tasks.
However, it still exhibits certain limitations in areas such as multi-turn dialogues and instruction following.

------------

`[2406.11477] Vocabulary Expansion for Low-resource Cross-lingual Transfer <https://arxiv.org/abs/2406.11477>`__ 

::

    Mon, 17 Jun 2024 12:42:34 GMT
    Atsuki Yamaguchi, Aline Villavicencio, Nikolaos Aletras

Large language models (LLMs) have shown remarkable capabilities in many languages beyond English. Yet, LLMs require more inference steps when generating non-English text due to their reliance on English-centric tokenizers, vocabulary, and pre-training data, resulting in higher usage costs to non-English speakers. Vocabulary expansion with target language tokens is a widely used cross-lingual vocabulary adaptation approach to remedy this issue.
Despite its effectiveness in inference speedup, the majority of previous work has focused on high-resource settings assuming access to a substantial amount of target language data to effectively initialize the embeddings of the new tokens and adapt the LLM to the target language. However, vocabulary expansion for LLMs in low-resource settings (i.e. languages and compute) has yet to be explored. In this paper, we investigate sample-efficient adaptation strategies from different angles, including target vocabulary size and initialization methods, and the amount of target data available for adaptation. Extensive experiments across typologically diverse languages, tasks and models show that simpler heuristic-based embedding initialization is more efficient and robust to changes in target vocabulary size and adaptation data in low-resource settings, outperforming a popular random initialization and a more sophisticated state-of-the-art approach that relies on external data and model.

------------

`[2406.11486] Analysing zero-shot temporal relation extraction on clinical notes using temporal consistency <https://arxiv.org/abs/2406.11486>`__ 利用时序一致性分析临床病历的零样本时序关系抽取

::

    Mon, 17 Jun 2024 12:53:21 GMT
    Vasiliki Kougia, Anastasiia Sedova, Andreas Stephan, Klim Zaporojets, Benjamin Roth

This paper presents the first study for temporal relation extraction in a zero-shot setting focusing on biomedical text. We employ two types of prompts and five LLMs (GPT-3.5, Mixtral, Llama 2, Gemma, and PMC-LLaMA) to obtain responses about the temporal relations between two events. Our experiments demonstrate that LLMs struggle in the zero-shot setting performing worse than fine-tuned specialized models in terms of F1 score, showing that this is a challenging task for LLMs. We further contribute a novel comprehensive temporal analysis by calculating consistency scores for each LLM. Our findings reveal that LLMs face challenges in providing responses consistent to the temporal properties of uniqueness and transitivity. Moreover, we study the relation between the temporal consistency of an LLM and its accuracy and whether the latter can be improved by solving temporal inconsistencies. Our analysis shows that even when temporal consistency is achieved, the predictions can remain inaccurate.

------------

`[2406.11514] Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs <https://arxiv.org/abs/2406.11514>`__ 对llm消除幻觉的预设立场进行反事实辩论

::

    Mon, 17 Jun 2024 13:21:23 GMT
    Yi Fang, Moxin Li, Wenjie Wang, Hui Lin, Fuli Feng

Large Language Models (LLMs) excel in various natural language processing tasks but struggle with hallucination issues. Existing solutions have considered utilizing LLMs' inherent reasoning abilities to alleviate hallucination, such as self-correction and diverse sampling methods. However, these methods often overtrust LLMs' initial answers due to inherent biases. The key to alleviating this issue lies in overriding LLMs' inherent biases for answer inspection. To this end, we propose a CounterFactual Multi-Agent Debate (CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent biases by compelling LLMs to generate justifications for a predetermined answer's correctness. The LLMs with different predetermined stances are engaged with a skeptical critic for counterfactual debate on the rationality of generated justifications. Finally, the debate process is evaluated by a third-party judge to determine the final answer. Extensive experiments on four datasets of three tasks demonstrate the superiority of CFMAD over existing methods.

------------

`[2406.11565] Extrinsic Evaluation of Cultural Competence in Large Language Models <https://arxiv.org/abs/2406.11565>`__ 大型语言模型文化能力的外在评估

::

    Mon, 17 Jun 2024 14:03:27 GMT
    Shaily Bhatt and Fernando Diaz

Productive interactions between diverse users and language technologies require outputs from the latter to be culturally relevant and sensitive. Prior works have evaluated models' knowledge of cultural norms, values, and artifacts, without considering how this knowledge manifests in downstream applications. In this work, we focus on extrinsic evaluation of cultural competence in two text generation tasks, open-ended question answering and story generation. We quantitatively and qualitatively evaluate model outputs when an explicit cue of culture, specifically nationality, is perturbed in the prompts. Although we find that model outputs do vary when varying nationalities and feature culturally relevant words, we also find weak correlations between text similarity of outputs for different countries and the cultural values of these countries. Finally, we discuss important considerations in designing comprehensive evaluation of cultural competence in user-facing tasks.

------------

`[2406.11566] MEMLA: Enhancing Multilingual Knowledge Editing with Neuron-Masked Low-Rank Adaptation <https://arxiv.org/abs/2406.11566>`__ MEMLA:基于神经元掩码低秩自适应的多语言知识编辑增强

::

    Mon, 17 Jun 2024 14:03:50 GMT
    Jiakuan Xie, Pengfei Cao, Yuheng Chen, Yubo Chen, Kang Liu, Jun Zhao

Knowledge editing aims to adjust the knowledge within large language models (LLMs) to prevent their responses from becoming obsolete or inaccurate.
However, existing works on knowledge editing are primarily conducted in a single language, which is inadequate for multilingual language models. In this paper, we focus on multilingual knowledge editing (MKE), which requires propagating updates across multiple languages. This necessity poses a significant challenge for the task. Furthermore, the limited availability of a comprehensive dataset for MKE exacerbates this challenge, hindering progress in this area. Hence, we introduce the Multilingual Knowledge Editing Benchmark (MKEB), a novel dataset comprising 12 languages and providing a complete evaluation framework. Additionally, we propose a method that enhances Multilingual knowledge Editing with neuron-Masked Low-Rank Adaptation (MEMLA).
Specifically, we identify two categories of knowledge neurons to improve editing precision. Moreover, we perform LoRA-based editing with neuron masks to efficiently modify parameters and facilitate the propagation of updates across multiple languages. Experiments demonstrate that our method outperforms existing baselines and significantly enhances the multi-hop reasoning capability of the edited model, with minimal impact on its downstream task performance. The dataset and code will be made publicly available.

------------

`[2406.11614] Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces <https://arxiv.org/abs/2406.11614>`__ 

::

    Mon, 17 Jun 2024 15:00:35 GMT
    Yihuai Hong, Lei Yu, Shauli Ravfogel, Haiqin Yang, Mor Geva

The task of "unlearning" certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance for mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general methodology for eliciting directions in the parameter space (termed "concept vectors") that encode concrete concepts, and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors, while directly ablating these vectors demonstrably removes the associated knowledge from the LLMs and significantly reduces their susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parametric-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.

------------

`[2406.11622] Building Knowledge-Guided Lexica to Model Cultural Variation <https://arxiv.org/abs/2406.11622>`__ 构建知识引导的文化变异词典模型

::

    Mon, 17 Jun 2024 15:05:43 GMT
    Shreya Havaldar, Salvatore Giorgi, Sunny Rai, Thomas Talhelm, Sharath Chandra Guntuku, Lyle Ungar

Cultural variation exists between nations (e.g., the United States vs.
China), but also within regions (e.g., California vs. Texas, Los Angeles vs.
San Francisco). Measuring this regional cultural variation can illuminate how and why people think and behave differently. Historically, it has been difficult to computationally model cultural variation due to a lack of training data and scalability constraints. In this work, we introduce a new research problem for the NLP community: How do we measure variation in cultural constructs across regions using language? We then provide a scalable solution: building knowledge-guided lexica to model cultural variation, encouraging future work at the intersection of NLP and cultural understanding. We also highlight modern LLMs' failure to measure cultural variation or generate culturally varied language.

------------

`[2406.11651] A Two-dimensional Zero-shot Dialogue State Tracking Evaluation Method using GPT-4 <https://arxiv.org/abs/2406.11651>`__ 基于GPT-4的二维零样本对话状态跟踪评估方法

::

    Mon, 17 Jun 2024 15:32:17 GMT
    Ming Gu, Yan Yang

Dialogue state tracking (DST) is evaluated by exact matching methods, which rely on large amounts of labeled data and ignore semantic consistency, leading to over-evaluation. Currently, leveraging large language models (LLM) in evaluating natural language processing tasks has achieved promising results.
However, using LLM for DST evaluation is still under explored. In this paper, we propose a two-dimensional zero-shot evaluation method for DST using GPT-4, which divides the evaluation into two dimensions: accuracy and completeness.
Furthermore, we also design two manual reasoning paths in prompting to further improve the accuracy of evaluation. Experimental results show that our method achieves better performance compared to the baselines, and is consistent with traditional exact matching based methods.

------------

`[2406.11657] Can LLM be a Personalized Judge? <https://arxiv.org/abs/2406.11657>`__ LLM可以成为一个个性化的法官吗?

::

    Mon, 17 Jun 2024 15:41:30 GMT
    Yijiang River Dong, Tiancheng Hu, Nigel Collier

Ensuring that large language models (LLMs) reflect diverse user values and preferences is crucial as their user bases expand globally. It is therefore encouraging to see the growing interest in LLM personalization within the research community. However, current works often rely on the LLM-as-a-Judge approach for evaluation without thoroughly examining its validity. In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas. Our findings suggest that directly applying LLM-as-a-Personalized-Judge is less reliable than previously assumed, showing low and inconsistent agreement with human ground truth. The personas typically used are often overly simplistic, resulting in low predictive power. To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments. This adjustment leads to much higher agreement (above 80%) on high-certainty samples for binary tasks.
Through human evaluation, we find that the LLM-as-a-Personalized-Judge achieves comparable performance to third-party humans evaluation and even surpasses human performance on high-certainty samples. Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and scalable methods for evaluating LLM personalization.

------------

`[2406.11661] Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting <https://arxiv.org/abs/2406.11661>`__ 文化条件作用还是安慰剂?社会人口激励的有效性

::

    Mon, 17 Jun 2024 15:43:45 GMT
    Sagnik Mukherjee, Muhammad Farid Adilazuarda, Sunayana Sitaram, Kalika Bali, Alham Fikri Aji, Monojit Choudhury

Socio-demographic prompting is a commonly employed approach to study cultural biases in LLMs as well as for aligning models to certain cultures. In this paper, we systematically probe four LLMs (Llama 3, Mistral v0.2, GPT-3.5 Turbo and GPT-4) with prompts that are conditioned on culturally sensitive and non-sensitive cues, on datasets that are supposed to be culturally sensitive (EtiCor and CALI) or neutral (MMLU and ETHICS). We observe that all models except GPT-4 show significant variations in their responses on both kinds of datasets for both kinds of prompts, casting doubt on the robustness of the culturally-conditioned prompting as a method for eliciting cultural bias in models or as an alignment strategy. The work also calls rethinking the control experiment design to tease apart the cultural conditioning of responses from "placebo effect", i.e., random perturbations of model responses due to arbitrary tokens in the prompt.

------------

`[2406.11668] "Not Aligned" is Not "Malicious": Being Careful about Hallucinations of Large Language Models' Jailbreak <https://arxiv.org/abs/2406.11668>`__ “不对齐”不是“恶意”:小心大型语言模型越狱的幻觉

::

    Mon, 17 Jun 2024 15:51:01 GMT
    Lingrui Mei and Shenghua Liu and Yiwei Wang and Baolong Bi and Jiayi Mao and Xueqi Cheng

"Jailbreak" is a major safety concern of Large Language Models (LLMs), which occurs when malicious prompts lead LLMs to produce harmful outputs, raising issues about the reliability and safety of LLMs. Therefore, an effective evaluation of jailbreaks is very crucial to develop its mitigation strategies.
However, our research reveals that many jailbreaks identified by current evaluations may actually be hallucinations-erroneous outputs that are mistaken for genuine safety breaches. This finding suggests that some perceived vulnerabilities might not represent actual threats, indicating a need for more precise red teaming benchmarks. To address this problem, we propose the $\textbf{B}$enchmark for reli$\textbf{AB}$ilit$\textbf{Y}$ and jail$\textbf{B}$reak ha$\textbf{L}$l$\textbf{U}$cination $\textbf{E}$valuation (BabyBLUE). BabyBLUE introduces a specialized validation framework including various evaluators to enhance existing jailbreak benchmarks, ensuring outputs are useful malicious instructions. Additionally, BabyBLUE presents a new dataset as an augmentation to the existing red teaming benchmarks, specifically addressing hallucinations in jailbreaks, aiming to evaluate the true potential of jailbroken LLM outputs to cause harm to human society.

------------

`[2406.11674] Endor: Hardware-Friendly Sparse Format for Offloaded LLM Inference <https://arxiv.org/abs/2406.11674>`__ Endor:用于卸载LLM推理的硬件友好的稀疏格式

::

    Mon, 17 Jun 2024 15:55:08 GMT
    Donghyeon Joo, Ramyad Hadidi, Soheil Feizi, Bahar Asgari

The increasing size of large language models (LLMs) challenges their usage on resource-constrained platforms. For example, memory on modern GPUs is insufficient to hold LLMs that are hundreds of Gigabytes in size. Offloading is a popular method to escape this constraint by storing weights of an LLM model to host CPU memory and SSD, then loading each weight to GPU before every use.
In our case study of offloaded inference, we found that due to the low bandwidth between storage devices and GPU, the latency of transferring large model weights from its offloaded location to GPU memory becomes the critical bottleneck with actual compute taking nearly 0% of runtime. To effectively reduce the weight transfer latency, we propose a novel sparse format that compresses the unstructured sparse pattern of pruned LLM weights to non-zero values with high compression ratio and low decompression overhead. Endor achieves this by expressing the positions of non-zero elements with a bitmap.
Compared to offloaded inference using the popular Huggingface Accelerate, applying Endor accelerates OPT-66B by 1.70x and Llama2-70B by 1.78x. When direct weight transfer from SSD to GPU is leveraged, Endor achieves 2.25x speedup on OPT-66B and 2.37x speedup on Llama2-70B.

------------

`[2406.11682] Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack <https://arxiv.org/abs/2406.11682>`__ 从知识到越狱:一个知识点值得一次攻击

::

    Mon, 17 Jun 2024 15:59:59 GMT
    Shangqing Tu, Zhuoran Pan, Wenxuan Wang, Zhexin Zhang, Yuliang Sun, Jifan Yu, Hongning Wang, Lei Hou, Juanzi Li

Large language models (LLMs) have been increasingly applied to various domains, which triggers increasing concerns about LLMs' safety on specialized domains, e.g. medicine. However, testing the domain-specific safety of LLMs is challenging due to the lack of domain knowledge-driven attacks in existing benchmarks. To bridge this gap, we propose a new task, knowledge-to-jailbreak, which aims to generate jailbreaks from domain knowledge to evaluate the safety of LLMs when applied to those domains. We collect a large-scale dataset with 12,974 knowledge-jailbreak pairs and fine-tune a large language model as jailbreak-generator, to produce domain knowledge-specific jailbreaks.
Experiments on 13 domains and 8 target LLMs demonstrate the effectiveness of jailbreak-generator in generating jailbreaks that are both relevant to the given knowledge and harmful to the target LLMs. We also apply our method to an out-of-domain knowledge base, showing that jailbreak-generator can generate jailbreaks that are comparable in harmfulness to those crafted by human experts. Data and code: https://github.com/THU-KEG/Knowledge-to-Jailbreak/.

------------

`[2406.11683] HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing <https://arxiv.org/abs/2406.11683>`__ 好莱坞:通过角色扮演释放大型语言模型在编剧中的创造力

::

    Mon, 17 Jun 2024 16:01:33 GMT
    Jing Chen, Xinyu Zhu, Cheng Yang, Chufan Shi, Yadong Xi, Yuxiang Zhang, Junjie Wang, Jiashu Pu, Rongsheng Zhang, Yujiu Yang, Tian Feng

Generative AI has demonstrated unprecedented creativity in the field of computer vision, yet such phenomena have not been observed in natural language processing. In particular, large language models (LLMs) can hardly produce written works at the level of human experts due to the extremely high complexity of literature writing. In this paper, we present HoLLMwood, an automated framework for unleashing the creativity of LLMs and exploring their potential in screenwriting, which is a highly demanding task. Mimicking the human creative process, we assign LLMs to different roles involved in the real-world scenario. In addition to the common practice of treating LLMs as ${Writer}$, we also apply LLMs as ${Editor}$, who is responsible for providing feedback and revision advice to ${Writer}$. Besides, to enrich the characters and deepen the plots, we introduce a role-playing mechanism and adopt LLMs as ${Actors}$ that can communicate and interact with each other. Evaluations on automatically generated screenplays show that HoLLMwood substantially outperforms strong baselines in terms of coherence, relevance, interestingness and overall quality.

------------

`[2406.11687] Tokenization Falling Short: The Curse of Tokenization <https://arxiv.org/abs/2406.11687>`__ 标记化不达标:标记化的诅咒

::

    Mon, 17 Jun 2024 16:05:32 GMT
    Yekun Chai, Yewei Fang, Qiwei Peng, Xuhong Li

Language models typically tokenize raw text into sequences of subword identifiers from a predefined vocabulary, a process inherently sensitive to typographical errors, length variations, and largely oblivious to the internal structure of tokens-issues we term the curse of tokenization. In this study, we delve into these drawbacks and demonstrate that large language models (LLMs) remain susceptible to these problems. This study systematically investigates these challenges and their impact on LLMs through three critical research questions: (1) complex problem solving, (2) token structure probing, and (3) resilience to typographical variation. Our findings reveal that scaling model parameters can mitigate the issue of tokenization; however, LLMs still suffer from biases induced by typos and other text format variations. Our experiments show that subword regularization such as BPE-dropout can mitigate this issue.
We will release our code and data to facilitate further research.

------------

`[2406.11709] Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging <https://arxiv.org/abs/2406.11709>`__ 指示，而不是辅助:基于llm的多轮规划和分层提问，用于苏格拉底式代码调试

::

    Mon, 17 Jun 2024 16:28:21 GMT
    Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani-Tur, Jiawei Han

Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student's conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes -- all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct's state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct's ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning.

------------

`[2406.11721] Zero-Shot Generalization during Instruction Tuning: Insights from Similarity and Granularity <https://arxiv.org/abs/2406.11721>`__ 

::

    Mon, 17 Jun 2024 16:40:21 GMT
    Bingxiang He, Ning Ding, Cheng Qian, Jia Deng, Ganqu Cui, Lifan Yuan, Huan-ang Gao, Huimin Chen, Zhiyuan Liu, Maosong Sun

Understanding alignment techniques begins with comprehending zero-shot generalization brought by instruction tuning, but little of the mechanism has been understood. Existing work has largely been confined to the task level, without considering that tasks are artificially defined and, to LLMs, merely consist of tokens and representations. This line of research has been limited to examining transfer between tasks from a task-pair perspective, with few studies focusing on understanding zero-shot generalization from the perspective of the data itself. To bridge this gap, we first demonstrate through multiple metrics that zero-shot generalization during instruction tuning happens very early. Next, we investigate the facilitation of zero-shot generalization from both data similarity and granularity perspectives, confirming that encountering highly similar and fine-grained training data earlier during instruction tuning, without the constraints of defined "tasks", enables better generalization. Finally, we propose a more grounded training data arrangement method, Test-centric Multi-turn Arrangement, and show its effectiveness in promoting continual learning and further loss reduction. For the first time, we show that zero-shot generalization during instruction tuning is a form of similarity-based generalization between training and test data at the instance level. We hope our analysis will advance the understanding of zero-shot generalization during instruction tuning and contribute to the development of more aligned LLMs. Our code is released at https://github.com/HBX-hbx/dynamics_of_zero-shot_generalization.

------------

`[2406.11736] Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models <https://arxiv.org/abs/2406.11736>`__ 交互式演化:面向大型语言模型的神经符号自训练框架

::

    Mon, 17 Jun 2024 16:52:56 GMT
    Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jun Liu, Yu Qiao, Zhiyong Wu

One of the primary driving forces contributing to the superior performance of Large Language Models (LLMs) is the extensive availability of human-annotated natural language data, which is used for alignment fine-tuning. This inspired researchers to investigate self-training methods to mitigate the extensive reliance on human annotations. However, the current success of self-training has been primarily observed in natural language scenarios, rather than in the increasingly important neural-symbolic scenarios. To this end, we propose an environment-guided neural-symbolic self-training framework named ENVISIONS. It aims to overcome two main challenges: (1) the scarcity of symbolic data, and (2) the limited proficiency of LLMs in processing symbolic language. Extensive evaluations conducted on three distinct domains demonstrate the effectiveness of our approach. Additionally, we have conducted a comprehensive analysis to uncover the factors contributing to ENVISIONS's success, thereby offering valuable insights for future research in this area. Code will be available at \url{https://github.com/xufangzhi/ENVISIONS}.

------------

`[2406.11785] CELL your Model: Contrastive Explanation Methods for Large Language Models <https://arxiv.org/abs/2406.11785>`__ CELL your Model:大型语言模型的对比解释方法

::

    Mon, 17 Jun 2024 17:39:10 GMT
    Ronny Luss, Erik Miehling, Amit Dhurandhar

The advent of black-box deep neural network classification models has sparked the need to explain their decisions. However, in the case of generative AI such as large language models (LLMs), there is no class prediction to explain.
Rather, one can ask why an LLM output a particular response to a given prompt.
In this paper, we answer this question by proposing, to the best of our knowledge, the first contrastive explanation methods requiring simply black-box/query access. Our explanations suggest that an LLM outputs a reply to a given prompt because if the prompt was slightly modified, the LLM would have given a different response that is either less preferable or contradicts the original response. The key insight is that contrastive explanations simply require a distance function that has meaning to the user and not necessarily a real valued representation of a specific response (viz. class label). We offer two algorithms for finding contrastive explanations: i) A myopic algorithm, which although effective in creating contrasts, requires many model calls and ii) A budgeted algorithm, our main algorithmic contribution, which intelligently creates contrasts adhering to a query budget, necessary for longer contexts. We show the efficacy of these methods on diverse natural language tasks such as open-text generation, automated red teaming, and explaining conversational degradation.

------------

`[2406.11801] Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations <https://arxiv.org/abs/2406.11801>`__ 安全算法:基于控制参数和激活的语言模型测试时安全对齐框架

::

    Mon, 17 Jun 2024 17:48:13 GMT
    Rima Hazra, Sayan Layek, Somnath Banerjee, Soujanya Poria

Ensuring the safe alignment of large language models (LLMs) with human values is critical as they become integral to applications like translation and question answering. Current alignment methods struggle with dynamic user intentions and complex objectives, making models vulnerable to generating harmful content. We propose Safety Arithmetic, a training-free framework enhancing LLM safety across different scenarios: Base models, Supervised fine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm Direction Removal to avoid harmful content and Safety Alignment to promote safe responses. Additionally, we present NoIntentEdit, a dataset highlighting edit instances that could compromise model safety if used unintentionally. Our experiments show that Safety Arithmetic significantly improves safety measures, reduces over-safety, and maintains model utility, outperforming existing methods in ensuring safe content generation.

------------

`[2406.11813] How Do Large Language Models Acquire Factual Knowledge During Pretraining? <https://arxiv.org/abs/2406.11813>`__ 

::

    Mon, 17 Jun 2024 17:54:40 GMT
    Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo

Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this gap by studying how LLMs acquire factual knowledge during pretraining. The findings reveal several important insights into the dynamics of factual knowledge acquisition during pretraining. First, counterintuitively, we observe that pretraining on more data shows no significant improvement in the model's capability to acquire and maintain factual knowledge. Next, there is a power-law relationship between training steps and forgetting of memorization and generalization of factual knowledge, and LLMs trained with duplicated training data exhibit faster forgetting. Third, training LLMs with larger batch sizes can enhance the models' robustness to forgetting. Overall, our observations suggest that factual knowledge acquisition in LLM pretraining occurs by progressively increasing the probability of factual knowledge presented in the pretraining data at each step. However, this increase is diluted by subsequent forgetting. Based on this interpretation, we demonstrate that we can provide plausible explanations for recently observed behaviors of LLMs, such as the poor performance of LLMs on long-tail knowledge and the benefits of deduplicating the pretraining corpus.

------------

`[2406.11817] Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level <https://arxiv.org/abs/2406.11817>`__ 迭代长度正则化的直接偏好优化:改进7B语言模型到GPT-4水平的案例研究

::

    Mon, 17 Jun 2024 17:55:38 GMT
    Jie Liu, Zhanhui Zhou, Jiaheng Liu, Xingyuan Bu, Chao Yang, Han-Sen Zhong, Wanli Ouyang

Direct Preference Optimization (DPO), a standard method for aligning language models with human preferences, is traditionally applied to offline preferences.
Recent studies show that DPO benefits from iterative training with online preferences labeled by a trained reward model. In this work, we identify a pitfall of vanilla iterative DPO - improved response quality can lead to increased verbosity. To address this, we introduce iterative length-regularized DPO (iLR-DPO) to penalize response length. Our empirical results show that iLR-DPO can enhance a 7B model to perform on par with GPT-4 without increasing verbosity. Specifically, our 7B model achieves a $50.5\%$ length-controlled win rate against $\texttt{GPT-4 Preview}$ on AlpacaEval 2.0, and excels across standard benchmarks including MT-Bench, Arena-Hard and OpenLLM Leaderboard.
These results demonstrate the effectiveness of iterative DPO in aligning language models with human feedback.

------------

`[2406.11827] WPO: Enhancing RLHF with Weighted Preference Optimization <https://arxiv.org/abs/2406.11827>`__ WPO:基于加权偏好优化的RLHF算法

::

    Mon, 17 Jun 2024 17:59:13 GMT
    Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, Chenguang Zhu

Reinforcement learning from human feedback (RLHF) is a promising solution to align large language models (LLMs) more closely with human values. Off-policy preference optimization, where the preference data is obtained from other models, is widely adopted due to its cost efficiency and scalability. However, off-policy preference optimization often suffers from a distributional gap between the policy used for data collection and the target policy, leading to suboptimal optimization. In this paper, we propose a novel strategy to mitigate this problem by simulating on-policy learning with off-policy preference data.
Our Weighted Preference Optimization (WPO) method adapts off-policy data to resemble on-policy data more closely by reweighting preference pairs according to their probability under the current policy. This method not only addresses the distributional gap problem but also enhances the optimization process without incurring additional costs. We validate our method on instruction following benchmarks including Alpaca Eval 2 and MT-bench. WPO not only outperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2 but also establishes a remarkable length-controlled winning rate against GPT-4-turbo of 48.6% based on Llama-3-8B-Instruct, making it the strongest 8B model on the leaderboard. We will release the code and models at https://github.com/wzhouad/WPO.

------------

`[2406.10366] Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework <https://arxiv.org/abs/2406.10366>`__ 使用Estimands框架提高AI/ML评估的有效性和实用价值

::

    Fri, 14 Jun 2024 18:47:37 GMT
    Olivier Binette and Jerome P. Reiter

Commonly, AI or machine learning (ML) models are evaluated on benchmark datasets. This practice supports innovative methodological research, but benchmark performance can be poorly correlated with performance in real-world applications -- a construct validity issue. To improve the validity and practical usefulness of evaluations, we propose using an estimands framework adapted from international clinical trials guidelines. This framework provides a systematic structure for inference and reporting in evaluations, emphasizing the importance of a well-defined estimation target. We illustrate our proposal on examples of commonly used evaluation methodologies - involving cross-validation, clustering evaluation, and LLM benchmarking - that can lead to incorrect rankings of competing models (rank reversals) with high probability, even when performance differences are large. We demonstrate how the estimands framework can help uncover underlying issues, their causes, and potential solutions. Ultimately, we believe this framework can improve the validity of evaluations through better-aligned inference, and help decision-makers and model users interpret reported results more effectively.

------------

`[2406.10576] Optimization-based Structural Pruning for Large Language Models without Back-Propagation <https://arxiv.org/abs/2406.10576>`__ 基于优化的无反向传播大型语言模型结构剪枝

::

    Sat, 15 Jun 2024 09:31:03 GMT
    Yuan Gao, Zujing Liu, Weizhong Zhang, Bo Du, Gui-Song Xia

Compared to the moderate size of neural network models, structural weight pruning on the Large-Language Models (LLMs) imposes a novel challenge on the efficiency of the pruning algorithms, due to the heavy computation/memory demands of the LLMs. Recent efficient LLM pruning methods typically operate at the post-training phase without the expensive weight finetuning, however, their pruning criteria often rely on heuristically designed metrics, potentially leading to suboptimal performance. We instead propose a novel optimization-based structural pruning that learns the pruning masks in a probabilistic space directly by optimizing the loss of the pruned model. To preserve the efficiency, our method 1) works at post-training phase} and 2) eliminates the back-propagation through the LLM per se during the optimization (i.e., only requires the forward pass of the LLM). We achieve this by learning an underlying Bernoulli distribution to sample binary pruning masks, where we decouple the Bernoulli parameters from the LLM loss, thus facilitating an efficient optimization via a policy gradient estimator without back-propagation. As a result, our method is able to 1) operate at structural granularities of channels, heads, and layers, 2) support global and heterogeneous pruning (i.e., our method automatically determines different redundancy for different layers), and 3) optionally use a metric-based method as initialization (of our Bernoulli distributions). Extensive experiments on LLaMA, LLaMA-2, and Vicuna using the C4 and WikiText2 datasets demonstrate that our method operates for 2.7 hours with around 35GB memory for the 13B models on a single A100 GPU, and our pruned models outperform the state-of-the-arts w.r.t. perplexity. Codes will be released.

------------

`[2406.10903] New Solutions on LLM Acceleration, Optimization, and Application <https://arxiv.org/abs/2406.10903>`__ LLM加速、优化和应用的新解决方案

::

    Sun, 16 Jun 2024 11:56:50 GMT
    Yingbing Huang, Lily Jiaxin Wan, Hanchen Ye, Manvi Jha, Jinghua Wang, Yuhong Li, Xiaofan Zhang, Deming Chen

Large Language Models (LLMs) have become extremely potent instruments with exceptional capacities for comprehending and producing human-like text in a wide range of applications. However, the increasing size and complexity of LLMs present significant challenges in both training and deployment, leading to substantial computational and storage costs as well as heightened energy consumption. In this paper, we provide a review of recent advancements and research directions aimed at addressing these challenges and enhancing the efficiency of LLM-based systems. We begin by discussing algorithm-level acceleration techniques focused on optimizing LLM inference speed and resource utilization. We also explore LLM-hardware co-design strategies with a vision to improve system efficiency by tailoring hardware architectures to LLM requirements. Further, we delve into LLM-to-accelerator compilation approaches, which involve customizing hardware accelerators for efficient LLM deployment.
Finally, as a case study to leverage LLMs for assisting circuit design, we examine LLM-aided design methodologies for an important task: High-Level Synthesis (HLS) functional verification, by creating a new dataset that contains a large number of buggy and bug-free codes, which can be essential for training LLMs to specialize on HLS verification and debugging. For each aspect mentioned above, we begin with a detailed background study, followed by the presentation of several novel solutions proposed to overcome specific challenges. We then outline future research directions to drive further advancements. Through these efforts, we aim to pave the way for more efficient and scalable deployment of LLMs across a diverse range of applications.

------------

`[2406.10918] Embodied Question Answering via Multi-LLM Systems <https://arxiv.org/abs/2406.10918>`__ 基于多llm系统的具身问答

::

    Sun, 16 Jun 2024 12:46:40 GMT
    Bhrij Patel, Vishnu Sashank Dorbala, Amrit Singh Bedi

Embodied Question Answering (EQA) is an important problem, which involves an agent exploring the environment to answer user queries. In the existing literature, EQA has exclusively been studied in single-agent scenarios, where exploration can be time-consuming and costly. In this work, we consider EQA in a multi-agent framework involving multiple large language models (LLM) based agents independently answering queries about a household environment. To generate one answer for each query, we use the individual responses to train a Central Answer Model (CAM) that aggregates responses for a robust answer. Using CAM, we observe a $50\%$ higher EQA accuracy when compared against aggregation methods for ensemble LLM, such as voting schemes and debates. CAM does not require any form of agent communication, alleviating it from the associated costs. We ablate CAM with various nonlinear (neural network, random forest, decision tree, XGBoost) and linear (logistic regression classifier, SVM) algorithms. Finally, we present a feature importance analysis for CAM via permutation feature importance (PFI), quantifying CAMs reliance on each independent agent and query context.

------------

`[2406.10976] Promoting Data and Model Privacy in Federated Learning through Quantized LoRA <https://arxiv.org/abs/2406.10976>`__ 通过量化LoRA促进联邦学习中的数据和模型隐私

::

    Sun, 16 Jun 2024 15:23:07 GMT
    JianHao Zhu, Changze Lv, Xiaohua Wang, Muling Wu, Wenhao Liu, Tianlong Li, Zixuan Ling, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang

Conventional federated learning primarily aims to secure the privacy of data distributed across multiple edge devices, with the global model dispatched to edge devices for parameter updates during the learning process. However, the development of large language models (LLMs) requires substantial data and computational resources, rendering them valuable intellectual properties for their developers and owners. To establish a mechanism that protects both data and model privacy in a federated learning context, we introduce a method that just needs to distribute a quantized version of the model's parameters during training. This method enables accurate gradient estimations for parameter updates while preventing clients from accessing a model whose performance is comparable to the centrally hosted one. Moreover, we combine this quantization strategy with LoRA, a popular and parameter-efficient fine-tuning method, to significantly reduce communication costs in federated learning. The proposed framework, named \textsc{FedLPP}, successfully ensures both data and model privacy in the federated learning context. Additionally, the learned central model exhibits good generalization and can be trained in a resource-efficient manner.

------------

`[2406.11187] Save It All: Enabling Full Parameter Tuning for Federated Large Language Models via Cycle Black Gradient Descent <https://arxiv.org/abs/2406.11187>`__ 全部保存:通过循环黑色梯度下降实现联邦大型语言模型的全参数调优

::

    Mon, 17 Jun 2024 03:49:44 GMT
    Lin Wang, Zhichao Wang, Xiaoying Tang

The advent of large language models (LLMs) has revolutionized the deep learning paradigm, yielding impressive results across a wide array of tasks.
However, the pre-training or fine-tuning of LLMs within a federated learning (FL) framework poses substantial challenges, including considerable computational and memory resource demands, as well as communication bottlenecks between servers and clients. Existing solutions either make the unrealistic assumption that the entire model is exchanged for training, or apply parameter-effective fine-tuning methods from centralized learning to train LLMs in FL which tend to underperform during training or fine-tuning stages due to the limited search subspace of parameter updating. In this paper, we introduce a novel method for the efficient training and fine-tuning of LLMs in FL, with minimal resource consumption. Our approach, termed FedCyBGD, utilizes Cycle Block Gradient Descent to periodically update the model. In particular, we design a compression scheme for FedCyBGD, aiming to further decrease the model download cost. It enables full parameter training in FL with only selected block updates and uploads, thereby reducing communication, computation, and memory costs. Our method achieves state-of-the-art performance for FL LLM training, while significantly reducing associated costs. Codes are provided here.

------------

`[2406.11235] QTIP: Quantization with Trellises and Incoherence Processing <https://arxiv.org/abs/2406.11235>`__ 

::

    Mon, 17 Jun 2024 06:03:13 GMT
    Albert Tseng, Qingyao Sun, David Hou, Christopher De Sa

Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing weights to low-precision datatypes. Since LLM inference is usually memory-bound, PTQ methods can improve inference throughput. Recent state-of-the-art PTQ approaches have converged on using vector quantization (VQ) to quantize multiple weights at once, which improves information utilization through better shaping. However, VQ requires a codebook with size exponential in the dimension. This limits current VQ-based PTQ works to low VQ dimensions ($\le 8$) that in turn limit quantization quality. Here, we introduce QTIP, which instead uses trellis coded quantization (TCQ) to achieve ultra-high-dimensional quantization. TCQ uses a stateful decoder that separates the codebook size from the bitrate and effective dimension. QTIP introduces a spectrum of lookup-only to computed lookup-free trellis codes designed for a hardware-efficient "bitshift" trellis structure; these codes achieve state-of-the-art results in both quantization quality and inference speed.

------------

`[2406.11257] ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking <https://arxiv.org/abs/2406.11257>`__ ExCP:基于重量动量关节收缩的极限LLM检查点压缩

::

    Mon, 17 Jun 2024 06:47:29 GMT
    Wenshuo Li, Xinghao Chen, Han Shu, Yehui Tang, Yunhe Wang

Large language models (LLM) have recently attracted significant attention in the field of artificial intelligence. However, the training process of these models poses significant challenges in terms of computational and storage capacities, thus compressing checkpoints has become an urgent problem. In this paper, we propose a novel Extreme Checkpoint Compression (ExCP) framework, which significantly reduces the required storage of training checkpoints while achieving nearly lossless performance. We first calculate the residuals of adjacent checkpoints to obtain the essential but sparse information for higher compression ratio. To further excavate the redundancy parameters in checkpoints, we then propose a weight-momentum joint shrinking method to utilize another important information during the model optimization, i.e., momentum. In particular, we exploit the information of both model and optimizer to discard as many parameters as possible while preserving critical information to ensure optimal performance. Furthermore, we utilize non-uniform quantization to further compress the storage of checkpoints. We extensively evaluate our proposed ExCP framework on several models ranging from 410M to 7B parameters and demonstrate significant storage reduction while maintaining strong performance. For instance, we achieve approximately $70\times$ compression for the Pythia-410M model, with the final performance being as accurate as the original model on various downstream tasks. Codes will be available at https://github.com/Gaffey/ExCP.

------------

`[2406.11353] $\texttt{MoE-RBench}$: Towards Building Reliable Language Models with Sparse Mixture-of-Experts <https://arxiv.org/abs/2406.11353>`__ $\texttt{MoE-RBench}$基于稀疏专家混合构建可靠的语言模型

::

    Mon, 17 Jun 2024 09:17:05 GMT
    Guanjie Chen, Xinyu Zhao, Tianlong Chen and Yu Cheng

Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new domains such as in fine-tuning MoE models sometimes underperform their dense counterparts. Motivated by the research gap and counter-intuitive phenomenon, we propose $\texttt{MoE-RBench}$, the first comprehensive assessment of SMoE reliability from three aspects: $\textit{(i)}$ safety and hallucination, $\textit{(ii)}$ resilience to adversarial attacks, and $\textit{(iii)}$ out-of-distribution robustness. Extensive models and datasets are tested to compare the MoE to dense networks from these reliability dimensions. Our empirical observations suggest that with appropriate hyperparameters, training recipes, and inference techniques, we can build the MoE model more reliably than the dense LLM. In particular, we find that the robustness of SMoE is sensitive to the basic training settings. We hope that this study can provide deeper insights into how to adapt the pre-trained MoE model to other tasks with higher-generation security, quality, and stability.
Codes are available at https://github.com/UNITES-Lab/MoE-RBench

------------

`[2406.11391] P-TA: Using Proximal Policy Optimization to Enhance Tabular Data Augmentation via Large Language Models <https://arxiv.org/abs/2406.11391>`__ P-TA:利用近端策略优化通过大型语言模型增强表格数据扩充

::

    Mon, 17 Jun 2024 10:22:00 GMT
    Shuo Yang, Chenchen Yuan, Yao Rong, Felix Steinbauer and Gjergji Kasneci

A multitude of industries depend on accurate and reasonable tabular data augmentation for their business processes. Contemporary methodologies in generating tabular data revolve around utilizing Generative Adversarial Networks (GAN) or fine-tuning Large Language Models (LLM). However, GAN-based approaches are documented to produce samples with common-sense errors attributed to the absence of external knowledge. On the other hand, LLM-based methods exhibit a limited capacity to capture the disparities between synthesized and actual data distribution due to the absence of feedback from a discriminator during training. Furthermore, the decoding of LLM-based generation introduces gradient breakpoints, impeding the backpropagation of loss from a discriminator, thereby complicating the integration of these two approaches. To solve this challenge, we propose using proximal policy optimization (PPO) to apply GANs, guiding LLMs to enhance the probability distribution of tabular features. This approach enables the utilization of LLMs as generators for GANs in synthesizing tabular data. Our experiments demonstrate that PPO leads to an approximately 4\% improvement in the accuracy of models trained on synthetically generated data over state-of-the-art across three real-world datasets.

------------

`[2406.11569] Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs <https://arxiv.org/abs/2406.11569>`__ 基于空中联邦元学习的预训练和个性化微调:收敛-泛化权衡

::

    Mon, 17 Jun 2024 14:06:13 GMT
    Haifeng Wen, Hong Xing, Osvaldo Simeone

For modern artificial intelligence (AI) applications such as large language models (LLMs), the training paradigm has recently shifted to pre-training followed by fine-tuning. Furthermore, owing to dwindling open repositories of data and thanks to efforts to democratize access to AI models, pre-training is expected to increasingly migrate from the current centralized deployments to federated learning (FL) implementations. Meta-learning provides a general framework in which pre-training and fine-tuning can be formalized.
Meta-learning-based personalized FL (meta-pFL) moves beyond basic personalization by targeting generalization to new agents and tasks. This paper studies the generalization performance of meta-pFL for a wireless setting in which the agents participating in the pre-training phase, i.e., meta-learning, are connected via a shared wireless channel to the server. Adopting over-the-air computing, we study the trade-off between generalization to new agents and tasks, on the one hand, and convergence, on the other hand. The trade-off arises from the fact that channel impairments may enhance generalization, while degrading convergence. Extensive numerical results validate the theory.

------------

`[2406.11675] BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models <https://arxiv.org/abs/2406.11675>`__ BLoB:基于反向传播的大型语言模型贝叶斯低秩自适应

::

    Mon, 17 Jun 2024 15:55:38 GMT
    Yibin Wang, Haizhou Shi, Ligong Han, Dimitris Metaxas, Hao Wang

Large Language Models (LLMs) often suffer from overconfidence during inference, particularly when adapted to downstream domain-specific tasks with limited data. Previous work addresses this issue by employing approximate Bayesian estimation after the LLMs are trained, enabling them to quantify uncertainty. However, such post-training approaches' performance is severely limited by the parameters learned during training. In this paper, we go beyond post-training Bayesianization and propose Bayesian Low-Rank Adaptation by Backpropagation (BLoB), an algorithm that continuously and jointly adjusts both the mean and covariance of LLM parameters throughout the whole fine-tuning process. Our empirical results verify the effectiveness of BLoB in terms of generalization and uncertainty estimation, when evaluated on both in-distribution and out-of-distribution data.

------------

`[2406.11686] The Role of Inherent Bellman Error in Offline Reinforcement Learning with Linear Function Approximation <https://arxiv.org/abs/2406.11686>`__ 固有Bellman误差在线性函数逼近的离线强化学习中的作用

::

    Mon, 17 Jun 2024 16:04:06 GMT
    Noah Golowich and Ankur Moitra

In this paper, we study the offline RL problem with linear function approximation. Our main structural assumption is that the MDP has low inherent Bellman error, which stipulates that linear value functions have linear Bellman backups with respect to the greedy policy. This assumption is natural in that it is essentially the minimal assumption required for value iteration to succeed. We give a computationally efficient algorithm which succeeds under a single-policy coverage condition on the dataset, namely which outputs a policy whose value is at least that of any policy which is well-covered by the dataset. Even in the setting when the inherent Bellman error is 0 (termed linear Bellman completeness), our algorithm yields the first known guarantee under single-policy coverage.
In the setting of positive inherent Bellman error ${\varepsilon_{\mathrm{BE}}} > 0$, we show that the suboptimality error of our algorithm scales with $\sqrt{\varepsilon_{\mathrm{BE}}}$. Furthermore, we prove that the scaling of the suboptimality with $\sqrt{\varepsilon_{\mathrm{BE}}}$ cannot be improved for any algorithm. Our lower bound stands in contrast to many other settings in reinforcement learning with misspecification, where one can typically obtain performance that degrades linearly with the misspecification error.

------------

`[2406.11715] Measuring memorization in RLHF for code completion <https://arxiv.org/abs/2406.11715>`__ 基于RLHF的代码补全记忆性度量

::

    Mon, 17 Jun 2024 16:33:35 GMT
    Aneesh Pappu, Billy Porter, Ilia Shumailov, Jamie Hayes

Reinforcement learning with human feedback (RLHF) has become the dominant method to align large models to user preferences. Unlike fine-tuning, for which there are many studies regarding training data memorization, it is not clear how memorization is affected by or introduced in the RLHF alignment process.
Understanding this relationship is important as real user data may be collected and used to align large models; if user data is memorized during RLHF and later regurgitated, this could raise privacy concerns. In this work, we analyze how training data memorization can surface and propagate through each phase of RLHF. We focus our study on code completion models, as code completion is one of the most popular use cases for large language models. We find that RLHF significantly decreases the chance that data used for reward modeling and reinforcement learning is memorized, in comparison to aligning via directly fine-tuning on this data, but that examples already memorized during the fine-tuning stage of RLHF, will, in the majority of cases, remain memorized after RLHF.

------------

`[2406.11717] Refusal in Language Models Is Mediated by a Single Direction <https://arxiv.org/abs/2406.11717>`__ 语言模型中的拒绝是由单一方向调节的

::

    Mon, 17 Jun 2024 16:36:12 GMT
    Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, Neel Nanda

Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.

------------

`[2406.10279] We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs <https://arxiv.org/abs/2406.10279>`__ 我们有你的包裹!通过代码生成LLMs全面分析包幻觉

::

    Wed, 12 Jun 2024 03:29:06 GMT
    Joseph Spracklen, Raveen Wijewickrama, A H M Nazmus Sakib, Anindya Maiti, Murtuza Jadliwala

The reliance of popular programming languages such as Python and JavaScript on centralized package repositories and open-source software, combined with the emergence of code-generating Large Language Models (LLMs), has created a new type of threat to the software supply chain: package hallucinations. These hallucinations, which arise from fact-conflicting errors when generating code using LLMs, represent a novel form of package confusion attack that poses a critical threat to the integrity of the software supply chain. This paper conducts a rigorous and comprehensive evaluation of package hallucinations across different programming languages, settings, and parameters, exploring how different configurations of LLMs affect the likelihood of generating erroneous package recommendations and identifying the root causes of this phenomena.
Using 16 different popular code generation models, across two programming languages and two unique prompt datasets, we collect 576,000 code samples which we analyze for package hallucinations. Our findings reveal that 19.7% of generated packages across all the tested LLMs are hallucinated, including a staggering 205,474 unique examples of hallucinated package names, further underscoring the severity and pervasiveness of this threat. We also implemented and evaluated mitigation strategies based on Retrieval Augmented Generation (RAG), self-detected feedback, and supervised fine-tuning. These techniques demonstrably reduced package hallucinations, with hallucination rates for one model dropping below 3%. While the mitigation efforts were effective in reducing hallucination rates, our study reveals that package hallucinations are a systemic and persistent phenomenon that pose a significant challenge for code generating LLMs.

------------

`[2406.10305] Unlock the Correlation between Supervised Fine-Tuning and Reinforcement Learning in Training Code Large Language Models <https://arxiv.org/abs/2406.10305>`__ 解锁在训练代码大型语言模型中监督微调和强化学习之间的相关性

::

    Fri, 14 Jun 2024 03:39:01 GMT
    Jie Chen, Xintian Han, Yu Ma, Xun Zhou, Liang Xiang

Automatic code generation has been a longstanding research topic. With the advancement of general-purpose large language models (LLMs), the ability to code stands out as one important measure to the model's reasoning performance.
Usually, a two-stage training paradigm is implemented to obtain a Code LLM, namely the pretraining and the fine-tuning. Within the fine-tuning, supervised fine-tuning (SFT), and reinforcement learning (RL) are often used to improve the model's zero-shot ability. A large number of work has been conducted to improve the model's performance on code-related benchmarks with either modifications to the algorithm or refinement of the dataset. However, we still lack a deep insight into the correlation between SFT and RL. For instance, what kind of dataset should be used to ensure generalization, or what if we abandon the SFT phase in fine-tuning. In this work, we make an attempt to understand the correlation between SFT and RL. To facilitate our research, we manually craft 100 basis python functions, called atomic functions, and then a synthesizing pipeline is deployed to create a large number of synthetic functions on top of the atomic ones. In this manner, we ensure that the train and test sets remain distinct, preventing data contamination. Through comprehensive ablation study, we find: (1) Both atomic and synthetic functions are indispensable for SFT's generalization, and only a handful of synthetic functions are adequate; (2) Through RL, the SFT's generalization to target domain can be greatly enhanced, even with the same training prompts; (3) Training RL from scratch can alleviate the over-fitting issue introduced in the SFT phase.

------------

`[2406.10320] Out of style: Misadventures with LLMs and code style transfer <https://arxiv.org/abs/2406.10320>`__ 过时:llm和代码风格转换的错误

::

    Fri, 14 Jun 2024 17:04:56 GMT
    Karl Munson, Chih-Kai Ting, Serenity Wade, Anish Savla, Julian Dolby, Kiran Kate and Kavitha Srinivas

Like text, programs have styles, and certain programming styles are more desirable than others for program readability, maintainability, and performance. Code style transfer, however, is difficult to automate except for trivial style guidelines such as limits on line length. Inspired by the success of using language models for text style transfer, we investigate if code language models can perform code style transfer. Code style transfer, unlike text transfer, has rigorous requirements: the system needs to identify lines of code to change, change them correctly, and leave the rest of the program untouched. We designed CSB (Code Style Benchmark), a benchmark suite of code style transfer tasks across five categories including converting for-loops to list comprehensions, eliminating duplication in code, adding decorators to methods, etc. We then used these tests to see if large pre-trained code language models or fine-tuned models perform style transfer correctly, based on rigorous metrics to test that the transfer did occur, and the code still passes functional tests. Surprisingly, language models failed to perform all of the tasks, suggesting that they perform poorly on tasks that require code understanding. We will make available the large-scale corpora to help the community build better code models.

------------

`[2406.10424] What is the Visual Cognition Gap between Humans and Multimodal LLMs? <https://arxiv.org/abs/2406.10424>`__ 人类和多模态llm之间的视觉认知差距是什么?

::

    Fri, 14 Jun 2024 22:02:21 GMT
    Xu Cao, Bolin Lai, Wenqian Ye, Yunsheng Ma, Joerg Heintz, Jintai Chen, Jianguo Cao, James M. Rehg

Recently, Multimodal Large Language Models (MLLMs) have shown great promise in language-guided perceptual tasks such as recognition, segmentation, and object detection. However, their effectiveness in addressing visual cognition problems that require high-level reasoning is not well-established. One such challenge is abstract visual reasoning (AVR) -- the cognitive ability to discern relationships among patterns in a set of images and extrapolate to predict subsequent patterns. This skill is crucial during the early neurodevelopmental stages of children. Inspired by the AVR tasks in Raven's Progressive Matrices (RPM) and Wechsler Intelligence Scale for Children (WISC), we propose a new dataset MaRs-VQA and a new benchmark VCog-Bench containing three datasets to evaluate the zero-shot AVR capability of MLLMs and compare their performance with existing human intelligent investigation. Our comparative experiments with different open-source and closed-source MLLMs on the VCog-Bench revealed a gap between MLLMs and human intelligence, highlighting the visual cognitive limitations of current MLLMs. We believe that the public release of VCog-Bench, consisting of MaRs-VQA, and the inference pipeline will drive progress toward the next generation of MLLMs with human-like visual cognition abilities.

------------

`[2406.10450] TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation <https://arxiv.org/abs/2406.10450>`__ TokenRec:基于llm的生成式推荐的ID分词学习

::

    Sat, 15 Jun 2024 00:07:44 GMT
    Haohao Qu, Wenqi Fan, Zihuai Zhao, Qing Li

There is a growing interest in utilizing large-scale language models (LLMs) to advance next-generation Recommender Systems (RecSys), driven by their outstanding language understanding and in-context learning capabilities. In this scenario, tokenizing (i.e., indexing) users and items becomes essential for ensuring a seamless alignment of LLMs with recommendations. While several studies have made progress in representing users and items through textual contents or latent representations, challenges remain in efficiently capturing high-order collaborative knowledge into discrete tokens that are compatible with LLMs. Additionally, the majority of existing tokenization approaches often face difficulties in generalizing effectively to new/unseen users or items that were not in the training corpus. To address these challenges, we propose a novel framework called TokenRec, which introduces not only an effective ID tokenization strategy but also an efficient retrieval paradigm for LLM-based recommendations. Specifically, our tokenization strategy, Masked Vector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item representations learned from collaborative filtering into discrete tokens, thus achieving a smooth incorporation of high-order collaborative knowledge and a generalizable tokenization of users and items for LLM-based RecSys. Meanwhile, our generative retrieval paradigm is designed to efficiently recommend top-$K$ items for users to eliminate the need for the time-consuming auto-regressive decoding and beam search processes used by LLMs, thus significantly reducing inference time. Comprehensive experiments validate the effectiveness of the proposed methods, demonstrating that TokenRec outperforms competitive benchmarks, including both traditional recommender systems and emerging LLM-based recommender systems.

------------

`[2406.10574] Large Language Models Playing Mixed Strategy Nash Equilibrium Games <https://arxiv.org/abs/2406.10574>`__ 玩混合策略纳什均衡博弈的大型语言模型

::

    Sat, 15 Jun 2024 09:30:20 GMT
    Alonso Silva

Generative artificial intelligence (Generative AI), and in particular Large Language Models (LLMs) have gained significant popularity among researchers and industrial communities, paving the way for integrating LLMs in different domains, such as robotics, telecom, and healthcare. In this paper, we study the intersection of game theory and generative artificial intelligence, focusing on the capabilities of LLMs to find the Nash equilibrium in games with a mixed strategy Nash equilibrium and no pure strategy Nash equilibrium (that we denote mixed strategy Nash equilibrium games). The study reveals a significant enhancement in the performance of LLMs when they are equipped with the possibility to run code and are provided with a specific prompt to incentivize them to do so. However, our research also highlights the limitations of LLMs when the randomization strategy of the game is not easy to deduce. It is evident that while LLMs exhibit remarkable proficiency in well-known standard games, their performance dwindles when faced with slight modifications of the same games. This paper aims to contribute to the growing body of knowledge on the intersection of game theory and generative artificial intelligence while providing valuable insights into LLMs strengths and weaknesses. It also underscores the need for further research to overcome the limitations of LLMs, particularly in dealing with even slightly more complex scenarios, to harness their full potential.

------------

`[2406.10591] MINT: a Multi-modal Image and Narrative Text Dubbing Dataset for Foley Audio Content Planning and Generation <https://arxiv.org/abs/2406.10591>`__ MINT:用于Foley音频内容规划和生成的多模态图像和叙事文本配音数据集

::

    Sat, 15 Jun 2024 10:47:36 GMT
    Ruibo Fu, Shuchen Shi, Hongming Guo, Tao Wang, Chunyu Qiang, Zhengqi Wen, Jianhua Tao, Xin Qi, Yi Lu, Xiaopeng Wang, Zhiyong Wang, Yukun Liu, Xuefei Liu, Shuai Zhang, Guanjun Li

Foley audio, critical for enhancing the immersive experience in multimedia content, faces significant challenges in the AI-generated content (AIGC) landscape. Despite advancements in AIGC technologies for text and image generation, the foley audio dubbing remains rudimentary due to difficulties in cross-modal scene matching and content correlation. Current text-to-audio technology, which relies on detailed and acoustically relevant textual descriptions, falls short in practical video dubbing applications. Existing datasets like AudioSet, AudioCaps, Clotho, Sound-of-Story, and WavCaps do not fully meet the requirements for real-world foley audio dubbing task. To address this, we introduce the Multi-modal Image and Narrative Text Dubbing Dataset (MINT), designed to enhance mainstream dubbing tasks such as literary story audiobooks dubbing, image/silent video dubbing. Besides, to address the limitations of existing TTA technology in understanding and planning complex prompts, a Foley Audio Content Planning, Generation, and Alignment (CPGA) framework is proposed, which includes a content planning module leveraging large language models for complex multi-modal prompts comprehension.
Additionally, the training process is optimized using Proximal Policy Optimization based reinforcement learning, significantly improving the alignment and auditory realism of generated foley audio. Experimental results demonstrate that our approach significantly advances the field of foley audio dubbing, providing robust solutions for the challenges of multi-modal dubbing.
Even when utilizing the relatively lightweight GPT-2 model, our framework outperforms open-source multimodal large models such as LLaVA, DeepSeek-VL, and Moondream2. The dataset is available at https://github.com/borisfrb/MINT .

------------

`[2406.10816] Optimization of Armv9 architecture general large language model inference performance based on Llama.cpp <https://arxiv.org/abs/2406.10816>`__ 

::

    Sun, 16 Jun 2024 06:46:25 GMT
    Longhao Chen, Yina Zhao, Qiangjun Xie, Qinghua Sheng

This article optimizes the inference performance of the Qwen-1.8B model by performing Int8 quantization, vectorizing some operators in llama.cpp, and modifying the compilation script to improve the compiler optimization level. On the Yitian 710 experimental platform, the prefill performance is increased by 1.6 times, the decoding performance is increased by 24 times, the memory usage is reduced to 1/5 of the original, and the accuracy loss is almost negligible.

------------

`[2406.10889] VELOCITI: Can Video-Language Models Bind Semantic Concepts through Time? <https://arxiv.org/abs/2406.10889>`__ VELOCITI:视频语言模型可以通过时间绑定语义概念吗?

::

    Sun, 16 Jun 2024 10:42:21 GMT
    Darshana Saravanan, Darshan Singh, Varun Gupta, Zeeshan Khan, Vineet Gandhi, Makarand Tapaswi

Compositionality is a fundamental aspect of vision-language understanding and is especially required for videos since they contain multiple entities (e.g.
persons, actions, and scenes) interacting dynamically over time. Existing benchmarks focus primarily on perception capabilities. However, they do not study binding, the ability of a model to associate entities through appropriate relationships. To this end, we propose VELOCITI, a new benchmark building on complex movie clips and dense semantic role label annotations to test perception and binding in video language models (contrastive and Video-LLMs).
Our perception-based tests require discriminating video-caption pairs that share similar entities, and the binding tests require models to associate the correct entity to a given situation while ignoring the different yet plausible entities that also appear in the same video. While current state-of-the-art models perform moderately well on perception tests, accuracy is near random when both entities are present in the same video, indicating that they fail at binding tests. Even the powerful Gemini 1.5 Flash has a substantial gap (16-28%) with respect to human accuracy in such binding tests.

------------

`[2406.10920] Hamilton-Jacobi Based Policy-Iteration via Deep Operator Learning <https://arxiv.org/abs/2406.10920>`__ 基于深度算子学习的Hamilton-Jacobi策略迭代

::

    Sun, 16 Jun 2024 12:53:17 GMT
    Jae Yong Lee, Yeoneung Kim

The framework of deep operator network (DeepONet) has been widely exploited thanks to its capability of solving high dimensional partial differential equations. In this paper, we incorporate DeepONet with a recently developed policy iteration scheme to numerically solve optimal control problems and the corresponding Hamilton--Jacobi--Bellman (HJB) equations. A notable feature of our approach is that once the neural network is trained, the solution to the optimal control problem and HJB equations with different terminal functions can be inferred quickly thanks to the unique feature of operator learning.
Furthermore, a quantitative analysis of the accuracy of the algorithm is carried out via comparison principles of viscosity solutions. The effectiveness of the method is verified with various examples, including 10-dimensional linear quadratic regulator problems (LQRs).

------------

`[2406.11047] Enhancing Supermarket Robot Interaction: A Multi-Level LLM Conversational Interface for Handling Diverse Customer Intents <https://arxiv.org/abs/2406.11047>`__ 增强超市机器人交互:用于处理不同客户意图的多层次LLM对话界面

::

    Sun, 16 Jun 2024 19:13:01 GMT
    Chandran Nandkumar and Luka Peternel

This paper presents the design and evaluation of a novel multi-level LLM interface for supermarket robots to assist customers. The proposed interface allows customers to convey their needs through both generic and specific queries. While state-of-the-art systems like OpenAI's GPTs are highly adaptable and easy to build and deploy, they still face challenges such as increased response times and limitations in strategic control of the underlying model for tailored use-case and cost optimization. Driven by the goal of developing faster and more efficient conversational agents, this paper advocates for using multiple smaller, specialized LLMs fine-tuned to handle different user queries based on their specificity and user intent. We compare this approach to a specialized GPT model powered by GPT-4 Turbo, using the Artificial Social Agent Questionnaire (ASAQ) and qualitative participant feedback in a counterbalanced within-subjects experiment. Our findings show that our multi-LLM chatbot architecture outperformed the benchmarked GPT model across all 13 measured criteria, with statistically significant improvements in four key areas: performance, user satisfaction, user-agent partnership, and self-image enhancement. The paper also presents a method for supermarket robot navigation by mapping the final chatbot response to correct shelf numbers, enabling the robot to sequentially navigate towards the respective products, after which lower-level robot perception, control, and planning can be used for automated object retrieval. We hope this work encourages more efforts into using multiple, specialized smaller models instead of relying on a single powerful, but more expensive and slower model.

------------

`[2406.11118] Incentivizing Quality Text Generation via Statistical Contracts <https://arxiv.org/abs/2406.11118>`__ 通过统计契约激励高质量文本生成

::

    Mon, 17 Jun 2024 00:30:58 GMT
    Eden Saig, Ohad Einav, Inbal Talgam-Cohen

While the success of large language models (LLMs) increases demand for machine-generated text, current pay-per-token pricing schemes create a misalignment of incentives known in economics as moral hazard: Text-generating agents have strong incentive to cut costs by preferring a cheaper model over the cutting-edge one, and this can be done "behind the scenes" since the agent performs inference internally. In this work, we approach this issue from an economic perspective, by proposing a pay-for-performance, contract-based framework for incentivizing quality. We study a principal-agent game where the agent generates text using costly inference, and the contract determines the principal's payment for the text according to an automated quality evaluation.
Since standard contract theory is inapplicable when internal inference costs are unknown, we introduce cost-robust contracts. As our main theoretical contribution, we characterize optimal cost-robust contracts through a direct correspondence to optimal composite hypothesis tests from statistics, generalizing a result of Saig et al. (NeurIPS'23). We evaluate our framework empirically by deriving contracts for a range of objectives and LLM evaluation benchmarks, and find that cost-robust contracts sacrifice only a marginal increase in objective value compared to their cost-aware counterparts.

------------

`[2406.11156] DELRec: Distilling Sequential Pattern to Enhance LLM-based Recommendation <https://arxiv.org/abs/2406.11156>`__ DELRec:提取序列模式增强基于llm的推荐

::

    Mon, 17 Jun 2024 02:47:09 GMT
    Guohao Sun and Haoyi Zhang

Sequential recommendation (SR) tasks enhance recommendation accuracy by capturing the connection between users' past interactions and their changing preferences. Conventional models often focus solely on capturing sequential patterns within the training data, neglecting the broader context and semantic information embedded in item titles from external sources. This limits their predictive power and adaptability. Recently, large language models (LLMs) have shown promise in SR tasks due to their advanced understanding capabilities and strong generalization abilities. Researchers have attempted to enhance LLMs' recommendation performance by incorporating information from SR models.
However, previous approaches have encountered problems such as 1) only influencing LLMs at the result level;2) increased complexity of LLMs recommendation methods leading to reduced interpretability; 3) incomplete understanding and utilization of SR models information by LLMs. To address these problems, we proposes a novel framework, DELRec, which aims to extract knowledge from SR models and enable LLMs to easily comprehend and utilize this supplementary information for more effective sequential recommendations. DELRec consists of two main stages: 1) SR Models Pattern Distilling, focusing on extracting behavioral patterns exhibited by SR models using soft prompts through two well-designed strategies; 2) LLMs-based Sequential Recommendation, aiming to fine-tune LLMs to effectively use the distilled auxiliary information to perform SR tasks. Extensive experimental results conducted on three real datasets validate the effectiveness of the DELRec framework.

------------

`[2406.11227] Compound Schema Registry <https://arxiv.org/abs/2406.11227>`__ 

::

    Mon, 17 Jun 2024 05:50:46 GMT
    Silvery D. Fu, Xuewei Chen

Schema evolution is critical in managing database systems to ensure compatibility across different data versions. A schema registry typically addresses the challenges of schema evolution in real-time data streaming by managing, validating, and ensuring schema compatibility. However, current schema registries struggle with complex syntactic alterations like field renaming or type changes, which often require significant manual intervention and can disrupt service. To enhance the flexibility of schema evolution, we propose the use of generalized schema evolution (GSE) facilitated by a compound AI system. This system employs Large Language Models (LLMs) to interpret the semantics of schema changes, supporting a broader range of syntactic modifications without interrupting data streams. Our approach includes developing a task-specific language, Schema Transformation Language (STL), to generate schema mappings as an intermediate representation (IR), simplifying the integration of schema changes across different data processing platforms.
Initial results indicate that this approach can improve schema mapping accuracy and efficiency, demonstrating the potential of GSE in practical applications.

------------

`[2406.11231] Enabling robots to follow abstract instructions and complete complex dynamic tasks <https://arxiv.org/abs/2406.11231>`__ 使机器人能够遵循抽象的指令并完成复杂的动态任务

::

    Mon, 17 Jun 2024 05:55:35 GMT
    Ruaridh Mon-Williams, Gen Li, Ran Long, Wenqian Du, Chris Lucas

Completing complex tasks in unpredictable settings like home kitchens challenges robotic systems. These challenges include interpreting high-level human commands, such as "make me a hot beverage" and performing actions like pouring a precise amount of water into a moving mug. To address these challenges, we present a novel framework that combines Large Language Models (LLMs), a curated Knowledge Base, and Integrated Force and Visual Feedback (IFVF). Our approach interprets abstract instructions, performs long-horizon tasks, and handles various uncertainties. It utilises GPT-4 to analyse the user's query and surroundings, then generates code that accesses a curated database of functions during execution. It translates abstract instructions into actionable steps. Each step involves generating custom code by employing retrieval-augmented generalisation to pull IFVF-relevant examples from the Knowledge Base. IFVF allows the robot to respond to noise and disturbances during execution. We use coffee making and plate decoration to demonstrate our approach, including components ranging from pouring to drawer opening, each benefiting from distinct feedback types and methods. This novel advancement marks significant progress toward a scalable, efficient robotic framework for completing complex tasks in uncertain environments. Our findings are illustrated in an accompanying video and supported by an open-source GitHub repository (released upon paper acceptance).

------------

`[2406.11232] A Collaborative Data Analytics System with Recommender for Diverse Users <https://arxiv.org/abs/2406.11232>`__ 面向多样化用户推荐的协同数据分析系统

::

    Mon, 17 Jun 2024 05:59:13 GMT
    Siu Lung Ng, Hirad Baradaran Rezaei, Fethi Rabhi

This paper presents the SLEGO (Software-Lego) system, a collaborative analytics platform that bridges the gap between experienced developers and novice users using a cloud-based platform with modular, reusable microservices.
These microservices enable developers to share their analytical tools and workflows, while a simple graphical user interface (GUI) allows novice users to build comprehensive analytics pipelines without programming skills. Supported by a knowledge base and a Large Language Model (LLM) powered recommendation system, SLEGO enhances the selection and integration of microservices, increasing the efficiency of analytics pipeline construction. Case studies in finance and machine learning illustrate how SLEGO promotes the sharing and assembly of modular microservices, significantly improving resource reusability and team collaboration. The results highlight SLEGO's role in democratizing data analytics by integrating modular design, knowledge bases, and recommendation systems, fostering a more inclusive and efficient analytical environment.

------------

`[2406.11248] Performance Improvement of Language-Queried Audio Source Separation Based on Caption Augmentation From Large Language Models for DCASE Challenge 2024 Task 9 <https://arxiv.org/abs/2406.11248>`__ DCASE Challenge 2024任务9中基于大型语言模型描述扩充的语言查询音源分离性能提升

::

    Mon, 17 Jun 2024 06:19:14 GMT
    Do Hyun Lee, Yoonah Song, Hong Kook Kim

We present a prompt-engineering-based text-augmentation approach applied to a language-queried audio source separation (LASS) task. To enhance the performance of LASS, the proposed approach utilizes large language models (LLMs) to generate multiple captions corresponding to each sentence of the training dataset. To this end, we first perform experiments to identify the most effective prompts for caption augmentation with a smaller number of captions. A LASS model trained with these augmented captions demonstrates improved performance on the DCASE 2024 Task 9 validation set compared to that trained without augmentation. This study highlights the effectiveness of LLM-based caption augmentation in advancing language-queried audio source separation.

------------

`[2406.11290] Iterative Utility Judgment Framework via LLMs Inspired by Relevance in Philosophy <https://arxiv.org/abs/2406.11290>`__ 基于LLMs的迭代效用判断框架，受哲学相关性的启发

::

    Mon, 17 Jun 2024 07:52:42 GMT
    Hengran Zhang, Keping Bi, Jiafeng Guo, Xueqi Cheng

Utility and topical relevance are critical measures in information retrieval (IR), reflecting system and user perspectives, respectively. While topical relevance has long been emphasized, utility is a higher standard of relevance and is more useful for facilitating downstream tasks, e.g., in Retrieval-Augmented Generation (RAG). When we incorporate utility judgments into RAG, we realize that the topical relevance, utility, and answering in RAG are closely related to the three types of relevance that Schutz discussed from a philosophical perspective. They are topical relevance, interpretational relevance, and motivational relevance, respectively. Inspired by the dynamic iterations of the three types of relevance, we propose an Iterative utiliTy judgmEnt fraMework (ITEM) to promote each step of the cycle of RAG. We conducted extensive experiments on multi-grade passage retrieval and factoid question-answering datasets (i.e., TREC DL, WebAP, and NQ). Experimental results demonstrate significant improvements in utility judgments, ranking of topical relevance, and answer generation upon representative baselines, including multiple single-shot utility judging approaches. Our code and benchmark can be found at https://anonymous.4open.science/r/ITEM-B486/.

------------

`[2406.11432] AnyTrans: Translate AnyText in the Image with Large Scale Models <https://arxiv.org/abs/2406.11432>`__ AnyTrans:用大规模模型翻译图像中的任何文本

::

    Mon, 17 Jun 2024 11:37:48 GMT
    Zhipeng Qian, Pei Zhang, Baosong Yang, Kai Fan, Yiwei Ma, Derek F. Wong, Xiaoshuai Sun, Rongrong Ji

This paper introduces AnyTrans, an all-encompassing framework for the task-Translate AnyText in the Image (TATI), which includes multilingual text translation and text fusion within images. Our framework leverages the strengths of large-scale models, such as Large Language Models (LLMs) and text-guided diffusion models, to incorporate contextual cues from both textual and visual elements during translation. The few-shot learning capability of LLMs allows for the translation of fragmented texts by considering the overall context. Meanwhile, the advanced inpainting and editing abilities of diffusion models make it possible to fuse translated text seamlessly into the original image while preserving its style and realism. Additionally, our framework can be constructed entirely using open-source models and requires no training, making it highly accessible and easily expandable. To encourage advancement in the TATI task, we have meticulously compiled a test dataset called MTIT6, which consists of multilingual text image translation data from six language pairs.

------------

`[2406.11548] AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation <https://arxiv.org/abs/2406.11548>`__ AIC MLLM:面向鲁棒机器人操纵的自主交互修正MLLM

::

    Mon, 17 Jun 2024 13:44:53 GMT
    Chuyan Xiong, Chengyu Shen, Xiaoqi Li, Kaichen Zhou, Jiaming Liu, Ruiping Wang, Hao Dong

The ability to reflect on and correct failures is crucial for robotic systems to interact stably with real-life objects.Observing the generalization and reasoning capabilities of Multimodal Large Language Models (MLLMs), previous approaches have aimed to utilize these models to enhance robotic systems accordingly.However, these methods typically focus on high-level planning corrections using an additional MLLM, with limited utilization of failed samples to correct low-level contact poses. To address this gap, we propose an Autonomous Interactive Correction (AIC) MLLM, which makes use of previous low-level interaction experiences to correct SE(3) pose predictions.
Specifically, AIC MLLM is initially fine-tuned to acquire both pose prediction and feedback prompt comprehension abilities.We carefully design two types of prompt instructions through interactions with objects: 1) visual masks to highlight unmovable parts for position correction, and 2)textual descriptions to indicate potential directions for rotation correction.During inference, a Feedback Information Extraction module is introduced to recognize the failure cause, allowing AIC MLLM to adaptively correct the pose prediction using the corresponding prompts.To further enhance manipulation stability, we devise a Test Time Adaptation strategy that enables AIC MLLM to better adapt to the current scene configuration.Finally, extensive experiments are conducted in both simulated and real-world environments to evaluate the proposed method. The results demonstrate that our AIC MLLM can efficiently correct failure samples by leveraging interaction experience prompts.Real-world demonstration can be found at https://sites.google.com/view/aic-mllm

------------

`[2406.11589] CoSQA+: Enhancing Code Search Dataset with Matching Code <https://arxiv.org/abs/2406.11589>`__ 

::

    Mon, 17 Jun 2024 14:34:14 GMT
    Jing Gong, Yanghui Wu, Linxi Liang, Zibin Zheng, Yanlin Wang

Semantic code search, retrieving code that matches a given natural language query, is an important task to improve productivity in software engineering.
Existing code search datasets are problematic: either using unrealistic queries, or with mismatched codes, and typically using one-to-one query-code pairing, which fails to reflect the reality that a query might have multiple valid code matches. This paper introduces CoSQA+, pairing high-quality queries (reused from CoSQA) with multiple suitable codes. We collect code candidates from diverse sources and form candidate pairs by pairing queries with these codes. Utilizing the power of large language models (LLMs), we automate pair annotation, filtering, and code generation for queries without suitable matches. Through extensive experiments, CoSQA+ has demonstrated superior quality over CoSQA. Models trained on CoSQA+ exhibit improved performance.
Furthermore, we propose a new metric Mean Multi-choice Reciprocal Rank (MMRR), to assess one-to-N code search performance. We provide the code and data at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.

------------

`[2406.11818] Embodied Instruction Following in Unknown Environments <https://arxiv.org/abs/2406.11818>`__ 未知环境下的具身指令跟随

::

    Mon, 17 Jun 2024 17:55:40 GMT
    Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan

Enabling embodied agents to complete complex human instructions from natural language is crucial to autonomous systems in household services. Conventional methods can only accomplish human instructions in the known environment where all interactive objects are provided to the embodied agent, and directly deploying the existing approaches for the unknown environment usually generates infeasible plans that manipulate non-existing objects. On the contrary, we propose an embodied instruction following (EIF) method for complex tasks in the unknown environment, where the agent efficiently explores the unknown environment to generate feasible plans with existing objects to accomplish abstract instructions. Specifically, we build a hierarchical embodied instruction following framework including the high-level task planner and the low-level exploration controller with multimodal large language models. We then construct a semantic representation map of the scene with dynamic region attention to demonstrate the known visual clues, where the goal of task planning and scene exploration is aligned for human instruction. For the task planner, we generate the feasible step-by-step plans for human goal accomplishment according to the task completion process and the known visual clues. For the exploration controller, the optimal navigation or object interaction policy is predicted based on the generated step-wise plans and the known visual clues. The experimental results demonstrate that our method can achieve 45.09% success rate in 204 complex human instructions such as making breakfast and tidying rooms in large house-level scenes.

------------

`[2406.11839] mDPO: Conditional Preference Optimization for Multimodal Large Language Models <https://arxiv.org/abs/2406.11839>`__ mDPO:多模态大型语言模型的条件偏好优化

::

    Mon, 17 Jun 2024 17:59:58 GMT
    Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen

Direct preference optimization (DPO) has shown to be an effective method for large language model (LLM) alignment. Recent works have attempted to apply DPO to multimodal scenarios but have found it challenging to achieve consistent improvement. Through a comparative experiment, we identify the unconditional preference problem in multimodal preference optimization, where the model overlooks the image condition. To address this problem, we propose mDPO, a multimodal DPO objective that prevents the over-prioritization of language-only preferences by also optimizing image preference. Moreover, we introduce a reward anchor that forces the reward to be positive for chosen responses, thereby avoiding the decrease in their likelihood -- an intrinsic problem of relative preference optimization. Experiments on two multimodal LLMs of different sizes and three widely used benchmarks demonstrate that mDPO effectively addresses the unconditional preference problem in multimodal preference optimization and significantly improves model performance, particularly in reducing hallucination.

------------

`[2406.10264] Large Language Model-empowered multimodal strain sensory system for shape recognition, monitoring, and human interaction of tensegrity <https://arxiv.org/abs/2406.10264>`__ 面向形状识别、监测和张拉整体人机交互的大型语言模型授权多模态应变感觉系统

::

    Tue, 11 Jun 2024 06:26:04 GMT
    Zebing Mao, Ryota Kobayashi, Hiroyuki Nabae, Koichi Suzumori

A tensegrity-based system is a promising approach for dynamic exploration of uneven and unpredictable environments, particularly, space exploration.
However, implementing such systems presents challenges in terms of intelligent aspects: state recognition, wireless monitoring, human interaction, and smart analyzing and advising function. Here, we introduce a 6-strut tensegrity integrate with 24 multimodal strain sensors by leveraging both deep learning model and large language models to realize smart tensegrity. Using conductive flexible tendons assisted by long short-term memory model, the tensegrity achieves the self-shape reconstruction without extern sensors. Through integrating the flask server and gpt-3.5-turbo model, the tensegrity autonomously enables to send data to iPhone for wireless monitoring and provides data analysis, explanation, prediction, and suggestions to human for decision making. Finally, human interaction system of the tensegrity helps human obtain necessary information of tensegrity from the aspect of human language. Overall, this intelligent tensegrity-based system with self-sensing tendons showcases potential for future exploration, making it a versatile tool for real-world applications.

------------

`[2406.10274] Using General Large Language Models to Classify Mathematical Documents <https://arxiv.org/abs/2406.10274>`__ 用通用大型语言模型对数学文档进行分类

::

    Tue, 11 Jun 2024 20:15:57 GMT
    Patrick D.F. Ion and Stephen M. Watt

In this article we report on an initial exploration to assess the viability of using the general large language models (LLMs), recently made public, to classify mathematical documents. Automated classification would be useful from the applied perspective of improving the navigation of the literature and the more open-ended goal of identifying relations among mathematical results. The Mathematical Subject Classification MSC 2020, from MathSciNet and zbMATH, is widely used and there is a significant corpus of ground truth material in the open literature. We have evaluated the classification of preprint articles from arXiv.org according to MSC 2020. The experiment used only the title and abstract alone -- not the entire paper. Since this was early in the use of chatbots and the development of their APIs, we report here on what was carried out by hand. Of course, the automation of the process will have to follow if it is to be generally useful. We found that in about 60% of our sample the LLM produced a primary classification matching that already reported on arXiv. In about half of those instances, there were additional primary classifications that were not detected. In about 40% of our sample, the LLM suggested a different classification than what was provided. A detailed examination of these cases, however, showed that the LLM-suggested classifications were in most cases better than those provided.

------------

`[2406.10281] Watermarking Language Models with Error Correcting Codes <https://arxiv.org/abs/2406.10281>`__ 

::

    Wed, 12 Jun 2024 05:13:09 GMT
    Patrick Chao, Edgar Dobriban, Hamed Hassani

Recent progress in large language models enables the creation of realistic machine-generated content. Watermarking is a promising approach to distinguish machine-generated text from human text, embedding statistical signals in the output that are ideally undetectable to humans. We propose a watermarking framework that encodes such signals through an error correcting code. Our method, termed robust binary code (RBC) watermark, introduces no distortion compared to the original probability distribution, and no noticeable degradation in quality. We evaluate our watermark on base and instruction fine-tuned models and find our watermark is robust to edits, deletions, and translations. We provide an information-theoretic perspective on watermarking, a powerful statistical test for detection and for generating p-values, and theoretical guarantees. Our empirical findings suggest our watermark is fast, powerful, and robust, comparing favorably to the state-of-the-art.

------------

`[2406.10839] Reminding Multimodal Large Language Models of Object-aware Knowledge with Retrieved Tags <https://arxiv.org/abs/2406.10839>`__ 利用检索标签提醒多模态大型语言模型的对象感知知识

::

    Sun, 16 Jun 2024 08:20:12 GMT
    Daiqing Qi, Handong Zhao, Zijun Wei, Sheng Li

Despite recent advances in the general visual instruction-following ability of Multimodal Large Language Models (MLLMs), they still struggle with critical problems when required to provide a precise and detailed response to a visual instruction: (1) failure to identify novel objects or entities, (2) mention of non-existent objects, and (3) neglect of object's attributed details. Intuitive solutions include improving the size and quality of data or using larger foundation models. They show effectiveness in mitigating these issues, but at an expensive cost of collecting a vast amount of new data and introducing a significantly larger model. Standing at the intersection of these approaches, we examine the three object-oriented problems from the perspective of the image-to-text mapping process by the multimodal connector. In this paper, we first identify the limitations of multimodal connectors stemming from insufficient training data. Driven by this, we propose to enhance the mapping with retrieval-augmented tag tokens, which contain rich object-aware information such as object names and attributes. With our Tag-grounded visual instruction tuning with retrieval Augmentation (TUNA), we outperform baselines that share the same language model and training data on 12 benchmarks.
Furthermore, we show the zero-shot capability of TUNA when provided with specific datastores.

------------

`[2406.10958] City-LEO: Toward Transparent City Management Using LLM with End-to-End Optimization <https://arxiv.org/abs/2406.10958>`__ City- leo:使用端到端优化的LLM实现透明的城市管理

::

    Sun, 16 Jun 2024 14:25:08 GMT
    Zihao Jiao, Mengyi Sha, Haoyu Zhang, Xinyu Jiang

Existing operations research (OR) models and tools play indispensable roles in smart-city operations, yet their practical implementation is limited by the complexity of modeling and deficiencies in optimization proficiency. To generate more relevant and accurate solutions to users' requirements, we propose a large language model (LLM)-based agent ("City-LEO") that enhances the efficiency and transparency of city management through conversational interactions. Specifically, to accommodate diverse users' requirements and enhance computational tractability, City-LEO leverages LLM's logical reasoning capabilities on prior knowledge to scope down large-scale optimization problems efficiently. In the human-like decision process, City-LEO also incorporates End-to-end (E2E) model to synergize the prediction and optimization. The E2E framework be conducive to coping with environmental uncertainties and involving more query-relevant features, and then facilitates transparent and interpretable decision-making process. In case study, we employ City-LEO in the operations management of e-bike sharing (EBS) system. The numerical results demonstrate that City-LEO has superior performance when benchmarks against the full-scale optimization problem. With less computational time, City-LEO generates more satisfactory and relevant solutions to the users' requirements, and achieves lower global suboptimality without significantly compromising accuracy. In a broader sense, our proposed agent offers promise to develop LLM-embedded OR tools for smart-city operations management.

------------

`[2406.11025] Large Language Models for Dysfluency Detection in Stuttered Speech <https://arxiv.org/abs/2406.11025>`__ 用于口吃语音流畅性障碍检测的大型语言模型

::

    Sun, 16 Jun 2024 17:51:22 GMT
    Dominik Wagner, Sebastian P. Bayerl, Ilja Baumann, Korbinian Riedhammer, Elmar N\"oth, Tobias Bocklet

Accurately detecting dysfluencies in spoken language can help to improve the performance of automatic speech and language processing components and support the development of more inclusive speech and language technologies. Inspired by the recent trend towards the deployment of large language models (LLMs) as universal learners and processors of non-lexical inputs, such as audio and video, we approach the task of multi-label dysfluency detection as a language modeling problem. We present hypotheses candidates generated with an automatic speech recognition system and acoustic representations extracted from an audio encoder model to an LLM, and finetune the system to predict dysfluency labels on three datasets containing English and German stuttered speech. The experimental results show that our system effectively combines acoustic and lexical information and achieves competitive results on the multi-label stuttering detection task.

------------

`[2406.11171] SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations <https://arxiv.org/abs/2406.11171>`__ SUGARCREPE++数据集:视觉-语言模型对语义和词汇变化的敏感性

::

    Mon, 17 Jun 2024 03:22:20 GMT
    Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry, Evangelos Milios, Sageev Oore, Hassan Sajjad

Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SUGARCREPE++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly in object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SUGARCREPE++, there is a significant opportunity for improvement. We show that all the models which achieve better performance on compositionality datasets need not perform equally well on SUGARCREPE++, signifying that compositionality alone may not be sufficient for understanding semantic and lexical alterations. Given the importance of the property that the SUGARCREPE++ dataset targets, it serves as a new challenge to the vision-and-language community.

------------

`[2406.11285] Self and Cross-Model Distillation for LLMs: Effective Methods for Refusal Pattern Alignment <https://arxiv.org/abs/2406.11285>`__ 面向llm的自模型和跨模型蒸馏:拒绝模式对齐的有效方法

::

    Mon, 17 Jun 2024 07:46:45 GMT
    Jie Li, Yi Liu, Chongyang Liu, Xiaoning Ren, Ling Shi, Weisong Sun, Yinxing Xue

Large Language Models (LLMs) like OpenAI's GPT series, Anthropic's Claude, and Meta's LLaMa have shown remarkable capabilities in text generation.
However, their susceptibility to toxic prompts presents significant security challenges. This paper investigates alignment techniques, including Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), to mitigate these risks. We conduct an empirical study on refusal patterns across nine LLMs, revealing that models with uniform refusal patterns, such as Claude3, exhibit higher security. Based on these findings, we propose self-distilling and cross-model distilling methods to enhance LLM security. Our results show that these methods significantly improve refusal rates and reduce unsafe content, with cross-model distilling achieving refusal rates close to Claude3's 94.51%. These findings underscore the potential of distillation-based alignment in securing LLMs against toxic prompts.

------------

`[2406.11503] GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation <https://arxiv.org/abs/2406.11503>`__ GeoGPT4V:具有几何图像生成的几何多模态大型语言模型

::

    Mon, 17 Jun 2024 13:04:27 GMT
    Shihao Cai, Keqin Bao, Hangyu Guo, Jizhi Zhang, Jun Song, Bo Zheng

Large language models have seen widespread adoption in math problem-solving.
However, in geometry problems that usually require visual aids for better understanding, even the most advanced multi-modal models currently still face challenges in effectively using image information. High-quality data is crucial for enhancing the geometric capabilities of multi-modal models, yet existing open-source datasets and related efforts are either too challenging for direct model learning or suffer from misalignment between text and images. To overcome this issue, we introduce a novel pipeline that leverages GPT-4 and GPT-4V to generate relatively basic geometry problems with aligned text and images, facilitating model learning. We have produced a dataset of 4.9K geometry problems and combined it with 19K open-source data to form our GeoGPT4V dataset. Experimental results demonstrate that the GeoGPT4V dataset significantly improves the geometry performance of various models on the MathVista and MathVision benchmarks. The code is available at https://github.com/Lanyu0303/GeoGPT4V_Project

------------

`[2406.11678] TourRank: Utilizing Large Language Models for Documents Ranking with a Tournament-Inspired Strategy <https://arxiv.org/abs/2406.11678>`__ 

::

    Mon, 17 Jun 2024 15:58:22 GMT
    Yiqun Chen, Qi Liu, Yi Zhang, Weiwei Sun, Daiting Shi, Jiaxin Mao, Dawei Yin

Large Language Models (LLMs) are increasingly employed in zero-shot documents ranking, yielding commendable results. However, several significant challenges still persist in LLMs for ranking: (1) LLMs are constrained by limited input length, precluding them from processing a large number of documents simultaneously; (2) The output document sequence is influenced by the input order of documents, resulting in inconsistent ranking outcomes; (3) Achieving a balance between cost and ranking performance is quite challenging. To tackle these issues, we introduce a novel documents ranking method called TourRank, which is inspired by the tournament mechanism. This approach alleviates the impact of LLM's limited input length through intelligent grouping, while the tournament-like points system ensures robust ranking, mitigating the influence of the document input sequence. We test TourRank with different LLMs on the TREC DL datasets and the BEIR benchmark. Experimental results show that TourRank achieves state-of-the-art performance at a reasonable cost.

------------

`[2406.11745] Multi-Layer Ranking with Large Language Models for News Source Recommendation <https://arxiv.org/abs/2406.11745>`__ 基于大规模语言模型的多层排序新闻源推荐

::

    Mon, 17 Jun 2024 17:02:34 GMT
    Wenjia Zhang, Lin Gui, Rob Procter, Yulan He

To seek reliable information sources for news events, we introduce a novel task of expert recommendation, which aims to identify trustworthy sources based on their previously quoted statements. To achieve this, we built a novel dataset, called NewsQuote, consisting of 23,571 quote-speaker pairs sourced from a collection of news articles. We formulate the recommendation task as the retrieval of experts based on their likelihood of being associated with a given query. We also propose a multi-layer ranking framework employing Large Language Models to improve the recommendation performance. Our results show that employing an in-context learning based LLM ranker and a multi-layer ranking-based filter significantly improve both the predictive quality and behavioural quality of the recommender system.

------------

`[2406.10300] Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications <https://arxiv.org/abs/2406.10300>`__ 作为软件组件的大型语言模型:集成llm应用的分类法

::

    Thu, 13 Jun 2024 21:32:56 GMT
    Irene Weber

Large Language Models (LLMs) have become widely adopted recently. Research explores their use both as autonomous agents and as tools for software engineering. LLM-integrated applications, on the other hand, are software systems that leverage an LLM to perform tasks that would otherwise be impossible or require significant coding effort. While LLM-integrated application engineering is emerging as new discipline, its terminology, concepts and methods need to be established. This study provides a taxonomy for LLM-integrated applications, offering a framework for analyzing and describing these systems. It also demonstrates various ways to utilize LLMs in applications, as well as options for implementing such integrations.
Following established methods, we analyze a sample of recent LLM-integrated applications to identify relevant dimensions. We evaluate the taxonomy by applying it to additional cases. This review shows that applications integrate LLMs in numerous ways for various purposes. Frequently, they comprise multiple LLM integrations, which we term ``LLM components''. To gain a clear understanding of an application's architecture, we examine each LLM component separately. We identify thirteen dimensions along which to characterize an LLM component, including the LLM skills leveraged, the format of the output, and more. LLM-integrated applications are described as combinations of their LLM components. We suggest a concise representation using feature vectors for visualization.
The taxonomy is effective for describing LLM-integrated applications. It can contribute to theory building in the nascent field of LLM-integrated application engineering and aid in developing such systems. Researchers and practitioners explore numerous creative ways to leverage LLMs in applications.
Though challenges persist, integrating LLMs may revolutionize the way software systems are built.

------------

`[2406.10707] DataStates-LLM: Lazy Asynchronous Checkpointing for Large Language Models <https://arxiv.org/abs/2406.10707>`__ DataStates-LLM:大型语言模型的惰性异步检查点

::

    Sat, 15 Jun 2024 18:30:40 GMT
    Avinash Maurya, Robert Underwood, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae

LLMs have seen rapid adoption in all domains. They need to be trained on high-end high-performance computing (HPC) infrastructures and ingest massive amounts of input data. Unsurprisingly, at such a large scale, unexpected events (e.g., failures of components, instability of the software, undesirable learning patterns, etc.), are frequent and typically impact the training in a negative fashion. Thus, LLMs need to be checkpointed frequently so that they can be rolled back to a stable state and subsequently fine-tuned. However, given the large sizes of LLMs, a straightforward checkpointing solution that directly writes the model parameters and optimizer state to persistent storage (e.g., a parallel file system), incurs significant I/O overheads. To address this challenge, in this paper we study how to reduce the I/O overheads for enabling fast and scalable checkpointing for LLMs that can be applied at high frequency (up to the granularity of individual iterations) without significant impact on the training process. Specifically, we introduce a lazy asynchronous multi-level approach that takes advantage of the fact that the tensors making up the model and optimizer state shards remain immutable for extended periods of time, which makes it possible to copy their content in the background with minimal interference during the training process. We evaluate our approach at scales of up to 180 GPUs using different model sizes, parallelism settings, and checkpointing frequencies. The results show up to 48$\times$ faster checkpointing and 2.2$\times$ faster end-to-end training runtime compared with the state-of-art checkpointing approaches.

------------

`[2312.15692] Instruction Fusion: Advancing Prompt Evolution through Hybridization <https://arxiv.org/abs/2312.15692>`__ 指令融合:通过杂交推进快速进化

::

    replaced with revised version Mon, 17 Jun 2024 07:40:26 GMT
    Submission history From: Jiuding Yang [view email]
    [v1] Mon, 25 Dec 2023 11:00:37 UTC (8,568 KB)
    [v2] Wed, 27 Dec 2023 10:18:43 UTC (8,570 KB)
    [v3] Wed, 7 Feb 2024 08:14:57 UTC (8,571 KB)
    [v4] Mon, 17 Jun 2024 07:40:26 UTC (8,632 KB)
    Weidong Guo, Jiuding Yang, Kaitong Yang, Xiangyang Li, Zhuwei Rao, Yu Xu, Di Niu

The fine-tuning of Large Language Models (LLMs) specialized in code generation has seen notable advancements through the use of open-domain coding queries. Despite the successes, existing methodologies like Evol-Instruct encounter performance limitations, impeding further enhancements in code generation tasks. This paper examines the constraints of existing prompt evolution techniques and introduces a novel approach, Instruction Fusion (IF). IF innovatively combines two distinct prompts through a hybridization process, thereby enhancing the evolution of training prompts for code LLMs. Our experimental results reveal that the proposed novel method effectively addresses the shortcomings of prior methods, significantly improving the performance of Code LLMs across five code generation benchmarks, namely HumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the effectiveness of Instruction Fusion in advancing the capabilities of LLMs in code generation.

------------

`[2312.16127] LLM-SAP: Large Language Models Situational Awareness Based Planning <https://arxiv.org/abs/2312.16127>`__ LLM-SAP:基于情景感知规划的大型语言模型

::

    replaced with revised version Sun, 16 Jun 2024 16:00:55 GMT
    Submission history From: Liman Wang [view email]
    [v1] Tue, 26 Dec 2023 17:19:09 UTC (14,399 KB)
    [v2] Mon, 1 Jan 2024 04:33:57 UTC (14,399 KB)
    [v3] Wed, 3 Jan 2024 15:13:50 UTC (14,399 KB)
    [v4] Sun, 4 Feb 2024 23:50:11 UTC (14,399 KB)
    [v5] Sun, 16 Jun 2024 16:00:55 UTC (14,524 KB)
    Liman Wang, Hanyang Zhong

This study explores integrating large language models (LLMs) with situational awareness-based planning (SAP) to enhance the decision-making capabilities of AI agents in dynamic and uncertain environments. We employ a multi-agent reasoning framework to develop a methodology that anticipates and actively mitigates potential risks through iterative feedback and evaluation processes. Our approach diverges from traditional automata theory by incorporating the complexity of human-centric interactions into the planning process, thereby expanding the planning scope of LLMs beyond structured and predictable scenarios. The results demonstrate significant improvements in the model's ability to provide comparative safe actions within hazard interactions, offering a perspective on proactive and reactive planning strategies. This research highlights the potential of LLMs to perform human-like action planning, thereby paving the way for more sophisticated, reliable, and safe AI systems in unpredictable real-world applications.

------------

`[2401.08743] MMToM-QA: Multimodal Theory of Mind Question Answering <https://arxiv.org/abs/2401.08743>`__ MMToM-QA:多模态心智理论问答

::

    replaced with revised version Sat, 15 Jun 2024 10:13:14 GMT
    Submission history From: Chuanyang Jin [view email]
    [v1] Tue, 16 Jan 2024 18:59:24 UTC (6,170 KB)
    [v2] Sat, 15 Jun 2024 10:13:14 UTC (6,157 KB)
    Chuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo, Zhiting Hu, Tomer Ullman, Antonio Torralba, Joshua B. Tenenbaum, Tianmin Shu

Theory of Mind (ToM), the ability to understand people's mental states, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models.

------------

`[2402.15729] How Do Humans Write Code? Large Models Do It the Same Way Too <https://arxiv.org/abs/2402.15729>`__ 人类是如何编写代码的?大型模型也是这样做的

::

    replaced with revised version Mon, 17 Jun 2024 06:00:41 GMT
    Submission history From: Long Li [view email]
    [v1] Sat, 24 Feb 2024 05:40:01 UTC (387 KB)
    [v2] Mon, 17 Jun 2024 06:00:41 UTC (645 KB)
    Long Li, Xuzheng He

Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model's information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework

------------

`[2404.10160] Reinforcement Learning from Multi-role Debates as Feedback for Bias Mitigation in LLMs <https://arxiv.org/abs/2404.10160>`__ 多角色辩论中的强化学习作为llm中偏差缓解的反馈

::

    replaced with revised version Sun, 16 Jun 2024 16:34:42 GMT
    Submission history From: Rosy Cheng [view email]
    [v1] Mon, 15 Apr 2024 22:18:50 UTC (2,079 KB)
    [v2] Sun, 28 Apr 2024 04:08:39 UTC (2,239 KB)
    [v3] Wed, 12 Jun 2024 12:42:28 UTC (2,829 KB)
    [v4] Sun, 16 Jun 2024 16:34:42 UTC (1 KB) (withdrawn)
    Ruoxi Cheng, Haoxuan Ma, Shuirong Cao, Tianyu Shi

Biases in LLMs can harm user experience and societal outcomes. Current bias mitigation methods such as RLHF usually rely on costly human feedback, lack transferability to other topics, and show poor performance. We find that informing the LLMs that their generated content is not generated by them and querying about potential biases greatly boosts their awareness and ability to mitigate biases. Based on this, we propose RLDF (Reinforcement Learning from Multi-role Debates as Feedback), replacing human feedback with AI for bias mitigation. RLDF engages LLMs in multi-role debates to expose biases and gradually reduce biases in each iteration using a ranking scoring mechanism. The dialogue are then used to create a dataset composed of both high bias and low bias instances to train the reward model in reinforcement learning. This dataset can be generated by the same LLM for self-reflection or a superior LLM like an API which guides the former one in a teacher-student mode. Experimental results across different LLMs and types of bias show the effectiveness of our approach in bias mitigation.

------------

`[2406.06870] What's in an embedding? Would a rose by any embedding smell as sweet? <https://arxiv.org/abs/2406.06870>`__ 嵌入中有什么?任何一种嵌入的玫瑰闻起来都那么香吗?

::

    replaced with revised version Sat, 15 Jun 2024 06:35:33 GMT
    Submission history From: Venkat Venkatasubramanian [view email]
    [v1] Tue, 11 Jun 2024 01:10:40 UTC (267 KB)
    [v2] Wed, 12 Jun 2024 18:38:13 UTC (268 KB)
    [v3] Sat, 15 Jun 2024 06:35:33 UTC (269 KB)
    Venkat Venkatasubramanian

Large Language Models (LLMs) are often criticized for lacking true "understanding" and the ability to "reason" with their knowledge, being seen merely as autocomplete systems. We believe that this assessment might be missing a nuanced insight. We suggest that LLMs do develop a kind of empirical "understanding" that is "geometry"-like, which seems adequate for a range of applications in NLP, computer vision, coding assistance, etc. However, this "geometric" understanding, built from incomplete and noisy data, makes them unreliable, difficult to generalize, and lacking in inference capabilities and explanations, similar to the challenges faced by heuristics-based expert systems decades ago.
To overcome these limitations, we suggest that LLMs should be integrated with an "algebraic" representation of knowledge that includes symbolic AI elements used in expert systems. This integration aims to create large knowledge models (LKMs) that not only possess "deep" knowledge grounded in first principles, but also have the ability to reason and explain, mimicking human expert capabilities. To harness the full potential of generative AI safely and effectively, a paradigm shift is needed from LLM to more comprehensive LKM.

------------

`[2406.10162] Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models <https://arxiv.org/abs/2406.10162>`__ 对托词的谄媚:研究大型语言模型中的奖励篡改

::

    replaced with revised version Mon, 17 Jun 2024 16:13:29 GMT
    Submission history From: Carson Denison [view email]
    [v1] Fri, 14 Jun 2024 16:26:20 UTC (4,150 KB)
    [v2] Mon, 17 Jun 2024 16:13:29 UTC (4,380 KB)
    Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Ethan Perez, Evan Hubinger

In reinforcement learning, specification gaming occurs when AI systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. In this paper, we study whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. We construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an LLM not to game early-curriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to our gameable environments does not prevent reward-tampering. These results demonstrate that LLMs can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.

------------

`[2305.03518] Black-box Prompt Tuning with Subspace Learning <https://arxiv.org/abs/2305.03518>`__ 基于子空间学习的黑盒提示调优

::

    replaced with revised version Mon, 17 Jun 2024 03:26:59 GMT
    Submission history From: Yuanhang Zheng [view email]
    [v1] Thu, 4 May 2023 01:04:25 UTC (655 KB)
    [v2] Mon, 17 Jun 2024 03:26:59 UTC (125 KB)
    Yuanhang Zheng, Zhixing Tan, Peng Li and Yang Liu

Black-box prompt tuning employs derivative-free optimization algorithms to learn prompts within low-dimensional subspaces rather than back-propagating through the network of Large Language Models (LLMs). Recent studies reveal that black-box prompt tuning lacks versatility across tasks and LLMs, which we believe is related to the suboptimal choice of subspaces. In this paper, we introduce Black-box prompt tuning with Subspace Learning (BSL) to enhance the versatility of black-box prompt tuning. Based on the assumption that nearly optimal prompts for similar tasks reside in a common subspace, we propose identifying such subspaces through meta-learning on a collection of similar source tasks. Consequently, for a target task that shares similarities with the source tasks, we expect that optimizing within the identified subspace can yield a prompt that performs well on the target task. Experimental results confirm that our BSL framework consistently achieves competitive performance across various downstream tasks and LLMs.

------------

`[2305.14647] Scientific Opinion Summarization: Paper Meta-review Generation Dataset, Methods, and Evaluation <https://arxiv.org/abs/2305.14647>`__ 科学观点摘要:论文元评论生成数据集、方法与评估

::

    replaced with revised version Sun, 16 Jun 2024 03:44:52 GMT
    Submission history From: Qi Zeng [view email]
    [v1] Wed, 24 May 2023 02:33:35 UTC (7,091 KB)
    [v2] Mon, 13 Nov 2023 19:47:35 UTC (7,098 KB)
    [v3] Sun, 16 Jun 2024 03:44:52 UTC (240 KB)
    Qi Zeng, Mankeerat Sidhu, Ansel Blume, Hou Pong Chan, Lu Wang, Heng Ji

Opinions in scientific research papers can be divergent, leading to controversies among reviewers. However, most existing datasets for opinion summarization are centered around product reviews and assume that the analyzed opinions are non-controversial, failing to account for the variability seen in other contexts such as academic papers, political debates, or social media discussions. To address this gap, we propose the task of scientific opinion summarization, where research paper reviews are synthesized into meta-reviews. To facilitate this task, we introduce the ORSUM dataset covering 15,062 paper meta-reviews and 57,536 paper reviews from 47 conferences. Furthermore, we propose the Checklist-guided Iterative Introspection approach, which breaks down scientific opinion summarization into several stages, iteratively refining the summary under the guidance of questions from a checklist. Our experiments show that (1) human-written summaries do not always satisfy all necessary criteria such as depth of discussion, and identifying consensus and controversy for the specific domain, and (2) the combination of task decomposition and iterative self-refinement shows strong potential for enhancing the opinions and can be applied to other complex text generation using black-box LLMs.

------------

`[2309.12030] Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation <https://arxiv.org/abs/2309.12030>`__ 广告淘金:广告文本生成的标准化与探索

::

    replaced with revised version Mon, 17 Jun 2024 06:37:32 GMT
    Submission history From: Masato Mita [view email]
    [v1] Thu, 21 Sep 2023 12:51:24 UTC (272 KB)
    [v2] Mon, 17 Jun 2024 06:37:32 UTC (1,571 KB)
    Masato Mita, Soichiro Murakami, Akihiko Kato, Peinan Zhang

In response to the limitations of manual ad creation, significant research has been conducted in the field of automatic ad text generation (ATG). However, the lack of comprehensive benchmarks and well-defined problem sets has made comparing different methods challenging. To tackle these challenges, we standardize the task of ATG and propose a first benchmark dataset, CAMERA, carefully designed and enabling the utilization of multi-modal information and facilitating industry-wise evaluations. Our extensive experiments with a variety of nine baselines, from classical methods to state-of-the-art models including large language models (LLMs), show the current state and the remaining challenges. We also explore how existing metrics in ATG and an LLM-based evaluator align with human evaluations.

------------

`[2310.00378] ValueDCG: Measuring Comprehensive Human Value Understanding Ability of Language Models <https://arxiv.org/abs/2310.00378>`__ ValueDCG:语言模型的人类综合价值理解能力度量

::

    replaced with revised version Mon, 17 Jun 2024 07:58:00 GMT
    Submission history From: Zhaowei Zhang [view email]
    [v1] Sat, 30 Sep 2023 13:47:55 UTC (1,311 KB)
    [v2] Sat, 7 Oct 2023 09:18:51 UTC (1,311 KB)
    [v3] Thu, 19 Oct 2023 03:18:58 UTC (1,303 KB)
    [v4] Mon, 17 Jun 2024 07:58:00 UTC (373 KB)
    Zhaowei Zhang, Fengshuo Bai, Jun Gao, Yaodong Yang

Personal values are a crucial factor behind human decision-making. Considering that Large Language Models (LLMs) have been shown to impact human decisions significantly, it is essential to make sure they accurately understand human values to ensure their safety. However, evaluating their grasp of these values is complex due to the value's intricate and adaptable nature. We argue that truly understanding values in LLMs requires considering both "know what" and "know why". To this end, we present a comprehensive evaluation metric, ValueDCG (Value Discriminator-Critique Gap), to quantitatively assess the two aspects with an engineering implementation. We assess four representative LLMs and provide compelling evidence that the growth rates of LLM's "know what" and "know why" capabilities do not align with increases in parameter numbers, resulting in a decline in the models' capacity to understand human values as larger amounts of parameters. This may further suggest that LLMs might craft plausible explanations based on the provided context without truly understanding their inherent value, indicating potential risks.

------------

`[2310.11532] Multi-stage Large Language Model Correction for Speech Recognition <https://arxiv.org/abs/2310.11532>`__ 语音识别中的多阶段大型语言模型校正

::

    replaced with revised version Mon, 17 Jun 2024 15:21:50 GMT
    Submission history From: Jie Pu [view email]
    [v1] Tue, 17 Oct 2023 19:02:40 UTC (104 KB)
    [v2] Mon, 17 Jun 2024 15:21:50 UTC (447 KB)
    Jie Pu, Thai-Son Nguyen, Sebastian St\"uker

In this paper, we investigate the usage of large language models (LLMs) to improve the performance of competitive speech recognition systems. Different from previous LLM-based ASR error correction methods, we propose a novel multi-stage approach that utilizes uncertainty estimation of ASR outputs and reasoning capability of LLMs. Specifically, the proposed approach has two stages: the first stage is about ASR uncertainty estimation and exploits N-best list hypotheses to identify less reliable transcriptions; The second stage works on these identified transcriptions and performs LLM-based corrections. This correction task is formulated as a multi-step rule-based LLM reasoning process, which uses explicitly written rules in prompts to decompose the task into concrete reasoning steps. Our experimental results demonstrate the effectiveness of the proposed method by showing 10% ~ 20% relative improvement in WER over competitive ASR systems -- across multiple test domains and in zero-shot settings.

------------

`[2310.15819] Generative Language Models Exhibit Social Identity Biases <https://arxiv.org/abs/2310.15819>`__ 

::

    replaced with revised version Mon, 17 Jun 2024 11:19:56 GMT
    Submission history From: Tiancheng Hu [view email]
    [v1] Tue, 24 Oct 2023 13:17:40 UTC (345 KB)
    [v2] Mon, 17 Jun 2024 11:19:56 UTC (402 KB)
    Tiancheng Hu, Yara Kyrychenko, Steve Rathje, Nigel Collier, Sander van der Linden, Jon Roozenbeek

The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans. We investigate whether ingroup solidarity and outgroup hostility, fundamental social identity biases known from social psychology, are present in 56 large language models. We find that almost all foundational language models and some instruction fine-tuned models exhibit clear ingroup-positive and outgroup-negative associations when prompted to complete sentences (e.g., "We are..."). Our findings suggest that modern language models exhibit fundamental social identity biases to a similar degree as humans, both in the lab and in real-world conversations with LLMs, and that curating training data and instruction fine-tuning can mitigate such biases. Our results have practical implications for creating less biased large-language models and further underscore the need for more research into user interactions with LLMs to prevent potential bias reinforcement in humans.

------------

`[2311.04166] Perturbed examples reveal invariances shared by language models <https://arxiv.org/abs/2311.04166>`__ 扰动的例子揭示了语言模型共享的不变性

::

    replaced with revised version Fri, 14 Jun 2024 18:36:36 GMT
    Submission history From: Ruchit Rawal [view email]
    [v1] Tue, 7 Nov 2023 17:48:35 UTC (2,219 KB)
    [v2] Fri, 14 Jun 2024 18:36:36 UTC (3,243 KB)
    Ruchit Rawal, Mariya Toneva

The rapid growth in natural language processing (NLP) research has led to numerous new models, outpacing our understanding of how they compare to established ones. One major reason for this difficulty is saturating benchmarks, which may not well reflect differences in model performance in the wild. In this work, we introduce a novel framework to compare two NLP models by revealing their shared invariance to interpretable input perturbations targeting a specific linguistic capability. Via experiments on models from the same and different architecture families, this framework offers insights about how changes in models (e.g., distillation, size increase) affect linguistic capabilities. Furthermore, our framework enables evaluation of invariances between commercial black-box models (e.g., InstructGPT family) and models that are better understood (e.g., GPT-2). Across experiments, we observe that large language models share many invariances encoded by models of various sizes, whereas the invariances by large models are only shared by other large models. Possessing a wide variety of invariances may be key to the recent successes of large language models, and our framework can shed light on the types of invariances retained or emerging in new models. We make the code publicly available.

------------

`[2311.10395] Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads <https://arxiv.org/abs/2311.10395>`__ 偏见脱吗?分析基于transformer的语言模型注意力头中的偏差

::

    replaced with revised version Sun, 16 Jun 2024 03:33:03 GMT
    Submission history From: Hanyu Duan [view email]
    [v1] Fri, 17 Nov 2023 08:56:13 UTC (9,602 KB)
    [v2] Sun, 16 Jun 2024 03:33:03 UTC (9,720 KB)
    Yi Yang, Hanyu Duan, Ahmed Abbasi, John P. Lalor, Kar Yan Tam

Transformer-based pretrained large language models (PLM) such as BERT and GPT have achieved remarkable success in NLP tasks. However, PLMs are prone to encoding stereotypical biases. Although a burgeoning literature has emerged on stereotypical bias mitigation in PLMs, such as work on debiasing gender and racial stereotyping, how such biases manifest and behave internally within PLMs remains largely unknown. Understanding the internal stereotyping mechanisms may allow better assessment of model fairness and guide the development of effective mitigation strategies. In this work, we focus on attention heads, a major component of the Transformer architecture, and propose a bias analysis framework to explore and identify a small set of biased heads that are found to contribute to a PLM's stereotypical bias. We conduct extensive experiments to validate the existence of these biased heads and to better understand how they behave. We investigate gender and racial bias in the English language in two types of Transformer-based PLMs: the encoder-based BERT model and the decoder-based autoregressive GPT model. Overall, the results shed light on understanding the bias behavior in pretrained language models.

------------

`[2312.15561] README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP <https://arxiv.org/abs/2312.15561>`__ README:通过以数据为中心的NLP桥接医学术语并为患者教育奠定理解

::

    replaced with revised version Mon, 17 Jun 2024 02:12:24 GMT
    Submission history From: Zonghai Yao [view email]
    [v1] Sun, 24 Dec 2023 23:01:00 UTC (883 KB)
    [v2] Mon, 15 Apr 2024 18:44:25 UTC (936 KB)
    [v3] Mon, 17 Jun 2024 02:12:24 UTC (1,028 KB)
    Zonghai Yao, Nandyala Siddharth Kantu, Guanghao Wei, Hieu Tran, Zhangqi Duan, Sunjae Kwon, Zhichao Yang, README annotation team, Hong Yu

The advancement in healthcare has shifted focus toward patient-centric approaches, particularly in self-care and patient education, facilitated by access to Electronic Health Records (EHR). However, medical jargon in EHRs poses significant challenges in patient comprehension. To address this, we introduce a new task of automatically generating lay definitions, aiming to simplify complex medical terms into patient-friendly lay language. We first created the README dataset, an extensive collection of over 50,000 unique (medical term, lay definition) pairs and 300,000 mentions, each offering context-aware lay definitions manually annotated by domain experts. We have also engineered a data-centric Human-AI pipeline that synergizes data filtering, augmentation, and selection to improve data quality. We then used README as the training data for models and leveraged a Retrieval-Augmented Generation method to reduce hallucinations and improve the quality of model outputs. Our extensive automatic and human evaluations demonstrate that open-source mobile-friendly models, when fine-tuned with high-quality data, are capable of matching or even surpassing the performance of state-of-the-art closed-source large language models like ChatGPT. This research represents a significant stride in closing the knowledge gap in patient education and advancing patient-centric healthcare solutions.

------------

`[2401.04700] Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue <https://arxiv.org/abs/2401.04700>`__ 模型编辑损害了大型语言模型的通用能力:正则化

::

    replaced with revised version Sun, 16 Jun 2024 21:27:34 GMT
    Submission history From: Jia-Chen Gu [view email]
    [v1] Tue, 9 Jan 2024 18:03:15 UTC (873 KB)
    [v2] Sun, 4 Feb 2024 19:04:13 UTC (1,662 KB)
    [v3] Sun, 16 Jun 2024 21:27:34 UTC (2,615 KB)
    Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, Nanyun Peng

Model editing is a technique that edits the large language models (LLMs) with updated knowledge to alleviate hallucinations without resource-intensive retraining. While current model editing methods can effectively modify a model's behavior within a specific area of interest, they often overlook the potential unintended side effects on the general abilities of LLMs such as reasoning, natural language inference, and question answering. In this paper, we raise concerns that model editing's improvements on factuality may come at the cost of a significant degradation of the model's general abilities. We systematically analyze the side effects by evaluating four popular editing methods on three LLMs across eight representative tasks. Our extensive empirical experiments show that it is challenging for current editing methods to simultaneously improve factuality of LLMs and maintain their general abilities. Our analysis reveals that the side effects are caused by model editing altering the original model weights excessively, leading to overfitting to the edited facts. To mitigate this, a method named RECT (RElative Change in weighT) is proposed to regularize the edit update weights. Evaluation results show that RECT can significantly mitigate the side effects of editing while still maintaining over 94% editing performance.

------------

`[2401.06081] Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint <https://arxiv.org/abs/2401.06081>`__ 

::

    replaced with revised version Mon, 17 Jun 2024 05:52:06 GMT
    Submission history From: Zhipeng Chen [view email]
    [v1] Thu, 11 Jan 2024 17:58:41 UTC (7,455 KB)
    [v2] Mon, 17 Jun 2024 05:52:06 UTC (7,618 KB)
    Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang and Ji-Rong Wen

Reinforcement learning (RL) has been widely used in training large language models (LLMs) for preventing unexpected outputs, eg reducing harmfulness and errors. However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness. To address it, we propose a new RL method named RLMEC that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training. Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. The experiment results on mathematical tasks and question-answering tasks have demonstrated the effectiveness of our approach. Our code and data are available at this https URL.

------------

`[2401.06431] Human-AI Collaborative Essay Scoring: A Dual-Process Framework with LLMs <https://arxiv.org/abs/2401.06431>`__ 人- ai协同作文评分:基于llm的双过程框架

::

    replaced with revised version Sat, 15 Jun 2024 03:44:08 GMT
    Submission history From: Changrong Xiao [view email]
    [v1] Fri, 12 Jan 2024 07:50:10 UTC (8,331 KB)
    [v2] Sat, 15 Jun 2024 03:44:08 UTC (9,130 KB)
    Changrong Xiao, Wenxing Ma, Qingping Song, Sean Xin Xu, Kunpeng Zhang, Yufang Wang, Qi Fu

Receiving timely and personalized feedback is essential for second-language learners, especially when human instructors are unavailable. This study explores the effectiveness of Large Language Models (LLMs), including both proprietary and open-source models, for Automated Essay Scoring (AES). Through extensive experiments with public and private datasets, we find that while LLMs do not surpass conventional state-of-the-art (SOTA) grading models in performance, they exhibit notable consistency, generalizability, and explainability. We propose an open-source LLM-based AES system, inspired by the dual-process theory. Our system offers accurate grading and high-quality feedback, at least comparable to that of fine-tuned proprietary LLMs, in addition to its ability to alleviate misgrading. Furthermore, we conduct human-AI co-grading experiments with both novice and expert graders. We find that our system not only automates the grading process but also enhances the performance and efficiency of human graders, particularly for essays where the model has lower confidence. These results highlight the potential of LLMs to facilitate effective human-AI collaboration in the educational context, potentially transforming learning experiences through AI-generated feedback.

------------

`[2401.18070] Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners? <https://arxiv.org/abs/2401.18070>`__ 语言模型在解决问题时表现出与人类学习者相同的认知偏差吗?

::

    replaced with revised version Mon, 17 Jun 2024 15:08:05 GMT
    Submission history From: Andreas Opedal [view email]
    [v1] Wed, 31 Jan 2024 18:48:20 UTC (1,023 KB)
    [v2] Mon, 17 Jun 2024 15:08:05 UTC (972 KB)
    Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan Cotterell, Bernhard Sch\"olkopf, Abulhair Saparov, Mrinmaya Sachan

There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which properties of human cognition are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand whether current LLMs display the same cognitive biases as children in these steps. We generate a novel set of word problems for each of these tests, using a neuro-symbolic approach that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not in the final step, in which the arithmetic expressions are executed to obtain the answer.

------------

`[2402.01740] Reducing Selection Bias in Large Language Models <https://arxiv.org/abs/2402.01740>`__ 减少大型语言模型中的选择偏差

::

    replaced with revised version Sat, 15 Jun 2024 13:23:21 GMT
    Submission history From: Jonathan Eicher [view email]
    [v1] Mon, 29 Jan 2024 15:43:23 UTC (9,063 KB)
    [v2] Thu, 4 Apr 2024 19:54:07 UTC (9,063 KB)
    [v3] Sat, 15 Jun 2024 13:23:21 UTC (9,063 KB)
    J. E. Eicher and R. F. Irgoli\v{c}

Large Language Models (LLMs) like gpt-3.5-turbo-0613 and claude-instant-1.2 are vital in interpreting and executing semantic tasks. Unfortunately, these models' inherent biases adversely affect their performance Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we experiment manipulating temperature, list length, object identity, object type, prompt complexity, and model. We isolated and measured the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproportionately represented in outputs. The usage of guard rails, a prompt engineering method of ensuring a response structure, increases bias and decreases instruction adherence when to a selection task. The bias is ablated when the guard rail step is separated from the list sampling step, lowering the complexity of each individual task. We provide LLM applications and theoretically suggest that LLMs experience a form of cognitive load that is compensated for with bias.

------------

`[2402.05201] The Effect of Sampling Temperature on Problem Solving in Large Language Models <https://arxiv.org/abs/2402.05201>`__ 采样温度对大型语言模型问题求解的影响

::

    replaced with revised version Fri, 14 Jun 2024 18:41:51 GMT
    Submission history From: Matthew Renze [view email]
    [v1] Wed, 7 Feb 2024 19:11:23 UTC (637 KB)
    [v2] Fri, 14 Jun 2024 18:41:51 UTC (756 KB)
    Matthew Renze and Erhan Guven

In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used nine popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. All code, data, and supplemental materials are available on GitHub at: this https URL

------------

`[2402.08702] PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling <https://arxiv.org/abs/2402.08702>`__ 多步骤任务中的提示优化(PROMST):融合人工反馈和启发式采样

::

    replaced with revised version Sun, 16 Jun 2024 18:01:06 GMT
    Submission history From: Yongchao Chen [view email]
    [v1] Tue, 13 Feb 2024 16:38:01 UTC (20,848 KB)
    [v2] Tue, 16 Apr 2024 18:29:43 UTC (29,515 KB)
    [v3] Sun, 16 Jun 2024 18:01:06 UTC (30,466 KB)
    Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, Chuchu Fan

Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework PROMST that incorporates human-designed feedback rules to automatically offer direct suggestions for improvement. We also use an extra learned heuristic model that predicts prompt performance to efficiently sample from prompt candidates. This approach significantly outperforms both human-engineered prompts and several other prompt optimization methods across 11 representative multi-step tasks (an average 10.6\%-29.3\% improvement to current best methods on five LLMs respectively). We believe our work can serve as a benchmark for automatic prompt optimization for LLM-driven multi-step tasks. Datasets and Codes are available at this https URL. Project Page is available at this https URL.

------------

`[2402.10496] Comparing Hallucination Detection Metrics for Multilingual Generation <https://arxiv.org/abs/2402.10496>`__ 多语言生成的幻觉检测指标比较

::

    replaced with revised version Sun, 16 Jun 2024 00:44:28 GMT
    Submission history From: Haoqiang Kang [view email]
    [v1] Fri, 16 Feb 2024 08:10:34 UTC (7,014 KB)
    [v2] Sun, 16 Jun 2024 00:44:28 UTC (7,028 KB)
    Haoqiang Kang, Terra Blevins, Luke Zettlemoyer

While many hallucination detection techniques have been evaluated on English text, their effectiveness in multilingual contexts remains unknown. This paper assesses how well various factual hallucination detection metrics (lexical metrics like ROUGE and Named Entity Overlap, and Natural Language Inference (NLI)-based metrics) identify hallucinations in generated biographical summaries across languages. We compare how well automatic metrics correlate to each other and whether they agree with human judgments of factuality. Our analysis reveals that while the lexical metrics are ineffective, NLI-based metrics perform well, correlating with human annotations in many settings and often outperforming supervised models. However, NLI metrics are still limited, as they do not detect single-fact hallucinations well and fail for lower-resource languages. Therefore, our findings highlight the gaps in exisiting hallucination detection methods for non-English languages and motivate future research to develop more robust multilingual detection methods for LLM hallucinations.

------------

`[2402.10528] Can We Verify Step by Step for Incorrect Answer Detection? <https://arxiv.org/abs/2402.10528>`__ 我们可以一步一步地验证错误答案检测吗?

::

    replaced with revised version Sat, 15 Jun 2024 16:53:58 GMT
    Submission history From: Xin Xu [view email]
    [v1] Fri, 16 Feb 2024 09:29:50 UTC (9,130 KB)
    [v2] Sat, 15 Jun 2024 16:53:58 UTC (8,198 KB)
    Xin Xu, Shizhe Diao, Can Yang, Yang Wang

Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the answer-checking baseline by a large margin. Concretely, this resulted in an average of $5.1\%$ increase in the F1 score and $2.97\%$ improvement in AUC-PR across all 45 subsets within R2PE. We further demonstrate our PDS's efficacy in advancing open-domain QA accuracy. Data and code are available at this https URL.

------------

`[2402.10567] InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain? <https://arxiv.org/abs/2402.10567>`__ InSaAF:通过准确性和公平性纳入安全性| LLMs准备好进入印度法律领域了吗?

::

    replaced with revised version Mon, 17 Jun 2024 17:46:07 GMT
    Submission history From: Sahil Girhepuje [view email]
    [v1] Fri, 16 Feb 2024 10:54:10 UTC (2,276 KB)
    [v2] Mon, 19 Feb 2024 15:16:14 UTC (2,276 KB)
    [v3] Wed, 21 Feb 2024 05:16:49 UTC (2,276 KB)
    [v4] Mon, 17 Jun 2024 17:46:07 UTC (2,274 KB)
    Yogesh Tripathi, Raghav Donakanti, Sahil Girhepuje, Ishan Kavathekar, Bhaskara Hanuma Vedula, Gokul S Krishnan, Shreya Goyal, Anmol Goel, Balaraman Ravindran, Ponnurangam Kumaraguru

Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate that the proposed $LSS_{\beta}$ metric can effectively determine the readiness of a model for safe usage in the legal sector. We also propose finetuning pipelines, utilising specialised legal datasets, as a potential method to mitigate bias and improve model safety. The finetuning procedures on LLaMA and LLaMA--2 models increase the $LSS_{\beta}$, improving their usability in the Indian legal domain. Our code is publicly released.

------------

`[2402.10618] Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement <https://arxiv.org/abs/2402.10618>`__ 通过积极查询增强角色扮演系统:评估和改进

::

    replaced with revised version Sat, 15 Jun 2024 13:51:38 GMT
    Submission history From: Yihong Tang [view email]
    [v1] Fri, 16 Feb 2024 12:12:05 UTC (7,656 KB)
    [v2] Sat, 15 Jun 2024 13:51:38 UTC (1 KB) (withdrawn)
    Yihong Tang, Jiao Ou, Che Liu, Fuzheng Zhang, Di Zhang, Kun Gai

The advent of Large Language Models (LLMs) has propelled dialogue generation into new realms, particularly in the field of role-playing systems (RPSs). While enhanced with ordinary role-relevant training dialogues, existing LLM-based RPSs still struggle to align with roles when handling intricate and trapped queries in boundary scenarios. In this paper, we design the Modular ORchestrated Trap-setting Interaction SystEm (MORTISE) to benchmark and improve the role-playing LLMs' performance. MORTISE can produce highly role-relevant aggressive queries through the collaborative effort of multiple LLM-based modules, and formulate corresponding responses to create an adversarial training dataset via a consistent response generator. We select 190 Chinese and English roles to construct aggressive queries to benchmark existing role-playing LLMs. Through comprehensive evaluation, we find that existing models exhibit a general deficiency in role alignment capabilities. We further select 180 of the roles to collect an adversarial training dataset (named RoleAD) and retain the other 10 roles for testing. Experiments on models improved by RoleAD indicate that our adversarial dataset ameliorates this deficiency, with the improvements demonstrating a degree of generalizability in ordinary scenarios.

------------

`[2402.10646] AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation <https://arxiv.org/abs/2402.10646>`__ AbsInstruct:基于解释调优和可信度估计的llm抽象能力提取

::

    replaced with revised version Mon, 17 Jun 2024 16:39:02 GMT
    Submission history From: Zhaowei Wang [view email]
    [v1] Fri, 16 Feb 2024 12:47:11 UTC (904 KB)
    [v2] Mon, 17 Jun 2024 16:39:02 UTC (908 KB)
    Zhaowei Wang, Wei Fan, Qing Zong, Hongming Zhang, Sehyun Choi, Tianqing Fang, Xin Liu, Yangqiu Song, Ginny Y. Wong, Simon See

Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study. Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored. In this work, we design the framework AbsInstruct to enhance LLMs' abstraction ability through instruction tuning. The framework builds instructions with in-depth explanations to assist LLMs in capturing the underlying rationale of abstraction. Meanwhile, we introduce a plausibility estimator to select instructions that are more consistent with the abstraction knowledge of LLMs to be aligned. Then, our framework combines abstraction instructions with general-purpose ones to build a hybrid dataset. Extensive experiments and analyses demonstrate that our framework can considerably enhance LLMs' abstraction ability with strong generalization performance while maintaining their general instruction-following abilities.

------------

`[2402.10811] Quantifying the Persona Effect in LLM Simulations <https://arxiv.org/abs/2402.10811>`__ LLM仿真中的角色效应量化

::

    replaced with revised version Mon, 17 Jun 2024 11:06:57 GMT
    Submission history From: Tiancheng Hu [view email]
    [v1] Fri, 16 Feb 2024 16:35:35 UTC (906 KB)
    [v2] Mon, 17 Jun 2024 11:06:57 UTC (921 KB)
    Tiancheng Hu and Nigel Collier

Large language models (LLMs) have shown remarkable promise in simulating human language and behavior. This study investigates how integrating persona variables-demographic, social, and behavioral factors-impacts LLMs' ability to simulate diverse perspectives. We find that persona variables account for <10% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating persona variables via prompting in LLMs provides modest but statistically significant improvements. Persona prompting is most effective in samples where many annotators disagree, but their disagreements are relatively minor. Notably, we find a linear relationship in our setting: the stronger the correlation between persona variables and human annotations, the more accurate the LLM predictions are using persona prompting. In a zero-shot setting, a powerful 70b model with persona prompting captures 81% of the annotation variance achievable by linear regression trained on ground truth annotations. However, for most subjective NLP datasets, where persona variables have limited explanatory power, the benefits of persona prompting are limited.

------------

`[2402.10979] SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs <https://arxiv.org/abs/2402.10979>`__ SportsMetrics:混合文本和数字数据以理解llm中的信息融合

::

    replaced with revised version Sun, 16 Jun 2024 06:43:50 GMT
    Submission history From: Yebowen Hu [view email]
    [v1] Thu, 15 Feb 2024 20:26:07 UTC (9,188 KB)
    [v2] Sun, 16 Jun 2024 06:43:50 UTC (1,661 KB)
    Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, Dong Yu, Fei Liu

Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs' numerical reasoning and fusion skills.

------------

`[2402.11541] Large Language Models Can Better Understand Knowledge Graphs Than We Thought <https://arxiv.org/abs/2402.11541>`__ 大型语言模型可以比我们想象的更好地理解知识图谱

::

    replaced with revised version Sun, 16 Jun 2024 14:16:56 GMT
    Submission history From: Xinbang Dai [view email]
    [v1] Sun, 18 Feb 2024 10:44:03 UTC (416 KB)
    [v2] Tue, 9 Apr 2024 07:39:47 UTC (811 KB)
    [v3] Sun, 16 Jun 2024 14:16:56 UTC (682 KB)
    Xinbang Dai, Yuncheng Hua, Tongtong Wu, Yang Sheng, Qiu Ji, Guilin Qi

As the parameter scale of large language models (LLMs) grows, jointly training knowledge graph (KG) embeddings with model parameters to enhance LLM capabilities becomes increasingly costly. Consequently, the community has shown interest in developing prompt strategies that effectively integrate KG information into LLMs. However, the format for incorporating KGs into LLMs lacks standardization; for instance, KGs can be transformed into linearized triples or natural language (NL) text. Current prompting methods often rely on a trial-and-error approach, leaving researchers with an incomplete understanding of which KG input format best facilitates LLM comprehension of KG content. To elucidate this, we design a series of experiments to explore LLMs' understanding of different KG input formats within the context of prompt engineering. Our analysis examines both literal and attention distribution levels. Through extensive experiments, we indicate a counter-intuitive phenomenon: when addressing fact-related questions, unordered linearized triples are more effective for LLMs' understanding of KGs compared to fluent NL text. Furthermore, noisy, incomplete, or marginally relevant subgraphs can still enhance LLM performance. Finally, different LLMs have distinct preferences for different formats of organizing unordered triples.

------------

`[2402.11811] FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema <https://arxiv.org/abs/2402.11811>`__ FIPO:基于偏好数据集和模块微调模式的自由形式指令导向提示优化

::

    replaced with revised version Sun, 16 Jun 2024 10:29:03 GMT
    Submission history From: Junru Lu [view email]
    [v1] Mon, 19 Feb 2024 03:56:44 UTC (1,800 KB)
    [v2] Sun, 16 Jun 2024 10:29:03 UTC (9,810 KB)
    Junru Lu and Siyu An and Min Zhang and Yulan He and Di Yin and Xing Sun

When the quality of naive prompts is carefully optimized by human experts, the task performance of large language models (LLMs) can be significantly improved. However, expert-based prompt optimizations are expensive. Herein, some works have proposed Automatic Prompt Optimization (APO), to optimize naive prompts according to task outputs of given in-box testing models, with the help of advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing schemes suffer from poor generalization ability and privacy risk. To this end, we collect the first large-scale Prompt Optimization Preference dataset (POP), fine-tune offline local LLM-based optimizers, then fairly test with various downstream models. Our method allows accurate optimization of the core task instruction part within the naive prompt in a model-agnostic manner, and thus is named Free-from Instruction-oriented Prompt Optimization (FIPO). In specific, FIPO uses a modular APO template that dynamically integrate the naive task instruction, optional instruction responses, and optional ground truth to produce finely optimized prompts. The POP dataset is meticulously constructed using advanced LLMs, undergoing rigorous cross-validation by human experts and analytical models. Leveraging insights from the data with Tulu2 models and diverse fine-tuning strategies, we validate the efficacy of FIPO framework across five public benchmarks and three testing models. Check codes and data here: this https URL.

------------

`[2402.13035] Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models <https://arxiv.org/abs/2402.13035>`__ 学习检查:释放大型语言模型的自我纠正潜力

::

    replaced with revised version Mon, 17 Jun 2024 15:24:29 GMT
    Submission history From: Zhenyang Xiao [view email]
    [v1] Tue, 20 Feb 2024 14:23:23 UTC (170 KB)
    [v2] Fri, 23 Feb 2024 01:51:19 UTC (170 KB)
    [v3] Mon, 17 Jun 2024 15:24:29 UTC (683 KB)
    Che Zhang and Zhenyang Xiao and Chengcheng Han and Yixin Lian and Yuejian Fang

Self-correction has achieved impressive results in enhancing the style and security of the generated output from large language models (LLMs). However, recent studies suggest that self-correction might be limited or even counterproductive in reasoning tasks due to LLMs' difficulties in identifying logical mistakes.
In this paper, we aim to enhance the self-checking capabilities of LLMs by constructing training data for checking tasks. Specifically, we apply the Chain of Thought (CoT) methodology to self-checking tasks, utilizing fine-grained step-level analyses and explanations to assess the correctness of reasoning paths. We propose a specialized checking format called "Step CoT Check". Following this format, we construct a checking-correction dataset that includes detailed step-by-step analysis and checking. Then we fine-tune LLMs to enhance their error detection and correction abilities.
Our experiments demonstrate that fine-tuning with the "Step CoT Check" format significantly improves the self-checking and self-correction abilities of LLMs across multiple benchmarks. This approach outperforms other formats, especially in locating the incorrect position, with greater benefits observed in larger models.
For reproducibility, all the datasets and code are provided in this https URL.

------------

`[2402.13606] A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models <https://arxiv.org/abs/2402.13606>`__ 大型语言模型上多语言置信度估计的全面研究

::

    replaced with revised version Sun, 16 Jun 2024 07:28:26 GMT
    Submission history From: Boyang Xue [view email]
    [v1] Wed, 21 Feb 2024 08:20:06 UTC (8,951 KB)
    [v2] Sun, 16 Jun 2024 07:28:26 UTC (10,085 KB)
    Boyang Xue, Hongru Wang, Rui Wang, Sheng Wang, Zezhong Wang, Yiming Du, Kam-Fai Wong

The tendency of Large Language Models (LLMs) to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of \textbf Multi\textbf{ling}ual \textbf{Conf}idence estimation (\textsc{MlingConf}) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Subsequently, we delve into the performance of several confidence estimation methods across diverse languages and examine how these confidence scores can enhance LLM performance through self-refinement. Extensive experiments conducted on the multilingual QA dataset demonstrate that confidence estimation results vary in different languages, and the verbalized numerical confidence estimation method exhibits the best performance among most languages over other methods. Finally, the obtained confidence scores can consistently improve performance as self-refinement feedback across various languages.

------------

`[2402.13671] KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection <https://arxiv.org/abs/2402.13671>`__ KInIT在SemEval-2024任务8:多语言机器生成文本检测的微调llm

::

    replaced with revised version Mon, 17 Jun 2024 13:43:28 GMT
    Submission history From: Dominik Macko [view email]
    [v1] Wed, 21 Feb 2024 10:09:56 UTC (7,982 KB)
    [v2] Mon, 17 Jun 2024 13:43:28 UTC (7,982 KB)
    Michal Spiegel and Dominik Macko

SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection. Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts. We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification. We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance. Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner.

------------

`[2402.13731] Cracking Factual Knowledge: A Comprehensive Analysis of Degenerate Knowledge Neurons in Large Language Models <https://arxiv.org/abs/2402.13731>`__ 事实知识破解:大型语言模型中退化知识神经元的综合分析

::

    replaced with revised version Mon, 17 Jun 2024 03:44:10 GMT
    Submission history From: Chen Yuheng [view email]
    [v1] Wed, 21 Feb 2024 11:50:32 UTC (19,920 KB)
    [v2] Mon, 17 Jun 2024 03:44:10 UTC (3,328 KB)
    Yuheng Chen, Pengfei Cao, Yubo Chen, Yining Wang, Shengping Liu, Kang Liu, Jun Zhao

Large language models (LLMs) store extensive factual knowledge, but the underlying mechanisms remain unclear. Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). Despite the novelty and unique properties of this concept, it has not been rigorously defined or systematically studied. We first consider the connection weight patterns of MLP neurons and define DKNs from both structural and functional aspects. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, inspired by cognitive science, we explore the relationship between DKNs and the robustness, evolvability, and complexity of LLMs. Our execution of 34 experiments under 6 settings demonstrates the connection between DKNs and these three properties. The code will be available soon.

------------

`[2402.14296] Mitigating Biases of Large Language Models in Stance Detection with Calibration <https://arxiv.org/abs/2402.14296>`__ 用校准缓解大型语言模型在立场检测中的偏差

::

    replaced with revised version Sun, 16 Jun 2024 12:04:24 GMT
    Submission history From: Ang Li [view email]
    [v1] Thu, 22 Feb 2024 05:17:49 UTC (8,029 KB)
    [v2] Sun, 16 Jun 2024 12:04:24 UTC (342 KB)
    Ang Li, Jingqian Zhao, Bin Liang, Lin Gui, Hui Wang, Xi Zeng, Xingwei Liang, Kam-Fai Wong and Ruifeng Xu

Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to sentiment-stance spurious correlations and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). To be specific, a novel calibration network is devised to calibrate potential bias in the stance prediction of LLMs. Further, to address the challenge of effectively learning bias representations and the difficulty in the generalizability of debiasing, we construct counterfactual augmented data. This approach enhances the calibration network, facilitating the debiasing and out-of-domain generalization. Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results.

------------

`[2402.14889] COBIAS: Contextual Reliability in Bias Assessment <https://arxiv.org/abs/2402.14889>`__ COBIAS:偏倚评估中的上下文可靠性

::

    replaced with revised version Mon, 17 Jun 2024 09:01:25 GMT
    Submission history From: Priyanshul Govil [view email]
    [v1] Thu, 22 Feb 2024 10:46:11 UTC (1,674 KB)
    [v2] Mon, 17 Jun 2024 09:01:25 UTC (2,754 KB)
    Priyanshul Govil, Hemang Jain, Vamshi Krishna Bonagiri, Aman Chadha, Ponnurangam Kumaraguru, Manas Gaur, Sanorita Dey

Large Language Models (LLMs) are trained on extensive web corpora, which enable them to understand and generate human-like text. However, this training process also results in inherent biases within the models. These biases arise from web data's diverse and often uncurated nature, containing various stereotypes and prejudices. Previous works on debiasing models rely on benchmark datasets to measure their method's performance. However, these datasets suffer from several pitfalls due to the highly subjective understanding of bias, highlighting a critical need for contextual exploration. We propose understanding the context of inputs by considering the diverse situations in which they may arise. Our contribution is two-fold: (i) we augment 2,291 stereotyped statements from two existing bias-benchmark datasets with points for adding context; (ii) we develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to assess a statement's contextual reliability in measuring bias. Our metric aligns with human judgment on contextual reliability of statements (Spearman's $\rho = 0.65, p = 3.4 * 10^{-60}$) and can be used to create reliable datasets, which would assist bias mitigation works.

------------

`[2402.17916] Adversarial Math Word Problem Generation <https://arxiv.org/abs/2402.17916>`__ 对抗性数学应用题生成

::

    replaced with revised version Sat, 15 Jun 2024 22:36:20 GMT
    Submission history From: Roy Xie [view email]
    [v1] Tue, 27 Feb 2024 22:07:52 UTC (2,407 KB)
    [v2] Sat, 30 Mar 2024 04:16:20 UTC (4,508 KB)
    [v3] Sat, 15 Jun 2024 22:36:20 UTC (3,449 KB)
    Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra

Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis to investigate the cause of failure, providing further insights into the limitations of LLMs.

------------

`[2403.00165] TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision <https://arxiv.org/abs/2403.00165>`__ TELEClass:分类法充实和llm增强的最小监督层次文本分类

::

    replaced with revised version Sun, 16 Jun 2024 19:10:39 GMT
    Submission history From: Yunyi Zhang [view email]
    [v1] Thu, 29 Feb 2024 22:26:07 UTC (285 KB)
    [v2] Sun, 16 Jun 2024 19:10:39 UTC (197 KB)
    Yunyi Zhang, Ruozhen Yang, Xueqiang Xu, Rui Li, Jinfeng Xiao, Jiaming Shen, Jiawei Han

Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with the minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative features. To tackle the above challenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced weakly-supervised hierarchical text Classification, which (1) automatically enriches the label taxonomy with class-indicative terms to facilitate classifier training and (2) utilizes LLMs for both data annotation and creation tailored for the hierarchical label space. Experiments show that TELEClass can outperform previous weakly-supervised methods and LLM-based zero-shot prompting methods on two public datasets.

------------

`[2403.02839] On the Limitations of Fine-tuned Judge Models for LLM Evaluation <https://arxiv.org/abs/2403.02839>`__ 微调评判模型在LLM评估中的局限性

::

    replaced with revised version Mon, 17 Jun 2024 12:10:34 GMT
    Submission history From: Hui Huang Mr. [view email]
    [v1] Tue, 5 Mar 2024 10:20:52 UTC (2,892 KB)
    [v2] Mon, 17 Jun 2024 12:10:34 UTC (4,238 KB)
    Hui Huang, Yingqi Qu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, Tiejun Zhao

Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-source models, especially GPT-4, as the evaluator. Alternatively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. While the fine-tuned judge models are claimed to achieve comparable evaluation capability with GPT-4, in this study, we conduct an empirical study of judge models. Our findings indicate that although the fine-tuned judge models achieve high performance on in-domain test sets, even surpassing GPT-4, they underperform GPT-4 across several dimensions, including generalizability, fairness, aspect-specific evaluation, and scalability. We also reveal that the fine-tuned judge model inherently operates as a task-specific classifier, consequently imposing the limitations. Finally, we propose an effective indicator to measure the reliability of fine-tuned judges, with the aim of maximizing their utility in LLM evaluation.

------------

`[2403.02990] Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges <https://arxiv.org/abs/2403.02990>`__ 使用llm进行数据增强:数据视角、学习范式和挑战

::

    replaced with revised version Sun, 16 Jun 2024 14:50:50 GMT
    Submission history From: Bosheng Ding [view email]
    [v1] Tue, 5 Mar 2024 14:11:54 UTC (1,468 KB)
    [v2] Sun, 16 Jun 2024 14:50:50 UTC (1,470 KB)
    Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, Shafiq Joty

In the rapidly evolving field of large language models (LLMs), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of LLMs on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From both data and learning perspectives, we examine various strategies that utilize LLMs for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for diverse forms of further training. Additionally, this paper highlights the primary open challenges faced in this domain, ranging from controllable data augmentation to multi-modal data augmentation. This survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve as a comprehensive guide for researchers and practitioners.

------------

`[2403.04224] Aligners: Decoupling LLMs and Alignment <https://arxiv.org/abs/2403.04224>`__ Aligners:解耦llm和Alignment

::

    replaced with revised version Sun, 16 Jun 2024 15:59:11 GMT
    Submission history From: Lilian Ngweta [view email]
    [v1] Thu, 7 Mar 2024 04:54:56 UTC (1,033 KB)
    [v2] Mon, 11 Mar 2024 07:04:42 UTC (1,034 KB)
    [v3] Sun, 16 Jun 2024 15:59:11 UTC (1,744 KB)
    Lilian Ngweta, Mayank Agarwal, Subha Maity, Alex Gittens, Yuekai Sun, Mikhail Yurochkin

Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We use the same synthetic data to train inspectors, binary miss-alignment classification models to guide a "squad" of multiple aligners. Our empirical results demonstrate consistent improvements when applying aligner squad to various LLMs, including chat-aligned models, across several instruction-following and red-teaming datasets.

------------

`[2403.07088] SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation <https://arxiv.org/abs/2403.07088>`__ SPA:面向计算友好的云端和设备上协作Seq2seq个性化生成

::

    replaced with revised version Sun, 16 Jun 2024 19:27:11 GMT
    Submission history From: Yanming Liu [view email]
    [v1] Mon, 11 Mar 2024 18:26:02 UTC (1,663 KB)
    [v2] Sat, 25 May 2024 11:19:31 UTC (910 KB)
    [v3] Thu, 30 May 2024 05:21:23 UTC (910 KB)
    [v4] Sun, 16 Jun 2024 19:27:11 UTC (910 KB)
    Yanming Liu, Xinyue Peng, Jiannan Cao, Le Dai, Xingzu Liu, Weihao Liu

Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require substantial memory storage on low-resource devices. More critically, the computational speed on these devices is also severely limited. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and featured personal feature. Further more, SPA provides a framework to keep feature-base parameters on low computational devices while leave the parameters containing general information on the high computational devices.

------------

`[2403.07183] Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews <https://arxiv.org/abs/2403.07183>`__ 大规模监控人工智能修改内容:ChatGPT对人工智能会议同行评审影响的案例研究

::

    replaced with revised version Sat, 15 Jun 2024 05:23:06 GMT
    Submission history From: Yaohui Zhang [view email]
    [v1] Mon, 11 Mar 2024 21:51:39 UTC (570 KB)
    [v2] Sat, 15 Jun 2024 05:23:06 UTC (575 KB)
    Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, Daniel A. McFarland, James Y. Zou

We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.

------------

`[2403.09207] TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks <https://arxiv.org/abs/2403.09207>`__ TaxoLLaMA:基于wordnet的多词汇语义任务求解模型

::

    replaced with revised version Mon, 17 Jun 2024 16:43:10 GMT
    Submission history From: Viktor Moskvoretskii [view email]
    [v1] Thu, 14 Mar 2024 09:21:25 UTC (2,935 KB)
    [v2] Mon, 17 Jun 2024 16:43:10 UTC (2,958 KB)
    Viktor Moskvoretskii, Ekaterina Neminova, Alina Lobanova, Alexander Panchenko, Irina Nikishina

In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due to 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and model are available online at this https URL

------------

`[2403.10301] Uni-SMART: Universal Science Multimodal Analysis and Research Transformer <https://arxiv.org/abs/2403.10301>`__ Uni-SMART:通用科学多模态分析与研究Transformer

::

    replaced with revised version Sat, 15 Jun 2024 16:03:29 GMT
    Submission history From: Hengxing Cai [view email]
    [v1] Fri, 15 Mar 2024 13:43:47 UTC (10,981 KB)
    [v2] Sat, 15 Jun 2024 16:03:29 UTC (12,785 KB)
    Hengxing Cai, Xiaochen Cai, Shuwen Yang, Jiankun Wang, Lin Yao, Zhifeng Gao, Junhan Chang, Sihang Li, Mingjun Xu, Changxin Wang, Hongshuai Wang, Yongge Li, Mujie Lin, Yaqi Li, Yuqi Yin, Linfeng Zhang, Guolin Ke

In scientific research and its application, scientific literature analysis is crucial as it allows researchers to build on the work of others. However, the fast growth of scientific knowledge has led to a massive increase in scholarly articles, making in-depth literature analysis increasingly challenging and time-consuming. The emergence of Large Language Models (LLMs) has offered a new way to address this challenge. Known for their strong abilities in summarizing texts, LLMs are seen as a potential tool to improve the analysis of scientific literature. However, existing LLMs have their own limits. Scientific literature often includes a wide range of multimodal elements, such as tables, charts, and molecule, which are hard for text-focused LLMs to understand and analyze. This issue points to the urgent need for new solutions that can fully understand and analyze multimodal content in scientific literature. To answer this demand, we present \textbf{Uni-SMART} (Universal Science Multimodal Analysis and Research Transformer), an innovative model designed for in-depth understanding of multimodal scientific literature. Through rigorous quantitative evaluation across several domains, Uni-SMART demonstrates superior performance over other text-focused LLMs. Furthermore, our exploration extends to practical applications, including patent infringement detection and nuanced analysis of charts. These applications not only highlight Uni-SMART's adaptability but also its potential to revolutionize how we interact with scientific literature.

------------

`[2403.11456] HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models <https://arxiv.org/abs/2403.11456>`__ HateCOT:基于大型语言模型的可泛化攻击性语音检测解释增强数据集

::

    replaced with revised version Sun, 16 Jun 2024 20:55:25 GMT
    Submission history From: Huy Nghiem [view email]
    [v1] Mon, 18 Mar 2024 04:12:35 UTC (9,901 KB)
    [v2] Wed, 17 Apr 2024 16:59:35 UTC (10,385 KB)
    [v3] Sun, 16 Jun 2024 20:55:25 UTC (9,777 KB)
    Huy Nghiem, Hal Daum\'e III

The widespread use of social media necessitates reliable and efficient detection of offensive content to mitigate harmful effects. Although sophisticated models perform well on individual datasets, they often fail to generalize due to varying definitions and labeling of "offensive content." In this paper, we introduce HateCOT, an English dataset with over 52,000 samples from diverse sources, featuring explanations generated by GPT-3.5Turbo and curated by humans. We demonstrate that pretraining on HateCOT significantly enhances the performance of open-source Large Language Models on three benchmark datasets for offensive content detection in both zero-shot and few-shot settings, despite differences in domain and task. Additionally, HateCOT facilitates effective K-shot fine-tuning of LLMs with limited data and improves the quality of their explanations, as confirmed by our human evaluation.

------------

`[2403.12242] Reference-based Metrics Disprove Themselves in Question Generation <https://arxiv.org/abs/2403.12242>`__ 基于引用的指标在问题生成中证明自己是错误的

::

    replaced with revised version Mon, 17 Jun 2024 15:33:37 GMT
    Submission history From: Bang Nguyen [view email]
    [v1] Mon, 18 Mar 2024 20:47:10 UTC (215 KB)
    [v2] Mon, 17 Jun 2024 15:33:37 UTC (361 KB)
    Bang Nguyen, Mengxia Yu, Yun Huang, Meng Jiang

Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our metric accurately distinguishes between high-quality questions and flawed ones, and achieves state-of-the-art alignment with human judgment.

------------

`[2404.02657] Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models <https://arxiv.org/abs/2404.02657>`__ 对大型语言模型知识蒸馏中的Kullback-Leibler散度的再思考

::

    replaced with revised version Sun, 16 Jun 2024 14:32:48 GMT
    Submission history From: Taiqiang Wu [view email]
    [v1] Wed, 3 Apr 2024 11:40:17 UTC (301 KB)
    [v2] Sun, 16 Jun 2024 14:32:48 UTC (216 KB)
    Taiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, Ngai Wong

Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine FKL and RKL. Metric-based and GPT-4-based evaluations demonstrate that the proposed AKL outperforms the baselines across various tasks and improves the diversity and quality of generated responses.

------------

`[2404.11553] Quantifying Multilingual Performance of Large Language Models Across Languages <https://arxiv.org/abs/2404.11553>`__ 跨语言大型语言模型的多语言性能量化

::

    replaced with revised version Sun, 16 Jun 2024 08:24:32 GMT
    Submission history From: Mengnan Du [view email]
    [v1] Wed, 17 Apr 2024 16:53:16 UTC (12,004 KB)
    [v2] Sun, 16 Jun 2024 08:24:32 UTC (8,400 KB)
    Zihao Li, Yucheng Shi, Zirui Liu, Fan Yang, Ali Payani, Ninghao Liu, Mengnan Du

The development of Large Language Models (LLMs) relies on extensive text corpora, which are often unevenly distributed across languages. This imbalance results in LLMs performing significantly better on high-resource languages like English, German, and French, while their capabilities in low-resource languages remain inadequate. Currently, there is a lack of quantitative methods to evaluate the performance of LLMs in these low-resource languages. To address this gap, we propose the Language Ranker, an intrinsic metric designed to benchmark and rank languages based on LLM performance using internal representations. By comparing the LLM's internal representation of various languages against a baseline derived from English, we can assess the model's multilingual capabilities in a robust and language-agnostic manner. Our analysis reveals that high-resource languages exhibit higher similarity scores with English, demonstrating superior performance, while low-resource languages show lower similarity scores, underscoring the effectiveness of our metric in assessing language-specific capabilities. Besides, the experiments show that there is a strong correlation between the LLM's performance in different languages and the proportion of those languages in its pre-training corpus. These insights underscore the efficacy of the Language Ranker as a tool for evaluating LLM performance across different languages, particularly those with limited resources.

------------

`[2404.11972] Aligning Language Models to Explicitly Handle Ambiguity <https://arxiv.org/abs/2404.11972>`__ 对齐语言模型以明确处理歧义

::

    replaced with revised version Mon, 17 Jun 2024 03:04:32 GMT
    Submission history From: Hyuhng Joon Kim [view email]
    [v1] Thu, 18 Apr 2024 07:59:53 UTC (10,946 KB)
    [v2] Mon, 17 Jun 2024 03:04:32 UTC (8,579 KB)
    Hyuhng Joon Kim, Youna Kim, Cheonbok Park, Junyeob Kim, Choonghyun Park, Kang Min Yoo, Sang-goo Lee, Taeuk Kim

In interactions between users and language model agents, user utterances frequently exhibit ellipsis (omission of words or phrases) or imprecision (lack of exactness) to prioritize efficiency. This can lead to varying interpretations of the same input based on different assumptions or background knowledge. It is thus crucial for agents to adeptly handle the inherent ambiguity in queries to ensure reliability. However, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge. To address these issues, we propose Alignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to manage ambiguous queries by leveraging their own assessment of ambiguity (i.e., perceived ambiguity). Experimental results on question-answering datasets demonstrate that APA empowers LLMs to explicitly detect and manage ambiguous queries while retaining the ability to answer clear questions. Furthermore, our finding proves that APA excels beyond training with gold-standard labels, especially in out-of-distribution scenarios.

------------

`[2404.13599] "A good pun is its own reword": Can Large Language Models Understand Puns? <https://arxiv.org/abs/2404.13599>`__ “一个好的双关语就是它自己的复述”:大型语言模型能理解双关语吗?

::

    replaced with revised version Sun, 16 Jun 2024 11:31:18 GMT
    Submission history From: Zhijun Xu [view email]
    [v1] Sun, 21 Apr 2024 09:42:05 UTC (571 KB)
    [v2] Sun, 16 Jun 2024 11:31:18 UTC (569 KB)
    Zhijun Xu, Siyu Yuan, Lingjie Chen, Deqing Yang

Puns play a vital role in academic research due to their distinct structure and clear definition, which aid in the comprehensive analysis of linguistic humor. However, the understanding of puns in large language models (LLMs) has not been thoroughly examined, limiting their use in creative writing and humor creation. In this paper, we leverage three popular tasks, i.e., pun recognition, explanation and generation to systematically evaluate the capabilities of LLMs in pun understanding. In addition to adopting the automated evaluation metrics from prior research, we introduce new evaluation methods and metrics that are better suited to the in-context learning paradigm of LLMs. These new metrics offer a more rigorous assessment of an LLM's ability to understand puns and align more closely with human cognition than previous metrics. Our findings reveal the "lazy pun generation" pattern and identify the primary challenges LLMs encounter in understanding puns.

------------

`[2404.14469] SnapKV: LLM Knows What You are Looking for Before Generation <https://arxiv.org/abs/2404.14469>`__ SnapKV: LLM知道你在寻找什么

::

    replaced with revised version Mon, 17 Jun 2024 03:01:58 GMT
    Submission history From: Yuhong Li [view email]
    [v1] Mon, 22 Apr 2024 17:42:58 UTC (512 KB)
    [v2] Mon, 17 Jun 2024 03:01:58 UTC (633 KB)
    Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, Deming Chen

Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.
We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.

------------

`[2404.17785] Temporal Scaling Law for Large Language Models <https://arxiv.org/abs/2404.17785>`__ 大型语言模型的时间尺度律

::

    replaced with revised version Sun, 16 Jun 2024 11:06:01 GMT
    Submission history From: Yizhe Xiong [view email]
    [v1] Sat, 27 Apr 2024 05:49:11 UTC (127 KB)
    [v2] Sun, 16 Jun 2024 11:06:01 UTC (331 KB)
    Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Zhenpeng Su, Jianwei Niu, Guiguang Ding

Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps. Our temporal scaling law has broad practical applications. First, it enables direct and efficient hyperparameter selection on the target LLM, such as data mixture proportions. Secondly, viewing the LLM pre-training dynamics from the token position granularity provides some insights to enhance the understanding of LLM pre-training.

------------

`[2405.07001] Evaluating Task-based Effectiveness of MLLMs on Charts <https://arxiv.org/abs/2405.07001>`__ 在图表上评估mllm基于任务的有效性

::

    replaced with revised version Mon, 17 Jun 2024 15:44:33 GMT
    Submission history From: Lutao Yan [view email]
    [v1] Sat, 11 May 2024 12:33:46 UTC (26,853 KB)
    [v2] Mon, 17 Jun 2024 15:44:33 UTC (1 KB) (withdrawn)
    Yifan Wu, Lutao Yan, Yuyu Luo, Yunhai Wang, Nan Tang

In this paper, we explore a forward-thinking question: Is GPT-4V effective at low-level data analysis tasks on charts? To this end, we first curate a large-scale dataset, named ChartInsights, consisting of 89,388 quartets (chart, task, question, answer) and covering 10 widely-used low-level data analysis tasks on 7 chart types. Firstly, we conduct systematic evaluations to understand the capabilities and limitations of 18 advanced MLLMs, which include 12 open-source models and 6 closed-source models. Starting with a standard textual prompt approach, the average accuracy rate across the 18 MLLMs is 36.17%. Among all the models, GPT-4V achieves the highest accuracy, reaching 56.13%. To understand the limitations of multimodal large models in low-level data analysis tasks, we have designed various experiments to conduct an in-depth test of capabilities of GPT-4V. We further investigate how visual modifications to charts, such as altering visual elements (e.g. changing color schemes) and introducing perturbations (e.g. adding image noise), affect performance of GPT-4V. Secondly, we present 12 experimental findings. These findings suggest potential of GPT-4V to revolutionize interaction with charts and uncover the gap between human analytic needs and capabilities of GPT-4V. Thirdly, we propose a novel textual prompt strategy, named Chain-of-Charts, tailored for low-level analysis tasks, which boosts model performance by 24.36%, resulting in an accuracy of 80.49%. Furthermore, by incorporating a visual prompt strategy that directs attention of GPT-4V to question-relevant visual elements, we further improve accuracy to 83.83%. Our study not only sheds light on the capabilities and limitations of GPT-4V in low-level data analysis tasks but also offers valuable insights for future research.

------------

`[2405.13907] Just rephrase it! Uncertainty estimation in closed-source language models via multiple rephrased queries <https://arxiv.org/abs/2405.13907>`__ 换个说法吧!通过多重重短语查询对闭源语言模型中的不确定性估计

::

    replaced with revised version Sun, 16 Jun 2024 13:49:53 GMT
    Submission history From: Konstantinos Pitas [view email]
    [v1] Wed, 22 May 2024 18:28:26 UTC (2,752 KB)
    [v2] Sun, 16 Jun 2024 13:49:53 UTC (2,808 KB)
    Adam Yang, Chen Chen, Konstantinos Pitas

State-of-the-art large language models are sometimes distributed as open-source software but are also increasingly provided as a closed-source service. These closed-source large-language models typically see the widest usage by the public, however, they often do not provide an estimate of their uncertainty when responding to queries. As even the best models are prone to ``hallucinating" false information with high confidence, a lack of a reliable estimate of uncertainty limits the applicability of these models in critical settings. We explore estimating the uncertainty of closed-source LLMs via multiple rephrasings of an original base query. Specifically, we ask the model, multiple rephrased questions, and use the similarity of the answers as an estimate of uncertainty. We diverge from previous work in i) providing rules for rephrasing that are simple to memorize and use in practice ii) proposing a theoretical framework for why multiple rephrased queries obtain calibrated uncertainty estimates. Our method demonstrates significant improvements in the calibration of uncertainty estimates compared to the baseline and provides intuition as to how query strategies should be designed for optimal test calibration.

------------

`[2405.16282] Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models <https://arxiv.org/abs/2405.16282>`__ 幕后的信心:大型语言模型的信心概率对齐研究

::

    replaced with revised version Sat, 15 Jun 2024 22:18:06 GMT
    Submission history From: Ali Emami Dr. [view email]
    [v1] Sat, 25 May 2024 15:42:04 UTC (9,154 KB)
    [v2] Wed, 29 May 2024 13:05:16 UTC (9,152 KB)
    [v3] Mon, 3 Jun 2024 16:41:53 UTC (9,152 KB)
    [v4] Fri, 7 Jun 2024 22:48:31 UTC (2,558 KB)
    [v5] Sat, 15 Jun 2024 22:18:06 UTC (2,559 KB)
    Abhishek Kumar, Robert Morabito, Sanzhar Umbet, Jad Kabbara, and Ali Emami

As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM's internal confidence, quantified by token probabilities, to the confidence conveyed in the model's response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model's confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI's GPT-4 showed the strongest confidence-probability alignment, with an average Spearman's $\hat{\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness.

------------

`[2406.02376] Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs <https://arxiv.org/abs/2406.02376>`__ 高压缩比下保留关键信息:基于查询引导的llm压缩器

::

    replaced with revised version Mon, 17 Jun 2024 15:02:11 GMT
    Submission history From: Zhiwei Cao [view email]
    [v1] Tue, 4 Jun 2024 14:53:24 UTC (218 KB)
    [v2] Mon, 17 Jun 2024 15:02:11 UTC (218 KB)
    Zhiwei Cao, Qian Cao, Yu Lu, Ningxin Peng, Luyang Huang, Shanbo Cheng, Jinsong Su

The growing popularity of Large Language Models has sparked interest in context compression for Large Language Models (LLMs). However, the performance of previous methods degrades dramatically as compression ratios increase, sometimes even falling to the closed-book level. This decline can be attributed to the loss of key information during the compression process. Our preliminary study supports this hypothesis, emphasizing the significance of retaining key information to maintain model performance under high compression ratios. As a result, we introduce Query-Guided Compressor (QGC), which leverages queries to guide the context compression process, effectively preserving key information within the compressed context. Additionally, we employ a dynamic compression strategy. We validate the effectiveness of our proposed QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets. Experimental results show that QGC can consistently perform well even at high compression ratios, which also offers significant benefits in terms of inference cost and throughput.

------------

`[2406.04371] Phased Instruction Fine-Tuning for Large Language Models <https://arxiv.org/abs/2406.04371>`__ 大型语言模型的分阶段指令微调

::

    replaced with revised version Sun, 16 Jun 2024 21:20:29 GMT
    Submission history From: Wei Pang Xubu [view email]
    [v1] Sat, 1 Jun 2024 04:25:26 UTC (4,955 KB)
    [v2] Sun, 16 Jun 2024 21:20:29 UTC (7,535 KB)
    Wei Pang and Chuan Zhou and Xiao-Hua Zhou and Xiaojie Wang

Instruction Fine-Tuning enhances pre-trained language models from basic next-word prediction to complex instruction-following. However, existing One-off Instruction Fine-Tuning (One-off IFT) method, applied on a diverse instruction, may not effectively boost models' adherence to instructions due to the simultaneous handling of varying instruction complexities. To improve this, Phased Instruction Fine-Tuning (Phased IFT) is proposed, based on the idea that learning to follow instructions is a gradual process. It assesses instruction difficulty using GPT-4, divides the instruction data into subsets of increasing difficulty, and uptrains the model sequentially on these subsets. Experiments with Llama-2 7B/13B/70B, Llama3 8/70B and Mistral-7B models using Alpaca data show that Phased IFT significantly outperforms One-off IFT, supporting the progressive alignment hypothesis and providing a simple and efficient way to enhance large language models. Codes and datasets from our experiments are freely available at this https URL.

------------

`[2406.06326] Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching <https://arxiv.org/abs/2406.06326>`__ 自调整:指导llm通过自我教学有效地获取新知识

::

    replaced with revised version Sat, 15 Jun 2024 09:45:37 GMT
    Submission history From: Xiaoying Zhang [view email]
    [v1] Mon, 10 Jun 2024 14:42:20 UTC (4,820 KB)
    [v2] Tue, 11 Jun 2024 15:03:43 UTC (4,821 KB)
    [v3] Sat, 15 Jun 2024 09:45:37 UTC (4,821 KB)
    Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Yipeng Zhang, Haitao Mi, Helen Meng

Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. In addition, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on Llama2 family models reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge.

------------

`[2406.07794] Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests <https://arxiv.org/abs/2406.07794>`__ 通过综合生成间接用户请求，使面向任务的对话数据集更加自然

::

    replaced with revised version Sun, 16 Jun 2024 21:20:34 GMT
    Submission history From: Amogh Mannekote [view email]
    [v1] Wed, 12 Jun 2024 01:18:04 UTC (2,446 KB)
    [v2] Sun, 16 Jun 2024 21:20:34 UTC (2,447 KB)
    Amogh Mannekote, Jinseok Nam, Ziming Li, Jian Gao, Kristy Elizabeth Boyer, Bonnie J. Dorr

Indirect User Requests (IURs), such as "It's cold in here" instead of "Could you please increase the temperature?" are common in human-human task-oriented dialogue and require world knowledge and pragmatic reasoning from the listener. While large language models (LLMs) can handle these requests effectively, smaller models deployed on virtual assistants often struggle due to resource constraints. Moreover, existing task-oriented dialogue benchmarks lack sufficient examples of complex discourse phenomena such as indirectness. To address this, we propose a set of linguistic criteria along with an LLM-based pipeline for generating realistic IURs to test natural language understanding (NLU) and dialogue state tracking (DST) models before deployment in a new domain. We also release IndirectRequests, a dataset of IURs based on the Schema Guided Dialog (SGD) corpus, as a comparative testbed for evaluating the performance of smaller models in handling indirect requests.

------------

`[2406.07882] Designing a Dashboard for Transparency and Control of Conversational AI <https://arxiv.org/abs/2406.07882>`__ 为对话式AI的透明度和控制设计仪表板

::

    replaced with revised version Sat, 15 Jun 2024 05:06:29 GMT
    Submission history From: Yida Chen [view email]
    [v1] Wed, 12 Jun 2024 05:20:16 UTC (8,183 KB)
    [v2] Sat, 15 Jun 2024 05:06:29 UTC (8,184 KB)
    Yida Chen, Aoyu Wu, Trevor DePodesta, Catherine Yeh, Kenneth Li, Nicholas Castillo Marin, Oam Patel, Jan Riecke, Shivam Raval, Olivia Seow, Martin Wattenberg, Fernanda Vi\'egas

Conversational LLMs function as black box systems, leaving users guessing about why they see the output they do. This lack of transparency is potentially problematic, especially given concerns around bias and truthfulness. To address this issue, we present an end-to-end prototype-connecting interpretability techniques with user experience design-that seeks to make chatbots more transparent. We begin by showing evidence that a prominent open-source LLM has a "user model": examining the internal state of the system, we can extract data related to a user's age, gender, educational level, and socioeconomic status. Next, we describe the design of a dashboard that accompanies the chatbot interface, displaying this user model in real time. The dashboard can also be used to control the user model and the system's behavior. Finally, we discuss a study in which users conversed with the instrumented system. Our results suggest that users appreciate seeing internal states, which helped them expose biased behavior and increased their sense of control. Participants also made valuable suggestions that point to future directions for both design and machine learning research. The project page and video demo of our TalkTuner system are available at this https URL

------------

`[2406.09043] Language Models are Crossword Solvers <https://arxiv.org/abs/2406.09043>`__ 语言模型是填字游戏求解器

::

    replaced with revised version Fri, 14 Jun 2024 21:29:40 GMT
    Submission history From: Soumadeep Saha [view email]
    [v1] Thu, 13 Jun 2024 12:29:27 UTC (1,412 KB)
    [v2] Fri, 14 Jun 2024 21:29:40 UTC (1,401 KB)
    Soumadeep Saha and Sutanoya Chakraborty and Saptarshi Saha and Utpal Garain

Crosswords are a form of word puzzle that require a solver to demonstrate a high degree of proficiency in natural language understanding, wordplay, reasoning, and world knowledge, along with adherence to character and length constraints. In this paper we tackle the challenge of solving crosswords with Large Language Models (LLMs). We demonstrate that the current generation of state-of-the art (SoTA) language models show significant competence at deciphering cryptic crossword clues, and outperform previously reported SoTA results by a factor of 2-3 in relevant benchmarks. We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with LLMs for the very first time, achieving an accuracy of 93\% on New York Times crossword puzzles. Contrary to previous work in this area which concluded that LLMs lag human expert performance significantly, our research suggests this gap is a lot narrower.

------------

`[2311.16822] Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop <https://arxiv.org/abs/2311.16822>`__ 大型语言模型受其自身输出的影响:自消耗训练循环的分析

::

    replaced with revised version Mon, 17 Jun 2024 07:07:30 GMT
    Submission history From: Martin Briesch [view email]
    [v1] Tue, 28 Nov 2023 14:36:43 UTC (643 KB)
    [v2] Mon, 17 Jun 2024 07:07:30 UTC (760 KB)
    Martin Briesch, Dominik Sobania, Franz Rothlauf

Large Language Models (LLM) are already widely used to generate content for a variety of online platforms. As we are not able to safely distinguish LLM-generated content from human-produced content, LLM-generated content is used to train the next generation of LLMs, giving rise to a self-consuming training loop. From the image generation domain we know that such a self-consuming training loop reduces both quality and diversity of images finally ending in a model collapse. However, it is unclear whether this alarming effect can also be observed for LLMs. Therefore, we present the first study investigating the self-consuming training loop for LLMs. Further, we propose a novel method based on logic expressions that allows us to unambiguously verify the correctness of LLM-generated content, which is difficult for natural language text. We find that the self-consuming training loop produces correct outputs, however, the output declines in its diversity depending on the proportion of the used generated data. Fresh data can slow down this decline, but not stop it. Given these concerning results, we encourage researchers to study methods to negate this process.

------------

`[2312.04916] EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism <https://arxiv.org/abs/2312.04916>`__ EE-LLM:具有3D并行性的早期退出大型语言模型的大规模训练和推理

::

    replaced with revised version Sun, 16 Jun 2024 08:37:25 GMT
    Submission history From: Yanxi Chen [view email]
    [v1] Fri, 8 Dec 2023 09:31:50 UTC (1,625 KB)
    [v2] Thu, 1 Feb 2024 11:58:27 UTC (1,561 KB)
    [v3] Sun, 16 Jun 2024 08:37:25 UTC (1,267 KB)
    Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou

We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared to standard LLM training, as well as outstanding inference speedup without compromising output quality. To facilitate further research and adoption, we release EE-LLM at this https URL.

------------

`[2401.01335] Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models <https://arxiv.org/abs/2401.01335>`__ 自玩微调将弱语言模型转换为强语言模型

::

    replaced with revised version Fri, 14 Jun 2024 21:17:17 GMT
    Submission history From: Zixiang Chen [view email]
    [v1] Tue, 2 Jan 2024 18:53:13 UTC (833 KB)
    [v2] Mon, 12 Feb 2024 22:22:37 UTC (833 KB)
    [v3] Fri, 14 Jun 2024 21:17:17 UTC (937 KB)
    Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and Quanquan Gu

Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents. Codes are available at this https URL.

------------

`[2402.05926] On the Convergence of Zeroth-Order Federated Tuning for Large Language Models <https://arxiv.org/abs/2402.05926>`__ 大型语言模型零阶联邦调优的收敛性研究

::

    replaced with revised version Mon, 17 Jun 2024 16:00:59 GMT
    Submission history From: Daoyuan Chen [view email]
    [v1] Thu, 8 Feb 2024 18:56:40 UTC (2,467 KB)
    [v2] Tue, 20 Feb 2024 14:21:37 UTC (2,278 KB)
    [v3] Mon, 17 Jun 2024 16:00:59 UTC (2,284 KB)
    Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen

The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on clients with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we term as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as FedAvg but also significantly reduces GPU memory usage during training to levels comparable to those during inference. Moreover, the proposed personalized FL strategy that is built upon the theoretical insights to customize the client-wise learning rate can effectively accelerate loss reduction. We hope our work can help to bridge theoretical and practical aspects of federated fine-tuning for LLMs, thereby stimulating further advancements and research in this area.

------------

`[2402.12038] Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations <https://arxiv.org/abs/2402.12038>`__ 自我放大:用自事后解释改进小型语言模型

::

    replaced with revised version Mon, 17 Jun 2024 08:52:29 GMT
    Submission history From: Milan Bhan [view email]
    [v1] Mon, 19 Feb 2024 10:47:09 UTC (441 KB)
    [v2] Mon, 15 Apr 2024 14:22:05 UTC (1,791 KB)
    [v3] Mon, 17 Jun 2024 08:52:29 UTC (2,145 KB)
    Milan Bhan and Jean-Noel Vittaut and Nicolas Chesneau and Marie-Jeanne Lesot

Incorporating natural language rationales in the prompt and In-Context Learning (ICL) have led to a significant improvement of Large Language Models (LLMs) performance. However, generating high-quality rationales require human-annotation or the use of auxiliary proxy models. In this work, we propose Self-AMPLIFY to automatically generate rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on four SLMs and five datasets requiring strong reasoning abilities. Self-AMPLIFY achieves good results against competitors, leading to strong accuracy improvement. Self-AMPLIFY is the first method to apply post hoc explanation methods to autoregressive language models to generate rationales to improve their own performance in a fully automated manner.

------------

`[2402.19348] Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook <https://arxiv.org/abs/2402.19348>`__ 面向城市计算跨领域数据融合的深度学习:分类、进展与展望

::

    replaced with revised version Sun, 16 Jun 2024 10:16:00 GMT
    Submission history From: Xingchen Zou [view email]
    [v1] Thu, 29 Feb 2024 16:56:23 UTC (23,621 KB)
    [v2] Sun, 16 Jun 2024 10:16:00 UTC (26,352 KB)
    Xingchen Zou, Yibo Yan, Xixuan Hao, Yuehong Hu, Haomin Wen, Erdong Liu, Junbo Zhang, Yong Li, Tianrui Li, Yu Zheng, Yuxuan Liang

As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applications into seven types: urban planning, transportation, economy, public safety, society, environment, and energy. Compared with previous surveys, we focus more on the synergy of deep learning methods with urban computing applications. Furthermore, we shed light on the interplay between Large Language Models (LLMs) and urban computing, postulating future research directions that could revolutionize the field. We firmly believe that the taxonomy, progress, and prospects delineated in our survey stand poised to significantly enrich the research community. The summary of the comprehensive and up-to-date paper list can be found at this https URL.

------------

`[2404.03147] Eigenpruning: an Interpretability-Inspired PEFT Method <https://arxiv.org/abs/2404.03147>`__ Eigenpruning:一种基于可解释性的PEFT方法

::

    replaced with revised version Sat, 15 Jun 2024 17:56:07 GMT
    Submission history From: Tomás Vergara Browne [view email]
    [v1] Thu, 4 Apr 2024 01:42:28 UTC (163 KB)
    [v2] Mon, 29 Apr 2024 07:48:32 UTC (164 KB)
    [v3] Tue, 30 Apr 2024 01:12:37 UTC (164 KB)
    [v4] Sat, 15 Jun 2024 17:56:07 UTC (164 KB)
    Tom\'as Vergara-Browne, \'Alvaro Soto, Akiko Aizawa

We introduce eigenpruning, a method that removes singular values from weight matrices in an LLM to improve its performance in a particular task. This method is inspired by interpretability methods designed to automatically find subnetworks of a model which solve a specific task. In our tests, the pruned model outperforms the original model by a large margin, while only requiring minimal computation to prune the weight matrices. In the case of a small synthetic task in integer multiplication, the Phi-2 model can improve its accuracy in the test set from 13.75% to 97.50%. Interestingly, these results seem to indicate the existence of a computation path that can solve the task very effectively, but it was not being used by the original model. Finally, we publicly release our implementation.

------------

`[2404.04575] To Cool or not to Cool? Temperature Network Meets Large Foundation Models via DRO <https://arxiv.org/abs/2404.04575>`__ 

::

    replaced with revised version Sun, 16 Jun 2024 12:43:39 GMT
    Submission history From: Zi-Hao Qiu [view email]
    [v1] Sat, 6 Apr 2024 09:55:03 UTC (1,158 KB)
    [v2] Thu, 13 Jun 2024 16:00:06 UTC (1,158 KB)
    [v3] Sun, 16 Jun 2024 12:43:39 UTC (1,158 KB)
    Zi-Hao Qiu, Siqi Guo, Mao Xu, Tuo Zhao, Lijun Zhang, Tianbao Yang

The temperature parameter plays a profound role during training and/or inference with large foundation models (LFMs) such as large language models (LLMs) and CLIP models. Particularly, it adjusts the logits in the softmax function in LLMs, which is crucial for next token generation, and it scales the similarities in the contrastive loss for training CLIP models. A significant question remains: Is it viable to learn a neural network to predict a personalized temperature of any input data for enhancing LFMs"? In this paper, we present a principled framework for learning a small yet generalizable temperature prediction network (TempNet) to improve LFMs. Our solution is composed of a novel learning framework with a robust loss underpinned by constrained distributionally robust optimization (DRO), and a properly designed TempNet with theoretical inspiration. TempNet can be trained together with a large foundation model from scratch or learned separately given a pretrained foundation model. It is not only useful for predicting personalized temperature to promote the training of LFMs but also generalizable and transferable to new tasks. Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models, e.g. Table 1. The code to reproduce the experimental results in this paper can be found at this https URL.

------------

`[2404.08707] Large Language Model Can Continue Evolving From Mistakes <https://arxiv.org/abs/2404.08707>`__ 大型语言模型可以从错误中不断演化

::

    replaced with revised version Mon, 17 Jun 2024 11:32:29 GMT
    Submission history From: Haokun Zhao [view email]
    [v1] Thu, 11 Apr 2024 17:44:56 UTC (8,891 KB)
    [v2] Fri, 19 Apr 2024 07:22:54 UTC (1 KB) (withdrawn)
    [v3] Tue, 21 May 2024 05:20:20 UTC (8,914 KB)
    [v4] Mon, 17 Jun 2024 11:32:29 UTC (9,181 KB)
    Haokun Zhao and Haixia Han and Jie Shi and Chengyu Du and Jiaqing Liang and Yanghua Xiao

As world knowledge evolves and new task paradigms emerge, Continual Learning (CL) is crucial for keeping Large Language Models (LLMs) up-to-date and addressing their shortcomings. In practical applications, LLMs often require both continual instruction tuning (CIT) and continual pre-training (CPT) to adapt to new task paradigms and acquire necessary knowledge for task-solving. However, it remains challenging to collect CPT data that addresses the knowledge deficiencies in models while maintaining adequate volume, and improving the efficiency of utilizing this data also presents significant difficulties. Inspired by the 'summarizing mistakes' learning skill, we propose the Continue Evolving from Mistakes (CEM) method, aiming to provide a data-efficient approach for collecting CPT data and continually improving LLMs' performance through iterative evaluation and supplementation with mistake-relevant knowledge. To efficiently utilize these CPT data and mitigate forgetting, we design a novel CL training set construction paradigm that integrates parallel CIT and CPT data. Extensive experiments demonstrate the efficacy of the CEM method, achieving up to a 17% improvement in accuracy in the best case. Furthermore, additional experiments confirm the potential of combining CEM with catastrophic forgetting mitigation methods, enabling iterative and continual model evolution.

------------

`[2405.02347] COPAL: Continual Pruning in Large Language Generative Models <https://arxiv.org/abs/2405.02347>`__ COPAL:大型语言生成模型的持续剪枝

::

    replaced with revised version Fri, 14 Jun 2024 18:06:47 GMT
    Submission history From: Srikanth Malla [view email]
    [v1] Thu, 2 May 2024 18:24:41 UTC (154 KB)
    [v2] Fri, 14 Jun 2024 18:06:47 UTC (157 KB)
    Srikanth Malla, Joon Hee Choi and Chiho Choi

Adapting pre-trained large language models to different domains in natural language processing requires two key considerations: high computational demands and model's inability to continual adaptation. To simultaneously address both issues, this paper presents COPAL (COntinual Pruning in Adaptive Language settings), an algorithm developed for pruning large language generative models under a continual model adaptation setting. While avoiding resource-heavy finetuning or retraining, our pruning process is guided by the proposed sensitivity analysis. The sensitivity effectively measures model's ability to withstand perturbations introduced by the new dataset and finds model's weights that are relevant for all encountered datasets. As a result, COPAL allows seamless model adaptation to new domains while enhancing the resource efficiency. Our empirical evaluation on a various size of LLMs show that COPAL outperforms baseline models, demonstrating its efficacy in efficiency and adaptability.

------------

`[2303.13856] Unleashing GPT on the Metaverse: Savior or Destroyer? <https://arxiv.org/abs/2303.13856>`__ 在超宇宙中释放GPT:救世主还是毁灭者?

::

    replaced with revised version Sat, 15 Jun 2024 09:29:25 GMT
    Submission history From: Pengyuan Zhou [view email]
    [v1] Fri, 24 Mar 2023 08:35:37 UTC (14,206 KB)
    [v2] Wed, 12 Apr 2023 08:46:21 UTC (14,206 KB)
    [v3] Sat, 15 Jun 2024 09:29:25 UTC (14,395 KB)
    Pengyuan Zhou

Incorporating artificial intelligence (AI) technology, particularly large language models (LLMs), is becoming increasingly vital for developing immersive and interactive metaverse experiences. GPT, a representative LLM developed by OpenAI, is leading LLM development and gaining attention for its potential in building the metaverse. The article delves into the pros and cons of utilizing GPT for metaverse-based education, entertainment, personalization, and support. Dynamic and personalized experiences are possible with this technology, but there are also legitimate privacy, bias, and ethical issues to consider. This article aims to help readers understand the possible influence of GPT, according to its unique technological advantages, on the metaverse and how it may be used to effectively create a more immersive and engaging virtual environment by evaluating these opportunities and obstacles.

------------

`[2402.10340] Highlighting the Safety Concerns of Deploying LLMs/VLMs in Robotics <https://arxiv.org/abs/2402.10340>`__ 强调了在机器人中部署LLMs/ vlm的安全性问题

::

    replaced with revised version Sun, 16 Jun 2024 21:31:55 GMT
    Submission history From: Xiyang Wu [view email]
    [v1] Thu, 15 Feb 2024 22:01:45 UTC (5,613 KB)
    [v2] Mon, 19 Feb 2024 01:43:55 UTC (5,800 KB)
    [v3] Sat, 24 Feb 2024 20:34:35 UTC (5,798 KB)
    [v4] Sun, 16 Jun 2024 21:31:55 UTC (5,923 KB)
    Xiyang Wu, Souradip Chakraborty, Ruiqi Xian, Jing Liang, Tianrui Guan, Fuxiao Liu, Brian M. Sadler, Dinesh Manocha, Amrit Singh Bedi

In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works focus on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation and navigation. Despite these improvements, analyzing the safety of such systems remains underexplored yet extremely critical. LLMs and VLMs are highly susceptible to adversarial inputs, prompting a significant inquiry into the safety of robotic systems. This concern is important because robotics operate in the physical world where erroneous actions can result in severe consequences. This paper explores this issue thoroughly, presenting a mathematical formulation of potential attacks on LLM/VLM-based robotic systems and offering experimental evidence of the safety challenges. Our empirical findings highlight a significant vulnerability: simple modifications to the input can drastically reduce system effectiveness. Specifically, our results demonstrate an average performance deterioration of 19.4% under minor input prompt modifications and a more alarming 29.1% under slight perceptual changes. These findings underscore the urgent need for robust countermeasures to ensure the safe and reliable deployment of advanced LLM/VLM-based robotic systems.

------------

`[2402.14683] Visual Hallucinations of Multi-modal Large Language Models <https://arxiv.org/abs/2402.14683>`__ 多模态大型语言模型的视觉幻觉

::

    replaced with revised version Sun, 16 Jun 2024 18:43:50 GMT
    Submission history From: Hongbin Liu [view email]
    [v1] Thu, 22 Feb 2024 16:40:33 UTC (43,089 KB)
    [v2] Sun, 16 Jun 2024 18:43:50 UTC (31,218 KB)
    Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong

Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are publicly available: this https URL.

------------

`[2402.19379] Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy <https://arxiv.org/abs/2402.19379>`__ 硅群体的智慧:LLM集合预测能力可与人类群体的准确性相媲美

::

    replaced with revised version Mon, 17 Jun 2024 11:38:00 GMT
    Submission history From: Peter S. Park [view email]
    [v1] Thu, 29 Feb 2024 17:27:59 UTC (248 KB)
    [v2] Wed, 6 Mar 2024 18:44:13 UTC (248 KB)
    [v3] Fri, 3 May 2024 10:37:24 UTC (248 KB)
    [v4] Mon, 6 May 2024 10:47:59 UTC (248 KB)
    [v5] Mon, 17 Jun 2024 11:38:00 UTC (313 KB)
    Philipp Schoenegger, Indre Tuminauskaite, Peter S. Park, Rafael Valdece Sousa Bastos, Philip E. Tetlock

Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human-crowd forecasting-tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of 12 LLMs. We compare the aggregated LLM predictions on 31 binary questions to those of a crowd of 925 human forecasters from a three-month forecasting tournament. Our preregistered main analysis shows that the LLM crowd outperforms a simple no-information benchmark, and is not statistically different from the human crowd. We also observe a set of human-like biases in machine responses, such as an acquiescence effect and a tendency to favour round numbers. In Study 2, we test whether LLM predictions (of GPT-4 and Claude 2) can be improved by drawing on human cognitive output. We find that both models' forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between 17% and 28%, though this leads to less accurate predictions than simply averaging human and machine forecasts. Our results suggest that LLMs can achieve forecasting accuracy rivaling that of the human crowd: via the simple, practically applicable method of forecast aggregation.

------------

`[2403.00822] InteraRec: Screenshot Based Recommendations Using Multimodal Large Language Models <https://arxiv.org/abs/2403.00822>`__ 

::

    replaced with revised version Sun, 16 Jun 2024 00:40:15 GMT
    Submission history From: Saketh Reddy Karra [view email]
    [v1] Mon, 26 Feb 2024 17:47:57 UTC (15,376 KB)
    [v2] Sun, 16 Jun 2024 00:40:15 UTC (21,571 KB)
    Saketh Reddy Karra, Theja Tulabandhula

Weblogs, comprised of records detailing user activities on any website, offer valuable insights into user preferences, behavior, and interests. Numerous recommendation algorithms, employing strategies such as collaborative filtering, content-based filtering, and hybrid methods, leverage the data mined through these weblogs to provide personalized recommendations to users. Despite the abundance of information available in these weblogs, identifying and extracting pertinent information and key features from them necessitate extensive engineering endeavors. The intricate nature of the data also poses a challenge for interpretation, especially for non-experts. In this study, we introduce a sophisticated and interactive recommendation framework denoted as InteraRec, which diverges from conventional approaches that exclusively depend on weblogs for recommendation generation. InteraRec framework captures high-frequency screenshots of web pages as users navigate through a website. Leveraging state-of-the-art multimodal large language models (MLLMs), it extracts valuable insights into user preferences from these screenshots by generating a textual summary based on predefined keywords. Subsequently, an LLM-integrated optimization setup utilizes this summary to generate tailored recommendations. Through our experiments, we demonstrate the effectiveness of InteraRec in providing users with valuable and personalized offerings. Furthermore, we explore the integration of session-based recommendation systems into the InteraRec framework, aiming to enhance its overall performance. Finally, we curate a new dataset comprising of screenshots from product web pages on the Amazon website for the validation of the InteraRec framework. Detailed experiments demonstrate the efficacy of the InteraRec framework in delivering valuable and personalized recommendations tailored to individual user preferences.

------------

`[2403.02253] KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection <https://arxiv.org/abs/2403.02253>`__ KnowPhish:大型语言模型与多模态知识图谱相结合，增强基于参考的网络钓鱼检测

::

    replaced with revised version Sat, 15 Jun 2024 11:34:45 GMT
    Submission history From: Yuexin Li [view email]
    [v1] Mon, 4 Mar 2024 17:38:32 UTC (3,872 KB)
    [v2] Sat, 15 Jun 2024 11:34:45 UTC (7,683 KB)
    Yuexin Li, Chengyu Huang, Shumin Deng, Mei Lin Lock, Tri Cao, Nay Oo, Hoon Wei Lim, Bryan Hooi

Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches. Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach. However, a major limitation of existing RBPDs is that they rely on a manually constructed brand knowledge base, making it infeasible to scale to a large number of brands, which results in false negative errors due to the insufficient brand coverage of the knowledge base. To address this issue, we propose an automated knowledge collection pipeline, using which we collect a large-scale multimodal brand knowledge base, KnowPhish, containing 20k brands with rich information about each brand. KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner. A second limitation of existing RBPDs is that they solely rely on the image modality, ignoring useful textual information present in the webpage HTML. To utilize this textual information, we propose a Large Language Model (LLM)-based approach to extract brand information of webpages from text. Our resulting multimodal phishing detection approach, KnowPhish Detector (KPD), can detect phishing webpages with or without logos. We evaluate KnowPhish and KPD on a manually validated dataset, and a field study under Singapore's local context, showing substantial improvements in effectiveness and efficiency compared to state-of-the-art baselines.

------------

`[2404.01012] Query Performance Prediction using Relevance Judgments Generated by Large Language Models <https://arxiv.org/abs/2404.01012>`__ 基于大型语言模型生成的相关性判断的查询性能预测

::

    replaced with revised version Mon, 17 Jun 2024 11:23:20 GMT
    Submission history From: Chuan Meng [view email]
    [v1] Mon, 1 Apr 2024 09:33:05 UTC (187 KB)
    [v2] Mon, 17 Jun 2024 11:23:20 UTC (198 KB)
    Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, Maarten de Rijke

Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of predicting the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgments as pseudo-labels. This also allows us to interpret predicted IR evaluation measures, and identify, track and rectify errors in generated relevance judgments to improve QPP quality. We predict an item's relevance by using open-source large language models (LLMs) to ensure scientific reproducibility.
We face two main challenges: (i) excessive computational costs of judging an entire corpus for predicting a metric considering recall, and (ii) limited performance in prompting open-source LLMs in a zero-/few-shot manner. To solve the challenges, we devise an approximation strategy to predict an IR measure considering recall and propose to fine-tune open-source LLMs using human-labeled relevance judgments. Experiments on the TREC 2019-2022 deep learning tracks show that QPP-GenRE achieves state-of-the-art QPP quality for both lexical and neural rankers.

------------

`[2405.00021] SIMPLOT: Enhancing Chart Question Answering by Distilling Essentials <https://arxiv.org/abs/2405.00021>`__ SIMPLOT:通过提取要素增强图表问题回答

::

    replaced with revised version Mon, 17 Jun 2024 12:21:33 GMT
    Submission history From: Wonjoong Kim [view email]
    [v1] Thu, 22 Feb 2024 14:04:22 UTC (2,807 KB)
    [v2] Mon, 17 Jun 2024 12:21:33 UTC (3,616 KB)
    Wonjoong Kim, Sangwu Park, Yeonjun In, Seokwon Han, Chanyoung Park

Recently, interpreting complex charts with logical reasoning has emerged as challenges due to the development of vision-language models. A prior state-of-the-art (SOTA) model has presented an end-to-end method that leverages the vision-language model to convert charts into table format utilizing Large Language Model (LLM) for reasoning. However, unlike natural images, charts contain a mix of essential and irrelevant information required for chart reasoning, and we discover that this characteristic can lower the performance of chart-to-table extraction. In this paper, we introduce SIMPLOT, a method designed to extract only the elements necessary for chart reasoning. The proposed method involves two steps: 1) training to mimic a simple plot that contains only the essential information from a complex chart for table extraction, followed by 2) performing reasoning based on the table. Our model enables accurate chart reasoning without the need for additional annotations or datasets, and its effectiveness is demonstrated through various experiments. Furthermore, we propose a novel prompt mimicking how human interpret charts for more accurate reasoning. Our source code is available at this https URL.

------------

`[2405.17053] WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence <https://arxiv.org/abs/2405.17053>`__ WirelessLLM:面向无线智能的大型语言模型

::

    replaced with revised version Sat, 15 Jun 2024 07:01:54 GMT
    Submission history From: Jiawei Shao [view email]
    [v1] Mon, 27 May 2024 11:18:25 UTC (4,072 KB)
    [v2] Sat, 15 Jun 2024 07:01:54 UTC (4,074 KB)
    Jiawei Shao, Jingwen Tong, Qiong Wu, Wei Guo, Zijian Li, Zehong Lin, Jun Zhang

The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed. Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems. However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding. To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks. We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution. Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning. Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks. Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research.

------------

`[2405.19360] ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users <https://arxiv.org/abs/2405.19360>`__ ART:文本到图像模型的自动red - teamaming，以保护良性用户

::

    replaced with revised version Mon, 17 Jun 2024 10:00:33 GMT
    Submission history From: GuanLin Li [view email]
    [v1] Fri, 24 May 2024 07:44:27 UTC (2,674 KB)
    [v2] Mon, 17 Jun 2024 10:00:33 UTC (2,674 KB)
    Guanlin Li, Kangjie Chen, Shudong Zhang, Jie Zhang, Tianwei Zhang

Large-scale pre-trained generative models are taking the world by storm, due to their abilities in generating creative content. Meanwhile, safeguards for these generative models are developed, to protect users' rights and safety, most of which are designed for large language models. Existing methods primarily focus on jailbreak and adversarial attacks, which mainly evaluate the model's safety under malicious prompts. Recent work found that manually crafted safe prompts can unintentionally trigger unsafe generations. To further systematically evaluate the safety risks of text-to-image models, we propose a novel Automatic Red-Teaming framework, ART. Our method leverages both vision language model and large language model to establish a connection between unsafe generations and their prompts, thereby more efficiently identifying the model's vulnerabilities. With our comprehensive experiments, we reveal the toxicity of the popular open-source text-to-image models. The experiments also validate the effectiveness, adaptability, and great diversity of ART. Additionally, we introduce three large-scale red-teaming datasets for studying the safety risks associated with text-to-image models. Datasets and models can be found in this https URL.

------------

`[2405.20797] Ovis: Structural Embedding Alignment for Multimodal Large Language Model <https://arxiv.org/abs/2405.20797>`__ Ovis:多模态大型语言模型的结构嵌入对齐

::

    replaced with revised version Mon, 17 Jun 2024 17:51:50 GMT
    Submission history From: Shiyin Lu [view email]
    [v1] Fri, 31 May 2024 13:59:18 UTC (3,438 KB)
    [v2] Mon, 17 Jun 2024 17:51:50 UTC (3,439 KB)
    Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Han-Jia Ye

Current Multimodal Large Language Models (MLLMs) typically integrate a pre-trained LLM with another pre-trained vision transformer through a connector, such as an MLP, endowing the LLM with visual capabilities. However, the misalignment between two embedding strategies in MLLMs -- the structural textual embeddings based on an embedding look-up table and the continuous embeddings generated directly by the vision encoder -- makes challenges for a more seamless fusion of visual and textual information. We propose Ovis, a novel MLLM architecture designed to structurally align visual and textual embeddings. Ovis integrates an additional learnable visual embedding table into the visual encoder's process. To capture rich visual semantics, each image patch indexes the visual embedding table multiple times, resulting in a final visual embedding that is a probabilistic combination of the indexed embeddings. This structural approach mirrors the method used for generating textual embeddings. Empirical evaluations on various multimodal benchmarks show that Ovis outperforms open-source MLLMs of similar parameter scales and even surpasses the proprietary model Qwen-VL-Plus overall. These results highlight the potential of Ovis' structured visual representation for advancing MLLM architectural design and promoting more effective multimodal learning. Code, datasets, and models are available at this https URL.

------------

`[2406.08269] Analyzing constrained LLM through PDFA-learning <https://arxiv.org/abs/2406.08269>`__ 基于PDFA-learning的约束LLM分析

::

    replaced with revised version Sat, 15 Jun 2024 04:00:54 GMT
    Submission history From: Franz Mayr [view email]
    [v1] Wed, 12 Jun 2024 14:35:19 UTC (465 KB)
    [v2] Sat, 15 Jun 2024 04:00:54 UTC (467 KB)
    Mat\'ias Carrasco, Franz Mayr, Sergio Yovine, Johny Kidd, Mart\'in Iturbide, Juan Pedro da Silva, Alejo Garat

We define a congruence that copes with null next-symbol probabilities that arise when the output of a language model is constrained by some means during text generation. We develop an algorithm for efficiently learning the quotient with respect to this congruence and evaluate it on case studies for analyzing statistical properties of LLM.

------------

`[2401.02906] MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance <https://arxiv.org/abs/2401.02906>`__ MLLM- protector:确保MLLM的安全性，同时不影响性能

::

    replaced with revised version Mon, 17 Jun 2024 16:53:49 GMT
    Submission history From: Renjie Pi [view email]
    [v1] Fri, 5 Jan 2024 17:05:42 UTC (2,136 KB)
    [v2] Wed, 17 Jan 2024 12:58:36 UTC (2,136 KB)
    [v3] Mon, 17 Jun 2024 16:53:49 UTC (4,888 KB)
    Renjie Pi, Tianyang Han, Jianshu Zhang, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang

The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. This paper investigates the novel challenge of defending MLLMs against such attacks. Compared to large language models (LLMs), MLLMs include an additional image modality. We discover that images act as a ``foreign language" that is not considered during safety alignment, making MLLMs more prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover all possible scenarios. This vulnerability is exacerbated by the fact that most state-of-the-art MLLMs are fine-tuned on limited image-text pairs that are much fewer than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during safety fine-tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play strategy that solves two subtasks: 1) identifying harmful responses via a lightweight harm detector, and 2) transforming harmful responses into harmless ones via a detoxifier. This approach effectively mitigates the risks posed by malicious visual inputs without compromising the original performance of MLLMs. Our results demonstrate that MLLM-Protector offers a robust solution to a previously unaddressed aspect of MLLM security.

------------

`[2406.01946] Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature <https://arxiv.org/abs/2406.01946>`__ Bileve:利用双层签名保护大型语言模型中的文本出处不受欺骗

::

    replaced with revised version Mon, 17 Jun 2024 15:11:11 GMT
    Submission history From: Tong Zhou [view email]
    [v1] Tue, 4 Jun 2024 03:58:14 UTC (1,400 KB)
    [v2] Mon, 17 Jun 2024 15:11:11 UTC (1,401 KB)
    Tong Zhou, Xuandong Zhao, Xiaolin Xu, Shaolei Ren

Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability.

------------

`[2406.07476] VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs <https://arxiv.org/abs/2406.07476>`__ VideoLLaMA 2:推进视频llm中的时空建模和音频理解

::

    replaced with revised version Mon, 17 Jun 2024 16:40:43 GMT
    Submission history From: Sicong Leng [view email]
    [v1] Tue, 11 Jun 2024 17:22:23 UTC (1,813 KB)
    [v2] Mon, 17 Jun 2024 16:40:43 UTC (1,815 KB)
    Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, Lidong Bing

In this paper, we present the VideoLLaMA 2, a set of Video Large Language Models (Video-LLMs) designed to enhance spatial-temporal modeling and audio understanding in video and audio-oriented tasks. Building upon its predecessor, VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC) connector, which effectively captures the intricate spatial and temporal dynamics of video data. Additionally, we integrate an Audio Branch into the model through joint training, thereby enriching the multimodal understanding capabilities of the model by seamlessly incorporating audio cues. Comprehensive evaluations on multiple-choice video question answering (MC-VQA), open-ended video question answering (OE-VQA), and video captioning (VC) tasks demonstrate that VideoLLaMA 2 consistently achieves competitive results among open-source models and even gets close to some proprietary models on several benchmarks. Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and audio-video question-answering (AQA & OE-AVQA) benchmarks over existing models. These advancements underline VideoLLaMA 2's superior performance in multimodal comprehension, setting a new standard for intelligent video analysis systems. All models are public to facilitate further research.

------------

`[2401.12961] Eloquent: A More Robust Transmission Scheme for LLM Token Streaming <https://arxiv.org/abs/2401.12961>`__ Eloquent:一种更健壮的LLM令牌流传输方案

::

    replaced with revised version Sun, 16 Jun 2024 17:17:41 GMT
    Submission history From: Hanchen Li [view email]
    [v1] Tue, 23 Jan 2024 18:45:27 UTC (2,918 KB)
    [v2] Sun, 16 Jun 2024 17:17:41 UTC (2,843 KB)
    Hanchen Li, Yuhan Liu, Yihua Cheng, Siddhant Ray, Kuntai Du, Junchen Jiang

To render each generated token in real-time for users, the Large Language Model (LLM) server generates tokens one by one and streams each token (or group of a few tokens) through the network to the user right after generation, which we refer to as LLM token streaming. However, under unstable network conditions, the LLM token streaming experience could suffer greatly from stalls since one packet loss could block the rendering of later tokens even if the packets containing them arrive on time. With a measurement study, we show that current applications suffer from increased stalls under unstable networks.
For this emerging token streaming problem in LLM Chatbots that differs from previous multimedia and text applications, we propose a novel transmission scheme, called Eloquent, which puts newly generated tokens as well as currently unacknowledged tokens in the next outgoing packet. This ensures that each packet contains some new tokens and, in the meantime, is independently rendered when received, avoiding the aforementioned stalls caused by missing packets. Through simulation under various networks, we show Eloquent reduces stall ratio (proportion of token rendering wait time) by 71.0% compared to the retransmission method commonly used by real chatbot applications and by 31.6% compared to the baseline packet duplication scheme. By tailoring Eloquent to fit the token-by-token generation of LLM, we enable the Chatbots to respond like an eloquent speaker for users to better enjoy pervasive AI.

------------

`[2405.01964] Position: Understanding LLMs Requires More Than Statistical Generalization <https://arxiv.org/abs/2405.01964>`__ 职位:理解llm需要的不仅仅是统计概括

::

    replaced with revised version Mon, 17 Jun 2024 12:48:46 GMT
    Submission history From: Patrik Reizinger [view email]
    [v1] Fri, 3 May 2024 09:41:39 UTC (152 KB)
    [v2] Wed, 29 May 2024 18:22:26 UTC (154 KB)
    [v3] Mon, 17 Jun 2024 12:48:46 UTC (153 KB)
    Patrik Reizinger, Szilvia Ujv\'ary, Anna M\'esz\'aros, Anna Kerekes, Wieland Brendel, Ferenc Husz\'ar

The last decade has seen blossoming research in deep learning theory attempting to answer, "Why does deep learning generalize?" A powerful shift in perspective precipitated this progress: the study of overparametrized models in the interpolation regime. In this paper, we argue that another perspective shift is due, since some of the desirable qualities of LLMs are not a consequence of good statistical generalization and require a separate theoretical explanation. Our core argument relies on the observation that AR probabilistic models are inherently non-identifiable: models zero or near-zero KL divergence apart -- thus, equivalent test loss -- can exhibit markedly different behaviors. We support our position with mathematical examples and empirical observations, illustrating why non-identifiability has practical relevance through three case studies: (1) the non-identifiability of zero-shot rule extrapolation; (2) the approximate non-identifiability of in-context learning; and (3) the non-identifiability of fine-tunability. We review promising research directions focusing on LLM-relevant generalization measures, transferability, and inductive biases.

------------

`[2406.03636] Synthetic Programming Elicitation and Repair for Text-to-Code in Very Low-Resource Programming Languages <https://arxiv.org/abs/2406.03636>`__ 低资源编程语言文本到代码的综合编程启发与修复

::

    replaced with revised version Fri, 14 Jun 2024 22:35:33 GMT
    Submission history From: Federico Mora [view email]
    [v1] Wed, 5 Jun 2024 22:16:19 UTC (145 KB)
    [v2] Fri, 14 Jun 2024 22:35:33 UTC (149 KB)
    Federico Mora, Justin Wong, Haley Lepe, Sahil Bhatia, Karim Elmaaroufi, George Varghese, Joseph E. Gonzalez, Elizabeth Polgreen, Sanjit A. Seshia

Recent advances in large language models (LLMs) for code applications have demonstrated remarkable zero-shot fluency and instruction following on challenging code related tasks ranging from test case generation to self-repair. Unsurprisingly, however, models struggle to compose syntactically valid programs in programming languages unrepresented in pre-training, referred to as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial settings, including domain-specific languages for internal tools and tool-chains for legacy languages. Inspired by an HCI technique called natural program elicitation, we propose designing an intermediate language that LLMs ``naturally'' know how to use and which can be automatically compiled to a target VLPL. When LLMs generate code that lies outside of this intermediate language, we use compiler techniques to repair the code into programs in the intermediate language. Overall, we introduce \emph{synthetic programming elicitation and compilation} (SPEAC), an approach that enables LLMs to generate syntactically valid code even for VLPLs. We empirically evaluate the performance of SPEAC in a case study and find that, compared to existing retrieval and fine-tuning baselines, SPEAC produces syntactically correct programs significantly more frequently without sacrificing semantic correctness.

------------

-----------
Index (274)
-----------

`[2406.10249] A Reality check of the benefits of LLM in business <https://arxiv.org/abs/2406.10249>`__ 对LLM在商业中的好处的现实检查

`[2406.10268] Autograding Mathematical Induction Proofs with Natural Language Processing <https://arxiv.org/abs/2406.10268>`__ 基于自然语言处理的数学归纳法证明自动分级

`[2406.10479] Unlocking Large Language Model's Planning Capabilities with Maximum Diversity Fine-tuning <https://arxiv.org/abs/2406.10479>`__

`[2406.10504] Task Facet Learning: A Structured Approach to Prompt Optimization <https://arxiv.org/abs/2406.10504>`__ 任务层面学习:一种结构化的提示优化方法

`[2406.10515] Reactor Mk.1 performances: MMLU, HumanEval and BBH test results <https://arxiv.org/abs/2406.10515>`__ Mk.1反应堆性能:MMLU、HumanEval和BBH测试结果

`[2406.10540] Generating and Evolving Reward Functions for Highway Driving with Large Language Models <https://arxiv.org/abs/2406.10540>`__ 基于大型语言模型的高速公路驾驶奖励函数生成与演化

`[2406.10593] QDA-SQL: Questions Enhanced Dialogue Augmentation for Multi-Turn Text-to-SQL <https://arxiv.org/abs/2406.10593>`__ QDA-SQL:面向多轮文本到sql的问题增强对话增强

`[2406.10690] Bridging the Gap in Drug Safety Data Analysis: Large Language Models for SQL Query Generation <https://arxiv.org/abs/2406.10690>`__ 弥合药品安全数据分析中的差距:用于SQL查询生成的大型语言模型

`[2406.10710] SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task <https://arxiv.org/abs/2406.10710>`__ SyntheT2C:在Text2Cypher任务上为微调大型语言模型生成合成数据

`[2406.10786] Evaluating LLMs with Multiple Problems at once: A New Paradigm for Probing LLM Capabilities <https://arxiv.org/abs/2406.10786>`__

`[2406.10803] HiddenTables & PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies <https://arxiv.org/abs/2406.10803>`__ HiddenTables & PyQTax:用于TableQA的合作游戏和数据集，以确保各种分类的规模和数据隐私

`[2406.10847] TorchOpera: A Compound AI System for LLM Safety <https://arxiv.org/abs/2406.10847>`__ TorchOpera:用于LLM安全的复合AI系统

`[2406.10942] Effective Generative AI: The Human-Algorithm Centaur <https://arxiv.org/abs/2406.10942>`__ 有效的生成式人工智能:人类算法的半人马

`[2406.11301] Optimizing and Testing Instruction-Following: Analyzing the Impact of Fine-Grained Instruction Variants on instruction-tuned LLMs <https://arxiv.org/abs/2406.11301>`__

`[2406.11757] STAR: SocioTechnical Approach to Red Teaming Language Models <https://arxiv.org/abs/2406.11757>`__ STAR:红色团队语言模型的社会技术方法

`[2406.10247] QCQA: Quality and Capacity-aware grouped Query Attention <https://arxiv.org/abs/2406.10247>`__ QCQA:质量和能力感知的分组查询注意力

`[2406.10248] On the Worst Prompt Performance of Large Language Models <https://arxiv.org/abs/2406.10248>`__ 大型语言模型的最差提示性能研究

`[2406.10254] Towards Signal Processing In Large Language Models <https://arxiv.org/abs/2406.10254>`__ 大型语言模型中的信号处理

`[2406.10260] Flextron: Many-in-One Flexible Large Language Model <https://arxiv.org/abs/2406.10260>`__ Flextron:多合一灵活的大型语言模型

`[2406.10261] FoodSky: A Food-oriented Large Language Model that Passes the Chef and Dietetic Examination <https://arxiv.org/abs/2406.10261>`__

`[2406.10267] Unused information in token probability distribution of generative LLM: improving LLM reading comprehension through calculation of expected values <https://arxiv.org/abs/2406.10267>`__ 生成式LLM token概率分布中的未使用信息:通过计算期望值提高LLM阅读理解

`[2406.10269] Markov Constraint as Large Language Model Surrogate <https://arxiv.org/abs/2406.10269>`__ 马尔可夫约束作为大型语言模型代理

`[2406.10273] Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis <https://arxiv.org/abs/2406.10273>`__ Beyond Words:大型语言模型在关键任务风险分析中的可操作性

`[2406.10278] Prompt-Based Length Controlled Generation with Multiple Control Types <https://arxiv.org/abs/2406.10278>`__ 基于提示的长度控制生成具有多种控制类型

`[2406.10288] Mimicking User Data: On Mitigating Fine-Tuning Risks in Closed Large Language Models <https://arxiv.org/abs/2406.10288>`__ 模拟用户数据:降低封闭大型语言模型的微调风险

`[2406.10295] Robustness of Structured Data Extraction from In-plane Rotated Documents using Multi-Modal Large Language Models (LLM) <https://arxiv.org/abs/2406.10295>`__ 基于多模态大语言模型(LLM)的平面内旋转文档结构化数据提取的鲁棒性

`[2406.10296] CLST: Cold-Start Mitigation in Knowledge Tracing by Aligning a Generative Language Model as a Students' Knowledge Tracer <https://arxiv.org/abs/2406.10296>`__ CLST:通过将生成语言模型对齐为学生知识追踪器来缓解知识追踪中的冷启动

`[2406.10307] What is the best model? Application-driven Evaluation for Large Language Models <https://arxiv.org/abs/2406.10307>`__ 什么是最好的模型?应用驱动的大型语言模型评估

`[2406.10323] GenQA: Generating Millions of Instructions from a Handful of Prompts <https://arxiv.org/abs/2406.10323>`__ GenQA:从少量提示中生成数百万条指令

`[2406.10400] Self-Reflection Outcome is Sensitive to Prompt Construction <https://arxiv.org/abs/2406.10400>`__

`[2406.10442] Domain-Specific Shorthand for Generation Based on Context-Free Grammar <https://arxiv.org/abs/2406.10442>`__ 基于上下文无关文法生成的特定领域简写

`[2406.10459] CancerLLM: A Large Language Model in Cancer Domain <https://arxiv.org/abs/2406.10459>`__ CancerLLM:癌症领域的大型语言模型

`[2406.10486] Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender? <https://arxiv.org/abs/2406.10486>`__ 大型语言模型在招聘决策中是否会基于种族、民族和性别进行歧视?

`[2406.10492] Large Language Models as Event Forecasters <https://arxiv.org/abs/2406.10492>`__ 大型语言模型的事件预测

`[2406.10505] CroPrompt: Cross-task Interactive Prompting for Zero-shot Spoken Language Understanding <https://arxiv.org/abs/2406.10505>`__ CroPrompt:面向零样本口语理解的跨任务交互式提示

`[2406.10552] Large Language Model Enhanced Clustering for News Event Detection <https://arxiv.org/abs/2406.10552>`__

`[2406.10560] Facts-and-Feelings: Capturing both Objectivity and Subjectivity in Table-to-Text Generation <https://arxiv.org/abs/2406.10560>`__ 事实和感觉:在表到文本生成中同时捕获客观性和主观性

`[2406.10561] We Care: Multimodal Depression Detection and Knowledge Infused Mental Health Therapeutic Response Generation <https://arxiv.org/abs/2406.10561>`__ We Care:多模态抑郁检测和知识注入的心理健康治疗反应生成

`[2406.10594] BlockPruner: Fine-grained Pruning for Large Language Models <https://arxiv.org/abs/2406.10594>`__ BlockPruner:大型语言模型的细粒度剪枝

`[2406.10602] Multilingual Large Language Models and Curse of Multilinguality <https://arxiv.org/abs/2406.10602>`__ 多语言大型语言模型与多语言诅咒

`[2406.10630] Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models <https://arxiv.org/abs/2406.10630>`__ 大型语言模型联邦指令调优中的新兴安全攻防

`[2406.10701] MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding <https://arxiv.org/abs/2406.10701>`__ MIND:从大型视觉-语言模型中提取的面向电子商务购买理解的多模态购物意图

`[2406.10764] GNOME: Generating Negotiations through Open-Domain Mapping of Exchanges <https://arxiv.org/abs/2406.10764>`__ GNOME:通过交换的开放域映射生成谈判

`[2406.10773] Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles <https://arxiv.org/abs/2406.10773>`__ 用真实世界和生成的新闻文章语料库量化生成媒体偏见

`[2406.10794] Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis <https://arxiv.org/abs/2406.10794>`__ LLMs中的越狱攻击理解:表示空间分析

`[2406.10802] KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge Graphs <https://arxiv.org/abs/2406.10802>`__ KGPA:基于跨领域知识图谱的大型语言模型鲁棒性评估

`[2406.10811] LLMFactor: Extracting Profitable Factors through Prompts for Explainable Stock Movement Prediction <https://arxiv.org/abs/2406.10811>`__

`[2406.10813] Self-Evolution Fine-Tuning for Policy Optimization <https://arxiv.org/abs/2406.10813>`__ 策略优化的自进化微调

`[2406.10842] Large Language Models for Automatic Milestone Detection in Group Discussions <https://arxiv.org/abs/2406.10842>`__ 面向群组讨论里程碑自动检测的大型语言模型

`[2406.10868] Analyzing Key Neurons in Large Language Models <https://arxiv.org/abs/2406.10868>`__ 大型语言模型中的关键神经元分析

`[2406.10880] Exploring the Potential of Multimodal LLM with Knowledge-Intensive Multimodal ASR <https://arxiv.org/abs/2406.10880>`__ 利用知识密集型多模态ASR探索多模态LLM的潜力

`[2406.10881] Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals <https://arxiv.org/abs/2406.10881>`__ 教大型语言模型从自身信号表达知识边界

`[2406.10886] Distilling Opinions at Scale: Incremental Opinion Summarization using XL-OPSUMM <https://arxiv.org/abs/2406.10886>`__ 大规模观点提取:基于XL-OPSUMM的增量式观点摘要

`[2406.10922] Generating Tables from the Parametric Knowledge of Language Models <https://arxiv.org/abs/2406.10922>`__ 从语言模型的参数知识生成表格

`[2406.10950] E-Bench: Towards Evaluating the Ease-of-Use of Large Language Models <https://arxiv.org/abs/2406.10950>`__ E-Bench:大型语言模型易用性评估

`[2406.10952] Avoiding Copyright Infringement via Machine Unlearning <https://arxiv.org/abs/2406.10952>`__ 基于机器遗忘的版权侵权避免

`[2406.10957] Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence <https://arxiv.org/abs/2406.10957>`__ 下采样KL散度消除直接偏好优化的偏置长度依赖

`[2406.10965] DocNet: Semantic Structure in Inductive Bias Detection Models <https://arxiv.org/abs/2406.10965>`__

`[2406.10977] Toward Optimal LLM Alignments Using Two-Player Games <https://arxiv.org/abs/2406.10977>`__ 基于双人博弈的最佳LLM对齐

`[2406.10985] Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens <https://arxiv.org/abs/2406.10985>`__ 深呼吸:用哨兵标记增强大型语言模型的语言建模

`[2406.10991] Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers <https://arxiv.org/abs/2406.10991>`__ 自适应查询重写:根据会话答案的边际概率对重写器进行对齐

`[2406.10996] THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation <https://arxiv.org/abs/2406.10996>`__ THEANINE:基于时间线增强响应生成的长时对话记忆管理研究

`[2406.11030] FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture <https://arxiv.org/abs/2406.11030>`__

`[2406.11036] garak: A Framework for Security Probing Large Language Models <https://arxiv.org/abs/2406.11036>`__ garak:大型语言模型安全探测框架

`[2406.11044] Evaluating the Performance of Large Language Models via Debates <https://arxiv.org/abs/2406.11044>`__ 通过辩论评估大型语言模型的性能

`[2406.11050] A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners <https://arxiv.org/abs/2406.11050>`__ Token偏差一瞥:大型语言模型还不是真正的推理机

`[2406.11065] Can LLMs Understand the Implication of Emphasized Sentences in Dialogue? <https://arxiv.org/abs/2406.11065>`__ LLMs能否理解对话中强调句的含义?

`[2406.11073] Exploring the Limitations of Detecting Machine-Generated Text <https://arxiv.org/abs/2406.11073>`__ 探索检测机器生成文本的局限性

`[2406.11085] Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing <https://arxiv.org/abs/2406.11085>`__ 多源比一源好:在低资源着色中融入外部知识

`[2406.11096] The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models <https://arxiv.org/abs/2406.11096>`__

`[2406.11097] InstructCMP: Length Control in Sentence Compression through Instruction-based Large Language Models <https://arxiv.org/abs/2406.11097>`__ InstructCMP:基于指令的大型语言模型在句子压缩中的长度控制

`[2406.11102] Grading Massive Open Online Courses Using Large Language Models <https://arxiv.org/abs/2406.11102>`__ 基于大型语言模型的大规模在线开放课程评分

`[2406.11106] From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models <https://arxiv.org/abs/2406.11106>`__ 从意图到技术:大型语言模型文本水印的全面分类与挑战

`[2406.11107] Exploring Safety-Utility Trade-Offs in Personalized Language Models <https://arxiv.org/abs/2406.11107>`__ 探索个性化语言模型的安全性-效用权衡

`[2406.11109] Investigating Annotator Bias in Large Language Models for Hate Speech Detection <https://arxiv.org/abs/2406.11109>`__ 面向仇恨言论检测的大型语言模型标注者偏见研究

`[2406.11115] Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification <https://arxiv.org/abs/2406.11115>`__ 文本嫁接:文本分类中少数类的近分布弱监督

`[2406.11116] Grammaticality Representation in ChatGPT as Compared to Linguists and Laypeople <https://arxiv.org/abs/2406.11116>`__ 与语言学家和外行人比较ChatGPT中的语法表示

`[2406.11131] Are Large Language Models a Good Replacement of Taxonomies? <https://arxiv.org/abs/2406.11131>`__

`[2406.11139] Breaking Boundaries: Investigating the Effects of Model Editing on Cross-linguistic Performance <https://arxiv.org/abs/2406.11139>`__ 突破边界:研究模型编辑对跨语言表现的影响

`[2406.11149] GoldCoin: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory <https://arxiv.org/abs/2406.11149>`__ GoldCoin:基于上下文完整性理论的隐私法大型语言模型

`[2406.11162] How Good are LLMs at Relation Extraction under Low-Resource Scenario? Comprehensive Evaluation <https://arxiv.org/abs/2406.11162>`__ llm在低资源场景下的关系提取效果如何?综合评价

`[2406.11177] TIFG: Text-Informed Feature Generation with Large Language Models <https://arxiv.org/abs/2406.11177>`__ TIFG:基于大型语言模型的文本特征生成

`[2406.11190] Aligning Large Language Models from Self-Reference AI Feedback with one General Principle <https://arxiv.org/abs/2406.11190>`__ 用一个一般原则对齐自参考人工智能反馈中的大型语言模型

`[2406.11192] Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets and Languages for Open Named Entity Recognition <https://arxiv.org/abs/2406.11192>`__ 超越边界:为开放命名实体识别学习跨数据集和语言的通用实体分类法

`[2406.11193] MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model <https://arxiv.org/abs/2406.11193>`__ MMNeuron:多模态大型语言模型中神经元级特定领域解释的发现

`[2406.11201] Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models <https://arxiv.org/abs/2406.11201>`__ 微调还是微调失败?揭穿大型语言模型的性能神话

`[2406.11214] Global Data Constraints: Ethical and Effectiveness Challenges in Large Language Model <https://arxiv.org/abs/2406.11214>`__ 全局数据约束:大型语言模型中的伦理和有效性挑战

`[2406.11234] MiniConGTS: A Near Ultimate Minimalist Contrastive Grid Tagging Scheme for Aspect Sentiment Triplet Extraction <https://arxiv.org/abs/2406.11234>`__ MiniConGTS:面向方面情感三元组抽取的极简对比网格标注方案

`[2406.11238] What Kinds of Tokens Benefit from Distant Text? An Analysis on Long Context Language Modeling <https://arxiv.org/abs/2406.11238>`__ 哪些类型的标记可以从远程文本中受益?长上下文语言建模分析

`[2406.11239] Evading AI-Generated Content Detectors using Homoglyphs <https://arxiv.org/abs/2406.11239>`__ 使用同体文字规避ai生成的内容检测器

`[2406.11250] Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs <https://arxiv.org/abs/2406.11250>`__

`[2406.11260] Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection <https://arxiv.org/abs/2406.11260>`__ 基于大规模语言模型的对抗风格增强鲁棒假新闻检测

`[2406.11263] The Fall of ROME: Understanding the Collapse of LLMs in Model Editing <https://arxiv.org/abs/2406.11263>`__ 罗马的衰落:理解模型编辑中llm的崩溃

`[2406.11267] Mitigating Large Language Model Hallucination with Faithful Finetuning <https://arxiv.org/abs/2406.11267>`__ 通过忠实微调缓解大型语言模型幻觉

`[2406.11275] Self-training Large Language Models through Knowledge Detection <https://arxiv.org/abs/2406.11275>`__ 基于知识检测的大型语言模型自训练

`[2406.11278] Do Not Design, Learn: A Trainable Scoring Function for Uncertainty Estimation in Generative LLMs <https://arxiv.org/abs/2406.11278>`__ 不设计，学习:生成式llm中不确定性估计的可训练评分函数

`[2406.11341] A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences <https://arxiv.org/abs/2406.11341>`__ 大型语言模型作为软推理器的系统分析:三段论推理的情况

`[2406.11345] Full-ECE: A Metric For Token-level Calibration on Large Language Models <https://arxiv.org/abs/2406.11345>`__ Full-ECE:大型语言模型标记级校准指标

`[2406.11354] Preserving Knowledge in Large Language Model: A Model-Agnostic Self-Decompression Approach <https://arxiv.org/abs/2406.11354>`__ 大型语言模型中的知识保存:一种与模型无关的自解压方法

`[2406.11370] Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments <https://arxiv.org/abs/2406.11370>`__ 更公平的偏好引发改进的人类对齐的大型语言模型判断

`[2406.11380] A Realistic Evaluation of LLMs for Quotation Attribution in Literary Texts: A Case Study of LLaMa3 <https://arxiv.org/abs/2406.11380>`__ 文学文本中引用归因的llm现实评估:以LLaMa3为例

`[2406.11385] MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic <https://arxiv.org/abs/2406.11385>`__

`[2406.11400] Large Language Models and Knowledge Graphs for Astronomical Entity Disambiguation <https://arxiv.org/abs/2406.11400>`__ 面向天文实体消歧的大型语言模型和知识图谱

`[2406.11410] HARE: HumAn pRiors, a key to small language model Efficiency <https://arxiv.org/abs/2406.11410>`__ HARE:人类先验，小语言模型效率的关键

`[2406.11431] Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization <https://arxiv.org/abs/2406.11431>`__ 超级(官方)对齐:强模型可能在弱到强泛化中欺骗弱模型

`[2406.11455] Adaptive Reinforcement Learning Planning: Harnessing Large Language Models for Complex Information Extraction <https://arxiv.org/abs/2406.11455>`__ 自适应强化学习规划:利用大型语言模型进行复杂信息提取

`[2406.11473] Promises, Outlooks and Challenges of Diffusion Language Modeling <https://arxiv.org/abs/2406.11473>`__ 扩散语言建模的前景、展望与挑战

`[2406.11474] How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment <https://arxiv.org/abs/2406.11474>`__ 上下文对齐能走多远?探索语境内对齐的状态

`[2406.11477] Vocabulary Expansion for Low-resource Cross-lingual Transfer <https://arxiv.org/abs/2406.11477>`__

`[2406.11486] Analysing zero-shot temporal relation extraction on clinical notes using temporal consistency <https://arxiv.org/abs/2406.11486>`__ 利用时序一致性分析临床病历的零样本时序关系抽取

`[2406.11514] Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs <https://arxiv.org/abs/2406.11514>`__ 对llm消除幻觉的预设立场进行反事实辩论

`[2406.11565] Extrinsic Evaluation of Cultural Competence in Large Language Models <https://arxiv.org/abs/2406.11565>`__ 大型语言模型文化能力的外在评估

`[2406.11566] MEMLA: Enhancing Multilingual Knowledge Editing with Neuron-Masked Low-Rank Adaptation <https://arxiv.org/abs/2406.11566>`__ MEMLA:基于神经元掩码低秩自适应的多语言知识编辑增强

`[2406.11614] Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces <https://arxiv.org/abs/2406.11614>`__

`[2406.11622] Building Knowledge-Guided Lexica to Model Cultural Variation <https://arxiv.org/abs/2406.11622>`__ 构建知识引导的文化变异词典模型

`[2406.11651] A Two-dimensional Zero-shot Dialogue State Tracking Evaluation Method using GPT-4 <https://arxiv.org/abs/2406.11651>`__ 基于GPT-4的二维零样本对话状态跟踪评估方法

`[2406.11657] Can LLM be a Personalized Judge? <https://arxiv.org/abs/2406.11657>`__ LLM可以成为一个个性化的法官吗?

`[2406.11661] Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting <https://arxiv.org/abs/2406.11661>`__ 文化条件作用还是安慰剂?社会人口激励的有效性

`[2406.11668] "Not Aligned" is Not "Malicious": Being Careful about Hallucinations of Large Language Models' Jailbreak <https://arxiv.org/abs/2406.11668>`__ “不对齐”不是“恶意”:小心大型语言模型越狱的幻觉

`[2406.11674] Endor: Hardware-Friendly Sparse Format for Offloaded LLM Inference <https://arxiv.org/abs/2406.11674>`__ Endor:用于卸载LLM推理的硬件友好的稀疏格式

`[2406.11682] Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack <https://arxiv.org/abs/2406.11682>`__ 从知识到越狱:一个知识点值得一次攻击

`[2406.11683] HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing <https://arxiv.org/abs/2406.11683>`__ 好莱坞:通过角色扮演释放大型语言模型在编剧中的创造力

`[2406.11687] Tokenization Falling Short: The Curse of Tokenization <https://arxiv.org/abs/2406.11687>`__ 标记化不达标:标记化的诅咒

`[2406.11709] Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging <https://arxiv.org/abs/2406.11709>`__ 指示，而不是辅助:基于llm的多轮规划和分层提问，用于苏格拉底式代码调试

`[2406.11721] Zero-Shot Generalization during Instruction Tuning: Insights from Similarity and Granularity <https://arxiv.org/abs/2406.11721>`__

`[2406.11736] Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models <https://arxiv.org/abs/2406.11736>`__ 交互式演化:面向大型语言模型的神经符号自训练框架

`[2406.11785] CELL your Model: Contrastive Explanation Methods for Large Language Models <https://arxiv.org/abs/2406.11785>`__ CELL your Model:大型语言模型的对比解释方法

`[2406.11801] Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations <https://arxiv.org/abs/2406.11801>`__ 安全算法:基于控制参数和激活的语言模型测试时安全对齐框架

`[2406.11813] How Do Large Language Models Acquire Factual Knowledge During Pretraining? <https://arxiv.org/abs/2406.11813>`__

`[2406.11817] Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level <https://arxiv.org/abs/2406.11817>`__ 迭代长度正则化的直接偏好优化:改进7B语言模型到GPT-4水平的案例研究

`[2406.11827] WPO: Enhancing RLHF with Weighted Preference Optimization <https://arxiv.org/abs/2406.11827>`__ WPO:基于加权偏好优化的RLHF算法

`[2406.10366] Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework <https://arxiv.org/abs/2406.10366>`__ 使用Estimands框架提高AI/ML评估的有效性和实用价值

`[2406.10576] Optimization-based Structural Pruning for Large Language Models without Back-Propagation <https://arxiv.org/abs/2406.10576>`__ 基于优化的无反向传播大型语言模型结构剪枝

`[2406.10903] New Solutions on LLM Acceleration, Optimization, and Application <https://arxiv.org/abs/2406.10903>`__ LLM加速、优化和应用的新解决方案

`[2406.10918] Embodied Question Answering via Multi-LLM Systems <https://arxiv.org/abs/2406.10918>`__ 基于多llm系统的具身问答

`[2406.10976] Promoting Data and Model Privacy in Federated Learning through Quantized LoRA <https://arxiv.org/abs/2406.10976>`__ 通过量化LoRA促进联邦学习中的数据和模型隐私

`[2406.11187] Save It All: Enabling Full Parameter Tuning for Federated Large Language Models via Cycle Black Gradient Descent <https://arxiv.org/abs/2406.11187>`__ 全部保存:通过循环黑色梯度下降实现联邦大型语言模型的全参数调优

`[2406.11235] QTIP: Quantization with Trellises and Incoherence Processing <https://arxiv.org/abs/2406.11235>`__

`[2406.11257] ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking <https://arxiv.org/abs/2406.11257>`__ ExCP:基于重量动量关节收缩的极限LLM检查点压缩

`[2406.11353] $\texttt{MoE-RBench}$: Towards Building Reliable Language Models with Sparse Mixture-of-Experts <https://arxiv.org/abs/2406.11353>`__ $\texttt{MoE-RBench}$基于稀疏专家混合构建可靠的语言模型

`[2406.11391] P-TA: Using Proximal Policy Optimization to Enhance Tabular Data Augmentation via Large Language Models <https://arxiv.org/abs/2406.11391>`__ P-TA:利用近端策略优化通过大型语言模型增强表格数据扩充

`[2406.11569] Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs <https://arxiv.org/abs/2406.11569>`__ 基于空中联邦元学习的预训练和个性化微调:收敛-泛化权衡

`[2406.11675] BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models <https://arxiv.org/abs/2406.11675>`__ BLoB:基于反向传播的大型语言模型贝叶斯低秩自适应

`[2406.11686] The Role of Inherent Bellman Error in Offline Reinforcement Learning with Linear Function Approximation <https://arxiv.org/abs/2406.11686>`__ 固有Bellman误差在线性函数逼近的离线强化学习中的作用

`[2406.11715] Measuring memorization in RLHF for code completion <https://arxiv.org/abs/2406.11715>`__ 基于RLHF的代码补全记忆性度量

`[2406.11717] Refusal in Language Models Is Mediated by a Single Direction <https://arxiv.org/abs/2406.11717>`__ 语言模型中的拒绝是由单一方向调节的

`[2406.10279] We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs <https://arxiv.org/abs/2406.10279>`__ 我们有你的包裹!通过代码生成LLMs全面分析包幻觉

`[2406.10305] Unlock the Correlation between Supervised Fine-Tuning and Reinforcement Learning in Training Code Large Language Models <https://arxiv.org/abs/2406.10305>`__ 解锁在训练代码大型语言模型中监督微调和强化学习之间的相关性

`[2406.10320] Out of style: Misadventures with LLMs and code style transfer <https://arxiv.org/abs/2406.10320>`__ 过时:llm和代码风格转换的错误

`[2406.10424] What is the Visual Cognition Gap between Humans and Multimodal LLMs? <https://arxiv.org/abs/2406.10424>`__ 人类和多模态llm之间的视觉认知差距是什么?

`[2406.10450] TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation <https://arxiv.org/abs/2406.10450>`__ TokenRec:基于llm的生成式推荐的ID分词学习

`[2406.10574] Large Language Models Playing Mixed Strategy Nash Equilibrium Games <https://arxiv.org/abs/2406.10574>`__ 玩混合策略纳什均衡博弈的大型语言模型

`[2406.10591] MINT: a Multi-modal Image and Narrative Text Dubbing Dataset for Foley Audio Content Planning and Generation <https://arxiv.org/abs/2406.10591>`__ MINT:用于Foley音频内容规划和生成的多模态图像和叙事文本配音数据集

`[2406.10816] Optimization of Armv9 architecture general large language model inference performance based on Llama.cpp <https://arxiv.org/abs/2406.10816>`__

`[2406.10889] VELOCITI: Can Video-Language Models Bind Semantic Concepts through Time? <https://arxiv.org/abs/2406.10889>`__ VELOCITI:视频语言模型可以通过时间绑定语义概念吗?

`[2406.10920] Hamilton-Jacobi Based Policy-Iteration via Deep Operator Learning <https://arxiv.org/abs/2406.10920>`__ 基于深度算子学习的Hamilton-Jacobi策略迭代

`[2406.11047] Enhancing Supermarket Robot Interaction: A Multi-Level LLM Conversational Interface for Handling Diverse Customer Intents <https://arxiv.org/abs/2406.11047>`__ 增强超市机器人交互:用于处理不同客户意图的多层次LLM对话界面

`[2406.11118] Incentivizing Quality Text Generation via Statistical Contracts <https://arxiv.org/abs/2406.11118>`__ 通过统计契约激励高质量文本生成

`[2406.11156] DELRec: Distilling Sequential Pattern to Enhance LLM-based Recommendation <https://arxiv.org/abs/2406.11156>`__ DELRec:提取序列模式增强基于llm的推荐

`[2406.11227] Compound Schema Registry <https://arxiv.org/abs/2406.11227>`__

`[2406.11231] Enabling robots to follow abstract instructions and complete complex dynamic tasks <https://arxiv.org/abs/2406.11231>`__ 使机器人能够遵循抽象的指令并完成复杂的动态任务

`[2406.11232] A Collaborative Data Analytics System with Recommender for Diverse Users <https://arxiv.org/abs/2406.11232>`__ 面向多样化用户推荐的协同数据分析系统

`[2406.11248] Performance Improvement of Language-Queried Audio Source Separation Based on Caption Augmentation From Large Language Models for DCASE Challenge 2024 Task 9 <https://arxiv.org/abs/2406.11248>`__ DCASE Challenge 2024任务9中基于大型语言模型描述扩充的语言查询音源分离性能提升

`[2406.11290] Iterative Utility Judgment Framework via LLMs Inspired by Relevance in Philosophy <https://arxiv.org/abs/2406.11290>`__ 基于LLMs的迭代效用判断框架，受哲学相关性的启发

`[2406.11432] AnyTrans: Translate AnyText in the Image with Large Scale Models <https://arxiv.org/abs/2406.11432>`__ AnyTrans:用大规模模型翻译图像中的任何文本

`[2406.11548] AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation <https://arxiv.org/abs/2406.11548>`__ AIC MLLM:面向鲁棒机器人操纵的自主交互修正MLLM

`[2406.11589] CoSQA+: Enhancing Code Search Dataset with Matching Code <https://arxiv.org/abs/2406.11589>`__

`[2406.11818] Embodied Instruction Following in Unknown Environments <https://arxiv.org/abs/2406.11818>`__ 未知环境下的具身指令跟随

`[2406.11839] mDPO: Conditional Preference Optimization for Multimodal Large Language Models <https://arxiv.org/abs/2406.11839>`__ mDPO:多模态大型语言模型的条件偏好优化

`[2406.10264] Large Language Model-empowered multimodal strain sensory system for shape recognition, monitoring, and human interaction of tensegrity <https://arxiv.org/abs/2406.10264>`__ 面向形状识别、监测和张拉整体人机交互的大型语言模型授权多模态应变感觉系统

`[2406.10274] Using General Large Language Models to Classify Mathematical Documents <https://arxiv.org/abs/2406.10274>`__ 用通用大型语言模型对数学文档进行分类

`[2406.10281] Watermarking Language Models with Error Correcting Codes <https://arxiv.org/abs/2406.10281>`__

`[2406.10839] Reminding Multimodal Large Language Models of Object-aware Knowledge with Retrieved Tags <https://arxiv.org/abs/2406.10839>`__ 利用检索标签提醒多模态大型语言模型的对象感知知识

`[2406.10958] City-LEO: Toward Transparent City Management Using LLM with End-to-End Optimization <https://arxiv.org/abs/2406.10958>`__ City- leo:使用端到端优化的LLM实现透明的城市管理

`[2406.11025] Large Language Models for Dysfluency Detection in Stuttered Speech <https://arxiv.org/abs/2406.11025>`__ 用于口吃语音流畅性障碍检测的大型语言模型

`[2406.11171] SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations <https://arxiv.org/abs/2406.11171>`__ SUGARCREPE++数据集:视觉-语言模型对语义和词汇变化的敏感性

`[2406.11285] Self and Cross-Model Distillation for LLMs: Effective Methods for Refusal Pattern Alignment <https://arxiv.org/abs/2406.11285>`__ 面向llm的自模型和跨模型蒸馏:拒绝模式对齐的有效方法

`[2406.11503] GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation <https://arxiv.org/abs/2406.11503>`__ GeoGPT4V:具有几何图像生成的几何多模态大型语言模型

`[2406.11678] TourRank: Utilizing Large Language Models for Documents Ranking with a Tournament-Inspired Strategy <https://arxiv.org/abs/2406.11678>`__

`[2406.11745] Multi-Layer Ranking with Large Language Models for News Source Recommendation <https://arxiv.org/abs/2406.11745>`__ 基于大规模语言模型的多层排序新闻源推荐

`[2406.10300] Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications <https://arxiv.org/abs/2406.10300>`__ 作为软件组件的大型语言模型:集成llm应用的分类法

`[2406.10707] DataStates-LLM: Lazy Asynchronous Checkpointing for Large Language Models <https://arxiv.org/abs/2406.10707>`__ DataStates-LLM:大型语言模型的惰性异步检查点

`[2312.15692] Instruction Fusion: Advancing Prompt Evolution through Hybridization <https://arxiv.org/abs/2312.15692>`__ 指令融合:通过杂交推进快速进化

`[2312.16127] LLM-SAP: Large Language Models Situational Awareness Based Planning <https://arxiv.org/abs/2312.16127>`__ LLM-SAP:基于情景感知规划的大型语言模型

`[2401.08743] MMToM-QA: Multimodal Theory of Mind Question Answering <https://arxiv.org/abs/2401.08743>`__ MMToM-QA:多模态心智理论问答

`[2402.15729] How Do Humans Write Code? Large Models Do It the Same Way Too <https://arxiv.org/abs/2402.15729>`__ 人类是如何编写代码的?大型模型也是这样做的

`[2404.10160] Reinforcement Learning from Multi-role Debates as Feedback for Bias Mitigation in LLMs <https://arxiv.org/abs/2404.10160>`__ 多角色辩论中的强化学习作为llm中偏差缓解的反馈

`[2406.06870] What's in an embedding? Would a rose by any embedding smell as sweet? <https://arxiv.org/abs/2406.06870>`__ 嵌入中有什么?任何一种嵌入的玫瑰闻起来都那么香吗?

`[2406.10162] Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models <https://arxiv.org/abs/2406.10162>`__ 对托词的谄媚:研究大型语言模型中的奖励篡改

`[2305.03518] Black-box Prompt Tuning with Subspace Learning <https://arxiv.org/abs/2305.03518>`__ 基于子空间学习的黑盒提示调优

`[2305.14647] Scientific Opinion Summarization: Paper Meta-review Generation Dataset, Methods, and Evaluation <https://arxiv.org/abs/2305.14647>`__ 科学观点摘要:论文元评论生成数据集、方法与评估

`[2309.12030] Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation <https://arxiv.org/abs/2309.12030>`__ 广告淘金:广告文本生成的标准化与探索

`[2310.00378] ValueDCG: Measuring Comprehensive Human Value Understanding Ability of Language Models <https://arxiv.org/abs/2310.00378>`__ ValueDCG:语言模型的人类综合价值理解能力度量

`[2310.11532] Multi-stage Large Language Model Correction for Speech Recognition <https://arxiv.org/abs/2310.11532>`__ 语音识别中的多阶段大型语言模型校正

`[2310.15819] Generative Language Models Exhibit Social Identity Biases <https://arxiv.org/abs/2310.15819>`__

`[2311.04166] Perturbed examples reveal invariances shared by language models <https://arxiv.org/abs/2311.04166>`__ 扰动的例子揭示了语言模型共享的不变性

`[2311.10395] Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads <https://arxiv.org/abs/2311.10395>`__ 偏见脱吗?分析基于transformer的语言模型注意力头中的偏差

`[2312.15561] README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP <https://arxiv.org/abs/2312.15561>`__ README:通过以数据为中心的NLP桥接医学术语并为患者教育奠定理解

`[2401.04700] Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue <https://arxiv.org/abs/2401.04700>`__ 模型编辑损害了大型语言模型的通用能力:正则化

`[2401.06081] Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint <https://arxiv.org/abs/2401.06081>`__

`[2401.06431] Human-AI Collaborative Essay Scoring: A Dual-Process Framework with LLMs <https://arxiv.org/abs/2401.06431>`__ 人- ai协同作文评分:基于llm的双过程框架

`[2401.18070] Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners? <https://arxiv.org/abs/2401.18070>`__ 语言模型在解决问题时表现出与人类学习者相同的认知偏差吗?

`[2402.01740] Reducing Selection Bias in Large Language Models <https://arxiv.org/abs/2402.01740>`__ 减少大型语言模型中的选择偏差

`[2402.05201] The Effect of Sampling Temperature on Problem Solving in Large Language Models <https://arxiv.org/abs/2402.05201>`__ 采样温度对大型语言模型问题求解的影响

`[2402.08702] PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling <https://arxiv.org/abs/2402.08702>`__ 多步骤任务中的提示优化(PROMST):融合人工反馈和启发式采样

`[2402.10496] Comparing Hallucination Detection Metrics for Multilingual Generation <https://arxiv.org/abs/2402.10496>`__ 多语言生成的幻觉检测指标比较

`[2402.10528] Can We Verify Step by Step for Incorrect Answer Detection? <https://arxiv.org/abs/2402.10528>`__ 我们可以一步一步地验证错误答案检测吗?

`[2402.10567] InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain? <https://arxiv.org/abs/2402.10567>`__ InSaAF:通过准确性和公平性纳入安全性| LLMs准备好进入印度法律领域了吗?

`[2402.10618] Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement <https://arxiv.org/abs/2402.10618>`__ 通过积极查询增强角色扮演系统:评估和改进

`[2402.10646] AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation <https://arxiv.org/abs/2402.10646>`__ AbsInstruct:基于解释调优和可信度估计的llm抽象能力提取

`[2402.10811] Quantifying the Persona Effect in LLM Simulations <https://arxiv.org/abs/2402.10811>`__ LLM仿真中的角色效应量化

`[2402.10979] SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs <https://arxiv.org/abs/2402.10979>`__ SportsMetrics:混合文本和数字数据以理解llm中的信息融合

`[2402.11541] Large Language Models Can Better Understand Knowledge Graphs Than We Thought <https://arxiv.org/abs/2402.11541>`__ 大型语言模型可以比我们想象的更好地理解知识图谱

`[2402.11811] FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema <https://arxiv.org/abs/2402.11811>`__ FIPO:基于偏好数据集和模块微调模式的自由形式指令导向提示优化

`[2402.13035] Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models <https://arxiv.org/abs/2402.13035>`__ 学习检查:释放大型语言模型的自我纠正潜力

`[2402.13606] A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models <https://arxiv.org/abs/2402.13606>`__ 大型语言模型上多语言置信度估计的全面研究

`[2402.13671] KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection <https://arxiv.org/abs/2402.13671>`__ KInIT在SemEval-2024任务8:多语言机器生成文本检测的微调llm

`[2402.13731] Cracking Factual Knowledge: A Comprehensive Analysis of Degenerate Knowledge Neurons in Large Language Models <https://arxiv.org/abs/2402.13731>`__ 事实知识破解:大型语言模型中退化知识神经元的综合分析

`[2402.14296] Mitigating Biases of Large Language Models in Stance Detection with Calibration <https://arxiv.org/abs/2402.14296>`__ 用校准缓解大型语言模型在立场检测中的偏差

`[2402.14889] COBIAS: Contextual Reliability in Bias Assessment <https://arxiv.org/abs/2402.14889>`__ COBIAS:偏倚评估中的上下文可靠性

`[2402.17916] Adversarial Math Word Problem Generation <https://arxiv.org/abs/2402.17916>`__ 对抗性数学应用题生成

`[2403.00165] TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision <https://arxiv.org/abs/2403.00165>`__ TELEClass:分类法充实和llm增强的最小监督层次文本分类

`[2403.02839] On the Limitations of Fine-tuned Judge Models for LLM Evaluation <https://arxiv.org/abs/2403.02839>`__ 微调评判模型在LLM评估中的局限性

`[2403.02990] Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges <https://arxiv.org/abs/2403.02990>`__ 使用llm进行数据增强:数据视角、学习范式和挑战

`[2403.04224] Aligners: Decoupling LLMs and Alignment <https://arxiv.org/abs/2403.04224>`__ Aligners:解耦llm和Alignment

`[2403.07088] SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation <https://arxiv.org/abs/2403.07088>`__ SPA:面向计算友好的云端和设备上协作Seq2seq个性化生成

`[2403.07183] Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews <https://arxiv.org/abs/2403.07183>`__ 大规模监控人工智能修改内容:ChatGPT对人工智能会议同行评审影响的案例研究

`[2403.09207] TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks <https://arxiv.org/abs/2403.09207>`__ TaxoLLaMA:基于wordnet的多词汇语义任务求解模型

`[2403.10301] Uni-SMART: Universal Science Multimodal Analysis and Research Transformer <https://arxiv.org/abs/2403.10301>`__ Uni-SMART:通用科学多模态分析与研究Transformer

`[2403.11456] HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models <https://arxiv.org/abs/2403.11456>`__ HateCOT:基于大型语言模型的可泛化攻击性语音检测解释增强数据集

`[2403.12242] Reference-based Metrics Disprove Themselves in Question Generation <https://arxiv.org/abs/2403.12242>`__ 基于引用的指标在问题生成中证明自己是错误的

`[2404.02657] Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models <https://arxiv.org/abs/2404.02657>`__ 对大型语言模型知识蒸馏中的Kullback-Leibler散度的再思考

`[2404.11553] Quantifying Multilingual Performance of Large Language Models Across Languages <https://arxiv.org/abs/2404.11553>`__ 跨语言大型语言模型的多语言性能量化

`[2404.11972] Aligning Language Models to Explicitly Handle Ambiguity <https://arxiv.org/abs/2404.11972>`__ 对齐语言模型以明确处理歧义

`[2404.13599] "A good pun is its own reword": Can Large Language Models Understand Puns? <https://arxiv.org/abs/2404.13599>`__ “一个好的双关语就是它自己的复述”:大型语言模型能理解双关语吗?

`[2404.14469] SnapKV: LLM Knows What You are Looking for Before Generation <https://arxiv.org/abs/2404.14469>`__ SnapKV: LLM知道你在寻找什么

`[2404.17785] Temporal Scaling Law for Large Language Models <https://arxiv.org/abs/2404.17785>`__ 大型语言模型的时间尺度律

`[2405.07001] Evaluating Task-based Effectiveness of MLLMs on Charts <https://arxiv.org/abs/2405.07001>`__ 在图表上评估mllm基于任务的有效性

`[2405.13907] Just rephrase it! Uncertainty estimation in closed-source language models via multiple rephrased queries <https://arxiv.org/abs/2405.13907>`__ 换个说法吧!通过多重重短语查询对闭源语言模型中的不确定性估计

`[2405.16282] Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models <https://arxiv.org/abs/2405.16282>`__ 幕后的信心:大型语言模型的信心概率对齐研究

`[2406.02376] Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs <https://arxiv.org/abs/2406.02376>`__ 高压缩比下保留关键信息:基于查询引导的llm压缩器

`[2406.04371] Phased Instruction Fine-Tuning for Large Language Models <https://arxiv.org/abs/2406.04371>`__ 大型语言模型的分阶段指令微调

`[2406.06326] Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching <https://arxiv.org/abs/2406.06326>`__ 自调整:指导llm通过自我教学有效地获取新知识

`[2406.07794] Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests <https://arxiv.org/abs/2406.07794>`__ 通过综合生成间接用户请求，使面向任务的对话数据集更加自然

`[2406.07882] Designing a Dashboard for Transparency and Control of Conversational AI <https://arxiv.org/abs/2406.07882>`__ 为对话式AI的透明度和控制设计仪表板

`[2406.09043] Language Models are Crossword Solvers <https://arxiv.org/abs/2406.09043>`__ 语言模型是填字游戏求解器

`[2311.16822] Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop <https://arxiv.org/abs/2311.16822>`__ 大型语言模型受其自身输出的影响:自消耗训练循环的分析

`[2312.04916] EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism <https://arxiv.org/abs/2312.04916>`__ EE-LLM:具有3D并行性的早期退出大型语言模型的大规模训练和推理

`[2401.01335] Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models <https://arxiv.org/abs/2401.01335>`__ 自玩微调将弱语言模型转换为强语言模型

`[2402.05926] On the Convergence of Zeroth-Order Federated Tuning for Large Language Models <https://arxiv.org/abs/2402.05926>`__ 大型语言模型零阶联邦调优的收敛性研究

`[2402.12038] Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations <https://arxiv.org/abs/2402.12038>`__ 自我放大:用自事后解释改进小型语言模型

`[2402.19348] Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook <https://arxiv.org/abs/2402.19348>`__ 面向城市计算跨领域数据融合的深度学习:分类、进展与展望

`[2404.03147] Eigenpruning: an Interpretability-Inspired PEFT Method <https://arxiv.org/abs/2404.03147>`__ Eigenpruning:一种基于可解释性的PEFT方法

`[2404.04575] To Cool or not to Cool? Temperature Network Meets Large Foundation Models via DRO <https://arxiv.org/abs/2404.04575>`__

`[2404.08707] Large Language Model Can Continue Evolving From Mistakes <https://arxiv.org/abs/2404.08707>`__ 大型语言模型可以从错误中不断演化

`[2405.02347] COPAL: Continual Pruning in Large Language Generative Models <https://arxiv.org/abs/2405.02347>`__ COPAL:大型语言生成模型的持续剪枝

`[2303.13856] Unleashing GPT on the Metaverse: Savior or Destroyer? <https://arxiv.org/abs/2303.13856>`__ 在超宇宙中释放GPT:救世主还是毁灭者?

`[2402.10340] Highlighting the Safety Concerns of Deploying LLMs/VLMs in Robotics <https://arxiv.org/abs/2402.10340>`__ 强调了在机器人中部署LLMs/ vlm的安全性问题

`[2402.14683] Visual Hallucinations of Multi-modal Large Language Models <https://arxiv.org/abs/2402.14683>`__ 多模态大型语言模型的视觉幻觉

`[2402.19379] Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy <https://arxiv.org/abs/2402.19379>`__ 硅群体的智慧:LLM集合预测能力可与人类群体的准确性相媲美

`[2403.00822] InteraRec: Screenshot Based Recommendations Using Multimodal Large Language Models <https://arxiv.org/abs/2403.00822>`__

`[2403.02253] KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection <https://arxiv.org/abs/2403.02253>`__ KnowPhish:大型语言模型与多模态知识图谱相结合，增强基于参考的网络钓鱼检测

`[2404.01012] Query Performance Prediction using Relevance Judgments Generated by Large Language Models <https://arxiv.org/abs/2404.01012>`__ 基于大型语言模型生成的相关性判断的查询性能预测

`[2405.00021] SIMPLOT: Enhancing Chart Question Answering by Distilling Essentials <https://arxiv.org/abs/2405.00021>`__ SIMPLOT:通过提取要素增强图表问题回答

`[2405.17053] WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence <https://arxiv.org/abs/2405.17053>`__ WirelessLLM:面向无线智能的大型语言模型

`[2405.19360] ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users <https://arxiv.org/abs/2405.19360>`__ ART:文本到图像模型的自动red - teamaming，以保护良性用户

`[2405.20797] Ovis: Structural Embedding Alignment for Multimodal Large Language Model <https://arxiv.org/abs/2405.20797>`__ Ovis:多模态大型语言模型的结构嵌入对齐

`[2406.08269] Analyzing constrained LLM through PDFA-learning <https://arxiv.org/abs/2406.08269>`__ 基于PDFA-learning的约束LLM分析

`[2401.02906] MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance <https://arxiv.org/abs/2401.02906>`__ MLLM- protector:确保MLLM的安全性，同时不影响性能

`[2406.01946] Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature <https://arxiv.org/abs/2406.01946>`__ Bileve:利用双层签名保护大型语言模型中的文本出处不受欺骗

`[2406.07476] VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs <https://arxiv.org/abs/2406.07476>`__ VideoLLaMA 2:推进视频llm中的时空建模和音频理解

`[2401.12961] Eloquent: A More Robust Transmission Scheme for LLM Token Streaming <https://arxiv.org/abs/2401.12961>`__ Eloquent:一种更健壮的LLM令牌流传输方案

`[2405.01964] Position: Understanding LLMs Requires More Than Statistical Generalization <https://arxiv.org/abs/2405.01964>`__ 职位:理解llm需要的不仅仅是统计概括

`[2406.03636] Synthetic Programming Elicitation and Repair for Text-to-Code in Very Low-Resource Programming Languages <https://arxiv.org/abs/2406.03636>`__ 低资源编程语言文本到代码的综合编程启发与修复

