240612
========

----------
Survey (3)
----------

`[2406.07353] Toxic Memes: A Survey of Computational Perspectives on the Detection and Explanation of Meme Toxicities <https://arxiv.org/abs/2406.07353>`__ 有毒模因:模因毒性检测和解释的计算视角综述

::

    Tue, 11 Jun 2024 15:22:48 GMT
    Delfina Sol Martinez Pandiani, Erik Tjong Kim Sang, Davide Ceolin

Internet memes, channels for humor, social commentary, and cultural expression, are increasingly used to spread toxic messages. Studies on the computational analyses of toxic memes have significantly grown over the past five years, and the only three surveys on computational toxic meme analysis cover only work published until 2022, leading to inconsistent terminology and unexplored trends. Our work fills this gap by surveying content-based computational perspectives on toxic memes, and reviewing key developments until early 2024. Employing the PRISMA methodology, we systematically extend the previously considered papers, achieving a threefold result. First, we survey 119 new papers, analyzing 158 computational works focused on content-based toxic meme analysis. We identify over 30 datasets used in toxic meme analysis and examine their labeling systems. Second, after observing the existence of unclear definitions of meme toxicity in computational works, we introduce a new taxonomy for categorizing meme toxicity types. We also note an expansion in computational tasks beyond the simple binary classification of memes as toxic or non-toxic, indicating a shift towards achieving a nuanced comprehension of toxicity. Third, we identify three content-based dimensions of meme toxicity under automatic study: target, intent, and conveyance tactics. We develop a framework illustrating the relationships between these dimensions and meme toxicities. The survey analyzes key challenges and recent trends, such as enhanced cross-modal reasoning, integrating expert and cultural knowledge, the demand for automatic toxicity explanations, and handling meme toxicity in low-resource languages. Also, it notes the rising use of Large Language Models (LLMs) and generative AI for detecting and generating toxic memes. Finally, it proposes pathways for advancing toxic meme detection and interpretation.

------------

`[2406.06852] A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures <https://arxiv.org/abs/2406.06852>`__ 

::

    Mon, 10 Jun 2024 23:54:21 GMT
    Shuai Zhao, Meihuizi Jia, Zhongliang Guo, Leilei Gan, Jie Fu, Yichao Feng, Fengjun Pan, Luu Anh Tuan

The large language models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings. Despite the demonstrable efficacy of LMMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms. However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and attacks without fine-tuning.
Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms.

------------

`[2310.05694] A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics <https://arxiv.org/abs/2310.05694>`__ 医疗保健大型语言模型调查:从数据、技术和应用到问责和道德

::

    replaced with revised version Tue, 11 Jun 2024 13:13:59 GMT
    Submission history From: Kai He [view email]
    [v1] Mon, 9 Oct 2023 13:15:23 UTC (16,955 KB)
    [v2] Tue, 11 Jun 2024 13:13:59 UTC (19,220 KB)
    Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, Erik Cambria

The utilization of large language models (LLMs) in the Healthcare domain has generated both excitement and concern due to their ability to effectively respond to freetext queries with certain professional knowledge. This survey outlines the capabilities of the currently developed LLMs for Healthcare and explicates their development process, with the aim of providing an overview of the development roadmap from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, as well as comparing various LLMs with each other. Then we summarize related Healthcare training data, training methods, optimization strategies, and usage. Finally, the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics. Our survey provide a comprehensive investigation from perspectives of both computer science and Healthcare specialty. Besides the discussion about Healthcare concerns, we supports the computer science community by compiling a collection of open source resources, such as accessible datasets, the latest methodologies, code implementations, and evaluation benchmarks in the Github. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a shift from model-centered methodologies to data-centered methodologies. Also, we determine that the biggest obstacle of using LLMs in Healthcare are fairness, accountability, transparency and ethics.

------------

--------------
Benchmark (14)
--------------

`[2406.07275] DCA-Bench: A Benchmark for Dataset Curation Agents <https://arxiv.org/abs/2406.07275>`__ DCA-Bench:数据集策展代理基准

::

    Tue, 11 Jun 2024 14:02:23 GMT
    Benhao Huang, Yingzhuo Yu, Jin Huang, Xingjian Zhang, Jiaqi Ma

The quality of datasets plays an increasingly crucial role in the research and development of modern artificial intelligence (AI). Despite the proliferation of open dataset platforms nowadays, data quality issues, such as insufficient documentation, inaccurate annotations, and ethical concerns, remain common in datasets widely used in AI. Furthermore, these issues are often subtle and difficult to be detected by rule-based scripts, requiring expensive manual identification and verification by dataset users or maintainers. With the increasing capability of large language models (LLMs), it is promising to streamline the curation of datasets with LLM agents. In this work, as the initial step towards this goal, we propose a dataset curation agent benchmark, DCA-Bench, to measure LLM agents' capability of detecting hidden dataset quality issues. Specifically, we collect diverse real-world dataset quality issues from eight open dataset platforms as a testbed.
Additionally, to establish an automatic pipeline for evaluating the success of LLM agents, which requires a nuanced understanding of the agent outputs, we implement a dedicated Evaluator using another LLM agent. We demonstrate that the LLM-based Evaluator empirically aligns well with human evaluation, allowing reliable automatic evaluation on the proposed benchmark. We further conduct experiments on several baseline LLM agents on the proposed benchmark and demonstrate the complexity of the task, indicating that applying LLMs to real-world dataset curation still requires further in-depth exploration and innovation. Finally, the proposed benchmark can also serve as a testbed for measuring the capability of LLMs in problem discovery rather than just problem-solving. The benchmark suite is available at \url{https://github.com/TRAIS-Lab/dca-bench}.

------------

`[2406.06565] MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures <https://arxiv.org/abs/2406.06565>`__ MixEval:从LLM基准混合物中汲取大众智慧

::

    Mon, 3 Jun 2024 05:47:05 GMT
    Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, Yang You

Evaluating large language models (LLMs) is challenging. Traditional ground-truth-based benchmarks fail to capture the comprehensiveness and nuance of real-world queries, while LLM-as-judge benchmarks suffer from grading biases and limited query quantity. Both of them may also become contaminated over time. User-facing evaluation, such as Chatbot Arena, provides reliable signals but is costly and slow. In this work, we propose MixEval, a new paradigm for establishing efficient, gold-standard LLM evaluation by strategically mixing off-the-shelf benchmarks. It bridges (1) comprehensive and well-distributed real-world user queries and (2) efficient and fairly-graded ground-truth-based benchmarks, by matching queries mined from the web with similar queries from existing benchmarks. Based on MixEval, we further build MixEval-Hard, which offers more room for model improvement. Our benchmarks' advantages lie in (1) a 0.96 model ranking correlation with Chatbot Arena arising from the highly impartial query distribution and grading mechanism, (2) fast, cheap, and reproducible execution (6% of the time and cost of MMLU), and (3) dynamic evaluation enabled by the rapid and stable data update pipeline. We provide extensive meta-evaluation and analysis for our and existing LLM benchmarks to deepen the community's understanding of LLM evaluation and guide future research directions.

------------

`[2406.07057] Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study <https://arxiv.org/abs/2406.07057>`__ 多模态大型语言模型可信度基准测试:一项全面研究

::

    Tue, 11 Jun 2024 08:38:13 GMT
    Yichi Zhang, Yao Huang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Yifan Wang, Huanran Chen, Xiao Yang, Xingxing Wei, Hang Su, Yinpeng Dong, Jun Zhu

Despite the superior capabilities of Multimodal Large Language Models (MLLMs) across diverse tasks, they still face significant trustworthiness challenges.
Yet, current literature on the assessment of trustworthy MLLMs remains limited, lacking a holistic evaluation to offer thorough insights into future improvements. In this work, we establish MultiTrust, the first comprehensive and unified benchmark on the trustworthiness of MLLMs across five primary aspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark employs a rigorous evaluation strategy that addresses both multimodal risks and cross-modal impacts, encompassing 32 diverse tasks with self-curated datasets.
Extensive experiments with 21 modern MLLMs reveal some previously unexplored trustworthiness issues and risks, highlighting the complexities introduced by the multimodality and underscoring the necessity for advanced methodologies to enhance their reliability. For instance, typical proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking and adversarial attacks; MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images in inference, indicating that the multimodality amplifies the internal risks from base LLMs. Additionally, we release a scalable toolbox for standardized trustworthiness research, aiming to facilitate future advancements in this important field. Code and resources are publicly available at: https://multi-trust.github.io/.

------------

`[2406.07070] HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation <https://arxiv.org/abs/2406.07070>`__ 

::

    Tue, 11 Jun 2024 08:56:18 GMT
    Wen Luo and Tianshu Shen and Wei Li and Guangyue Peng and Richeng Xuan and Houfeng Wang and Xi Yang

Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), achieving remarkable performance across diverse tasks and enabling widespread real-world applications. However, LLMs are prone to hallucination, generating content that either conflicts with established knowledge or is unfaithful to the original sources. Existing hallucination benchmarks primarily focus on sentence- or passage-level hallucination detection, neglecting dialogue-level evaluation, hallucination localization, and rationale provision. They also predominantly target factuality hallucinations while underestimating faithfulness hallucinations, often relying on labor-intensive or non-specialized evaluators. To address these limitations, we propose HalluDial, the first comprehensive large-scale benchmark for automatic dialogue-level hallucination evaluation. HalluDial encompasses both spontaneous and induced hallucination scenarios, covering factuality and faithfulness hallucinations. The benchmark includes 4,094 dialogues with a total of 146,856 samples. Leveraging HalluDial, we conduct a comprehensive meta-evaluation of LLMs' hallucination evaluation capabilities in information-seeking dialogues and introduce a specialized judge language model, HalluJudge. The high data quality of HalluDial enables HalluJudge to achieve superior or competitive performance in hallucination evaluation, facilitating the automatic assessment of dialogue-level hallucinations in LLMs and providing valuable insights into this phenomenon. The dataset and the code are available at https://github.com/FlagOpen/HalluDial.

------------

`[2406.07545] Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena <https://arxiv.org/abs/2406.07545>`__ open - llm排行榜:从多选到开放式llm评估、基准和竞技场问题

::

    Tue, 11 Jun 2024 17:59:47 GMT
    Aidar Myrzakhan and Sondos Mahmoud Bsharat and Zhiqiang Shen

Multiple-choice questions (MCQ) are frequently used to assess large language models (LLMs). Typically, an LLM is given a question and selects the answer deemed most probable after adjustments for factors like length. Unfortunately, LLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to inherent biases of priori unbalanced probabilities, influencing the prediction of answers based on these IDs. Previous research has introduced methods to reduce this ''selection bias'' by simply permutating options on a few test samples and applying to new ones. Another problem of MCQ is the lottery ticket choice by ''random guessing''. The LLM does not learn particular knowledge, but the option is guessed correctly. This situation is especially serious for those small-scale LLMs. To address them, a more thorough approach involves shifting from MCQ to open-style questions, which can fundamentally eliminate selection bias and random guessing issues. However, transitioning causes its own set of challenges in (1) identifying suitable open-style questions and (2) validating the correctness of LLM open-style responses against human-annotated ground-truths. This work aims to tackle these significant difficulties, and establish a new LLM evaluation benchmark through entirely open-style questions.
Consequently, we introduce the Open-LLM-Leaderboard to track various LLMs' performance and reflect true capability of them, such as GPT-4o/4/3.5, Claude 3, Gemini, etc. Our code and dataset are available at https://github.com/VILA-Lab/Open-LLM-Leaderboard.

------------

`[2406.06555] An Evaluation Benchmark for Autoformalization in Lean4 <https://arxiv.org/abs/2406.06555>`__ Lean4中自动形式化的评估基准

::

    Sat, 1 Jun 2024 07:06:57 GMT
    Aryan Gulati, Devanshu Ladsaria, Shubhra Mishra, Jasdeep Sidhu, Brando Miranda

Large Language Models (LLMs) hold the potential to revolutionize autoformalization. The introduction of Lean4, a mathematical programming language, presents an unprecedented opportunity to rigorously assess the autoformalization capabilities of LLMs. This paper introduces a novel evaluation benchmark designed for Lean4, applying it to test the abilities of state-of-the-art LLMs, including GPT-3.5, GPT-4, and Gemini Pro. Our comprehensive analysis reveals that, despite recent advancements, these LLMs still exhibit limitations in autoformalization, particularly in more complex areas of mathematics. These findings underscore the need for further development in LLMs to fully harness their potential in scientific research and development. This study not only benchmarks current LLM capabilities but also sets the stage for future enhancements in autoformalization.

------------

`[2406.06647] How Efficient is LLM-Generated Code? A Rigorous & High-Standard Benchmark <https://arxiv.org/abs/2406.06647>`__ llm生成的代码的效率如何?一个严格且高标准的基准

::

    Mon, 10 Jun 2024 04:19:20 GMT
    Ruizhong Qiu, Weiliang Will Zeng, Hanghang Tong, James Ezick, Christopher Lott

The emergence of large language models (LLMs) has significantly pushed the frontiers of program synthesis. Advancement of LLM-based program synthesis calls for a thorough evaluation of LLM-generated code. Most evaluation frameworks focus on the (functional) correctness of generated code; efficiency, as an important measure of code quality, has been overlooked in existing evaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a rigorous and high-standard benchmark for evaluating the capability of LLMs in generating efficient code. Firstly, we propose a new efficiency metric called eff@k, which generalizes the pass@k metric from correctness to efficiency and appropriately handles right-censored execution time. Furthermore, we derive an unbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we also provide a numerically stable implementation for the new estimator.
Secondly, to set a high-standard for efficiency evaluation, we employ a human expert to design best algorithms and implementations as our reference solutions of efficiency, many of which are much more efficient than existing canonical solutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous evaluation, we employ a human expert to curate strong test case generators to filter out wrong code and differentiate suboptimal algorithms. An extensive study across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still fall short of generating expert-level efficient code. Using two subsets of our problem set, we demonstrate that such deficiency is because current LLMs struggle in designing advanced algorithms and are barely aware of implementation optimization. Our benchmark is publicly available at https://github.com/q-rz/enamel .

------------

`[2406.07146] Benchmarking and Boosting Radiology Report Generation for 3D High-Resolution Medical Images <https://arxiv.org/abs/2406.07146>`__ 3D高分辨率医学图像的放射学报告生成基准测试和增强

::

    Tue, 11 Jun 2024 10:45:59 GMT
    Che Liu, Zhongwei Wan, Yuqi Wang, Hui Shen, Haozhe Wang, Kangyu Zheng, Mi Zhang, Rossella Arcucci

Automatic radiology report generation can significantly benefit the labor-intensive process of report writing by radiologists, especially for 3D radiographs like CT scans, which are crucial for broad clinical diagnostics yet underexplored compared to 2D radiographs. Existing methods often handle 3D volumes either slice-wise or with aggressive downsampling due to current GPU memory limitations, which results in a loss of the inherent 3D nature and critical details. To overcome these issues, we introduce a novel framework that efficiently and effectively generates radiology reports for high-resolution (HR) 3D volumes, based on large language models (LLMs). Specifically, our framework utilizes low-resolution (LR) visual tokens as queries to mine information from HR tokens, preserving detailed HR information while reducing computational costs by only processing HR informed LR visual queries. Further benefiting the field, we curate and release BIMCV-RG, a new dataset with 5,328 HR 3D volumes and paired reports, establishing the first benchmarks for report generation from 3D HR medical images. Our method consistently surpasses existing methods on this benchmark across three different settings: normal-resolution, high-resolution inputs, and zero-shot domain transfer, all at an acceptable computational cost, trainable on a single A100-80G.

------------

`[2406.06737] Raccoon: Prompt Extraction Benchmark of LLM-Integrated Applications <https://arxiv.org/abs/2406.06737>`__ Raccoon: llm集成应用程序的提示提取基准

::

    Mon, 10 Jun 2024 18:57:22 GMT
    Junlin Wang, Tianyi Yang, Roy Xie, Bhuwan Dhingra

With the proliferation of LLM-integrated applications such as GPT-s, millions are deployed, offering valuable services through proprietary instruction prompts. These systems, however, are prone to prompt extraction attacks through meticulously designed queries. To help mitigate this problem, we introduce the Raccoon benchmark which comprehensively evaluates a model's susceptibility to prompt extraction attacks. Our novel evaluation method assesses models under both defenseless and defended scenarios, employing a dual approach to evaluate the effectiveness of existing defenses and the resilience of the models. The benchmark encompasses 14 categories of prompt extraction attacks, with additional compounded attacks that closely mimic the strategies of potential attackers, alongside a diverse collection of defense templates. This array is, to our knowledge, the most extensive compilation of prompt theft attacks and defense mechanisms to date. Our findings highlight universal susceptibility to prompt theft in the absence of defenses, with OpenAI models demonstrating notable resilience when protected. This paper aims to establish a more systematic benchmark for assessing LLM robustness against prompt extraction attacks, offering insights into their causes and potential countermeasures.
Resources of Raccoon are publicly available at https://github.com/M0gician/RaccoonBench.

------------

`[2402.04788] MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark <https://arxiv.org/abs/2402.04788>`__ MLLM-as-a-Judge:用视觉-语言基准评估多模态llm -as- judge

::

    replaced with revised version Tue, 11 Jun 2024 06:21:46 GMT
    Submission history From: Dongping Chen [view email]
    [v1] Wed, 7 Feb 2024 12:28:32 UTC (3,194 KB)
    [v2] Thu, 6 Jun 2024 13:38:13 UTC (3,443 KB)
    [v3] Tue, 11 Jun 2024 06:21:46 UTC (3,443 KB)
    Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, Lichao Sun

Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence of multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparison, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a closer examination reveals persistent challenges in the judgment capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: \url{this https URL}.

------------

`[2406.05862] II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models <https://arxiv.org/abs/2406.05862>`__ II-Bench:面向多模态大型语言模型的图像蕴涵理解基准

::

    replaced with revised version Tue, 11 Jun 2024 12:33:42 GMT
    Submission history From: Ziqiang Liu [view email]
    [v1] Sun, 9 Jun 2024 17:25:47 UTC (11,110 KB)
    [v2] Tue, 11 Jun 2024 12:33:42 UTC (11,111 KB)
    Ziqiang Liu, Feiteng Fang, Xi Feng, Xinrun Du, Chenhao Zhang, Zekun Wang, Yuelin Bai, Qixuan Zhao, Liyang Fan, Chengguang Gan, Hongquan Lin, Jiaming Li, Yuansheng Ni, Haihong Wu, Yaswanth Narsupalli, Zhigang Zheng, Chengming Li, Xiping Hu, Ruifeng Xu, Xiaojun Chen, Min Yang, Jiaheng Liu, Ruibo Liu, Wenhao Huang, Ge Zhang, Shiwen Ni

The rapid advancements in the development of multimodal large language models (MLLMs) have consistently led to new breakthroughs on various benchmarks. In response, numerous challenging and comprehensive benchmarks have been proposed to more accurately assess the capabilities of MLLMs. However, there is a dearth of exploration of the higher-order perceptual capabilities of MLLMs. To fill this gap, we propose the Image Implication understanding Benchmark, II-Bench, which aims to evaluate the model's higher-order perception of images. Through extensive experiments on II-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs attains 74.8%, whereas human accuracy averages 90%, peaking at an impressive 98%. Subsequently, MLLMs perform worse on abstract and complex images, suggesting limitations in their ability to understand high-level semantics and capture image details. Finally, it is observed that most models exhibit enhanced accuracy when image sentiment polarity hints are incorporated into the prompts. This observation underscores a notable deficiency in their inherent understanding of image sentiment. We believe that II-Bench will inspire the community to develop the next generation of MLLMs, advancing the journey towards expert artificial general intelligence (AGI). II-Bench is publicly available at this https URL.

------------

`[2406.06196] LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages <https://arxiv.org/abs/2406.06196>`__ LINGOLY:资源稀缺和灭绝语言的奥林匹克语言推理基准

::

    replaced with revised version Tue, 11 Jun 2024 10:19:52 GMT
    Submission history From: Andrew Bean [view email]
    [v1] Mon, 10 Jun 2024 11:50:29 UTC (5,073 KB)
    [v2] Tue, 11 Jun 2024 10:19:52 UTC (5,073 KB)
    Andrew M. Bean, Simi Hellsten, Harry Mayne, Jabez Magomere, Ethan A. Chi, Ryan Chi, Scott A. Hale, Hannah Rose Kirk

In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.

------------

`[2312.06722] EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning <https://arxiv.org/abs/2312.06722>`__ EgoPlan-Bench:面向人类规划的多模态大型语言模型基准测试

::

    replaced with revised version Tue, 11 Jun 2024 06:53:44 GMT
    Submission history From: Yi Chen [view email]
    [v1] Mon, 11 Dec 2023 03:35:58 UTC (1,919 KB)
    [v2] Wed, 17 Apr 2024 13:56:06 UTC (2,514 KB)
    [v3] Tue, 11 Jun 2024 06:53:44 UTC (2,237 KB)
    Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, Xihui Liu

The pursuit of artificial general intelligence (AGI) has been accelerated by Multimodal Large Language Models (MLLMs), which exhibit superior reasoning, generalization capabilities, and proficiency in processing multimodal inputs. A crucial milestone in the evolution of AGI is the attainment of human-level planning, a fundamental ability for making informed decisions in complex environments, and solving a wide range of real-world problems. Despite the impressive advancements in MLLMs, a question remains: How far are current MLLMs from achieving human-level planning? To shed light on this question, we introduce EgoPlan-Bench, a comprehensive benchmark to evaluate the planning abilities of MLLMs in real-world scenarios from an egocentric perspective, mirroring human perception. EgoPlan-Bench emphasizes the evaluation of planning capabilities of MLLMs, featuring realistic tasks, diverse action plans, and intricate visual observations. Our rigorous evaluation of a wide range of MLLMs reveals that EgoPlan-Bench poses significant challenges, highlighting a substantial scope for improvement in MLLMs to achieve human-level task planning. To facilitate this advancement, we further present EgoPlan-IT, a specialized instruction-tuning dataset that effectively enhances model performance on EgoPlan-Bench. We have made all codes, data, and a maintained benchmark leaderboard available to advance future research.

------------

`[2402.07844] Mercury: A Code Efficiency Benchmark for Code Large Language Models <https://arxiv.org/abs/2402.07844>`__ Mercury:代码大型语言模型的代码效率基准

::

    replaced with revised version Tue, 11 Jun 2024 17:44:56 GMT
    Submission history From: Mingzhe Du [view email]
    [v1] Mon, 12 Feb 2024 17:53:22 UTC (2,757 KB)
    [v2] Sat, 11 May 2024 06:21:01 UTC (3,629 KB)
    [v3] Thu, 6 Jun 2024 09:42:17 UTC (11,936 KB)
    [v4] Tue, 11 Jun 2024 17:44:56 UTC (11,936 KB)
    Mingzhe Du, Anh Tuan Luu, Bin Ji, Qian Liu, See-Kiong Ng

Amidst the recent strides in evaluating Large Language Models for Code (Code LLMs), existing benchmarks have mainly focused on the functional correctness of generated code, neglecting the importance of their computational efficiency. To fill the gap, we present Mercury, the first code efficiency benchmark for Code LLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutions that serve as real-world efficiency baselines, enabling a comprehensive analysis of the runtime distribution. Based on the distribution, we introduce a new metric Beyond, which computes a runtime-percentile-weighted Pass score to reflect functional correctness and code efficiency simultaneously. On Mercury, leading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Given that an ideal Beyond score would be aligned with the Pass score, it indicates that while Code LLMs exhibit impressive capabilities in generating functionally correct code, there remains a notable gap in their efficiency. Finally, our empirical experiments reveal that Direct Preference Optimization (DPO) serves as a robust baseline for enhancing code efficiency compared with Supervised Fine Tuning (SFT), which paves a promising avenue for future exploration of efficient code generation. Our code and data are available on GitHub: this https URL.

------------

---------------
Accelerate (11)
---------------

`[2406.06571] SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling for LLM <https://arxiv.org/abs/2406.06571>`__ SUBLLM:一种新颖高效的LLM Token序列下采样架构

::

    Mon, 3 Jun 2024 16:43:04 GMT
    Quandong Wang, Yuxuan Yuan, Xiaoyu Yang, Ruike Zhang, Kang Zhao, Wei Liu, Jian Luan, Daniel Povey and Bin Wang

While Large Language Models (LLMs) have achieved remarkable success in various fields, the efficiency of training and inference remains a major challenge. To address this issue, we propose SUBLLM, short for Subsampling-Upsampling-Bypass Large Language Model, an innovative architecture that extends the core decoder-only framework by incorporating subsampling, upsampling, and bypass modules. The subsampling modules are responsible for shortening the sequence, while the upsampling modules restore the sequence length, and the bypass modules enhance convergence. In comparison to LLaMA, the proposed SUBLLM exhibits significant enhancements in both training and inference speeds as well as memory usage, while maintaining competitive few-shot performance. During training, SUBLLM increases speeds by 26% and cuts memory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces memory by 1GB per GPU. The training and inference speeds can be enhanced by 34% and 52% respectively when the context window is expanded to 8192. We shall release the source code of the proposed architecture in the published version.

------------

`[2406.06606] Prototypical Reward Network for Data-Efficient RLHF <https://arxiv.org/abs/2406.06606>`__ 数据高效RLHF奖励网络原型

::

    Thu, 6 Jun 2024 15:23:30 GMT
    Jinghan Zhang, Xiting Wang, Yiqiao Jin, Changyu Chen, Xinhao Zhang, Kunpeng Liu

The reward model for Reinforcement Learning from Human Feedback (RLHF) has proven effective in fine-tuning Large Language Models (LLMs). Notably, collecting human feedback for RLHF can be resource-intensive and lead to scalability issues for LLMs and complex tasks. Our proposed framework Proto-RM leverages prototypical networks to enhance reward models under limited human feedback. By enabling stable and reliable structural learning from fewer samples, Proto-RM significantly enhances LLMs' adaptability and accuracy in interpreting human preferences. Extensive experiments on various datasets demonstrate that Proto-RM significantly improves the performance of reward models and LLMs in human feedback tasks, achieving comparable and usually better results than traditional methods, while requiring significantly less data. in data-limited scenarios. This research offers a promising direction for enhancing the efficiency of reward models and optimizing the fine-tuning of language models under restricted feedback conditions.

------------

`[2406.06657] Harnessing AI for efficient analysis of complex policy documents: a case study of Executive Order 14110 <https://arxiv.org/abs/2406.06657>`__ 利用人工智能高效分析复杂政策文件:第14110号行政命令案例研究

::

    Mon, 10 Jun 2024 11:19:28 GMT
    Mark A. Kramer, Allen Leavens, Alexander Scarlat

Policy documents, such as legislation, regulations, and executive orders, are crucial in shaping society. However, their length and complexity make interpretation and application challenging and time-consuming. Artificial intelligence (AI), particularly large language models (LLMs), has the potential to automate the process of analyzing these documents, improving accuracy and efficiency. This study aims to evaluate the potential of AI in streamlining policy analysis and to identify the strengths and limitations of current AI approaches. The research focuses on question answering and tasks involving content extraction from policy documents. A case study was conducted using Executive Order 14110 on "Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence" as a test case. Four commercial AI systems were used to analyze the document and answer a set of representative policy questions.
The performance of the AI systems was compared to manual analysis conducted by human experts. The study found that two AI systems, Gemini 1.5 Pro and Claude 3 Opus, demonstrated significant potential for supporting policy analysis, providing accurate and reliable information extraction from complex documents.
They performed comparably to human analysts but with significantly higher efficiency. However, achieving reproducibility remains a challenge, necessitating further research and development.

------------

`[2406.07081] Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning <https://arxiv.org/abs/2406.07081>`__ 基于上下文学习高效探索文档级机器翻译的大型语言模型

::

    Tue, 11 Jun 2024 09:11:17 GMT
    Menglong Cui, Jiangcun Du, Shaolin Zhu, Deyi Xiong

Large language models (LLMs) exhibit outstanding performance in machine translation via in-context learning. In contrast to sentence-level translation, document-level translation (DOCMT) by LLMs based on in-context learning faces two major challenges: firstly, document translations generated by LLMs are often incoherent; secondly, the length of demonstration for in-context learning is usually limited. To address these issues, we propose a Context-Aware Prompting method (CAP), which enables LLMs to generate more accurate, cohesive, and coherent translations via in-context learning. CAP takes into account multi-level attention, selects the most relevant sentences to the current one as context, and then generates a summary from these collected sentences.
Subsequently, sentences most similar to the summary are retrieved from the datastore as demonstrations, which effectively guide LLMs in generating cohesive and coherent translations. We conduct extensive experiments across various DOCMT tasks, and the results demonstrate the effectiveness of our approach, particularly in zero pronoun translation (ZPT) and literary translation tasks.

------------

`[2406.07138] Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement <https://arxiv.org/abs/2406.07138>`__ 绝不错过:具有一致“中间”增强的大型语言模型上下文窗口扩展的有效方法

::

    Tue, 11 Jun 2024 10:35:49 GMT
    Tong Wu, Yanpeng Zhao, Zilong Zheng

Recently, many methods have been developed to extend the context length of pre-trained large language models (LLMs), but they often require fine-tuning at the target length ($\gg4K$) and struggle to effectively utilize information from the middle part of the context. To address these issues, we propose $\textbf{C}$ontinuity-$\textbf{R}$elativity ind$\textbf{E}$xing with g$\textbf{A}$ussian $\textbf{M}$iddle (CREAM), which interpolates positional encodings by manipulating position indices. Apart from being simple, CREAM is training-efficient: it only requires fine-tuning at the pre-trained context window (eg, Llama 2-4K) and can extend LLMs to a much longer target context length (eg, 256K). To ensure that the model focuses more on the information in the middle, we introduce a truncated Gaussian to encourage sampling from the middle part of the context during fine-tuning, thus alleviating the ``Lost-in-the-Middle'' problem faced by long-context LLMs. Experimental results show that CREAM successfully extends LLMs to the target length for both Base and Chat versions of $\texttt{Llama2-7B}$ with ``Never Miss A Beat''. Our code will be publicly available soon.

------------

`[2406.07368] When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models <https://arxiv.org/abs/2406.07368>`__ 当线性注意力遇到自回归解码:迈向更有效和高效的线性化大型语言模型

::

    Tue, 11 Jun 2024 15:34:43 GMT
    Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Yingyan (Celine) Lin

Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs.
Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.

------------

`[2406.07084] Leveraging Large Language Models for Efficient Failure Analysis in Game Development <https://arxiv.org/abs/2406.07084>`__ 在游戏开发中利用大型语言模型进行有效的失败分析

::

    Tue, 11 Jun 2024 09:21:50 GMT
    Leonardo Marini, Linus Gissl\'en, and Alessandro Sestini

In games, and more generally in the field of software development, early detection of bugs is vital to maintain a high quality of the final product.
Automated tests are a powerful tool that can catch a problem earlier in development by executing periodically. As an example, when new code is submitted to the code base, a new automated test verifies these changes.
However, identifying the specific change responsible for a test failure becomes harder when dealing with batches of changes -- especially in the case of a large-scale project such as a AAA game, where thousands of people contribute to a single code base. This paper proposes a new approach to automatically identify which change in the code caused a test to fail. The method leverages Large Language Models (LLMs) to associate error messages with the corresponding code changes causing the failure. We investigate the effectiveness of our approach with quantitative and qualitative evaluations. Our approach reaches an accuracy of 71% in our newly created dataset, which comprises issues reported by developers at EA over a period of one year. We further evaluated our model through a user study to assess the utility and usability of the tool from a developer perspective, resulting in a significant reduction in time -- up to 60% -- spent investigating issues.

------------

`[2406.06647] How Efficient is LLM-Generated Code? A Rigorous & High-Standard Benchmark <https://arxiv.org/abs/2406.06647>`__ llm生成的代码的效率如何?一个严格且高标准的基准

::

    Mon, 10 Jun 2024 04:19:20 GMT
    Ruizhong Qiu, Weiliang Will Zeng, Hanghang Tong, James Ezick, Christopher Lott

The emergence of large language models (LLMs) has significantly pushed the frontiers of program synthesis. Advancement of LLM-based program synthesis calls for a thorough evaluation of LLM-generated code. Most evaluation frameworks focus on the (functional) correctness of generated code; efficiency, as an important measure of code quality, has been overlooked in existing evaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a rigorous and high-standard benchmark for evaluating the capability of LLMs in generating efficient code. Firstly, we propose a new efficiency metric called eff@k, which generalizes the pass@k metric from correctness to efficiency and appropriately handles right-censored execution time. Furthermore, we derive an unbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we also provide a numerically stable implementation for the new estimator.
Secondly, to set a high-standard for efficiency evaluation, we employ a human expert to design best algorithms and implementations as our reference solutions of efficiency, many of which are much more efficient than existing canonical solutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous evaluation, we employ a human expert to curate strong test case generators to filter out wrong code and differentiate suboptimal algorithms. An extensive study across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still fall short of generating expert-level efficient code. Using two subsets of our problem set, we demonstrate that such deficiency is because current LLMs struggle in designing advanced algorithms and are barely aware of implementation optimization. Our benchmark is publicly available at https://github.com/q-rz/enamel .

------------

`[2404.01331] LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model <https://arxiv.org/abs/2404.01331>`__ 

::

    replaced with revised version Mon, 10 Jun 2024 20:59:48 GMT
    Submission history From: Matthew Olson [view email]
    [v1] Fri, 29 Mar 2024 21:32:50 UTC (608 KB)
    [v2] Mon, 10 Jun 2024 20:59:48 UTC (608 KB)
    Musashi Hinck, Matthew L. Olson, David Cobbley, Shao-Yen Tseng, Vasudev Lal

We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code and weights for our models for the LLaVA-Gemma models.

------------

`[2405.03103] Learning from Students: Applying t-Distributions to Explore Accurate and Efficient Formats for LLMs <https://arxiv.org/abs/2405.03103>`__ 向学生学习:应用t分布来探索llm的准确和高效的格式

::

    replaced with revised version Mon, 10 Jun 2024 23:41:18 GMT
    Submission history From: Jordan Dotzel [view email]
    [v1] Mon, 6 May 2024 01:39:59 UTC (2,647 KB)
    [v2] Mon, 10 Jun 2024 23:41:18 UTC (2,647 KB)
    Jordan Dotzel, Yuzong Chen, Bahaa Kotb, Sushma Prasad, Gang Wu, Sheng Li, Mohamed S. Abdelfattah, Zhiru Zhang

The increasing size of large language models (LLMs) traditionally requires low-precision integer formats to meet strict latency and power demands. Yet recently, alternative formats such as Normal Float (NF4) have increased model accuracy at the cost of increased chip area. In this work, we first conduct a large-scale analysis of LLM weights and activations across 30 networks and conclude that most distributions follow a Student's t-distribution. We then derive a new theoretically optimal format, Student Float (SF4), that improves over NF4 across modern LLMs, for example increasing the average accuracy on LLaMA2-7B by 0.76% across tasks. Using this format as a high-accuracy reference, we then propose augmenting E2M1 with two variants of supernormal support for higher model accuracy. Finally, we explore the quality and efficiency frontier across 11 datatypes by evaluating their model accuracy and hardware complexity. We discover a Pareto curve composed of INT4, E2M1, and E2M1 with supernormal support, which offers a continuous tradeoff between model accuracy and chip area. For example, E2M1 with supernormal support increases the accuracy of Phi-2 by up to 2.19% with 1.22% area overhead, enabling more LLM-based applications to be run at four bits. The supporting code is hosted at this https URL.

------------

`[2406.05981] ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization <https://arxiv.org/abs/2406.05981>`__ ShiftAddLLM:基于训练后无乘法重参数化的预训练llm加速

::

    replaced with revised version Tue, 11 Jun 2024 15:14:30 GMT
    Submission history From: Haoran You [view email]
    [v1] Mon, 10 Jun 2024 02:47:55 UTC (372 KB)
    [v2] Tue, 11 Jun 2024 15:14:30 UTC (372 KB)
    Haoran You, Yipin Guo, Yichao Fu, Wei Zhou, Huihong Shi, Xiaofan Zhang, Souvik Kundu, Amir Yazdanbakhsh, Yingyan (Celine) Lin

Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. To address this, we propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, we present a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3 and 2 bits, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at this https URL.

------------

-----------------------
In-Context Learning (2)
-----------------------

`[2406.06699] In-Context Learning and Fine-Tuning GPT for Argument Mining <https://arxiv.org/abs/2406.06699>`__ 基于上下文学习和微调GPT的论元挖掘

::

    Mon, 10 Jun 2024 18:01:55 GMT
    J\'er\'emie Cabessa, Hugo Hernault and Umer Mushtaq

Large Language Models (LLMs) have become ubiquitous in NLP and deep learning.
In-Context Learning (ICL) has been suggested as a bridging paradigm between the training-free and fine-tuning LLMs settings. In ICL, an LLM is conditioned to solve tasks by means of a few solved demonstration examples included as prompt.
Argument Mining (AM) aims to extract the complex argumentative structure of a text, and Argument Type Classification (ATC) is an essential sub-task of AM. We introduce an ICL strategy for ATC combining kNN-based examples selection and majority vote ensembling. In the training-free ICL setting, we show that GPT-4 is able to leverage relevant information from only a few demonstration examples and achieve very competitive classification accuracy on ATC. We further set up a fine-tuning strategy incorporating well-crafted structural features given directly in textual form. In this setting, GPT-3.5 achieves state-of-the-art performance on ATC. Overall, these results emphasize the emergent ability of LLMs to grasp global discursive flow in raw text in both off-the-shelf and fine-tuned setups.

------------

`[2406.07081] Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning <https://arxiv.org/abs/2406.07081>`__ 基于上下文学习高效探索文档级机器翻译的大型语言模型

::

    Tue, 11 Jun 2024 09:11:17 GMT
    Menglong Cui, Jiangcun Du, Shaolin Zhu, Deyi Xiong

Large language models (LLMs) exhibit outstanding performance in machine translation via in-context learning. In contrast to sentence-level translation, document-level translation (DOCMT) by LLMs based on in-context learning faces two major challenges: firstly, document translations generated by LLMs are often incoherent; secondly, the length of demonstration for in-context learning is usually limited. To address these issues, we propose a Context-Aware Prompting method (CAP), which enables LLMs to generate more accurate, cohesive, and coherent translations via in-context learning. CAP takes into account multi-level attention, selects the most relevant sentences to the current one as context, and then generates a summary from these collected sentences.
Subsequently, sentences most similar to the summary are retrieved from the datastore as demonstrations, which effectively guide LLMs in generating cohesive and coherent translations. We conduct extensive experiments across various DOCMT tasks, and the results demonstrate the effectiveness of our approach, particularly in zero pronoun translation (ZPT) and literary translation tasks.

------------

--------------
Reasoning (12)
--------------

`[2406.06561] Brainstorming Brings Power to Large Language Models of Knowledge Reasoning <https://arxiv.org/abs/2406.06561>`__ 头脑风暴为知识推理的大型语言模型提供了力量

::

    Sun, 2 Jun 2024 14:47:14 GMT
    Zining Qin, Chenhao Wang, Huiling Qin, Weijia Jia

Large Language Models (LLMs) have demonstrated amazing capabilities in language generation, text comprehension, and knowledge reasoning. While a single powerful model can already handle multiple tasks, relying on a single perspective can lead to biased and unstable results. Recent studies have further improved the model's reasoning ability on a wide range of tasks by introducing multi-model collaboration. However, models with different capabilities may produce conflicting answers on the same problem, and how to reasonably obtain the correct answer from multiple candidate models has become a challenging problem. In this paper, we propose the multi-model brainstorming based on prompt. It incorporates different models into a group for brainstorming, and after multiple rounds of reasoning elaboration and re-inference, a consensus answer is reached within the group. We conducted experiments on three different types of datasets, and demonstrate that the brainstorming can significantly improve the effectiveness in logical reasoning and fact extraction. Furthermore, we find that two small-parameter models can achieve accuracy approximating that of larger-parameter models through brainstorming, which provides a new solution for distributed deployment of LLMs.

------------

`[2406.06586] Bi-Chainer: Automated Large Language Models Reasoning with Bidirectional Chaining <https://arxiv.org/abs/2406.06586>`__ Bi-Chainer:基于双向链的大型语言模型自动推理

::

    Wed, 5 Jun 2024 08:15:38 GMT
    Shuqi Liu, Bowei He, Linqi Song

Large Language Models (LLMs) have shown human-like reasoning abilities but still face challenges in solving complex logical problems. Existing unidirectional chaining methods, such as forward chaining and backward chaining, suffer from issues like low prediction accuracy and efficiency. To address these, we propose a bidirectional chaining method, Bi-Chainer, which dynamically switches to depth-first reasoning in the opposite reasoning direction when it encounters multiple branching options within the current direction. Thus, the intermediate reasoning results can be utilized as guidance to facilitate the reasoning process. We show that Bi-Chainer achieves sizable accuracy boots over unidirectional chaining frameworks on four challenging logical reasoning datasets. Moreover, Bi-Chainer enhances the accuracy of intermediate proof steps and reduces the average number of inference calls, resulting in more efficient and accurate reasoning.

------------

`[2406.06588] Assessing the Emergent Symbolic Reasoning Abilities of Llama Large Language Models <https://arxiv.org/abs/2406.06588>`__ Llama大型语言模型的新兴符号推理能力评估

::

    Wed, 5 Jun 2024 12:22:43 GMT
    Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti

Large Language Models (LLMs) achieve impressive performance in a wide range of tasks, even if they are often trained with the only objective of chatting fluently with users. Among other skills, LLMs show emergent abilities in mathematical reasoning benchmarks, which can be elicited with appropriate prompting methods. In this work, we systematically investigate the capabilities and limitations of popular open-source LLMs on different symbolic reasoning tasks. We evaluate three models of the Llama 2 family on two datasets that require solving mathematical formulas of varying degrees of difficulty. We test a generalist LLM (Llama 2 Chat) as well as two fine-tuned versions of Llama 2 (MAmmoTH and MetaMath) specifically designed to tackle mathematical problems.
We observe that both increasing the scale of the model and fine-tuning it on relevant tasks lead to significant performance gains. Furthermore, using fine-grained evaluation measures, we find that such performance gains are mostly observed with mathematical formulas of low complexity, which nevertheless often remain challenging even for the largest fine-tuned models.

------------

`[2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision <https://arxiv.org/abs/2406.06592>`__ 通过自动化过程监督改进语言模型中的数学推理

::

    Wed, 5 Jun 2024 19:25:40 GMT
    Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun and Abhinav Rastogi

Complex multi-step reasoning tasks, such as solving mathematical problems or generating code, remain a significant hurdle for even the most advanced large language models (LLMs). Verifying LLM outputs with an Outcome Reward Model (ORM) is a standard inference-time technique aimed at enhancing the reasoning performance of LLMs. However, this still proves insufficient for reasoning tasks with a lengthy or multi-hop reasoning chain, where the intermediate outcomes are neither properly rewarded nor penalized. Process supervision addresses this limitation by assigning intermediate rewards during the reasoning process. To date, the methods used to collect process supervision data have relied on either human annotation or per-step Monte Carlo estimation, both prohibitively expensive to scale, thus hindering the broad application of this technique. In response to this challenge, we propose a novel divide-and-conquer style Monte Carlo Tree Search (MCTS) algorithm named \textit{OmegaPRM} for the efficient collection of high-quality process supervision data. This algorithm swiftly identifies the first error in the Chain of Thought (CoT) with binary search and balances the positive and negative examples, thereby ensuring both efficiency and quality. As a result, we are able to collect over 1.5 million process supervision annotations to train a Process Reward Model (PRM). Utilizing this fully automated process supervision alongside the weighted self-consistency algorithm, we have enhanced the instruction tuned Gemini Pro model's math reasoning performance, achieving a 69.4\% success rate on the MATH benchmark, a 36\% relative improvement from the 51\% base model performance. Additionally, the entire process operates without any human intervention, making our method both financially and computationally cost-effective compared to existing methods.

------------

`[2406.06613] GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents <https://arxiv.org/abs/2406.06613>`__ GameBench:评估LLM智能体的策略推理能力

::

    Fri, 7 Jun 2024 00:28:43 GMT
    Anthony Costarelli, Mat Allen, Roman Hauksson, Grace Sodunke, Suhas Hariharan, Carlson Cheng, Wenjie Li, Arjun Yadav

Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks. Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents' performance across various types of reasoning found in games. To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP). Our results show that none of the tested models match human performance, and at worse GPT-4 performs worse than random action. CoT and RAP both improve scores but not comparable to human levels.

------------

`[2406.07080] DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs <https://arxiv.org/abs/2406.07080>`__ DARA:面向知识图谱问答的分解-对齐-推理自主语言Agent

::

    Tue, 11 Jun 2024 09:09:37 GMT
    Haishuo Fang, Xiaodan Zhu, Iryna Gurevych

Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning autonomous language agents in various real-life applications. To improve the neural-symbolic reasoning capabilities of language agents powered by Large Language Models (LLMs) in KGQA, we propose the DecompositionAlignment-Reasoning Agent (DARA) framework. DARA effectively parses questions into formal queries through a dual mechanism: high-level iterative task decomposition and low-level task grounding. Importantly, DARA can be efficiently trained with a small number of high-quality reasoning trajectories. Our experimental results demonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms both in-context learning-based agents with GPT-4 and alternative fine-tuned agents, across different benchmarks in zero-shot evaluation, making such models more accessible for real-life applications. We also show that DARA attains performance comparable to state-of-the-art enumerating-and-ranking-based methods for KGQA.

------------

`[2406.07393] Limited Out-of-Context Knowledge Reasoning in Large Language Models <https://arxiv.org/abs/2406.07393>`__ 大型语言模型有限的断章取义知识推理

::

    Tue, 11 Jun 2024 15:58:59 GMT
    Peng Hu, Changjiang Gao, Ruiqi Gao, Jiajun Chen, and Shujian Huang

Large Language Models (LLMs) have demonstrated strong capabilities as knowledge bases and significant in-context reasoning capabilities. However, previous work challenges their out-of-context reasoning ability, i.e., the ability to infer information from their training data, instead of from the context or prompt. This paper focuses on a significant facet of out-of-context reasoning: Out-of-Context Knowledge Reasoning (OCKR), which is to combine multiple knowledge to infer new knowledge. We designed a synthetic dataset with seven representative OCKR tasks to systematically assess the OCKR capabilities of LLMs. Using this dataset, we evaluated the LLaMA2-13B-chat model and discovered that its proficiency in this aspect is limited, regardless of whether the knowledge is trained in a separate or adjacent training settings.
Moreover, training the model to reason with complete reasoning data did not result in significant improvement. Training the model to perform explicit knowledge retrieval helps in only one of the tasks, indicating that the model's limited OCKR capabilities are due to difficulties in retrieving relevant knowledge. Furthermore, we treat cross-lingual knowledge transfer as a distinct form of OCKR, and evaluate this ability. Our results show that the evaluated model also exhibits limited ability in transferring knowledge across languages.
The dataset used in this study is available at https://github.com/NJUNLP/ID-OCKR.

------------

`[2406.06863] Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity <https://arxiv.org/abs/2406.06863>`__ Ollabench:评估llm对以人为中心的相互依赖网络安全的推理

::

    Tue, 11 Jun 2024 00:35:39 GMT
    Tam n. Nguyen

Large Language Models (LLMs) have the potential to enhance Agent-Based Modeling by better representing complex interdependent cybersecurity systems, improving cybersecurity threat modeling and risk management. However, evaluating LLMs in this context is crucial for legal compliance and effective application development. Existing LLM evaluation frameworks often overlook the human factor and cognitive computing capabilities essential for interdependent cybersecurity. To address this gap, I propose OllaBench, a novel evaluation framework that assesses LLMs' accuracy, wastefulness, and consistency in answering scenario-based information security compliance and non-compliance questions. OllaBench is built on a foundation of 24 cognitive behavioral theories and empirical evidence from 38 peer-reviewed papers. OllaBench was used to evaluate 21 LLMs, including both open-weight and commercial models from OpenAI, Anthropic, Google, Microsoft, Meta and so on. The results reveal that while commercial LLMs have the highest overall accuracy scores, there is significant room for improvement. Smaller low-resolution open-weight LLMs are not far behind in performance, and there are significant differences in token efficiency and consistency among the evaluated models. OllaBench provides a user-friendly interface and supports a wide range of LLM platforms, making it a valuable tool for researchers and solution developers in the field of human-centric interdependent cybersecurity and beyond.

------------

`[2401.06853] Large Language Models Can Learn Temporal Reasoning <https://arxiv.org/abs/2401.06853>`__ 大型语言模型可以学习时序推理

::

    replaced with revised version Tue, 11 Jun 2024 02:38:27 GMT
    Submission history From: Siheng Xiong [view email]
    [v1] Fri, 12 Jan 2024 19:00:26 UTC (7,033 KB)
    [v2] Tue, 20 Feb 2024 00:14:31 UTC (7,186 KB)
    [v3] Mon, 22 Apr 2024 04:00:00 UTC (7,192 KB)
    [v4] Sat, 1 Jun 2024 22:38:32 UTC (7,191 KB)
    [v5] Tue, 11 Jun 2024 02:38:27 UTC (7,177 KB)
    Siheng Xiong, Ali Payani, Ramana Kompella, Faramarz Fekri

While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards language-based TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation.

------------

`[2405.18357] Faithful Logical Reasoning via Symbolic Chain-of-Thought <https://arxiv.org/abs/2405.18357>`__ 通过符号思维链进行忠实的逻辑推理

::

    replaced with revised version Tue, 11 Jun 2024 07:41:03 GMT
    Submission history From: Jundong Xu [view email]
    [v1] Tue, 28 May 2024 16:55:33 UTC (517 KB)
    [v2] Tue, 11 Jun 2024 07:41:03 UTC (517 KB)
    Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, Wynne Hsu

While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at this https URL.

------------

`[2406.06196] LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages <https://arxiv.org/abs/2406.06196>`__ LINGOLY:资源稀缺和灭绝语言的奥林匹克语言推理基准

::

    replaced with revised version Tue, 11 Jun 2024 10:19:52 GMT
    Submission history From: Andrew Bean [view email]
    [v1] Mon, 10 Jun 2024 11:50:29 UTC (5,073 KB)
    [v2] Tue, 11 Jun 2024 10:19:52 UTC (5,073 KB)
    Andrew M. Bean, Simi Hellsten, Harry Mayne, Jabez Magomere, Ethan A. Chi, Ryan Chi, Scott A. Hale, Hannah Rose Kirk

In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.

------------

`[2406.06461] Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies <https://arxiv.org/abs/2406.06461>`__ 代币经济中的推理:LLM推理策略的预算感知评估

::

    replaced with revised version Tue, 11 Jun 2024 02:12:17 GMT
    Submission history From: Junlin Wang [view email]
    [v1] Mon, 10 Jun 2024 16:55:08 UTC (4,133 KB)
    [v2] Tue, 11 Jun 2024 02:12:17 UTC (4,129 KB)
    Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, Ben Athiwaratkun

A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don't surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.

------------

-----------
ToolUse (4)
-----------

`[2406.07115] Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees <https://arxiv.org/abs/2406.07115>`__ 推进工具增强的大型语言模型:整合推理树中错误的见解

::

    Tue, 11 Jun 2024 10:00:18 GMT
    Sijia Chen, Yibo Wang, Yi-Feng Wu, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Lijun Zhang

Tool-augmented large language models (LLMs) leverage tools, often in the form of APIs, to enhance their reasoning capabilities on complex tasks, thus taking on the role of intelligent agents interacting with the real world. The recently introduced ToolLLaMA model by Qin et al. [2024] utilizes the depth-first search-based decision tree (DFSDT) method for reasoning with $16000+$ real-world APIs, which effectively improves the planning and inferencing performance of tool-augmented LLMs compared to traditional chain reasoning approaches. However, their approach only employs successful paths from decision trees (also called inference trees) for supervised fine-tuning (SFT) during training, which does not fully exploit the advantages of the tree of thought.
In this study, we propose an inference trajectory optimization framework based on the preference data extracted from decision trees to address this limitation. We first introduce a novel method for constructing preference data from the tree of thought, capitalizing on the failed explorations previously overlooked in the trees. Specifically, we generate an effective step-wise preference dataset, named ToolPreference, for tool use based on the ToolBench dataset. In the subsequent training phase, we first fine-tune the LLM with tool-usage expert trajectories and then use these step-wise preference pairs for direct preference optimization (DPO) to update the policy of the LLM, resulting in our ToolPrefer-LLaMA (TP-LLaMA) model. Our experiments demonstrate that by obtaining insights from errors in inference trees, TP-LLaMA significantly outperforms the baselines across almost all test scenarios by a large margin and exhibits better generalization capabilities with unseen APIs.
At the same time, TP-LLaMA has also demonstrated superior reasoning efficiency compared to the baselines, making it more suitable for complex tool-usage reasoning tasks.

------------

`[2406.06799] LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data Caching <https://arxiv.org/abs/2406.06799>`__ LLM-dCache:用gpt驱动的本地化数据缓存改进工具增强的llm

::

    Mon, 10 Jun 2024 21:08:39 GMT
    Simranjit Singh, Michael Fore, Andreas Karatzas, Chaehong Lee, Yanan Jian, Longfei Shangguan, Fuxun Yu, Iraklis Anagnostopoulos, Dimitrios Stamoulis

As Large Language Models (LLMs) broaden their capabilities to manage thousands of API calls, they are confronted with complex data operations across vast datasets with significant overhead to the underlying system. In this work, we introduce LLM-dCache to optimize data accesses by treating cache operations as callable API functions exposed to the tool-augmented agent. We grant LLMs the autonomy to manage cache decisions via prompting, seamlessly integrating with existing function-calling mechanisms. Tested on an industry-scale massively parallel platform that spans hundreds of GPT endpoints and terabytes of imagery, our method improves Copilot times by an average of 1.24x across various LLMs and prompting techniques.

------------

`[2402.15960] Budget-Constrained Tool Learning with Planning <https://arxiv.org/abs/2402.15960>`__ 基于规划的预算约束工具学习

::

    replaced with revised version Tue, 11 Jun 2024 01:02:19 GMT
    Submission history From: Yuanhang Zheng [view email]
    [v1] Sun, 25 Feb 2024 02:46:33 UTC (229 KB)
    [v2] Tue, 11 Jun 2024 01:02:19 UTC (283 KB)
    Yuanhang Zheng, Peng Li, Ming Yan, Ji Zhang, Fei Huang and Yang Liu

Despite intensive efforts devoted to tool learning, the problem of budget-constrained tool learning, which focuses on resolving user queries within a specific budget constraint, has been widely overlooked. This paper proposes a novel method for budget-constrained tool learning. Our approach involves creating a preferable plan under the budget constraint before utilizing the tools. This plan outlines the feasible tools and the maximum number of times they can be employed, offering a comprehensive overview of the tool learning process for large language models. This allows them to allocate the budget from a broader perspective. To devise the plan without incurring significant extra costs, we suggest initially estimating the usefulness of the candidate tools based on past experience. Subsequently, we employ dynamic programming to formulate the plan. Experimental results demonstrate that our method can be integrated with various tool learning methods, significantly enhancing their effectiveness under strict budget constraints.

------------

`[2405.15729] Optimizing Large Language Models for OpenAPI Code Completion <https://arxiv.org/abs/2405.15729>`__ 面向OpenAPI代码补全的大型语言模型优化

::

    replaced with revised version Mon, 10 Jun 2024 21:58:24 GMT
    Submission history From: Mantas Lukoševičius [view email]
    [v1] Fri, 24 May 2024 17:19:03 UTC (443 KB)
    [v2] Mon, 10 Jun 2024 21:58:24 UTC (110 KB)
    Bohdan Petryshyn and Mantas Luko\v{s}evi\v{c}ius

Recent advancements in Large Language Models (LLMs) and their utilization in code generation tasks have significantly reshaped the field of software development. Despite the remarkable efficacy of code completion solutions in mainstream programming languages, their performance lags when applied to less ubiquitous formats such as OpenAPI definitions. This study evaluates the OpenAPI completion performance of GitHub Copilot, a prevalent commercial code completion tool, and proposes a set of task-specific optimizations leveraging Meta's open-source model Code Llama. A semantics-aware OpenAPI completion benchmark proposed in this research is used to perform a series of experiments through which the impact of various prompt-engineering and fine-tuning techniques on the Code Llama model's performance is analyzed. The fine-tuned Code Llama model reaches a peak correctness improvement of 55.2% over GitHub Copilot despite utilizing 25 times fewer parameters than the commercial solution's underlying Codex model. Additionally, this research proposes an enhancement to a widely used code infilling training technique, addressing the issue of underperformance when the model is prompted with context sizes smaller than those used during training. The dataset, the benchmark, and the model fine-tuning code are made publicly available.

------------

-----------------------
Retrieval-Augmented (9)
-----------------------

`[2406.06566] RAG Enabled Conversations about Household Electricity Monitoring <https://arxiv.org/abs/2406.06566>`__ RAG促成了关于家用电力监控的对话

::

    Mon, 3 Jun 2024 07:44:32 GMT
    Carolina Fortuna, Vid Han\v{z}el and Bla\v{z} Bertalani\v{c}

In this paper, we investigate the integration of Retrieval Augmented Generation (RAG) with large language models (LLMs) such as ChatGPT, Gemini, and Llama to enhance the accuracy and specificity of responses to complex questions about electricity datasets. Recognizing the limitations of LLMs in generating precise and contextually relevant answers due to their dependency on the patterns in training data rather than factual understanding, we propose a solution that leverages a specialized electricity knowledge graph. This approach facilitates the retrieval of accurate, real-time data which is then synthesized with the generative capabilities of LLMs. Our findings illustrate that the RAG approach not only reduces the incidence of incorrect information typically generated by LLMs but also significantly improves the quality of the output by grounding responses in verifiable data. This paper details our methodology, presents a comparative analysis of responses with and without RAG, and discusses the implications of our findings for future applications of AI in specialized sectors like energy data analysis.

------------

`[2406.06572] Graph Neural Network Enhanced Retrieval for Question Answering of LLMs <https://arxiv.org/abs/2406.06572>`__ 图神经网络增强的llm问答检索

::

    Mon, 3 Jun 2024 17:07:46 GMT
    Zijian Li, Qingyan Guo, Jiawei Shao, Lei Song, Jiang Bian, Jun Zhang, Rui Wang

Retrieval augmented generation has revolutionized large language model (LLM) outputs by providing factual supports. Nevertheless, it struggles to capture all the necessary knowledge for complex reasoning questions. Existing retrieval methods typically divide reference documents into passages, treating them in isolation. These passages, however, are often interrelated, such as passages that are contiguous or share the same keywords. Therefore, recognizing the relatedness is crucial for enhancing the retrieval process. In this paper, we propose a novel retrieval method, called GNN-Ret, which leverages graph neural networks (GNNs) to enhance retrieval by considering the relatedness between passages. Specifically, we first construct a graph of passages by connecting passages that are structure-related and keyword-related. A graph neural network (GNN) is then leveraged to exploit the relationships between passages and improve the retrieval of supporting passages. Furthermore, we extend our method to handle multi-hop reasoning questions using a recurrent graph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates the graphs of passages from previous steps, thereby enhancing the retrieval of supporting passages. Extensive experiments on benchmark datasets demonstrate that GNN-Ret achieves higher accuracy for question answering with a single query of LLMs than strong baselines that require multiple queries, and RGNN-Ret further improves accuracy and achieves state-of-the-art performance, with up to 10.4% accuracy improvement on the 2WikiMQA dataset.

------------

`[2406.06575] Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and Abbreviation De-hallucination <https://arxiv.org/abs/2406.06575>`__ Ask-EDA:由LLM授权的设计助理，混合RAG和缩写De-hallucination

::

    Mon, 3 Jun 2024 19:40:28 GMT
    Luyao Shi, Michael Kazda, Bradley Sears, Nick Shropshire, Ruchir Puri

Electronic design engineers are challenged to find relevant information efficiently for a myriad of tasks within design construction, verification and technology development. Large language models (LLM) have the potential to help improve productivity by serving as conversational agents that effectively function as subject-matter experts. In this paper we demonstrate Ask-EDA, a chat agent designed to serve as a 24x7 expert available to provide guidance to design engineers. Ask-EDA leverages LLM, hybrid retrieval augmented generation (RAG) and abbreviation de-hallucination (ADH) techniques to deliver more relevant and accurate responses. We curated three evaluation datasets, namely q2a-100, cmds-100 and abbr-100. Each dataset is tailored to assess a distinct aspect: general design question answering, design command handling and abbreviation resolution. We demonstrated that hybrid RAG offers over a 40% improvement in Recall on the q2a-100 dataset and over a 60% improvement on the cmds-100 dataset compared to not using RAG, while ADH yields over a 70% enhancement in Recall on the abbr-100 dataset. The evaluation results show that Ask-EDA can effectively respond to design-related inquiries.

------------

`[2406.06723] Leveraging Large Language Models for Knowledge-free Weak Supervision in Clinical Natural Language Processing <https://arxiv.org/abs/2406.06723>`__ 在临床自然语言处理中利用大型语言模型实现无知识弱监督

::

    Mon, 10 Jun 2024 18:34:48 GMT
    Enshuo Hsu, Kirk Roberts

The performance of deep learning-based natural language processing systems is based on large amounts of labeled training data which, in the clinical domain, are not easily available or affordable. Weak supervision and in-context learning offer partial solutions to this issue, particularly using large language models (LLMs), but their performance still trails traditional supervised methods with moderate amounts of gold-standard data. In particular, inferencing with LLMs is computationally heavy. We propose an approach leveraging fine-tuning LLMs and weak supervision with virtually no domain knowledge that still achieves consistently dominant performance. Using a prompt-based approach, the LLM is used to generate weakly-labeled data for training a downstream BERT model. The weakly supervised model is then further fine-tuned on small amounts of gold standard data. We evaluate this approach using Llama2 on three different n2c2 datasets. With no more than 10 gold standard notes, our final BERT models weakly supervised by fine-tuned Llama2-13B consistently outperformed out-of-the-box PubMedBERT by 4.7% to 47.9% in F1 scores. With only 50 gold standard notes, our models achieved close performance to fully fine-tuned systems.

------------

`[2406.07084] Leveraging Large Language Models for Efficient Failure Analysis in Game Development <https://arxiv.org/abs/2406.07084>`__ 在游戏开发中利用大型语言模型进行有效的失败分析

::

    Tue, 11 Jun 2024 09:21:50 GMT
    Leonardo Marini, Linus Gissl\'en, and Alessandro Sestini

In games, and more generally in the field of software development, early detection of bugs is vital to maintain a high quality of the final product.
Automated tests are a powerful tool that can catch a problem earlier in development by executing periodically. As an example, when new code is submitted to the code base, a new automated test verifies these changes.
However, identifying the specific change responsible for a test failure becomes harder when dealing with batches of changes -- especially in the case of a large-scale project such as a AAA game, where thousands of people contribute to a single code base. This paper proposes a new approach to automatically identify which change in the code caused a test to fail. The method leverages Large Language Models (LLMs) to associate error messages with the corresponding code changes causing the failure. We investigate the effectiveness of our approach with quantitative and qualitative evaluations. Our approach reaches an accuracy of 71% in our newly created dataset, which comprises issues reported by developers at EA over a period of one year. We further evaluated our model through a user study to assess the utility and usability of the tool from a developer perspective, resulting in a significant reduction in time -- up to 60% -- spent investigating issues.

------------

`[2406.07348] DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation for Question-Answering <https://arxiv.org/abs/2406.07348>`__ DR-RAG:将动态文档相关性应用于问答检索增强生成

::

    Tue, 11 Jun 2024 15:15:33 GMT
    Zijian Hei and Weiling Wei and Wenjie Ou and Juyi Qiao and Junming Jiao and Zhiqing Zhu and Guowen Song

Retrieval-Augmented Generation (RAG) has significantly demonstrated the performance of Large Language Models (LLMs) in the knowledge-intensive tasks, such as Question-Answering (QA). RAG expands the query context by incorporating external knowledge bases to enhance the response accuracy. However, it would be inefficient to access LLMs multiple times for each query and unreliable to retrieve all the relevant documents by a single query. We find that even though there is low relevance between some critical documents and query, it is possible to retrieve the remaining documents by combining parts of the documents with the query. To mine the relevance, a two-stage retrieval framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is proposed to improve document retrieval recall and the accuracy of answers while maintaining efficiency. Also, a small classifier is applied to two different selection strategies to determine the contribution of the retrieved documents to answering the query and retrieve the relatively relevant documents.
Meanwhile, DR-RAG call the LLMs only once, which significantly improves the efficiency of the experiment. The experimental results on multi-hop QA datasets show that DR-RAG can significantly improve the accuracy of the answers and achieve new progress in QA systems.

------------

`[2406.07113] Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene Graph <https://arxiv.org/abs/2406.07113>`__ 超越简单查询:基于3D场景图的开放词汇表对象检索

::

    Tue, 11 Jun 2024 09:57:04 GMT
    Sergey Linok, Tatiana Zemskova, Svetlana Ladanova, Roman Titkov, Dmitry Yudin

Locating objects referred to in natural language poses a significant challenge for autonomous agents. Existing CLIP-based open-vocabulary methods successfully perform 3D object retrieval with simple (bare) queries but cannot cope with ambiguous descriptions that demand an understanding of object relations. To tackle this problem, we propose a modular approach called BBQ (Beyond Bare Queries), which constructs 3D scene spatial graph representation with metric edges and utilizes a large language model as a human-to-agent interface through our deductive scene reasoning algorithm. BBQ employs robust DINO-powered associations to form 3D objects, an advanced raycasting algorithm to project them to 2D, and a vision-language model to describe them as graph nodes. On Replica and ScanNet datasets, we show that the designed method accurately constructs 3D object-centric maps. We have demonstrated that their quality takes a leading place for open-vocabulary 3D semantic segmentation against other zero-shot methods. Also, we show that leveraging spatial relations is especially effective for scenes containing multiple entities of the same semantic class. On Sr3D and Nr3D benchmarks, our deductive approach demonstrates a significant improvement, enabling retrieving objects by complex queries compared to other state-of-the-art methods. Considering our design solutions, we achieved a processing speed approximately x3 times faster than the closest analog. This promising performance enables our approach for usage in applied intelligent robotics projects. We make the code publicly available at linukc.github.io/bbq/.

------------

`[2406.07053] TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs <https://arxiv.org/abs/2406.07053>`__ TelecomRAG:用检索增强生成和llm来驯服电信标准

::

    Tue, 11 Jun 2024 08:35:23 GMT
    Girma M. Yilma, Jose A. Ayala-Romero, Andres Garcia-Saavedra, Xavier Costa-Perez

Large Language Models (LLMs) have immense potential to transform the telecommunications industry. They could help professionals understand complex standards, generate code, and accelerate development. However, traditional LLMs struggle with the precision and source verification essential for telecom work.
To address this, specialized LLM-based solutions tailored to telecommunication standards are needed. Retrieval-augmented generation (RAG) offers a way to create precise, fact-based answers. This paper proposes TelecomRAG, a framework for a Telecommunication Standards Assistant that provides accurate, detailed, and verifiable responses. Our implementation, using a knowledge base built from 3GPP Release 16 and Release 18 specification documents, demonstrates how this assistant surpasses generic LLMs, offering superior accuracy, technical depth, and verifiability, and thus significant value to the telecommunications field.

------------

`[2402.11457] When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation <https://arxiv.org/abs/2402.11457>`__ 什么时候llm需要检索增强?减轻llm的过度自信有助于检索增强

::

    replaced with revised version Tue, 11 Jun 2024 08:08:47 GMT
    Submission history From: Shiyu Ni [view email]
    [v1] Sun, 18 Feb 2024 04:57:19 UTC (8,477 KB)
    [v2] Tue, 11 Jun 2024 08:08:47 UTC (8,474 KB)
    Shiyu Ni, Keping Bi, Jiafeng Guo, Xueqi Cheng

Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls.

------------

---------
Agent (8)
---------

`[2406.07155] Scaling Large-Language-Model-based Multi-Agent Collaboration <https://arxiv.org/abs/2406.07155>`__ 基于大型语言模型的多agent协作扩展

::

    Tue, 11 Jun 2024 11:02:04 GMT
    Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun

Pioneering advancements in large language model-powered agents have underscored the design pattern of multi-agent collaboration, demonstrating that collective intelligence can surpass the capabilities of each individual.
Inspired by the neural scaling law, which posits that increasing neurons leads to emergent abilities, this study investigates whether a similar principle applies to increasing agents in multi-agent collaboration. Technically, we propose multi-agent collaboration networks (MacNet), which utilize directed acyclic graphs to organize agents and streamline their interactive reasoning via topological ordering, with solutions derived from their dialogues.
Extensive experiments show that MacNet consistently outperforms baseline models, enabling effective agent collaboration across various network topologies and supporting cooperation among more than a thousand agents.
Notably, we observed a small-world collaboration phenomenon, where topologies resembling small-world properties achieved superior performance. Additionally, we identified a collaborative scaling law, indicating that normalized solution quality follows a logistic growth pattern as scaling agents, with collaborative emergence occurring much earlier than previously observed instances of neural emergence. The code and data will be available at https://github.com/OpenBMB/ChatDev.

------------

`[2406.07275] DCA-Bench: A Benchmark for Dataset Curation Agents <https://arxiv.org/abs/2406.07275>`__ DCA-Bench:数据集策展代理基准

::

    Tue, 11 Jun 2024 14:02:23 GMT
    Benhao Huang, Yingzhuo Yu, Jin Huang, Xingjian Zhang, Jiaqi Ma

The quality of datasets plays an increasingly crucial role in the research and development of modern artificial intelligence (AI). Despite the proliferation of open dataset platforms nowadays, data quality issues, such as insufficient documentation, inaccurate annotations, and ethical concerns, remain common in datasets widely used in AI. Furthermore, these issues are often subtle and difficult to be detected by rule-based scripts, requiring expensive manual identification and verification by dataset users or maintainers. With the increasing capability of large language models (LLMs), it is promising to streamline the curation of datasets with LLM agents. In this work, as the initial step towards this goal, we propose a dataset curation agent benchmark, DCA-Bench, to measure LLM agents' capability of detecting hidden dataset quality issues. Specifically, we collect diverse real-world dataset quality issues from eight open dataset platforms as a testbed.
Additionally, to establish an automatic pipeline for evaluating the success of LLM agents, which requires a nuanced understanding of the agent outputs, we implement a dedicated Evaluator using another LLM agent. We demonstrate that the LLM-based Evaluator empirically aligns well with human evaluation, allowing reliable automatic evaluation on the proposed benchmark. We further conduct experiments on several baseline LLM agents on the proposed benchmark and demonstrate the complexity of the task, indicating that applying LLMs to real-world dataset curation still requires further in-depth exploration and innovation. Finally, the proposed benchmark can also serve as a testbed for measuring the capability of LLMs in problem discovery rather than just problem-solving. The benchmark suite is available at \url{https://github.com/TRAIS-Lab/dca-bench}.

------------

`[2406.06613] GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents <https://arxiv.org/abs/2406.06613>`__ GameBench:评估LLM智能体的策略推理能力

::

    Fri, 7 Jun 2024 00:28:43 GMT
    Anthony Costarelli, Mat Allen, Roman Hauksson, Grace Sodunke, Suhas Hariharan, Carlson Cheng, Wenjie Li, Arjun Yadav

Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks. Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents' performance across various types of reasoning found in games. To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP). Our results show that none of the tested models match human performance, and at worse GPT-4 performs worse than random action. CoT and RAP both improve scores but not comparable to human levels.

------------

`[2406.06910] Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large Language Models <https://arxiv.org/abs/2406.06910>`__ Agent-SiMT:基于大型语言模型的agent辅助同传机器翻译

::

    Tue, 11 Jun 2024 03:09:20 GMT
    Shoutao Guo, Shaolei Zhang, Zhengrui Ma, Min Zhang, Yang Feng

Simultaneous Machine Translation (SiMT) generates target translations while reading the source sentence. It relies on a policy to determine the optimal timing for reading sentences and generating translations. Existing SiMT methods generally adopt the traditional Transformer architecture, which concurrently determines the policy and generates translations. While they excel at determining policies, their translation performance is suboptimal. Conversely, Large Language Models (LLMs), trained on extensive corpora, possess superior generation capabilities, but it is difficult for them to acquire translation policy through the training methods of SiMT. Therefore, we introduce Agent-SiMT, a framework combining the strengths of LLMs and traditional SiMT methods. Agent-SiMT contains the policy-decision agent and the translation agent. The policy-decision agent is managed by a SiMT model, which determines the translation policy using partial source sentence and translation. The translation agent, leveraging an LLM, generates translation based on the partial source sentence. The two agents collaborate to accomplish SiMT.
Experiments demonstrate that Agent-SiMT attains state-of-the-art performance.

------------

`[2406.07054] CoEvol: Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation <https://arxiv.org/abs/2406.07054>`__ CoEvol:基于多智能体合作构建更好的指令微调响应

::

    Tue, 11 Jun 2024 08:35:37 GMT
    Renhao Li, Minghuan Tan, Derek F. Wong, Min Yang

In recent years, instruction fine-tuning (IFT) on large language models (LLMs) has garnered considerable attention to enhance model performance on unseen tasks. Attempts have been made on automatic construction and effective selection for IFT data. However, we posit that previous methods have not fully harnessed the potential of LLMs for enhancing data quality. The responses within IFT data could be further enhanced by leveraging the capabilities of LLMs themselves. In this paper, we propose CoEvol, an LLM-based multi-agent cooperation framework for the improvement of responses to instructions. To effectively refine the responses, we develop an iterative framework following a debate-advise-edit-judge paradigm. A two-stage multi-agent debate strategy is further devised to ensure the diversity and reliability of editing suggestions within the framework. Empirically, models equipped with CoEvol outperform competitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its effectiveness in enhancing instruction-following capabilities for LLMs.

------------

`[2406.07080] DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs <https://arxiv.org/abs/2406.07080>`__ DARA:面向知识图谱问答的分解-对齐-推理自主语言Agent

::

    Tue, 11 Jun 2024 09:09:37 GMT
    Haishuo Fang, Xiaodan Zhu, Iryna Gurevych

Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning autonomous language agents in various real-life applications. To improve the neural-symbolic reasoning capabilities of language agents powered by Large Language Models (LLMs) in KGQA, we propose the DecompositionAlignment-Reasoning Agent (DARA) framework. DARA effectively parses questions into formal queries through a dual mechanism: high-level iterative task decomposition and low-level task grounding. Importantly, DARA can be efficiently trained with a small number of high-quality reasoning trajectories. Our experimental results demonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms both in-context learning-based agents with GPT-4 and alternative fine-tuned agents, across different benchmarks in zero-shot evaluation, making such models more accessible for real-life applications. We also show that DARA attains performance comparable to state-of-the-art enumerating-and-ranking-based methods for KGQA.

------------

`[2406.03679] On the Effects of Data Scale on Computer Control Agents <https://arxiv.org/abs/2406.03679>`__ 数据规模对计算机控制agent的影响

::

    replaced with revised version Tue, 11 Jun 2024 13:19:38 GMT
    Submission history From: Wei Li [view email]
    [v1] Thu, 6 Jun 2024 01:49:29 UTC (4,608 KB)
    [v2] Tue, 11 Jun 2024 13:19:38 UTC (4,608 KB)
    Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, Oriana Riva

Autonomous agents that control computer interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether fine-tuning alone is a viable approach for building real-world computer control agents. In particularly, we investigate how performance measured on both high and low-level tasks in domain and out of domain scales as more training data is collected. To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, AndroidControl is the most diverse computer control dataset to date, including 15,283 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data. Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance.

------------

`[2406.06464] Transforming Wearable Data into Health Insights using Large Language Model Agents <https://arxiv.org/abs/2406.06464>`__ 利用大型语言模型代理将可穿戴数据转化为健康见解

::

    replaced with revised version Tue, 11 Jun 2024 15:17:43 GMT
    Submission history From: Xin Liu [view email]
    [v1] Mon, 10 Jun 2024 17:00:54 UTC (673 KB)
    [v2] Tue, 11 Jun 2024 15:17:43 UTC (673 KB)
    Mike A. Merrill, Akshay Paruchuri, Naghmeh Rezaei, Geza Kovacs, Javier Perez, Yun Liu, Erik Schenck, Nova Hammerquist, Jake Sunshine, Shyam Tailor, Kumar Ayush, Hao-Wei Su, Qian He, Cory Y. McLean, Mark Malhotra, Shwetak Patel, Jiening Zhan, Tim Althoff, Daniel McDuff, Xin Liu

Despite the proliferation of wearable health trackers and the importance of sleep and exercise to health, deriving actionable personalized insights from wearable data remains a challenge because doing so requires non-trivial open-ended analysis of these data. The recent rise of large language model (LLM) agents, which can use tools to reason about and interact with the world, presents a promising opportunity to enable such personalized analysis at scale. Yet, the application of LLM agents in analyzing personal health is still largely untapped. In this paper, we introduce the Personal Health Insights Agent (PHIA), an agent system that leverages state-of-the-art code generation and information retrieval tools to analyze and interpret behavioral health data from wearables. We curate two benchmark question-answering datasets of over 4000 health insights questions. Based on 650 hours of human and expert evaluation we find that PHIA can accurately address over 84% of factual numerical questions and more than 83% of crowd-sourced open-ended questions. This work has implications for advancing behavioral health across the population, potentially enabling individuals to interpret their own wearable data, and paving the way for a new era of accessible, personalized wellness regimens that are informed by data-driven insights.

------------

-----------
Other (122)
-----------

`[2406.06865] Eyeballing Combinatorial Problems: A Case Study of Using Multimodal Large Language Models to Solve Traveling Salesman Problems <https://arxiv.org/abs/2406.06865>`__ 目测组合问题:使用多模态大型语言模型解决旅行商问题的案例研究

::

    Tue, 11 Jun 2024 00:41:08 GMT
    Mohammed Elhenawy, Ahmed Abdelhay, Taqwa I. Alhadidi, Huthaifa I Ashqar, Shadi Jaradat, Ahmed Jaber, Sebastien Glaser, and Andry Rakotonirainy

Multimodal Large Language Models (MLLMs) have demonstrated proficiency in processing di-verse modalities, including text, images, and audio. These models leverage extensive pre-existing knowledge, enabling them to address complex problems with minimal to no specific training examples, as evidenced in few-shot and zero-shot in-context learning scenarios. This paper investigates the use of MLLMs' visual capabilities to 'eyeball' solutions for the Traveling Salesman Problem (TSP) by analyzing images of point distributions on a two-dimensional plane. Our experiments aimed to validate the hypothesis that MLLMs can effectively 'eyeball' viable TSP routes. The results from zero-shot, few-shot, self-ensemble, and self-refine zero-shot evaluations show promising outcomes. We anticipate that these findings will inspire further exploration into MLLMs' visual reasoning abilities to tackle other combinatorial problems.

------------

`[2406.06870] What's in an embedding? Would a rose by any embedding smell as sweet? <https://arxiv.org/abs/2406.06870>`__ 嵌入中有什么?任何一种嵌入的玫瑰闻起来都那么香吗?

::

    Tue, 11 Jun 2024 01:10:40 GMT
    Venkat Venkatasubramanian

Large Language Models (LLMs) are often criticized for lacking true "understanding" and an ability to "reason" with their knowledge, being seen merely as advanced autocomplete systems. We believe that this perspective might be missing an important insight. We suggest that LLMs do develop a kind of empirical "understanding" that is "geometry"-like, which seems quite sufficient for a range of applications in NLP, computer vision, coding assistance, etc.
However, this "geometric" understanding, built from incomplete and noisy data, makes them unreliable, difficult to generalize, and lacking in inference capabilities and explanations, similar to the challenges faced by heuristics-based expert systems decades ago.
To overcome these limitations, we suggest that LLMs should be integrated with an "algebraic" representation of knowledge that includes symbolic AI elements used in expert systems. This integration aims to create large knowledge models (LKMs) that not only possess "deep" knowledge grounded in first principles, but also have the ability to reason and explain, mimicking human expert capabilities. To harness the full potential of generative AI safely and effectively, a paradigm shift from LLMs to the more comprehensive LKMs is needed.

------------

`[2406.06874] Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback <https://arxiv.org/abs/2406.06874>`__ 联合演示和偏好学习改进了与人工反馈的政策一致性

::

    Tue, 11 Jun 2024 01:20:53 GMT
    Chenliang Li, Siliang Zeng, Zeyi Liao, Jiaxiang Li, Dongyeop Kang, Alfredo Garcia, Mingyi Hong

Aligning human preference and value is an important requirement for building contemporary foundation models and embodied AI. However, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into successive stages, such as supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL), each performing one specific learning task. Such a sequential approach results in serious issues such as significant under-utilization of data and distribution mismatch between the learned reward model and generated policy, which eventually lead to poor alignment performance. We develop a single stage approach named Alignment with Integrated Human Feedback (AIHF), capable of integrating both human preference and demonstration to train reward models and the policy. The proposed approach admits a suite of efficient algorithms, which can easily reduce to, and leverage, popular alignment algorithms such as RLHF and Directly Policy Optimization (DPO), and only requires minor changes to the existing alignment pipelines. We demonstrate the efficiency of the proposed solutions with extensive experiments involving alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithms such as RLHF and DPO by large margins, especially when the amount of high-quality preference data is relatively limited.

------------

`[2406.06947] CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front-End UI Only <https://arxiv.org/abs/2406.06947>`__ CAAP:面向前端用户界面的情景感知动作规划方法

::

    Tue, 11 Jun 2024 05:21:20 GMT
    Junhee Cho, Jihoon Kim, Daseul Bae, Jinho Choo, Youngjune Gwon, Yeong-Dae Kwon

Software robots have long been deployed in Robotic Process Automation (RPA) to automate mundane and repetitive computer tasks. The advent of Large Language Models (LLMs) with advanced reasoning capabilities has set the stage for these agents to now undertake more complex and even previously unseen tasks. However, the LLM-based automation techniques in recent literature frequently rely on HTML source codes for input, limiting their application to web environments.
Moreover, the information contained in HTML codes is often inaccurate or incomplete, making the agent less reliable for practical applications. We propose an LLM-based agent that functions solely on the basis of screenshots for recognizing environments, while leveraging in-context learning to eliminate the need for collecting large datasets of human demonstration. Our strategy, named Context-Aware Action Planning (CAAP) prompting encourages the agent to meticulously review the context in various angles. Through our proposed methodology, we achieve a success rate of 94.4% on 67~types of MiniWoB++ problems, utilizing only 1.48~demonstrations per problem type. Our method offers the potential for broader applications, especially for tasks that require inter-application coordination on computers or smartphones, showcasing a significant advancement in the field of automation agents. Codes and models are accessible at https://github.com/caap-agent/caap-agent.

------------

`[2406.07327] 3D-Properties: Identifying Challenges in DPO and Charting a Path Forward <https://arxiv.org/abs/2406.07327>`__ 3d属性:识别DPO中的挑战并绘制前进道路

::

    Tue, 11 Jun 2024 14:59:24 GMT
    Yuzi Yan, Yibo Miao, Jialian Li, Yipin Zhang, Jian Xie, Zhijie Deng, Dong Yan

Aligning large language models (LLMs) with human preference has recently gained tremendous attention, with the canonical yet costly RLHF-PPO and the simple and straightforward Direct Preference Optimization (DPO) as two examples. Despite the efficiency, DPO has rarely be used in the state-of-the-art production-level LLMs, implying its potential pathologies. In this work, we revisit DPO with a comprehensive examination of its empirical efficacy and a systematic comparison with RLHF-PPO. We identify the \textbf{3D}-properties of DPO's learning outcomes: the \textbf{D}rastic drop in the likelihood of rejected responses, the \textbf{D}egradation into LLM unlearning, and the \textbf{D}ispersion effect on unseen responses through experiments with both a carefully designed toy model and practical LLMs on tasks including mathematical problem-solving and instruction following. These findings inherently connect to some observations made by related works and we additionally contribute a plausible theoretical explanation for them.
Accordingly, we propose easy regularization methods to mitigate the issues caused by \textbf{3D}-properties, improving the training stability and final performance of DPO. Our contributions also include an investigation into how the distribution of the paired preference data impacts the effectiveness of DPO. We hope this work could offer research directions to narrow the gap between reward-free preference learning methods and reward-based ones.

------------

`[2406.07378] Large Language Models for Constrained-Based Causal Discovery <https://arxiv.org/abs/2406.07378>`__ 

::

    Tue, 11 Jun 2024 15:45:24 GMT
    Kai-Hendrik Cohrs, Gherardo Varando, Emiliano Diaz, Vasileios Sitokonstantinou, Gustau Camps-Valls

Causality is essential for understanding complex systems, such as the economy, the brain, and the climate. Constructing causal graphs often relies on either data-driven or expert-driven approaches, both fraught with challenges.
The former methods, like the celebrated PC algorithm, face issues with data requirements and assumptions of causal sufficiency, while the latter demand substantial time and domain knowledge. This work explores the capabilities of Large Language Models (LLMs) as an alternative to domain experts for causal graph generation. We frame conditional independence queries as prompts to LLMs and employ the PC algorithm with the answers. The performance of the LLM-based conditional independence oracle on systems with known causal graphs shows a high degree of variability. We improve the performance through a proposed statistical-inspired voting schema that allows some control over false-positive and false-negative rates. Inspecting the chain-of-thought argumentation, we find causal reasoning to justify its answer to a probabilistic query. We show evidence that knowledge-based CIT could eventually become a complementary tool for data-driven causal discovery.

------------

`[2406.07381] World Models with Hints of Large Language Models for Goal Achieving <https://arxiv.org/abs/2406.07381>`__ 面向目标实现的大型语言模型提示的世界模型

::

    Tue, 11 Jun 2024 15:49:08 GMT
    Zeyuan Liu, Ziyu Huan, Xiyao Wang, Jiafei Lyu, Jian Tao, Xiu Li, Furong Huang, Huazhe Xu

Reinforcement learning struggles in the face of long-horizon tasks and sparse goals due to the difficulty in manual reward specification. While existing methods address this by adding intrinsic rewards, they may fail to provide meaningful guidance in long-horizon decision-making tasks with large state and action spaces, lacking purposeful exploration. Inspired by human cognition, we propose a new multi-modal model-based RL approach named Dreaming with Large Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the LLMs into the model rollouts to encourage goal discovery and reaching in challenging tasks. By assigning higher intrinsic rewards to samples that align with the hints outlined by the language model during model rollouts, DLLM guides the agent toward meaningful and efficient exploration. Extensive experiments demonstrate that the DLLM outperforms recent methods in various challenging, sparse-reward environments such as HomeGrid, Crafter, and Minecraft by 27.7\%, 21.1\%, and 9.9\%, respectively.

------------

`[2406.07394] Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B <https://arxiv.org/abs/2406.07394>`__ 利用LLaMa-3 8B通过蒙特卡罗树自求精访问GPT-4级数学奥林匹克解

::

    Tue, 11 Jun 2024 16:01:07 GMT
    Di Zhang, Jiatong Li, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang

This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.

------------

`[2406.06556] Enhancing Presentation Slide Generation by LLMs with a Multi-Staged End-to-End Approach <https://arxiv.org/abs/2406.06556>`__ 用多阶段端到端方法增强llm的演示文稿幻灯片生成

::

    Sat, 1 Jun 2024 07:49:31 GMT
    Sambaran Bandyopadhyay, Himanshu Maheshwari, Anandhavelu Natarajan, Apoorv Saxena

Generating presentation slides from a long document with multimodal elements such as text and images is an important task. This is time consuming and needs domain expertise if done manually. Existing approaches for generating a rich presentation from a document are often semi-automatic or only put a flat summary into the slides ignoring the importance of a good narrative. In this paper, we address this research gap by proposing a multi-staged end-to-end model which uses a combination of LLM and VLM. We have experimentally shown that compared to applying LLMs directly with state-of-the-art prompting, our proposed multi-staged solution is better in terms of automated metrics and human evaluation.

------------

`[2406.06558] Enhancing Text Authenticity: A Novel Hybrid Approach for AI-Generated Text Detection <https://arxiv.org/abs/2406.06558>`__ 增强文本真实性:一种新的人工智能生成文本检测混合方法

::

    Sat, 1 Jun 2024 10:21:54 GMT
    Ye Zhang, Qian Leng, Mengran Zhu, Rui Ding, Yue Wu, Jintong Song, Yulu Gong

The rapid advancement of Large Language Models (LLMs) has ushered in an era where AI-generated text is increasingly indistinguishable from human-generated content. Detecting AI-generated text has become imperative to combat misinformation, ensure content authenticity, and safeguard against malicious uses of AI. In this paper, we propose a novel hybrid approach that combines traditional TF-IDF techniques with advanced machine learning models, including Bayesian classifiers, Stochastic Gradient Descent (SGD), Categorical Gradient Boosting (CatBoost), and 12 instances of Deberta-v3-large models. Our approach aims to address the challenges associated with detecting AI-generated text by leveraging the strengths of both traditional feature extraction methods and state-of-the-art deep learning models. Through extensive experiments on a comprehensive dataset, we demonstrate the effectiveness of our proposed method in accurately distinguishing between human and AI-generated text. Our approach achieves superior performance compared to existing methods. This research contributes to the advancement of AI-generated text detection techniques and lays the foundation for developing robust solutions to mitigate the challenges posed by AI-generated content.

------------

`[2406.06559] Harnessing Business and Media Insights with Large Language Models <https://arxiv.org/abs/2406.06559>`__ 用大型语言模型利用商业和媒体见解

::

    Sun, 2 Jun 2024 06:24:38 GMT
    Yujia Bao, Ankit Parag Shah, Neeru Narang, Jonathan Rivers, Rajeev Maksey, Lan Guan, Louise N. Barrere, Shelley Evenson, Rahul Basole, Connie Miao, Ankit Mehta, Fabien Boulay, Su Min Park, Natalie E. Pearson, Eldhose Joy, Tiger He, Sumiran Thakur, Koustav Ghosal, Josh On, Phoebe Morrison, Tim Major, Eva Siqi Wang, Gina Escobar, Jiaheng Wei, Tharindu Cyril Weerasooriya, Queena Song, Daria Lashkevich, Clare Chen, Gyuhak Kim, Dengpan Yin, Don Hejna, Mo Nomeli, Wei Wei

This paper introduces Fortune Analytics Language Model (FALM). FALM empowers users with direct access to comprehensive business analysis, including market trends, company performance metrics, and expert insights. Unlike generic LLMs, FALM leverages a curated knowledge base built from professional journalism, enabling it to deliver precise and in-depth answers to intricate business questions. Users can further leverage natural language queries to directly visualize financial data, generating insightful charts and graphs to understand trends across diverse business sectors clearly. FALM fosters user trust and ensures output accuracy through three novel methods: 1) Time-aware reasoning guarantees accurate event registration and prioritizes recent updates. 2) Thematic trend analysis explicitly examines topic evolution over time, providing insights into emerging business landscapes. 3) Content referencing and task decomposition enhance answer fidelity and data visualization accuracy.
We conduct both automated and human evaluations, demonstrating FALM's significant performance improvements over baseline methods while prioritizing responsible AI practices. These benchmarks establish FALM as a cutting-edge LLM in the business and media domains, with exceptional accuracy and trustworthiness.

------------

`[2406.06560] Inverse Constitutional AI: Compressing Preferences into Principles <https://arxiv.org/abs/2406.06560>`__ 反宪法AI:将偏好压缩为原则

::

    Sun, 2 Jun 2024 11:54:50 GMT
    Arduin Findeis, Timo Kaufmann, Eyke H\"ullermeier, Samuel Albanie, Robert Mullins

Feedback data plays an important role in fine-tuning and evaluating state-of-the-art AI models. Often pairwise text preferences are used: given two texts, human (or AI) annotators select the "better" one. Such feedback data is widely used to align models to human preferences (e.g., reinforcement learning from human feedback), or to rank models according to human preferences (e.g., Chatbot Arena). Despite its wide-spread use, prior work has demonstrated that human-annotated pairwise text preference data often exhibits unintended biases.
For example, human annotators have been shown to prefer assertive over truthful texts in certain contexts. Models trained or evaluated on this data may implicitly encode these biases in a manner hard to identify. In this paper, we formulate the interpretation of existing pairwise text preference data as a compression task: the Inverse Constitutional AI (ICAI) problem. In constitutional AI, a set of principles (or constitution) is used to provide feedback and fine-tune AI models. The ICAI problem inverts this process: given a dataset of feedback, we aim to extract a constitution that best enables a large language model (LLM) to reconstruct the original annotations. We propose a corresponding initial ICAI algorithm and validate its generated constitutions quantitatively based on reconstructed annotations. Generated constitutions have many potential use-cases -- they may help identify undesirable biases, scale feedback to unseen data or assist with adapting LLMs to individual user preferences. We demonstrate our approach on a variety of datasets: (a) synthetic feedback datasets with known underlying principles; (b) the AlpacaEval dataset of cross-annotated human feedback; and (c) the crowdsourced Chatbot Arena data set. We release the code for our algorithm and experiments at https://github.com/rdnfn/icai .

------------

`[2406.06562] Achieving Sparse Activation in Small Language Models <https://arxiv.org/abs/2406.06562>`__ 实现小型语言模型的稀疏激活

::

    Mon, 3 Jun 2024 03:21:49 GMT
    Jifeng Song, Kai Huang, Xiangyu Yin, Boyuan Yang, Wei Gao

Sparse activation, which selectively activates only an input-dependent set of neurons in inference, is a useful technique to reduce the computing cost of Large Language Models (LLMs) without retraining or adaptation efforts. However, whether it can be applied to the recently emerging Small Language Models (SLMs) remains questionable, because SLMs are generally less over-parameterized than LLMs. In this paper, we aim to achieve sparse activation in SLMs. We first show that the existing sparse activation schemes in LLMs that build on neurons' output magnitudes cannot be applied to SLMs, and activating neurons based on their attribution scores is a better alternative. Further, we demonstrated and quantified the large errors of existing attribution metrics when being used for sparse activation, due to the interdependency among attribution scores of neurons across different layers. Based on these observations, we proposed a new attribution metric that can provably correct such errors and achieve precise sparse activation. Experiments over multiple popular SLMs and datasets show that our approach can achieve 80% sparsification ratio with <5% model accuracy loss, comparable to the sparse activation achieved in LLMs. The source code is available at: https://github.com/pittisl/Sparse-Activation.

------------

`[2406.06563] Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models <https://arxiv.org/abs/2406.06563>`__ 

::

    Mon, 3 Jun 2024 03:58:41 GMT
    Tianwen Wei, Bo Zhu, Liang Zhao, Cheng Cheng, Biye Li, Weiwei L\"u, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Liang Zeng, Xiaokun Wang, Yutuan Ma, Rui Hu, Shuicheng Yan, Han Fang, Yahui Zhou

In this technical report, we introduce the training methodologies implemented in the development of Skywork-MoE, a high-performance mixture-of-experts (MoE) large language model (LLM) with 146 billion parameters and 16 experts. It is initialized from the pre-existing dense checkpoints of our Skywork-13B model.
We explore the comparative effectiveness of upcycling versus training from scratch initializations. Our findings suggest that the choice between these two approaches should consider both the performance of the existing dense checkpoints and the MoE training budget. We highlight two innovative techniques: gating logit normalization, which improves expert diversification, and adaptive auxiliary loss coefficients, allowing for layer-specific adjustment of auxiliary loss coefficients. Our experimental results validate the effectiveness of these methods. Leveraging these techniques and insights, we trained our upcycled Skywork-MoE on a condensed subset of our SkyPile corpus. The evaluation results demonstrate that our model delivers strong performance across a wide range of benchmarks.

------------

`[2406.06573] MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering <https://arxiv.org/abs/2406.06573>`__ MedFuzz:探索大型语言模型在医疗问答中的鲁棒性

::

    Mon, 3 Jun 2024 18:15:56 GMT
    Robert Osazuwa Ness, Katie Matton, Hayden Helm, Sheng Zhang, Junaid Bajwa, Carey E. Priebe, Eric Horvitz

Large language models (LLM) have achieved impressive performance on medical question-answering benchmarks. However, high benchmark accuracy does not imply that the performance generalizes to real-world clinical settings. Medical question-answering benchmarks rely on assumptions consistent with quantifying LLM performance but that may not hold in the open world of the clinic. Yet LLMs learn broad knowledge that can help the LLM generalize to practical conditions regardless of unrealistic assumptions in celebrated benchmarks. We seek to quantify how well LLM medical question-answering benchmark performance generalizes when benchmark assumptions are violated. Specifically, we present an adversarial method that we call MedFuzz (for medical fuzzing). MedFuzz attempts to modify benchmark questions in ways aimed at confounding the LLM. We demonstrate the approach by targeting strong assumptions about patient characteristics presented in the MedQA benchmark. Successful "attacks" modify a benchmark item in ways that would be unlikely to fool a medical expert but nonetheless "trick" the LLM into changing from a correct to an incorrect answer. Further, we present a permutation test technique that can ensure a successful attack is statistically significant. We show how to use performance on a "MedFuzzed" benchmark, as well as individual successful attacks. The methods show promise at providing insights into the ability of an LLM to operate robustly in more realistic settings.

------------

`[2406.06574] Towards Transparency: Exploring LLM Trainings Datasets through Visual Topic Modeling and Semantic Frame <https://arxiv.org/abs/2406.06574>`__ 迈向透明度:通过视觉主题建模和语义框架探索LLM训练数据集

::

    Mon, 3 Jun 2024 18:44:13 GMT
    Charles de Dampierre, Andrei Mogoutov and Nicolas Baumard

LLMs are now responsible for making many decisions on behalf of humans: from answering questions to classifying things, they have become an important part of everyday life. While computation and model architecture have been rapidly expanding in recent years, the efforts towards curating training datasets are still in their beginnings. This underappreciation of training datasets has led LLMs to create biased and low-quality content. In order to solve that issue, we present Bunka, a software that leverages AI and Cognitive Science to improve the refinement of textual datasets. We show how Topic Modeling coupled with 2-dimensional Cartography can increase the transparency of datasets. We then show how the same Topic Modeling techniques can be applied to Preferences datasets to accelerate the fine-tuning process and increase the capacities of the model on different benchmarks. Lastly, we show how using Frame Analysis can give insights into existing biases in the training corpus. Overall, we argue that we need better tools to explore and increase the quality and transparency of LLMs training datasets.

------------

`[2406.06576] OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step <https://arxiv.org/abs/2406.06576>`__ OccamLLM:一步快速精确语言模型算法

::

    Tue, 4 Jun 2024 04:17:40 GMT
    Owen Dugan, Donato Manuel Jimenez Beneto, Charlotte Loh, Zhuo Chen, Rumen Dangovski, Marin Solja\v{c}i\'c

Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations. To achieve accurate calculations, language model systems often enable LLMs to generate code for arithmetic operations. However, this approach compromises speed and security and, if finetuning is involved, risks the language model losing prior capabilities. We propose a framework that enables exact arithmetic in \textit{a single autoregressive step}, providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities. We use the hidden states of an LLM to control a symbolic architecture which performs arithmetic. Our implementation using Llama 3 8B Instruct with OccamNet as a symbolic model (OccamLlama) achieves 100\% accuracy on single arithmetic operations ($+,-,\times,\div,\sin{},\cos{},\log{},\exp{},\sqrt{}$), outperforming GPT 4o and on par with GPT 4o using a code interpreter. OccamLlama also outperforms both Llama 3 8B Instruct and GPT 3.5 Turbo on multistep reasoning problems involving challenging arithmetic, thus enabling small LLMs to match the arithmetic performance of even much larger models. We will make our code public shortly.

------------

`[2406.06579] From Redundancy to Relevance: Enhancing Explainability in Multimodal Large Language Models <https://arxiv.org/abs/2406.06579>`__ 从冗余到相关:增强多模态大型语言模型的可解释性

::

    Tue, 4 Jun 2024 13:52:54 GMT
    Xiaofeng Zhang, Chen Shen, Xiaosong Yuan, Shaotian Yan, Liang Xie, Wenxiao Wang, Chaochen Gu, Hao Tang, Jieping Ye

Recently, multimodal large language models have exploded with an endless variety, most of the popular Large Vision Language Models (LVLMs) depend on sequential visual representation, where images are converted into hundreds or thousands of tokens before being input into the Large Language Model (LLM) along with language prompts. The black-box design hinders the interpretability of visual-language models, especially regarding more complex reasoning tasks.
To explore the interaction process between image and text in complex reasoning tasks, we introduce the information flow method to visualize the interaction mechanism. By analyzing the dynamic flow of the information flow, we find that the information flow appears to converge in the shallow layer. Further investigation revealed a redundancy of the image token in the shallow layer.
Consequently, a truncation strategy was introduced to aggregate image tokens within these shallow layers. This approach has been validated through experiments across multiple models, yielding consistent improvements.

------------

`[2406.06580] Break the Chain: Large Language Models Can be Shortcut Reasoners <https://arxiv.org/abs/2406.06580>`__ 

::

    Tue, 4 Jun 2024 14:02:53 GMT
    Mengru Ding, Hanmeng Liu, Zhizhang Fu, Jian Song, Wenbo Xie, Yue Zhang

Recent advancements in Chain-of-Thought (CoT) reasoning utilize complex modules but are hampered by high token consumption, limited applicability, and challenges in reproducibility. This paper conducts a critical evaluation of CoT prompting, extending beyond arithmetic to include complex logical and commonsense reasoning tasks, areas where standard CoT methods fall short. We propose the integration of human-like heuristics and shortcuts into language models (LMs) through "break the chain" strategies. These strategies disrupt traditional CoT processes using controlled variables to assess their efficacy.
Additionally, we develop innovative zero-shot prompting strategies that encourage the use of shortcuts, enabling LMs to quickly exploit reasoning clues and bypass detailed procedural steps. Our comprehensive experiments across various LMs, both commercial and open-source, reveal that LMs maintain effective performance with "break the chain" strategies. We also introduce ShortcutQA, a dataset specifically designed to evaluate reasoning through shortcuts, compiled from competitive tests optimized for heuristic reasoning tasks such as forward/backward reasoning and simplification. Our analysis confirms that ShortcutQA not only poses a robust challenge to LMs but also serves as an essential benchmark for enhancing reasoning efficiency in AI.

------------

`[2406.06581] Set-Based Prompting: Provably Solving the Language Model Order Dependency Problem <https://arxiv.org/abs/2406.06581>`__ 集合提示:可证解决语言模型顺序依赖问题

::

    Tue, 4 Jun 2024 16:09:13 GMT
    Reid McIlroy-Young, Katrina Brown, Conlan Olson, Linjun Zhang, Cynthia Dwork

The development of generative language models that can create long and coherent textual outputs via autoregression has lead to a proliferation of uses and a corresponding sweep of analyses as researches work to determine the limitations of this new paradigm. Unlike humans, these 'Large Language Models' (LLMs) are highly sensitive to small changes in their inputs, leading to unwanted inconsistency in their behavior. One problematic inconsistency when LLMs are used to answer multiple-choice questions or analyze multiple inputs is order dependency: the output of an LLM can (and often does) change significantly when sub-sequences are swapped, despite both orderings being semantically identical. In this paper we present , a technique that guarantees the output of an LLM will not have order dependence on a specified set of sub-sequences. We show that this method provably eliminates order dependency, and that it can be applied to any transformer-based LLM to enable text generation that is unaffected by re-orderings. Delving into the implications of our method, we show that, despite our inputs being out of distribution, the impact on expected accuracy is small, where the expectation is over the order of uniformly chosen shuffling of the candidate responses, and usually significantly less in practice. Thus, can be used as a 'dropped-in' method on fully trained models. Finally, we discuss how our method's success suggests that other strong guarantees can be obtained on LLM performance via modifying the input representations.

------------

`[2406.06582] Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing <https://arxiv.org/abs/2406.06582>`__ 基于预训练大型语言模型的离散多模态transformer混合监督语音处理

::

    Tue, 4 Jun 2024 20:08:25 GMT
    Viet Anh Trinh, Rosy Southwell, Yiwen Guan, Xinlu He, Zhiyong Wang, Jacob Whitehill

Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.

------------

`[2406.06584] Evaluating the Efficacy of Large Language Models in Detecting Fake News: A Comparative Analysis <https://arxiv.org/abs/2406.06584>`__ 大型语言模型检测假新闻的有效性评估:对比分析

::

    Wed, 5 Jun 2024 02:55:21 GMT
    Sahas Koka, Anthony Vuong, Anish Kataria

In an era increasingly influenced by artificial intelligence, the detection of fake news is crucial, especially in contexts like election seasons where misinformation can have significant societal impacts. This study evaluates the effectiveness of various LLMs in identifying and filtering fake news content.
Utilizing a comparative analysis approach, we tested four large LLMs -- GPT-4, Claude 3 Sonnet, Gemini Pro 1.0, and Mistral Large -- and two smaller LLMs -- Gemma 7B and Mistral 7B. By using fake news dataset samples from Kaggle, this research not only sheds light on the current capabilities and limitations of LLMs in fake news detection but also discusses the implications for developers and policymakers in enhancing AI-driven informational integrity.

------------

`[2406.06587] Exploring Human-AI Perception Alignment in Sensory Experiences: Do LLMs Understand Textile Hand? <https://arxiv.org/abs/2406.06587>`__ 探索感官体验中的人类- ai感知对齐:llm理解纺织手吗?

::

    Wed, 5 Jun 2024 08:46:07 GMT
    Shu Zhong, Elia Gatti, Youngjun Cho, and Marianna Obrist

Aligning large language models (LLMs) behaviour with human intent is critical for future AI. An important yet often overlooked aspect of this alignment is the perceptual alignment. Perceptual modalities like touch are more multifaceted and nuanced compared to other sensory modalities such as vision.
This work investigates how well LLMs align with human touch experiences using the "textile hand" task. We created a "Guess What Textile" interaction in which participants were given two textile samples -- a target and a reference -- to handle. Without seeing them, participants described the differences between them to the LLM. Using these descriptions, the LLM attempted to identify the target textile by assessing similarity within its high-dimensional embedding space. Our results suggest that a degree of perceptual alignment exists, however varies significantly among different textile samples. For example, LLM predictions are well aligned for silk satin, but not for cotton denim.
Moreover, participants didn't perceive their textile experiences closely matched by the LLM predictions. This is only the first exploration into perceptual alignment around touch, exemplified through textile hand. We discuss possible sources of this alignment variance, and how better human-AI perceptual alignment can benefit future everyday tasks.

------------

`[2406.06589] PatentEval: Understanding Errors in Patent Generation <https://arxiv.org/abs/2406.06589>`__ PatentEval:理解专利生成中的错误

::

    Wed, 5 Jun 2024 13:55:27 GMT
    You Zuo (ALMAnaCH), Kim Gerdes (LISN), Eric Villemonte de La Clergerie (ALMAnaCH), Beno\^it Sagot (ALMAnaCH)

In this work, we introduce a comprehensive error typology specifically designed for evaluating two distinct tasks in machine-generated patent texts: claims-to-abstract generation, and the generation of the next claim given previous ones. We have also developed a benchmark, PatentEval, for systematically assessing language models in this context. Our study includes a comparative analysis, annotated by humans, of various models. These range from those specifically adapted during training for tasks within the patent domain to the latest general-purpose large language models (LLMs). Furthermore, we explored and evaluated some metrics to approximate human judgments in patent text evaluation, analyzing the extent to which these metrics align with expert assessments. These approaches provide valuable insights into the capabilities and limitations of current language models in the specialized field of patent text generation.

------------

`[2406.06590] Are LLMs classical or nonmonotonic reasoners? Lessons from generics <https://arxiv.org/abs/2406.06590>`__ llm是经典推理机还是非单调推理机?来自泛型的教训

::

    Wed, 5 Jun 2024 15:23:11 GMT
    Alina Leidinger, Robert van Rooij, Ekaterina Shutova

Recent scholarship on reasoning in LLMs has supplied evidence of impressive performance and flexible adaptation to machine generated or human feedback.
Nonmonotonic reasoning, crucial to human cognition for navigating the real world, remains a challenging, yet understudied task. In this work, we study nonmonotonic reasoning capabilities of seven state-of-the-art LLMs in one abstract and one commonsense reasoning task featuring generics, such as 'Birds fly', and exceptions, 'Penguins don't fly' (see Fig. 1). While LLMs exhibit reasoning patterns in accordance with human nonmonotonic reasoning abilities, they fail to maintain stable beliefs on truth conditions of generics at the addition of supporting examples ('Owls fly') or unrelated information ('Lions have manes'). Our findings highlight pitfalls in attributing human reasoning behaviours to LLMs, as well as assessing general capabilities, while consistent reasoning remains elusive.

------------

`[2406.06591] Exploring Multilingual Large Language Models for Enhanced TNM classification of Radiology Report in lung cancer staging <https://arxiv.org/abs/2406.06591>`__ 

::

    Wed, 5 Jun 2024 16:11:55 GMT
    Hidetoshi Matsuo, Mizuho Nishio, Takaaki Matsunaga, Koji Fujimoto, Takamichi Murakami

Background: Structured radiology reports remains underdeveloped due to labor-intensive structuring and narrative-style reporting. Deep learning, particularly large language models (LLMs) like GPT-3.5, offers promise in automating the structuring of radiology reports in natural languages. However, although it has been reported that LLMs are less effective in languages other than English, their radiological performance has not been extensively studied.
Purpose: This study aimed to investigate the accuracy of TNM classification based on radiology reports using GPT3.5-turbo (GPT3.5) and the utility of multilingual LLMs in both Japanese and English. Material and Methods: Utilizing GPT3.5, we developed a system to automatically generate TNM classifications from chest CT reports for lung cancer and evaluate its performance. We statistically analyzed the impact of providing full or partial TNM definitions in both languages using a Generalized Linear Mixed Model. Results: Highest accuracy was attained with full TNM definitions and radiology reports in English (M = 94%, N = 80%, T = 47%, and ALL = 36%). Providing definitions for each of the T, N, and M factors statistically improved their respective accuracies (T: odds ratio (OR) = 2.35, p < 0.001; N: OR = 1.94, p < 0.01; M: OR = 2.50, p < 0.001). Japanese reports exhibited decreased N and M accuracies (N accuracy: OR = 0.74 and M accuracy: OR = 0.21). Conclusion: This study underscores the potential of multilingual LLMs for automatic TNM classification in radiology reports. Even without additional model training, performance improvements were evident with the provided TNM definitions, indicating LLMs' relevance in radiology contexts.

------------

`[2406.06596] Are Large Language Models the New Interface for Data Pipelines? <https://arxiv.org/abs/2406.06596>`__ 大型语言模型是数据管道的新接口吗?

::

    Thu, 6 Jun 2024 08:10:32 GMT
    Sylvio Barbon Junior, Paolo Ceravolo, Sven Groppe, Mustafa Jarrar, Samira Maghool, Florence S\`edes, Soror Sahri, and Maurice Van Keulen

A Language Model is a term that encompasses various types of models designed to understand and generate human communication. Large Language Models (LLMs) have gained significant attention due to their ability to process text with human-like fluency and coherence, making them valuable for a wide range of data-related tasks fashioned as pipelines. The capabilities of LLMs in natural language understanding and generation, combined with their scalability, versatility, and state-of-the-art performance, enable innovative applications across various AI-related fields, including eXplainable Artificial Intelligence (XAI), Automated Machine Learning (AutoML), and Knowledge Graphs (KG).
Furthermore, we believe these models can extract valuable insights and make data-driven decisions at scale, a practice commonly referred to as Big Data Analytics (BDA). In this position paper, we provide some discussions in the direction of unlocking synergies among these technologies, which can lead to more powerful and intelligent AI solutions, driving improvements in data pipelines across a wide range of applications and domains integrating humans, computers, and knowledge.

------------

`[2406.06599] Anna Karenina Strikes Again: Pre-Trained LLM Embeddings May Favor High-Performing Learners <https://arxiv.org/abs/2406.06599>`__ Anna Karenina再次强调:预训练的LLM嵌入可能更适合高性能的学习者

::

    Thu, 6 Jun 2024 10:36:48 GMT
    Abigail Gurin Schleifer, Beata Beigman Klebanov, Moriah Ariely and Giora Alexandron

Unsupervised clustering of student responses to open-ended questions into behavioral and cognitive profiles using pre-trained LLM embeddings is an emerging technique, but little is known about how well this captures pedagogically meaningful information. We investigate this in the context of student responses to open-ended questions in biology, which were previously analyzed and clustered by experts into theory-driven Knowledge Profiles (KPs).
Comparing these KPs to ones discovered by purely data-driven clustering techniques, we report poor discoverability of most KPs, except for the ones including the correct answers. We trace this "discoverability bias" to the representations of KPs in the pre-trained LLM embeddings space.

------------

`[2406.06610] Reinterpreting 'the Company a Word Keeps': Towards Explainable and Ontologically Grounded Language Models <https://arxiv.org/abs/2406.06610>`__ 重新解读“一个词保留的公司”:走向可解释和本体论基础的语言模型

::

    Thu, 6 Jun 2024 20:38:35 GMT
    Walid S. Saba

We argue that the relative success of large language models (LLMs) is not a reflection on the symbolic vs. subsymbolic debate but a reflection on employing a successful bottom-up strategy of a reverse engineering of language at scale.
However, and due to their subsymbolic nature whatever knowledge these systems acquire about language will always be buried in millions of weights none of which is meaningful on its own, rendering such systems utterly unexplainable.
Furthermore, and due to their stochastic nature, LLMs will often fail in making the correct inferences in various linguistic contexts that require reasoning in intensional, temporal, or modal contexts. To remedy these shortcomings we suggest employing the same successful bottom-up strategy employed in LLMs but in a symbolic setting, resulting in explainable, language-agnostic, and ontologically grounded language models.

------------

`[2406.06615] Language Guided Skill Discovery <https://arxiv.org/abs/2406.06615>`__ 语言引导的技能发现

::

    Fri, 7 Jun 2024 04:25:38 GMT
    Seungeun Rho, Laura Smith, Tianyu Li, Sergey Levine, Xue Bin Peng, Sehoon Ha

Skill discovery methods enable agents to learn diverse emergent behaviors without explicit rewards. To make learned skills useful for unknown downstream tasks, obtaining a semantically diverse repertoire of skills is essential.
While some approaches introduce a discriminator to distinguish skills and others aim to increase state coverage, no existing work directly addresses the "semantic diversity" of skills. We hypothesize that leveraging the semantic knowledge of large language models (LLMs) can lead us to improve semantic diversity of resulting behaviors. In this sense, we introduce Language Guided Skill Discovery (LGSD), a skill discovery framework that aims to directly maximize the semantic diversity between skills. LGSD takes user prompts as input and outputs a set of semantically distinctive skills. The prompts serve as a means to constrain the search space into a semantically desired subspace, and the generated LLM outputs guide the agent to visit semantically diverse states within the subspace. We demonstrate that LGSD enables legged robots to visit different user-intended areas on a plane by simply changing the prompt.
Furthermore, we show that language guidance aids in discovering more diverse skills compared to five existing skill discovery methods in robot-arm manipulation environments. Lastly, LGSD provides a simple way of utilizing learned skills via natural language.

------------

`[2406.06616] Transforming Dental Diagnostics with Artificial Intelligence: Advanced Integration of ChatGPT and Large Language Models for Patient Care <https://arxiv.org/abs/2406.06616>`__ 用人工智能改造牙科诊断:ChatGPT和大型语言模型的高级集成用于患者护理

::

    Fri, 7 Jun 2024 06:44:09 GMT
    Masoumeh Farhadi Nia, Mohsen Ahmadi, Elyas Irankhah

Artificial intelligence has dramatically reshaped our interaction with digital technologies, ushering in an era where advancements in AI algorithms and Large Language Models (LLMs) have natural language processing (NLP) systems like ChatGPT. This study delves into the impact of cutting-edge LLMs, notably OpenAI's ChatGPT, on medical diagnostics, with a keen focus on the dental sector. Leveraging publicly accessible datasets, these models augment the diagnostic capabilities of medical professionals, streamline communication between patients and healthcare providers, and enhance the efficiency of clinical procedures. The advent of ChatGPT-4 is poised to make substantial inroads into dental practices, especially in the realm of oral surgery. This paper sheds light on the current landscape and explores potential future research directions in the burgeoning field of LLMs, offering valuable insights for both practitioners and developers. Furthermore, it critically assesses the broad implications and challenges within various sectors, including academia and healthcare, thus mapping out an overview of AI's role in transforming dental diagnostics for enhanced patient care.

------------

`[2406.06621] LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering <https://arxiv.org/abs/2406.06621>`__ 

::

    Fri, 7 Jun 2024 15:28:31 GMT
    Harry Li, Gabriel Appleby, Ashley Suh

We present LinkQ, a system that leverages a large language model (LLM) to facilitate knowledge graph (KG) query construction through natural language question-answering. Traditional approaches often require detailed knowledge of complex graph querying languages, limiting the ability for users -- even experts -- to acquire valuable insights from KG data. LinkQ simplifies this process by first interpreting a user's question, then converting it into a well-formed KG query. By using the LLM to construct a query instead of directly answering the user's question, LinkQ guards against the LLM hallucinating or generating false, erroneous information. By integrating an LLM into LinkQ, users are able to conduct both exploratory and confirmatory data analysis, with the LLM helping to iteratively refine open-ended questions into precise ones.
To demonstrate the efficacy of LinkQ, we conducted a qualitative study with five KG practitioners and distill their feedback. Our results indicate that practitioners find LinkQ effective for KG question-answering, and desire future LLM-assisted systems for the exploratory analysis of graph databases.

------------

`[2406.06622] Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs <https://arxiv.org/abs/2406.06622>`__ 对抗性调优:针对llm的越狱攻击防御

::

    Fri, 7 Jun 2024 15:37:15 GMT
    Fan Liu, Zhao Xu, Hao Liu

Although safely enhanced Large Language Models (LLMs) have achieved remarkable success in tackling various complex tasks in a zero-shot manner, they remain susceptible to jailbreak attacks, particularly the unknown jailbreak attack. To enhance LLMs' generalized defense capabilities, we propose a two-stage adversarial tuning framework, which generates adversarial prompts to explore worst-case scenarios by optimizing datasets containing pairs of adversarial prompts and their safe responses. In the first stage, we introduce the hierarchical meta-universal adversarial prompt learning to efficiently and effectively generate token-level adversarial prompts. In the second stage, we propose the automatic adversarial prompt learning to iteratively refine semantic-level adversarial prompts, further enhancing LLM's defense capabilities. We conducted comprehensive experiments on three widely used jailbreak datasets, comparing our framework with six defense baselines under five representative attack scenarios. The results underscore the superiority of our proposed methods. Furthermore, our adversarial tuning framework exhibits empirical generalizability across various attack strategies and target LLMs, highlighting its potential as a transferable defense mechanism.

------------

`[2406.06636] LLM Questionnaire Completion for Automatic Psychiatric Assessment <https://arxiv.org/abs/2406.06636>`__ 精神病学自动评估的LLM问卷完成

::

    Sun, 9 Jun 2024 09:03:11 GMT
    Gony Rosenman, Lior Wolf, Talma Hendler

We employ a Large Language Model (LLM) to convert unstructured psychological interviews into structured questionnaires spanning various psychiatric and personality domains. The LLM is prompted to answer these questionnaires by impersonating the interviewee. The obtained answers are coded as features, which are used to predict standardized psychiatric measures of depression (PHQ-8) and PTSD (PCL-C), using a Random Forest regressor. Our approach is shown to enhance diagnostic accuracy compared to multiple baselines. It thus establishes a novel framework for interpreting unstructured psychological interviews, bridging the gap between narrative-driven and data-driven approaches for mental health assessment.

------------

`[2406.06773] Evaluating Zero-Shot Long-Context LLM Compression <https://arxiv.org/abs/2406.06773>`__ 零样本长上下文LLM压缩的评估

::

    Mon, 10 Jun 2024 20:19:55 GMT
    Chenyu Wang and Yihan Wang

This study evaluates the effectiveness of zero-shot compression techniques on large language models (LLMs) under long-context. We identify the tendency for computational errors to increase under long-context when employing certain compression methods. We propose a hypothesis to explain the varied behavior of different LLM compression techniques and explore remedies to mitigate the performance decline observed in some techniques under long-context. This is a course report for COS 598D Machine Learning and Systems by Prof. Kai Li at Princeton University. Due to limited computational resources, our experiments were conducted only on LLaMA-2-7B-32K.

------------

`[2406.06840] Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles <https://arxiv.org/abs/2406.06840>`__ 无声信号，巨大冲击:用于编码狗哨词义消歧的llm

::

    Mon, 10 Jun 2024 23:09:19 GMT
    Julia Kruk, Michela Marchini, Rijul Ragu, Caleb Ziems, David Muchlinski, Diyi Yang

A dog whistle is a form of coded communication that carries a secondary meaning to specific audiences and is often weaponized for racial and socioeconomic discrimination. Dog whistling historically originated from United States politics, but in recent years has taken root in social media as a means of evading hate speech detection systems and maintaining plausible deniability.
In this paper, we present an approach for word-sense disambiguation of dog whistles from standard speech using Large Language Models (LLMs), and leverage this technique to create a dataset of 16,550 high-confidence coded examples of dog whistles used in formal and informal communication. Silent Signals is the largest dataset of disambiguated dog whistle usage, created for applications in hate speech detection, neology, and political science.
The dataset can be found at https://huggingface.co/datasets/SALT-NLP/silent_signals.

------------

`[2406.06950] A Probabilistic Framework for LLM Hallucination Detection via Belief Tree Propagation <https://arxiv.org/abs/2406.06950>`__ 

::

    Tue, 11 Jun 2024 05:21:37 GMT
    Bairu Hou, Yang Zhang, Jacob Andreas, Shiyu Chang

This paper focuses on the task of hallucination detection, which aims to determine the truthfulness of LLM-generated statements. To address this problem, a popular class of methods utilize the LLM's self-consistencies in its beliefs in a set of logically related augmented statements generated by the LLM, which does not require external knowledge databases and can work with both white-box and black-box LLMs. However, in many existing approaches, the augmented statements tend to be very monotone and unstructured, which makes it difficult to integrate meaningful information from the LLM beliefs in these statements. Also, many methods work with the binarized version of the LLM's belief, instead of the continuous version, which significantly loses information. To overcome these limitations, in this paper, we propose Belief Tree Propagation (BTProp), a probabilistic framework for LLM hallucination detection. BTProp introduces a belief tree of logically related statements by recursively decomposing a parent statement into child statements with three decomposition strategies, and builds a hidden Markov tree model to integrate the LLM's belief scores in these statements in a principled way. Experiment results show that our method improves baselines by 3%-9% (evaluated by AUROC and AUC-PR) on multiple hallucination detection benchmarks. Code is available at https://github.com/UCSB-NLP-Chang/BTProp.

------------

`[2406.06962] Evolving Subnetwork Training for Large Language Models <https://arxiv.org/abs/2406.06962>`__ 面向大型语言模型的演化子网络训练

::

    Tue, 11 Jun 2024 05:44:56 GMT
    Hanqi Li, Lu Chen, Da Ma, Zijian Wu, Su Zhu, Kai Yu

Large language models have ushered in a new era of artificial intelligence research. However, their substantial training costs hinder further development and widespread adoption. In this paper, inspired by the redundancy in the parameters of large language models, we propose a novel training paradigm: Evolving Subnetwork Training (EST). EST samples subnetworks from the layers of the large language model and from commonly used modules within each layer, Multi-Head Attention (MHA) and Multi-Layer Perceptron (MLP). By gradually increasing the size of the subnetworks during the training process, EST can save the cost of training. We apply EST to train GPT2 model and TinyLlama model, resulting in 26.7\% FLOPs saving for GPT2 and 25.0\% for TinyLlama without an increase in loss on the pre-training dataset. Moreover, EST leads to performance improvements in downstream tasks, indicating that it benefits generalization. Additionally, we provide intuitive theoretical studies based on training dynamics and Dropout theory to ensure the feasibility of EST. Our code is available at https://github.com/OpenDFM/EST.

------------

`[2406.07001] Mitigating Boundary Ambiguity and Inherent Bias for Text Classification in the Era of Large Language Models <https://arxiv.org/abs/2406.07001>`__ 在大型语言模型时代缓解文本分类的边界模糊和固有偏见

::

    Tue, 11 Jun 2024 06:53:19 GMT
    Zhenyi Lu, Jie Tian, Wei Wei, Xiaoye Qu, Yu Cheng, Wenfeng xie, Dangyang Chen

Text classification is a crucial task encountered frequently in practical scenarios, yet it is still under-explored in the era of large language models (LLMs). This study shows that LLMs are vulnerable to changes in the number and arrangement of options in text classification. Our extensive empirical analyses reveal that the key bottleneck arises from ambiguous decision boundaries and inherent biases towards specific tokens and positions. To mitigate these issues, we make the first attempt and propose a novel two-stage classification framework for LLMs. Our approach is grounded in the empirical observation that pairwise comparisons can effectively alleviate boundary ambiguity and inherent bias. Specifically, we begin with a self-reduction technique to efficiently narrow down numerous options, which contributes to reduced decision space and a faster comparison process. Subsequently, pairwise contrastive comparisons are employed in a chain-of-thought manner to draw out nuances and distinguish confusable options, thus refining the ambiguous decision boundary. Extensive experiments on four datasets (Banking77, HWU64, LIU54, and Clinic150) verify the effectiveness of our framework. Furthermore, benefitting from our framework, various LLMs can achieve consistent improvements. Our code and data are available in \url{https://github.com/Chuge0335/PC-CoT}.

------------

`[2406.07007] Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference <https://arxiv.org/abs/2406.07007>`__ 蜡笔:通过即时适配器混合和边缘服务器混合推理定制的设备上LLM

::

    Tue, 11 Jun 2024 07:00:08 GMT
    Jihwan Bang, Juntae Lee, Kyuhong Shim, Seunghan Yang, Simyung Chang

The customization of large language models (LLMs) for user-specified tasks gets important. However, maintaining all the customized LLMs on cloud servers incurs substantial memory and computational overheads, and uploading user data can also lead to privacy concerns. On-device LLMs can offer a promising solution by mitigating these issues. Yet, the performance of on-device LLMs is inherently constrained by the limitations of small-scaled models. To overcome these restrictions, we first propose Crayon, a novel approach for on-device LLM customization. Crayon begins by constructing a pool of diverse base adapters, and then we instantly blend them into a customized adapter without extra training. In addition, we develop a device-server hybrid inference strategy, which deftly allocates more demanding queries or non-customized tasks to a larger, more capable LLM on a server. This ensures optimal performance without sacrificing the benefits of on-device customization. We carefully craft a novel benchmark from multiple question-answer datasets, and show the efficacy of our method in the LLM customization.

------------

`[2406.07016] Delving into ChatGPT usage in academic writing through excess vocabulary <https://arxiv.org/abs/2406.07016>`__ 通过词汇量挖掘学术写作中的ChatGPT用法

::

    Tue, 11 Jun 2024 07:16:34 GMT
    Dmitry Kobak, Rita Gonz\'alez M\'arquez, Em\H{o}ke-\'Agnes Horv\'at, Jan Lause

Recent large language models (LLMs) can generate and revise text with human-level performance, and have been widely commercialized in systems like ChatGPT. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists have been using them to assist their scholarly writing. How wide-spread is LLM usage in the academic literature currently? To answer this question, we use an unbiased, large-scale approach, free from any assumptions on academic LLM usage. We study vocabulary changes in 14 million PubMed abstracts from 2010-2024, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. Our analysis based on excess words usage suggests that at least 10% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, and was as high as 30% for some PubMed sub-corpora. We show that the appearance of LLM-based writing assistants has had an unprecedented impact in the scientific literature, surpassing the effect of major world events such as the Covid pandemic.

------------

`[2406.07036] Paying More Attention to Source Context: Mitigating Unfaithful Translations from Large Language Model <https://arxiv.org/abs/2406.07036>`__ 更加关注源上下文:减少大型语言模型的不忠实翻译

::

    Tue, 11 Jun 2024 07:49:04 GMT
    Hongbin Zhang, Kehai Chen, Xuefeng Bai, Yang Xiang, Min Zhang

Large language models (LLMs) have showcased impressive multilingual machine translation ability. However, unlike encoder-decoder style models, decoder-only LLMs lack an explicit alignment between source and target contexts. Analyzing contribution scores during generation processes revealed that LLMs can be biased towards previously generated tokens over corresponding source tokens, leading to unfaithful translations. To address this issue, we propose to encourage LLMs to pay more attention to the source context from both source and target perspectives in zeroshot prompting: 1) adjust source context attention weights; 2) suppress irrelevant target prefix influence; Additionally, we propose 3) avoiding over-reliance on the target prefix in instruction tuning.
Experimental results from both human-collected unfaithfulness test sets focusing on LLM-generated unfaithful translations and general test sets, verify our methods' effectiveness across multiple language pairs. Further human evaluation shows our method's efficacy in reducing hallucinatory translations and facilitating faithful translation generation.

------------

`[2406.07056] Effectively Compress KV Heads for LLM <https://arxiv.org/abs/2406.07056>`__ 有效压缩LLM KV头

::

    Tue, 11 Jun 2024 08:37:33 GMT
    Hao Yu, Zelan Yang, Shen Li, Yong Li, Jianxin Wu

The advent of pre-trained large language models (LLMs) has revolutionized various natural language processing tasks. These models predominantly employ an auto-regressive decoding mechanism that utilizes Key-Value (KV) caches to eliminate redundant calculations for previous tokens. Nevertheless, as context lengths and batch sizes increase, the linear expansion in memory footprint of KV caches becomes a key bottleneck of LLM deployment, which decreases generation speeds significantly. To mitigate this issue, previous techniques like multi-query attention (MQA) and grouped-query attention (GQA) have been developed, in order to reduce KV heads to accelerate inference with comparable accuracy to multi-head attention (MHA). Despite their effectiveness, existing strategies for compressing MHA often overlook the intrinsic properties of the KV caches. In this work, we explore the low-rank characteristics of the KV caches and propose a novel approach for compressing KV heads. In particular, we carefully optimize the MHA-to-GQA transformation to minimize compression error, and to remain compatible with rotary position embeddings (RoPE), we also introduce specialized strategies for key caches with RoPE. We demonstrate that our method can compress half or even three-quarters of KV heads while maintaining performance comparable to the original LLMs, which presents a promising direction for more efficient LLM deployment in resource-constrained environments.

------------

`[2406.07168] Teaching Language Models to Self-Improve by Learning from Language Feedback <https://arxiv.org/abs/2406.07168>`__ 从语言反馈中学习自我提高的语言教学模式

::

    Tue, 11 Jun 2024 11:20:05 GMT
    Chi Hu, Yimin Hu, Hang Cao, Tong Xiao, Jingbo Zhu

Aligning Large Language Models (LLMs) with human intentions and values is crucial yet challenging. Current methods primarily rely on human preferences, which are costly and insufficient in capturing nuanced feedback expressed in natural language. In this paper, we present Self-Refinement Tuning (SRT), a method that leverages model feedback for alignment, thereby reducing reliance on human annotations. SRT uses a base language model (e.g., Tulu2) to generate initial responses, which are critiqued and refined by a more advanced model (e.g., GPT-4-Turbo). This process enables the base model to self-evaluate and improve its outputs, facilitating continuous learning. SRT further optimizes the model by learning from its self-generated feedback and refinements, creating a feedback loop that promotes model improvement. Our empirical evaluations demonstrate that SRT significantly outperforms strong baselines across diverse tasks and model sizes. When applied to a 70B parameter model, SRT increases the win rate from 9.6\% to 25.8\% on the AlpacaEval 2.0 benchmark, surpassing well-established systems such as GPT-4-0314, Claude 2, and Gemini. Our analysis highlights the crucial role of language feedback in the success of SRT, suggesting potential for further exploration in this direction.

------------

`[2406.07188] Merging Improves Self-Critique Against Jailbreak Attacks <https://arxiv.org/abs/2406.07188>`__ 合并改进了对越狱攻击的自我批评

::

    Tue, 11 Jun 2024 12:01:09 GMT
    Victor Gallego

The robustness of large language models (LLMs) against adversarial manipulations, such as jailbreak attacks, remains a significant challenge. In this work, we propose an approach that enhances the self-critique capability of the LLM and further fine-tunes it over sanitized synthetic data. This is done with the addition of an external critic model that can be merged with the original, thus bolstering self-critique capabilities and improving the robustness of the LLMs response to adversarial prompts. Our results demonstrate that the combination of merging and self-critique can reduce the attack success rate of adversaries significantly, thus offering a promising defense mechanism against jailbreak attacks. Code, data and models released at https://github.com/vicgalle/merging-self-critique-jailbreaks .

------------

`[2406.07212] Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems with Large Language Models <https://arxiv.org/abs/2406.07212>`__ 面向医疗保健中的人- ai协作:基于大型语言模型的指导延迟系统

::

    Tue, 11 Jun 2024 12:41:54 GMT
    Joshua Strong, Qianhui Men, Alison Noble

Large language models (LLMs) present a valuable technology for various applications in healthcare, but their tendency to hallucinate introduces unacceptable uncertainty in critical decision-making situations. Human-AI collaboration (HAIC) can mitigate this uncertainty by combining human and AI strengths for better outcomes. This paper presents a novel guided deferral system that provides intelligent guidance when AI defers cases to human decision-makers. We leverage LLMs' verbalisation capabilities and internal states to create this system, demonstrating that fine-tuning smaller LLMs with data from larger models enhances performance while maintaining computational efficiency. A pilot study showcases the effectiveness of our deferral system.

------------

`[2406.07222] Improving Autoformalization using Type Checking <https://arxiv.org/abs/2406.07222>`__ 使用类型检查改进自动形式化

::

    Tue, 11 Jun 2024 13:01:50 GMT
    Auguste Poiroux, Gail Weiss, Viktor Kun\v{c}ak, Antoine Bosselut

Large language models show promise for autoformalization, the task of automatically translating natural language into formal languages. However, current autoformalization methods remain limited. The last reported state-of-the-art performance on the ProofNet formalization benchmark for the Lean proof assistant, achieved using Codex for Lean 3, only showed successful formalization of 16.1% of informal statements. Similarly, our evaluation of GPT-4o for Lean 4 only produces successful translations 34.9% of the time. Our analysis shows that the performance of these models is largely limited by their inability to generate formal statements that successfully type-check (i.e., are syntactically correct and consistent with types) - with a whopping 86.6% of GPT-4o errors starting from a type-check failure. In this work, we propose a method to fix this issue through decoding with type-check filtering, where we initially sample a diverse set of candidate formalizations for an informal statement, then use the Lean proof assistant to filter out candidates that do not type-check. Using GPT-4o as a base model, and combining our method with self-consistency, we obtain a +18.3% absolute increase in formalization accuracy, and achieve a new state-of-the-art of 53.2% on ProofNet with Lean 4.

------------

`[2406.07232] DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms <https://arxiv.org/abs/2406.07232>`__ Dual - reflect:利用双重学习反馈机制增强大型语言模型的反思性翻译

::

    Tue, 11 Jun 2024 13:10:39 GMT
    Andong Chen, Lianzhang Lou, Kehai Chen, Xuefeng Bai, Yang Xiang, Muyun Yang, Tiejun Zhao, Min Zhang

Recently, large language models (LLMs) enhanced by self-reflection have achieved promising performance on machine translation. The key idea is guiding LLMs to generate translation with human-like feedback. However, existing self-reflection methods lack effective feedback information, limiting the translation performance. To address this, we introduce a DUAL-REFLECT framework, leveraging the dual learning of translation tasks to provide effective feedback, thereby enhancing the models' self-reflective abilities and improving translation performance. The application of this method across various translation tasks has proven its effectiveness in improving translation accuracy and eliminating ambiguities, especially in translation tasks with low-resource language pairs.

------------

`[2406.07243] MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs <https://arxiv.org/abs/2406.07243>`__ 

::

    Tue, 11 Jun 2024 13:23:14 GMT
    Vera Neplenbroek, Arianna Bisazza, Raquel Fern\'andez

Generative large language models (LLMs) have been shown to exhibit harmful biases and stereotypes. While safety fine-tuning typically takes place in English, if at all, these models are being used by speakers of many different languages. There is existing evidence that the performance of these models is inconsistent across languages and that they discriminate based on demographic factors of the user. Motivated by this, we investigate whether the social stereotypes exhibited by LLMs differ as a function of the language used to prompt them, while controlling for cultural differences and task accuracy. To this end, we present MBBQ (Multilingual Bias Benchmark for Question-answering), a carefully curated version of the English BBQ dataset extended to Dutch, Spanish, and Turkish, which measures stereotypes commonly held across these languages. We further complement MBBQ with a parallel control dataset to measure task performance on the question-answering task independently of bias.
Our results based on several open-source and proprietary LLMs confirm that some non-English languages suffer from bias more than English, even when controlling for cultural shifts. Moreover, we observe significant cross-lingual differences in bias behaviour for all except the most accurate models. With the release of MBBQ, we hope to encourage further research on bias in multilingual settings.
The dataset and code are available at https://github.com/Veranep/MBBQ.

------------

`[2406.07257] Scholarly Question Answering using Large Language Models in the NFDI4DataScience Gateway <https://arxiv.org/abs/2406.07257>`__ 在NFDI4DataScience网关中使用大型语言模型的学术问答

::

    Tue, 11 Jun 2024 13:36:19 GMT
    Hamed Babaei Giglou, Tilahun Abedissa Taffa, Rana Abdullah, Aida Usmanova, Ricardo Usbeck, Jennifer D'Souza, S\"oren Auer

This paper introduces a scholarly Question Answering (QA) system on top of the NFDI4DataScience Gateway, employing a Retrieval Augmented Generation-based (RAG) approach. The NFDI4DS Gateway, as a foundational framework, offers a unified and intuitive interface for querying various scientific databases using federated search. The RAG-based scholarly QA, powered by a Large Language Model (LLM), facilitates dynamic interaction with search results, enhancing filtering capabilities and fostering a conversational engagement with the Gateway search.
The effectiveness of both the Gateway and the scholarly QA system is demonstrated through experimental analysis.

------------

`[2406.07259] Scientific Computing with Large Language Models <https://arxiv.org/abs/2406.07259>`__ 基于大型语言模型的科学计算

::

    Tue, 11 Jun 2024 13:39:07 GMT
    Christopher Culver, Peter Hicks, Mihailo Milenkovic, Sanjif Shanmugavelu, Tobias Becker

We provide an overview of the emergence of large language models for scientific computing applications. We highlight use cases that involve natural language processing of scientific documents and specialized languages designed to describe physical systems. For the former, chatbot style applications appear in medicine, mathematics and physics and can be used iteratively with domain experts for problem solving. We also review specialized languages within molecular biology, the languages of molecules, proteins, and DNA where language models are being used to predict properties and even create novel physical systems at much faster rates than traditional computing methods.

------------

`[2406.07302] BertaQA: How Much Do Language Models Know About Local Culture? <https://arxiv.org/abs/2406.07302>`__ BertaQA:语言模型对当地文化了解多少?

::

    Tue, 11 Jun 2024 14:30:34 GMT
    Julen Etxaniz and Gorka Azkune and Aitor Soroa and Oier Lopez de Lacalle and Mikel Artetxe

Large Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.

------------

`[2406.07444] On the Robustness of Document-Level Relation Extraction Models to Entity Name Variations <https://arxiv.org/abs/2406.07444>`__ 文档级关系抽取模型对实体名称变化的鲁棒性研究

::

    Tue, 11 Jun 2024 16:51:14 GMT
    Shiao Meng, Xuming Hu, Aiwei Liu, Fukun Ma, Yawen Yang, Shuang Li, Lijie Wen

Driven by the demand for cross-sentence and large-scale relation extraction, document-level relation extraction (DocRE) has attracted increasing research interest. Despite the continuous improvement in performance, we find that existing DocRE models which initially perform well may make more mistakes when merely changing the entity names in the document, hindering the generalization to novel entity names. To this end, we systematically investigate the robustness of DocRE models to entity name variations in this work. We first propose a principled pipeline to generate entity-renamed documents by replacing the original entity names with names from Wikidata. By applying the pipeline to DocRED and Re-DocRED datasets, we construct two novel benchmarks named Env-DocRED and Env-Re-DocRED for robustness evaluation. Experimental results show that both three representative DocRE models and two in-context learned large language models consistently lack sufficient robustness to entity name variations, particularly on cross-sentence relation instances and documents with more entities. Finally, we propose an entity variation robust training method which not only improves the robustness of DocRE models but also enhances their understanding and reasoning capabilities. We further verify that the basic idea of this method can be effectively transferred to in-context learning for DocRE as well.

------------

`[2406.07483] Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing <https://arxiv.org/abs/2406.07483>`__ 社交媒体帖子中的立场标注:大型语言模型与众包的比较分析

::

    Tue, 11 Jun 2024 17:26:07 GMT
    Mao Li, Frederick Conrad

In the rapidly evolving landscape of Natural Language Processing (NLP), the use of Large Language Models (LLMs) for automated text annotation in social media posts has garnered significant interest. Despite the impressive innovations in developing LLMs like ChatGPT, their efficacy, and accuracy as annotation tools are not well understood. In this paper, we analyze the performance of eight open-source and proprietary LLMs for annotating the stance expressed in social media posts, benchmarking their performance against human annotators' (i.e., crowd-sourced) judgments. Additionally, we investigate the conditions under which LLMs are likely to disagree with human judgment. A significant finding of our study is that the explicitness of text expressing a stance plays a critical role in how faithfully LLMs' stance judgments match humans'. We argue that LLMs perform well when human annotators do, and when LLMs fail, it often corresponds to situations in which human annotators struggle to reach an agreement. We conclude with recommendations for a comprehensive approach that combines the precision of human expertise with the scalability of LLM predictions. This study highlights the importance of improving the accuracy and comprehensiveness of automated stance detection, aiming to advance these technologies for more efficient and unbiased analysis of social media.

------------

`[2406.07494] CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization <https://arxiv.org/abs/2406.07494>`__ 

::

    Tue, 11 Jun 2024 17:30:22 GMT
    Frederic Kirstein, Jan Philip Wahle, Bela Gipp, Terry Ruas

Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities. We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement. We observe that only a few datasets span across all subdomains. The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant.

------------

`[2406.07496] TextGrad: Automatic "Differentiation" via Text <https://arxiv.org/abs/2406.07496>`__ TextGrad:通过文本自动“区分”

::

    Tue, 11 Jun 2024 17:32:21 GMT
    Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, James Zou

AI is undergoing a paradigm shift, with breakthroughs achieved by systems orchestrating multiple large language models (LLMs) and other complex components. As a result, developing principled and automated optimization methods for compound AI systems is one of the most important new challenges.
Neural networks faced a similar challenge in its early days until backpropagation and automatic differentiation transformed the field by making optimization turn-key. Inspired by this, we introduce TextGrad, a powerful framework performing automatic ``differentiation'' via text. TextGrad backpropagates textual feedback provided by LLMs to improve individual components of a compound AI system. In our framework, LLMs provide rich, general, natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular structures. TextGrad follows PyTorch's syntax and abstraction and is flexible and easy-to-use. It works out-of-the-box for a variety of tasks, where the users only provide the objective function without tuning components or prompts of the framework. We showcase TextGrad's effectiveness and generality across a diverse range of applications, from question answering and molecule optimization to radiotherapy treatment planning. Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\%$ to $55\%$, yields $20\%$ relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity. TextGrad lays a foundation to accelerate the development of the next-generation of AI systems.

------------

`[2406.07505] THaLLE: Text Hyperlocally Augmented Large Language Extension -- Technical Report <https://arxiv.org/abs/2406.07505>`__ THaLLE:文本超局部增强大型语言扩展-技术报告

::

    Tue, 11 Jun 2024 17:40:00 GMT
    KBTG Labs, Danupat Khamnuansin, Atthakorn Petchsod, Anuruth Lertpiya, Pornchanan Balee, Thanawat Lodkaew, Tawunrat Chalothorn, Thadpong Pongthawornkamol, Monchai Lertsutthiwong

Recent advancements in Large Language Models (LLMs) have revealed new capabilities and opportunities across the technological landscape. However, the practicality of very large LLMs is challenged by their high compute cost, which does not justify the benefits given their limited capability compared to humans. While smaller, more practical LLMs have shown potential in financial analysis, though they are not yet fully proficient, as evidenced by their near-passing performance on the Chartered Financial Analyst (CFA) exam. In this work, we present Financial Analyst Extension to our Text Hyperlocally Augmented Large Language Extension (THaLLE), a series of 8B LLMs consistently achieving highest performance on mock CFA exams against models of comparable size. We thoroughly document the fine-tuning techniques used to facilitate future research. Additionally, we introduce the use of Flare CFA, a publicly available dataset for evaluating LLMs as a financial advisor.

------------

`[2406.06564] Revolutionizing Large Language Model Training through Dynamic Parameter Adjustment <https://arxiv.org/abs/2406.06564>`__ 通过动态参数调整彻底改变大型语言模型训练

::

    Mon, 3 Jun 2024 05:40:34 GMT
    Kaiye Zhou, Shucheng Wang

In the era of large language models, the demand for efficient use of computational resources has become critically important. Although parameter-efficient fine-tuning techniques have achieved results comparable to full fine-tuning, their application during the pre-training phase poses significant challenges. Specifically, employing parameter-efficient strategies at the onset of pre-training can severely compromise efficiency, especially in larger models. In this paper, building upon the fine-tuning method LoRA, we introduce a novel parameter-efficient training technique that frequently alters trainable part of parameters, facilitating effective pre-training. Our method not only achieves memory reductions and computational overhead comparable to current state-of-the-art parameter-efficient algorithms during the pre-training phase but also maintains accuracy levels comparable to those of full pre-training. We provide both theoretical analyses and empirical evidence to demonstrate the effectiveness of our approach.

------------

`[2406.06567] DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion <https://arxiv.org/abs/2406.06567>`__ DHA:基于自适应头融合的Transformer检查点解耦头注意力学习

::

    Mon, 3 Jun 2024 13:28:43 GMT
    Yilong Chen, Linhao Zhang, Junyuan Shang, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun

Large language models (LLMs) with billions of parameters demonstrate impressive performance. However, the widely used Multi-Head Attention (MHA) in LLMs incurs substantial computational and memory costs during inference. While some efforts have optimized attention mechanisms by pruning heads or sharing parameters among heads, these methods often lead to performance degradation or necessitate substantial continued pre-training costs to restore performance.
Based on the analysis of attention redundancy, we design a Decoupled-Head Attention (DHA) mechanism. DHA adaptively configures group sharing for key heads and value heads across various layers, achieving a better balance between performance and efficiency. Inspired by the observation of clustering similar heads, we propose to progressively transform the MHA checkpoint into the DHA model through linear fusion of similar head parameters step by step, retaining the parametric knowledge of the MHA checkpoint. We construct DHA models by transforming various scales of MHA checkpoints given target head budgets. Our experiments show that DHA remarkably requires a mere 0.25\% of the original model's pre-training budgets to achieve 97.6\% of performance while saving 75\% of KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\times$ training acceleration, a maximum of 13.93\% performance improvement under 0.01\% pre-training budget, and 4\% relative improvement under 0.05\% pre-training budget.

------------

`[2406.06600] HORAE: A Domain-Agnostic Modeling Language for Automating Multimodal Service Regulation <https://arxiv.org/abs/2406.06600>`__ HORAE:一种领域无关的多模态服务自动化建模语言

::

    Thu, 6 Jun 2024 13:44:57 GMT
    Yutao Sun, Mingshuai Chen, Kangjia Zhao, He Li, Jintao Chen, Linyu Yang, Zhongyi Wang, Tiancheng Zhao, and Jianwei Yin

Artificial intelligence is rapidly encroaching on the field of service regulation. This work presents the design principles behind HORAE, a unified specification language to model multimodal regulation rules across a diverse set of domains. We show how HORAE facilitates an intelligent service regulation pipeline by further exploiting a fine-tuned large language model named HORAE that automates the HORAE modeling process, thereby yielding an end-to-end framework for fully automated intelligent service regulation.

------------

`[2406.06623] Spectrum: Targeted Training on Signal to Noise Ratio <https://arxiv.org/abs/2406.06623>`__ 频谱:有针对性的信噪比训练

::

    Fri, 7 Jun 2024 21:20:57 GMT
    Eric Hartford and Lucas Atkins and Fernando Fernandes Neto and David Golchinfar

Efficiently post-training large language models remains a challenging task due to the vast computational resources required. We present Spectrum, a method that accelerates LLM training by selectively targeting layer modules based on their signal-to-noise ratio (SNR), and freezing the remaining modules. Our approach, which utilizes an algorithm to compute module SNRs prior to training, has shown to effectively match the performance of full fine-tuning while reducing GPU memory usage. Experiments comparing Spectrum to existing methods such as QLoRA demonstrate its effectiveness in terms of model quality and VRAM efficiency in distributed environments.

------------

`[2406.06858] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion <https://arxiv.org/abs/2406.06858>`__ FLUX:通过内核融合实现gpu上基于软件的快速通信重叠

::

    Tue, 11 Jun 2024 00:17:39 GMT
    Liwen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Xuanrun Zhang, Zuquan Song, Ziheng Jiang, Haibin Lin, Xin Liu

Large deep learning models have demonstrated strong ability to solve many tasks across a wide range of applications. Those large models typically require training and inference to be distributed. Tensor parallelism is a common technique partitioning computation of an operation or layer across devices to overcome the memory capacity limitation of a single processor, and/or to accelerate computation to meet a certain latency requirement. However, this kind of parallelism introduces additional communication that might contribute a significant portion of overall runtime. Thus limits scalability of this technique within a group of devices with high speed interconnects, such as GPUs with NVLinks in a node. This paper proposes a novel method, Flux, to significantly hide communication latencies with dependent computations for GPUs. Flux over-decomposes communication and computation operations into much finer-grained operations and further fuses them into a larger kernel to effectively hide communication without compromising kernel efficiency. Flux can potentially overlap up to 96% of communication given a fused kernel. Overall, it can achieve up to 1.24x speedups for training over Megatron-LM on a cluster of 128 GPUs with various GPU generations and interconnects, and up to 1.66x and 1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8 GPUs with various GPU generations and interconnects.

------------

`[2406.07017] MoreauPruner: Robust Pruning of Large Language Models against Weight Perturbations <https://arxiv.org/abs/2406.07017>`__ 

::

    Tue, 11 Jun 2024 07:19:04 GMT
    Zixiao Wang, Jingwei Zhang, Wenqian Zhao, Farzan Farnia, Bei Yu

Few-shot gradient methods have been extensively utilized in existing model pruning methods, where the model weights are regarded as static values and the effects of potential weight perturbations are not considered. However, the widely used large language models (LLMs) have several billion model parameters, which could increase the fragility of few-shot gradient pruning. In this work, we experimentally show that one-shot gradient pruning algorithms could lead to unstable results under perturbations to model weights. And the minor error of switching between data formats bfloat16 and float16 could result in drastically different outcomes. To address such instabilities, we leverage optimization analysis and propose an LLM structural pruning method, called MoreauPruner, with provable robustness against weight perturbations. In MoreauPruner, the model weight importance is estimated based on the neural network's Moreau envelope, which can be flexibly combined with $\ell_1$-norm regularization techniques to induce the sparsity required in the pruning task. We extensively evaluate the MoreauPruner algorithm on several well-known LLMs, including LLaMA-7B, LLaMA-13B, LLaMA3-8B, and Vicuna-7B. Our numerical results suggest the robustness of MoreauPruner against weight perturbations, and indicate the MoreauPruner's successful accuracy-based scores in comparison to several existing pruning methods. We have released the code in \url{https://github.com/ShiningSord/MoreauPruner}.

------------

`[2406.07025] Entropy-Reinforced Planning with Large Language Models for Drug Discovery <https://arxiv.org/abs/2406.07025>`__ 基于大规模语言模型的熵增强药物发现规划

::

    Tue, 11 Jun 2024 07:29:13 GMT
    Xuefeng Liu, Chih-chan Tien, Peng Ding, Songhao Jiang, Rick L. Stevens

The objective of drug discovery is to identify chemical compounds that possess specific pharmaceutical properties toward a binding target. Existing large language models (LLMS) can achieve high token matching scores in terms of likelihood for molecule generation. However, relying solely on LLM decoding often results in the generation of molecules that are either invalid due to a single misused token, or suboptimal due to unbalanced exploration and exploitation as a consequence of the LLMs prior experience. Here we propose ERP, Entropy-Reinforced Planning for Transformer Decoding, which employs an entropy-reinforced planning algorithm to enhance the Transformer decoding process and strike a balance between exploitation and exploration. ERP aims to achieve improvements in multiple properties compared to direct sampling from the Transformer. We evaluated ERP on the SARS-CoV-2 virus (3CLPro) and human cancer cell target protein (RTCB) benchmarks and demonstrated that, in both benchmarks, ERP consistently outperforms the current state-of-the-art algorithm by 1-5 percent, and baselines by 5-10 percent, respectively. Moreover, such improvement is robust across Transformer models trained with different objectives. Finally, to further illustrate the capabilities of ERP, we tested our algorithm on three code generation benchmarks and outperformed the current state-of-the-art approach as well. Our code is publicly available at: https://github.com/xuefeng-cs/ERP.

------------

`[2406.07177] TernaryLLM: Ternarized Large Language Model <https://arxiv.org/abs/2406.07177>`__ TernaryLLM:三元化大型语言模型

::

    Tue, 11 Jun 2024 11:40:12 GMT
    Tianqi Chen, Zhe Li, Weixiang Xu, Zeyu Zhu, Dong Li, Lu Tian, Emad Barsoum, Peisong Wang, Jian Cheng

Large language models (LLMs) have achieved remarkable performance on Natural Language Processing (NLP) tasks, but they are hindered by high computational costs and memory requirements. Ternarization, an extreme form of quantization, offers a solution by reducing memory usage and enabling energy-efficient floating-point additions. However, applying ternarization to LLMs faces challenges stemming from outliers in both weights and activations. In this work, observing asymmetric outliers and non-zero means in weights, we introduce Dual Learnable Ternarization (DLT), which enables both scales and shifts to be learnable. We also propose Outlier-Friendly Feature Knowledge Distillation (OFF) to recover the information lost in extremely low-bit quantization. The proposed OFF can incorporate semantic information and is insensitive to outliers. At the core of OFF is maximizing the mutual information between features in ternarized and floating-point models using cosine similarity.
Extensive experiments demonstrate that our TernaryLLM surpasses previous low-bit quantization methods on the standard text generation and zero-shot benchmarks for different LLM families. Specifically, for one of the most powerful open-source models, LLaMA-3, our approach (W1.58A16) outperforms the previous state-of-the-art method (W2A16) by 5.8 in terms of perplexity on C4 and by 8.2% in terms of average accuracy on zero-shot tasks.

------------

`[2406.07217] A Synthetic Dataset for Personal Attribute Inference <https://arxiv.org/abs/2406.07217>`__ 用于个人属性推断的合成数据集

::

    Tue, 11 Jun 2024 12:50:53 GMT
    Hanna Yukhymenko, Robin Staab, Mark Vero, Martin Vechev

Recently, powerful Large Language Models (LLMs) have become easily accessible to hundreds of millions of users worldwide. However, their strong capabilities and vast world knowledge do not come without associated privacy risks. In this work, we focus on the emerging privacy threat LLMs pose - the ability to accurately infer personal information from online texts. Despite the growing importance of LLM-based author profiling, research in this area has been hampered by a lack of suitable public datasets, largely due to ethical and privacy concerns associated with real personal data. In this work, we take two steps to address this problem: (i) we construct a simulation framework for the popular social media platform Reddit using LLM agents seeded with synthetic personal profiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic dataset of over 7800 comments manually labeled for personal attributes. We validate our dataset with a human study showing that humans barely outperform random guessing on the task of distinguishing our synthetic comments from real ones. Further, we verify that our dataset enables meaningful personal attribute inference research by showing across 18 state-of-the-art LLMs that our synthetic comments allow us to draw the same conclusions as real-world data. Together, this indicates that our dataset and pipeline provide a strong and privacy-preserving basis for future research toward understanding and mitigating the inference-based privacy threats LLMs pose.

------------

`[2406.07400] Guiding LLM Temporal Logic Generation with Explicit Separation of Data and Control <https://arxiv.org/abs/2406.07400>`__ 以显式的数据和控制分离指导LLM时序逻辑生成

::

    Tue, 11 Jun 2024 16:07:24 GMT
    William Murphy, Nikolaus Holzer, Nathan Koenig, Leyi Cui, Raven Rothkopf, Feitong Qiao, and Mark Santolucito

Temporal logics are powerful tools that are widely used for the synthesis and verification of reactive systems. The recent progress on Large Language Models (LLMs) has the potential to make the process of writing such specifications more accessible. However, writing specifications in temporal logics remains challenging for all but the most expert users. A key question in using LLMs for temporal logic specification engineering is to understand what kind of guidance is most helpful to the LLM and the users to easily produce specifications.
Looking specifically at the problem of reactive program synthesis, we explore the impact of providing an LLM with guidance on the separation of control and data--making explicit for the LLM what functionality is relevant for the specification, and treating the remaining functionality as an implementation detail for a series of pre-defined functions and predicates. We present a benchmark set and find that this separation of concerns improves specification generation. Our benchmark provides a test set against which to verify future work in LLM generation of temporal logic specifications.

------------

`[2406.07455] Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis <https://arxiv.org/abs/2406.07455>`__ 无奖励推理的人工反馈强化学习:无模型算法和依赖实例分析

::

    Tue, 11 Jun 2024 17:01:41 GMT
    Qining Zhang, Honghao Wei, Lei Ying

In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model. We developed a model-free RLHF best policy identification algorithm, called $\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM). The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one. $\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\tilde{\mathcal{O}}(c_{\mathcal{M}}SA^3H^3M\log\frac{1}{\delta})$ which resembles the result in classic RL, where $c_{\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size. Moreover, $\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach. Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift.

------------

`[2406.07457] Estimating the Hallucination Rate of Generative AI <https://arxiv.org/abs/2406.07457>`__ 估计生成式AI的幻觉率

::

    Tue, 11 Jun 2024 17:01:52 GMT
    Andrew Jesson and Nicolas Beltran-Velez and Quentin Chu and Sweta Karlekar and Jannik Kossen and Yarin Gal and John P. Cunningham and David Blei

This work is about estimating the hallucination rate for in-context learning (ICL) with Generative AI. In ICL, a conditional generative model (CGM) is prompted with a dataset and asked to make a prediction based on that dataset.
The Bayesian interpretation of ICL assumes that the CGM is calculating a posterior predictive distribution over an unknown Bayesian model of a latent parameter and data. With this perspective, we define a \textit{hallucination} as a generated prediction that has low-probability under the true latent parameter. We develop a new method that takes an ICL problem -- that is, a CGM, a dataset, and a prediction question -- and estimates the probability that a CGM will generate a hallucination. Our method only requires generating queries and responses from the model and evaluating its response log probability. We empirically evaluate our method on synthetic regression and natural language ICL tasks using large language models.

------------

`[2406.07515] Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement <https://arxiv.org/abs/2406.07515>`__ 

::

    Tue, 11 Jun 2024 17:46:16 GMT
    Yunzhen Feng, Elvis Dohmatob, Pu Yang, Francois Charton, Julia Kempe

Synthesized data from generative models is increasingly considered as an alternative to human-annotated data for fine-tuning Large Language Models. This raises concerns about model collapse: a drop in performance of models fine-tuned on generated data. Considering that it is easier for both humans and machines to tell between good and bad examples than to generate high-quality samples, we investigate the use of feedback on synthesized data to prevent model collapse. We derive theoretical conditions under which a Gaussian mixture classification model can achieve asymptotically optimal performance when trained on feedback-augmented synthesized data, and provide supporting simulations for finite regimes. We illustrate our theoretical predictions on two practical problems: computing matrix eigenvalues with transformers and news summarization with large language models, which both undergo model collapse when trained on model-generated data. We show that training from feedback-augmented synthesized data, either by pruning incorrect predictions or by selecting the best of several guesses, can prevent model collapse, validating popular approaches like RLHF.

------------

`[2406.07528] QuickLLaMA: Query-aware Inference Acceleration for Large Language Models <https://arxiv.org/abs/2406.07528>`__ quicklama:面向大型语言模型的查询感知推理加速

::

    Tue, 11 Jun 2024 17:55:03 GMT
    Jingyao Li, Han Shi, Xin Jiang, Zhenguo Li, Hong Xu, Jiaya Jia

The capacity of Large Language Models (LLMs) to comprehend and reason over long contexts is pivotal for advancements in diverse fields. Yet, they still stuggle with capturing long-distance dependencies within sequences to deeply understand semantics. To address this issue, we introduce Query-aware Inference for LLMs (Q-LLM), a system designed to process extensive sequences akin to human cognition. By focusing on memory data relevant to a given query, Q-LLM can accurately capture pertinent information within a fixed window size and provide precise answers to queries. It doesn't require extra training and can be seamlessly integrated with any LLMs. Q-LLM using LLaMA3 (QuickLLaMA) can read Harry Potter within 30s and accurately answer the questions. Q-LLM improved by 7.17% compared to the current state-of-the-art on LLaMA3, and by 3.26% on Mistral on the $\infty$-bench. In the Needle-in-a-Haystack task, On widely recognized benchmarks, Q-LLM improved upon the current SOTA by 7.0% on Mistral and achieves 100% on LLaMA3. Our code can be found in https://github.com/dvlab-research/Q-LLM.

------------

`[2406.06549] Large Language Model (LLM) for Standard Cell Layout Design Optimization <https://arxiv.org/abs/2406.06549>`__ 面向标准单元布局优化的大型语言模型

::

    Fri, 24 May 2024 04:59:58 GMT
    Chia-Tung Ho, Haoxing Ren

Standard cells are essential components of modern digital circuit designs.
With process technologies advancing toward 2nm, more routability issues have arisen due to the decreasing number of routing tracks, increasing number and complexity of design rules, and strict patterning rules. The state-of-the-art standard cell design automation framework is able to automatically design standard cell layouts in advanced nodes, but it is still struggling to generate highly competitive Performance-Power-Area (PPA) and routable cell layouts for complex sequential cell designs. Consequently, a novel and efficient methodology incorporating the expertise of experienced human designers to incrementally optimize the PPA of cell layouts is highly necessary and essential. High-quality device clustering, with consideration of netlist topology, diffusion sharing/break and routability in the layouts, can reduce complexity and assist in finding highly competitive PPA, and routable layouts faster. In this paper, we leverage the natural language and reasoning ability of Large Language Model (LLM) to generate high-quality cluster constraints incrementally to optimize the cell layout PPA and debug the routability with ReAct prompting. On a benchmark of sequential standard cells in 2nm, we demonstrate that the proposed method not only achieves up to 19.4% smaller cell area, but also generates 23.5% more LVS/DRC clean cell layouts than previous work. In summary, the proposed method not only successfully reduces cell area by 4.65% on average, but also is able to fix routability in the cell layout designs.

------------

`[2406.06637] Exploring the Efficacy of Large Language Models (GPT-4) in Binary Reverse Engineering <https://arxiv.org/abs/2406.06637>`__ 探索大型语言模型(GPT-4)在二进制逆向工程中的功效

::

    Sun, 9 Jun 2024 09:23:58 GMT
    Saman Pordanesh, Benjamin Tan

This study investigates the capabilities of Large Language Models (LLMs), specifically GPT-4, in the context of Binary Reverse Engineering (RE).
Employing a structured experimental approach, we analyzed the LLM's performance in interpreting and explaining human-written and decompiled codes. The research encompassed two phases: the first on basic code interpretation and the second on more complex malware analysis. Key findings indicate LLMs' proficiency in general code understanding, with varying effectiveness in detailed technical and security analyses. The study underscores the potential and current limitations of LLMs in reverse engineering, revealing crucial insights for future applications and improvements. Also, we examined our experimental methodologies, such as methods of evaluation and data constraints, which provided us with a technical vision for any future research activity in this field.

------------

`[2406.06729] Synthetic Query Generation using Large Language Models for Virtual Assistants <https://arxiv.org/abs/2406.06729>`__ 基于大型语言模型的虚拟助手合成查询生成

::

    Mon, 10 Jun 2024 18:50:57 GMT
    Sonal Sannigrahi and Thiago Fraga-Silva and Youssef Oualil and Christophe Van Gysel

Virtual Assistants (VAs) are important Information Retrieval platforms that help users accomplish various tasks through spoken commands. The speech recognition system (speech-to-text) uses query priors, trained solely on text, to distinguish between phonetically confusing alternatives. Hence, the generation of synthetic queries that are similar to existing VA usage can greatly improve upon the VA's abilities -- especially for use-cases that do not (yet) occur in paired audio/text data.
In this paper, we provide a preliminary exploration of the use of Large Language Models (LLMs) to generate synthetic queries that are complementary to template-based methods. We investigate whether the methods (a) generate queries that are similar to randomly sampled, representative, and anonymized user queries from a popular VA, and (b) whether the generated queries are specific.
We find that LLMs generate more verbose queries, compared to template-based methods, and reference aspects specific to the entity. The generated queries are similar to VA user queries, and are specific enough to retrieve the relevant entity. We conclude that queries generated by LLMs and templates are complementary.

------------

`[2406.06730] TRINS: Towards Multimodal Language Models that Can Read <https://arxiv.org/abs/2406.06730>`__ TRINS:面向可阅读的多模态语言模型

::

    Mon, 10 Jun 2024 18:52:37 GMT
    Ruiyi Zhang, Yanzhe Zhang, Jian Chen, Yufan Zhou, Jiuxiang Gu, Changyou Chen, Tong Sun

Large multimodal language models have shown remarkable proficiency in understanding and editing images. However, a majority of these visually-tuned models struggle to comprehend the textual content embedded in images, primarily due to the limitation of training data. In this work, we introduce TRINS: a Text-Rich image INStruction dataset, with the objective of enhancing the reading ability of the multimodal large language model. TRINS is built upon LAION using hybrid data annotation strategies that include machine-assisted and human-assisted annotation processes. It contains 39,153 text-rich images, captions, and 102,437 questions. Specifically, we show that the number of words per annotation in TRINS is significantly longer than that of related datasets, providing new challenges. Furthermore, we introduce a simple and effective architecture, called a Language-vision Reading Assistant (LaRA), which is good at understanding textual content within images. LaRA outperforms existing state-of-the-art multimodal large language models on the TRINS dataset, as well as other classical benchmarks. Lastly, we conducted a comprehensive evaluation with TRINS on various text-rich image understanding and generation tasks, demonstrating its effectiveness.

------------

`[2406.06777] MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension <https://arxiv.org/abs/2406.06777>`__ MolX:基于多模态扩展的分子学习大型语言模型增强

::

    Mon, 10 Jun 2024 20:25:18 GMT
    Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Nitesh V. Chawla

Recently, Large Language Models (LLMs) with their strong task-handling capabilities have shown remarkable advancements across a spectrum of fields, moving beyond natural language understanding. However, their proficiency within the chemistry domain remains restricted, especially in solving professional molecule-related tasks. This challenge is attributed to their inherent limitations in comprehending molecules using only common textual representations, i.e., SMILES strings. In this study, we seek to enhance the ability of LLMs to comprehend molecules by designing and equipping them with a multi-modal external module, namely MolX. In particular, instead of directly using a SMILES string to represent a molecule, we utilize specific encoders to extract fine-grained features from both SMILES string and 2D molecular graph representations for feeding into an LLM. Moreover, a human-defined molecular fingerprint is incorporated to leverage its embedded domain knowledge. Then, to establish an alignment between MolX and the LLM's textual input space, the whole model in which the LLM is frozen, is pre-trained with a versatile strategy including a diverse set of tasks. Extensive experimental evaluations demonstrate that our proposed method only introduces a small number of trainable parameters while outperforming baselines on various downstream molecule-related tasks ranging from molecule-to-text translation to retrosynthesis, with and without fine-tuning the LLM.

------------

`[2406.06822] An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection <https://arxiv.org/abs/2406.06822>`__ 一种llm辅助的易于触发的代码补全模型后门攻击:针对强检测注入伪装漏洞

::

    Mon, 10 Jun 2024 22:10:05 GMT
    Shenao Yan, Shen Wang, Yue Duan, Hanbin Hong, Kiho Lee, Doowon Kim, Yuan Hong

Large Language Models (LLMs) have transformed code completion tasks, providing context-based suggestions to boost developer productivity in software engineering. As users often fine-tune these models for specific applications, poisoning and backdoor attacks can covertly alter the model outputs. To address this critical security challenge, we introduce CodeBreaker, a pioneering LLM-assisted backdoor attack framework on code completion models. Unlike recent attacks that embed malicious payloads in detectable or irrelevant sections of the code (e.g., comments), CodeBreaker leverages LLMs (e.g., GPT-4) for sophisticated payload transformation (without affecting functionalities), ensuring that both the poisoned data for fine-tuning and generated code can evade strong vulnerability detection. CodeBreaker stands out with its comprehensive coverage of vulnerabilities, making it the first to provide such an extensive set for evaluation. Our extensive experimental evaluations and user studies underline the strong attack performance of CodeBreaker across various settings, validating its superiority over existing approaches. By integrating malicious payloads directly into the source code with minimal transformation, CodeBreaker challenges current security measures, underscoring the critical need for more robust defenses for code completion.

------------

`[2406.06864] Validating LLM-Generated Programs with Metamorphic Prompt Testing <https://arxiv.org/abs/2406.06864>`__ 用蜕变提示测试验证llm生成的程序

::

    Tue, 11 Jun 2024 00:40:17 GMT
    Xiaoyin Wang and Dakai Zhu

The latest paradigm shift in software development brings in the innovation and automation afforded by Large Language Models (LLMs), showcased by Generative Pre-trained Transformer (GPT), which has shown remarkable capacity to generate code autonomously, significantly reducing the manual effort required for various programming tasks. Although, the potential benefits of LLM-generated code are vast, most notably in efficiency and rapid prototyping, as LLMs become increasingly integrated into the software development lifecycle and hence the supply chain, complex and multifaceted challenges arise as the code generated from these language models carry profound questions on quality and correctness. Research is required to comprehensively explore these critical concerns surrounding LLM-generated code.
In this paper, we propose a novel solution called metamorphic prompt testing to address these challenges. Our intuitive observation is that intrinsic consistency always exists among correct code pieces but may not exist among flawed code pieces, so we can detect flaws in the code by detecting inconsistencies. Therefore, we can vary a given prompt to multiple prompts with paraphrasing, and to ask the LLM to acquire multiple versions of generated code, so that we can validate whether the semantic relations still hold in the acquired code through cross-validation. Our evaluation on HumanEval shows that metamorphic prompt testing is able to detect 75 percent of the erroneous programs generated by GPT-4, with a false positive rate of 8.6 percent.

------------

`[2406.07230] Needle In A Multimodal Haystack <https://arxiv.org/abs/2406.07230>`__ 多式联运大海捞针

::

    Tue, 11 Jun 2024 13:09:16 GMT
    Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, Xizhou Zhu, Ping Luo, Yu Qiao, Jifeng Dai, Wenqi Shao, Wenhai Wang

With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive. However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored. In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH.

------------

`[2406.06663] SecureNet: A Comparative Study of DeBERTa and Large Language Models for Phishing Detection <https://arxiv.org/abs/2406.06663>`__ SecureNet: DeBERTa与大型语言模型在网络钓鱼检测中的比较研究

::

    Mon, 10 Jun 2024 13:13:39 GMT
    Sakshi Mahendru, Tejul Pandit

Phishing, whether through email, SMS, or malicious websites, poses a major threat to organizations by using social engineering to trick users into revealing sensitive information. It not only compromises company's data security but also incurs significant financial losses. In this paper, we investigate whether the remarkable performance of Large Language Models (LLMs) can be leveraged for particular task like text classification, particularly detecting malicious content and compare its results with state-of-the-art Deberta V3 (DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing) model. We systematically assess the potential and limitations of both approaches using comprehensive public datasets comprising diverse data sources such as email, HTML, URL, SMS, and synthetic data generation. Additionally, we demonstrate how LLMs can generate convincing phishing emails, making it harder to spot scams and evaluate the performance of both models in this context. Our study delves further into the challenges encountered by DeBERTa V3 during its training phases, fine-tuning methodology and transfer learning processes. Similarly, we examine the challenges associated with LLMs and assess their respective performance. Among our experimental approaches, the transformer-based DeBERTa method emerged as the most effective, achieving a test dataset (HuggingFace phishing dataset) recall (sensitivity) of 95.17% closely followed by GPT-4 providing a recall of 91.04%.
We performed additional experiments with other datasets on the trained DeBERTa V3 model and LLMs like GPT 4 and Gemini 1.5. Based on our findings, we provide valuable insights into the effectiveness and robustness of these advanced language models, offering a detailed comparative analysis that can inform future research efforts in strengthening cybersecurity measures for detecting and mitigating phishing threats.

------------

`[2406.07268] Advancing Grounded Multimodal Named Entity Recognition via LLM-Based Reformulation and Box-Based Segmentation <https://arxiv.org/abs/2406.07268>`__ 

::

    Tue, 11 Jun 2024 13:52:29 GMT
    Jinyuan Li, Ziyan Li, Han Li, Jianfei Yu, Rui Xia, Di Sun, Gang Pan

Grounded Multimodal Named Entity Recognition (GMNER) task aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging attributes: 1) The tenuous correlation between images and text on social media contributes to a notable proportion of named entities being ungroundable. 2) There exists a distinction between coarse-grained noun phrases used in similar tasks (e.g., phrase localization) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as connecting bridges. This reformulation brings two benefits: 1) It enables us to optimize the MNER module for optimal MNER performance and eliminates the need to pre-extract region features using object detection methods, thus naturally addressing the two major limitations of existing GMNER methods. 2) The introduction of Entity Expansion Expression module and Visual Entailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG).
This endows the proposed framework with unlimited data and model scalability.
Furthermore, to address the potential ambiguity stemming from the coarse-grained bounding box output in GMNER, we further construct the new Segmented Multimodal Named Entity Recognition (SMNER) task and corresponding Twitter-SMNER dataset aimed at generating fine-grained segmentation masks, and experimentally demonstrate the feasibility and effectiveness of using box prompt-based Segment Anything Model (SAM) to empower any GMNER model with the ability to accomplish the SMNER task. Extensive experiments demonstrate that RiVEG significantly outperforms SoTA methods on four datasets across the MNER, GMNER, and SMNER tasks.

------------

`[2406.07296] Instruct Large Language Models to Drive like Humans <https://arxiv.org/abs/2406.07296>`__ 指导大型语言模型像人一样驾驶

::

    Tue, 11 Jun 2024 14:24:45 GMT
    Ruijun Zhang, Xianda Guo, Wenzhao Zheng, Chenming Zhang, Kurt Keutzer, Long Chen

Motion planning in complex scenarios is the core challenge in autonomous driving. Conventional methods apply predefined rules or learn from driving data to plan the future trajectory. Recent methods seek the knowledge preserved in large language models (LLMs) and apply them in the driving scenarios. Despite the promising results, it is still unclear whether the LLM learns the underlying human logic to drive. In this paper, we propose an InstructDriver method to transform LLM into a motion planner with explicit instruction tuning to align its behavior with humans. We derive driving instruction data based on human logic (e.g., do not cause collisions) and traffic rules (e.g., proceed only when green lights). We then employ an interpretable InstructChain module to further reason the final planning reflecting the instructions. Our InstructDriver allows the injection of human rules and learning from driving data, enabling both interpretability and data scalability. Different from existing methods that experimented on closed-loop or simulated settings, we adopt the real-world closed-loop motion planning nuPlan benchmark for better evaluation. InstructDriver demonstrates the effectiveness of the LLM planner in a real-world closed-loop setting. Our code is publicly available at https://github.com/bonbon-rj/InstructDriver.

------------

`[2406.07310] MM-KWS: Multi-modal Prompts for Multilingual User-defined Keyword Spotting <https://arxiv.org/abs/2406.07310>`__ MM-KWS:多语言自定义关键词识别的多模态提示

::

    Tue, 11 Jun 2024 14:38:29 GMT
    Zhiqi Ai and Zhiyong Chen and Shugong Xu

In this paper, we propose MM-KWS, a novel approach to user-defined keyword spotting leveraging multi-modal enrollments of text and speech templates.
Unlike previous methods that focus solely on either text or speech features, MM-KWS extracts phoneme, text, and speech embeddings from both modalities.
These embeddings are then compared with the query speech embedding to detect the target keywords. To ensure the applicability of MM-KWS across diverse languages, we utilize a feature extractor incorporating several multilingual pre-trained models. Subsequently, we validate its effectiveness on Mandarin and English tasks. In addition, we have integrated advanced data augmentation tools for hard case mining to enhance MM-KWS in distinguishing confusable words.
Experimental results on the LibriPhrase and WenetPhrase datasets demonstrate that MM-KWS outperforms prior methods significantly.

------------

`[2406.07411] VersiCode: Towards Version-controllable Code Generation <https://arxiv.org/abs/2406.07411>`__ VersiCode:面向版本可控的代码生成

::

    Tue, 11 Jun 2024 16:15:06 GMT
    Tongtong Wu, Weigang Wu, Xingyu Wang, Kang Xu, Suyu Ma, Bo Jiang, Ping Yang, Zhenchang Xing, Yuan-Fang Li, Gholamreza Haffari

Significant research has focused on improving the performance of large language model on code-related tasks due to their practical importance.
Although performance is typically evaluated using public benchmark datasets, the existing datasets do not account for the concept of \emph{version}, which is crucial in professional software development. In this paper, we introduce VersiCode, the first comprehensive dataset designed to assess the ability of large language models to generate verifiable code for specific library versions. VersiCode encompasses 300 libraries across more than 2,000 versions spanning 9 years. We design two dedicated evaluation tasks: version-specific code completion (VSCC) and version-aware code editing (VACE). Comprehensive experiments are conducted to benchmark the performance of LLMs, revealing the challenging nature of these tasks and VersiCode, that even state-of-the-art LLMs struggle to generate version-correct code. This dataset, together with the proposed tasks, sheds light on LLMs' capabilities and limitations in handling version-specific code generation, and opens up an important new area of research for further investigation. The resources can be found at https://github.com/wutong8023/VersiCode.

------------

`[2406.07476] VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs <https://arxiv.org/abs/2406.07476>`__ VideoLLaMA 2:推进视频llm中的时空建模和音频理解

::

    Tue, 11 Jun 2024 17:22:23 GMT
    Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, Lidong Bing

In this paper, we present the VideoLLaMA 2, a set of Video Large Language Models (Video-LLMs) designed to enhance spatial-temporal modeling and audio understanding in video and audio-oriented tasks. Building upon its predecessor, VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC) connector, which effectively captures the intricate spatial and temporal dynamics of video data. Additionally, we integrate an Audio Branch into the model through joint training, thereby enriching the multimodal understanding capabilities of the model by seamlessly incorporating audio cues. Comprehensive evaluations on multiple-choice video question answering (MC-VQA), open-ended video question answering (OE-VQA), and video captioning (VC) tasks demonstrate that VideoLLaMA 2 consistently achieves competitive results among open-source models and even gets close to some proprietary models on several benchmarks.
Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and audio-video question-answering (AQA & OE-AVQA) benchmarks over existing models.
These advancements underline VideoLLaMA 2's superior performance in multimodal comprehension, setting a new standard for intelligent video analysis systems.
All models are public to facilitate further research.

------------

`[2406.07502] Image Textualization: An Automatic Framework for Creating Accurate and Detailed Image Descriptions <https://arxiv.org/abs/2406.07502>`__ 图像纹理化:一种自动创建精确和详细的图像描述的框架

::

    Tue, 11 Jun 2024 17:37:45 GMT
    Renjie Pi, Jianshu Zhang, Jipeng Zhang, Rui Pan, Zhekai Chen, Tong Zhang

Image description datasets play a crucial role in the advancement of various applications such as image understanding, text-to-image generation, and text-image retrieval. Currently, image description datasets primarily originate from two sources. One source is the scraping of image-text pairs from the web.
Despite their abundance, these descriptions are often of low quality and noisy.
Another is through human labeling. Datasets such as COCO are generally very short and lack details. Although detailed image descriptions can be annotated by humans, the high annotation cost limits the feasibility. These limitations underscore the need for more efficient and scalable methods to generate accurate and detailed image descriptions. In this paper, we propose an innovative framework termed Image Textualization (IT), which automatically produces high-quality image descriptions by leveraging existing multi-modal large language models (MLLMs) and multiple vision expert models in a collaborative manner, which maximally convert the visual information into text.
To address the current lack of benchmarks for detailed descriptions, we propose several benchmarks for comprehensive evaluation, which verifies the quality of image descriptions created by our framework. Furthermore, we show that LLaVA-7B, benefiting from training on IT-curated descriptions, acquire improved capability to generate richer image descriptions, substantially increasing the length and detail of their output with less hallucination.

------------

`[2406.06531] Quantum Reinforcement Learning in Non-Abelian Environments: Unveiling Novel Formulations and Quantum Advantage Exploration <https://arxiv.org/abs/2406.06531>`__ 非阿贝尔环境中的量子强化学习:新配方的揭开和量子优势探索

::

    Thu, 11 Apr 2024 13:21:49 GMT
    Shubhayan Ghosal

This paper delves into recent advancements in Quantum Reinforcement Learning (QRL), particularly focusing on non-commutative environments, which represent uncharted territory in this field. Our research endeavors to redefine the boundaries of decision-making by introducing formulations and strategies that harness the inherent properties of quantum systems.
At the core of our investigation characterization of the agent's state space within a Hilbert space ($\mathcal{H}$). Here, quantum states emerge as complex superpositions of classical state introducing non-commutative quantum actions governed by unitary operators, necessitating a reimagining of state transitions. Complementing this framework is a refined reward function, rooted in quantum mechanics as a Hermitian operator on $\mathcal{H}$. This reward function serves as the foundation for the agent's decision-making process. By leveraging the quantum Bellman equation, we establish a methodology for maximizing expected cumulative reward over an infinite horizon, considering the entangled dynamics of quantum systems. We also connect the Quantum Bellman Equation to the Degree of Non Commutativity of the Environment, evident in Pure Algebra.
We design a quantum advantage function. This ingeniously designed function exploits latent quantum parallelism inherent in the system, enhancing the agent's decision-making capabilities and paving the way for exploration of quantum advantage in uncharted territories. Furthermore, we address the significant challenge of quantum exploration directly, recognizing the limitations of traditional strategies in this complex environment.

------------

`[2406.06855] Design and Scheduling of an AI-based Queueing System <https://arxiv.org/abs/2406.06855>`__ 基于人工智能的排队系统的设计与调度

::

    Tue, 11 Jun 2024 00:01:42 GMT
    Jiung Lee, Hongseok Namkoong, Yibo Zeng

To leverage prediction models to make optimal scheduling decisions in service systems, we must understand how predictive errors impact congestion due to externalities on the delay of other jobs. Motivated by applications where prediction models interact with human servers (e.g., content moderation), we consider a large queueing system comprising of many single server queues where the class of a job is estimated using a prediction model. By characterizing the impact of mispredictions on congestion cost in heavy traffic, we design an index-based policy that incorporates the predicted class information in a near-optimal manner. Our theoretical results guide the design of predictive models by providing a simple model selection procedure with downstream queueing performance as a central concern, and offer novel insights on how to design queueing systems with AI-based triage. We illustrate our framework on a content moderation task based on real online comments, where we construct toxicity classifiers by finetuning large language models.

------------

`[2402.05894] Large Language Model Meets Graph Neural Network in Knowledge Distillation <https://arxiv.org/abs/2402.05894>`__ 知识蒸馏中大型语言模型与图神经网络的相遇

::

    replaced with revised version Tue, 11 Jun 2024 13:17:12 GMT
    Submission history From: Shengxiang Hu [view email]
    [v1] Thu, 8 Feb 2024 18:33:21 UTC (437 KB)
    [v2] Fri, 9 Feb 2024 08:08:57 UTC (437 KB)
    [v3] Wed, 5 Jun 2024 11:38:12 UTC (799 KB)
    [v4] Tue, 11 Jun 2024 13:17:12 UTC (1,335 KB)
    Shengxiang Hu, Guobing Zou, Song Yang, Yanglan Gan, Bofeng Zhang, Yixin Chen

In service-oriented architectures, accurately predicting the Quality of Service (QoS) is crucial for maintaining reliability and enhancing user satisfaction. However, significant challenges remain due to existing methods always overlooking high-order latent collaborative relationships between users and services and failing to dynamically adjust feature learning for every specific user-service invocation, which are critical for learning accurate features. Additionally, reliance on RNNs for capturing QoS evolution hampers models' ability to detect long-term trends due to difficulties in managing long-range dependencies. To address these challenges, we propose the \underline{T}arget-Prompt \underline{O}nline \underline{G}raph \underline{C}ollaborative \underline{L}earning (TOGCL) framework for temporal-aware QoS prediction. TOGCL leverages a dynamic user-service invocation graph to model historical interactions, providing a comprehensive representation of user-service relationships. Building on this graph, it develops a target-prompt graph attention network to extract online deep latent features of users and services at each time slice, simultaneously considering implicit collaborative relationships between target users/services and their neighbors, as well as relevant historical QoS values. Additionally, a multi-layer Transformer encoder is employed to uncover temporal feature evolution patterns of users and services, leading to temporal-aware QoS prediction. Extensive experiments conducted on the WS-DREAM dataset demonstrate that our proposed TOGCL framework significantly outperforms state-of-the-art methods across multiple metrics, achieving improvements of up to 38.80\%. These results underscore the effectiveness of the TOGCL framework for precise temporal QoS prediction.

------------

`[2403.14077] Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics <https://arxiv.org/abs/2403.14077>`__ ChatGPT能检测DeepFakes吗?使用多模态大型语言模型进行媒体取证的研究

::

    replaced with revised version Tue, 11 Jun 2024 16:24:45 GMT
    Submission history From: Shan Jia [view email]
    [v1] Thu, 21 Mar 2024 01:57:30 UTC (34,762 KB)
    [v2] Tue, 26 Mar 2024 16:02:36 UTC (34,762 KB)
    [v3] Tue, 16 Apr 2024 01:08:20 UTC (39,214 KB)
    [v4] Tue, 11 Jun 2024 16:24:45 UTC (36,151 KB)
    Shan Jia, Reilin Lyu, Kangran Zhao, Yize Chen, Zhiyuan Yan, Yan Ju, Chuanbo Hu, Xin Li, Baoyuan Wu, Siwei Lyu

DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.

------------

`[2405.12147] Eliciting Problem Specifications via Large Language Models <https://arxiv.org/abs/2405.12147>`__ 

::

    replaced with revised version Mon, 10 Jun 2024 19:05:57 GMT
    Submission history From: Robert Wray [view email]
    [v1] Mon, 20 May 2024 16:19:02 UTC (6,227 KB)
    [v2] Mon, 10 Jun 2024 19:05:57 UTC (7,310 KB)
    Robert E. Wray, James R. Kirk, John E. Laird

Cognitive systems generally require a human to translate a problem definition into some specification that the cognitive system can use to attempt to solve the problem or perform the task. In this paper, we illustrate that large language models (LLMs) can be utilized to map a problem class, defined in natural language, into a semi-formal specification that can then be utilized by an existing reasoning and learning system to solve instances from the problem class. We present the design of LLM-enabled cognitive task analyst agent(s). Implemented with LLM agents, this system produces a definition of problem spaces for tasks specified in natural language. LLM prompts are derived from the definition of problem spaces in the AI literature and general problem-solving strategies (Polya's How to Solve It). A cognitive system can then use the problem-space specification, applying domain-general problem solving strategies ("weak methods" such as search), to solve multiple instances of problems from the problem class. This result, while preliminary, suggests the potential for speeding cognitive systems research via disintermediation of problem formulation while also retaining core capabilities of cognitive systems, such as robust inference and online learning.

------------

`[2305.12307] OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing <https://arxiv.org/abs/2305.12307>`__ OntoType:本体引导的预训练语言模型辅助的细粒度实体分类

::

    replaced with revised version Tue, 11 Jun 2024 16:16:19 GMT
    Submission history From: Tanay Komarlu [view email]
    [v1] Sun, 21 May 2023 00:32:37 UTC (1,458 KB)
    [v2] Mon, 10 Jun 2024 01:12:48 UTC (1,607 KB)
    [v3] Tue, 11 Jun 2024 16:16:19 UTC (1,623 KB)
    Tanay Komarlu, Minhao Jiang, Xuan Wang, Jiawei Han

Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, fine-grained semantic types, is a basic but important task for knowledge extraction from unstructured text. FET has been studied extensively in natural language processing and typically relies on human-annotated corpora for training, which is costly and difficult to scale. Recent studies explore the utilization of pre-trained language models (PLMs) as a knowledge base to generate rich and context-aware weak supervision for FET. However, a PLM still requires direction and guidance to serve as a knowledge base as they often generate a mixture of rough and fine-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel annotation-free, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot fine-grained entity typing methods as well as a typical LLM method, ChatGPT. Our error analysis shows that refinement of the existing ontology structures will further improve fine-grained entity typing.

------------

`[2308.04913] LLaMA-E: Empowering E-commerce Authoring with Object-Interleaved Instruction Following <https://arxiv.org/abs/2308.04913>`__ LLaMA-E:基于对象交错指令遵循的电子商务创作

::

    replaced with revised version Tue, 11 Jun 2024 02:14:06 GMT
    Submission history From: Kaize Shi [view email]
    [v1] Wed, 9 Aug 2023 12:26:37 UTC (2,692 KB)
    [v2] Tue, 11 Jun 2024 02:14:06 UTC (10,459 KB)
    Kaize Shi, Xueyao Sun, Dingxian Wang, Yinlin Fu, Guandong Xu, Qing Li

E-commerce authoring entails creating engaging, diverse, and targeted content to enhance preference elicitation and retrieval experience. While Large Language Models (LLMs) have revolutionized content generation, they often fall short in e-commerce applications due to their limited memorization of domain-specific features. This paper proposes LLaMA-E, the unified e-commerce authoring models that address the contextual preferences of customers, sellers, and platforms, the essential objects in e-commerce operation. We design the instruction set derived from tasks of ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general e-commerce Q&A. The instruction formulation ensures the interleaved cover of the presented and required object features, allowing the alignment of base models to parameterise e-commerce knowledge comprehensively. The proposed LLaMA-E models achieve state-of-the-art evaluation performance and exhibit the advantage in zero-shot practical applications. To our knowledge, this is the first LLM tailored to empower authoring applications with comprehensive scenario understanding by integrating features focused on participated objects.

------------

`[2309.04550] Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges <https://arxiv.org/abs/2309.04550>`__ 用llm从电子病历中检索证据:可能性和挑战

::

    replaced with revised version Mon, 10 Jun 2024 20:11:42 GMT
    Submission history From: Hiba Ahsan [view email]
    [v1] Fri, 8 Sep 2023 18:44:47 UTC (593 KB)
    [v2] Sun, 3 Mar 2024 18:48:02 UTC (346 KB)
    [v3] Mon, 10 Jun 2024 20:11:42 UTC (347 KB)
    Hiba Ahsan, Denis Jered McInerney, Jisoo Kim, Christopher Potter, Geoffrey Young, Silvio Amir, Byron C. Wallace

Unstructured data in Electronic Health Records (EHRs) often contains critical information -- complementary to imaging -- that could inform radiologists' diagnoses. But the large volume of notes often associated with patients together with time constraints renders manually identifying relevant evidence practically infeasible. In this work we propose and evaluate a zero-shot strategy for using LLMs as a mechanism to efficiently retrieve and summarize unstructured evidence in patient EHR relevant to a given query. Our method entails tasking an LLM to infer whether a patient has, or is at risk of, a particular condition on the basis of associated notes; if so, we ask the model to summarize the supporting evidence. Under expert evaluation, we find that this LLM-based approach provides outputs consistently preferred to a pre-LLM information retrieval baseline. Manual evaluation is expensive, so we also propose and validate a method using an LLM to evaluate (other) LLM outputs for this task, allowing us to scale up evaluation. Our findings indicate the promise of LLMs as interfaces to EHR, but also highlight the outstanding challenge posed by "hallucinations". In this setting, however, we show that model confidence in outputs strongly correlates with faithful summaries, offering a practical means to limit confabulations.

------------

`[2310.02174] Ask Again, Then Fail: Large Language Models' Vacillations in Judgment <https://arxiv.org/abs/2310.02174>`__ 再问，再失败:大型语言模型的判断摇摆不定

::

    replaced with revised version Tue, 11 Jun 2024 15:22:07 GMT
    Submission history From: Qiming Xie [view email]
    [v1] Tue, 3 Oct 2023 16:08:41 UTC (1,707 KB)
    [v2] Mon, 26 Feb 2024 08:26:30 UTC (2,268 KB)
    [v3] Tue, 27 Feb 2024 14:17:53 UTC (2,268 KB)
    [v4] Wed, 28 Feb 2024 03:29:56 UTC (2,268 KB)
    [v5] Tue, 11 Jun 2024 15:22:07 UTC (2,718 KB)
    Qiming Xie, Zengzhi Wang, Yi Feng, and Rui Xia

We observe that current conversational language models often waver in their judgments when faced with follow-up questions, even if the original judgment was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a \textsc{Follow-up Questioning Mechanism} along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework \textsc{Unwavering-FQ} that teaches language models to maintain their originally correct judgments through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models.

------------

`[2311.07470] Finding and Editing Multi-Modal Neurons in Pre-Trained Transformers <https://arxiv.org/abs/2311.07470>`__ 在预训练transformer中发现和编辑多模态神经元

::

    replaced with revised version Tue, 11 Jun 2024 12:30:02 GMT
    Submission history From: Haowen Pan [view email]
    [v1] Mon, 13 Nov 2023 17:03:02 UTC (7,365 KB)
    [v2] Tue, 11 Jun 2024 12:30:02 UTC (12,765 KB)
    Haowen Pan, Yixin Cao, Xiaozhi Wang, Xun Yang, Meng Wang

Understanding the internal mechanisms by which multi-modal large language models (LLMs) interpret different modalities and integrate cross-modal representations is becoming increasingly critical for continuous improvements in both academia and industry. In this paper, we propose a novel method to identify key neurons for interpretability -- how multi-modal LLMs bridge visual and textual concepts for captioning. Our method improves conventional works upon efficiency and applied range by removing needs of costly gradient computation. Based on those identified neurons, we further design a multi-modal knowledge editing method, beneficial to mitigate sensitive words or hallucination. For rationale of our design, we provide theoretical assumption. For empirical evaluation, we have conducted extensive quantitative and qualitative experiments. The results not only validate the effectiveness of our methods, but also offer insightful findings that highlight three key properties of multi-modal neurons: sensitivity, specificity and causal-effect, to shed light for future research.

------------

`[2311.08718] Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling <https://arxiv.org/abs/2311.08718>`__ 基于输入澄清集成的大型语言模型不确定性分解

::

    replaced with revised version Mon, 10 Jun 2024 21:17:24 GMT
    Submission history From: Bairu Hou [view email]
    [v1] Wed, 15 Nov 2023 05:58:35 UTC (155 KB)
    [v2] Mon, 10 Jun 2024 21:17:24 UTC (293 KB)
    Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, Yang Zhang

Uncertainty decomposition refers to the task of decomposing the total uncertainty of a predictive model into aleatoric (data) uncertainty, resulting from inherent randomness in the data-generating process, and epistemic (model) uncertainty, resulting from missing information in the model's training data. In large language models (LLMs) specifically, identifying sources of uncertainty is an important step toward improving reliability, trustworthiness, and interpretability, but remains an important open research question. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarification ensembling, which can be applied to any pre-trained LLM. Our approach generates a set of clarifications for the input, feeds them into an LLM, and ensembles the corresponding predictions. We show that, when aleatoric uncertainty arises from ambiguity or under-specification in LLM inputs, this approach makes it possible to factor an (unclarified) LLM's predictions into separate aleatoric and epistemic terms, using a decomposition similar to the one employed by Bayesian neural networks. Empirical evaluations demonstrate that input clarification ensembling provides accurate and reliable uncertainty quantification on several language processing tasks. Code and data are available at this https URL.

------------

`[2312.02073] A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia <https://arxiv.org/abs/2312.02073>`__ 矩阵出了故障?定位和检测基于假百科的语言模型

::

    replaced with revised version Tue, 11 Jun 2024 02:37:23 GMT
    Submission history From: Giovanni Monea [view email]
    [v1] Mon, 4 Dec 2023 17:35:42 UTC (7,449 KB)
    [v2] Tue, 20 Feb 2024 17:27:17 UTC (7,816 KB)
    [v3] Tue, 11 Jun 2024 02:37:23 UTC (510 KB)
    Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre K{\i}c{\i}man, Hamid Palangi, Barun Patra, Robert West

Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.

------------

`[2402.02872] How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning <https://arxiv.org/abs/2402.02872>`__ 大型语言模型如何在上下文中学习?上下文中头的查询矩阵和关键矩阵是度量学习的双塔

::

    replaced with revised version Tue, 11 Jun 2024 12:58:51 GMT
    Submission history From: Zeping Yu [view email]
    [v1] Mon, 5 Feb 2024 10:39:32 UTC (8,788 KB)
    [v2] Tue, 11 Jun 2024 12:58:51 UTC (8,389 KB)
    Zeping Yu, Sophia Ananiadou

We investigate the mechanism of in-context learning (ICL) on sentence classification tasks with semantically-unrelated labels ("foo"/"bar"). We find intervening in only 1\% heads (named "in-context heads") significantly affects ICL accuracy from 87.6\% to 24.4\%. To understand this phenomenon, we analyze the value-output vectors in these heads and discover that the vectors at each label position contain substantial information about the corresponding labels. Furthermore, we observe that the prediction shift from "foo" to "bar" is due to the respective reduction and increase in these heads' attention scores at "foo" and "bar" positions. Therefore, we propose a hypothesis for ICL: in in-context heads, the value-output matrices extract label features, while the query-key matrices compute the similarity between the features at the last position and those at each label position. The query and key matrices can be considered as two towers that learn the similarity metric between the last position's features and each demonstration at label positions. Using this hypothesis, we explain the majority label bias and recency bias in ICL and propose two methods to reduce these biases by 22\% and 17\%, respectively.

------------

`[2402.09267] Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation <https://arxiv.org/abs/2402.09267>`__ 面向事实性的自我对齐:通过自我评估减轻llm中的幻觉

::

    replaced with revised version Tue, 11 Jun 2024 12:22:14 GMT
    Submission history From: Xiaoying Zhang [view email]
    [v1] Wed, 14 Feb 2024 15:52:42 UTC (1,326 KB)
    [v2] Tue, 11 Jun 2024 12:22:14 UTC (1,338 KB)
    Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, Helen Meng

Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. "hallucinations", even when they hold relevant knowledge. To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN.

------------

`[2402.17882] BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra <https://arxiv.org/abs/2402.17882>`__ BlendSQL:一种可扩展的关系代数混合问答统一方言

::

    replaced with revised version Mon, 10 Jun 2024 20:27:28 GMT
    Submission history From: Parker Glenn [view email]
    [v1] Tue, 27 Feb 2024 20:48:24 UTC (1,834 KB)
    [v2] Mon, 10 Jun 2024 20:27:28 UTC (4,252 KB)
    Parker Glenn, Parag Pravin Dakle, Liang Wang, Preethi Raghavan

Many existing end-to-end systems for hybrid question answering tasks can often be boiled down to a "prompt-and-pray" paradigm, where the user has limited control and insight into the intermediate reasoning steps used to achieve the final result. Additionally, due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting. We introduce BlendSQL, a superset of SQLite to act as a unified dialect for orchestrating reasoning across both unstructured and structured data. For hybrid question answering tasks involving multi-hop reasoning, we encode the full decomposed reasoning roadmap into a single interpretable BlendSQL query. Notably, we show that BlendSQL can scale to massive datasets and improve the performance of end-to-end systems while using 35% fewer tokens. Our code is available and installable as a package at this https URL.

------------

`[2403.03329] Guardrail Baselines for Unlearning in LLMs <https://arxiv.org/abs/2403.03329>`__ llm中忘记学习的护栏基线

::

    replaced with revised version Tue, 11 Jun 2024 15:47:39 GMT
    Submission history From: Pratiksha Thaker [view email]
    [v1] Tue, 5 Mar 2024 21:19:06 UTC (69 KB)
    [v2] Thu, 6 Jun 2024 19:45:09 UTC (77 KB)
    [v3] Tue, 11 Jun 2024 15:47:39 UTC (77 KB)
    Pratiksha Thaker, Yash Maurya, Shengyuan Hu, Zhiwei Steven Wu, Virginia Smith

Recent work has demonstrated that finetuning is a promising approach to 'unlearn' concepts from large language models. However, finetuning can be expensive, as it requires both generating a set of examples and running iterations of finetuning to update the model. In this work, we show that simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to finetuning. We recommend that researchers investigate these lightweight baselines when evaluating the performance of more computationally intensive finetuning methods. While we do not claim that methods such as prompting or filtering are universal solutions to the problem of unlearning, our work suggests the need for evaluation metrics that can better separate the power of guardrails vs. finetuning, and highlights scenarios where guardrails expose possible unintended behavior in existing metrics and benchmarks.

------------

`[2403.07556] Truth-Aware Context Selection: Mitigating Hallucinations of Large Language Models Being Misled by Untruthful Contexts <https://arxiv.org/abs/2403.07556>`__ 真值感知语境选择:缓解大型语言模型被不真实语境误导的幻觉

::

    replaced with revised version Tue, 11 Jun 2024 03:53:10 GMT
    Submission history From: Tian Yu [view email]
    [v1] Tue, 12 Mar 2024 11:40:44 UTC (3,847 KB)
    [v2] Thu, 14 Mar 2024 02:40:22 UTC (3,847 KB)
    [v3] Tue, 11 Jun 2024 03:53:10 UTC (3,856 KB)
    Tian Yu, Shaolei Zhang and Yang Feng

Although Large Language Models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by untruthful contexts provided by users or knowledge augmentation tools, leading to hallucinations. To alleviate LLMs from being misled by untruthful context and take advantage of knowledge augmentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to adaptively recognize and mask untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information. Experimental results indicate that TACS can effectively filter untruthful context and significantly improve the overall quality of LLMs' responses when presented with misleading information.

------------

`[2403.16792] Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback <https://arxiv.org/abs/2403.16792>`__ 基于编译器反馈的精确代码生成的项目级代码上下文迭代精化

::

    replaced with revised version Tue, 11 Jun 2024 02:38:20 GMT
    Submission history From: Zhangqian Bi [view email]
    [v1] Mon, 25 Mar 2024 14:07:27 UTC (4,725 KB)
    [v2] Tue, 2 Apr 2024 08:17:12 UTC (4,732 KB)
    [v3] Tue, 11 Jun 2024 02:38:20 UTC (5,342 KB)
    Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin Lu, Zili Zhang, Yulei Sui, Hai Jin, Xuanhua Shi

Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.

------------

`[2403.16854] An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing <https://arxiv.org/abs/2403.16854>`__ 一个专家值一个令牌:通过专家令牌路由将多个专家llm协同为通才

::

    replaced with revised version Tue, 11 Jun 2024 15:12:09 GMT
    Submission history From: Ziwei Chai [view email]
    [v1] Mon, 25 Mar 2024 15:17:05 UTC (449 KB)
    [v2] Thu, 2 May 2024 08:57:38 UTC (448 KB)
    [v3] Tue, 11 Jun 2024 15:12:09 UTC (451 KB)
    Ziwei Chai, Guoyin Wang, Jing Su, Tianjie Zhang, Xuanwen Huang, Xuwu Wang, Jingjing Xu, Jianbo Yuan, Hongxia Yang, Fei Wu, Yang Yang

We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert LLMs. Our framework represents expert LLMs as special expert tokens within the vocabulary of a meta LLM. The meta LLM can route to an expert LLM like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert LLMs from existing instruction dataset but also allows for dynamic extension of new expert LLMs in a plug-and-play manner. It also conceals the detailed collaboration process from the user's perspective, facilitating interaction as though it were a singular LLM. Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.

------------

`[2404.03565] Personalized LLM Response Generation with Parameterized Memory Injection <https://arxiv.org/abs/2404.03565>`__ 基于参数化内存注入的个性化LLM响应生成

::

    replaced with revised version Tue, 11 Jun 2024 10:47:02 GMT
    Submission history From: Kai Zhang [view email]
    [v1] Thu, 4 Apr 2024 16:20:34 UTC (10,979 KB)
    [v2] Tue, 11 Jun 2024 10:47:02 UTC (11,369 KB)
    Kai Zhang, Lizhi Qing, Yangyang Kang, Xiaozhong Liu

Large Language Models (LLMs) have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, personalized LLM response generation holds the potential to offer substantial benefits for individuals in critical areas such as medical. Existing research has explored memory-augmented methods to prompt the LLM with pre-stored user-specific knowledge for personalized response generation in terms of new queries. We contend that such paradigm is unable to perceive fine-granularity information. In this study, we propose a novel \textbf{M}emory-\textbf{i}njected approach using parameter-efficient fine-tuning (PEFT) and along with a Bayesian Optimisation searching strategy to achieve \textbf{L}LM \textbf{P}ersonalization(\textbf{MiLP}).

------------

`[2404.15650] Return of EM: Entity-driven Answer Set Expansion for QA Evaluation <https://arxiv.org/abs/2404.15650>`__ EM回归:面向QA评估的实体驱动答案集扩展

::

    replaced with revised version Tue, 11 Jun 2024 09:14:02 GMT
    Submission history From: Dongryeol Lee [view email]
    [v1] Wed, 24 Apr 2024 05:08:55 UTC (1,475 KB)
    [v2] Tue, 11 Jun 2024 09:14:02 UTC (1,902 KB)
    Dongryeol Lee, Minwoo Lee, Kyungmin Min, Joonsuk Park, Kyomin Jung

Recently, directly using large language models (LLMs) has been shown to be the most reliable method to evaluate QA models. However, it suffers from limited interpretability, high cost, and environmental harm. To address these, we propose to use soft EM with entity-driven answer set expansion. Our approach expands the gold answer set to include diverse surface forms, based on the observation that the surface forms often follow particular patterns depending on the entity type. The experimental results show that our method outperforms traditional evaluation methods by a large margin. Moreover, the reliability of our evaluation method is comparable to that of LLM-based ones, while offering the benefits of high interpretability and reduced environmental harm.

------------

`[2405.00706] From Complexity to Clarity: How AI Enhances Perceptions of Scientists and the Public's Understanding of Science <https://arxiv.org/abs/2405.00706>`__ 从复杂到清晰:人工智能如何增强科学家的认知和公众对科学的理解

::

    replaced with revised version Tue, 11 Jun 2024 12:35:51 GMT
    Submission history From: David Markowitz [view email]
    [v1] Tue, 23 Apr 2024 14:43:35 UTC (274 KB)
    [v2] Tue, 11 Jun 2024 12:35:51 UTC (474 KB)
    David M. Markowitz

This paper evaluated the effectiveness of using generative AI to simplify science communication and enhance the public's understanding of science. By comparing lay summaries of journal articles from PNAS, yoked to those generated by AI, this work first assessed linguistic simplicity across such summaries and public perceptions. Study 1a analyzed simplicity features of PNAS abstracts (scientific summaries) and significance statements (lay summaries), observing that lay summaries were indeed linguistically simpler, but effect size differences were small. Study 1b used a large language model, GPT-4, to create significance statements based on paper abstracts and this more than doubled the average effect size without fine-tuning. Study 2 experimentally demonstrated that simply-written GPT summaries facilitated more favorable perceptions of scientists (they were perceived as more credible and trustworthy, but less intelligent) than more complexly-written human PNAS summaries. Crucially, Study 3 experimentally demonstrated that participants comprehended scientific writing better after reading simple GPT summaries compared to complex PNAS summaries. In their own words, participants also summarized scientific papers in a more detailed and concrete manner after reading GPT summaries compared to PNAS summaries of the same article. AI has the potential to engage scientific communities and the public via a simple language heuristic, advocating for its integration into scientific dissemination for a more informed society.

------------

`[2406.02528] Scalable MatMul-free Language Modeling <https://arxiv.org/abs/2406.02528>`__ 可扩展的无matmull语言建模

::

    replaced with revised version Tue, 11 Jun 2024 06:18:28 GMT
    Submission history From: Rui-Jie Zhu [view email]
    [v1] Tue, 4 Jun 2024 17:50:34 UTC (1,050 KB)
    [v2] Mon, 10 Jun 2024 14:55:29 UTC (1,051 KB)
    [v3] Tue, 11 Jun 2024 06:18:28 UTC (1,051 KB)
    Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, Jason K. Eshraghian

Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at this https URL.

------------

`[2406.02888] HYDRA: Model Factorization Framework for Black-Box LLM Personalization <https://arxiv.org/abs/2406.02888>`__ HYDRA:黑盒LLM个性化的模型分解框架

::

    replaced with revised version Tue, 11 Jun 2024 01:51:57 GMT
    Submission history From: Yuchen Zhuang [view email]
    [v1] Wed, 5 Jun 2024 03:08:46 UTC (1,293 KB)
    [v2] Tue, 11 Jun 2024 01:51:57 UTC (1,293 KB)
    Yuchen Zhuang, Haotian Sun, Yue Yu, Rushi Qiang, Qifan Wang, Chao Zhang, Bo Dai

Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences. Despite the remarkable few-shot capabilities exhibited by black-box large language models (LLMs), the inherent opacity of their model parameters presents significant challenges in aligning the generated output with individual expectations. Existing solutions have primarily focused on prompt design to incorporate user-specific profiles and behaviors; however, such approaches often struggle to generalize effectively due to their inability to capture shared knowledge among all users. To address these challenges, we propose HYDRA, a model factorization framework that captures both user-specific behavior patterns from historical data and shared general knowledge among all users to deliver personalized generation. In order to capture user-specific behavior patterns, we first train a reranker to prioritize the most useful information from top-retrieved relevant historical records. By combining the prioritized history with the corresponding query, we train an adapter to align the output with individual user-specific preferences, eliminating the reliance on access to inherent model parameters of black-box LLMs. Both the reranker and the adapter can be decomposed into a base model with multiple user-specific heads, resembling a hydra. The base model maintains shared knowledge across users, while the multiple personal heads capture user-specific preferences. Experimental results demonstrate that HYDRA outperforms existing state-of-the-art prompt-based methods by an average relative improvement of 9.01% across five diverse personalization tasks in the LaMP benchmark. Our implementation is available at this https URL.

------------

`[2406.03202] ChatLang-8: An LLM-Based Synthetic Data Generation Framework for Grammatical Error Correction <https://arxiv.org/abs/2406.03202>`__ ChatLang-8:基于llm的语法纠错合成数据生成框架

::

    replaced with revised version Tue, 11 Jun 2024 07:06:34 GMT
    Submission history From: Jeiyoon Park [view email]
    [v1] Wed, 5 Jun 2024 12:35:00 UTC (382 KB)
    [v2] Tue, 11 Jun 2024 07:06:34 UTC (382 KB)
    Jeiyoon Park, Chanjun Park, Heuiseok Lim

We explore and improve the capabilities of LLMs to generate data for grammatical error correction (GEC). When merely producing parallel sentences, their patterns are too simplistic to be valuable as a corpus. To address this issue, we propose an automated framework that includes a Subject Selector, Grammar Selector, Prompt Manager, and Evaluator. Additionally, we introduce a new dataset for GEC tasks, named ChatLang-8, which encompasses eight types of subject nouns and 23 types of grammar. It consists of 1 million pairs featuring human-like grammatical errors. Our experiments reveal that ChatLang-8 exhibits a more uniform pattern composition compared to existing GEC datasets. Furthermore, we observe improved model performance when using ChatLang-8 instead of existing GEC datasets. The experimental results suggest that our framework and ChatLang-8 are valuable resources for enhancing ChatGPT's data generation capabilities.

------------

`[2406.04289] What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages <https://arxiv.org/abs/2406.04289>`__ 哪些语言易于语言建模?从学习概率正则语言的角度来看

::

    replaced with revised version Mon, 10 Jun 2024 21:53:32 GMT
    Submission history From: Nadav Borenstein [view email]
    [v1] Thu, 6 Jun 2024 17:34:24 UTC (8,395 KB)
    [v2] Fri, 7 Jun 2024 08:30:02 UTC (8,366 KB)
    [v3] Mon, 10 Jun 2024 21:53:32 UTC (8,366 KB)
    Nadav Borenstein, Anej Svete, Robin Chan, Josef Valvoda, Franz Nowak, Isabelle Augenstein, Eleanor Chodroff, Ryan Cotterell

What can large language models learn? By definition, language models (LM) are distributions over strings. Therefore, an intuitive way of addressing the above question is to formalize it as a matter of learnability of classes of distributions over strings. While prior work in this direction focused on assessing the theoretical limits, in contrast, we seek to understand the empirical learnability. Unlike prior empirical work, we evaluate neural LMs on their home turf-learning probabilistic languages-rather than as classifiers of formal languages. In particular, we investigate the learnability of regular LMs (RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs as a function of various complexity parameters of the RLM and the hidden state size of the neural LM. We find that the RLM rank, which corresponds to the size of linear space spanned by the logits of its conditional distributions, and the expected length of sampled strings are strong and significant predictors of learnability for both RNNs and Transformers. Several other predictors also reach significance, but with differing patterns between RNNs and Transformers.

------------

`[2406.05232] Improving Logits-based Detector without Logits from Black-box LLMs <https://arxiv.org/abs/2406.05232>`__ 不使用黑盒llm的Logits改进基于Logits的检测器

::

    replaced with revised version Tue, 11 Jun 2024 16:41:52 GMT
    Submission history From: Shengkun Tang [view email]
    [v1] Fri, 7 Jun 2024 19:38:05 UTC (351 KB)
    [v2] Tue, 11 Jun 2024 16:41:52 UTC (351 KB)
    Cong Zeng, Shengkun Tang, Xianjun Yang, Yuanzhou Chen, Yiyou Sun, zhiqiang xu, Yao Li, Haifeng Chen, Wei Cheng, Dongkuan Xu

The advent of Large Language Models (LLMs) has revolutionized text generation, producing outputs that closely mimic human writing. This blurring of lines between machine- and human-written text presents new challenges in distinguishing one from the other a task further complicated by the frequent updates and closed nature of leading proprietary LLMs. Traditional logits-based detection methods leverage surrogate models for identifying LLM-generated content when the exact logits are unavailable from black-box LLMs. However, these methods grapple with the misalignment between the distributions of the surrogate and the often undisclosed target models, leading to performance degradation, particularly with the introduction of new, closed-source models. Furthermore, while current methodologies are generally effective when the source model is identified, they falter in scenarios where the model version remains unknown, or the test set comprises outputs from various source models. To address these limitations, we present Distribution-Aligned LLMs Detection (DALD), an innovative framework that redefines the state-of-the-art performance in black-box text detection even without logits from source LLMs. DALD is designed to align the surrogate model's distribution with that of unknown target LLMs, ensuring enhanced detection capability and resilience against rapid model iterations with minimal training investment. By leveraging corpus samples from publicly accessible outputs of advanced models such as ChatGPT, GPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize with unknown source model distributions effectively.

------------

`[2406.06326] Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching <https://arxiv.org/abs/2406.06326>`__ 自调整:指导llm通过自我教学有效地获取新知识

::

    replaced with revised version Tue, 11 Jun 2024 15:03:43 GMT
    Submission history From: Xiaoying Zhang [view email]
    [v1] Mon, 10 Jun 2024 14:42:20 UTC (4,820 KB)
    [v2] Tue, 11 Jun 2024 15:03:43 UTC (4,821 KB)
    Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Yipeng Zhang, Haitao Mi, Helen Meng

Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on Llama2 family models reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge.

------------

`[2406.00426] InterpreTabNet: Distilling Predictive Signals from Tabular Data by Salient Feature Interpretation <https://arxiv.org/abs/2406.00426>`__ InterpreTabNet:通过显著特征解释从表格数据中提取预测信号

::

    replaced with revised version Tue, 11 Jun 2024 12:53:03 GMT
    Submission history From: Jacob Si [view email]
    [v1] Sat, 1 Jun 2024 12:48:11 UTC (8,871 KB)
    [v2] Fri, 7 Jun 2024 00:35:25 UTC (8,878 KB)
    [v3] Tue, 11 Jun 2024 12:53:03 UTC (8,878 KB)
    Jacob Si, Wendy Yusi Cheng, Michael Cooper, Rahul G. Krishnan

Tabular data are omnipresent in various sectors of industries. Neural networks for tabular data such as TabNet have been proposed to make predictions while leveraging the attention mechanism for interpretability. However, the inferred attention masks are often dense, making it challenging to come up with rationales about the predictive signal. To remedy this, we propose InterpreTabNet, a variant of the TabNet model that models the attention mechanism as a latent variable sampled from a Gumbel-Softmax distribution. This enables us to regularize the model to learn distinct concepts in the attention masks via a KL Divergence regularizer. It prevents overlapping feature selection by promoting sparsity which maximizes the model's efficacy and improves interpretability to determine the important features when predicting the outcome. To assist in the interpretation of feature interdependencies from our model, we employ a large language model (GPT-4) and use prompt engineering to map from the learned feature mask onto natural language text describing the learned signal. Through comprehensive experiments on real-world datasets, we demonstrate that InterpreTabNet outperforms previous methods for interpreting tabular data while attaining competitive accuracy.

------------

`[2406.05955] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters <https://arxiv.org/abs/2406.05955>`__ Turbo Sparse:以最小激活参数实现LLM SOTA性能

::

    replaced with revised version Tue, 11 Jun 2024 02:15:47 GMT
    Submission history From: Yixin Song [view email]
    [v1] Mon, 10 Jun 2024 01:21:59 UTC (814 KB)
    [v2] Tue, 11 Jun 2024 02:15:47 UTC (814 KB)
    Yixin Song, Haotong Xie, Zhengyan Zhang, Bo Wen, Li Ma, Zeyu Mi, and Haibo Chen

Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance. However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity. Simply replacing these functions with ReLU fails to achieve sufficient sparsity. Moreover, inadequate training data can further increase the risk of performance degradation. To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification. Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to further boost efficiency. By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance. Evaluation results demonstrate that this sparsity achieves a 2-5x decoding speedup. Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second. Our models are available at \url{this https URL}

------------

`[2311.17647] Text as Images: Can Multimodal Large Language Models Follow Printed Instructions in Pixels? <https://arxiv.org/abs/2311.17647>`__ 文本即图像:多模态大型语言模型能否遵循像素级打印指令?

::

    replaced with revised version Mon, 10 Jun 2024 23:39:24 GMT
    Submission history From: Yujie Lu [view email]
    [v1] Wed, 29 Nov 2023 14:08:53 UTC (9,400 KB)
    [v2] Mon, 10 Jun 2024 23:39:24 UTC (3,380 KB)
    Xiujun Li, Yujie Lu, Zhe Gan, Jianfeng Gao, William Yang Wang, Yejin Choi

Recent multimodal large language models (MLLMs) have shown promising instruction following capabilities on vision-language tasks. In this work, we introduce VISUAL MODALITY INSTRUCTION (VIM), and investigate how well multimodal models can understand textual instructions provided in pixels, despite not being explicitly trained on such data during pretraining or fine-tuning. We adapt VIM to eight benchmarks, including OKVQA, MM-Vet, MathVista, MMMU, and probe diverse MLLMs in both the text-modality instruction (TEM) setting and VIM setting. Notably, we observe a significant performance disparity between the original TEM and VIM settings for open-source MLLMs, indicating that open-source MLLMs face greater challenges when text instruction is presented solely in image form. To address this issue, we train v-MLLM, a generalizable model that is capable to conduct robust instruction following in both text-modality and visual-modality instructions.

------------

`[2312.00886] Nash Learning from Human Feedback <https://arxiv.org/abs/2312.00886>`__ 纳什从人类反馈中学习

::

    replaced with revised version Tue, 11 Jun 2024 16:25:52 GMT
    Submission history From: Michal Valko [view email]
    [v1] Fri, 1 Dec 2023 19:26:23 UTC (264 KB)
    [v2] Tue, 5 Dec 2023 11:05:06 UTC (264 KB)
    [v3] Wed, 6 Dec 2023 14:07:10 UTC (264 KB)
    [v4] Tue, 11 Jun 2024 16:25:52 UTC (822 KB)
    R\'emi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, Bilal Piot

Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Typically, RLHF involves the initial step of learning a reward model from human feedback, often expressed as preferences between pairs of text generations produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by optimizing it to maximize the reward model through a reinforcement learning algorithm. However, an inherent limitation of current reward models is their inability to fully represent the richness of human preferences and their dependency on the sampling distribution.
In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a preference model, which is conditioned on two inputs given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF).
In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. To demonstrate the effectiveness of our approach, we present experimental results involving the fine-tuning of a LLM for a text summarization task. We believe NLHF offers a compelling avenue for preference learning and policy optimization with the potential of advancing the field of aligning LLMs with human preferences.

------------

`[2402.13521] Test-Driven Development for Code Generation <https://arxiv.org/abs/2402.13521>`__ 用于代码生成的测试驱动开发

::

    replaced with revised version Tue, 11 Jun 2024 15:53:35 GMT
    Submission history From: Noble Saji Mathews [view email]
    [v1] Wed, 21 Feb 2024 04:10:12 UTC (9,149 KB)
    [v2] Tue, 11 Jun 2024 15:53:35 UTC (3,981 KB)
    Noble Saji Mathews and Meiyappan Nagappan

Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.

------------

`[2402.19366] Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency <https://arxiv.org/abs/2402.19366>`__ 

::

    replaced with revised version Tue, 11 Jun 2024 10:01:05 GMT
    Submission history From: Mark Scanlon [view email]
    [v1] Thu, 29 Feb 2024 17:13:44 UTC (547 KB)
    [v2] Tue, 11 Jun 2024 10:01:05 UTC (1,482 KB)
    Akila Wickramasekara and Frank Breitinger and Mark Scanlon

The growing number of cases that require digital forensic analysis raises concerns about the ability of law enforcement to conduct investigations promptly. Consequently, this paper delves into the potential and effectiveness of integrating Large Language Models (LLMs) into digital forensic investigation to address these challenges. A comprehensive literature review is carried out, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the use of LLMs in investigations. The review identifies current challenges within existing digital forensic processes and explores both the obstacles and possibilities of incorporating LLMs. In conclusion, the study asserts that the adoption of LLMs in digital forensics, with appropriate constraints, has the potential to improve investigation efficiency, improve traceability, and alleviate technical and judicial barriers faced by law enforcement entities.

------------

`[2403.16687] Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography <https://arxiv.org/abs/2403.16687>`__ 基于脑电的ChatGPT在对话教学中的应用效果调查

::

    replaced with revised version Tue, 11 Jun 2024 03:35:03 GMT
    Submission history From: Jiayue Zhang [view email]
    [v1] Mon, 25 Mar 2024 12:23:12 UTC (454 KB)
    [v2] Mon, 8 Apr 2024 09:23:43 UTC (455 KB)
    [v3] Mon, 22 Apr 2024 14:48:00 UTC (495 KB)
    [v4] Mon, 3 Jun 2024 08:37:42 UTC (495 KB)
    [v5] Tue, 11 Jun 2024 03:35:03 UTC (494 KB)
    Jiayue Zhang, Yiheng Liu, Wenqi Cai, Lanlan Wu, Yali Peng, Jingjing Yu, Senqing Qi, Taotao Long, Bao Ge

In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course "Digital Image Processing". The research findings show comparable scores between the two groups on the retention test. However, students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test. Electroencephalography data revealed that students who interacted with ChatGPT exhibited higher levels of cognitive activity, suggesting that ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students. knowledge application and creativity were insignificant. Based upon the research findings, it is evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the dialogue teaching in information related courses. Combining ChatGPT with traditional human teachers might be a more ideal approach. The synergistic use of both can provide students with more comprehensive learning support, thus contributing to enhancing the quality of teaching.

------------

`[2402.10892] Proving membership in LLM pretraining data via data watermarks <https://arxiv.org/abs/2402.10892>`__ 通过数据水印证明LLM预训练数据中的成员关系

::

    replaced with revised version Mon, 10 Jun 2024 19:39:34 GMT
    Submission history From: Ryan Yixiang Wang [view email]
    [v1] Fri, 16 Feb 2024 18:49:27 UTC (7,088 KB)
    [v2] Mon, 10 Jun 2024 19:39:34 UTC (7,089 KB)
    Johnny Tian-Zheng Wei, Ryan Yixiang Wang, Robin Jia

Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks remain strong if the model size also increases. Finally, we view SHA hashes as natural watermarks and show that we can robustly detect hashes from BLOOM-176B's training data, as long as they occurred at least 90 times. Together, our results point towards a promising future for data watermarks in real world use.

------------

-----------
Index (122)
-----------

`[2406.06865] Eyeballing Combinatorial Problems: A Case Study of Using Multimodal Large Language Models to Solve Traveling Salesman Problems <https://arxiv.org/abs/2406.06865>`__ 目测组合问题:使用多模态大型语言模型解决旅行商问题的案例研究

`[2406.06870] What's in an embedding? Would a rose by any embedding smell as sweet? <https://arxiv.org/abs/2406.06870>`__ 嵌入中有什么?任何一种嵌入的玫瑰闻起来都那么香吗?

`[2406.06874] Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback <https://arxiv.org/abs/2406.06874>`__ 联合演示和偏好学习改进了与人工反馈的政策一致性

`[2406.06947] CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front-End UI Only <https://arxiv.org/abs/2406.06947>`__ CAAP:面向前端用户界面的情景感知动作规划方法

`[2406.07327] 3D-Properties: Identifying Challenges in DPO and Charting a Path Forward <https://arxiv.org/abs/2406.07327>`__ 3d属性:识别DPO中的挑战并绘制前进道路

`[2406.07378] Large Language Models for Constrained-Based Causal Discovery <https://arxiv.org/abs/2406.07378>`__

`[2406.07381] World Models with Hints of Large Language Models for Goal Achieving <https://arxiv.org/abs/2406.07381>`__ 面向目标实现的大型语言模型提示的世界模型

`[2406.07394] Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B <https://arxiv.org/abs/2406.07394>`__ 利用LLaMa-3 8B通过蒙特卡罗树自求精访问GPT-4级数学奥林匹克解

`[2406.06556] Enhancing Presentation Slide Generation by LLMs with a Multi-Staged End-to-End Approach <https://arxiv.org/abs/2406.06556>`__ 用多阶段端到端方法增强llm的演示文稿幻灯片生成

`[2406.06558] Enhancing Text Authenticity: A Novel Hybrid Approach for AI-Generated Text Detection <https://arxiv.org/abs/2406.06558>`__ 增强文本真实性:一种新的人工智能生成文本检测混合方法

`[2406.06559] Harnessing Business and Media Insights with Large Language Models <https://arxiv.org/abs/2406.06559>`__ 用大型语言模型利用商业和媒体见解

`[2406.06560] Inverse Constitutional AI: Compressing Preferences into Principles <https://arxiv.org/abs/2406.06560>`__ 反宪法AI:将偏好压缩为原则

`[2406.06562] Achieving Sparse Activation in Small Language Models <https://arxiv.org/abs/2406.06562>`__ 实现小型语言模型的稀疏激活

`[2406.06563] Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models <https://arxiv.org/abs/2406.06563>`__

`[2406.06573] MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering <https://arxiv.org/abs/2406.06573>`__ MedFuzz:探索大型语言模型在医疗问答中的鲁棒性

`[2406.06574] Towards Transparency: Exploring LLM Trainings Datasets through Visual Topic Modeling and Semantic Frame <https://arxiv.org/abs/2406.06574>`__ 迈向透明度:通过视觉主题建模和语义框架探索LLM训练数据集

`[2406.06576] OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step <https://arxiv.org/abs/2406.06576>`__ OccamLLM:一步快速精确语言模型算法

`[2406.06579] From Redundancy to Relevance: Enhancing Explainability in Multimodal Large Language Models <https://arxiv.org/abs/2406.06579>`__ 从冗余到相关:增强多模态大型语言模型的可解释性

`[2406.06580] Break the Chain: Large Language Models Can be Shortcut Reasoners <https://arxiv.org/abs/2406.06580>`__

`[2406.06581] Set-Based Prompting: Provably Solving the Language Model Order Dependency Problem <https://arxiv.org/abs/2406.06581>`__ 集合提示:可证解决语言模型顺序依赖问题

`[2406.06582] Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing <https://arxiv.org/abs/2406.06582>`__ 基于预训练大型语言模型的离散多模态transformer混合监督语音处理

`[2406.06584] Evaluating the Efficacy of Large Language Models in Detecting Fake News: A Comparative Analysis <https://arxiv.org/abs/2406.06584>`__ 大型语言模型检测假新闻的有效性评估:对比分析

`[2406.06587] Exploring Human-AI Perception Alignment in Sensory Experiences: Do LLMs Understand Textile Hand? <https://arxiv.org/abs/2406.06587>`__ 探索感官体验中的人类- ai感知对齐:llm理解纺织手吗?

`[2406.06589] PatentEval: Understanding Errors in Patent Generation <https://arxiv.org/abs/2406.06589>`__ PatentEval:理解专利生成中的错误

`[2406.06590] Are LLMs classical or nonmonotonic reasoners? Lessons from generics <https://arxiv.org/abs/2406.06590>`__ llm是经典推理机还是非单调推理机?来自泛型的教训

`[2406.06591] Exploring Multilingual Large Language Models for Enhanced TNM classification of Radiology Report in lung cancer staging <https://arxiv.org/abs/2406.06591>`__

`[2406.06596] Are Large Language Models the New Interface for Data Pipelines? <https://arxiv.org/abs/2406.06596>`__ 大型语言模型是数据管道的新接口吗?

`[2406.06599] Anna Karenina Strikes Again: Pre-Trained LLM Embeddings May Favor High-Performing Learners <https://arxiv.org/abs/2406.06599>`__ Anna Karenina再次强调:预训练的LLM嵌入可能更适合高性能的学习者

`[2406.06610] Reinterpreting 'the Company a Word Keeps': Towards Explainable and Ontologically Grounded Language Models <https://arxiv.org/abs/2406.06610>`__ 重新解读“一个词保留的公司”:走向可解释和本体论基础的语言模型

`[2406.06615] Language Guided Skill Discovery <https://arxiv.org/abs/2406.06615>`__ 语言引导的技能发现

`[2406.06616] Transforming Dental Diagnostics with Artificial Intelligence: Advanced Integration of ChatGPT and Large Language Models for Patient Care <https://arxiv.org/abs/2406.06616>`__ 用人工智能改造牙科诊断:ChatGPT和大型语言模型的高级集成用于患者护理

`[2406.06621] LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering <https://arxiv.org/abs/2406.06621>`__

`[2406.06622] Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs <https://arxiv.org/abs/2406.06622>`__ 对抗性调优:针对llm的越狱攻击防御

`[2406.06636] LLM Questionnaire Completion for Automatic Psychiatric Assessment <https://arxiv.org/abs/2406.06636>`__ 精神病学自动评估的LLM问卷完成

`[2406.06773] Evaluating Zero-Shot Long-Context LLM Compression <https://arxiv.org/abs/2406.06773>`__ 零样本长上下文LLM压缩的评估

`[2406.06840] Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles <https://arxiv.org/abs/2406.06840>`__ 无声信号，巨大冲击:用于编码狗哨词义消歧的llm

`[2406.06950] A Probabilistic Framework for LLM Hallucination Detection via Belief Tree Propagation <https://arxiv.org/abs/2406.06950>`__

`[2406.06962] Evolving Subnetwork Training for Large Language Models <https://arxiv.org/abs/2406.06962>`__ 面向大型语言模型的演化子网络训练

`[2406.07001] Mitigating Boundary Ambiguity and Inherent Bias for Text Classification in the Era of Large Language Models <https://arxiv.org/abs/2406.07001>`__ 在大型语言模型时代缓解文本分类的边界模糊和固有偏见

`[2406.07007] Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference <https://arxiv.org/abs/2406.07007>`__ 蜡笔:通过即时适配器混合和边缘服务器混合推理定制的设备上LLM

`[2406.07016] Delving into ChatGPT usage in academic writing through excess vocabulary <https://arxiv.org/abs/2406.07016>`__ 通过词汇量挖掘学术写作中的ChatGPT用法

`[2406.07036] Paying More Attention to Source Context: Mitigating Unfaithful Translations from Large Language Model <https://arxiv.org/abs/2406.07036>`__ 更加关注源上下文:减少大型语言模型的不忠实翻译

`[2406.07056] Effectively Compress KV Heads for LLM <https://arxiv.org/abs/2406.07056>`__ 有效压缩LLM KV头

`[2406.07168] Teaching Language Models to Self-Improve by Learning from Language Feedback <https://arxiv.org/abs/2406.07168>`__ 从语言反馈中学习自我提高的语言教学模式

`[2406.07188] Merging Improves Self-Critique Against Jailbreak Attacks <https://arxiv.org/abs/2406.07188>`__ 合并改进了对越狱攻击的自我批评

`[2406.07212] Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems with Large Language Models <https://arxiv.org/abs/2406.07212>`__ 面向医疗保健中的人- ai协作:基于大型语言模型的指导延迟系统

`[2406.07222] Improving Autoformalization using Type Checking <https://arxiv.org/abs/2406.07222>`__ 使用类型检查改进自动形式化

`[2406.07232] DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms <https://arxiv.org/abs/2406.07232>`__ Dual - reflect:利用双重学习反馈机制增强大型语言模型的反思性翻译

`[2406.07243] MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs <https://arxiv.org/abs/2406.07243>`__

`[2406.07257] Scholarly Question Answering using Large Language Models in the NFDI4DataScience Gateway <https://arxiv.org/abs/2406.07257>`__ 在NFDI4DataScience网关中使用大型语言模型的学术问答

`[2406.07259] Scientific Computing with Large Language Models <https://arxiv.org/abs/2406.07259>`__ 基于大型语言模型的科学计算

`[2406.07302] BertaQA: How Much Do Language Models Know About Local Culture? <https://arxiv.org/abs/2406.07302>`__ BertaQA:语言模型对当地文化了解多少?

`[2406.07444] On the Robustness of Document-Level Relation Extraction Models to Entity Name Variations <https://arxiv.org/abs/2406.07444>`__ 文档级关系抽取模型对实体名称变化的鲁棒性研究

`[2406.07483] Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing <https://arxiv.org/abs/2406.07483>`__ 社交媒体帖子中的立场标注:大型语言模型与众包的比较分析

`[2406.07494] CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization <https://arxiv.org/abs/2406.07494>`__

`[2406.07496] TextGrad: Automatic "Differentiation" via Text <https://arxiv.org/abs/2406.07496>`__ TextGrad:通过文本自动“区分”

`[2406.07505] THaLLE: Text Hyperlocally Augmented Large Language Extension -- Technical Report <https://arxiv.org/abs/2406.07505>`__ THaLLE:文本超局部增强大型语言扩展-技术报告

`[2406.06564] Revolutionizing Large Language Model Training through Dynamic Parameter Adjustment <https://arxiv.org/abs/2406.06564>`__ 通过动态参数调整彻底改变大型语言模型训练

`[2406.06567] DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion <https://arxiv.org/abs/2406.06567>`__ DHA:基于自适应头融合的Transformer检查点解耦头注意力学习

`[2406.06600] HORAE: A Domain-Agnostic Modeling Language for Automating Multimodal Service Regulation <https://arxiv.org/abs/2406.06600>`__ HORAE:一种领域无关的多模态服务自动化建模语言

`[2406.06623] Spectrum: Targeted Training on Signal to Noise Ratio <https://arxiv.org/abs/2406.06623>`__ 频谱:有针对性的信噪比训练

`[2406.06858] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion <https://arxiv.org/abs/2406.06858>`__ FLUX:通过内核融合实现gpu上基于软件的快速通信重叠

`[2406.07017] MoreauPruner: Robust Pruning of Large Language Models against Weight Perturbations <https://arxiv.org/abs/2406.07017>`__

`[2406.07025] Entropy-Reinforced Planning with Large Language Models for Drug Discovery <https://arxiv.org/abs/2406.07025>`__ 基于大规模语言模型的熵增强药物发现规划

`[2406.07177] TernaryLLM: Ternarized Large Language Model <https://arxiv.org/abs/2406.07177>`__ TernaryLLM:三元化大型语言模型

`[2406.07217] A Synthetic Dataset for Personal Attribute Inference <https://arxiv.org/abs/2406.07217>`__ 用于个人属性推断的合成数据集

`[2406.07400] Guiding LLM Temporal Logic Generation with Explicit Separation of Data and Control <https://arxiv.org/abs/2406.07400>`__ 以显式的数据和控制分离指导LLM时序逻辑生成

`[2406.07455] Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis <https://arxiv.org/abs/2406.07455>`__ 无奖励推理的人工反馈强化学习:无模型算法和依赖实例分析

`[2406.07457] Estimating the Hallucination Rate of Generative AI <https://arxiv.org/abs/2406.07457>`__ 估计生成式AI的幻觉率

`[2406.07515] Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement <https://arxiv.org/abs/2406.07515>`__

`[2406.07528] QuickLLaMA: Query-aware Inference Acceleration for Large Language Models <https://arxiv.org/abs/2406.07528>`__ quicklama:面向大型语言模型的查询感知推理加速

`[2406.06549] Large Language Model (LLM) for Standard Cell Layout Design Optimization <https://arxiv.org/abs/2406.06549>`__ 面向标准单元布局优化的大型语言模型

`[2406.06637] Exploring the Efficacy of Large Language Models (GPT-4) in Binary Reverse Engineering <https://arxiv.org/abs/2406.06637>`__ 探索大型语言模型(GPT-4)在二进制逆向工程中的功效

`[2406.06729] Synthetic Query Generation using Large Language Models for Virtual Assistants <https://arxiv.org/abs/2406.06729>`__ 基于大型语言模型的虚拟助手合成查询生成

`[2406.06730] TRINS: Towards Multimodal Language Models that Can Read <https://arxiv.org/abs/2406.06730>`__ TRINS:面向可阅读的多模态语言模型

`[2406.06777] MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension <https://arxiv.org/abs/2406.06777>`__ MolX:基于多模态扩展的分子学习大型语言模型增强

`[2406.06822] An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection <https://arxiv.org/abs/2406.06822>`__ 一种llm辅助的易于触发的代码补全模型后门攻击:针对强检测注入伪装漏洞

`[2406.06864] Validating LLM-Generated Programs with Metamorphic Prompt Testing <https://arxiv.org/abs/2406.06864>`__ 用蜕变提示测试验证llm生成的程序

`[2406.07230] Needle In A Multimodal Haystack <https://arxiv.org/abs/2406.07230>`__ 多式联运大海捞针

`[2406.06663] SecureNet: A Comparative Study of DeBERTa and Large Language Models for Phishing Detection <https://arxiv.org/abs/2406.06663>`__ SecureNet: DeBERTa与大型语言模型在网络钓鱼检测中的比较研究

`[2406.07268] Advancing Grounded Multimodal Named Entity Recognition via LLM-Based Reformulation and Box-Based Segmentation <https://arxiv.org/abs/2406.07268>`__

`[2406.07296] Instruct Large Language Models to Drive like Humans <https://arxiv.org/abs/2406.07296>`__ 指导大型语言模型像人一样驾驶

`[2406.07310] MM-KWS: Multi-modal Prompts for Multilingual User-defined Keyword Spotting <https://arxiv.org/abs/2406.07310>`__ MM-KWS:多语言自定义关键词识别的多模态提示

`[2406.07411] VersiCode: Towards Version-controllable Code Generation <https://arxiv.org/abs/2406.07411>`__ VersiCode:面向版本可控的代码生成

`[2406.07476] VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs <https://arxiv.org/abs/2406.07476>`__ VideoLLaMA 2:推进视频llm中的时空建模和音频理解

`[2406.07502] Image Textualization: An Automatic Framework for Creating Accurate and Detailed Image Descriptions <https://arxiv.org/abs/2406.07502>`__ 图像纹理化:一种自动创建精确和详细的图像描述的框架

`[2406.06531] Quantum Reinforcement Learning in Non-Abelian Environments: Unveiling Novel Formulations and Quantum Advantage Exploration <https://arxiv.org/abs/2406.06531>`__ 非阿贝尔环境中的量子强化学习:新配方的揭开和量子优势探索

`[2406.06855] Design and Scheduling of an AI-based Queueing System <https://arxiv.org/abs/2406.06855>`__ 基于人工智能的排队系统的设计与调度

`[2402.05894] Large Language Model Meets Graph Neural Network in Knowledge Distillation <https://arxiv.org/abs/2402.05894>`__ 知识蒸馏中大型语言模型与图神经网络的相遇

`[2403.14077] Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics <https://arxiv.org/abs/2403.14077>`__ ChatGPT能检测DeepFakes吗?使用多模态大型语言模型进行媒体取证的研究

`[2405.12147] Eliciting Problem Specifications via Large Language Models <https://arxiv.org/abs/2405.12147>`__

`[2305.12307] OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing <https://arxiv.org/abs/2305.12307>`__ OntoType:本体引导的预训练语言模型辅助的细粒度实体分类

`[2308.04913] LLaMA-E: Empowering E-commerce Authoring with Object-Interleaved Instruction Following <https://arxiv.org/abs/2308.04913>`__ LLaMA-E:基于对象交错指令遵循的电子商务创作

`[2309.04550] Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges <https://arxiv.org/abs/2309.04550>`__ 用llm从电子病历中检索证据:可能性和挑战

`[2310.02174] Ask Again, Then Fail: Large Language Models' Vacillations in Judgment <https://arxiv.org/abs/2310.02174>`__ 再问，再失败:大型语言模型的判断摇摆不定

`[2311.07470] Finding and Editing Multi-Modal Neurons in Pre-Trained Transformers <https://arxiv.org/abs/2311.07470>`__ 在预训练transformer中发现和编辑多模态神经元

`[2311.08718] Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling <https://arxiv.org/abs/2311.08718>`__ 基于输入澄清集成的大型语言模型不确定性分解

`[2312.02073] A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia <https://arxiv.org/abs/2312.02073>`__ 矩阵出了故障?定位和检测基于假百科的语言模型

`[2402.02872] How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning <https://arxiv.org/abs/2402.02872>`__ 大型语言模型如何在上下文中学习?上下文中头的查询矩阵和关键矩阵是度量学习的双塔

`[2402.09267] Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation <https://arxiv.org/abs/2402.09267>`__ 面向事实性的自我对齐:通过自我评估减轻llm中的幻觉

`[2402.17882] BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra <https://arxiv.org/abs/2402.17882>`__ BlendSQL:一种可扩展的关系代数混合问答统一方言

`[2403.03329] Guardrail Baselines for Unlearning in LLMs <https://arxiv.org/abs/2403.03329>`__ llm中忘记学习的护栏基线

`[2403.07556] Truth-Aware Context Selection: Mitigating Hallucinations of Large Language Models Being Misled by Untruthful Contexts <https://arxiv.org/abs/2403.07556>`__ 真值感知语境选择:缓解大型语言模型被不真实语境误导的幻觉

`[2403.16792] Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback <https://arxiv.org/abs/2403.16792>`__ 基于编译器反馈的精确代码生成的项目级代码上下文迭代精化

`[2403.16854] An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing <https://arxiv.org/abs/2403.16854>`__ 一个专家值一个令牌:通过专家令牌路由将多个专家llm协同为通才

`[2404.03565] Personalized LLM Response Generation with Parameterized Memory Injection <https://arxiv.org/abs/2404.03565>`__ 基于参数化内存注入的个性化LLM响应生成

`[2404.15650] Return of EM: Entity-driven Answer Set Expansion for QA Evaluation <https://arxiv.org/abs/2404.15650>`__ EM回归:面向QA评估的实体驱动答案集扩展

`[2405.00706] From Complexity to Clarity: How AI Enhances Perceptions of Scientists and the Public's Understanding of Science <https://arxiv.org/abs/2405.00706>`__ 从复杂到清晰:人工智能如何增强科学家的认知和公众对科学的理解

`[2406.02528] Scalable MatMul-free Language Modeling <https://arxiv.org/abs/2406.02528>`__ 可扩展的无matmull语言建模

`[2406.02888] HYDRA: Model Factorization Framework for Black-Box LLM Personalization <https://arxiv.org/abs/2406.02888>`__ HYDRA:黑盒LLM个性化的模型分解框架

`[2406.03202] ChatLang-8: An LLM-Based Synthetic Data Generation Framework for Grammatical Error Correction <https://arxiv.org/abs/2406.03202>`__ ChatLang-8:基于llm的语法纠错合成数据生成框架

`[2406.04289] What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages <https://arxiv.org/abs/2406.04289>`__ 哪些语言易于语言建模?从学习概率正则语言的角度来看

`[2406.05232] Improving Logits-based Detector without Logits from Black-box LLMs <https://arxiv.org/abs/2406.05232>`__ 不使用黑盒llm的Logits改进基于Logits的检测器

`[2406.06326] Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching <https://arxiv.org/abs/2406.06326>`__ 自调整:指导llm通过自我教学有效地获取新知识

`[2406.00426] InterpreTabNet: Distilling Predictive Signals from Tabular Data by Salient Feature Interpretation <https://arxiv.org/abs/2406.00426>`__ InterpreTabNet:通过显著特征解释从表格数据中提取预测信号

`[2406.05955] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters <https://arxiv.org/abs/2406.05955>`__ Turbo Sparse:以最小激活参数实现LLM SOTA性能

`[2311.17647] Text as Images: Can Multimodal Large Language Models Follow Printed Instructions in Pixels? <https://arxiv.org/abs/2311.17647>`__ 文本即图像:多模态大型语言模型能否遵循像素级打印指令?

`[2312.00886] Nash Learning from Human Feedback <https://arxiv.org/abs/2312.00886>`__ 纳什从人类反馈中学习

`[2402.13521] Test-Driven Development for Code Generation <https://arxiv.org/abs/2402.13521>`__ 用于代码生成的测试驱动开发

`[2402.19366] Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency <https://arxiv.org/abs/2402.19366>`__

`[2403.16687] Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography <https://arxiv.org/abs/2403.16687>`__ 基于脑电的ChatGPT在对话教学中的应用效果调查

`[2402.10892] Proving membership in LLM pretraining data via data watermarks <https://arxiv.org/abs/2402.10892>`__ 通过数据水印证明LLM预训练数据中的成员关系

