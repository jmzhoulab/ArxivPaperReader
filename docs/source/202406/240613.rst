240613
========

----------
Survey (4)
----------

`[2406.08068] Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey <https://arxiv.org/abs/2406.08068>`__ 大型语言模型满足以文本为中心的多模态情感分析综述

::

    Wed, 12 Jun 2024 10:36:27 GMT
    Hao Yang, Yanyan Zhao, Yang Wu, Shilong Wang, Tian Zheng, Hongbo Zhang, Wanxiang Che, Bing Qin

Compared to traditional sentiment analysis, which only considers text, multimodal sentiment analysis needs to consider emotional signals from multimodal sources simultaneously and is therefore more consistent with the way how humans process sentiment in real-world scenarios. It involves processing emotional information from various sources such as natural language, images, videos, audio, physiological signals, etc. However, although other modalities also contain diverse emotional cues, natural language usually contains richer contextual information and therefore always occupies a crucial position in multimodal sentiment analysis. The emergence of ChatGPT has opened up immense potential for applying large language models (LLMs) to text-centric multimodal tasks. However, it is still unclear how existing LLMs can adapt better to text-centric multimodal sentiment analysis tasks. This survey aims to (1) present a comprehensive review of recent research in text-centric multimodal sentiment analysis tasks, (2) examine the potential of LLMs for text-centric multimodal sentiment analysis, outlining their approaches, advantages, and limitations, (3) summarize the application scenarios of LLM-based multimodal sentiment analysis technology, and (4) explore the challenges and potential research directions for multimodal sentiment analysis in the future.

------------

`[2406.08426] Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL <https://arxiv.org/abs/2406.08426>`__ 

::

    Wed, 12 Jun 2024 17:13:17 GMT
    Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, Xiao Huang

Generating accurate SQL according to natural language questions (text-to-SQL) is a long-standing problem since it is challenging in user question understanding, database schema comprehension, and SQL generation. Conventional text-to-SQL systems include human engineering and deep neural networks.
Subsequently, pre-trained language models (PLMs) have been developed and utilized for text-to-SQL tasks, achieving promising performance. As modern databases become more complex and corresponding user questions more challenging, PLMs with limited comprehension capabilities can lead to incorrect SQL generation. This necessitates more sophisticated and tailored optimization methods, which, in turn, restricts the applications of PLM-based systems. Most recently, large language models (LLMs) have demonstrated significant abilities in natural language understanding as the model scale remains increasing.
Therefore, integrating the LLM-based implementation can bring unique opportunities, challenges, and solutions to text-to-SQL research. In this survey, we present a comprehensive review of LLM-based text-to-SQL.
Specifically, we propose a brief overview of the current challenges and the evolutionary process of text-to-SQL. Then, we provide a detailed introduction to the datasets and metrics designed to evaluate text-to-SQL systems. After that, we present a systematic analysis of recent advances in LLM-based text-to-SQL. Finally, we discuss the remaining challenges in this field and propose expectations for future directions.

------------

`[2406.08115] Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey <https://arxiv.org/abs/2406.08115>`__ 大规模分布式深度学习资源分配与负载调度研究综述

::

    Wed, 12 Jun 2024 11:51:44 GMT
    Feng Liang, Zhen Zhang, Haifeng Lu, Chengming Li, Victor C. M. Leung, Yanyi Guo, Xiping Hu

With rapidly increasing distributed deep learning workloads in large-scale data centers, efficient distributed deep learning framework strategies for resource allocation and workload scheduling have become the key to high-performance deep learning. The large-scale environment with large volumes of datasets, models, and computational and communication resources raises various unique challenges for resource allocation and workload scheduling in distributed deep learning, such as scheduling complexity, resource and workload heterogeneity, and fault tolerance. To uncover these challenges and corresponding solutions, this survey reviews the literature, mainly from 2019 to 2024, on efficient resource allocation and workload scheduling strategies for large-scale distributed DL. We explore these strategies by focusing on various resource types, scheduling granularity levels, and performance goals during distributed training and inference processes. We highlight critical challenges for each topic and discuss key insights of existing technologies. To illustrate practical large-scale resource allocation and workload scheduling in real distributed deep learning scenarios, we use a case study of training large language models. This survey aims to encourage computer science, artificial intelligence, and communications researchers to understand recent advances and explore future research directions for efficient framework strategies for large-scale distributed deep learning.

------------

`[2402.06853] History, Development, and Principles of Large Language Models-An Introductory Survey <https://arxiv.org/abs/2402.06853>`__ 大型语言模型的历史、发展和原理——导论综述

::

    replaced with revised version Wed, 12 Jun 2024 02:28:12 GMT
    Submission history From: Zhibo Chu [view email]
    [v1] Sat, 10 Feb 2024 01:18:15 UTC (1,386 KB)
    [v2] Wed, 12 Jun 2024 02:28:12 UTC (1,417 KB)
    Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Min Yang, Wenbin Zhang

Language models serve as a cornerstone in natural language processing (NLP), utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models (SLMs) to the contemporary landscape of large language models (LLMs). Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most LLMs reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience. It strives to facilitate a comprehensive understanding by exploring the historical background of language models and tracing their evolution over time. The survey further investigates the factors influencing the development of LLMs, emphasizing key contributions. Additionally, it concentrates on elucidating the underlying principles of LLMs, equipping audiences with essential theoretical knowledge. The survey also highlights the limitations of existing work and points out promising future directions.

------------

-------------
Benchmark (7)
-------------

`[2406.08184] MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents <https://arxiv.org/abs/2406.08184>`__ MobileAgentBench:一个高效且用户友好的移动LLM代理基准

::

    Wed, 12 Jun 2024 13:14:50 GMT
    Luyuan Wang, Yongyu Deng, Yiwei Zha, Guodong Mao, Qinmin Wang, Tianchen Min, Wei Chen and Shoufa Chen

Large language model (LLM)-based mobile agents are increasingly popular due to their capability to interact directly with mobile phone Graphic User Interfaces (GUIs) and their potential to autonomously manage daily tasks.
Despite their promising prospects in both academic and industrial sectors, little research has focused on benchmarking the performance of existing mobile agents, due to the inexhaustible states of apps and the vague definition of feasible action sequences. To address this challenge, we propose an efficient and user-friendly benchmark, MobileAgentBench, designed to alleviate the burden of extensive manual testing. We initially define 100 tasks across 10 open-source apps, categorized by multiple levels of difficulty. Subsequently, we evaluate several existing mobile agents, including AppAgent and MobileAgent, to thoroughly and systematically compare their performance. All materials are accessible on our project webpage: https://MobileAgentBench.github.io, contributing to the advancement of both academic and industrial fields.

------------

`[2406.08155] Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark <https://arxiv.org/abs/2406.08155>`__ 检查训练后专家混合量化:基准

::

    Wed, 12 Jun 2024 12:44:48 GMT
    Pingzhi Li, Xiaolong Jin, Yu Cheng, Tianlong Chen

Large Language Models~(LLMs) have become foundational in the realm of natural language processing, demonstrating performance improvements as model sizes increase. The Mixture-of-Experts~(MoE) approach offers a promising way to scale LLMs more efficiently by using fewer computational FLOPs through sparse activation. However, it suffers from significant memory overheads, necessitating model compression techniques. Post-training quantization, a popular method for model compression, proves less effective when directly applied to MoE models due to MoE's overlooked inherent sparsity. This paper explores several MoE structure-aware quantization heuristics, ranging from coarse to fine granularity, from MoE block to individual linear weight. Our investigations reveal critical principles: different MoE structures (i.e., blocks, experts, linear layers) require varying numbers of weight bits for effective and efficient quantization. Conclusions are supported by extensive benchmarking across two representative MoE models and six tasks. We further introduce novel enhancements to more accurately identify the most critical weights in MoE quantization that necessitate higher bit allocations, including the linear weight outlier scorer and MoE block scorer. Additionally, subsequent experiments validate our findings in the context of both weight and activation quantization.

------------

`[2406.07599] CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence <https://arxiv.org/abs/2406.07599>`__ CTIBench:网络威胁情报llm评估基准

::

    Tue, 11 Jun 2024 16:42:02 GMT
    Md Tanvirul Alam, Dipkamal Bhushl, Le Nguyen, Nidhi Rastogi

Cyber threat intelligence (CTI) is crucial in today's cybersecurity landscape, providing essential insights to understand and mitigate the ever-evolving cyber threats. The recent rise of Large Language Models (LLMs) have shown potential in this domain, but concerns about their reliability, accuracy, and hallucinations persist. While existing benchmarks provide general evaluations of LLMs, there are no benchmarks that address the practical and applied aspects of CTI-specific tasks. To bridge this gap, we introduce CTIBench, a benchmark designed to assess LLMs' performance in CTI applications.
CTIBench includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art models on these tasks provides insights into their strengths and weaknesses in CTI contexts, contributing to a better understanding of LLM capabilities in CTI.

------------

`[2406.08035] LVBench: An Extreme Long Video Understanding Benchmark <https://arxiv.org/abs/2406.08035>`__ LVBench:超长视频理解基准

::

    Wed, 12 Jun 2024 09:36:52 GMT
    Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang

Recent progress in multimodal large language models has markedly enhanced the understanding of short videos (typically under one minute), and several evaluation datasets have emerged accordingly. However, these advancements fall short of meeting the demands of real-world applications such as embodied intelligence for long-term decision-making, in-depth movie reviews and discussions, and live sports commentary, all of which require comprehension of long videos spanning several hours. To address this gap, we introduce LVBench, a benchmark specifically designed for long video understanding. Our dataset comprises publicly sourced videos and encompasses a diverse set of tasks aimed at long video comprehension and information extraction. LVBench is designed to challenge multimodal models to demonstrate long-term memory and extended comprehension capabilities. Our extensive evaluations reveal that current multimodal models still underperform on these demanding long video understanding tasks. Through LVBench, we aim to spur the development of more advanced models capable of tackling the complexities of long video comprehension. Our data and code are publicly available at: https://lvbench.github.io.

------------

`[2406.08467] DafnyBench: A Benchmark for Formal Software Verification <https://arxiv.org/abs/2406.08467>`__ DafnyBench:形式化软件验证的基准测试

::

    Wed, 12 Jun 2024 17:53:31 GMT
    Chloe Loughridge, Qinyi Sun, Seth Ahrenbach, Federico Cassano, Chuyue Sun, Ying Sheng, Anish Mudide, Md Rakib Hossain Misu, Nada Amin, Max Tegmark

We introduce DafnyBench, the largest benchmark of its kind for training and evaluating machine learning systems for formal software verification. We test the ability of LLMs such as GPT-4 and Claude 3 to auto-generate enough hints for the Dafny formal verification engine to successfully verify over 750 programs with about 53,000 lines of code. The best model and prompting scheme achieved 68% success rate, and we quantify how this rate improves when retrying with error message feedback and how it deteriorates with the amount of required code and hints. We hope that DafnyBench will enable rapid improvements from this baseline as LLMs and verification techniques grow in quality.

------------

`[2305.14225] ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media <https://arxiv.org/abs/2305.14225>`__ ManiTweet:识别社交媒体上新闻操纵的新基准

::

    replaced with revised version Wed, 12 Jun 2024 06:25:15 GMT
    Submission history From: Kung-Hsiang Huang [view email]
    [v1] Tue, 23 May 2023 16:40:07 UTC (7,149 KB)
    [v2] Wed, 12 Jun 2024 06:25:15 UTC (7,731 KB)
    Kung-Hsiang Huang, Hou Pong Chan, Kathleen McKeown, Heng Ji

Considerable advancements have been made to tackle the misrepresentation of information derived from reference articles in the domains of fact-checking and faithful summarization. However, an unaddressed aspect remains - the identification of social media posts that manipulate information within associated news articles. This task presents a significant challenge, primarily due to the prevalence of personal opinions in such posts. We present a novel task, identifying manipulation of news on social media, which aims to detect manipulation in social media posts and identify manipulated or inserted information. To study this task, we have proposed a data collection schema and curated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and corresponding articles. Our analysis demonstrates that this task is highly challenging, with large language models (LLMs) yielding unsatisfactory performance. Additionally, we have developed a simple yet effective basic model that outperforms LLMs significantly on the ManiTweet dataset. Finally, we have conducted an exploratory analysis of human-written tweets, unveiling intriguing connections between manipulation and the domain and factuality of news articles, as well as revealing that manipulated sentences are more likely to encapsulate the main story or consequences of a news outlet.

------------

`[2312.02219] Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models <https://arxiv.org/abs/2312.02219>`__ 神奇的背后，MERLIM:大型图像-语言模型的多模态评估基准

::

    replaced with revised version Wed, 12 Jun 2024 14:59:55 GMT
    Submission history From: Andrés Villa [view email]
    [v1] Sun, 3 Dec 2023 16:39:36 UTC (7,499 KB)
    [v2] Wed, 12 Jun 2024 14:59:55 UTC (6,798 KB)
    Andr\'es Villa, Juan Carlos Le\'on Alc\'azar, Alvaro Soto, Bernard Ghanem

Large Vision and Language Models have enabled significant advances in fully supervised and zero-shot visual tasks. These large architectures serve as the baseline to what is currently known as Instruction Tuning Large Vision and Language models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal assistants whose responses are modulated by natural language instructions and visual data. Despite this versatility, IT-LVLM effectiveness in fundamental computer vision problems remains unclear, primarily due to the absence of a standardized evaluation benchmark. This paper introduces a Multi-modal Evaluation Benchmark named MERLIM, a scalable test-bed to assess the capabilities of IT-LVLMs on fundamental computer vision tasks. MERLIM contains over 300K image-question pairs and has a strong focus on detecting cross-modal "hallucination" events in IT-LVLMs. Our results bring important insights on the performance of state-of-the-art IT-LVMLs including limitations at identifying fine-grained visual concepts, object hallucinations across tasks, and biases towards the language query. Our findings also suggest that these models have weak visual grounding, but manage to make adequate guesses from global visual patterns or language biases contained in the LLM component.

------------

---------------
Accelerate (11)
---------------

`[2406.08184] MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents <https://arxiv.org/abs/2406.08184>`__ MobileAgentBench:一个高效且用户友好的移动LLM代理基准

::

    Wed, 12 Jun 2024 13:14:50 GMT
    Luyuan Wang, Yongyu Deng, Yiwei Zha, Guodong Mao, Qinmin Wang, Tianchen Min, Wei Chen and Shoufa Chen

Large language model (LLM)-based mobile agents are increasingly popular due to their capability to interact directly with mobile phone Graphic User Interfaces (GUIs) and their potential to autonomously manage daily tasks.
Despite their promising prospects in both academic and industrial sectors, little research has focused on benchmarking the performance of existing mobile agents, due to the inexhaustible states of apps and the vague definition of feasible action sequences. To address this challenge, we propose an efficient and user-friendly benchmark, MobileAgentBench, designed to alleviate the burden of extensive manual testing. We initially define 100 tasks across 10 open-source apps, categorized by multiple levels of difficulty. Subsequently, we evaluate several existing mobile agents, including AppAgent and MobileAgent, to thoroughly and systematically compare their performance. All materials are accessible on our project webpage: https://MobileAgentBench.github.io, contributing to the advancement of both academic and industrial fields.

------------

`[2406.07855] VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment <https://arxiv.org/abs/2406.07855>`__ 

::

    Wed, 12 Jun 2024 04:09:44 GMT
    Bing Han, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Yanming Qian, Yanqing Liu, Sheng Zhao, Jinyu Li and Furu Wei

With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E.
Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.

------------

`[2406.07657] OPTune: Efficient Online Preference Tuning <https://arxiv.org/abs/2406.07657>`__ OPTune:高效的在线偏好调整

::

    Tue, 11 Jun 2024 18:55:04 GMT
    Lichang Chen, Jiuhai Chen, Chenxi Liu, John Kirchenbauer, Davit Soselia, Chen Zhu, Tom Goldstein, Tianyi Zhou, Heng Huang

Reinforcement learning with human feedback~(RLHF) is critical for aligning Large Language Models (LLMs) with human preference. Compared to the widely studied offline version of RLHF, \emph{e.g.} direct preference optimization (DPO), recent works have shown that the online variants achieve even better alignment. However, online alignment requires on-the-fly generation of new training data, which is costly, hard to parallelize, and suffers from varying quality and utility. In this paper, we propose a more efficient data exploration strategy for online preference tuning (OPTune), which does not rely on human-curated or pre-collected teacher responses but dynamically samples informative responses for on-policy preference alignment. During data generation, OPTune only selects prompts whose (re)generated responses can potentially provide more informative and higher-quality training signals than the existing responses. In the training objective, OPTune reweights each generated response (pair) by its utility in improving the alignment so that learning can be focused on the most helpful samples. Throughout our evaluations, OPTune'd LLMs maintain the instruction-following benefits provided by standard preference tuning whilst enjoying 1.27-1.56x faster training speed due to the efficient data exploration strategy.

------------

`[2406.08334] ProTrain: Efficient LLM Training via Memory-Aware Techniques <https://arxiv.org/abs/2406.08334>`__ ProTrain:基于记忆感知技术的高效LLM训练

::

    Wed, 12 Jun 2024 15:40:06 GMT
    Hanmei Yang, Jin Zhou, Yao Fu, Xiaoqun Wang, Ramine Roane, Hui Guan, Tongping Liu

It is extremely memory-hungry to train Large Language Models (LLM). To solve this problem, existing work exploits the combination of CPU and GPU for the training process, such as ZeRO-Offload. Such a technique largely democratizes billion-scale model training, making it possible to train with few consumer graphics cards. However, based on our observation, existing frameworks often provide coarse-grained memory management and require experienced experts in configuration tuning, leading to suboptimal hardware utilization and performance. This paper proposes ProTrain, a novel training system that intelligently balances memory usage and performance by coordinating memory, computation, and IO. ProTrain achieves adaptive memory management through Chunk-Based Model State Management and Block-Wise Activation Management, guided by a Memory-Aware Runtime Profiler without user intervention. ProTrain does not change the training algorithm and thus does not compromise accuracy.
Experiments show that ProTrain improves training throughput by 1.43$\times$ to 2.71$\times$ compared to the SOTA training systems.

------------

`[2406.07588] AIM: Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning <https://arxiv.org/abs/2406.07588>`__ 目的:让任何多模态大型语言模型都能实现高效的上下文学习

::

    Tue, 11 Jun 2024 08:12:43 GMT
    Jun Gao, Qian Qiao, Ziqiang Cao, Zili Wang, Wenjie Li

In-context learning (ICL) facilitates Large Language Models (LLMs) exhibiting emergent ability on downstream tasks without updating billions of parameters.
However, in the area of multi-modal Large Language Models (MLLMs), two problems hinder the application of multi-modal ICL: (1) Most primary MLLMs are only trained on single-image datasets, making them unable to read multi-modal demonstrations. (2) With the demonstrations increasing, thousands of visual tokens highly challenge hardware and degrade ICL performance. During preliminary explorations, we discovered that the inner LLM tends to focus more on the linguistic modality within multi-modal demonstrations to generate responses. Therefore, we propose a general and light-weighted framework \textbf{AIM} to tackle the mentioned problems through \textbf{A}ggregating \textbf{I}mage information of \textbf{M}ultimodal demonstrations to the dense latent space of the corresponding linguistic part. Specifically, AIM first uses the frozen backbone MLLM to read each image-text demonstration and extracts the vector representations on top of the text. These vectors naturally fuse the information of the image-text pair, and AIM transforms them into fused virtual tokens acceptable for the inner LLM via a trainable projection layer.
Ultimately, these fused tokens function as variants of multi-modal demonstrations, fed into the MLLM to direct its response to the current query as usual. Because these fused tokens stem from the textual component of the image-text pair, a multi-modal demonstration is nearly reduced to a pure textual demonstration, thus seamlessly applying to any MLLMs. With its de facto MLLM frozen, AIM is parameter-efficient and we train it on public multi-modal web corpora which have nothing to do with downstream test tasks.

------------

`[2406.08413] Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference <https://arxiv.org/abs/2406.08413>`__ 内存就是你所需要的一切:加速大型语言模型推理的内存计算架构概述

::

    Wed, 12 Jun 2024 16:57:58 GMT
    Christopher Wolters, Xiaoxuan Yang, Ulf Schlichtmann, Toyotaro Suzumura

Large language models (LLMs) have recently transformed natural language processing, enabling machines to generate human-like text and engage in meaningful conversations. This development necessitates speed, efficiency, and accessibility in LLM inference as the computational and memory requirements of these systems grow exponentially. Meanwhile, advancements in computing and memory capabilities are lagging behind, exacerbated by the discontinuation of Moore's law. With LLMs exceeding the capacity of single GPUs, they require complex, expert-level configurations for parallel processing. Memory accesses become significantly more expensive than computation, posing a challenge for efficient scaling, known as the memory wall. Here, compute-in-memory (CIM) technologies offer a promising solution for accelerating AI inference by directly performing analog computations in memory, potentially reducing latency and power consumption. By closely integrating memory and compute elements, CIM eliminates the von Neumann bottleneck, reducing data movement and improving energy efficiency. This survey paper provides an overview and analysis of transformer-based models, reviewing various CIM architectures and exploring how they can address the imminent challenges of modern AI computing systems. We discuss transformer-related operators and their hardware acceleration schemes and highlight challenges, trends, and insights in corresponding CIM designs.

------------

`[2311.05263] Model-Based Minimum Bayes Risk Decoding for Text Generation <https://arxiv.org/abs/2311.05263>`__ 基于模型的最小贝叶斯风险解码文本生成

::

    replaced with revised version Wed, 12 Jun 2024 01:00:09 GMT
    Submission history From: Yuu Jinnai [view email]
    [v1] Thu, 9 Nov 2023 10:46:09 UTC (67 KB)
    [v2] Wed, 12 Jun 2024 01:00:09 UTC (218 KB)
    Yuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, Kenshi Abe

Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative to beam search decoding in a variety of text generation tasks. MBR decoding selects a hypothesis from a pool of hypotheses that has the least expected risk under a probability model according to a given utility function. Since it is impractical to compute the expected risk exactly over all possible hypotheses, two approximations are commonly used in MBR. First, it integrates over a sampled set of hypotheses rather than over all possible hypotheses. Second, it estimates the probability of each hypothesis using a Monte Carlo estimator. While the first approximation is necessary to make it computationally feasible, the second is not essential since we typically have access to the model probability at inference time. We propose Model-Based MBR (MBMBR), a variant of MBR that uses the model probability itself as the estimate of the probability distribution instead of the Monte Carlo estimate. We show analytically and empirically that the model-based estimate is more promising than the Monte Carlo estimate in text generation tasks. Our experiments show that MBMBR outperforms MBR in several text generation tasks, both with encoder-decoder models and with large language models.

------------

`[2401.05054] Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding <https://arxiv.org/abs/2401.05054>`__ 基于最小贝叶斯风险解码生成多样化高质量文本

::

    replaced with revised version Wed, 12 Jun 2024 01:27:32 GMT
    Submission history From: Yuu Jinnai [view email]
    [v1] Wed, 10 Jan 2024 10:23:41 UTC (808 KB)
    [v2] Wed, 12 Jun 2024 01:27:32 UTC (372 KB)
    Yuu Jinnai, Ukyo Honda, Tetsuro Morimura, Peinan Zhang

One of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed for generating diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying methods. In this paper, we investigate an alternative approach -- we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and $k$-medoids MBR (KMBR), methods to generate a set of sentences with high quality and diversity. We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a large language model with prompting. The experimental results show that the proposed method achieves a better trade-off than the diverse beam search and sampling algorithms.

------------

`[2404.11932] CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment <https://arxiv.org/abs/2404.11932>`__ CrossIn:一种面向跨语言知识对齐的高效指令调优方法

::

    replaced with revised version Wed, 12 Jun 2024 09:35:48 GMT
    Submission history From: Geyu Lin [view email]
    [v1] Thu, 18 Apr 2024 06:20:50 UTC (630 KB)
    [v2] Wed, 12 Jun 2024 09:35:48 UTC (1,209 KB)
    Geyu Lin, Bin Wang, Zhengyuan Liu, Nancy F. Chen

Multilingual proficiency presents a significant challenge for large language models (LLMs). English-centric models are usually suboptimal in other languages, particularly those that are linguistically distant from English. This performance discrepancy mainly stems from the imbalanced distribution of training data across languages during pre-training and instruction tuning stages. To address this problem, we propose a novel approach called CrossIn, which utilizes a mixed composition of cross-lingual instruction tuning data. Our method leverages the compressed representation shared by various languages to efficiently enhance the model's task-solving capabilities and multilingual proficiency within a single process. In addition, we introduce a multi-task and multi-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental results demonstrate that our method substantially improves performance across tasks and languages, and we provide extensive insights into the impact of cross-lingual data volume and the integration of translation data on enhancing multilingual consistency and accuracy.

------------

`[2405.11966] Multiple-Choice Questions are Efficient and Robust LLM Evaluators <https://arxiv.org/abs/2405.11966>`__ 选择题是高效和稳健的LLM评估器

::

    replaced with revised version Wed, 12 Jun 2024 16:05:40 GMT
    Submission history From: Ziyin Zhang [view email]
    [v1] Mon, 20 May 2024 11:47:13 UTC (156 KB)
    [v2] Tue, 21 May 2024 15:16:46 UTC (156 KB)
    [v3] Wed, 12 Jun 2024 16:05:40 UTC (455 KB)
    Ziyin Zhang and Lizhen Xu and Zhaokun Jiang and Hongkun Hao and Rui Wang

We present GSM-MC and MATH-MC, two multiple-choice (MC) datasets constructed by collecting answers and incorrect predictions on GSM8K and MATH from 60 open-source models. Through extensive experiments, we show that LLMs' performance on the MC versions of these two popular benchmarks is strongly correlated with their performance on the original versions and is quite robust to distractor choices and option orders, while the evaluation time is reduced by a factor of up to 30. Following a similar procedure, we introduce PythonIO, a new program output prediction MC dataset constructed from two other popular LLM evaluation benchmarks, HumanEval and MBPP. Our data and code are available at this https URL.

------------

`[2402.09398] Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference <https://arxiv.org/abs/2402.09398>`__ 以少求多:综合递归与KV缓存压缩实现高效的LLM推理

::

    replaced with revised version Wed, 12 Jun 2024 06:08:58 GMT
    Submission history From: Harry Dong [view email]
    [v1] Wed, 14 Feb 2024 18:54:56 UTC (27,238 KB)
    [v2] Wed, 12 Jun 2024 06:08:58 UTC (27,234 KB)
    Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, Beidi Chen

Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at this https URL.

------------

-----------------------
In-Context Learning (5)
-----------------------

`[2406.07913] DeTriever: Decoder-representation-based Retriever for Improving NL2SQL In-Context Learning <https://arxiv.org/abs/2406.07913>`__ DeTriever:基于解码器表示的改进NL2SQL上下文学习的检索器

::

    Wed, 12 Jun 2024 06:33:54 GMT
    Yuxi Feng, Raymond Li, Zhenan Fan, Giuseppe Carenini, Mohammadreza Pourreza, Weiwei Zhang, Yong Zhang

While in-context Learning (ICL) has proven to be an effective technique to improve the performance of Large Language Models (LLMs) in a variety of complex tasks, notably in translating natural language questions into Structured Query Language (NL2SQL), the question of how to select the most beneficial demonstration examples remains an open research problem. While prior works often adapted off-the-shelf encoders to retrieve examples dynamically, an inherent discrepancy exists in the representational capacities between the external retrievers and the LLMs. Further, optimizing the selection of examples is a non-trivial task, since there are no straightforward methods to assess the relative benefits of examples without performing pairwise inference. To address these shortcomings, we propose DeTriever, a novel demonstration retrieval framework that learns a weighted combination of LLM hidden states, where rich semantic information is encoded. To train the model, we propose a proxy score that estimates the relative benefits of examples based on the similarities between output queries. Experiments on two popular NL2SQL benchmarks demonstrate that our method significantly outperforms the state-of-the-art baselines on one-shot NL2SQL tasks.

------------

`[2406.07970] Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation <https://arxiv.org/abs/2406.07970>`__ 通过机器翻译质量估计指导llm的上下文学习

::

    Wed, 12 Jun 2024 07:49:36 GMT
    Javad Pourmostafa Roshan Sharami, Dimitar Shterionov and Pieter Spronck

The quality of output from large language models (LLMs), particularly in machine translation (MT), is closely tied to the quality of in-context examples (ICEs) provided along with the query, i.e., the text to translate. The effectiveness of these ICEs is influenced by various factors, such as the domain of the source text, the order in which the ICEs are presented, the number of these examples, and the prompt templates used. Naturally, selecting the most impactful ICEs depends on understanding how these affect the resulting translation quality, which ultimately relies on translation references or human judgment. This paper presents a novel methodology for in-context learning (ICL) that relies on a search algorithm guided by domain-specific quality estimation (QE). Leveraging the XGLM model, our methodology estimates the resulting translation quality without the need for translation references, selecting effective ICEs for MT to maximize translation quality. Our results demonstrate significant improvements over existing ICL methods and higher translation performance compared to fine-tuning a pre-trained language model (PLM), specifically mBART-50.

------------

`[2406.07588] AIM: Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning <https://arxiv.org/abs/2406.07588>`__ 目的:让任何多模态大型语言模型都能实现高效的上下文学习

::

    Tue, 11 Jun 2024 08:12:43 GMT
    Jun Gao, Qian Qiao, Ziqiang Cao, Zili Wang, Wenjie Li

In-context learning (ICL) facilitates Large Language Models (LLMs) exhibiting emergent ability on downstream tasks without updating billions of parameters.
However, in the area of multi-modal Large Language Models (MLLMs), two problems hinder the application of multi-modal ICL: (1) Most primary MLLMs are only trained on single-image datasets, making them unable to read multi-modal demonstrations. (2) With the demonstrations increasing, thousands of visual tokens highly challenge hardware and degrade ICL performance. During preliminary explorations, we discovered that the inner LLM tends to focus more on the linguistic modality within multi-modal demonstrations to generate responses. Therefore, we propose a general and light-weighted framework \textbf{AIM} to tackle the mentioned problems through \textbf{A}ggregating \textbf{I}mage information of \textbf{M}ultimodal demonstrations to the dense latent space of the corresponding linguistic part. Specifically, AIM first uses the frozen backbone MLLM to read each image-text demonstration and extracts the vector representations on top of the text. These vectors naturally fuse the information of the image-text pair, and AIM transforms them into fused virtual tokens acceptable for the inner LLM via a trainable projection layer.
Ultimately, these fused tokens function as variants of multi-modal demonstrations, fed into the MLLM to direct its response to the current query as usual. Because these fused tokens stem from the textual component of the image-text pair, a multi-modal demonstration is nearly reduced to a pure textual demonstration, thus seamlessly applying to any MLLMs. With its de facto MLLM frozen, AIM is parameter-efficient and we train it on public multi-modal web corpora which have nothing to do with downstream test tasks.

------------

`[2404.02060] Long-context LLMs Struggle with Long In-context Learning <https://arxiv.org/abs/2404.02060>`__ 长上下文语言模型很难进行长上下文学习

::

    replaced with revised version Wed, 12 Jun 2024 02:46:16 GMT
    Submission history From: Tianle Li [view email]
    [v1] Tue, 2 Apr 2024 15:59:11 UTC (3,640 KB)
    [v2] Thu, 4 Apr 2024 00:01:25 UTC (3,625 KB)
    [v3] Wed, 12 Jun 2024 02:46:16 UTC (3,066 KB)
    Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen

Large Language Models (LLMs) have made significant strides in handling long sequences. Some models like Gemini could even to be capable of dealing with millions of tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their true abilities in more challenging, real-world scenarios. We introduce a benchmark (LongICLBench) for long in-context learning in extreme-label classification using six datasets with 28 to 174 classes and input lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate on 15 long-context LLMs and find that they perform well on less challenging classification tasks with smaller label space and shorter demonstrations. However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences. Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LongICLBench could serve as a more realistic evaluation for the future long-context LLMs.

------------

`[2405.10548] Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks <https://arxiv.org/abs/2405.10548>`__ 语言模型可以利用跨任务上下文学习来完成数据稀缺的新任务

::

    replaced with revised version Wed, 12 Jun 2024 08:00:19 GMT
    Submission history From: Anwoy Chatterjee [view email]
    [v1] Fri, 17 May 2024 05:20:49 UTC (7,150 KB)
    [v2] Mon, 20 May 2024 06:35:36 UTC (7,150 KB)
    [v3] Wed, 12 Jun 2024 08:00:19 UTC (7,137 KB)
    Anwoy Chatterjee, Eshaan Tanwar, Subhabrata Dutta, Tanmoy Chakraborty

Large Language Models (LLMs) have transformed NLP with their remarkable In-context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal models excel in zero-shot performance, their computational demands limit widespread use, and smaller language models struggle without context. This paper investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks. Drawing inspiration from biological neurons and the mechanistic interpretation of the Transformer architecture, we explore the potential for information sharing across tasks. We design a cross-task prompting setup with three LLMs and show that LLMs achieve significant performance improvements despite no examples from the target task in the context. Cross-task prompting leads to a remarkable performance boost of 107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 on average over zero-shot prompting, and performs comparable to standard in-context learning. The effectiveness of generating pseudo-labels for in-task examples is demonstrated, and our analyses reveal a strong correlation between the effect of cross-task examples and model activation similarities in source and target input tokens. This paper offers a first-of-its-kind exploration of LLMs' ability to solve novel tasks based on contextual signals from different task examples.

------------

-------------
Reasoning (4)
-------------

`[2401.04319] Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs <https://arxiv.org/abs/2401.04319>`__ 更好地了解你的需求:用类比推理增强的llm结构化地理解营销人员的需求

::

    replaced with revised version Wed, 12 Jun 2024 03:02:45 GMT
    Submission history From: Junjie Wang [view email]
    [v1] Tue, 9 Jan 2024 02:25:23 UTC (2,047 KB)
    [v2] Wed, 7 Feb 2024 15:01:21 UTC (3,546 KB)
    [v3] Wed, 12 Jun 2024 03:02:45 UTC (3,268 KB)
    Junjie Wang, Dan Yang, Binbin Hu, Yue Shen, Wen Zhang, Jinjie Gu

In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. In practical scenarios, the demands of non-expert marketers are often abstract and diverse. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. To stimulate the LLMs' reasoning ability, the chain-of-thought (CoT) prompting method is widely used, but existing methods still have some limitations in our scenario: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and concrete questions, making LLMs ineffective when the marketers' demands are abstract and diverse. (2) Previous methods are often implemented in closed-source models or excessively large models, which is not suitable in industrial practical scenarios. Based on these, we propose ARALLM (i.e., Analogical Reasoning Augmented Large Language Models) consisting of two modules: Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model Distillation. Part of our data and code can be found at this https URL.

------------

`[2402.04978] An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration <https://arxiv.org/abs/2402.04978>`__ 一种集成知识图谱协同的增强提示式LLM推理方案

::

    replaced with revised version Wed, 12 Jun 2024 16:03:31 GMT
    Submission history From: Yihao Li [view email]
    [v1] Wed, 7 Feb 2024 15:56:17 UTC (637 KB)
    [v2] Wed, 12 Jun 2024 16:03:31 UTC (629 KB)
    Yihao Li, Ru Zhang, Jianyi Liu

While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work. Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and LLMs, thereby enhancing LLMs' proficiency in solving complex issues.

------------

`[2402.12692] FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning <https://arxiv.org/abs/2402.12692>`__ formulasoning:基于公式的数值推理数据集

::

    replaced with revised version Wed, 12 Jun 2024 13:19:55 GMT
    Submission history From: Xiao Li [view email]
    [v1] Tue, 20 Feb 2024 03:39:49 UTC (9,374 KB)
    [v2] Wed, 21 Feb 2024 02:17:47 UTC (9,374 KB)
    [v3] Wed, 12 Jun 2024 13:19:55 UTC (1,700 KB)
    Xiao Li, Bolin Zhu, Sichen Liu, Yin Zhu, Yiwei Liu, Gong Cheng

The application of formulas is a fundamental ability of humans when addressing numerical reasoning problems. However, existing numerical reasoning datasets seldom explicitly indicate the formulas employed during the reasoning steps. To bridge this gap, we construct a dataset for formula-based numerical reasoning called FormulaReasoning, which consists of 5,420 reasoning-based questions. We employ it to conduct evaluations of LLMs with size ranging from 7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thought methods, and we further explore using retrieval-augmented LLMs provided with an external formula database associated with our dataset. We also experiment with supervised methods where we divide the reasoning process into formula generation, parameter extraction, and numerical calculation, and perform data augmentation. Our empirical findings underscore the significant potential for improvement in existing models when applied to our complex, formula-driven FormulaReasoning.

------------

`[2403.02567] Eliciting Better Multilingual Structured Reasoning from LLMs through Code <https://arxiv.org/abs/2403.02567>`__ 通过代码从llm中诱导更好的多语言结构化推理

::

    replaced with revised version Wed, 12 Jun 2024 07:13:01 GMT
    Submission history From: Tamer Alkhouli [view email]
    [v1] Tue, 5 Mar 2024 00:48:56 UTC (1,195 KB)
    [v2] Wed, 12 Jun 2024 07:13:01 UTC (1,103 KB)
    Bryan Li and Tamer Alkhouli and Daniele Bonadiman and Nikolaos Pappas and Saab Mansour

The development of large language models (LLM) has shown progress on reasoning, though studies have largely considered either English or simple reasoning tasks. To address this, we introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks.
We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multilingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reasoning subtask. Furthermore, the models show no regression on non-reasoning tasks, thus demonstrating our techniques maintain general-purpose abilities.

------------

-----------
ToolUse (1)
-----------

`[2406.08246] Leveraging Large Language Models for Web Scraping <https://arxiv.org/abs/2406.08246>`__ 利用大型语言模型进行Web抓取

::

    Wed, 12 Jun 2024 14:15:15 GMT
    Aman Ahluwalia, Suhrud Wani

Large Language Models (LLMs) demonstrate remarkable capabilities in replicating human tasks and boosting productivity. However, their direct application for data extraction presents limitations due to a prioritisation of fluency over factual accuracy and a restricted ability to manipulate specific information. Therefore to overcome these limitations, this research leverages the knowledge representation power of pre-trained LLMs and the targeted information access enabled by RAG models, this research investigates a general-purpose accurate data scraping recipe for RAG models designed for language generation. To capture knowledge in a more modular and interpretable way, we use pre trained language models with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus. We utilised RAG model architecture and did an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Chunking HTML text for effective understanding, and (iii) comparing results from different LLMs and ranking algorithms. While previous work has developed dedicated architectures and training procedures for HTML understanding and extraction, we show that LLMs pre-trained on standard natural language with an addition of effective chunking, searching and ranking algorithms, can prove to be efficient data scraping tool to extract complex data from unstructured text. Future research directions include addressing the challenges of provenance tracking and dynamic knowledge updates within the proposed RAG-based data extraction framework. By overcoming these limitations, this approach holds the potential to revolutionise data extraction from vast repositories of textual information.

------------

------------------------
Retrieval-Augmented (12)
------------------------

`[2406.07736] MultiPragEval: Multilingual Pragmatic Evaluation of Large Language Models <https://arxiv.org/abs/2406.07736>`__ MultiPragEval:大型语言模型的多语言语用评估

::

    Tue, 11 Jun 2024 21:46:03 GMT
    Dojun Park, Jiwoo Lee, Seohyun Park, Hyeyun Jeong, Youngeun Koo, Soonha Hwang, Seonwoo Park, Sungeun Lee

As the capabilities of LLMs expand, it becomes increasingly important to evaluate them beyond basic knowledge assessment, focusing on higher-level language understanding. This study introduces MultiPragEval, a robust test suite designed for the multilingual pragmatic evaluation of LLMs across English, German, Korean, and Chinese. Comprising 1200 question units categorized according to Grice's Cooperative Principle and its four conversational maxims, MultiPragEval enables an in-depth assessment of LLMs' contextual awareness and their ability to infer implied meanings. Our findings demonstrate that Claude3-Opus significantly outperforms other models in all tested languages, establishing a state-of-the-art in the field. Among open-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.
This study not only leads the way in the multilingual evaluation of LLMs in pragmatic inference but also provides valuable insights into the nuanced capabilities necessary for advanced language comprehension in AI systems.

------------

`[2406.07913] DeTriever: Decoder-representation-based Retriever for Improving NL2SQL In-Context Learning <https://arxiv.org/abs/2406.07913>`__ DeTriever:基于解码器表示的改进NL2SQL上下文学习的检索器

::

    Wed, 12 Jun 2024 06:33:54 GMT
    Yuxi Feng, Raymond Li, Zhenan Fan, Giuseppe Carenini, Mohammadreza Pourreza, Weiwei Zhang, Yong Zhang

While in-context Learning (ICL) has proven to be an effective technique to improve the performance of Large Language Models (LLMs) in a variety of complex tasks, notably in translating natural language questions into Structured Query Language (NL2SQL), the question of how to select the most beneficial demonstration examples remains an open research problem. While prior works often adapted off-the-shelf encoders to retrieve examples dynamically, an inherent discrepancy exists in the representational capacities between the external retrievers and the LLMs. Further, optimizing the selection of examples is a non-trivial task, since there are no straightforward methods to assess the relative benefits of examples without performing pairwise inference. To address these shortcomings, we propose DeTriever, a novel demonstration retrieval framework that learns a weighted combination of LLM hidden states, where rich semantic information is encoded. To train the model, we propose a proxy score that estimates the relative benefits of examples based on the similarities between output queries. Experiments on two popular NL2SQL benchmarks demonstrate that our method significantly outperforms the state-of-the-art baselines on one-shot NL2SQL tasks.

------------

`[2406.08116] Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling <https://arxiv.org/abs/2406.08116>`__ 基于支持度的检索增强语言建模知识重写

::

    Wed, 12 Jun 2024 11:52:35 GMT
    Zile Qiao, Wei Ye, Yong Jiang, Tong Mo, Pengjun Xie, Weiping Li, Fei Huang, Shikun Zhang

Retrieval-augmented language models (RALMs) have recently shown great potential in mitigating the limitations of implicit knowledge in LLMs, such as untimely updating of the latest expertise and unreliable retention of long-tail knowledge. However, since the external knowledge base, as well as the retriever, can not guarantee reliability, potentially leading to the knowledge retrieved not being helpful or even misleading for LLM generation. In this paper, we introduce Supportiveness-based Knowledge Rewriting (SKR), a robust and pluggable knowledge rewriter inherently optimized for LLM generation.
Specifically, we introduce the novel concept of "supportiveness"--which represents how effectively a knowledge piece facilitates downstream tasks--by considering the perplexity impact of augmented knowledge on the response text of a white-box LLM. Based on knowledge supportiveness, we first design a training data curation strategy for our rewriter model, effectively identifying and filtering out poor or irrelevant rewrites (e.g., with low supportiveness scores) to improve data efficacy. We then introduce the direct preference optimization (DPO) algorithm to align the generated rewrites to optimal supportiveness, guiding the rewriter model to summarize augmented content that better improves the final response. Comprehensive evaluations across six popular knowledge-intensive tasks and four LLMs have demonstrated the effectiveness and superiority of SKR. With only 7B parameters, SKR has shown better knowledge rewriting capability over GPT-4, the current state-of-the-art general-purpose LLM.

------------

`[2406.08124] Legend: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets <https://arxiv.org/abs/2406.08124>`__ 图例:利用表示工程对偏好数据集的安全裕度进行标注

::

    Wed, 12 Jun 2024 12:06:32 GMT
    Duanyu Feng, Bowen Qin, Chen Huang, Youcheng Huang, Zheng Zhang, Wenqiang Lei

The success of the reward model in distinguishing between responses with subtle safety differences depends critically on the high-quality preference dataset, which should capture the fine-grained nuances of harmful and harmless responses. This motivates the need to develop a dataset involving preference margins, which accurately quantify how harmless one response is compared to another. In this paper, we take the first step to propose an effective and cost-efficient framework to promote the margin-enhanced preference dataset development. Our framework, Legend, Leverages representation engineering to annotate preference datasets. It constructs the specific direction within the LLM's embedding space that represents safety. By leveraging this safety direction, Legend can then leverage the semantic distances of paired responses along this direction to annotate margins automatically. We experimentally demonstrate our effectiveness in both reward modeling and harmless alignment for LLMs. Legend also stands out for its efficiency, requiring only the inference time rather than additional training. This efficiency allows for easier implementation and scalability, making Legend particularly valuable for practical applications in aligning LLMs with safe conversations.

------------

`[2406.08246] Leveraging Large Language Models for Web Scraping <https://arxiv.org/abs/2406.08246>`__ 利用大型语言模型进行Web抓取

::

    Wed, 12 Jun 2024 14:15:15 GMT
    Aman Ahluwalia, Suhrud Wani

Large Language Models (LLMs) demonstrate remarkable capabilities in replicating human tasks and boosting productivity. However, their direct application for data extraction presents limitations due to a prioritisation of fluency over factual accuracy and a restricted ability to manipulate specific information. Therefore to overcome these limitations, this research leverages the knowledge representation power of pre-trained LLMs and the targeted information access enabled by RAG models, this research investigates a general-purpose accurate data scraping recipe for RAG models designed for language generation. To capture knowledge in a more modular and interpretable way, we use pre trained language models with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus. We utilised RAG model architecture and did an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Chunking HTML text for effective understanding, and (iii) comparing results from different LLMs and ranking algorithms. While previous work has developed dedicated architectures and training procedures for HTML understanding and extraction, we show that LLMs pre-trained on standard natural language with an addition of effective chunking, searching and ranking algorithms, can prove to be efficient data scraping tool to extract complex data from unstructured text. Future research directions include addressing the challenges of provenance tracking and dynamic knowledge updates within the proposed RAG-based data extraction framework. By overcoming these limitations, this approach holds the potential to revolutionise data extraction from vast repositories of textual information.

------------

`[2406.07796] Harnessing GenAI for Higher Education: A Study of a Retrieval Augmented Generation Chatbot's Impact on Human Learning <https://arxiv.org/abs/2406.07796>`__ 利用GenAI进行高等教育:检索增强代聊天机器人对人类学习影响的研究

::

    Wed, 12 Jun 2024 01:19:36 GMT
    Maung Thway, Jose Recatala-Gomez, Fun Siong Lim, Kedar Hippalgaonkar, Leonard W. T. Ng

The advent of generative artificial intelligence (GenAI) and large language models (LLMs) has opened new avenues for enhancing human learning. This study introduces Professor Leodar, a custom-built, Singlish-speaking Retrieval Augmented Generation (RAG) chatbot designed to enhance educational support for undergraduate engineering students. Deployed at Nanyang Technological University, Singapore, Professor Leodar offers a glimpse into the future of AI-assisted learning, offering personalized guidance, 24/7 availability, and contextually relevant information. Through a mixed-methods approach, we uncover the impact of Professor Leodar on student learning, engagement, and exam preparedness, with 97.1% of participants reporting positive experiences. These findings help define possible roles of AI in education and highlight the potential of custom GenAI chatbots. Our combination of chatbot development, in-class deployment and study of learning outcomes offers a benchmark for GenAI educational tools and serves as stepping stone for redefining the interplay between AI and human learning.

------------

`[2312.10904] Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI) <https://arxiv.org/abs/2312.10904>`__ 基于人工智能的动态检索增强本体生成(DRAGON-AI)

::

    replaced with revised version Wed, 12 Jun 2024 17:15:37 GMT
    Submission history From: Christopher Mungall [view email]
    [v1] Mon, 18 Dec 2023 03:19:31 UTC (745 KB)
    [v2] Wed, 12 Jun 2024 17:15:37 UTC (750 KB)
    Sabrina Toro, Anna V Anagnostopoulos, Sue Bello, Kai Blumberg, Rhiannon Cameron, Leigh Carmody, Alexander D Diehl, Damion Dooley, William Duncan, Petra Fey, Pascale Gaudet, Nomi L Harris, Marcin Joachimiak, Leila Kiani, Tiago Lubiana, Monica C Munoz-Torres, Shawn O'Neil, David Osumi-Sutherland, Aleix Puig, Justin P Reese, Leonore Reiser, Sofia Robb, Troy Ruemping, James Seager, Eric Sid, Ray Stefancsik, Magalie Weber, Valerie Wood, Melissa A Haendel, Christopher J Mungall

Background: Ontologies are fundamental components of informatics infrastructure in domains such as biomedical, environmental, and food sciences, representing consensus knowledge in an accurate and computable form. However, their construction and maintenance demand substantial resources and necessitate substantial collaboration between domain experts, curators, and ontology experts. We present Dynamic Retrieval Augmented Generation of Ontologies using AI (DRAGON-AI), an ontology generation method employing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG). DRAGON-AI can generate textual and logical ontology components, drawing from existing knowledge in multiple ontologies and unstructured text sources.
Results: We assessed performance of DRAGON-AI on de novo term construction across ten diverse ontologies, making use of extensive manual evaluation of results. Our method has high precision for relationship generation, but has slightly lower precision than from logic-based reasoning. Our method is also able to generate definitions deemed acceptable by expert evaluators, but these scored worse than human-authored definitions. Notably, evaluators with the highest level of confidence in a domain were better able to discern flaws in AI-generated definitions. We also demonstrated the ability of DRAGON-AI to incorporate natural language instructions in the form of GitHub issues.
Conclusions: These findings suggest DRAGON-AI's potential to substantially aid the manual ontology construction process. However, our results also underscore the importance of having expert curators and ontology editors drive the ontology generation process.

------------

`[2401.07103] Leveraging Large Language Models for NLG Evaluation: Advances and Challenges <https://arxiv.org/abs/2401.07103>`__ 利用大型语言模型进行NLG评估:进展与挑战

::

    replaced with revised version Wed, 12 Jun 2024 08:31:58 GMT
    Submission history From: Chongyang Tao [view email]
    [v1] Sat, 13 Jan 2024 15:59:09 UTC (7,300 KB)
    [v2] Wed, 12 Jun 2024 08:31:58 UTC (7,303 KB)
    Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, Shuai Ma

In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This paper aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this paper seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.

------------

`[2402.18150] Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation <https://arxiv.org/abs/2402.18150>`__ 面向检索增强生成的大型语言模型无监督信息精化训练

::

    replaced with revised version Wed, 12 Jun 2024 03:21:15 GMT
    Submission history From: Shicheng Xu [view email]
    [v1] Wed, 28 Feb 2024 08:24:38 UTC (7,210 KB)
    [v2] Wed, 12 Jun 2024 03:21:15 UTC (7,209 KB)
    Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, Jie Zhou

Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39\% relative points. InFO-RAG also shows advantages in in-context learning and robustness of RAG.

------------

`[2403.01193] RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots <https://arxiv.org/abs/2403.01193>`__ 粗糙的边缘:检索增强聊天机器人的双刃剑

::

    replaced with revised version Wed, 12 Jun 2024 12:00:52 GMT
    Submission history From: Philip Feldman [view email]
    [v1] Sat, 2 Mar 2024 12:19:04 UTC (520 KB)
    [v2] Wed, 13 Mar 2024 21:57:19 UTC (521 KB)
    [v3] Wed, 12 Jun 2024 12:00:52 UTC (521 KB)
    Philip Feldman, James R. Foulds, Shimei Pan

Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and discuss implications for the development of more trustworthy LLMs.

------------

`[2406.07348] DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation for Question-Answering <https://arxiv.org/abs/2406.07348>`__ DR-RAG:将动态文档相关性应用于问答检索增强生成

::

    replaced with revised version Wed, 12 Jun 2024 01:06:59 GMT
    Submission history From: Wenjie Ou [view email]
    [v1] Tue, 11 Jun 2024 15:15:33 UTC (167 KB)
    [v2] Wed, 12 Jun 2024 01:06:59 UTC (167 KB)
    Zijian Hei and Weiling Liu and Wenjie Ou and Juyi Qiao and Junming Jiao and Zhiqing Zhu and Guowen Song

Retrieval-Augmented Generation (RAG) has significantly demonstrated the performance of Large Language Models (LLMs) in the knowledge-intensive tasks, such as Question-Answering (QA). RAG expands the query context by incorporating external knowledge bases to enhance the response accuracy. However, it would be inefficient to access LLMs multiple times for each query and unreliable to retrieve all the relevant documents by a single query. We find that even though there is low relevance between some critical documents and query, it is possible to retrieve the remaining documents by combining parts of the documents with the query. To mine the relevance, a two-stage retrieval framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is proposed to improve document retrieval recall and the accuracy of answers while maintaining efficiency. Also, a small classifier is applied to two different selection strategies to determine the contribution of the retrieved documents to answering the query and retrieve the relatively relevant documents. Meanwhile, DR-RAG call the LLMs only once, which significantly improves the efficiency of the experiment. The experimental results on multi-hop QA datasets show that DR-RAG can significantly improve the accuracy of the answers and achieve new progress in QA systems.

------------

`[2403.08337] LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments <https://arxiv.org/abs/2403.08337>`__ llm辅助照明:利用大型语言模型能力在复杂城市环境中进行拟人交通信号控制

::

    replaced with revised version Wed, 12 Jun 2024 14:53:58 GMT
    Submission history From: Maonan Wang [view email]
    [v1] Wed, 13 Mar 2024 08:41:55 UTC (8,886 KB)
    [v2] Wed, 12 Jun 2024 14:53:58 UTC (8,980 KB)
    Maonan Wang, Aoyu Pang, Yuheng Kan, Man-On Pun, Chung Shue Chen, Bo Huang

Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system's adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at this https URL.

------------

---------
Agent (6)
---------

`[2406.07572] Domain-specific ReAct for physics-integrated iterative modeling: A case study of LLM agents for gas path analysis of gas turbines <https://arxiv.org/abs/2406.07572>`__ 特定领域的ReAct for physics集成迭代建模:用于燃气轮机气路分析的LLM代理案例研究

::

    Sat, 1 Jun 2024 13:35:18 GMT
    Tao Song and Yuwei Fan and Chenlong Feng and Keyu Song and Chao Liu and Dongxiang Jiang

This study explores the application of large language models (LLMs) with callable tools in energy and power engineering domain, focusing on gas path analysis of gas turbines. We developed a dual-agent tool-calling process to integrate expert knowledge, predefined tools, and LLM reasoning. We evaluated various LLMs, including LLama3, Qwen1.5 and GPT. Smaller models struggled with tool usage and parameter extraction, while larger models demonstrated favorable capabilities. All models faced challenges with complex, multi-component problems. Based on the test results, we infer that LLMs with nearly 100 billion parameters could meet professional scenario requirements with fine-tuning and advanced prompt design. Continued development are likely to enhance their accuracy and effectiveness, paving the way for more robust AI-driven solutions.

------------

`[2406.08184] MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents <https://arxiv.org/abs/2406.08184>`__ MobileAgentBench:一个高效且用户友好的移动LLM代理基准

::

    Wed, 12 Jun 2024 13:14:50 GMT
    Luyuan Wang, Yongyu Deng, Yiwei Zha, Guodong Mao, Qinmin Wang, Tianchen Min, Wei Chen and Shoufa Chen

Large language model (LLM)-based mobile agents are increasingly popular due to their capability to interact directly with mobile phone Graphic User Interfaces (GUIs) and their potential to autonomously manage daily tasks.
Despite their promising prospects in both academic and industrial sectors, little research has focused on benchmarking the performance of existing mobile agents, due to the inexhaustible states of apps and the vague definition of feasible action sequences. To address this challenge, we propose an efficient and user-friendly benchmark, MobileAgentBench, designed to alleviate the burden of extensive manual testing. We initially define 100 tasks across 10 open-source apps, categorized by multiple levels of difficulty. Subsequently, we evaluate several existing mobile agents, including AppAgent and MobileAgent, to thoroughly and systematically compare their performance. All materials are accessible on our project webpage: https://MobileAgentBench.github.io, contributing to the advancement of both academic and industrial fields.

------------

`[2406.07561] Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security <https://arxiv.org/abs/2406.07561>`__ 人工智能成为新的黑客:为攻击性安全开发代理

::

    Thu, 9 May 2024 18:15:12 GMT
    Leroy Jacob Valencia

In the vast domain of cybersecurity, the transition from reactive defense to offensive has become critical in protecting digital infrastructures. This paper explores the integration of Artificial Intelligence (AI) into offensive cybersecurity, particularly through the development of an autonomous AI agent, ReaperAI, designed to simulate and execute cyberattacks. Leveraging the capabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI demonstrates the potential to identify, exploit, and analyze security vulnerabilities autonomously.
This research outlines the core methodologies that can be utilized to increase consistency and performance, including task-driven penetration testing frameworks, AI-driven command generation, and advanced prompting techniques.
The AI agent operates within a structured environment using Python, enhanced by Retrieval Augmented Generation (RAG) for contextual understanding and memory retention. ReaperAI was tested on platforms including, Hack The Box, where it successfully exploited known vulnerabilities, demonstrating its potential power.
However, the deployment of AI in offensive security presents significant ethical and operational challenges. The agent's development process revealed complexities in command execution, error handling, and maintaining ethical constraints, highlighting areas for future enhancement.
This study contributes to the discussion on AI's role in cybersecurity by showcasing how AI can augment offensive security strategies. It also proposes future research directions, including the refinement of AI interactions with cybersecurity tools, enhancement of learning mechanisms, and the discussion of ethical guidelines for AI in offensive roles. The findings advocate for a unique approach to AI implementation in cybersecurity, emphasizing innovation.

------------

`[2311.09835] ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code <https://arxiv.org/abs/2311.09835>`__ ML-Bench:在库级代码上评估用于机器学习任务的大型语言模型和代理

::

    replaced with revised version Wed, 12 Jun 2024 10:31:57 GMT
    Submission history From: Xiangru Tang [view email]
    [v1] Thu, 16 Nov 2023 12:03:21 UTC (21,189 KB)
    [v2] Wed, 17 Apr 2024 17:13:03 UTC (12,439 KB)
    [v3] Wed, 12 Jun 2024 10:31:57 UTC (6,632 KB)
    Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Liang Chen, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yin Fang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, Mark Gerstein

Despite Large Language Models (LLMs) like GPT-4 achieving impressive results in function-level code generation, they struggle with repository-scale code understanding (e.g., coming up with the right arguments for calling routines), requiring a deeper comprehension of complex file interactions. Also, recently, people have developed LLM agents that attempt to interact with repository code (e.g., compiling and evaluating its execution), prompting the need to evaluate their performance. These gaps have motivated our development of ML-Bench, a benchmark rooted in real-world programming applications that leverage existing code repositories to perform tasks. Addressing the need for LLMs to interpret long code contexts and translate instructions into precise, executable scripts, ML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories, challenging LLMs to accommodate user-specified arguments and documentation intricacies effectively. To evaluate both LLMs and AI agents, two setups are employed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a predefined deployment environment, and ML-Agent-Bench for testing autonomous agents in an end-to-end task execution within a Linux sandbox environment. Our findings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%, there remains significant scope for improvement, highlighted by issues such as hallucinated outputs and difficulties with bash script generation. Notably, in the more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate, reflecting the efficacy of iterative action and feedback in complex task resolution.

------------

`[2405.20267] Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions <https://arxiv.org/abs/2405.20267>`__ LLM的自动竞技场:通过代理peer -battle和委员会讨论实现LLM评估的自动化

::

    replaced with revised version Wed, 12 Jun 2024 15:53:49 GMT
    Submission history From: Ruochen Zhao [view email]
    [v1] Thu, 30 May 2024 17:19:19 UTC (1,982 KB)
    [v2] Thu, 6 Jun 2024 11:36:09 UTC (2,134 KB)
    [v3] Wed, 12 Jun 2024 15:53:49 UTC (2,134 KB)
    Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Deli Zhao, Lidong Bing

As LLMs evolve on a daily basis, there is an urgent need for a trustworthy evaluation method that can provide robust evaluation results in a timely fashion. Currently, as static benchmarks are prone to contamination concerns, users tend to trust human voting platforms, such as Chatbot Arena. However, human annotations require extensive manual efforts. To provide an automatic, robust, and trustworthy evaluation framework, we innovatively propose the Auto-Arena of LLMs, which automates the entire evaluation process with LLM agents. Firstly, an examiner LLM devises queries. Then, a pair of candidate LLMs engage in a multi-round peer-battle around the query, during which the LLM's true performance gaps become visible. Finally, a committee of LLM judges collectively discuss and determine the winner, which alleviates bias and promotes fairness. In our extensive experiment on the 17 newest LLMs, Auto-Arena shows the highest correlation with human preferences, providing a promising alternative to human evaluation platforms.

------------

`[2406.06910] Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large Language Models <https://arxiv.org/abs/2406.06910>`__ Agent-SiMT:基于大型语言模型的agent辅助同传机器翻译

::

    replaced with revised version Wed, 12 Jun 2024 15:05:40 GMT
    Submission history From: Shoutao Guo [view email]
    [v1] Tue, 11 Jun 2024 03:09:20 UTC (573 KB)
    [v2] Wed, 12 Jun 2024 15:05:40 UTC (573 KB)
    Shoutao Guo, Shaolei Zhang, Zhengrui Ma, Min Zhang, Yang Feng

Simultaneous Machine Translation (SiMT) generates target translations while reading the source sentence. It relies on a policy to determine the optimal timing for reading sentences and generating translations. Existing SiMT methods generally adopt the traditional Transformer architecture, which concurrently determines the policy and generates translations. While they excel at determining policies, their translation performance is suboptimal. Conversely, Large Language Models (LLMs), trained on extensive corpora, possess superior generation capabilities, but it is difficult for them to acquire translation policy through the training methods of SiMT. Therefore, we introduce Agent-SiMT, a framework combining the strengths of LLMs and traditional SiMT methods. Agent-SiMT contains the policy-decision agent and the translation agent. The policy-decision agent is managed by a SiMT model, which determines the translation policy using partial source sentence and translation. The translation agent, leveraging an LLM, generates translation based on the partial source sentence. The two agents collaborate to accomplish SiMT. Experiments demonstrate that Agent-SiMT attains state-of-the-art performance.

------------

----------
Other (84)
----------

`[2406.07573] Investigating the Potential of Using Large Language Models for Scheduling <https://arxiv.org/abs/2406.07573>`__ 研究使用大型语言模型进行调度的潜力

::

    Tue, 4 Jun 2024 08:56:56 GMT
    Deddy Jobson, Yilin Li

The inaugural ACM International Conference on AI-powered Software introduced the AIware Challenge, prompting researchers to explore AI-driven tools for optimizing conference programs through constrained optimization. We investigate the use of Large Language Models (LLMs) for program scheduling, focusing on zero-shot learning and integer programming to measure paper similarity. Our study reveals that LLMs, even under zero-shot settings, create reasonably good first drafts of conference schedules. When clustering papers, using only titles as LLM inputs produces results closer to human categorization than using titles and abstracts with TFIDF. The code has been made publicly available.

------------

`[2406.07962] Toward a Method to Generate Capability Ontologies from Natural Language Descriptions <https://arxiv.org/abs/2406.07962>`__ 研究了一种从自然语言描述生成能力本体的方法

::

    Wed, 12 Jun 2024 07:41:44 GMT
    Luis Miguel Vieira da Silva, Aljosha K\"ocher, Felix Gehlhoff, Alexander Fay

To achieve a flexible and adaptable system, capability ontologies are increasingly leveraged to describe functions in a machine-interpretable way.
However, modeling such complex ontological descriptions is still a manual and error-prone task that requires a significant amount of effort and ontology expertise. This contribution presents an innovative method to automate capability ontology modeling using Large Language Models (LLMs), which have proven to be well suited for such tasks. Our approach requires only a natural language description of a capability, which is then automatically inserted into a predefined prompt using a few-shot prompting technique. After prompting an LLM, the resulting capability ontology is automatically verified through various steps in a loop with the LLM to check the overall correctness of the capability ontology. First, a syntax check is performed, then a check for contradictions, and finally a check for hallucinations and missing ontology elements. Our method greatly reduces manual effort, as only the initial natural language description and a final human review and possible correction are necessary, thereby streamlining the capability ontology generation process.

------------

`[2406.08223] Research Trends for the Interplay between Large Language Models and Knowledge Graphs <https://arxiv.org/abs/2406.08223>`__ 大型语言模型与知识图谱相互作用的研究趋势

::

    Wed, 12 Jun 2024 13:52:38 GMT
    Hanieh Khorashadizadeh, Fatima Zahra Amara, Morteza Ezzabady, Fr\'ed\'eric Ieng, Sanju Tiwari, Nandana Mihindukulasooriya, Jinghua Groppe, Soror Sahri, Farah Benamara, Sven Groppe

This survey investigates the synergistic relationship between Large Language Models (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's capabilities in understanding, reasoning, and language processing. It aims to address gaps in current research by exploring areas such as KG Question Answering, ontology generation, KG validation, and the enhancement of KG accuracy and consistency through LLMs. The paper further examines the roles of LLMs in generating descriptive texts and natural language queries for KGs.
Through a structured analysis that includes categorizing LLM-KG interactions, examining methodologies, and investigating collaborative uses and potential biases, this study seeks to provide new insights into the combined potential of LLMs and KGs. It highlights the importance of their interaction for improving AI applications and outlines future research directions.

------------

`[2406.07685] Out-Of-Context Prompting Boosts Fairness and Robustness in Large Language Model Predictions <https://arxiv.org/abs/2406.07685>`__ 

::

    Tue, 11 Jun 2024 20:05:15 GMT
    Leonardo Cotta and Chris J. Maddison

Frontier Large Language Models (LLMs) are increasingly being deployed for high-stakes decision-making. On the other hand, these models are still consistently making predictions that contradict users' or society's expectations, e.g., hallucinating, or discriminating. Thus, it is important that we develop test-time strategies to improve their trustworthiness. Inspired by prior work, we leverage causality as a tool to formally encode two aspects of trustworthiness in LLMs: fairness and robustness. Under this perspective, existing test-time solutions explicitly instructing the model to be fair or robust implicitly depend on the LLM's causal reasoning capabilities. In this work, we explore the opposite approach. Instead of explicitly asking the LLM for trustworthiness, we design prompts to encode the underlying causal inference algorithm that will, by construction, result in more trustworthy predictions. Concretely, we propose out-of-context prompting as a test-time solution to encourage fairness and robustness in LLMs. Out-of-context prompting leverages the user's prior knowledge of the task's causal model to apply (random) counterfactual transformations and improve the model's trustworthiness. Empirically, we show that out-of-context prompting consistently improves the fairness and robustness of frontier LLMs across five different benchmark datasets without requiring additional data, finetuning or pre-training.

------------

`[2406.07735] REAL Sampling: Boosting Factuality and Diversity of Open-Ended Generation via Asymptotic Entropy <https://arxiv.org/abs/2406.07735>`__ 真实采样:基于渐近熵增强开放式生成的事实性和多样性

::

    Tue, 11 Jun 2024 21:44:49 GMT
    Haw-Shiuan Chang, Nanyun Peng, Mohit Bansal, Anil Ramakrishna, Tagyoung Chung

Decoding methods for large language models (LLMs) usually struggle with the tradeoff between ensuring factuality and maintaining diversity. For example, a higher p threshold in the nucleus (top-p) sampling increases the diversity but decreases the factuality, and vice versa. In this paper, we propose REAL (Residual Entropy from Asymptotic Line) sampling, a decoding method that achieves improved factuality and diversity over nucleus sampling by predicting an adaptive threshold of $p$. Specifically, REAL sampling predicts the step-wise likelihood of an LLM to hallucinate, and lowers the p threshold when an LLM is likely to hallucinate. Otherwise, REAL sampling increases the p threshold to boost the diversity. To predict the step-wise hallucination likelihood without supervision, we construct a Token-level Hallucination Forecasting (THF) model to predict the asymptotic entropy (i.e., inherent uncertainty) of the next token by extrapolating the next-token entropies from a series of LLMs with different sizes. If a LLM's entropy is higher than the asymptotic entropy (i.e., the LLM is more uncertain than it should be), the THF model predicts a high hallucination hazard, which leads to a lower p threshold in REAL sampling. In the FactualityPrompts benchmark, we demonstrate that REAL sampling based on a 70M THF model can substantially improve the factuality and diversity of 7B LLMs simultaneously, judged by both retrieval-based metrics and human evaluation. After combined with contrastive decoding, REAL sampling outperforms 9 sampling methods, and generates texts that are more factual than the greedy sampling and more diverse than the nucleus sampling with $p=0.5$.
Furthermore, the predicted asymptotic entropy is also a useful unsupervised signal for hallucination detection tasks.

------------

`[2406.07739] UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback <https://arxiv.org/abs/2406.07739>`__ UICoder:通过自动反馈对大型语言模型进行微调以生成用户界面代码

::

    Tue, 11 Jun 2024 21:53:46 GMT
    Jason Wu and Eldon Schoop and Alan Leung and Titus Barik and Jeffrey P. Bigham and Jeffrey Nichols

Large language models (LLMs) struggle to consistently generate UI code that compiles and produces visually relevant designs. Existing approaches to improve generation rely on expensive human feedback or distilling a proprietary model.
In this paper, we explore the use of automated feedback (compilers and multi-modal models) to guide LLMs to generate high-quality UI code. Our method starts with an existing LLM and iteratively produces improved models by self-generating a large synthetic dataset using an original model, applying automated tools to aggressively filter, score, and de-duplicate the data into a refined higher quality dataset. The original LLM is improved by finetuning on this refined dataset. We applied our approach to several open-source LLMs and compared the resulting performance to baseline models with both automated metrics and human preferences. Our evaluation shows the resulting models outperform all other downloadable baselines and approach the performance of larger proprietary models.

------------

`[2406.07791] Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs <https://arxiv.org/abs/2406.07791>`__ 评判法官:LLMs两两比较评估中的立场偏差的系统调查

::

    Wed, 12 Jun 2024 01:12:28 GMT
    Lin Shi, Weicheng Ma, Soroush Vosoughi

LLM-as-a-Judge offers a promising alternative to human judges across various tasks, yet inherent biases, particularly position bias - a systematic preference for answers based on their position in the prompt - compromise its effectiveness. Our study investigates this issue by developing a framework to systematically study and quantify position bias using metrics such as repetitional consistency, positional consistency, and positional fairness. We conduct experiments with 9 judge models across 22 tasks from the MTBench and DevBench benchmarks and nearly 40 answer-generating models, generating approximately 80,000 evaluation instances. This comprehensive assessment reveals significant variations in bias across judges and tasks. Although GPT-4 often excels in positional consistency and fairness, some more cost-effective models perform comparably or even better in specific tasks, highlighting essential trade-offs between consistency, fairness, and cost. Our results also demonstrate high consistency of judgment across repetitions, confirming that position bias is not due to random variations. This research significantly contributes to the field by introducing new concepts for understanding position bias and providing a multi-dimensional framework for evaluation. These insights guide the selection of optimal judge models, enhance benchmark design, and lay the foundation for future research into effective debiasing strategies, ultimately enhancing the reliability of LLM evaluators.

------------

`[2406.07794] IndirectRequests: Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests <https://arxiv.org/abs/2406.07794>`__ IndirectRequests:通过合成间接用户请求使面向任务的对话数据集更加自然

::

    Wed, 12 Jun 2024 01:18:04 GMT
    Amogh Mannekote, Jinseok Nam, Ziming Li, Jian Gao, Kristy Elizabeth Boyer, Bonnie J. Dorr

Existing benchmark corpora of task-oriented dialogue are collected either using a "machines talking to machines" approach or by giving template-based goal descriptions to crowdworkers. These methods, however, often produce utterances that are markedly different from natural human conversations in which people often convey their preferences in indirect ways, such as through small talk. We term such utterances as Indirect User Requests (IURs).
Understanding such utterances demands considerable world knowledge and reasoning capabilities on the listener's part. Our study introduces an LLM-based pipeline to automatically generate realistic, high-quality IURs for a given domain, with the ultimate goal of supporting research in natural language understanding (NLU) and dialogue state tracking (DST) for task-oriented dialogue systems. Our findings show that while large LLMs such as GPT-3.5 and GPT-4 generate high-quality IURs, achieving similar quality with smaller models is more challenging. We release IndirectRequests, a dataset of IURs that advances beyond the initial Schema-Guided Dialog (SGD) dataset in that it provides a challenging testbed for testing the "in the wild" performance of NLU and DST models.

------------

`[2406.07815] Are Large Language Models Good Statisticians? <https://arxiv.org/abs/2406.07815>`__ 大型语言模型是优秀的统计学家吗?

::

    Wed, 12 Jun 2024 02:23:51 GMT
    Yizhang Zhu, Shiyin Du, Boyan Li, Yuyu Luo, Nan Tang

Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics, physics, and chemistry.
Despite their successes, the effectiveness of LLMs in handling complex statistical tasks remains systematically under-explored. To bridge this gap, we introduce StatQA, a new benchmark designed for statistical analysis tasks.
StatQA comprises 11,623 examples tailored to evaluate LLMs' proficiency in specialized statistical tasks and their applicability assessment capabilities, particularly for hypothesis testing methods. We systematically experiment with representative LLMs using various prompting strategies and show that even state-of-the-art models such as GPT-4o achieve a best performance of only 64.83%, indicating significant room for improvement. Notably, while open-source LLMs (e.g. LLaMA-3) show limited capability, those fine-tuned ones exhibit marked improvements, outperforming all in-context learning-based methods (e.g.
GPT-4o). Moreover, our comparative human experiments highlight a striking contrast in error types between LLMs and humans: LLMs primarily make applicability errors, whereas humans mostly make statistical task confusion errors. This divergence highlights distinct areas of proficiency and deficiency, suggesting that combining LLM and human expertise could lead to complementary strengths, inviting further investigation into their collaborative potential.

------------

`[2406.07835] SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature <https://arxiv.org/abs/2406.07835>`__ SciRIFF:增强科学文献语言模型指导遵循的资源

::

    Mon, 10 Jun 2024 21:22:08 GMT
    David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, Arman Cohan

We present SciRIFF (Scientific Resource for Instruction-Following and Finetuning), a dataset of 137K instruction-following demonstrations for 54 tasks covering five essential scientific literature understanding capabilities: information extraction, summarization, question answering, claim verification, and classification. SciRIFF demonstrations are notable for their long input contexts, detailed task specifications, and complex structured outputs. While instruction-following resources are available in specific domains such as clinical medicine and chemistry, SciRIFF is the first dataset focused on extracting and synthesizing information from research literature across a wide range of scientific fields. To demonstrate the utility of SciRIFF, we develop a sample-efficient strategy to adapt a general instruction-following model for science by performing additional finetuning on a mix of general-domain and SciRIFF demonstrations. In evaluations on nine held-out scientific tasks, our model -- called SciTulu -- improves over a strong LLM baseline by 28.1% and 6.5% at the 7B and 70B scales respectively, while maintaining general instruction-following performance within 2% of the baseline. We are optimistic that SciRIFF will facilitate the development and evaluation of LLMs to help researchers navigate the ever-growing body of scientific literature. We release our dataset, model checkpoints, and data processing and evaluation code to enable further research.

------------

`[2406.07882] Designing a Dashboard for Transparency and Control of Conversational AI <https://arxiv.org/abs/2406.07882>`__ 为对话式AI的透明度和控制设计仪表板

::

    Wed, 12 Jun 2024 05:20:16 GMT
    Yida Chen, Aoyu Wu, Trevor DePodesta, Catherine Yeh, Kenneth Li, Nicholas Castillo Marin, Oam Patel, Jan Riecke, Shivam Raval, Olivia Seow, Martin Wattenberg, Fernanda Vi\'egas

Conversational LLMs function as black box systems, leaving users guessing about why they see the output they do. This lack of transparency is potentially problematic, especially given concerns around bias and truthfulness. To address this issue, we present an end-to-end prototype-connecting interpretability techniques with user experience design-that seeks to make chatbots more transparent. We begin by showing evidence that a prominent open-source LLM has a "user model": examining the internal state of the system, we can extract data related to a user's age, gender, educational level, and socioeconomic status.
Next, we describe the design of a dashboard that accompanies the chatbot interface, displaying this user model in real time. The dashboard can also be used to control the user model and the system's behavior. Finally, we discuss a study in which users conversed with the instrumented system. Our results suggest that users appreciate seeing internal states, which helped them expose biased behavior and increased their sense of control. Participants also made valuable suggestions that point to future directions for both design and machine learning research. The project page and video demo of our TalkTuner system are available at https://bit.ly/talktuner-project-page

------------

`[2406.07933] Large Language Model Unlearning via Embedding-Corrupted Prompts <https://arxiv.org/abs/2406.07933>`__ 基于嵌入损坏提示的大型语言模型遗忘

::

    Wed, 12 Jun 2024 06:56:20 GMT
    Chris Yuhao Liu, Yaxuan Wang, Jeffrey Flanigan, Yang Liu

Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains. Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use. However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across state-of-the-art models with hundreds of billions of parameters. In this work, we present Embedding-COrrupted (ECO) Prompts, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency. Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget. We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference. We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting. Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at nearly zero side effects in general domains and domains closely related to the unlearned ones. Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases.

------------

`[2406.07935] Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation <https://arxiv.org/abs/2406.07935>`__ 人工评估指南中的脆弱性定义和检测:可靠NLG评估的初步研究

::

    Wed, 12 Jun 2024 06:59:31 GMT
    Jie Ruan, Wenqing Wang, Xiaojun Wan

Human evaluation serves as the gold standard for assessing the quality of Natural Language Generation (NLG) systems. Nevertheless, the evaluation guideline, as a pivotal element ensuring reliable and reproducible human assessment, has received limited attention.Our investigation revealed that only 29.84% of recent papers involving human evaluation at top conferences release their evaluation guidelines, with vulnerabilities identified in 77.09% of these guidelines. Unreliable evaluation guidelines can yield inaccurate assessment outcomes, potentially impeding the advancement of NLG in the right direction.
To address these challenges, we take an initial step towards reliable evaluation guidelines and propose the first human evaluation guideline dataset by collecting annotations of guidelines extracted from existing papers as well as generated via Large Language Models (LLMs). We then introduce a taxonomy of eight vulnerabilities and formulate a principle for composing evaluation guidelines. Furthermore, a method for detecting guideline vulnerabilities has been explored using LLMs, and we offer a set of recommendations to enhance reliability in human evaluation. The annotated human evaluation guideline dataset and code for the vulnerability detection method are publicly available online.

------------

`[2406.08050] Adversarial Evasion Attack Efficiency against Large Language Models <https://arxiv.org/abs/2406.08050>`__ 针对大型语言模型的对抗规避攻击效率

::

    Wed, 12 Jun 2024 10:02:27 GMT
    Jo\~ao Vitorino, Eva Maia, Isabel Pra\c{c}a

Large Language Models (LLMs) are valuable for text classification, but their vulnerabilities must not be disregarded. They lack robustness against adversarial examples, so it is pertinent to understand the impacts of different types of perturbations, and assess if those attacks could be replicated by common users with a small amount of perturbations and a small number of queries to a deployed LLM. This work presents an analysis of the effectiveness, efficiency, and practicality of three different types of adversarial attacks against five different LLMs in a sentiment classification task. The obtained results demonstrated the very distinct impacts of the word-level and character-level attacks. The word attacks were more effective, but the character and more constrained attacks were more practical and required a reduced number of perturbations and queries. These differences need to be considered during the development of adversarial defense strategies to train more robust LLMs for intelligent text classification applications.

------------

`[2406.08080] AustroTox: A Dataset for Target-Based Austrian German Offensive Language Detection <https://arxiv.org/abs/2406.08080>`__ AustroTox:基于目标的奥地利德语攻击性语言检测数据集

::

    Wed, 12 Jun 2024 11:04:11 GMT
    Pia Pachinger, Janis Goldzycher, Anna Maria Planitzer, Wojciech Kusa, Allan Hanbury, Julia Neidhardt

Model interpretability in toxicity detection greatly profits from token-level annotations. However, currently such annotations are only available in English.
We introduce a dataset annotated for offensive language detection sourced from a news forum, notable for its incorporation of the Austrian German dialect, comprising 4,562 user comments. In addition to binary offensiveness classification, we identify spans within each comment constituting vulgar language or representing targets of offensive statements. We evaluate fine-tuned language models as well as large language models in a zero- and few-shot fashion. The results indicate that while fine-tuned models excel in detecting linguistic peculiarities such as vulgar dialect, large language models demonstrate superior performance in detecting offensiveness in AustroTox. We publish the data and code.

------------

`[2406.08100] Multimodal Table Understanding <https://arxiv.org/abs/2406.08100>`__ 

::

    Wed, 12 Jun 2024 11:27:03 GMT
    Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, Weiping Wang

Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications. In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various table-related requests based on the given table image. To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks. On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings. The code and data is available at this https://github.com/SpursGoZmy/Table-LLaVA

------------

`[2406.08101] CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems <https://arxiv.org/abs/2406.08101>`__ CoXQL:会话XAI系统中解析解释请求的数据集

::

    Wed, 12 Jun 2024 11:27:10 GMT
    Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian M\"oller

Conversational explainable artificial intelligence (ConvXAI) systems based on large language models (LLMs) have garnered significant interest from the research community in natural language processing (NLP) and human-computer interaction (HCI). Such systems can provide answers to user questions about explanations, have the potential to enhance users' comprehension and offer more information about the decision-making and generation processes of LLMs.
Currently available ConvXAI systems are based on intent recognition rather than free chat. Thus, reliably grasping users' intentions in ConvXAI systems still presents a challenge, because there is a broad range of XAI methods to map requests onto and each of them can have multiple slots to take care of. In order to bridge this gap, we present CoXQL, the first dataset for user intent recognition in ConvXAI, covering 31 intents, seven of which require filling additional slots. Subsequently, we enhance an existing parsing approach by incorporating template validations, and conduct an evaluation of several LLMs on CoXQL using different parsing strategies. We conclude that the improved parsing approach (MP+) surpasses the performance of previous approaches. We also discover that intents with multiple slots remain highly challenging for LLMs.

------------

`[2406.08183] Underneath the Numbers: Quantitative and Qualitative Gender Fairness in LLMs for Depression Prediction <https://arxiv.org/abs/2406.08183>`__ 数字背后:用于抑郁症预测的llm中的数量和质量性别公平性

::

    Wed, 12 Jun 2024 13:14:19 GMT
    Micol Spitale, Jiaee Cheong, Hatice Gunes

Recent studies show bias in many machine learning models for depression detection, but bias in LLMs for this task remains unexplored. This work presents the first attempt to investigate the degree of gender bias present in existing LLMs (ChatGPT, LLaMA 2, and Bard) using both quantitative and qualitative approaches. From our quantitative evaluation, we found that ChatGPT performs the best across various performance metrics and LLaMA 2 outperforms other LLMs in terms of group fairness metrics. As qualitative fairness evaluation remains an open research question we propose several strategies (e.g., word count, thematic analysis) to investigate whether and how a qualitative evaluation can provide valuable insights for bias analysis beyond what is possible with quantitative evaluation. We found that ChatGPT consistently provides a more comprehensive, well-reasoned explanation for its prediction compared to LLaMA 2. We have also identified several themes adopted by LLMs to qualitatively evaluate gender fairness. We hope our results can be used as a stepping stone towards future attempts at improving qualitative evaluation of fairness for LLMs especially for high-stakes tasks such as depression detection.

------------

`[2406.08202] A Dialogue Game for Eliciting Balanced Collaboration <https://arxiv.org/abs/2406.08202>`__ 促进平衡合作的对话游戏

::

    Wed, 12 Jun 2024 13:35:10 GMT
    Isidora Jekni\'c, David Schlangen, Alexander Koller

Collaboration is an integral part of human dialogue. Typical task-oriented dialogue games assign asymmetric roles to the participants, which limits their ability to elicit naturalistic role-taking in collaboration and its negotiation. We present a novel and simple online setup that favors balanced collaboration: a two-player 2D object placement game in which the players must negotiate the goal state themselves. We show empirically that human players exhibit a variety of role distributions, and that balanced collaboration improves task performance. We also present an LLM-based baseline agent which demonstrates that automatic playing of our game is an interesting challenge for artificial systems.

------------

`[2406.08316] Is Programming by Example solved by LLMs? <https://arxiv.org/abs/2406.08316>`__ llm能解决实例编程吗?

::

    Wed, 12 Jun 2024 15:16:40 GMT
    Wen-Ding Li, Kevin Ellis

Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have `solved' PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.

------------

`[2406.08398] cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers <https://arxiv.org/abs/2406.08398>`__ 论文:科学论文中的情境和多模态交互对话数据集

::

    Wed, 12 Jun 2024 16:46:12 GMT
    Anirudh Sundar, Jin Xu, William Gay, Christopher Richardson, Larry Heck

An emerging area of research in situated and multimodal interactive conversations (SIMMC) includes interactions in scientific papers. Since scientific papers are primarily composed of text, equations, figures, and tables, SIMMC methods must be developed specifically for each component to support the depth of inquiry and interactions required by research scientists.
This work introduces Conversational Papers (cPAPERS), a dataset of conversational question-answer pairs from reviews of academic papers grounded in these paper components and their associated references from scientific documents available on arXiv. We present a data collection strategy to collect these question-answer pairs from OpenReview and associate them with contextual information from LaTeX source files. Additionally, we present a series of baseline approaches utilizing Large Language Models (LLMs) in both zero-shot and fine-tuned configurations to address the cPAPERS dataset.

------------

`[2406.08434] TasTe: Teaching Large Language Models to Translate through Self-Reflection <https://arxiv.org/abs/2406.08434>`__ TasTe:通过自我反思教会大型语言模型进行翻译

::

    Wed, 12 Jun 2024 17:21:21 GMT
    Yutong Wang, Jiali Zeng, Xuebo Liu, Fandong Meng, Jie Zhou, Min Zhang

Large language models (LLMs) have exhibited remarkable performance in various natural language processing tasks. Techniques like instruction tuning have effectively enhanced the proficiency of LLMs in the downstream task of machine translation. However, the existing approaches fail to yield satisfactory translation outputs that match the quality of supervised neural machine translation (NMT) systems. One plausible explanation for this discrepancy is that the straightforward prompts employed in these methodologies are unable to fully exploit the acquired instruction-following capabilities. To this end, we propose the TasTe framework, which stands for translating through self-reflection. The self-reflection process includes two stages of inference.
In the first stage, LLMs are instructed to generate preliminary translations and conduct self-assessments on these translations simultaneously. In the second stage, LLMs are tasked to refine these preliminary translations according to the evaluation results. The evaluation results in four language directions on the WMT22 benchmark reveal the effectiveness of our approach compared to existing methods. Our work presents a promising approach to unleash the potential of LLMs and enhance their capabilities in MT. The codes and datasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT.

------------

`[2406.08446] OLMES: A Standard for Language Model Evaluations <https://arxiv.org/abs/2406.08446>`__ OLMES:语言模型评估标准

::

    Wed, 12 Jun 2024 17:37:09 GMT
    Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, Hannaneh Hajishirzi

Progress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities. Evaluating language models in particular is challenging, as small changes to how a model is evaluated on a task can lead to large changes in measured performance. There is no common standard setup, so different models are evaluated on the same tasks in different ways, leading to claims about which models perform best not being reproducible. We propose OLMES, a completely documented, practical, open standard for reproducible LLM evaluations. In developing this standard, we identify and review the varying factors in evaluation practices adopted by the community - such as details of prompt formatting, choice of in-context examples, probability normalizations, and task formulation. In particular, OLMES supports meaningful comparisons between smaller base models that require the unnatural "cloze" formulation of multiple-choice questions against larger models that can utilize the original formulation. OLMES includes well-considered recommendations guided by results from existing literature as well as new experiments investigating open questions.

------------

`[2406.08464] Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing <https://arxiv.org/abs/2406.08464>`__ Magpie:通过提示对齐的llm，从头开始对齐数据合成

::

    Wed, 12 Jun 2024 17:52:30 GMT
    Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, Bill Yuchen Lin

High-quality instruction data is critical for aligning large language models (LLMs). Although some models, such as Llama-3-Instruct, have open weights, their alignment data remain private, which hinders the democratization of AI.
High human labor costs and a limited, predefined scope for prompting prevent existing open-source data creation methods from scaling effectively, potentially limiting the diversity and quality of public alignment datasets. Is it possible to synthesize high-quality instruction data at scale by extracting it directly from an aligned LLM? We present a self-synthesis method for generating large-scale alignment data named Magpie. Our key observation is that aligned LLMs like Llama-3-Instruct can generate a user query when we input only the left-side templates up to the position reserved for user messages, thanks to their auto-regressive nature. We use this method to prompt Llama-3-Instruct and generate 4 million instructions along with their corresponding responses.
We perform a comprehensive analysis of the extracted data and select 300K high-quality instances. To compare Magpie data with other public instruction datasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the performance of the fine-tuned models. Our results indicate that in some tasks, models fine-tuned with Magpie perform comparably to the official Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data points through supervised fine-tuning (SFT) and subsequent feedback learning.
We also show that using Magpie solely for SFT can surpass the performance of previous public datasets utilized for both SFT and preference optimization, such as direct preference optimization with UltraFeedback. This advantage is evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.

------------

`[2406.07777] Unifying Interpretability and Explainability for Alzheimer's Disease Progression Prediction <https://arxiv.org/abs/2406.07777>`__ 阿尔茨海默病进展预测可解释性和可解释性的统一

::

    Tue, 11 Jun 2024 23:54:42 GMT
    Raja Farrukh Ali and Stephanie Milani and John Woods and Emmanuel Adenij and Ayesha Farooq and Clayton Mansel and Jeffrey Burns and William Hsu

Reinforcement learning (RL) has recently shown promise in predicting Alzheimer's disease (AD) progression due to its unique ability to model domain knowledge. However, it is not clear which RL algorithms are well-suited for this task. Furthermore, these methods are not inherently explainable, limiting their applicability in real-world clinical scenarios. Our work addresses these two important questions. Using a causal, interpretable model of AD, we first compare the performance of four contemporary RL algorithms in predicting brain cognition over 10 years using only baseline (year 0) data. We then apply SHAP (SHapley Additive exPlanations) to explain the decisions made by each algorithm in the model. Our approach combines interpretability with explainability to provide insights into the key factors influencing AD progression, offering both global and individual, patient-level analysis. Our findings show that only one of the RL methods is able to satisfactorily model disease progression, but the post-hoc explanations indicate that all methods fail to properly capture the importance of amyloid accumulation, one of the pathological hallmarks of Alzheimer's disease. Our work aims to merge predictive accuracy with transparency, assisting clinicians and researchers in enhancing disease progression modeling for informed healthcare decisions. Code is available at https://github.com/rfali/xrlad.

------------

`[2406.07780] A Critical Look At Tokenwise Reward-Guided Text Generation <https://arxiv.org/abs/2406.07780>`__ 分词奖励引导文本生成的批判性审视

::

    Wed, 12 Jun 2024 00:19:40 GMT
    Ahmad Rashid, Ruotian Wu, Julia Grosse, Agustinus Kristiadi and Pascal Poupart

Large language models (LLMs) can significantly be improved by aligning to human preferences -- the so-called reinforcement learning from human feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive for many users.
Due to their ability to bypass LLM finetuning, tokenwise reward-guided text generation (RGTG) methods have recently been proposed. They use a reward model trained on full sequences to score partial sequences during a tokenwise decoding, in a bid to steer the generation towards sequences with high rewards.
However, these methods have so far been only heuristically motivated and poorly analyzed. In this work, we show that reward models trained on full sequences are not compatible with scoring partial sequences. To alleviate this issue, we propose to explicitly train a Bradley-Terry reward model on partial sequences, and autoregressively sample from the implied tokenwise policy during decoding time. We study the property of this reward model and the implied policy. In particular, we show that this policy is proportional to the ratio of two distinct RLHF policies. We show that our simple approach outperforms previous RGTG methods and achieves similar performance as strong offline baselines but without large-scale LLM finetuning.

------------

`[2406.07831] ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models <https://arxiv.org/abs/2406.07831>`__ ALPS:大型语言模型高稀疏单次剪枝的改进优化

::

    Wed, 12 Jun 2024 02:57:41 GMT
    Xiang Meng, Kayhan Behdin, Haoyue Wang, Rahul Mazumder

The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the OPT-30B model with 70% sparsity, ALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a 19% improvement in zero-shot benchmark performance compared to existing methods.

------------

`[2406.07904] Grounding Multimodal Large Language Models in Actions <https://arxiv.org/abs/2406.07904>`__ 多模态大型语言模型的行动基础

::

    Wed, 12 Jun 2024 06:12:04 GMT
    Andrew Szot, Bogdan Mazoure, Harsh Agrawal, Devon Hjelm, Zsolt Kira, Alexander Toshev

Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains, including Embodied AI. In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, with the goal of leveraging the multimodal world knowledge of the MLLM. We first generalize a number of methods through a unified architecture and the lens of action space adaptors. For continuous actions, we show that a learned tokenization allows for sufficient modeling precision, yielding the best performance on downstream tasks. For discrete actions, we demonstrate that semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance. We arrive at these lessons via a thorough study of seven action space adapters on five different environments, encompassing over 114 embodied tasks.

------------

`[2406.08074] A Concept-Based Explainability Framework for Large Multimodal Models <https://arxiv.org/abs/2406.08074>`__ 

::

    Wed, 12 Jun 2024 10:48:53 GMT
    Jayneel Parekh, Pegah Khayatan, Mustafa Shukor, Alasdair Newson, Matthieu Cord

Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks. Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery. In this paper, we present a novel framework for the interpretation of LMMs. We propose a dictionary learning based approach, applied to the representation of tokens. The elements of the learned dictionary correspond to our proposed concepts. We show that these concepts are well semantically grounded in both vision and text. Thus we refer to these as "multi-modal concepts". We qualitatively and quantitatively evaluate the results of the learnt concepts. We show that the extracted multimodal concepts are useful to interpret representations of test samples. Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually. We will publicly release our code.

------------

`[2406.08391] Large Language Models Must Be Taught to Know What They Don't Know <https://arxiv.org/abs/2406.08391>`__ 必须教会大型语言模型知道它们不知道的东西

::

    Wed, 12 Jun 2024 16:41:31 GMT
    Sanyam Kapoor, Nate Gruver, Manley Roberts, Katherine Collins, Arka Pal, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, Andrew Gordon Wilson

When using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions. Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive. In this work, we first argue that prompting on its own is insufficient to achieve good calibration and then show that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead. We show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA. We also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models. Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study.

------------

`[2406.08414] Discovering Preference Optimization Algorithms with and for Large Language Models <https://arxiv.org/abs/2406.08414>`__ 大型语言模型偏好优化算法的发现

::

    Wed, 12 Jun 2024 16:58:41 GMT
    Chris Lu, Samuel Holt, Claudio Fanconi, Alex J. Chan, Jakob Foerster, Mihaela van der Schaar, Robert Tjarko Lange

Offline preference optimization is a key method for enhancing and controlling the quality of Large Language Model (LLM) outputs. Typically, preference optimization is approached as an offline supervised learning task using manually-crafted convex loss functions. While these methods are based on theoretical insights, they are inherently constrained by human creativity, so the large search space of possible loss functions remains under explored. We address this by performing LLM-driven objective discovery to automatically discover new state-of-the-art preference optimization algorithms without (expert) human intervention. Specifically, we iteratively prompt an LLM to propose and implement new preference optimization loss functions based on previously-evaluated performance metrics. This process leads to the discovery of previously-unknown and performant preference optimization algorithms. The best performing of these we call Discovered Preference Optimization (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses.
Experiments demonstrate the state-of-the-art performance of DiscoPOP and its successful transfer to held-out tasks.

------------

`[2406.08447] The Impact of Initialization on LoRA Finetuning Dynamics <https://arxiv.org/abs/2406.08447>`__ 初始化对LoRA微调动态的影响

::

    Wed, 12 Jun 2024 17:38:20 GMT
    Soufiane Hayou, Nikhil Ghosh, Bin Yu

In this paper, we study the role of initialization in Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from the pretrained model as initialization for finetuning, one can either initialize B to zero and A to random (default initialization in PEFT package), or vice-versa. In both cases, the product BA is equal to zero at initialization, which makes finetuning starts from the pretrained model. These two initialization schemes are seemingly similar. They should in-principle yield the same performance and share the same optimal learning rate. We demonstrate that this is an incorrect intuition and that the first scheme (initializing B to zero and A to random) on average yields better performance compared to the other scheme. Our theoretical analysis shows that the reason behind this might be that the first initialization allows the use of larger learning rates (without causing output instability) compared to the second initialization, resulting in more efficient learning of the first scheme. We validate our results with extensive experiments on LLMs.

------------

`[2406.07594] MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models <https://arxiv.org/abs/2406.07594>`__ MLLMGuard:多模态大型语言模型多维安全评估套件

::

    Tue, 11 Jun 2024 13:41:33 GMT
    Tianle Gu, Zeyang Zhou, Kexin Huang, Dandan Liang, Yixu Wang, Haiquan Zhao, Yuanqi Yao, Xingge Qiao, Keqing Wang, Yujiu Yang, Yan Teng, Yu Qiao, Yingchun Wang

Powered by remarkable advancements in Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities in manifold tasks. However, the practical application scenarios of MLLMs are intricate, exposing them to potential malicious instructions and thereby posing safety risks. While current benchmarks do incorporate certain safety considerations, they often lack comprehensive coverage and fail to exhibit the necessary rigor and robustness. For instance, the common practice of employing GPT-4V as both the evaluator and a model to be evaluated lacks credibility, as it tends to exhibit a bias toward its own responses. In this paper, we present MLLMGuard, a multidimensional safety evaluation suite for MLLMs, including a bilingual image-text evaluation dataset, inference utilities, and a lightweight evaluator. MLLMGuard's assessment comprehensively covers two languages (English and Chinese) and five important safety dimensions (Privacy, Bias, Toxicity, Truthfulness, and Legality), each with corresponding rich subtasks. Focusing on these dimensions, our evaluation dataset is primarily sourced from platforms such as social media, and it integrates text-based and image-based red teaming techniques with meticulous annotation by human experts. This can prevent inaccurate evaluation caused by data leakage when using open-source datasets and ensures the quality and challenging nature of our benchmark. Additionally, a fully automated lightweight evaluator termed GuardRank is developed, which achieves significantly higher evaluation accuracy than GPT-4. Our evaluation results across 13 advanced models indicate that MLLMs still have a substantial journey ahead before they can be considered safe and responsible.

------------

`[2406.07595] VulDetectBench: Evaluating the Deep Capability of Vulnerability Detection with Large Language Models <https://arxiv.org/abs/2406.07595>`__ VulDetectBench:基于大型语言模型的漏洞检测深度能力评估

::

    Tue, 11 Jun 2024 13:42:57 GMT
    Yu Liu, Mingxin Yang, Yu Xie, Ping Chen, Xiaojin Zhang, Wei Chen

Large Language Models (LLMs) have training corpora containing large amounts of program code, greatly improving the model's code comprehension and generation capabilities. However, sound comprehensive research on detecting program vulnerabilities, a more specific task related to code, and evaluating the performance of LLMs in this more specialized scenario is still lacking. To address common challenges in vulnerability analysis, our study introduces a new benchmark, VulDetectBench, specifically designed to assess the vulnerability detection capabilities of LLMs. The benchmark comprehensively evaluates LLM's ability to identify, classify, and locate vulnerabilities through five tasks of increasing difficulty. We evaluate the performance of 17 models (both open- and closed-source) and find that while existing models can achieve over 80% accuracy on tasks related to vulnerability identification and classification, they still fall short on specific, more detailed vulnerability analysis tasks, with less than 30% accuracy, making it difficult to provide valuable auxiliary information for professional vulnerability mining. Our benchmark effectively evaluates the capabilities of various LLMs at different levels in the specific task of vulnerability detection, providing a foundation for future research and improvements in this critical area of code security. VulDetectBench is publicly available at https://github.com/Sweetaroo/VulDetectBench.

------------

`[2406.07714] LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing <https://arxiv.org/abs/2406.07714>`__ LLAMAFUZZ:大型语言模型增强的灰盒模糊测试

::

    Tue, 11 Jun 2024 20:48:28 GMT
    Hongxiang Zhang and Yuyang Rong and Yifeng He and Hao Chen

Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in programs. However, randomized mutation strategies have limited the fuzzer's performance on structured data. Specialized fuzzers can handle complex structured data, but require additional efforts in grammar and suffer from low throughput.
In this paper, we explore the potential of utilizing the Large Language Model to enhance greybox fuzzing for structured data. We utilize the pre-trained knowledge of LLM about data conversion and format to generate new valid inputs.
We further fine-tuned it with paired mutation seeds to learn structured format and mutation strategies effectively. Our LLM-based fuzzer, LLAMAFUZZ, integrates the power of LLM to understand and mutate structured data to fuzzing. We conduct experiments on the standard bug-based benchmark Magma and a wide variety of real-world programs. LLAMAFUZZ outperforms our top competitor by 41 bugs on average. We also identified 47 unique bugs across all trials.
Moreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and bug reached. Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in real-world program sets on average. We also demonstrate a case study to explain how LLMs enhance the fuzzing process in terms of code coverage.

------------

`[2406.07737] The Future of Software Engineering in an AI-Driven World <https://arxiv.org/abs/2406.07737>`__ 

::

    Tue, 11 Jun 2024 21:46:19 GMT
    Valerio Terragni and Partha Roop and Kelly Blincoe

A paradigm shift is underway in Software Engineering, with AI systems such as LLMs gaining increasing importance for improving software development productivity. This trend is anticipated to persist. In the next five years, we will likely see an increasing symbiotic partnership between human developers and AI. The Software Engineering research community cannot afford to overlook this trend; we must address the key research challenges posed by the integration of AI into the software development process. In this paper, we present our vision of the future of software development in an AI-Driven world and explore the key challenges that our research community should address to realize this vision.

------------

`[2406.07867] Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation <https://arxiv.org/abs/2406.07867>`__ Let's Go Real Talk:面向面对面对话的口语对话模型

::

    Wed, 12 Jun 2024 04:48:36 GMT
    Se Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu Kim, Joanna Hong, Jeong Hun Yeo, Yong Man Ro

In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. Through extensive experiments, we validate the effectiveness of our model in facilitating a face-to-face conversation. Demo and data are available at https://multidialog.github.io and https://huggingface.co/datasets/IVLLab/MultiDialog, respectively.

------------

`[2406.07923] CTC-aligned Audio-Text Embedding for Streaming Open-vocabulary Keyword Spotting <https://arxiv.org/abs/2406.07923>`__ 面向流式开放词汇表关键词检测的ctc对齐音频文本嵌入

::

    Wed, 12 Jun 2024 06:44:40 GMT
    Sichen Jin, Youngmoon Jung, Seungjin Lee, Jaeyoung Roh, Changwoo Han, Hoonyoung Cho

This paper introduces a novel approach for streaming openvocabulary keyword spotting (KWS) with text-based keyword enrollment. For every input frame, the proposed method finds the optimal alignment ending at the frame using connectionist temporal classification (CTC) and aggregates the frame-level acoustic embedding (AE) to obtain higher-level (i.e., character, word, or phrase) AE that aligns with the text embedding (TE) of the target keyword text.
After that, we calculate the similarity of the aggregated AE and the TE. To the best of our knowledge, this is the first attempt to dynamically align the audio and the keyword text on-the-fly to attain the joint audio-text embedding for KWS. Despite operating in a streaming fashion, our approach achieves competitive performance on the LibriPhrase dataset compared to the non-streaming methods with a mere 155K model parameters and a decoding algorithm with time complexity O(U), where U is the length of the target keyword at inference time.

------------

`[2406.07944] DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis <https://arxiv.org/abs/2406.07944>`__ DLLens:通过llm辅助合成测试深度学习库

::

    Wed, 12 Jun 2024 07:06:38 GMT
    Meiziniu Li, Dongze Li, Jianmeng Liu, Jialun Cao, Yongqiang Tian, Shing-Chi Cheung

Testing is a major approach to ensuring the quality of deep learning (DL) libraries. Existing testing techniques commonly adopt differential testing to relieve the need for test oracle construction. However, these techniques are limited in finding implementations that offer the same functionality and generating diverse test inputs for differential testing. This paper introduces DLLens, a novel differential testing technique for DL library testing. Our insight is that APIs in different DL libraries are commonly designed to accomplish various computations for the same set of published DL algorithms.
Although the mapping of these APIs is not often one-to-one, we observe that their computations can be mutually simulated after proper composition and adaptation. The use of these simulation counterparts facilitates differential testing for the detection of functional DL library bugs. Leveraging the insight, we propose DLLens as a novel mechanism that utilizes a large language model (LLM) to synthesize valid counterparts of DL library APIs. To generate diverse test inputs, DLLens incorporates a static analysis method aided by LLM to extract path constraints from all execution paths in each API and its counterpart's implementations. These path constraints are then used to guide the generation of diverse test inputs. We evaluate DLLens on two popular DL libraries, TensorFlow and PyTorch. Our evaluation shows that DLLens can synthesize counterparts for more than twice as many APIs found by state-of-the-art techniques on these libraries. Moreover, DLLens can extract 26.7% more constraints and detect 2.5 times as many bugs as state-of-the-art techniques. DLLens has successfully found 56 bugs in recent TensorFlow and PyTorch libraries. Among them, 41 are previously unknown, 39 of which have been confirmed by developers after reporting, and 19 of those confirmed bugs have been fixed by developers.

------------

`[2406.07954] Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition <https://arxiv.org/abs/2406.07954>`__ 数据集和从2024年SaTML LLM夺旗竞赛中吸取的教训

::

    Wed, 12 Jun 2024 07:27:28 GMT
    Edoardo Debenedetti, Javier Rando, Daniel Paleka, Silaghi Fineas Florin, Dragos Albastroiu, Niv Cohen, Yuval Lemberg, Reshmi Ghosh, Rui Wen, Ahmed Salem, Giovanni Cherubin, Santiago Zanella-Beguelin, Robin Schmid, Victor Klemm, Takahiro Miki, Chenhao Li, Stefan Kraft, Mario Fritz, Florian Tram\`er, Sahar Abdelnabi, Lea Sch\"onherr

Large language model systems face important security risks from maliciously crafted messages that aim to overwrite the system's original instructions or leak private data. To study this problem, we organized a capture-the-flag competition at IEEE SaTML 2024, where the flag is a secret string in the LLM system prompt. The competition was organized in two phases. In the first phase, teams developed defenses to prevent the model from leaking the secret. During the second phase, teams were challenged to extract the secrets hidden for defenses proposed by the other teams. This report summarizes the main insights from the competition. Notably, we found that all defenses were bypassed at least once, highlighting the difficulty of designing a successful defense and the necessity for additional research to protect LLM systems. To foster future research in this direction, we compiled a dataset with over 137k multi-turn attack chats and open-sourced the platform.

------------

`[2406.08112] Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio <https://arxiv.org/abs/2406.08112>`__ Codecfake:用于检测基于llm的Deepfake音频的初始数据集

::

    Wed, 12 Jun 2024 11:47:23 GMT
    Yi Lu, Yuankun Xie, Ruibo Fu, Zhengqi Wen, Jianhua Tao, Zhiyong Wang, Xin Qi, Xuefei Liu, Yongwei Li, Yukun Liu, Xiaopeng Wang, Shuchen Shi

With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods. Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.

------------

`[2406.08269] Analyzing constrained LLM through PDFA-learning <https://arxiv.org/abs/2406.08269>`__ 

::

    Wed, 12 Jun 2024 14:35:19 GMT
    Mat\'ias Carrasco, Franz Mayr, Sergio Yovine, Johny Kidd, Mart\'in Iturbide, Juan Pedro da Silva, Alejo Garat

We define a congruence that copes with null next-symbol probabilities that arise when the output of a language model is constrained by some means during text generation. We develop an algorithm for efficiently learning the quotient with respect to this congruence and evaluate it on case studies for analyzing statistical properties of LLM.

------------

`[2406.08407] MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos <https://arxiv.org/abs/2406.08407>`__ MMWorld:面向视频的多学科多面世界模型评估

::

    Wed, 12 Jun 2024 16:54:54 GMT
    Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Kevin Lin, William Yang Wang, Lijuan Wang, Xin Eric Wang

Multimodal Language Language Models (MLLMs) demonstrate the emerging abilities of "world models" -- interpreting and reasoning about complex real-world dynamics. To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and causalities. To this end, we introduce MMWorld, a new benchmark for multi-discipline, multi-faceted multimodal video understanding. MMWorld distinguishes itself from previous video understanding benchmarks with two unique advantages: (1) multi-discipline, covering various disciplines that often require domain expertise for comprehensive understanding; (2) multi-faceted reasoning, including explanation, counterfactual thinking, future prediction, etc. MMWorld consists of a human-annotated dataset to evaluate MLLMs with questions about the whole videos and a synthetic dataset to analyze MLLMs within a single modality of perception. Together, MMWorld encompasses 1,910 videos across seven broad disciplines and 69 subdisciplines, complete with 6,627 question-answer pairs and associated captions. The evaluation includes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld (e.g., GPT-4V performs the best with only 52.3\% accuracy), showing large room for improvement. Further ablation studies reveal other interesting findings such as models' different skill sets from humans. We hope MMWorld can serve as an essential step towards world model evaluation in videos.

------------

`[2406.08418] OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text <https://arxiv.org/abs/2406.08418>`__ OmniCorpus:与文本交织的100亿级图像的统一多模态语料库

::

    Wed, 12 Jun 2024 17:01:04 GMT
    Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, Jiashuo Yu, Hao Tian, Jiasheng Zhou, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Pei Chu, Yi Wang, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He, Jifeng Dai

Image-text interleaved data, consisting of multiple images and texts arranged in a natural document format, aligns with the presentation paradigm of internet data and closely resembles human reading habits. Recent studies have shown that such data aids multimodal in-context learning and maintains the capabilities of large language models during multimodal fine-tuning. However, the limited scale and diversity of current image-text interleaved data restrict the development of multimodal large language models. In this paper, we introduce OmniCorpus, a 10 billion-scale image-text interleaved dataset. Using an efficient data engine, we filter and extract large-scale high-quality documents, which contain 8.6 billion images and 1,696 billion text tokens. Compared to counterparts (e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while maintaining good data quality; 2) features more diverse sources, including both English and non-English websites as well as video-centric websites; 3) is more flexible, easily degradable from an image-text interleaved format to pure text corpus and image-text pairs. Through comprehensive analysis and experiments, we validate the quality, usability, and effectiveness of the proposed dataset. We hope this could provide a solid data foundation for future multimodal model research. Code and data are released at https://github.com/OpenGVLab/OmniCorpus.

------------

`[2406.08474] Real2Code: Reconstruct Articulated Objects via Code Generation <https://arxiv.org/abs/2406.08474>`__ Real2Code:通过代码生成重建铰接的对象

::

    Wed, 12 Jun 2024 17:57:06 GMT
    Zhao Mandi, Yijia Weng, Dominik Bauer, Shuran Song

We present Real2Code, a novel approach to reconstructing articulated objects via code generation. Given visual observations of an object, we first reconstruct its part geometry using an image segmentation model and a shape completion model. We then represent the object parts with oriented bounding boxes, which are input to a fine-tuned large language model (LLM) to predict joint articulation as code. By leveraging pre-trained vision and language models, our approach scales elegantly with the number of articulated parts, and generalizes from synthetic training data to real world objects in unstructured environments. Experimental results demonstrate that Real2Code significantly outperforms previous state-of-the-art in reconstruction accuracy, and is the first approach to extrapolate beyond objects' structural complexity in the training set, and reconstructs objects with up to 10 articulated parts. When incorporated with a stereo reconstruction model, Real2Code also generalizes to real world objects from a handful of multi-view RGB images, without the need for depth or camera information.

------------

`[2406.07553] Inference Acceleration for Large Language Models on CPUs <https://arxiv.org/abs/2406.07553>`__ cpu上大型语言模型的推理加速

::

    Mon, 4 Mar 2024 10:27:23 GMT
    Ditto PS, Jithin VG, Adarsh MS

In recent years, large language models have demonstrated remarkable performance across various natural language processing (NLP) tasks. However, deploying these models for real-world applications often requires efficient inference solutions to handle the computational demands. In this paper, we explore the utilization of CPUs for accelerating the inference of large language models. Specifically, we introduce a parallelized approach to enhance throughput by 1) Exploiting the parallel processing capabilities of modern CPU architectures, 2) Batching the inference request. Our evaluation shows the accelerated inference engine gives an 18-22x improvement in the generated token per sec. The improvement is more with longer sequence and larger models. In addition to this, we can also run multiple workers in the same machine with NUMA node isolation to further improvement in tokens/s. Table 2, we have received 4x additional improvement with 4 workers. This would also make Gen-AI based products and companies environment friendly, our estimates shows that CPU usage for Inference could reduce the power consumption of LLMs by 48.9% while providing production ready throughput and latency.

------------

`[2406.08402] Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models <https://arxiv.org/abs/2406.08402>`__ 

::

    Wed, 12 Jun 2024 16:51:54 GMT
    Chun-Yi Kuan, Wei-Ping Huang, Hung-yi Lee

Large audio-language models (LALMs) enhance traditional large language models by integrating audio perception capabilities, allowing them to tackle audio-related tasks. Previous research has primarily focused on assessing the performance of LALMs across various tasks, yet overlooking their reliability, particularly concerning issues like object hallucination. In our study, we introduce methods to assess the extent of object hallucination of publicly available LALMs. Our findings reveal that LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions, specifically those requiring the identification of the presence of particular object sounds within an audio clip. This limitation highlights a critical weakness in current LALMs: their inadequate understanding of discriminative queries. Moreover, we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions.

------------

`[2406.08478] What If We Recaption Billions of Web Images with LLaMA-3? <https://arxiv.org/abs/2406.08478>`__ 如果我们用LLaMA-3再现数十亿的网络图像呢?

::

    Wed, 12 Jun 2024 17:59:07 GMT
    Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, Cihang Xie

Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. Our paper aims to bridge this community effort, leveraging the powerful and \textit{open-sourced} LLaMA-3, a GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images from the DataComp-1B dataset. Our empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, we observe enhanced zero-shot performance in cross-modal retrieval tasks. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries. Our project page is https://www.haqtu.me/Recap-Datacomp-1B/

------------

`[2310.09605] Penetrative AI: Making LLMs Comprehend the Physical World <https://arxiv.org/abs/2310.09605>`__ 渗透性AI:让llm理解物理世界

::

    replaced with revised version Wed, 12 Jun 2024 13:52:36 GMT
    Submission history From: Huatao Xu [view email]
    [v1] Sat, 14 Oct 2023 15:48:15 UTC (6,022 KB)
    [v2] Sat, 13 Jan 2024 10:25:26 UTC (7,051 KB)
    [v3] Wed, 12 Jun 2024 13:52:36 UTC (9,523 KB)
    Huatao Xu, Liying Han, Qirui Yang, Mo Li, Mani Srivastava

Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term "Penetrative AI". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the embedded world knowledge for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs beyond traditional text-based tasks, but also enables new ways of incorporating human knowledge in cyber-physical systems.

------------

`[2402.01817] LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks <https://arxiv.org/abs/2402.01817>`__ llm不能规划，但可以帮助LLM-Modulo框架进行规划

::

    replaced with revised version Wed, 12 Jun 2024 01:13:11 GMT
    Submission history From: Subbarao Kambhampati [view email]
    [v1] Fri, 2 Feb 2024 14:43:18 UTC (4,551 KB)
    [v2] Tue, 6 Feb 2024 01:29:37 UTC (4,552 KB)
    [v3] Wed, 12 Jun 2024 01:13:11 UTC (6,405 KB)
    Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Saldyt, Anil Murthy

There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of {\bf LLM-Modulo Frameworks} that combine the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, and allows extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.

------------

`[2404.10160] Reinforcement Learning from Multi-role Debates as Feedback for Bias Mitigation in LLMs <https://arxiv.org/abs/2404.10160>`__ 多角色辩论中的强化学习作为llm中偏差缓解的反馈

::

    replaced with revised version Wed, 12 Jun 2024 12:42:28 GMT
    Submission history From: Rosy Cheng [view email]
    [v1] Mon, 15 Apr 2024 22:18:50 UTC (2,079 KB)
    [v2] Sun, 28 Apr 2024 04:08:39 UTC (2,239 KB)
    [v3] Wed, 12 Jun 2024 12:42:28 UTC (2,829 KB)
    Ruoxi Cheng, Haoxuan Ma, Shuirong Cao, Tianyu Shi

Biases in LLMs can harm user experience and societal outcomes. Current bias mitigation methods such as RLHF usually rely on costly human feedback, lack transferability to other topics, and show poor performance. We find that informing the LLMs that their generated content is not generated by them and querying about potential biases greatly boosts their awareness and ability to mitigate biases. Based on this, we propose RLDF (Reinforcement Learning from Multi-role Debates as Feedback), replacing human feedback with AI for bias mitigation. RLDF engages LLMs in multi-role debates to expose biases and gradually reduce biases in each iteration using a ranking scoring mechanism. The dialogue are then used to create a dataset composed of both high bias and low bias instances to train the reward model in reinforcement learning. This dataset can be generated by the same LLM for self-reflection or a superior LLM like an API which guides the former one in a teacher-student mode. Experimental results across different LLMs and types of bias show the effectiveness of our approach in bias mitigation.

------------

`[2406.05954] Aligning Large Language Models with Representation Editing: A Control Perspective <https://arxiv.org/abs/2406.05954>`__ 基于表示编辑的大型语言模型对齐:一个控制视角

::

    replaced with revised version Tue, 11 Jun 2024 21:18:24 GMT
    Submission history From: Lingkai Kong [view email]
    [v1] Mon, 10 Jun 2024 01:21:31 UTC (2,472 KB)
    [v2] Tue, 11 Jun 2024 21:18:24 UTC (2,472 KB)
    Lingkai Kong, Haorui Wang, Wenhao Mu, Yuanqi Du, Yuchen Zhuang, Yifei Zhou, Yue Song, Rongzhi Zhang, Kai Wang, Chao Zhang

Aligning large language models (LLMs) with human objectives is crucial for real-world applications. However, fine-tuning LLMs for alignment often suffers from unstable training and requires substantial computing resources. Test-time alignment techniques, such as prompting and guided decoding, do not modify the underlying model, and their performance remains dependent on the original model's capabilities. To address these challenges, we propose aligning LLMs through representation editing. The core of our method is to view a pre-trained autoregressive LLM as a discrete-time stochastic dynamical system. To achieve alignment for specific objectives, we introduce external control signals into the state space of this language dynamical system. We train a value function directly on the hidden states according to the Bellman equation, enabling gradient-based optimization to obtain the optimal control signals at test time. Our experiments demonstrate that our method outperforms existing test-time alignment techniques while requiring significantly fewer resources compared to fine-tuning methods.

------------

`[2305.12519] DPIC: Decoupling Prompt and Intrinsic Characteristics for LLM Generated Text Detection <https://arxiv.org/abs/2305.12519>`__ DPIC:解耦提示与内在特征的LLM生成文本检测

::

    replaced with revised version Wed, 12 Jun 2024 07:37:35 GMT
    Submission history From: Xiao Yu [view email]
    [v1] Sun, 21 May 2023 17:26:16 UTC (1,457 KB)
    [v2] Sat, 23 Mar 2024 11:34:49 UTC (6,950 KB)
    [v3] Wed, 12 Jun 2024 07:37:35 UTC (1,078 KB)
    Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Xiuwei Shang, Weiming Zhang and Nenghai Yu

Large language models (LLMs) have the potential to generate texts that pose risks of misuse, such as plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Consequently, detecting whether a text is generated by LLMs has become increasingly important. Existing high-quality detection methods usually require access to the interior of the model to extract the intrinsic characteristics. However, since we do not have access to the interior of the black-box model, we must resort to surrogate models, which impacts detection quality. In order to achieve high-quality detection of black-box models, we would like to extract deep intrinsic characteristics of the black-box model generated texts. We view the generation process as a coupled process of prompt and intrinsic characteristics of the generative model. Based on this insight, we propose to decouple prompt and intrinsic characteristics (DPIC) for LLM-generated text detection method. Specifically, given a candidate text, DPIC employs an auxiliary LLM to reconstruct the prompt corresponding to the candidate text, then uses the prompt to regenerate text by the auxiliary LLM, which makes the candidate text and the regenerated text align with their prompts, respectively. Then, the similarity between the candidate text and the regenerated text is used as a detection feature, thus eliminating the prompt in the detection process, which allows the detector to focus on the intrinsic characteristics of the generative model. Compared to the baselines, DPIC has achieved an average improvement of 6.76\% and 2.91\% in detecting texts from different domains generated by GPT4 and Claude3, respectively.

------------

`[2309.02726] Large Language Models for Automated Open-domain Scientific Hypotheses Discovery <https://arxiv.org/abs/2309.02726>`__ 面向自动化开放领域科学假设发现的大型语言模型

::

    replaced with revised version Wed, 12 Jun 2024 08:40:15 GMT
    Submission history From: Zonglin Yang [view email]
    [v1] Wed, 6 Sep 2023 05:19:41 UTC (3,023 KB)
    [v2] Fri, 16 Feb 2024 14:26:28 UTC (3,030 KB)
    [v3] Wed, 12 Jun 2024 08:40:15 UTC (7,258 KB)
    Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria

Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging. In this work, we tackle these problems by proposing the first dataset for social science academic hypotheses discovery, with the final goal to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus. Unlike previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity. A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation. To the best of our knowledge, this is the first work showing that LLMs are able to generate novel (''not existing in literature'') and valid (''reflecting reality'') scientific hypotheses.

------------

`[2309.14348] Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM <https://arxiv.org/abs/2309.14348>`__ 基于鲁棒对齐LLM防御对齐破坏攻击

::

    replaced with revised version Wed, 12 Jun 2024 00:27:06 GMT
    Submission history From: Bochuan Cao [view email]
    [v1] Mon, 18 Sep 2023 02:07:22 UTC (854 KB)
    [v2] Thu, 7 Dec 2023 20:33:49 UTC (862 KB)
    [v3] Wed, 12 Jun 2024 00:27:06 UTC (828 KB)
    Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen

Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments on open-source large language models, we demonstrate that RA-LLM can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100% to around 10% or less.

------------

`[2310.02107] Instances Need More Care: Rewriting Prompts for Instances with LLMs in the Loop Yields Better Zero-Shot Performance <https://arxiv.org/abs/2310.02107>`__ 实例需要更多的关注:在循环中使用llm重写实例的提示可以产生更好的零样本性能

::

    replaced with revised version Tue, 11 Jun 2024 19:00:03 GMT
    Submission history From: Saurabh Srivastava [view email]
    [v1] Tue, 3 Oct 2023 14:51:34 UTC (642 KB)
    [v2] Thu, 5 Oct 2023 22:46:24 UTC (647 KB)
    [v3] Sat, 9 Mar 2024 19:07:00 UTC (11,032 KB)
    [v4] Tue, 11 Jun 2024 19:00:03 UTC (11,042 KB)
    Saurabh Srivastava, Chengyue Huang, Weiguo Fan, Ziyu Yao

Large language models (LLMs) have revolutionized zero-shot task performance, mitigating the need for task-specific annotations while enhancing task generalizability. Despite its advancements, current methods using trigger phrases such as "Let's think step by step" remain limited. This study introduces PRomPTed, an approach that optimizes the zero-shot prompts for individual task instances following an innovative manner of "LLMs in the loop". Our comprehensive evaluation across 13 datasets and 10 task types based on GPT-4 reveals that PRomPTed significantly outperforms both the naive zero-shot approaches and a strong baseline (i.e., "Output Refinement") which refines the task output instead of the input prompt. Our experimental results also confirmed the generalization of this advantage to the relatively weaker GPT-3.5. Even more intriguingly, we found that leveraging GPT-3.5 to rewrite prompts for the stronger GPT-4 not only matches but occasionally exceeds the efficacy of using GPT-4 as the prompt rewriter. Our research thus presents a huge value in not only enhancing zero-shot LLM performance but also potentially enabling supervising LLMs with their weaker counterparts, a capability attracting much interest recently. Finally, our additional experiments confirm the generalization of the advantages to open-source LLMs such as Mistral 7B and Mixtral 8x7B.

------------

`[2311.08369] How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection <https://arxiv.org/abs/2311.08369>`__ 提示方式很重要!即使是指令中面向任务的约束也会影响llm生成的文本检测

::

    replaced with revised version Wed, 12 Jun 2024 06:30:39 GMT
    Submission history From: Ryuto Koike [view email]
    [v1] Tue, 14 Nov 2023 18:32:52 UTC (8,023 KB)
    [v2] Wed, 21 Feb 2024 21:40:00 UTC (9,086 KB)
    [v3] Wed, 12 Jun 2024 06:30:39 UTC (8,528 KB)
    Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki

To combat the misuse of Large Language Models (LLMs), many recent studies have presented LLM-generated-text detectors with promising performance. When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need. However, most recent studies do not cover such diverse instruction patterns when creating datasets for LLM detection. In this paper, we reveal that even task-oriented constraints -- constraints that would naturally be included in an instruction and are not related to detection-evasion -- cause existing powerful detectors to have a large variance in detection performance. We focus on student essay writing as a realistic domain and manually create task-oriented constraints based on several factors for essay quality. Our experiments show that the standard deviation (SD) of current detector performance on texts generated by an instruction with such a constraint is significantly larger (up to an SD of 14.4 F1-score) than that by generating texts multiple times or paraphrasing the instruction. We also observe an overall trend where the constraints can make LLM detection more challenging than without them. Finally, our analysis indicates that the high instruction-following ability of LLMs fosters the large impact of such constraints on detection performance.

------------

`[2311.09096] Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization <https://arxiv.org/abs/2311.09096>`__ 

::

    replaced with revised version Wed, 12 Jun 2024 08:28:15 GMT
    Submission history From: Zhexin Zhang [view email]
    [v1] Wed, 15 Nov 2023 16:42:29 UTC (9,084 KB)
    [v2] Wed, 12 Jun 2024 08:28:15 UTC (9,218 KB)
    Zhexin Zhang, Junxiao Yang, Pei Ke, Fei Mi, Hongning Wang, Minlie Huang

While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of effort in defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the intrinsic conflict between the goals of being helpful and ensuring safety. Accordingly, we propose to integrate goal prioritization at both training and inference stages to counteract. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And integrating goal prioritization into model training reduces the ASR from 71.0% to 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks, both because of their stronger ability in instruction following. Our work thus contributes to the comprehension of jailbreaking attacks and defenses, and sheds light on the relationship between LLMs' capability and safety. Our code is available at \url{this https URL}.

------------

`[2312.15842] Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments <https://arxiv.org/abs/2312.15842>`__ 面向科学教育评估自动评分的LLM知识蒸馏

::

    replaced with revised version Tue, 11 Jun 2024 18:01:09 GMT
    Submission history From: Ehsan  Latif [view email]
    [v1] Tue, 26 Dec 2023 01:24:25 UTC (493 KB)
    [v2] Fri, 9 Feb 2024 17:56:03 UTC (743 KB)
    [v3] Tue, 11 Jun 2024 18:01:09 UTC (275 KB)
    Ehsan Latif, Luyang Fang, Ping Ma, and Xiaoming Zhai

This study proposes a method for knowledge distillation (KD) of fine-tuned Large Language Models (LLMs) into smaller, more efficient, and accurate neural networks. We specifically target the challenge of deploying these models on resource-constrained devices. Our methodology involves training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model. This is achieved through a specialized loss function tailored to learn from the LLM's output probabilities, ensuring that the student model closely mimics the teacher's performance. To validate the performance of the KD approach, we utilized a large dataset, 7T, containing 6,684 student-written responses to science questions and three mathematical reasoning datasets with student-written responses graded by human experts. We compared accuracy with state-of-the-art (SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models. Results have shown that the KD approach has 3% and 2% higher scoring accuracy than ANN and TinyBERT, respectively, and comparable accuracy to the teacher model. Furthermore, the student model size is 0.03M, 4,000 times smaller in parameters and x10 faster in inferencing than the teacher model and TinyBERT, respectively. The significance of this research lies in its potential to make advanced AI technologies accessible in typical educational settings, particularly for automatic scoring.

------------

`[2402.09025] SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks <https://arxiv.org/abs/2402.09025>`__ SLEB:通过冗余验证和消除变压器块精简llm

::

    replaced with revised version Wed, 12 Jun 2024 01:40:12 GMT
    Submission history From: Jiwon Song [view email]
    [v1] Wed, 14 Feb 2024 09:01:13 UTC (8,750 KB)
    [v2] Wed, 29 May 2024 02:53:16 UTC (8,759 KB)
    [v3] Sat, 1 Jun 2024 12:10:48 UTC (8,760 KB)
    [v4] Wed, 12 Jun 2024 01:40:12 UTC (8,760 KB)
    Jiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon Kim

Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB outperforms previous LLM pruning methods in accelerating LLM inference while also maintaining superior perplexity and accuracy, making SLEB as a promising technique for enhancing the efficiency of LLMs. The code is available at: this https URL.

------------

`[2402.10073] Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence <https://arxiv.org/abs/2402.10073>`__ 两者都很重要:在不影响一般智力的情况下提高大型语言模型的情商

::

    replaced with revised version Wed, 12 Jun 2024 04:13:17 GMT
    Submission history From: Weixiang Zhao [view email]
    [v1] Thu, 15 Feb 2024 16:36:04 UTC (838 KB)
    [v2] Thu, 6 Jun 2024 02:13:27 UTC (840 KB)
    [v3] Wed, 12 Jun 2024 04:13:17 UTC (840 KB)
    Weixiang Zhao, Zhuojun Li, Shilong Wang, Yang Wang, Yulin Hu, Yanyan Zhao, Chen Wei, Bing Qin

Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel \underline{\textbf{Mo}}dular \underline{\textbf{E}}motional \underline{\textbf{I}}ntelligence enhancement method (\textbf{MoEI}), consisting of Modular Parameter Expansion and intra-inter modulation, is proposed to comprehensively enhance the EI of LLMs without compromise their GI. Extensive experiments on two representative LLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness of MoEI to improving EI while maintain GI.

------------

`[2402.16617] Long-Context Language Modeling with Parallel Context Encoding <https://arxiv.org/abs/2402.16617>`__ 基于并行上下文编码的长上下文语言建模

::

    replaced with revised version Tue, 11 Jun 2024 18:54:13 GMT
    Submission history From: Howard Yen [view email]
    [v1] Mon, 26 Feb 2024 14:47:35 UTC (246 KB)
    [v2] Tue, 11 Jun 2024 18:54:13 UTC (437 KB)
    Howard Yen, Tianyu Gao, Danqi Chen

Extending large language models (LLMs) to process longer inputs is crucial for a wide range of applications. However, the substantial computational cost of transformers and limited generalization of positional encoding restrict the size of their context window. We introduce Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only LLMs to extend their context window. CEPE employs a small encoder to process long inputs chunk by chunk, enabling the frozen decoder to utilize additional contexts via cross-attention. CEPE is efficient, generalizable, and versatile: trained with 8K-token documents, it extends the context window of LLAMA-2 to 128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE yields strong performance on language modeling and in-context learning. CEPE also excels in retrieval-augmented applications, while existing long-context models degenerate with retrieved contexts. We further introduce a CEPE variant that can extend the context window of instruction-tuned models using only unlabeled data, and showcase its effectiveness on LLAMA-2-CHAT, leading to a strong instruction-following model that can leverage very long contexts on downstream tasks.

------------

`[2404.01753] M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets <https://arxiv.org/abs/2404.01753>`__ M2SA:面向推文情感分析的多模态多语言模型

::

    replaced with revised version Wed, 12 Jun 2024 07:12:36 GMT
    Submission history From: Gaurish Thakkar Mr [view email]
    [v1] Tue, 2 Apr 2024 09:11:58 UTC (6,975 KB)
    [v2] Wed, 5 Jun 2024 13:34:55 UTC (6,975 KB)
    [v3] Wed, 12 Jun 2024 07:12:36 UTC (6,975 KB)
    Gaurish Thakkar, Sherzod Hakimov, Marko Tadi\'c

In recent years, multimodal natural language processing, aimed at learning from diverse data types, has garnered significant attention. However, there needs to be more clarity when it comes to analysing multimodal tasks in multi-lingual contexts. While prior studies on sentiment analysis of tweets have predominantly focused on the English language, this paper addresses this gap by transforming an existing textual Twitter sentiment dataset into a multimodal format through a straightforward curation process. Our work opens up new avenues for sentiment-related research within the research community. Additionally, we conduct baseline experiments utilising this augmented dataset and report the findings. Notably, our evaluations reveal that when comparing unimodal and multimodal configurations, using a sentiment-tuned large language model as a text encoder performs exceptionally well.

------------

`[2404.08700] DyKnow:Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs <https://arxiv.org/abs/2404.08700>`__ DyKnow:动态验证llm中的时间敏感事实知识

::

    replaced with revised version Wed, 12 Jun 2024 13:44:10 GMT
    Submission history From: Seyed Mahed Mousavi [view email]
    [v1] Wed, 10 Apr 2024 18:08:59 UTC (9,567 KB)
    [v2] Wed, 12 Jun 2024 13:44:10 UTC (7,740 KB)
    Seyed Mahed Mousavi, Simone Alghisi, Giuseppe Riccardi

LLMs acquire knowledge from massive data snapshots collected at different timestamps. Their knowledge is then commonly evaluated using static benchmarks. However, factual knowledge is generally subject to time-sensitive changes, and static benchmarks cannot address those cases. We present an approach to dynamically evaluate the knowledge in LLMs and their time-sensitiveness against Wikidata, a publicly available up-to-date knowledge graph. We evaluate the time-sensitive knowledge in twenty-four private and open-source LLMs, as well as the effectiveness of four editing methods in updating the outdated facts. Our results show that 1) outdatedness is a critical problem across state-of-the-art LLMs; 2) LLMs output inconsistent answers when prompted with slight variations of the question prompt; and 3) the performance of the state-of-the-art knowledge editing algorithms is very limited, as they can not reduce the cases of outdatedness and output inconsistency.

------------

`[2404.17991] Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension <https://arxiv.org/abs/2404.17991>`__ 

::

    replaced with revised version Wed, 12 Jun 2024 04:47:01 GMT
    Submission history From: Lin Ai [view email]
    [v1] Sat, 27 Apr 2024 19:42:51 UTC (9,397 KB)
    [v2] Wed, 12 Jun 2024 04:47:01 UTC (9,398 KB)
    Lin Ai, Zheng Hui, Zizhou Liu, Julia Hirschberg

Machine Reading Comprehension (MRC) poses a significant challenge in the field of Natural Language Processing (NLP). While mainstream MRC methods predominantly leverage extractive strategies using encoder-only models such as BERT, generative approaches face the issue of out-of-control generation -- a critical problem where answers generated are often incorrect, irrelevant, or unfaithful to the source text. To address these limitations in generative models for MRC, we introduce the Question-Attended Span Extraction (QASE) module. Integrated during the fine-tuning phase of pre-trained generative language models (PLMs), QASE significantly enhances their performance, allowing them to surpass the extractive capabilities of advanced Large Language Models (LLMs) such as GPT-4 in few-shot settings. Notably, these gains in performance do not come with an increase in computational demands. The efficacy of the QASE module has been rigorously tested across various datasets, consistently achieving or even surpassing state-of-the-art (SOTA) results, thereby bridging the gap between generative and extractive models in extractive MRC tasks.

------------

`[2406.06581] Set-Based Prompting: Provably Solving the Language Model Order Dependency Problem <https://arxiv.org/abs/2406.06581>`__ 集合提示:可证解决语言模型顺序依赖问题

::

    replaced with revised version Wed, 12 Jun 2024 13:59:13 GMT
    Submission history From: Reid McIlroy-Young [view email]
    [v1] Tue, 4 Jun 2024 16:09:13 UTC (697 KB)
    [v2] Wed, 12 Jun 2024 13:59:13 UTC (697 KB)
    Reid McIlroy-Young, Katrina Brown, Conlan Olson, Linjun Zhang, Cynthia Dwork

The development of generative language models that can create long and coherent textual outputs via autoregression has lead to a proliferation of uses and a corresponding sweep of analyses as researches work to determine the limitations of this new paradigm. Unlike humans, these 'Large Language Models' (LLMs) are highly sensitive to small changes in their inputs, leading to unwanted inconsistency in their behavior. One problematic inconsistency when LLMs are used to answer multiple-choice questions or analyze multiple inputs is order dependency: the output of an LLM can (and often does) change significantly when sub-sequences are swapped, despite both orderings being semantically identical. In this paper we present Set-Based Prompting, a technique that guarantees the output of an LLM will not have order dependence on a specified set of sub-sequences. We show that this method provably eliminates order dependency, and that it can be applied to any transformer-based LLM to enable text generation that is unaffected by re-orderings. Delving into the implications of our method, we show that, despite our inputs being out of distribution, the impact on expected accuracy is small, where the expectation is over the order of uniformly chosen shuffling of the candidate responses, and usually significantly less in practice. Thus, Set-Based Prompting can be used as a 'dropped-in' method on fully trained models. Finally, we discuss how our method's success suggests that other strong guarantees can be obtained on LLM performance via modifying the input representations.

------------

`[2406.06590] Are LLMs classical or nonmonotonic reasoners? Lessons from generics <https://arxiv.org/abs/2406.06590>`__ llm是经典推理机还是非单调推理机?来自泛型的教训

::

    replaced with revised version Wed, 12 Jun 2024 11:18:40 GMT
    Submission history From: Alina Leidinger [view email]
    [v1] Wed, 5 Jun 2024 15:23:11 UTC (8,421 KB)
    [v2] Wed, 12 Jun 2024 11:18:40 UTC (8,421 KB)
    Alina Leidinger, Robert van Rooij, Ekaterina Shutova

Recent scholarship on reasoning in LLMs has supplied evidence of impressive performance and flexible adaptation to machine generated or human feedback. Nonmonotonic reasoning, crucial to human cognition for navigating the real world, remains a challenging, yet understudied task. In this work, we study nonmonotonic reasoning capabilities of seven state-of-the-art LLMs in one abstract and one commonsense reasoning task featuring generics, such as 'Birds fly', and exceptions, 'Penguins don't fly' (see Fig. 1). While LLMs exhibit reasoning patterns in accordance with human nonmonotonic reasoning abilities, they fail to maintain stable beliefs on truth conditions of generics at the addition of supporting examples ('Owls fly') or unrelated information ('Lions have manes'). Our findings highlight pitfalls in attributing human reasoning behaviours to LLMs, as well as assessing general capabilities, while consistent reasoning remains elusive.

------------

`[2406.06591] Exploring Multilingual Large Language Models for Enhanced TNM classification of Radiology Report in lung cancer staging <https://arxiv.org/abs/2406.06591>`__ 探索多语言大型语言模型用于肺癌分期的增强TNM分类

::

    replaced with revised version Wed, 12 Jun 2024 15:19:46 GMT
    Submission history From: Hidetoshi Matsuo [view email]
    [v1] Wed, 5 Jun 2024 16:11:55 UTC (1,602 KB)
    [v2] Wed, 12 Jun 2024 15:19:46 UTC (1,239 KB)
    Hidetoshi Matsuo, Mizuho Nishio, Takaaki Matsunaga, Koji Fujimoto, Takamichi Murakami

Background: Structured radiology reports remains underdeveloped due to labor-intensive structuring and narrative-style reporting. Deep learning, particularly large language models (LLMs) like GPT-3.5, offers promise in automating the structuring of radiology reports in natural languages. However, although it has been reported that LLMs are less effective in languages other than English, their radiological performance has not been extensively studied. Purpose: This study aimed to investigate the accuracy of TNM classification based on radiology reports using GPT3.5-turbo (GPT3.5) and the utility of multilingual LLMs in both Japanese and English. Material and Methods: Utilizing GPT3.5, we developed a system to automatically generate TNM classifications from chest CT reports for lung cancer and evaluate its performance. We statistically analyzed the impact of providing full or partial TNM definitions in both languages using a Generalized Linear Mixed Model. Results: Highest accuracy was attained with full TNM definitions and radiology reports in English (M = 94%, N = 80%, T = 47%, and ALL = 36%). Providing definitions for each of the T, N, and M factors statistically improved their respective accuracies (T: odds ratio (OR) = 2.35, p < 0.001; N: OR = 1.94, p < 0.01; M: OR = 2.50, p < 0.001). Japanese reports exhibited decreased N and M accuracies (N accuracy: OR = 0.74 and M accuracy: OR = 0.21). Conclusion: This study underscores the potential of multilingual LLMs for automatic TNM classification in radiology reports. Even without additional model training, performance improvements were evident with the provided TNM definitions, indicating LLMs' relevance in radiology contexts.

------------

`[2406.07494] CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization <https://arxiv.org/abs/2406.07494>`__ CADS:抽象式对话摘要挑战的系统文献综述

::

    replaced with revised version Wed, 12 Jun 2024 10:47:09 GMT
    Submission history From: Frederic Kirstein [view email]
    [v1] Tue, 11 Jun 2024 17:30:22 UTC (6,240 KB)
    [v2] Wed, 12 Jun 2024 10:47:09 UTC (6,241 KB)
    Frederic Kirstein, Jan Philip Wahle, Bela Gipp, Terry Ruas

Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities. We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement. We observe that only a few datasets span across all subdomains. The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant.

------------

`[2401.09074] Code Simulation Challenges for Large Language Models <https://arxiv.org/abs/2401.09074>`__ 大型语言模型的代码模拟挑战

::

    replaced with revised version Wed, 12 Jun 2024 08:55:13 GMT
    Submission history From: Emanuele La Malfa [view email]
    [v1] Wed, 17 Jan 2024 09:23:59 UTC (8,355 KB)
    [v2] Sun, 21 Jan 2024 15:15:30 UTC (8,472 KB)
    [v3] Wed, 29 May 2024 17:56:58 UTC (20,052 KB)
    [v4] Wed, 12 Jun 2024 08:55:13 UTC (20,053 KB)
    Emanuele La Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin, Samuele Marro, Anthony Cohn, Nigel Shadbolt, Michael Wooldridge

Many reasoning, planning, and problem-solving tasks share an intrinsic algorithmic nature: correctly simulating each step is a sufficient condition to solve them correctly. This work studies to what extent Large Language Models (LLMs) can simulate coding and algorithmic tasks to provide insights into general capabilities in such algorithmic reasoning tasks. We introduce benchmarks for straight-line programs, code that contains critical paths, and approximate and redundant instructions. We further assess the simulation capabilities of LLMs with sorting algorithms and nested loops and show that a routine's computational complexity directly affects an LLM's ability to simulate its execution. While the most powerful LLMs exhibit relatively strong simulation capabilities, the process is fragile, seems to rely heavily on pattern recognition, and is affected by memorisation. We propose a novel off-the-shelf prompting method, Chain of Simulation (CoSm), which instructs LLMs to simulate code execution line by line/follow the computation pattern of compilers. CoSm efficiently helps LLMs reduce memorisation and shallow pattern recognition while improving simulation performance. We consider the success of CoSm in code simulation to be inspirational for other general routine simulation reasoning tasks.

------------

`[2403.02310] Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve <https://arxiv.org/abs/2403.02310>`__ 利用sarthi - serve优化LLM推理中的吞吐量-延迟权衡

::

    replaced with revised version Wed, 12 Jun 2024 03:13:20 GMT
    Submission history From: Amey Agrawal [view email]
    [v1] Mon, 4 Mar 2024 18:47:08 UTC (412 KB)
    [v2] Wed, 12 Jun 2024 03:13:20 UTC (1,714 KB)
    Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Alexey Tumanov, Ramachandran Ramjee

Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt and produces the first output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.
We introduce an efficient LLM inference scheduler, Sarathi-Serve, to address this throughput-latency tradeoff. Sarathi-Serve introduces chunked-prefills which splits a prefill request into near equal sized chunks and creates stall-free schedules that adds new requests in a batch without pausing ongoing decodes. Stall-free scheduling unlocks the opportunity to improve throughput with large batch sizes while minimizing the effect of batching on latency. Furthermore, uniform batches in Sarathi-Serve ameliorate the imbalance between iterations resulting in minimal pipeline bubbles.
Our techniques yield significant improvements in inference performance across models and hardware under tail latency constraints. For Mistral-7B on single A100 GPUs, we achieve 2.6x higher serving capacity and up to 3.7x higher serving capacity for the Yi-34B model on two A100 GPUs as compared to vLLM. When used with pipeline parallelism on Falcon-180B, Sarathi-Serve provides up to 5.6x gain in the end-to-end serving capacity. The source code for Sarathi-Serve is available at this https URL.

------------

`[2403.11901] Larimar: Large Language Models with Episodic Memory Control <https://arxiv.org/abs/2403.11901>`__ Larimar:具有情景记忆控制的大型语言模型

::

    replaced with revised version Tue, 11 Jun 2024 19:50:17 GMT
    Submission history From: Igor Melnyk [view email]
    [v1] Mon, 18 Mar 2024 16:01:42 UTC (5,792 KB)
    [v2] Tue, 11 Jun 2024 19:50:17 UTC (6,280 KB)
    Payel Das and Subhajit Chaudhury and Elliot Nelson and Igor Melnyk and Sarath Swaminathan and Sihui Dai and Aur\'elie Lozano and Georgios Kollias and Vijil Chenthamarakshan and Ji\v{r}\'i, Navr\'atil and Soham Dan and Pin-Yu Chen

Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 8-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting, information leakage prevention, and input context length generalization with Larimar and show their effectiveness. Our code is available at this https URL

------------

`[2404.00848] Predictive Performance Comparison of Decision Policies Under Confounding <https://arxiv.org/abs/2404.00848>`__ 混淆项下决策策略的预测性能比较

::

    replaced with revised version Wed, 12 Jun 2024 02:42:10 GMT
    Submission history From: Luke Guerdan [view email]
    [v1] Mon, 1 Apr 2024 01:27:07 UTC (677 KB)
    [v2] Wed, 12 Jun 2024 02:42:10 UTC (678 KB)
    Luke Guerdan, Amanda Coston, Kenneth Holstein, Zhiwei Steven Wu

Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals under no assumptions on the parametric form of the status quo policy. We verify our framework theoretically and via synthetic data experiments. We conclude with a real-world application using our framework to support a pre-deployment evaluation of a proposed modification to a healthcare enrollment policy.

------------

`[2405.07863] RLHF Workflow: From Reward Modeling to Online RLHF <https://arxiv.org/abs/2405.07863>`__ RLHF工作流:从奖励建模到在线RLHF

::

    replaced with revised version Wed, 12 Jun 2024 04:40:53 GMT
    Submission history From: Hanze Dong [view email]
    [v1] Mon, 13 May 2024 15:50:39 UTC (3,574 KB)
    [v2] Wed, 12 Jun 2024 04:40:53 UTC (3,613 KB)
    Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang

We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature. However, existing open-source RLHF projects are still largely confined to the offline learning setting. In this technical report, we aim to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF. In particular, since online human feedback is usually infeasible for open-source communities with limited resources, we start by constructing preference models using a diverse set of open-source datasets and use the constructed proxy preference model to approximate human feedback. Then, we discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation. Our trained LLM, LLaMA-3-8B-SFR-Iterative-DPO-R, achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA. We have shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets. Further, we have made our models, curated datasets, and comprehensive step-by-step code guidebooks publicly available. Please refer to this https URL and this https URL for more detailed information.

------------

`[2406.06282] PowerInfer-2: Fast Large Language Model Inference on a Smartphone <https://arxiv.org/abs/2406.06282>`__ powerinfer2:智能手机上的快速大型语言模型推理

::

    replaced with revised version Wed, 12 Jun 2024 09:39:41 GMT
    Submission history From: Zeyu Mi [view email]
    [v1] Mon, 10 Jun 2024 14:01:21 UTC (804 KB)
    [v2] Wed, 12 Jun 2024 09:39:41 UTC (804 KB)
    Zhenliang Xue, Yixin Song, Zeyu Mi, Le Chen, Yubin Xia, Haibo Chen

This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM. For more details, including a demonstration video, please visit the project site at this http URL.

------------

`[2406.06858] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion <https://arxiv.org/abs/2406.06858>`__ FLUX:通过内核融合实现gpu上基于软件的快速通信重叠

::

    replaced with revised version Wed, 12 Jun 2024 17:12:23 GMT
    Submission history From: Wenlei Bao Dr. [view email]
    [v1] Tue, 11 Jun 2024 00:17:39 UTC (8,300 KB)
    [v2] Wed, 12 Jun 2024 17:12:23 UTC (8,300 KB)
    Liwen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Ziheng Jiang, Haibin Lin, Xin Jin, Xin Liu

Large deep learning models have demonstrated strong ability to solve many tasks across a wide range of applications. Those large models typically require training and inference to be distributed. Tensor parallelism is a common technique partitioning computation of an operation or layer across devices to overcome the memory capacity limitation of a single processor, and/or to accelerate computation to meet a certain latency requirement. However, this kind of parallelism introduces additional communication that might contribute a significant portion of overall runtime. Thus limits scalability of this technique within a group of devices with high speed interconnects, such as GPUs with NVLinks in a node. This paper proposes a novel method, Flux, to significantly hide communication latencies with dependent computations for GPUs. Flux over-decomposes communication and computation operations into much finer-grained operations and further fuses them into a larger kernel to effectively hide communication without compromising kernel efficiency. Flux can potentially overlap up to 96% of communication given a fused kernel. Overall, it can achieve up to 1.24x speedups for training over Megatron-LM on a cluster of 128 GPUs with various GPU generations and interconnects, and up to 1.66x and 1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8 GPUs with various GPU generations and interconnects.

------------

`[2302.04914] Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models <https://arxiv.org/abs/2302.04914>`__ 使用通用语言模型从文本中提取材料数据的灵活、模型无关方法

::

    replaced with revised version Wed, 12 Jun 2024 14:25:15 GMT
    Submission history From: Maciej Polak [view email]
    [v1] Thu, 9 Feb 2023 19:56:37 UTC (573 KB)
    [v2] Sat, 27 Jan 2024 06:48:32 UTC (1,477 KB)
    [v3] Wed, 12 Jun 2024 14:25:15 UTC (584 KB)
    Maciej P. Polak, Shrey Modi, Anna Latosinska, Jinming Zhang, Ching-Wen Wang, Shaonan Wang, Ayan Deep Hazra, and Dane Morgan

Accurate and comprehensive material databases extracted from research papers are crucial for materials science and engineering, but their development requires significant human effort. With large language models (LLMs) transforming the way humans interact with text, LLMs provide an opportunity to revolutionize data extraction. In this study, we demonstrate a simple and efficient method for extracting materials data from full-text research papers leveraging the capabilities of LLMs combined with human supervision. This approach is particularly suitable for mid-sized databases and requires minimal to no coding or prior knowledge about the extracted property. It offers high recall and nearly perfect precision in the resulting database. The method is easily adaptable to new and superior language models, ensuring continued utility. We show this by evaluating and comparing its performance on GPT-3 and GPT-3.5/4 (which underlie ChatGPT), as well as free alternatives such as BART and DeBERTaV3. We provide a detailed analysis of the method's performance in extracting sentences containing bulk modulus data, achieving up to 90% precision at 96% recall, depending on the amount of human effort involved. We further demonstrate the method's broader effectiveness by developing a database of critical cooling rates for metallic glasses over twice the size of previous human curated databases.

------------

`[2308.07429] Semantic Similarity Loss for Neural Source Code Summarization <https://arxiv.org/abs/2308.07429>`__ 神经网络源代码摘要的语义相似度损失

::

    replaced with revised version Tue, 11 Jun 2024 19:57:56 GMT
    Submission history From: Chia-Yi Su [view email]
    [v1] Mon, 14 Aug 2023 19:51:45 UTC (679 KB)
    [v2] Tue, 11 Jun 2024 19:57:56 UTC (7,793 KB)
    Chia-Yi Su and Collin McMillan

This paper presents a procedure for and evaluation of using a semantic similarity metric as a loss function for neural source code summarization. Code summarization is the task of writing natural language descriptions of source code. Neural code summarization refers to automated techniques for generating these descriptions using neural networks. Almost all current approaches involve neural networks as either standalone models or as part of a pretrained large language models e.g., GPT, Codex, LLaMA. Yet almost all also use a categorical cross-entropy (CCE) loss function for network optimization. Two problems with CCE are that 1) it computes loss over each word prediction one-at-a-time, rather than evaluating a whole sentence, and 2) it requires a perfect prediction, leaving no room for partial credit for synonyms. In this paper, we extend our previous work on semantic similarity metrics to show a procedure for using semantic similarity as a loss function to alleviate this problem, and we evaluate this procedure in several settings in both metrics-driven and human studies. In essence, we propose to use a semantic similarity metric to calculate loss over the whole output sentence prediction per training batch, rather than just loss for each word. We also propose to combine our loss with CCE for each word, which streamlines the training process compared to baselines. We evaluate our approach over several baselines and report improvement in the vast majority of conditions.

------------

`[2403.15274] Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review <https://arxiv.org/abs/2403.15274>`__ ChatGPT生物信息学和生物医学信息学:第一年回顾

::

    replaced with revised version Wed, 12 Jun 2024 15:50:31 GMT
    Submission history From: Gangqing Hu [view email]
    [v1] Fri, 22 Mar 2024 15:16:23 UTC (714 KB)
    [v2] Wed, 12 Jun 2024 15:50:31 UTC (1,150 KB)
    Jinge Wang, Zien Cheng, Qiuming Yao, Li Liu, Dong Xu, Gangqing Hu

The year 2023 marked a significant surge in the exploration of applying large language model (LLM) chatbots, notably ChatGPT, across various disciplines. We surveyed the applications of ChatGPT in bioinformatics and biomedical informatics throughout the year, covering omics, genetics, biomedical text mining, drug discovery, biomedical image understanding, bioinformatics programming, and bioinformatics education. Our survey delineates the current strengths and limitations of this chatbot in bioinformatics and offers insights into potential avenues for future developments.

------------

`[2404.02587] The Surprising Effectiveness of Rankers Trained on Expanded Queries <https://arxiv.org/abs/2404.02587>`__ 

::

    replaced with revised version Wed, 12 Jun 2024 09:34:43 GMT
    Submission history From: Abhijit Anand [view email]
    [v1] Wed, 3 Apr 2024 09:12:22 UTC (100 KB)
    [v2] Wed, 12 Jun 2024 09:34:43 UTC (96 KB)
    Abhijit Anand, Venktesh V, Vinay Setty, Avishek Anand

An important problem in text-ranking systems is handling the hard queries that form the tail end of the query distribution. The difficulty may arise due to the presence of uncommon, underspecified, or incomplete queries. In this work, we improve the ranking performance of hard or difficult queries without compromising the performance of other queries. Firstly, we do LLM based query enrichment for training queries using relevant documents. Next, a specialized ranker is fine-tuned only on the enriched hard queries instead of the original queries. We combine the relevance scores from the specialized ranker and the base ranker, along with a query performance score estimated for each query. Our approach departs from existing methods that usually employ a single ranker for all queries, which is biased towards easy queries, which form the majority of the query distribution. In our extensive experiments on the DL-Hard dataset, we find that a principled query performance based scoring method using base and specialized ranker offers a significant improvement of up to 25% on the passage ranking task and up to 48.4% on the document ranking task when compared to the baseline performance of using original queries, even outperforming SOTA model.

------------

`[2405.20773] Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character <https://arxiv.org/abs/2405.20773>`__ visual - role -play:基于角色扮演图像特征的多模态大规模语言模型通用越狱攻击

::

    replaced with revised version Wed, 12 Jun 2024 07:13:48 GMT
    Submission history From: Siyuan Ma [view email]
    [v1] Sat, 25 May 2024 17:17:18 UTC (8,980 KB)
    [v2] Wed, 12 Jun 2024 07:13:48 UTC (8,975 KB)
    Siyuan Ma, Weidi Luo, Yu Wang, Xiaogeng Liu

With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), ensuring their safety has become increasingly critical. To achieve this objective, it requires us to proactively discover the vulnerability of MLLMs by exploring the attack methods. Thus, structure-based jailbreak attacks, where harmful semantic content is embedded within images, have been proposed to mislead the models. However, previous structure-based jailbreak methods mainly focus on transforming the format of malicious queries, such as converting harmful content into images through typography, which lacks sufficient jailbreak effectiveness and generalizability. To address these limitations, we first introduce the concept of "Role-play" into MLLM jailbreak attacks and propose a novel and effective method called Visual Role-play (VRP). Specifically, VRP leverages Large Language Models to generate detailed descriptions of high-risk characters and create corresponding images based on the descriptions. When paired with benign role-play instruction texts, these high-risk character images effectively mislead MLLMs into generating malicious responses by enacting characters with negative attributes. We further extend our VRP method into a universal setup to demonstrate its generalizability. Extensive experiments on popular benchmarks show that VRP outperforms the strongest baseline, Query relevant and FigStep, by an average Attack Success Rate (ASR) margin of 14.3% across all models.

------------

`[2406.05132] 3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination <https://arxiv.org/abs/2406.05132>`__ 3D-GRAND:用于3d - llm的百万级数据集，具有更好的基础和更少的幻觉

::

    replaced with revised version Wed, 12 Jun 2024 17:59:58 GMT
    Submission history From: Jianing Yang [view email]
    [v1] Fri, 7 Jun 2024 17:59:59 UTC (15,259 KB)
    [v2] Wed, 12 Jun 2024 17:59:58 UTC (16,568 KB)
    Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, David F. Fouhey, Joyce Chai

The integration of language and 3D perception is crucial for developing embodied agents and robots that comprehend and interact with the physical world. While large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, their adaptation to 3D environments (3D-LLMs) remains in its early stages. A primary challenge is the absence of large-scale datasets that provide dense grounding between language and 3D scenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset comprising 40,087 household scenes paired with 6.2 million densely-grounded scene-language instructions. Our results show that instruction tuning with 3D-GRAND significantly enhances grounding capabilities and reduces hallucinations in 3D-LLMs. As part of our contributions, we propose a comprehensive benchmark 3D-POPE to systematically evaluate hallucination in 3D-LLMs, enabling fair comparisons among future models. Our experiments highlight a scaling effect between dataset size and 3D-LLM performance, emphasizing the critical role of large-scale 3D-text datasets in advancing embodied AI research. Notably, our results demonstrate early signals for effective sim-to-real transfer, indicating that models trained on large synthetic data can perform well on real-world 3D scans. Through 3D-GRAND and 3D-POPE, we aim to equip the embodied AI community with essential resources and insights, setting the stage for more reliable and better-grounded 3D-LLMs. Project website: this https URL

------------

`[2406.06400] An Empirical Design Justice Approach to Identifying Ethical Considerations in the Intersection of Large Language Models and Social Robotics <https://arxiv.org/abs/2406.06400>`__ 用经验设计正义方法在大型语言模型和社会机器人的交叉领域识别伦理考虑

::

    replaced with revised version Wed, 12 Jun 2024 09:25:36 GMT
    Submission history From: Alva Markelius [view email]
    [v1] Mon, 10 Jun 2024 15:53:50 UTC (1,383 KB)
    [v2] Wed, 12 Jun 2024 09:25:36 UTC (1,383 KB)
    Alva Markelius

The integration of Large Language Models (LLMs) in social robotics presents a unique set of ethical challenges and social impacts. This research is set out to identify ethical considerations that arise in the design and development of these two technologies in combination. Using LLMs for social robotics may provide benefits, such as enabling natural language open-domain dialogues. However, the intersection of these two technologies also gives rise to ethical concerns related to misinformation, non-verbal cues, emotional disruption, and biases. The robot's physical social embodiment adds complexity, as ethical hazards associated with LLM-based Social AI, such as hallucinations and misinformation, can be exacerbated due to the effects of physical embodiment on social perception and communication. To address these challenges, this study employs an empirical design justice-based methodology, focusing on identifying socio-technical ethical considerations through a qualitative co-design and interaction study. The purpose of the study is to identify ethical considerations relevant to the process of co-design of, and interaction with a humanoid social robot as the interface of a LLM, and to evaluate how a design justice methodology can be used in the context of designing LLMs-based social robotics. The findings reveal a mapping of ethical considerations arising in four conceptual dimensions: interaction, co-design, terms of service and relationship and evaluates how a design justice approach can be used empirically in the intersection of LLMs and social robotics.

------------

`[2304.07647] LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision <https://arxiv.org/abs/2304.07647>`__ LASER:弱监督时空场景图学习的神经符号框架

::

    replaced with revised version Wed, 12 Jun 2024 17:16:39 GMT
    Submission history From: Jiani Huang [view email]
    [v1] Sat, 15 Apr 2023 22:24:05 UTC (32,455 KB)
    [v2] Tue, 21 Nov 2023 07:21:50 UTC (32,418 KB)
    [v3] Wed, 22 Nov 2023 05:20:22 UTC (32,418 KB)
    [v4] Wed, 12 Jun 2024 17:16:39 UTC (37,809 KB)
    Jiani Huang, Ziyang Li, Mayur Naik, Ser-Nam Lim

We propose LASER, a neuro-symbolic approach to learn semantic video representations that capture rich spatial and temporal properties in video data by leveraging high-level logic specifications. In particular, we formulate the problem in terms of alignment between raw videos and spatio-temporal logic specifications. The alignment algorithm leverages a differentiable symbolic reasoner and a combination of contrastive, temporal, and semantics losses. It effectively and efficiently trains low-level perception models to extract a fine-grained video representation in the form of a spatio-temporal scene graph that conforms to the desired high-level specification. To practically reduce the manual effort of obtaining ground truth labels, we derive logic specifications from captions by employing a large language model with a generic prompting template. In doing so, we explore a novel methodology that weakly supervises the learning of spatio-temporal scene graphs with widely accessible video-caption data. We evaluate our method on three datasets with rich spatial and temporal specifications: 20BN-Something-Something, MUGEN, and OpenPVSG. We demonstrate that our method learns better fine-grained video semantics than existing baselines.

------------

----------
Index (84)
----------

`[2406.07573] Investigating the Potential of Using Large Language Models for Scheduling <https://arxiv.org/abs/2406.07573>`__ 研究使用大型语言模型进行调度的潜力

`[2406.07962] Toward a Method to Generate Capability Ontologies from Natural Language Descriptions <https://arxiv.org/abs/2406.07962>`__ 研究了一种从自然语言描述生成能力本体的方法

`[2406.08223] Research Trends for the Interplay between Large Language Models and Knowledge Graphs <https://arxiv.org/abs/2406.08223>`__ 大型语言模型与知识图谱相互作用的研究趋势

`[2406.07685] Out-Of-Context Prompting Boosts Fairness and Robustness in Large Language Model Predictions <https://arxiv.org/abs/2406.07685>`__

`[2406.07735] REAL Sampling: Boosting Factuality and Diversity of Open-Ended Generation via Asymptotic Entropy <https://arxiv.org/abs/2406.07735>`__ 真实采样:基于渐近熵增强开放式生成的事实性和多样性

`[2406.07739] UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback <https://arxiv.org/abs/2406.07739>`__ UICoder:通过自动反馈对大型语言模型进行微调以生成用户界面代码

`[2406.07791] Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs <https://arxiv.org/abs/2406.07791>`__ 评判法官:LLMs两两比较评估中的立场偏差的系统调查

`[2406.07794] IndirectRequests: Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests <https://arxiv.org/abs/2406.07794>`__ IndirectRequests:通过合成间接用户请求使面向任务的对话数据集更加自然

`[2406.07815] Are Large Language Models Good Statisticians? <https://arxiv.org/abs/2406.07815>`__ 大型语言模型是优秀的统计学家吗?

`[2406.07835] SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature <https://arxiv.org/abs/2406.07835>`__ SciRIFF:增强科学文献语言模型指导遵循的资源

`[2406.07882] Designing a Dashboard for Transparency and Control of Conversational AI <https://arxiv.org/abs/2406.07882>`__ 为对话式AI的透明度和控制设计仪表板

`[2406.07933] Large Language Model Unlearning via Embedding-Corrupted Prompts <https://arxiv.org/abs/2406.07933>`__ 基于嵌入损坏提示的大型语言模型遗忘

`[2406.07935] Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation <https://arxiv.org/abs/2406.07935>`__ 人工评估指南中的脆弱性定义和检测:可靠NLG评估的初步研究

`[2406.08050] Adversarial Evasion Attack Efficiency against Large Language Models <https://arxiv.org/abs/2406.08050>`__ 针对大型语言模型的对抗规避攻击效率

`[2406.08080] AustroTox: A Dataset for Target-Based Austrian German Offensive Language Detection <https://arxiv.org/abs/2406.08080>`__ AustroTox:基于目标的奥地利德语攻击性语言检测数据集

`[2406.08100] Multimodal Table Understanding <https://arxiv.org/abs/2406.08100>`__

`[2406.08101] CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems <https://arxiv.org/abs/2406.08101>`__ CoXQL:会话XAI系统中解析解释请求的数据集

`[2406.08183] Underneath the Numbers: Quantitative and Qualitative Gender Fairness in LLMs for Depression Prediction <https://arxiv.org/abs/2406.08183>`__ 数字背后:用于抑郁症预测的llm中的数量和质量性别公平性

`[2406.08202] A Dialogue Game for Eliciting Balanced Collaboration <https://arxiv.org/abs/2406.08202>`__ 促进平衡合作的对话游戏

`[2406.08316] Is Programming by Example solved by LLMs? <https://arxiv.org/abs/2406.08316>`__ llm能解决实例编程吗?

`[2406.08398] cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers <https://arxiv.org/abs/2406.08398>`__ 论文:科学论文中的情境和多模态交互对话数据集

`[2406.08434] TasTe: Teaching Large Language Models to Translate through Self-Reflection <https://arxiv.org/abs/2406.08434>`__ TasTe:通过自我反思教会大型语言模型进行翻译

`[2406.08446] OLMES: A Standard for Language Model Evaluations <https://arxiv.org/abs/2406.08446>`__ OLMES:语言模型评估标准

`[2406.08464] Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing <https://arxiv.org/abs/2406.08464>`__ Magpie:通过提示对齐的llm，从头开始对齐数据合成

`[2406.07777] Unifying Interpretability and Explainability for Alzheimer's Disease Progression Prediction <https://arxiv.org/abs/2406.07777>`__ 阿尔茨海默病进展预测可解释性和可解释性的统一

`[2406.07780] A Critical Look At Tokenwise Reward-Guided Text Generation <https://arxiv.org/abs/2406.07780>`__ 分词奖励引导文本生成的批判性审视

`[2406.07831] ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models <https://arxiv.org/abs/2406.07831>`__ ALPS:大型语言模型高稀疏单次剪枝的改进优化

`[2406.07904] Grounding Multimodal Large Language Models in Actions <https://arxiv.org/abs/2406.07904>`__ 多模态大型语言模型的行动基础

`[2406.08074] A Concept-Based Explainability Framework for Large Multimodal Models <https://arxiv.org/abs/2406.08074>`__

`[2406.08391] Large Language Models Must Be Taught to Know What They Don't Know <https://arxiv.org/abs/2406.08391>`__ 必须教会大型语言模型知道它们不知道的东西

`[2406.08414] Discovering Preference Optimization Algorithms with and for Large Language Models <https://arxiv.org/abs/2406.08414>`__ 大型语言模型偏好优化算法的发现

`[2406.08447] The Impact of Initialization on LoRA Finetuning Dynamics <https://arxiv.org/abs/2406.08447>`__ 初始化对LoRA微调动态的影响

`[2406.07594] MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models <https://arxiv.org/abs/2406.07594>`__ MLLMGuard:多模态大型语言模型多维安全评估套件

`[2406.07595] VulDetectBench: Evaluating the Deep Capability of Vulnerability Detection with Large Language Models <https://arxiv.org/abs/2406.07595>`__ VulDetectBench:基于大型语言模型的漏洞检测深度能力评估

`[2406.07714] LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing <https://arxiv.org/abs/2406.07714>`__ LLAMAFUZZ:大型语言模型增强的灰盒模糊测试

`[2406.07737] The Future of Software Engineering in an AI-Driven World <https://arxiv.org/abs/2406.07737>`__

`[2406.07867] Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation <https://arxiv.org/abs/2406.07867>`__ Let's Go Real Talk:面向面对面对话的口语对话模型

`[2406.07923] CTC-aligned Audio-Text Embedding for Streaming Open-vocabulary Keyword Spotting <https://arxiv.org/abs/2406.07923>`__ 面向流式开放词汇表关键词检测的ctc对齐音频文本嵌入

`[2406.07944] DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis <https://arxiv.org/abs/2406.07944>`__ DLLens:通过llm辅助合成测试深度学习库

`[2406.07954] Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition <https://arxiv.org/abs/2406.07954>`__ 数据集和从2024年SaTML LLM夺旗竞赛中吸取的教训

`[2406.08112] Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio <https://arxiv.org/abs/2406.08112>`__ Codecfake:用于检测基于llm的Deepfake音频的初始数据集

`[2406.08269] Analyzing constrained LLM through PDFA-learning <https://arxiv.org/abs/2406.08269>`__

`[2406.08407] MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos <https://arxiv.org/abs/2406.08407>`__ MMWorld:面向视频的多学科多面世界模型评估

`[2406.08418] OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text <https://arxiv.org/abs/2406.08418>`__ OmniCorpus:与文本交织的100亿级图像的统一多模态语料库

`[2406.08474] Real2Code: Reconstruct Articulated Objects via Code Generation <https://arxiv.org/abs/2406.08474>`__ Real2Code:通过代码生成重建铰接的对象

`[2406.07553] Inference Acceleration for Large Language Models on CPUs <https://arxiv.org/abs/2406.07553>`__ cpu上大型语言模型的推理加速

`[2406.08402] Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models <https://arxiv.org/abs/2406.08402>`__

`[2406.08478] What If We Recaption Billions of Web Images with LLaMA-3? <https://arxiv.org/abs/2406.08478>`__ 如果我们用LLaMA-3再现数十亿的网络图像呢?

`[2310.09605] Penetrative AI: Making LLMs Comprehend the Physical World <https://arxiv.org/abs/2310.09605>`__ 渗透性AI:让llm理解物理世界

`[2402.01817] LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks <https://arxiv.org/abs/2402.01817>`__ llm不能规划，但可以帮助LLM-Modulo框架进行规划

`[2404.10160] Reinforcement Learning from Multi-role Debates as Feedback for Bias Mitigation in LLMs <https://arxiv.org/abs/2404.10160>`__ 多角色辩论中的强化学习作为llm中偏差缓解的反馈

`[2406.05954] Aligning Large Language Models with Representation Editing: A Control Perspective <https://arxiv.org/abs/2406.05954>`__ 基于表示编辑的大型语言模型对齐:一个控制视角

`[2305.12519] DPIC: Decoupling Prompt and Intrinsic Characteristics for LLM Generated Text Detection <https://arxiv.org/abs/2305.12519>`__ DPIC:解耦提示与内在特征的LLM生成文本检测

`[2309.02726] Large Language Models for Automated Open-domain Scientific Hypotheses Discovery <https://arxiv.org/abs/2309.02726>`__ 面向自动化开放领域科学假设发现的大型语言模型

`[2309.14348] Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM <https://arxiv.org/abs/2309.14348>`__ 基于鲁棒对齐LLM防御对齐破坏攻击

`[2310.02107] Instances Need More Care: Rewriting Prompts for Instances with LLMs in the Loop Yields Better Zero-Shot Performance <https://arxiv.org/abs/2310.02107>`__ 实例需要更多的关注:在循环中使用llm重写实例的提示可以产生更好的零样本性能

`[2311.08369] How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection <https://arxiv.org/abs/2311.08369>`__ 提示方式很重要!即使是指令中面向任务的约束也会影响llm生成的文本检测

`[2311.09096] Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization <https://arxiv.org/abs/2311.09096>`__

`[2312.15842] Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments <https://arxiv.org/abs/2312.15842>`__ 面向科学教育评估自动评分的LLM知识蒸馏

`[2402.09025] SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks <https://arxiv.org/abs/2402.09025>`__ SLEB:通过冗余验证和消除变压器块精简llm

`[2402.10073] Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence <https://arxiv.org/abs/2402.10073>`__ 两者都很重要:在不影响一般智力的情况下提高大型语言模型的情商

`[2402.16617] Long-Context Language Modeling with Parallel Context Encoding <https://arxiv.org/abs/2402.16617>`__ 基于并行上下文编码的长上下文语言建模

`[2404.01753] M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets <https://arxiv.org/abs/2404.01753>`__ M2SA:面向推文情感分析的多模态多语言模型

`[2404.08700] DyKnow:Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs <https://arxiv.org/abs/2404.08700>`__ DyKnow:动态验证llm中的时间敏感事实知识

`[2404.17991] Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension <https://arxiv.org/abs/2404.17991>`__

`[2406.06581] Set-Based Prompting: Provably Solving the Language Model Order Dependency Problem <https://arxiv.org/abs/2406.06581>`__ 集合提示:可证解决语言模型顺序依赖问题

`[2406.06590] Are LLMs classical or nonmonotonic reasoners? Lessons from generics <https://arxiv.org/abs/2406.06590>`__ llm是经典推理机还是非单调推理机?来自泛型的教训

`[2406.06591] Exploring Multilingual Large Language Models for Enhanced TNM classification of Radiology Report in lung cancer staging <https://arxiv.org/abs/2406.06591>`__ 探索多语言大型语言模型用于肺癌分期的增强TNM分类

`[2406.07494] CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization <https://arxiv.org/abs/2406.07494>`__ CADS:抽象式对话摘要挑战的系统文献综述

`[2401.09074] Code Simulation Challenges for Large Language Models <https://arxiv.org/abs/2401.09074>`__ 大型语言模型的代码模拟挑战

`[2403.02310] Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve <https://arxiv.org/abs/2403.02310>`__ 利用sarthi - serve优化LLM推理中的吞吐量-延迟权衡

`[2403.11901] Larimar: Large Language Models with Episodic Memory Control <https://arxiv.org/abs/2403.11901>`__ Larimar:具有情景记忆控制的大型语言模型

`[2404.00848] Predictive Performance Comparison of Decision Policies Under Confounding <https://arxiv.org/abs/2404.00848>`__ 混淆项下决策策略的预测性能比较

`[2405.07863] RLHF Workflow: From Reward Modeling to Online RLHF <https://arxiv.org/abs/2405.07863>`__ RLHF工作流:从奖励建模到在线RLHF

`[2406.06282] PowerInfer-2: Fast Large Language Model Inference on a Smartphone <https://arxiv.org/abs/2406.06282>`__ powerinfer2:智能手机上的快速大型语言模型推理

`[2406.06858] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion <https://arxiv.org/abs/2406.06858>`__ FLUX:通过内核融合实现gpu上基于软件的快速通信重叠

`[2302.04914] Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models <https://arxiv.org/abs/2302.04914>`__ 使用通用语言模型从文本中提取材料数据的灵活、模型无关方法

`[2308.07429] Semantic Similarity Loss for Neural Source Code Summarization <https://arxiv.org/abs/2308.07429>`__ 神经网络源代码摘要的语义相似度损失

`[2403.15274] Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review <https://arxiv.org/abs/2403.15274>`__ ChatGPT生物信息学和生物医学信息学:第一年回顾

`[2404.02587] The Surprising Effectiveness of Rankers Trained on Expanded Queries <https://arxiv.org/abs/2404.02587>`__

`[2405.20773] Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character <https://arxiv.org/abs/2405.20773>`__ visual - role -play:基于角色扮演图像特征的多模态大规模语言模型通用越狱攻击

`[2406.05132] 3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination <https://arxiv.org/abs/2406.05132>`__ 3D-GRAND:用于3d - llm的百万级数据集，具有更好的基础和更少的幻觉

`[2406.06400] An Empirical Design Justice Approach to Identifying Ethical Considerations in the Intersection of Large Language Models and Social Robotics <https://arxiv.org/abs/2406.06400>`__ 用经验设计正义方法在大型语言模型和社会机器人的交叉领域识别伦理考虑

`[2304.07647] LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision <https://arxiv.org/abs/2304.07647>`__ LASER:弱监督时空场景图学习的神经符号框架

