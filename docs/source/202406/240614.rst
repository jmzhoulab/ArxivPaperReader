240614
========

----------
Survey (2)
----------

`[2406.08787] A Survey on Compositional Learning of AI Models: Theoretical and Experimetnal Practices <https://arxiv.org/abs/2406.08787>`__ 人工智能模型的组合学习综述:理论和实验实践

::

    Thu, 13 Jun 2024 03:46:21 GMT
    Sania Sinha, Tanawan Premsri, Parisa Kordjamshidi

Compositional learning, mastering the ability to combine basic concepts and construct more intricate ones, is crucial for human cognition, especially in human language comprehension and visual perception. This notion is tightly connected to generalization over unobserved situations. Despite its integral role in intelligence, there is a lack of systematic theoretical and experimental research methodologies, making it difficult to analyze the compositional learning abilities of computational models. In this paper, we survey the literature on compositional learning of AI models and the connections made to cognitive studies. We identify abstract concepts of compositionality in cognitive and linguistic studies and connect these to the computational challenges faced by language and vision models in compositional reasoning. We overview the formal definitions, tasks, evaluation benchmarks, variety of computational models, and theoretical findings. We cover modern studies on large language models to provide a deeper understanding of the cutting-edge compositional capabilities exhibited by state-of-the-art AI models and pinpoint important directions for future research.

------------

`[2406.09062] State-Space Modeling in Long Sequence Processing: A Survey on Recurrence in the Transformer Era <https://arxiv.org/abs/2406.09062>`__ 长序列处理中的状态空间建模:Transformer时代的递归性综述

::

    Thu, 13 Jun 2024 12:51:22 GMT
    Matteo Tiezzi, Michele Casoni, Alessandro Betti, Marco Gori and Stefano Melacci

Effectively learning from sequential data is a longstanding goal of Artificial Intelligence, especially in the case of long sequences. From the dawn of Machine Learning, several researchers engaged in the search of algorithms and architectures capable of processing sequences of patterns, retaining information about the past inputs while still leveraging the upcoming data, without losing precious long-term dependencies and correlations. While such an ultimate goal is inspired by the human hallmark of continuous real-time processing of sensory information, several solutions simplified the learning paradigm by artificially limiting the processed context or dealing with sequences of limited length, given in advance. These solutions were further emphasized by the large ubiquity of Transformers, that have initially shaded the role of Recurrent Neural Nets. However, recurrent networks are facing a strong recent revival due to the growing popularity of (deep) State-Space models and novel instances of large-context Transformers, which are both based on recurrent computations to go beyond several limits of currently ubiquitous technologies. In fact, the fast development of Large Language Models enhanced the interest in efficient solutions to process data over time. This survey provides an in-depth summary of the latest approaches that are based on recurrent models for sequential data processing. A complete taxonomy over the latest trends in architectural and algorithmic solutions is reported and discussed, guiding researchers in this appealing research field. The emerging picture suggests that there is room for thinking of novel routes, constituted by learning algorithms which depart from the standard Backpropagation Through Time, towards a more realistic scenario where patterns are effectively processed online, leveraging local-forward computations, opening to further research on this topic.

------------

--------------
Benchmark (12)
--------------

`[2406.08587] CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery <https://arxiv.org/abs/2406.08587>`__ CS-Bench:面向计算机科学掌握的大型语言模型综合基准

::

    Wed, 12 Jun 2024 18:47:28 GMT
    Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, Weihao Zeng, Yejie Wang, Zhuoma GongQue, Jianing Yu, Qiuna Tan, Weiran Xu

Computer Science (CS) stands as a testament to the intricacies of human intelligence, profoundly advancing the development of artificial intelligence and modern society. However, the current community of large language models (LLMs) overly focuses on benchmarks for analyzing specific foundational skills (e.g. mathematics and code generation), neglecting an all-round evaluation of the computer science field. To bridge this gap, we introduce CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning. Utilizing CS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs, revealing the relationship between CS performance and model scales. We also quantitatively analyze the reasons for failures in existing LLMs and highlight directions for improvements, including knowledge supplementation and CS-specific reasoning. Further cross-capability experiments show a high correlation between LLMs' capabilities in computer science and their abilities in mathematics and coding. Moreover, expert LLMs specialized in mathematics and coding also demonstrate strong performances in several CS subfields. Looking ahead, we envision CS-Bench serving as a cornerstone for LLM applications in the CS field and paving new avenues in assessing LLMs' diverse reasoning capabilities. The CS-Bench data and evaluation code are available at https://github.com/csbench/csbench.

------------

`[2406.08598] Language Model Council: Benchmarking Foundation Models on Highly Subjective Tasks by Consensus <https://arxiv.org/abs/2406.08598>`__ 语言模型委员会:通过共识对高度主观任务上的基础模型进行基准测试

::

    Wed, 12 Jun 2024 19:05:43 GMT
    Justin Zhao, Flor Miriam Plaza-del-Arco, Amanda Cercas Curry

The rapid advancement of Large Language Models (LLMs) necessitates robust and challenging benchmarks. Leaderboards like Chatbot Arena rank LLMs based on how well their responses align with human preferences. However, many tasks such as those related to emotional intelligence, creative writing, or persuasiveness, are highly subjective and often lack majoritarian human agreement. Judges may have irreconcilable disagreements about what constitutes a better response. To address the challenge of ranking LLMs on highly subjective tasks, we propose a novel benchmarking framework, the Language Model Council (LMC). The LMC operates through a democratic process to: 1) formulate a test set through equal participation, 2) administer the test among council members, and 3) evaluate responses as a collective jury. We deploy a council of 20 newest LLMs on an open-ended emotional intelligence task: responding to interpersonal dilemmas.
Our results show that the LMC produces rankings that are more separable, robust, and less biased than those from any individual LLM judge, and is more consistent with a human-established leaderboard compared to other benchmarks.

------------

`[2406.08747] StreamBench: Towards Benchmarking Continuous Improvement of Language Agents <https://arxiv.org/abs/2406.08747>`__ StreamBench:面向基准测试语言代理的持续改进

::

    Thu, 13 Jun 2024 02:08:28 GMT
    Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Hung-yi Lee

Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios.

------------

`[2406.09056] CUDRT: Benchmarking the Detection of Human vs. Large Language Models Generated Texts <https://arxiv.org/abs/2406.09056>`__ CUDRT:人工与大型语言模型生成文本检测的基准测试

::

    Thu, 13 Jun 2024 12:43:40 GMT
    Zhen Tao, Zhiyu Li, Dinghao Xi, Wei Xu

The proliferation of large language models (LLMs) has significantly enhanced text generation capabilities across various industries. However, these models' ability to generate human-like text poses substantial challenges in discerning between human and AI authorship. Despite the effectiveness of existing AI-generated text detectors, their development is hindered by the lack of comprehensive, publicly available benchmarks. Current benchmarks are limited to specific scenarios, such as question answering and text polishing, and predominantly focus on English texts, failing to capture the diverse applications and linguistic nuances of LLMs. To address these limitations, this paper constructs a comprehensive bilingual benchmark in both Chinese and English to evaluate mainstream AI-generated text detectors. We categorize LLM text generation into five distinct operations: Create, Update, Delete, Rewrite, and Translate (CUDRT), encompassing all current LLMs activities. We also establish a robust benchmark evaluation framework to support scalable and reproducible experiments. For each CUDRT category, we have developed extensive datasets to thoroughly assess detector performance. By employing the latest mainstream LLMs specific to each language, our datasets provide a thorough evaluation environment. Extensive experimental results offer critical insights for optimizing AI-generated text detectors and suggest future research directions to improve detection accuracy and generalizability across various scenarios.

------------

`[2406.09170] Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning <https://arxiv.org/abs/2406.09170>`__ 时间测试:评估llm时间推理的基准

::

    Thu, 13 Jun 2024 14:31:19 GMT
    Bahare Fatemi, Mehran Kazemi, Anton Tsitsulin, Karishma Malkan, Jinyeong Yim, John Palowitch, Sungyong Seo, Jonathan Halcrow, and Bryan Perozzi

Large language models (LLMs) have showcased remarkable reasoning capabilities, yet they remain susceptible to errors, particularly in temporal reasoning tasks involving complex temporal logic. Existing research has explored LLM performance on temporal reasoning using diverse datasets and benchmarks. However, these studies often rely on real-world data that LLMs may have encountered during pre-training or employ anonymization techniques that can inadvertently introduce factual inconsistencies. In this work, we address these limitations by introducing novel synthetic datasets specifically designed to assess LLM temporal reasoning abilities in various scenarios. The diversity of question types across these datasets enables systematic investigation into the impact of the problem structure, size, question type, fact order, and other factors on LLM performance. Our findings provide valuable insights into the strengths and weaknesses of current LLMs in temporal reasoning tasks. To foster further research in this area, we are open-sourcing the datasets and evaluation framework used in our experiments: https://huggingface.co/datasets/baharef/ToT.

------------

`[2406.09324] Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs <https://arxiv.org/abs/2406.09324>`__ 技巧包:在llm上对越狱攻击进行基准测试

::

    Thu, 13 Jun 2024 17:01:40 GMT
    Zhao Xu, Fan Liu, Hao Liu

Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks. However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues, we evaluate the impact of various attack settings on LLM performance and provide a baseline benchmark for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives.
We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 320 experiments with about 50,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is available at https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking.

------------

`[2406.09397] Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms <https://arxiv.org/abs/2406.09397>`__ 检索中视觉模型与人类美学的对齐:基准和算法

::

    Thu, 13 Jun 2024 17:59:20 GMT
    Miaosen Zhang, Yixuan Wei, Zhen Xing, Yifei Ma, Zuxuan Wu, Ji Li, Zheng Zhang, Qi Dai, Chong Luo, Xin Geng, Baining Guo

Modern vision models are trained on very large noisy datasets. While these models acquire strong capabilities, they may not follow the user's intent to output the desired results in certain aspects, e.g., visual aesthetic, preferred style, and responsibility. In this paper, we target the realm of visual aesthetics and aim to align vision models with human aesthetic standards in a retrieval system. Advanced retrieval systems usually adopt a cascade of aesthetic models as re-rankers or filters, which are limited to low-level features like saturation and perform poorly when stylistic, cultural or knowledge contexts are involved. We find that utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations can make up for this shortcoming. Based on the above findings, we propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic performance with their strong abilities. As aesthetic assessment is one of the most subjective tasks, to validate the robustness of LMM, we further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics.
Experiments demonstrate that our method significantly enhances the aesthetic behaviors of the vision models, under several metrics. We believe the proposed algorithm can be a general practice for aligning vision models with human values.

------------

`[2406.09411] MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding <https://arxiv.org/abs/2406.09411>`__ MuirBench:鲁棒多图像理解的综合基准

::

    Thu, 13 Jun 2024 17:59:52 GMT
    Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, Muhao Chen

We introduce MuirBench, a comprehensive benchmark that focuses on robust multi-image understanding capabilities of multimodal LLMs. MuirBench consists of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that involve 10 categories of multi-image relations (e.g., multiview, temporal relations). Comprising 11,264 images and 2,600 multiple-choice questions, MuirBench is created in a pairwise manner, where each standard instance is paired with an unanswerable variant that has minimal semantic differences, in order for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our results reveal that even the best-performing models like GPT-4o and Gemini Pro find it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.
Open-source multimodal LLMs trained on single images can hardly generalize to multi-image questions, hovering below 33.3% in accuracy. These results highlight the importance of MuirBench in encouraging the community to develop multimodal LLMs that can look beyond a single image, suggesting potential pathways for future improvements.

------------

`[2305.08144] Mobile-Env: Building Qualified Evaluation Benchmarks for LLM-GUI Interaction <https://arxiv.org/abs/2305.08144>`__ Mobile-Env:为LLM-GUI交互建立合格的评估基准

::

    replaced with revised version Thu, 13 Jun 2024 11:51:37 GMT
    Submission history From: Danyang Zhang [view email]
    [v1] Sun, 14 May 2023 12:31:03 UTC (923 KB)
    [v2] Wed, 14 Jun 2023 09:20:46 UTC (1,420 KB)
    [v3] Sat, 24 Feb 2024 12:43:14 UTC (1,969 KB)
    [v4] Thu, 13 Jun 2024 11:51:37 UTC (1,638 KB)
    Danyang Zhang, Zhennan Shen, Rui Xie, Situo Zhang, Tianbao Xie, Zihan Zhao, Siyuan Chen, Lu Chen, Hongshen Xu, Ruisheng Cao, Kai Yu

The Graphical User Interface (GUI) is pivotal for human interaction with the digital world, enabling efficient device control and the completion of complex tasks. Recent progress in Large Language Models (LLMs) and Vision Language Models (VLMs) offers the chance to create advanced GUI agents. To ensure their effectiveness, there's a pressing need for qualified benchmarks that provide trustworthy and reproducible evaluations -- a challenge current benchmarks often fail to address. To tackle this issue, we introduce Mobile-Env, a comprehensive toolkit tailored for creating GUI benchmarks in the Android mobile environment. Mobile-Env offers an isolated and controllable setting for reliable evaluations, and accommodates intermediate instructions and rewards to reflect real-world usage more naturally. Utilizing Mobile-Env, we collect an open-world task set across various real-world apps and a fixed world set, WikiHow, which captures a significant amount of dynamic online contents for fully controllable and reproducible evaluation. We conduct comprehensive evaluations of LLM agents using these benchmarks. Our findings reveal that even advanced models (e.g., GPT-4V and LLaMA-3) struggle with tasks that are relatively simple for humans. This highlights a crucial gap in current models and underscores the importance of developing more capable foundation models and more effective GUI agent frameworks.

------------

`[2311.09861] ConceptPsy:A Benchmark Suite with Conceptual Comprehensiveness in Psychology <https://arxiv.org/abs/2311.09861>`__ conceptsy:心理学中具有概念全面性的基准套件

::

    replaced with revised version Thu, 13 Jun 2024 13:56:20 GMT
    Submission history From: Junlei Zhang [view email]
    [v1] Thu, 16 Nov 2023 12:43:18 UTC (1,432 KB)
    [v2] Fri, 17 Nov 2023 03:17:05 UTC (1,432 KB)
    [v3] Thu, 13 Jun 2024 13:56:20 UTC (9,461 KB)
    Junlei Zhang, Hongliang He, Nirui Song, Zhanchao Zhou, Shuyuan He, Shuai Zhang, Huachuan Qiu, Anqi Li, Yong Dai, Lizhi Ma, Zhenzhong Lan

The critical field of psychology necessitates a comprehensive benchmark to enhance the evaluation and development of domain-specific Large Language Models (LLMs). Existing MMLU-type benchmarks, such as C-EVAL and CMMLU, include psychology-related subjects, but their limited number of questions and lack of systematic concept sampling strategies mean they cannot cover the concepts required in psychology. Consequently, despite their broad subject coverage, these benchmarks lack the necessary depth in the psychology domain, making them inadequate as psychology-specific evaluation suite. To address this issue, this paper presents ConceptPsy, designed to evaluate Chinese complex reasoning and knowledge abilities in psychology. ConceptPsy includes 12 core subjects and 1383 manually collected concepts. Specifically, we prompt GPT-4 to generate questions for each concept using carefully designed diverse prompts and hire professional psychologists to review these questions. To help to understand the fine-grained performances and enhance the weaknesses, we annotate each question with a chapter label and provide chapter-wise accuracy. Based on ConceptPsy, we evaluate a broad range of LLMs. We observe that, although some LLMs achieve similar accuracies on overall performances, they exhibit significant performance variations across different psychology concepts, even when they are models from the same series. We hope our work can facilitate the development of LLMs in the field of psychology.

------------

`[2402.16040] EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries <https://arxiv.org/abs/2402.16040>`__ EHRNoteQA:使用出院摘要进行真实世界临床实践的LLM基准

::

    replaced with revised version Thu, 13 Jun 2024 05:15:33 GMT
    Submission history From: Sunjun Kweon [view email]
    [v1] Sun, 25 Feb 2024 09:41:50 UTC (8,319 KB)
    [v2] Tue, 27 Feb 2024 06:25:25 UTC (8,319 KB)
    [v3] Thu, 13 Jun 2024 05:15:33 UTC (988 KB)
    Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim, Jeewon Yang, Seunghyun Won, Edward Choi

Discharge summaries in Electronic Health Records (EHRs) are crucial for clinical decision-making, but their length and complexity make information extraction challenging, especially when dealing with accumulated summaries across multiple patient admissions. Large Language Models (LLMs) show promise in addressing this challenge by efficiently analyzing vast and complex data. Existing benchmarks, however, fall short in properly evaluating LLMs' capabilities in this context, as they typically focus on single-note information or limited topics, failing to reflect the real-world inquiries required by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel benchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. Every QA pair is initially generated using GPT-4 and then manually reviewed and refined by three clinicians to ensure clinical relevance. EHRNoteQA includes questions that require information across multiple discharge summaries and covers eight diverse topics, mirroring the complexity and diversity of real clinical inquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice question answering, and propose a reliable evaluation method for each. We evaluate 27 LLMs using EHRNoteQA and examine various factors affecting the model performance (e.g., the length and number of discharge summaries). Furthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations in clinical practice, we measure the correlation between the LLM performance on EHRNoteQA, and the LLM performance manually evaluated by clinicians. Results show that LLM performance on EHRNoteQA have higher correlation with clinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to other benchmarks, demonstrating its practical relevance in evaluating LLMs in clinical settings.

------------

`[2403.07350] VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark <https://arxiv.org/abs/2403.07350>`__ VLKEB:一个大型视觉-语言模型知识编辑基准

::

    replaced with revised version Thu, 13 Jun 2024 10:47:48 GMT
    Submission history From: Han Huang [view email]
    [v1] Tue, 12 Mar 2024 06:16:33 UTC (254 KB)
    [v2] Thu, 13 Jun 2024 10:47:48 UTC (2,654 KB)
    Han Huang, Haitian Zhong, Tao Yu, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan

Recently, knowledge editing on large language models (LLMs) has received considerable attention. Compared to this, editing Large Vision-Language Models (LVLMs) faces extra challenges from diverse data modalities and complicated model components, and data for LVLMs editing are limited. The existing LVLM editing benchmark, which comprises three metrics (Reliability, Locality, and Generality), falls short in the quality of synthesized evaluation images and cannot assess whether models apply edited knowledge in relevant content. Therefore, we employ more reliable data collection methods to construct a new Large $\textbf{V}$ision-$\textbf{L}$anguage Model $\textbf{K}$nowledge $\textbf{E}$diting $\textbf{B}$enchmark, $\textbf{VLKEB}$, and extend the Portability metric for more comprehensive evaluation. Leveraging a multi-modal knowledge graph, our image data are bound with knowledge entities. This can be further used to extract entity-related knowledge, which constitutes the base of editing data. We conduct experiments of different editing methods on five LVLMs, and thoroughly analyze how do they impact the models. The results reveal strengths and deficiencies of these methods and hopefully provide insights for future research. The codes and dataset are available at: $\href{this https URL}{\text{this https URL}}$.

------------

--------------
Accelerate (4)
--------------

`[2406.08607] Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference <https://arxiv.org/abs/2406.08607>`__ 遗忘-保留目标的逆转:基于Logit差分的高效LLM遗忘框架

::

    Wed, 12 Jun 2024 19:26:35 GMT
    Jiabao Ji, Yujian Liu, Yang Zhang, Gaowen Liu, Ramana Rao Kompella, Sijia Liu, Shiyu Chang

As Large Language Models (LLMs) demonstrate extensive capability in learning from documents, LLM unlearning becomes an increasingly important research area to address concerns of LLMs in terms of privacy, copyright, etc. A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents, and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents. To achieve both goals, a mainstream class of LLM unlearning methods introduces an optimization framework with a combination of two objectives - maximizing the prediction loss on the forget documents while minimizing that on the retain documents, which suffers from two challenges, degenerated output and catastrophic forgetting. In this paper, we propose a novel unlearning framework called Unlearning from Logit Difference (ULD), which introduces an assistant LLM that aims to achieve the opposite of the unlearning goals: remembering the forget documents and forgetting the retain knowledge. ULD then derives the unlearned LLM by computing the logit difference between the target and the assistant LLMs. We show that such reversed objectives would naturally resolve both aforementioned challenges while significantly improving the training efficiency. Extensive experiments demonstrate that our method efficiently achieves the intended forgetting while preserving the LLM's overall capabilities, reducing training time by more than threefold. Notably, our method loses 0% of model utility on the ToFU benchmark, whereas baseline methods may sacrifice 17% of utility on average to achieve comparable forget quality. Our code will be publicly available at https://github.com/UCSB-NLP-Chang/ULD.

------------

`[2406.09041] ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models <https://arxiv.org/abs/2406.09041>`__ ME-Switch:面向大型语言模型的高效内存专家切换框架

::

    Thu, 13 Jun 2024 12:27:55 GMT
    Jing Liu, Ruihao Gong, Mingyang Zhang, Yefei He, Jianfei Cai, Bohan Zhuang

The typical process for developing LLMs involves pre-training a general foundation model on massive data, followed by fine-tuning on task-specific data to create specialized experts. Serving these experts poses challenges, as loading all experts onto devices is impractical, and frequent switching between experts in response to user requests incurs substantial I/O costs, increasing latency and expenses. Previous approaches decompose expert weights into pre-trained model weights and residual delta weights, then quantize the delta weights to reduce model size. However, these methods often lead to significant quantization errors at extremely low bitwidths and assume the appropriate model for a user request is known in advance, which is not practical. To address these issues, we introduce ME-Switch, a memory-efficient expert switching framework for LLM serving. ME-Switch uses mixed-precision quantization, selectively quantizing non-salient input channels of delta weights to extremely low bits while keeping salient ones intact, significantly reducing storage demands while maintaining performance. Additionally, we develop a routing method that efficiently directs user queries to the most suitable expert by transforming the model selection problem into a domain classification problem.
Extensive experiments show ME-Switch's promising memory efficiency and routing performance. For example, when serving three models from the Mistral-7B family, ME-Switch reduces model size by 1.74x while maintaining nearly lossless performance on instruction, mathematical reasoning, and code generation tasks.
Furthermore, ME-Switch can efficiently serve 16 models from the Mistral-7B family on a single NVIDIA A100 GPU.

------------

`[2406.09044] MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning <https://arxiv.org/abs/2406.09044>`__ MiLoRA:利用次要奇异成分实现参数高效的LLM微调

::

    Thu, 13 Jun 2024 12:30:02 GMT
    Hanqing Wang, Zeguan Xiao, Yixia Li, Shuo Wang, Guanhua Chen, Yun Chen

Efficient finetuning of large language models (LLMs) aims to adapt the LLMs with reduced computation and memory cost. Previous LoRA-based approaches initialize the low-rank matrices with gaussian distribution and zero values, while keeping the original weight matrices frozen. However, the trainable model parameters optimized in an unguided subspace might have interference with the well-learned subspace of the pretrained weight matrix. In this paper, we propose MiLoRA, a simple yet effective LLM finetuning approach that only updates the minor singular components of the weight matrix while keeping the principle singular components frozen. It is observed that the minor matrix corresponds to the noisy or long-tail information, while the principle matrix contains important knowledge. The MiLoRA initializes the low-rank matrices within a subspace that is orthogonal to the principle matrix, thus the pretrained knowledge is expected to be well preserved. During finetuning, MiLoRA makes the most use of the less-optimized subspace for learning the finetuning dataset. Extensive experiments on commonsense reasoning, math reasoning and instruction following benchmarks present the superior performance of our method.

------------

`[2403.11886] QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback-based Self-Correction <https://arxiv.org/abs/2403.11886>`__ QueryAgent:一种基于环境反馈自修正的可靠高效推理框架

::

    replaced with revised version Thu, 13 Jun 2024 13:18:43 GMT
    Submission history From: Sitao Cheng [view email]
    [v1] Mon, 18 Mar 2024 15:39:14 UTC (2,691 KB)
    [v2] Thu, 13 Jun 2024 13:18:43 UTC (2,695 KB)
    Xiang Huang, Sitao Cheng, Shanshan Huang, Jiayu Shen, Yong Xu, Chaoyun Zhang, Yuzhong Qu

Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, revealing the strong transferability of our approach.

------------

-----------------------
In-Context Learning (1)
-----------------------

`[2311.08894] Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning <https://arxiv.org/abs/2311.08894>`__ 基于小样本迁移学习的知识库问答:监督模型与上下文学习的融合

::

    replaced with revised version Thu, 13 Jun 2024 12:06:27 GMT
    Submission history From: Mayur Patidar [view email]
    [v1] Wed, 15 Nov 2023 11:56:56 UTC (39 KB)
    [v2] Tue, 27 Feb 2024 13:04:44 UTC (228 KB)
    [v3] Thu, 13 Jun 2024 12:06:27 UTC (363 KB)
    Mayur Patidar, Riya Sawhney, Avinash Singh, Biswajit Chatterjee, Mausam, Indrajit Bhattacharya

Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms. These are further refined using execution-guided feedback. Experiments over multiple source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments show that FuSIC-KBQA also outperforms SoTA KBQA models in the in-domain setting when training data is limited.

------------

--------------
Reasoning (10)
--------------

`[2406.08657] Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning Enhancement in RLHF and Effective-Merged LLMs <https://arxiv.org/abs/2406.08657>`__ 从粗到细的Actor用于增强RLHF和有效合并的llm的分析和推理

::

    Wed, 12 Jun 2024 21:42:13 GMT
    Chen Zheng, Ke Sun, Xun Zhou

Despite the advances in Large Language Models (LLMs), exemplified by models like GPT-4 and Claude, smaller-scale LLMs such as Llama and Mistral often struggle with generating in-depth and coherent dialogues. This paper presents a novel two-step Coarse-to-Fine Actor model to address the inherent limitations in conversational and analytical capabilities of small-sized LLMs. Our approach begins with the Policy-based Coarse Actor, employing a technique we term "Continuous Maximization". The Coarse Actor establishes an enhanced, knowledge-rich pool adept at aligning with human preference styles in analysis and reasoning. Through the RLHF process, it employs Continuous Maximization, a strategy that dynamically and adaptively extends the output length limit, enabling the generation of more detailed and analytical content. Subsequently, the Fine Actor refines this analytical content, addressing the generation of excessively redundant information from the Coarse Actor. We introduce a "Knowledge Residue Merger" approach, refining the content from the Coarse Actor and merging it with an existing Instruction model to improve quality, correctness, and reduce redundancies. We applied our methodology to the popular Mistral model, creating Mistral-C2F, which has demonstrated exceptional performance across 11 general language tasks and the MT-Bench Dialogue task, outperforming similar-scale models and even larger models with 13B and 30B parameters. Our model has significantly improved conversational and analytical reasoning abilities.

------------

`[2406.09072] Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning? <https://arxiv.org/abs/2406.09072>`__ 活在当下:大型语言模型能掌握共时推理吗?

::

    Thu, 13 Jun 2024 12:56:21 GMT
    Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen, Yu Cheng, Min zhang

Temporal reasoning is fundamental for large language models (LLMs) to comprehend the world. Current temporal reasoning datasets are limited to questions about single or isolated events, falling short in mirroring the realistic temporal characteristics involving concurrent nature and intricate temporal interconnections. In this paper, we introduce CoTempQA, a comprehensive co-temporal Question Answering (QA) benchmark containing four co-temporal scenarios (Equal, Overlap, During, Mix) with 4,748 samples for evaluating the co-temporal comprehension and reasoning abilities of LLMs. Our extensive experiments reveal a significant gap between the performance of current LLMs and human-level reasoning on CoTempQA tasks. Even when enhanced with Chain of Thought (CoT) methodologies, models consistently struggle with our task. In our preliminary exploration, we discovered that mathematical reasoning plays a significant role in handling co-temporal events and proposed a strategy to boost LLMs' co-temporal reasoning from a mathematical perspective. We hope that our CoTempQA datasets will encourage further advancements in improving the co-temporal reasoning capabilities of LLMs. Our code is available at https://github.com/zhaochen0110/Cotempqa.

------------

`[2406.09136] Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs <https://arxiv.org/abs/2406.09136>`__ 偏好链优化:改进llm中的思维链推理

::

    Thu, 13 Jun 2024 14:07:02 GMT
    Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, Min Lin

The recent development of chain-of-thought (CoT) decoding has enabled large language models (LLMs) to generate explicit logical reasoning paths for complex problem-solving. However, research indicates that these paths are not always deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching to extensively explore the reasoning space and find better reasoning paths that CoT decoding might overlook. This deliberation, however, comes at the cost of significantly increased inference complexity. In this work, we demonstrate that fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to achieve similar or better performance, thereby avoiding the substantial inference burden. This is achieved through Chain of Preference Optimization (CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process. Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness. Our code is available at https://github.com/sail-sg/CPO.

------------

`[2406.09170] Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning <https://arxiv.org/abs/2406.09170>`__ 时间测试:评估llm时间推理的基准

::

    Thu, 13 Jun 2024 14:31:19 GMT
    Bahare Fatemi, Mehran Kazemi, Anton Tsitsulin, Karishma Malkan, Jinyeong Yim, John Palowitch, Sungyong Seo, Jonathan Halcrow, and Bryan Perozzi

Large language models (LLMs) have showcased remarkable reasoning capabilities, yet they remain susceptible to errors, particularly in temporal reasoning tasks involving complex temporal logic. Existing research has explored LLM performance on temporal reasoning using diverse datasets and benchmarks. However, these studies often rely on real-world data that LLMs may have encountered during pre-training or employ anonymization techniques that can inadvertently introduce factual inconsistencies. In this work, we address these limitations by introducing novel synthetic datasets specifically designed to assess LLM temporal reasoning abilities in various scenarios. The diversity of question types across these datasets enables systematic investigation into the impact of the problem structure, size, question type, fact order, and other factors on LLM performance. Our findings provide valuable insights into the strengths and weaknesses of current LLMs in temporal reasoning tasks. To foster further research in this area, we are open-sourcing the datasets and evaluation framework used in our experiments: https://huggingface.co/datasets/baharef/ToT.

------------

`[2406.08527] Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning <https://arxiv.org/abs/2406.08527>`__ 基于决策树推理的llm优化表格数据特征生成

::

    Wed, 12 Jun 2024 08:31:34 GMT
    Jaehyun Nam, Kyuyoung Kim, Seunghyuk Oh, Jihoon Tack, Jaehyung Kim, Jinwoo Shin

Learning effective representations from raw data is crucial for the success of deep learning methods. However, in the tabular domain, practitioners often prefer augmenting raw column features over using learned representations, as conventional tree-based algorithms frequently outperform competing approaches.
As a result, feature engineering methods that automatically generate candidate features have been widely used. While these approaches are often effective, there remains ambiguity in defining the space over which to search for candidate features. Moreover, they often rely solely on validation scores to select good features, neglecting valuable feedback from past experiments that could inform the planning of future experiments. To address the shortcomings, we propose a new tabular learning framework based on large language models (LLMs), coined Optimizing Column feature generator with decision Tree reasoning (OCTree). Our key idea is to leverage LLMs' reasoning capabilities to find good feature generation rules without manually specifying the search space and provide language-based reasoning information highlighting past experiments as feedback for iterative rule improvements. Here, we choose a decision tree as reasoning as it can be interpreted in natural language, effectively conveying knowledge of past experiments (i.e., the prediction models trained with the generated features) to the LLM. Our empirical results demonstrate that this simple framework consistently enhances the performance of various prediction models across diverse tabular benchmarks, outperforming competing automatic feature engineering methods.

------------

`[2406.09187] GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning <https://arxiv.org/abs/2406.09187>`__ GuardAgent:通过知识激活推理，由守卫代理保护LLM代理

::

    Thu, 13 Jun 2024 14:49:26 GMT
    Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, Carl Yang, Dawn Song, Bo Li

The rapid advancement of large language models (LLMs) has catalyzed the deployment of LLM-powered agents across numerous applications, raising new concerns regarding their safety and trustworthiness. Existing methods for enhancing the safety of LLMs are not directly transferable to LLM-powered agents due to their diverse objectives and output modalities. In this paper, we propose GuardAgent, the first LLM agent as a guardrail to other LLM agents.
Specifically, GuardAgent oversees a target LLM agent by checking whether its inputs/outputs satisfy a set of given guard requests defined by the users.
GuardAgent comprises two steps: 1) creating a task plan by analyzing the provided guard requests, and 2) generating guardrail code based on the task plan and executing the code by calling APIs or using external engines. In both steps, an LLM is utilized as the core reasoning component, supplemented by in-context demonstrations retrieved from a memory module. Such knowledge-enabled reasoning allows GuardAgent to understand various textual guard requests and accurately "translate" them into executable code that provides reliable guardrails. Furthermore, GuardAgent is equipped with an extendable toolbox containing functions and APIs and requires no additional LLM training, which underscores its generalization capabilities and low operational overhead. Additionally, we propose two novel benchmarks: an EICU-AC benchmark for assessing privacy-related access control for healthcare agents and a Mind2Web-SC benchmark for safety evaluation for web agents. We show the effectiveness of GuardAgent on these two benchmarks with 98.7% and 90.0% accuracy in moderating invalid inputs and outputs for the two types of agents, respectively. We also show that GuardAgent is able to define novel functions in adaption to emergent LLM agents and guard requests, which underscores its strong generalization capabilities.

------------

`[2406.09175] ReMI: A Dataset for Reasoning with Multiple Images <https://arxiv.org/abs/2406.09175>`__ ReMI:用于多图像推理的数据集

::

    Thu, 13 Jun 2024 14:37:04 GMT
    Mehran Kazemi, Nishanth Dikkala, Ankit Anand, Petar Devic, Ishita Dasgupta, Fangyu Liu, Bahare Fatemi, Pranjal Awasthi, Dee Guo, Sreenivas Gollapudi, Ahmed Qureshi

With the continuous advancement of large language models (LLMs), it is essential to create new benchmarks to effectively evaluate their expanding capabilities and identify areas for improvement. This work focuses on multi-image reasoning, an emerging capability in state-of-the-art LLMs. We introduce ReMI, a dataset designed to assess LLMs' ability to Reason with Multiple Images. This dataset encompasses a diverse range of tasks, spanning various reasoning domains such as math, physics, logic, code, table/chart understanding, and spatial and temporal reasoning. It also covers a broad spectrum of characteristics found in multi-image reasoning scenarios. We have benchmarked several cutting-edge LLMs using ReMI and found a substantial gap between their performance and human-level proficiency. This highlights the challenges in multi-image reasoning and the need for further research. Our analysis also reveals the strengths and weaknesses of different models, shedding light on the types of reasoning that are currently attainable and areas where future models require improvement. To foster further research in this area, we are releasing ReMI publicly: https://huggingface.co/datasets/mehrankazemi/ReMI.

------------

`[2310.03309] Concise and Organized Perception Facilitates Reasoning in Large Language Models <https://arxiv.org/abs/2310.03309>`__ 简洁和有组织的感知有助于大型语言模型的推理

::

    replaced with revised version Thu, 13 Jun 2024 06:26:46 GMT
    Submission history From: Shaotian Yan [view email]
    [v1] Thu, 5 Oct 2023 04:47:49 UTC (840 KB)
    [v2] Fri, 1 Mar 2024 03:47:50 UTC (840 KB)
    [v3] Thu, 6 Jun 2024 06:28:02 UTC (710 KB)
    [v4] Thu, 13 Jun 2024 06:26:46 UTC (710 KB)
    Junjie Liu, Shaotian Yan, Chen Shen, Liang Xie, Wenxiao Wang and Jieping Ye

Exploiting large language models (LLMs) to tackle reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex logical problems, characterized by plenty of premises within the prompt and requiring multi-hop reasoning. In particular, the reasoning capabilities of LLMs are brittle to disorder and distractibility. In this work, we first examine the mechanism from the perspective of information flow and reveal that LLMs exhibit failure patterns akin to human-like cognitive biases when dealing with disordered and irrelevant content in reasoning tasks. However, in contrast to LLMs, disordered and irrelevant content does not significantly decrease human performance, as humans have a propensity to distill the most relevant information and systematically organize their thoughts, aiding them in responding to questions. Stem from that, we further propose a novel reasoning approach named Concise and Organized Perception (COP). COP carefully analyzes the given statements to identify the most pertinent information while eliminating redundancy efficiently. It then prompts the LLMs in a more organized form that adapts to the model's inference process. By perceiving concise and organized context, the reasoning abilities of LLMs can be better elicited. Extensive experimental results on several popular logical benchmarks (ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and math benchmark (DI-GSM) show that COP significantly outperforms previous state-of-the-art methods.

------------

`[2402.15610] Selective "Selective Prediction": Reducing Unnecessary Abstention in Vision-Language Reasoning <https://arxiv.org/abs/2402.15610>`__ 选择性"选择性预测":减少视觉-语言推理中的不必要弃权

::

    replaced with revised version Wed, 12 Jun 2024 21:09:39 GMT
    Submission history From: Tejas Srinivasan [view email]
    [v1] Fri, 23 Feb 2024 21:16:52 UTC (4,454 KB)
    [v2] Wed, 12 Jun 2024 21:09:39 UTC (4,555 KB)
    Tejas Srinivasan, Jack Hessel, Tanmay Gupta, Bill Yuchen Lin, Yejin Choi, Jesse Thomason, Khyathi Raghavi Chandu

Selective prediction minimizes incorrect predictions from vision-language models (VLMs) by allowing them to abstain from answering when uncertain. However, when deploying a vision-language system with low tolerance for inaccurate predictions, selective prediction may be over-cautious and abstain too frequently, even on many correct predictions. We introduce ReCoVERR, an inference-time algorithm to reduce the over-abstention of a selective vision-language system without increasing the error rate of the system's predictions. When the VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries to find relevant clues in the image that provide additional evidence for the prediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if enough evidence confirms the prediction the system makes a prediction instead of abstaining. ReCoVERR enables three VLMs (BLIP2, InstructBLIP, and LLaVA-1.5) to answer up to 20% more questions on the VQAv2 and A-OKVQA tasks without decreasing system accuracy, thus improving overall system reliability. Our code is available at this https URL.

------------

`[2403.11886] QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback-based Self-Correction <https://arxiv.org/abs/2403.11886>`__ QueryAgent:一种基于环境反馈自修正的可靠高效推理框架

::

    replaced with revised version Thu, 13 Jun 2024 13:18:43 GMT
    Submission history From: Sitao Cheng [view email]
    [v1] Mon, 18 Mar 2024 15:39:14 UTC (2,691 KB)
    [v2] Thu, 13 Jun 2024 13:18:43 UTC (2,695 KB)
    Xiang Huang, Sitao Cheng, Shanshan Huang, Jiayu Shen, Yong Xu, Chaoyun Zhang, Yuzhong Qu

Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, revealing the strong transferability of our approach.

------------

-----------
ToolUse (1)
-----------

`[2406.09321] JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models <https://arxiv.org/abs/2406.09321>`__ JailbreakEval:一个用于评估针对大型语言模型的越狱尝试的集成工具包

::

    Thu, 13 Jun 2024 16:59:43 GMT
    Delong Ran, Jinyuan Liu, Yichen Gong, Jingyi Zheng, Xinlei He, Tianshuo Cong, Anyu Wang

Jailbreak attacks aim to induce Large Language Models (LLMs) to generate harmful responses for forbidden instructions, presenting severe misuse threats to LLMs. Up to now, research into jailbreak attacks and defenses is emerging, however, there is (surprisingly) no consensus on how to evaluate whether a jailbreak attempt is successful. In other words, the methods to assess the harmfulness of an LLM's response are varied, such as manual annotation or prompting GPT-4 in specific ways. Each approach has its own set of strengths and weaknesses, impacting their alignment with human values, as well as the time and financial cost. This diversity in evaluation presents challenges for researchers in choosing suitable evaluation methods and conducting fair comparisons across different jailbreak attacks and defenses. In this paper, we conduct a comprehensive analysis of jailbreak evaluation methodologies, drawing from nearly ninety jailbreak research released between May 2023 and April 2024.
Our study introduces a systematic taxonomy of jailbreak evaluators, offering in-depth insights into their strengths and weaknesses, along with the current status of their adaptation. Moreover, to facilitate subsequent research, we propose JailbreakEval, a user-friendly toolkit focusing on the evaluation of jailbreak attempts. It includes various well-known evaluators out-of-the-box, so that users can obtain evaluation results with only a single command.
JailbreakEval also allows users to customize their own evaluation workflow in a unified framework with the ease of development and comparison. In summary, we regard JailbreakEval to be a catalyst that simplifies the evaluation process in jailbreak research and fosters an inclusive standard for jailbreak evaluation within the community.

------------

-----------------------
Retrieval-Augmented (3)
-----------------------

`[2406.08718] Enhancing Psychotherapy Counseling: A Data Augmentation Pipeline Leveraging Large Language Models for Counseling Conversations <https://arxiv.org/abs/2406.08718>`__ 增强心理咨询:利用大型语言模型进行咨询对话的数据增强管道

::

    Thu, 13 Jun 2024 00:48:44 GMT
    Jun-Woo Kim, Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang

We introduce a pipeline that leverages Large Language Models (LLMs) to transform single-turn psychotherapy counseling sessions into multi-turn interactions. While AI-supported online counseling services for individuals with mental disorders exist, they are often constrained by the limited availability of multi-turn training datasets and frequently fail to fully utilize therapists' expertise. Our proposed pipeline effectively addresses these limitations. The pipeline comprises two main steps: 1) Information Extraction and 2) Multi-turn Counseling Generation. Each step is meticulously designed to extract and generate comprehensive multi-turn counseling conversations from the available datasets. Experimental results from both zero-shot and few-shot generation scenarios demonstrate that our approach significantly enhances the ability of LLMs to produce higher quality multi-turn dialogues in the context of mental health counseling. Our pipeline and dataset are publicly available https://github.com/jwkim-chat/A-Data-Augmentation-Pipeline-Leveraging-Large-Language-Models-for-Counseling-Conversations.

------------

`[2406.09397] Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms <https://arxiv.org/abs/2406.09397>`__ 检索中视觉模型与人类美学的对齐:基准和算法

::

    Thu, 13 Jun 2024 17:59:20 GMT
    Miaosen Zhang, Yixuan Wei, Zhen Xing, Yifei Ma, Zuxuan Wu, Ji Li, Zheng Zhang, Qi Dai, Chong Luo, Xin Geng, Baining Guo

Modern vision models are trained on very large noisy datasets. While these models acquire strong capabilities, they may not follow the user's intent to output the desired results in certain aspects, e.g., visual aesthetic, preferred style, and responsibility. In this paper, we target the realm of visual aesthetics and aim to align vision models with human aesthetic standards in a retrieval system. Advanced retrieval systems usually adopt a cascade of aesthetic models as re-rankers or filters, which are limited to low-level features like saturation and perform poorly when stylistic, cultural or knowledge contexts are involved. We find that utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations can make up for this shortcoming. Based on the above findings, we propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic performance with their strong abilities. As aesthetic assessment is one of the most subjective tasks, to validate the robustness of LMM, we further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics.
Experiments demonstrate that our method significantly enhances the aesthetic behaviors of the vision models, under several metrics. We believe the proposed algorithm can be a general practice for aligning vision models with human values.

------------

`[2402.17019] Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling <https://arxiv.org/abs/2402.17019>`__ 利用大型语言模型通过讲故事来学习复杂的法律概念

::

    replaced with revised version Thu, 13 Jun 2024 08:10:39 GMT
    Submission history From: Hang Jiang [view email]
    [v1] Mon, 26 Feb 2024 20:56:06 UTC (3,242 KB)
    [v2] Thu, 13 Jun 2024 08:10:39 UTC (3,245 KB)
    [v3] Fri, 14 Jun 2024 06:22:51 UTC (3,245 KB)
    Hang Jiang, Xiajie Zhang, Robert Mahari, Daniel Kessler, Eric Ma, Tal August, Irene Li, Alex 'Sandy' Pentland, Yoon Kim, Jad Kabbara, Deb Roy

Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 294 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop approach to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through randomized controlled trials (RCTs) with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond.

------------

---------
Agent (6)
---------

`[2406.08747] StreamBench: Towards Benchmarking Continuous Improvement of Language Agents <https://arxiv.org/abs/2406.08747>`__ StreamBench:面向基准测试语言代理的持续改进

::

    Thu, 13 Jun 2024 02:08:28 GMT
    Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Hung-yi Lee

Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios.

------------

`[2406.08979] Multi-Agent Software Development through Cross-Team Collaboration <https://arxiv.org/abs/2406.08979>`__ 基于跨团队协作的多agent软件开发

::

    Thu, 13 Jun 2024 10:18:36 GMT
    Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, Cheng Yang

The latest breakthroughs in Large Language Models (LLMs), eg., ChatDev, have catalyzed profound transformations, particularly through multi-agent collaboration for software development. LLM agents can collaborate in teams like humans, and follow the waterfall model to sequentially work on requirements analysis, development, review, testing, and other phases to perform autonomous software generation. However, for an agent team, each phase in a single development process yields only one possible outcome. This results in the completion of only one development chain, thereby losing the opportunity to explore multiple potential decision paths within the solution space.
Consequently, this may lead to obtaining suboptimal results. To address this challenge, we introduce Cross-Team Collaboration (CTC), a scalable multi-team framework that enables orchestrated teams to jointly propose various decisions and communicate with their insights in a cross-team collaboration environment for superior content generation. Experimental results in software development reveal a notable increase in quality compared to state-of-the-art baselines, underscoring the efficacy of our framework. The significant improvements in story generation demonstrate the promising generalization ability of our framework across various domains. We anticipate that our work will guide LLM agents towards a cross-team paradigm and contribute to their significant growth in but not limited to software development. The code and data will be available at https://github.com/OpenBMB/ChatDev.

------------

`[2406.09187] GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning <https://arxiv.org/abs/2406.09187>`__ GuardAgent:通过知识激活推理，由守卫代理保护LLM代理

::

    Thu, 13 Jun 2024 14:49:26 GMT
    Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, Carl Yang, Dawn Song, Bo Li

The rapid advancement of large language models (LLMs) has catalyzed the deployment of LLM-powered agents across numerous applications, raising new concerns regarding their safety and trustworthiness. Existing methods for enhancing the safety of LLMs are not directly transferable to LLM-powered agents due to their diverse objectives and output modalities. In this paper, we propose GuardAgent, the first LLM agent as a guardrail to other LLM agents.
Specifically, GuardAgent oversees a target LLM agent by checking whether its inputs/outputs satisfy a set of given guard requests defined by the users.
GuardAgent comprises two steps: 1) creating a task plan by analyzing the provided guard requests, and 2) generating guardrail code based on the task plan and executing the code by calling APIs or using external engines. In both steps, an LLM is utilized as the core reasoning component, supplemented by in-context demonstrations retrieved from a memory module. Such knowledge-enabled reasoning allows GuardAgent to understand various textual guard requests and accurately "translate" them into executable code that provides reliable guardrails. Furthermore, GuardAgent is equipped with an extendable toolbox containing functions and APIs and requires no additional LLM training, which underscores its generalization capabilities and low operational overhead. Additionally, we propose two novel benchmarks: an EICU-AC benchmark for assessing privacy-related access control for healthcare agents and a Mind2Web-SC benchmark for safety evaluation for web agents. We show the effectiveness of GuardAgent on these two benchmarks with 98.7% and 90.0% accuracy in moderating invalid inputs and outputs for the two types of agents, respectively. We also show that GuardAgent is able to define novel functions in adaption to emergent LLM agents and guard requests, which underscores its strong generalization capabilities.

------------

`[2406.08689] Security of AI Agents <https://arxiv.org/abs/2406.08689>`__ 人工智能agent的安全性

::

    Wed, 12 Jun 2024 23:16:45 GMT
    Yifeng He, Ethan Wang, Yuyang Rong, Zifei Cheng, Hao Chen

The study and development of AI agents have been boosted by large language models. AI agents can function as intelligent assistants and complete tasks on behalf of their users with access to tools and the ability to execute commands in their environments, Through studying and experiencing the workflow of typical AI agents, we have raised several concerns regarding their security.
These potential vulnerabilities are not addressed by the frameworks used to build the agents, nor by research aimed at improving the agents. In this paper, we identify and describe these vulnerabilities in detail from a system security perspective, emphasizing their causes and severe effects. Furthermore, we introduce defense mechanisms corresponding to each vulnerability with meticulous design and experiments to evaluate their viability. Altogether, this paper contextualizes the security issues in the current development of AI agents and delineates methods to make AI agents safer and more reliable.

------------

`[2406.03679] On the Effects of Data Scale on Computer Control Agents <https://arxiv.org/abs/2406.03679>`__ 数据规模对计算机控制agent的影响

::

    replaced with revised version Thu, 13 Jun 2024 13:31:05 GMT
    Submission history From: Wei Li [view email]
    [v1] Thu, 6 Jun 2024 01:49:29 UTC (4,608 KB)
    [v2] Tue, 11 Jun 2024 13:19:38 UTC (4,608 KB)
    [v3] Thu, 13 Jun 2024 13:31:05 UTC (4,608 KB)
    Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, Oriana Riva

Autonomous agents that control computer interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether fine-tuning alone is a viable approach for building real-world computer control agents. In particularly, we investigate how performance measured on both high and low-level tasks in domain and out of domain scales as more training data is collected. To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, AndroidControl is the most diverse computer control dataset to date, including 15,283 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data. Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance.

------------

`[2403.11886] QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback-based Self-Correction <https://arxiv.org/abs/2403.11886>`__ QueryAgent:一种基于环境反馈自修正的可靠高效推理框架

::

    replaced with revised version Thu, 13 Jun 2024 13:18:43 GMT
    Submission history From: Sitao Cheng [view email]
    [v1] Mon, 18 Mar 2024 15:39:14 UTC (2,691 KB)
    [v2] Thu, 13 Jun 2024 13:18:43 UTC (2,695 KB)
    Xiang Huang, Sitao Cheng, Shanshan Huang, Jiayu Shen, Yong Xu, Chaoyun Zhang, Yuzhong Qu

Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, revealing the strong transferability of our approach.

------------

----------
Other (64)
----------

`[2406.08713] Batch-Instructed Gradient for Prompt Evolution:Systematic Prompt Optimization for Enhanced Text-to-Image Synthesis <https://arxiv.org/abs/2406.08713>`__ 批次指导梯度快速演化:增强文本到图像合成的系统提示优化

::

    Thu, 13 Jun 2024 00:33:29 GMT
    Xinrui Yang, Zhuohan Wang, Anthony Hu

Text-to-image models have shown remarkable progress in generating high-quality images from user-provided prompts. Despite this, the quality of these images varies due to the models' sensitivity to human language nuances.
With advancements in large language models, there are new opportunities to enhance prompt design for image generation tasks. Existing research primarily focuses on optimizing prompts for direct interaction, while less attention is given to scenarios involving intermediary agents, like the Stable Diffusion model. This study proposes a Multi-Agent framework to optimize input prompts for text-to-image generation models. Central to this framework is a prompt generation mechanism that refines initial queries using dynamic instructions, which evolve through iterative performance feedback. High-quality prompts are then fed into a state-of-the-art text-to-image model. A professional prompts database serves as a benchmark to guide the instruction modifier towards generating high-caliber prompts. A scoring system evaluates the generated images, and an LLM generates new instructions based on calculated gradients.
This iterative process is managed by the Upper Confidence Bound (UCB) algorithm and assessed using the Human Preference Score version 2 (HPS v2). Preliminary ablation studies highlight the effectiveness of various system components and suggest areas for future improvements.

------------

`[2406.08751] 3D Building Generation in Minecraft via Large Language Models <https://arxiv.org/abs/2406.08751>`__ 基于大型语言模型的Minecraft 3D建筑生成

::

    Thu, 13 Jun 2024 02:21:07 GMT
    Shiying Hu, Zengrong Huang, Chengpeng Hu, Jialin Liu

Recently, procedural content generation has exhibited considerable advancements in the domain of 2D game level generation such as Super Mario Bros. and Sokoban through large language models (LLMs). To further validate the capabilities of LLMs, this paper explores how LLMs contribute to the generation of 3D buildings in a sandbox game, Minecraft. We propose a Text to Building in Minecraft (T2BM) model, which involves refining prompts, decoding interlayer representation and repairing. Facade, indoor scene and functional blocks like doors are supported in the generation. Experiments are conducted to evaluate the completeness and satisfaction of buildings generated via LLMs. It shows that LLMs hold significant potential for 3D building generation. Given appropriate prompts, LLMs can generate correct buildings in Minecraft with complete structures and incorporate specific building blocks such as windows and beds, meeting the specified requirements of human users.

------------

`[2406.09363] ElicitationGPT: Text Elicitation Mechanisms via Language Models <https://arxiv.org/abs/2406.09363>`__ elicitationongpt:基于语言模型的文本诱导机制

::

    Thu, 13 Jun 2024 17:49:10 GMT
    Yifan Wu, Jason Hartline

Scoring rules evaluate probabilistic forecasts of an unknown state against the realized state and are a fundamental building block in the incentivized elicitation of information and the training of machine learning models. This paper develops mechanisms for scoring elicited text against ground truth text using domain-knowledge-free queries to a large language model (specifically ChatGPT) and empirically evaluates their alignment with human preferences. The empirical evaluation is conducted on peer reviews from a peer-grading dataset and in comparison to manual instructor scores for the peer reviews.

------------

`[2406.08582] Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods <https://arxiv.org/abs/2406.08582>`__ 使用QLoRA探索llm中的事实记忆和风格模仿:一项实验研究和质量评估方法

::

    Wed, 12 Jun 2024 18:38:40 GMT
    Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk

There are various methods for adapting LLMs to different domains. The most common methods are prompting, finetuning, and RAG. In this work, we explore the possibility of adapting a model using one of the PEFT methods - QLoRA. The experiment aims to simulate human responses based on their interviews. The simulation quality is assessed by comparing the quality of the style and the quality of the generated facts.

------------

`[2406.08660] Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification <https://arxiv.org/abs/2406.08660>`__ 微调的"小" LLMs(仍然)在文本分类中明显优于零样本生成式AI模型

::

    Wed, 12 Jun 2024 21:46:13 GMT
    Martin Juan Jos\'e Bucher, Marco Martini

Generative AI offers a simple, prompt-based alternative to fine-tuning smaller BERT-style LLMs for text classification tasks. This promises to eliminate the need for manually labeled training data and task-specific model training. However, it remains an open question whether tools like ChatGPT can deliver on this promise. In this paper, we show that smaller, fine-tuned LLMs (still) consistently and significantly outperform larger, zero-shot prompted models in text classification. We compare three major generative AI models (ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs across a diverse set of classification tasks (sentiment, approval/disapproval, emotions, party positions) and text categories (news, tweets, speeches). We find that fine-tuning with application-specific training data achieves superior performance in all cases. To make this approach more accessible to a broader audience, we provide an easy-to-use toolkit alongside this paper. Our toolkit, accompanied by non-technical step-by-step guidance, enables users to select and fine-tune BERT-like LLMs for any classification task with minimal technical and computational effort.

------------

`[2406.08673] HelpSteer2: Open-source dataset for training top-performing reward models <https://arxiv.org/abs/2406.08673>`__ HelpSteer2:用于训练表现最好的奖励模型的开源数据集

::

    Wed, 12 Jun 2024 22:28:08 GMT
    Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, Oleksii Kuchaiev

High-quality preference datasets are essential for training reward models that can effectively guide large language models (LLMs) in generating high-quality responses aligned with human preferences. As LLMs become stronger and better aligned, permissively licensed preference datasets, such as Open Assistant, HH-RLHF, and HelpSteer need to be updated to remain effective for reward modeling. Methods that distil preference data from proprietary LLMs such as GPT-4 have restrictions on commercial usage imposed by model providers. To improve upon both generated responses and attribute labeling quality, we release HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0).
Using a powerful internal base model trained on HelpSteer2, we are able to achieve the SOTA score (92.0%) on Reward-Bench's primary dataset, outperforming currently listed open and proprietary models, as of June 12th, 2024. Notably, HelpSteer2 consists of only ten thousand response pairs, an order of magnitude fewer than existing preference datasets (e.g., HH-RLHF), which makes it highly efficient for training reward models. Our extensive experiments demonstrate that reward models trained with HelpSteer2 are effective in aligning LLMs. In particular, we propose SteerLM 2.0, a model alignment approach that can effectively make use of the rich multi-attribute score predicted by our reward models. HelpSteer2 is available at https://huggingface.co/datasets/nvidia/HelpSteer2 and code is available at https://github.com/NVIDIA/NeMo-Aligner

------------

`[2406.08680] Analyzing Large Language Models for Classroom Discussion Assessment <https://arxiv.org/abs/2406.08680>`__ 面向课堂讨论评价的大型语言模型分析

::

    Wed, 12 Jun 2024 22:43:38 GMT
    Nhat Tran, Benjamin Pierce, Diane Litman, Richard Correnti, Lindsay Clare Matsumura

Automatically assessing classroom discussion quality is becoming increasingly feasible with the help of new NLP advancements such as large language models (LLMs). In this work, we examine how the assessment performance of 2 LLMs interacts with 3 factors that may affect performance: task formulation, context length, and few-shot examples. We also explore the computational efficiency and predictive consistency of the 2 LLMs. Our results suggest that the 3 aforementioned factors do affect the performance of the tested LLMs and there is a relation between consistency and performance. We recommend a LLM-based assessment approach that has a good balance in terms of predictive performance, computational efficiency, and consistency.

------------

`[2406.08707] mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus <https://arxiv.org/abs/2406.08707>`__ mOSCAR:大规模多语言多模态文档级语料库

::

    Thu, 13 Jun 2024 00:13:32 GMT
    Matthieu Futeral, Armel Zebaze, Pedro Ortiz Suarez, Julien Abadji, R\'emi Lacroix, Cordelia Schmid, Rachel Bawden, Beno\^it Sagot

Multimodal Large Language Models (mLLMs) are trained on a large amount of text-image data. While most mLLMs are trained on caption-like data only, Alayrac et al. [2022] showed that additionally training them on interleaved sequences of text and images can lead to the emergence of in-context learning capabilities. However, the dataset they used, M3W, is not public and is only in English. There have been attempts to reproduce their results but the released datasets are English-only. In contrast, current multilingual and multimodal datasets are either composed of caption-like only or medium-scale or fully private data. This limits mLLM research for the 7,000 other languages spoken in the world. We therefore introduce mOSCAR, to the best of our knowledge the first large-scale multilingual and multimodal document corpus crawled from the web. It covers 163 languages, 315M documents, 214B tokens and 1.2B images. We carefully conduct a set of filtering and evaluation steps to make sure mOSCAR is sufficiently safe, diverse and of good quality. We additionally train two types of multilingual model to prove the benefits of mOSCAR: (1) a model trained on a subset of mOSCAR and captioning data and (2) a model train on captioning data only. The model additionally trained on mOSCAR shows a strong boost in few-shot learning performance across various multilingual image-text tasks and benchmarks, confirming previous findings for English-only mLLMs.

------------

`[2406.08726] Standard Language Ideology in AI-Generated Language <https://arxiv.org/abs/2406.08726>`__ 人工智能生成语言中的标准语言意识形态

::

    Thu, 13 Jun 2024 01:08:40 GMT
    Genevieve Smith, Eve Fleisig, Madeline Bossi, Ishita Rustagi, Xavier Yin

In this position paper, we explore standard language ideology in language generated by large language models (LLMs). First, we outline how standard language ideology is reflected and reinforced in LLMs. We then present a taxonomy of open problems regarding standard language ideology in AI-generated language with implications for minoritized language communities. We introduce the concept of standard AI-generated language ideology, the process by which AI-generated language regards Standard American English (SAE) as a linguistic default and reinforces a linguistic bias that SAE is the most "appropriate" language. Finally, we discuss tensions that remain, including reflecting on what desirable system behavior looks like, as well as advantages and drawbacks of generative AI tools imitating--or often not--different English language varieties. Throughout, we discuss standard language ideology as a manifestation of existing global power structures in and through AI-generated language before ending with questions to move towards alternative, more emancipatory digital futures.

------------

`[2406.08754] StructuralSleight: Automated Jailbreak Attacks on Large Language Models Utilizing Uncommon Text-Encoded Structure <https://arxiv.org/abs/2406.08754>`__ StructuralSleight:利用不常见的文本编码结构对大型语言模型进行自动化越狱攻击

::

    Thu, 13 Jun 2024 02:24:08 GMT
    Bangxin Li and Hengrui Xing and Chao Huang and Jin Qian and Huangqing Xiao and Linfeng Feng and Cong Tian

Large Language Models (LLMs) are widely used in natural language processing but face the risk of jailbreak attacks that maliciously induce them to generate harmful content. Existing jailbreak attacks, including character-level and context-level attacks, mainly focus on the prompt of the plain text without specifically exploring the significant influence of its structure. In this paper, we focus on studying how prompt structure contributes to the jailbreak attack. We introduce a novel structure-level attack method based on tail structures that are rarely used during LLM training, which we refer to as Uncommon Text-Encoded Structure (UTES). We extensively study 12 UTESs templates and 6 obfuscation methods to build an effective automated jailbreak tool named StructuralSleight that contains three escalating attack strategies: Structural Attack, Structural and Character/Context Obfuscation Attack, and Fully Obfuscated Structural Attack. Extensive experiments on existing LLMs show that StructuralSleight significantly outperforms baseline methods. In particular, the attack success rate reaches 94.62\% on GPT-4o, which has not been addressed by state-of-the-art techniques.

------------

`[2406.08811] Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large Language Models <https://arxiv.org/abs/2406.08811>`__ 技能混合:学习优化数据使用以微调大型语言模型

::

    Thu, 13 Jun 2024 05:01:28 GMT
    Minghao Wu, Thuy-Trang Vu, Lizhen Qu, Gholamreza Haffari

Large language models (LLMs) are typically fine-tuned on diverse and extensive datasets sourced from various origins to develop a comprehensive range of skills, such as writing, reasoning, chatting, coding, and more. Each skill has unique characteristics, and these datasets are often heterogeneous and imbalanced, making the fine-tuning process highly challenging. Balancing the development of each skill while ensuring the model maintains its overall performance requires sophisticated techniques and careful dataset curation. In this work, we propose a general, model-agnostic, reinforcement learning framework, Mixture-of-Skills (MoS), that learns to optimize data usage automatically during the fine-tuning process. This framework ensures the optimal comprehensive skill development of LLMs by dynamically adjusting the focus on different datasets based on their current learning state. To validate the effectiveness of MoS, we conduct extensive experiments using three diverse LLM backbones on two widely used benchmarks and demonstrate that MoS substantially enhances model performance. Building on the success of MoS, we propose MoSpec, an adaptation for task-specific fine-tuning, which harnesses the utilities of various datasets for a specific purpose. Our work underlines the significance of dataset rebalancing and present MoS as a powerful, general solution for optimizing data usage in the fine-tuning of LLMs for various purposes.

------------

`[2406.08842] ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions <https://arxiv.org/abs/2406.08842>`__ ContraSolver:通过解决内部偏好矛盾实现语言模型的自我对齐

::

    Thu, 13 Jun 2024 06:08:04 GMT
    Xu Zhang, Xunjian Yin and Xiaojun Wan

While substantial advancements have been made in developing large language models (LLMs), achieving control over their behavior can be difficult. Direct preference optimization (DPO) assumes the existence of a latent reward function to evaluate the responses of LLMs. This assumption indicates a strict preference ordering of different responses to the same input. However, there always exist contradictions of preference in LLMs according to our experimental observations. In this paper, we construct a graph structure of the preference relationship among different responses with self-annotation to find contradictions in the preference order. We propose ContraSolver, an algorithm that traverses all edges on the preference graph to identify those that might cause contradictions. ContraSolver initializes the graph with a maximum spanning tree and identifies contradictory edges, prioritizing the resolution of low-confidence preferences while preserving high-confidence ones.
Experimental results on four different generation tasks show that the performance of different LLMs can be largely improved through our completely unsupervised self-alignment. Furthermore, by analyzing the preference graphs of LLMs with and without self-alignment by ContraSolver, we quantify the reduction in contradictions, suggesting that resolving preference contradictions is crucial for achieving better alignment performance.

------------

`[2406.08848] An Approach to Build Zero-Shot Slot-Filling System for Industry-Grade Conversational Assistants <https://arxiv.org/abs/2406.08848>`__ 一种面向工业级会话助手的零样本槽填充系统构建方法

::

    Thu, 13 Jun 2024 06:24:52 GMT
    G P Shrivatsa Bhargav, Sumit Neelam, Udit Sharma, Shajith Ikbal, Dheeraj Sreedhar, Hima Karanam, Sachindra Joshi, Pankaj Dhoolia, Dinesh Garg, Kyle Croutwater, Haode Qi, Eric Wayne, J William Murdock

We present an approach to build Large Language Model (LLM) based slot-filling system to perform Dialogue State Tracking in conversational assistants serving across a wide variety of industry-grade applications. Key requirements of this system include: 1) usage of smaller-sized models to meet low latency requirements and to enable convenient and cost-effective cloud and customer premise deployments, and 2) zero-shot capabilities to serve across a wide variety of domains, slot types and conversational scenarios. We adopt a fine-tuning approach where a pre-trained LLM is fine-tuned into a slot-filling model using task specific data. The fine-tuning data is prepared carefully to cover a wide variety of slot-filling task scenarios that the model is expected to face across various domains. We give details of the data preparation and model building process. We also give a detailed analysis of the results of our experimental evaluations. Results show that our prescribed approach for slot-filling model building has resulted in 6.9% relative improvement of F1 metric over the best baseline on a realistic benchmark, while at the same time reducing the latency by 57%. More over, the data we prepared has helped improve F1 on an average by 4.2% relative across various slot-types.

------------

`[2406.08860] Plan, Generate and Complicate: Improving Low-resource Dialogue State Tracking via Easy-to-Difficult Zero-shot Data Augmentation <https://arxiv.org/abs/2406.08860>`__ 规划、生成和复杂化:通过易-难的零样本数据增强改进低资源对话状态跟踪

::

    Thu, 13 Jun 2024 06:49:03 GMT
    Ming Gu, Yan Yang

Data augmentation methods have been a promising direction to improve the performance of small models for low-resource dialogue state tracking. However, traditional methods rely on pre-defined user goals and neglect the importance of data complexity in this task. In this paper, we propose EDZ-DA, an Easy-to-Difficult Zero-shot Data Augmentation framework for low-resource dialogue state tracking that utilizes large language models to automatically catch the relationships of different domains and then generate the dialogue data. We also complicate the dialogues based on the domain relation to enhance the model's capability for co-reference slot tracking. Furthermore, we permute slot values to mitigate the influence of output orders and the problem of incomplete value generation. Experimental results illustrate the superiority of our proposed method compared to previous strong data augmentation baselines on MultiWOZ.

------------

`[2406.08903] Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models <https://arxiv.org/abs/2406.08903>`__ Delta-CoMe:大型语言模型混合精度的免训练增量压缩

::

    Thu, 13 Jun 2024 07:57:27 GMT
    Bowen Ping, Shuo Wang, Hanqing Wang, Xu Han, Yuzhuang Xu, Yukun Yan, Yun Chen, Baobao Chang, Zhiyuan Liu, Maosong Sun

Fine-tuning is a crucial process for adapting large language models (LLMs) to diverse applications. In certain scenarios, such as multi-tenant serving, deploying multiple LLMs becomes necessary to meet complex demands. Recent studies suggest decomposing a fine-tuned LLM into a base model and corresponding delta weights, which are then compressed using low-rank or low-bit approaches to reduce costs. In this work, we observe that existing low-rank and low-bit compression methods can significantly harm the model performance for task-specific fine-tuned LLMs (e.g., WizardMath for math problems). Motivated by the long-tail distribution of singular values in the delta weights, we propose a delta quantization approach using mixed-precision.
This method employs higher-bit representation for singular vectors corresponding to larger singular values. We evaluate our approach on various fine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.
Experimental results demonstrate that our approach performs comparably to full fine-tuned LLMs, surpassing both low-rank and low-bit baselines by a considerable margin. Additionally, we show that our method is compatible with various backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its generalizability.

------------

`[2406.08922] Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors <https://arxiv.org/abs/2406.08922>`__ 导航阴影:揭开现代AI内容检测器的有效干扰

::

    Thu, 13 Jun 2024 08:37:01 GMT
    Ying Zhou, Ben He, Le Sun

With the launch of ChatGPT, large language models (LLMs) have attracted global attention. In the realm of article writing, LLMs have witnessed extensive utilization, giving rise to concerns related to intellectual property protection, personal privacy, and academic integrity. In response, AI-text detection has emerged to distinguish between human and machine-generated content. However, recent research indicates that these detection systems often lack robustness and struggle to effectively differentiate perturbed texts.
Currently, there is a lack of systematic evaluations regarding detection performance in real-world applications, and a comprehensive examination of perturbation techniques and detector robustness is also absent. To bridge this gap, our work simulates real-world scenarios in both informal and professional writing, exploring the out-of-the-box performance of current detectors.
Additionally, we have constructed 12 black-box text perturbation methods to assess the robustness of current detection models across various perturbation granularities. Furthermore, through adversarial learning experiments, we investigate the impact of perturbation data augmentation on the robustness of AI-text detectors. We have released our code and data at https://github.com/zhouying20/ai-text-detector-evaluation.

------------

`[2406.09008] LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language Models <https://arxiv.org/abs/2406.09008>`__ LLM阅读茶叶:用大型语言模型自动评估主题模型

::

    Thu, 13 Jun 2024 11:19:50 GMT
    Xiaohao Yang, He Zhao, Dinh Phung, Wray Buntine, Lan Du

Topic modeling has been a widely used tool for unsupervised text analysis.
However, comprehensive evaluations of a topic model remain challenging.
Existing evaluation methods are either less comparable across different models (e.g., perplexity) or focus on only one specific aspect of a model (e.g., topic quality or document representation quality) at a time, which is insufficient to reflect the overall model performance. In this paper, we propose WALM (Words Agreement with Language Model), a new evaluation method for topic modeling that comprehensively considers the semantic quality of document representations and topics in a joint manner, leveraging the power of large language models (LLMs).
With extensive experiments involving different types of topic models, WALM is shown to align with human judgment and can serve as a complementary evaluation method to the existing ones, bringing a new perspective to topic modeling. Our software package will be available at https://github.com/Xiaohao-Yang/Topic_Model_Evaluation, which can be integrated with many widely used topic models.

------------

`[2406.09012] Bayesian Statistical Modeling with Predictors from LLMs <https://arxiv.org/abs/2406.09012>`__ 基于llm预测变量的贝叶斯统计模型

::

    Thu, 13 Jun 2024 11:33:30 GMT
    Michael Franke, Polina Tsvilodub, Fausto Carcassi

State of the art large language models (LLMs) have shown impressive performance on a variety of benchmark tasks and are increasingly used as components in larger applications, where LLM-based predictions serve as proxies for human judgements or decision. This raises questions about the human-likeness of LLM-derived information, alignment with human intuition, and whether LLMs could possibly be considered (parts of) explanatory models of (aspects of) human cognition or language use. To shed more light on these issues, we here investigate the human-likeness of LLMs' predictions for multiple-choice decision tasks from the perspective of Bayesian statistical modeling. Using human data from a forced-choice experiment on pragmatic language use, we find that LLMs do not capture the variance in the human data at the item-level. We suggest different ways of deriving full distributional predictions from LLMs for aggregate, condition-level data, and find that some, but not all ways of obtaining condition-level predictions yield adequate fits to human data. These results suggests that assessment of LLM performance depends strongly on seemingly subtle choices in methodology, and that LLMs are at best predictors of human behavior at the aggregate, condition-level, for which they are, however, not designed to, or usually used to, make predictions in the first place.

------------

`[2406.09043] Language Models are Crossword Solvers <https://arxiv.org/abs/2406.09043>`__ 语言模型是填字游戏求解器

::

    Thu, 13 Jun 2024 12:29:27 GMT
    Soumadeep Saha and Sutanoya Chakraborty and Saptarshi Saha and Utpal Garain

Crosswords are a form of word puzzle that require a solver to demonstrate a high degree of proficiency in natural language understanding, wordplay, reasoning, and world knowledge, along with adherence to character and length constraints. In this paper we tackle the challenge of solving crosswords with Large Language Models (LLMs). We demonstrate that the current generation of state-of-the art (SoTA) language models show significant competence at deciphering cryptic crossword clues, and outperform previously reported SoTA results by a factor of 2-3 in relevant benchmarks. We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with LLMs for the very first time, achieving an accuracy of 93\% on New York Times crossword puzzles. Contrary to previous work in this area which concluded that LLMs lag human expert performance significantly, our research suggests this gap is a lot narrower.

------------

`[2406.09098] SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large Language Models <https://arxiv.org/abs/2406.09098>`__ SciKnowEval:评估大型语言模型的多层次科学知识

::

    Thu, 13 Jun 2024 13:27:52 GMT
    Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, Huajun Chen

The burgeoning utilization of Large Language Models (LLMs) in scientific research necessitates advanced benchmarks capable of evaluating their understanding and application of scientific knowledge comprehensively. To address this need, we introduce the SciKnowEval benchmark, a novel framework that systematically evaluates LLMs across five progressive levels of scientific knowledge: studying extensively, inquiring earnestly, thinking profoundly, discerning clearly, and practicing assiduously. These levels aim to assess the breadth and depth of scientific knowledge in LLMs, including knowledge coverage, inquiry and exploration capabilities, reflection and reasoning abilities, ethic and safety considerations, as well as practice proficiency.
Specifically, we take biology and chemistry as the two instances of SciKnowEval and construct a dataset encompassing 50K multi-level scientific problems and solutions. By leveraging this dataset, we benchmark 20 leading open-source and proprietary LLMs using zero-shot and few-shot prompting strategies. The results reveal that despite achieving state-of-the-art performance, the proprietary LLMs still have considerable room for improvement, particularly in addressing scientific computations and applications. We anticipate that SciKnowEval will establish a comprehensive standard for benchmarking LLMs in science research and discovery, and promote the development of LLMs that integrate scientific knowledge with strong safety awareness. The dataset and code are publicly available at https://github.com/hicai-zju/sciknoweval .

------------

`[2406.09103] Chain-of-Though (CoT) prompting strategies for medical error detection and correction <https://arxiv.org/abs/2406.09103>`__ 面向医疗差错检测与纠正的CoT激励策略

::

    Thu, 13 Jun 2024 13:31:04 GMT
    Zhaolong Wu, Abul Hasan, Jinge Wu, Yunsoo Kim, Jason P.Y. Cheung, Teng Zhang, Honghan Wu

This paper describes our submission to the MEDIQA-CORR 2024 shared task for automatically detecting and correcting medical errors in clinical notes. We report results for three methods of few-shot In-Context Learning (ICL) augmented with Chain-of-Thought (CoT) and reason prompts using a large language model (LLM). In the first method, we manually analyse a subset of train and validation dataset to infer three CoT prompts by examining error types in the clinical notes. In the second method, we utilise the training dataset to prompt the LLM to deduce reasons about their correctness or incorrectness. The constructed CoTs and reasons are then augmented with ICL examples to solve the tasks of error detection, span identification, and error correction. Finally, we combine the two methods using a rule-based ensemble method. Across the three sub-tasks, our ensemble method achieves a ranking of 3rd for both sub-task 1 and 2, while securing 7th place in sub-task 3 among all submissions.

------------

`[2406.09140] Investigating the translation capabilities of Large Language Models trained on parallel data only <https://arxiv.org/abs/2406.09140>`__ 仅研究在并行数据上训练的大型语言模型的翻译能力

::

    Thu, 13 Jun 2024 14:08:56 GMT
    Javier Garc\'ia Gilabert, Carlos Escolano, Aleix Sant Savall, Francesca De Luca Fornaciari, Audrey Mash, Xixian Liao, Maite Melero

In recent years, Large Language Models (LLMs) have demonstrated exceptional proficiency across a broad spectrum of Natural Language Processing (NLP) tasks, including Machine Translation. However, previous methods predominantly relied on iterative processes such as instruction fine-tuning or continual pre-training, leaving unexplored the challenges of training LLMs solely on parallel data. In this work, we introduce PLUME (Parallel Language Model), a collection of three 2B LLMs featuring varying vocabulary sizes (32k, 128k, and 256k) trained exclusively on Catalan-centric parallel examples. These models perform comparably to previous encoder-decoder architectures on 16 supervised translation directions and 56 zero-shot ones. Utilizing this set of models, we conduct a thorough investigation into the translation capabilities of LLMs, probing their performance, the impact of the different elements of the prompt, and their cross-lingual representation space.

------------

`[2406.09155] DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation <https://arxiv.org/abs/2406.09155>`__ DefAn:用于llm幻觉评估的确定答案数据集

::

    Thu, 13 Jun 2024 14:18:13 GMT
    A B M Ashikur Rahman, Saeed Anwar, Muhammad Usman, Ajmal Mian

Large Language Models (LLMs) have demonstrated remarkable capabilities, revolutionizing the integration of AI in daily life applications. However, they are prone to hallucinations, generating claims that contradict established facts, deviating from prompts, and producing inconsistent responses when the same prompt is presented multiple times. Addressing these issues is challenging due to the lack of comprehensive and easily assessable benchmark datasets. Most existing datasets are small and rely on multiple-choice questions, which are inadequate for evaluating the generative prowess of LLMs. To measure hallucination in LLMs, this paper introduces a comprehensive benchmark dataset comprising over 75,000 prompts across eight domains. These prompts are designed to elicit definitive, concise, and informative answers. The dataset is divided into two segments: one publicly available for testing and assessing LLM performance and a hidden segment for benchmarking various LLMs. In our experiments, we tested six LLMs-GPT-3.5, LLama 2, LLama 3, Gemini, Mixtral, and Zephyr-revealing that overall factual hallucination ranges from 59% to 82% on the public dataset and 57% to 76% in the hidden benchmark. Prompt misalignment hallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the hidden counterpart. Average consistency ranges from 21% to 61% and 22% to 63%, respectively. Domain-wise analysis shows that LLM performance significantly deteriorates when asked for specific numeric information while performing moderately with person, location, and date queries. Our dataset demonstrates its efficacy and serves as a comprehensive benchmark for LLM performance evaluation. Our dataset and LLMs responses are available at \href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn}.

------------

`[2406.09205] ReadCtrl: Personalizing text generation with readability-controlled instruction learning <https://arxiv.org/abs/2406.09205>`__ ReadCtrl:具有可读性控制的教学学习个性化文本生成

::

    Thu, 13 Jun 2024 15:03:46 GMT
    Hieu Tran, Zonghai Yao, Lingxi Li, Hong Yu

Content generation conditioning on users's readability is an important application for personalization. In an era of large language models (LLMs), readability-controlled text generation based on LLMs has become increasingly important. This paper introduces a novel methodology called "Readability-Controlled Instruction Learning (ReadCtrl)," which aims to instruction-tune LLMs to tailor users' readability levels. Unlike the traditional methods, which primarily focused on categorical readability adjustments typically classified as high, medium, and low or expert and layperson levels with limited success, ReadCtrl introduces a dynamic framework that enables LLMs to generate content at various (near continuous level) complexity levels, thereby enhancing their versatility across different applications. Our results show that the ReadCtrl-Mistral-7B models significantly outperformed strong baseline models such as GPT-4 and Claude-3, with a win rate of 52.1%:35.7% against GPT-4 in human evaluations. Furthermore, Read-Ctrl has shown significant improvements in automatic evaluations, as evidenced by better readability metrics (e.g., FOG, FKGL) and generation quality metrics (e.g., BLEU, SARI, SummaC-Factuality, UniEval-Consistency and Coherence). These results underscore Read-Ctrl's effectiveness and tenacity in producing high-quality, contextually appropriate outputs that closely align with targeted readability levels, marking a significant advancement in personalized content generation using LLMs.

------------

`[2406.09265] Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs <https://arxiv.org/abs/2406.09265>`__ 共享问题:在llm中跨语言和任务分析神经元

::

    Thu, 13 Jun 2024 16:04:11 GMT
    Weixuan Wang, Barry Haddow, Wei Peng, Alexandra Birch

Multilingual large language models (LLMs) have greatly increased the ceiling of performance on non-English tasks. However the mechanisms behind multilingualism in these LLMs are poorly understood. Of particular interest is the degree to which internal representations are shared between languages.
Recent work on neuron analysis of LLMs has focused on the monolingual case, and the limited work on the multilingual case has not considered the interaction between tasks and linguistic representations. In our work, we investigate how neuron activation is shared across languages by categorizing neurons into four distinct groups according to their responses across different languages for a particular input: all-shared, partial-shared, specific, and non-activated. This categorization is combined with a study of neuron attribution, i.e. the importance of a neuron w.r.t an output. Our analysis reveals the following insights: (i) the linguistic sharing patterns are strongly affected by the type of task, but neuron behaviour changes across different inputs even for the same task; (ii) all-shared neurons play a key role in generating correct responses; (iii) boosting multilingual alignment by increasing all-shared neurons can enhance accuracy on multilingual tasks. The code is available at https://github.com/weixuan-wang123/multilingual-neurons.

------------

`[2406.09282] On the Effects of Heterogeneous Data Sources on Speech-to-Text Foundation Models <https://arxiv.org/abs/2406.09282>`__ 异构数据源对语音-文本基础模型的影响

::

    Thu, 13 Jun 2024 16:22:37 GMT
    Jinchuan Tian, Yifan Peng, William Chen, Kwanghee Choi, Karen Livescu, Shinji Watanabe

The Open Whisper-style Speech Model (OWSM) series was introduced to achieve full transparency in building advanced speech-to-text (S2T) foundation models.
To this end, OWSM models are trained on 25 public speech datasets, which are heterogeneous in multiple ways. In this study, we advance the OWSM series by introducing OWSM v3.2, which improves on prior models by investigating and addressing the impacts of this data heterogeneity. Our study begins with a detailed analysis of each dataset, from which we derive two key strategies: data filtering with proxy task to enhance data quality, and the incorporation of punctuation and true-casing using an open large language model (LLM). With all other configurations staying the same, OWSM v3.2 improves performance over the OWSM v3.1 baseline while using 15% less training data.

------------

`[2406.09289] Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models <https://arxiv.org/abs/2406.09289>`__ 理解越狱成功:大型语言模型潜空间动力学研究

::

    Thu, 13 Jun 2024 16:26:47 GMT
    Sarah Ball, Frauke Kreuter, Nina Rimsky

Conversational Large Language Models are trained to refuse to answer harmful questions. However, emergent jailbreaking techniques can still elicit unsafe outputs, presenting an ongoing challenge for model alignment. To better understand how different jailbreak types circumvent safeguards, this paper analyses model activations on different jailbreak inputs. We find that it is possible to extract a jailbreak vector from a single class of jailbreaks that works to mitigate jailbreak effectiveness from other classes. This may indicate that different kinds of effective jailbreaks operate via similar internal mechanisms. We investigate a potential common mechanism of harmfulness feature suppression, and provide evidence for its existence by looking at the harmfulness vector component. These findings offer actionable insights for developing more robust jailbreak countermeasures and lay the groundwork for a deeper, mechanistic understanding of jailbreak dynamics in language models.

------------

`[2406.09325] REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space <https://arxiv.org/abs/2406.09325>`__ REVS:基于词汇空间排序编辑的语言模型敏感信息遗忘

::

    Thu, 13 Jun 2024 17:02:32 GMT
    Tomer Ashuach, Martin Tutek, Yonatan Belinkov

Large language models (LLMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset scrubbing, or model filtering through unlearning and model editing, which can be bypassed through extraction attacks. We propose REVS, a novel model editing method for unlearning sensitive information from LLMs. REVS identifies and modifies a small subset of neurons relevant for each piece of sensitive information. By projecting these neurons to the vocabulary space (unembedding), we pinpoint the components driving its generation. We then compute a model edit based on the pseudo-inverse of the unembedding matrix, and apply it to de-promote generation of the targeted sensitive data. To adequately evaluate our method on truly sensitive information, we curate two datasets: an email dataset inherently memorized by GPT-J, and a synthetic social security number dataset that we tune the model to memorize. Compared to other state-of-the-art model editing methods, REVS demonstrates superior performance in both eliminating sensitive information and robustness to extraction attacks, while retaining integrity of the underlying model. The code and a demo notebook are available at https://technion-cs-nlp.github.io/REVS.

------------

`[2406.09330] Learning from Natural Language Explanations for Generalizable Entity Matching <https://arxiv.org/abs/2406.09330>`__ 基于自然语言解释的泛化实体匹配学习

::

    Thu, 13 Jun 2024 17:08:58 GMT
    Somin Wadhwa, Adit Krishnan, Runhui Wang, Byron C. Wallace, Chris Kong

Entity matching is the task of linking records from different sources that refer to the same real-world entity. Past work has primarily treated entity linking as a standard supervised learning problem. However, supervised entity matching models often do not generalize well to new data, and collecting exhaustive labeled training data is often cost prohibitive. Further, recent efforts have adopted LLMs for this task in few/zero-shot settings, exploiting their general knowledge. But LLMs are prohibitively expensive for performing inference at scale for real-world entity matching tasks.
As an efficient alternative, we re-cast entity matching as a conditional generation task as opposed to binary classification. This enables us to "distill" LLM reasoning into smaller entity matching models via natural language explanations. This approach achieves strong performance, especially on out-of-domain generalization tests (10.85% F-1) where standalone generative methods struggle. We perform ablations that highlight the importance of explanations, both for performance and model robustness.

------------

`[2406.09345] DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding <https://arxiv.org/abs/2406.09345>`__ DiscreteSLU:基于自监督离散语音单元的口语理解大型语言模型

::

    Thu, 13 Jun 2024 17:28:13 GMT
    Suwon Shon, Kwangyoun Kim, Yi-Te Hsu, Prashant Sridhar, Shinji Watanabe, Karen Livescu

The integration of pre-trained text-based large language models (LLM) with speech input has enabled instruction-following capabilities for diverse speech tasks. This integration requires the use of a speech encoder, a speech adapter, and an LLM, trained on diverse tasks. We propose the use of discrete speech units (DSU), rather than continuous-valued speech encoder outputs, that are converted to the LLM token embedding space using the speech adapter. We generate DSU using a self-supervised speech encoder followed by k-means clustering. The proposed model shows robust performance on speech inputs from seen/unseen domains and instruction-following capability in spoken question answering. We also explore various types of DSU extracted from different layers of the self-supervised speech encoder, as well as Mel frequency Cepstral Coefficients (MFCC). Our findings suggest that the ASR task and datasets are not crucial in instruction-tuning for spoken question answering tasks.

------------

`[2406.08765] LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices <https://arxiv.org/abs/2406.08765>`__ 边缘计算设备上基于llm的时序数据分析知识剪枝

::

    Thu, 13 Jun 2024 02:51:18 GMT
    Ruibing Jin, Qing Xu, Min Wu, Yuecong Xu, Dan Li, Xiaoli Li, Zhenghua Chen

Limited by the scale and diversity of time series data, the neural networks trained on time series data often overfit and show unsatisfacotry performances.
In comparison, large language models (LLMs) recently exhibit impressive generalization in diverse fields. Although massive LLM based approaches are proposed for time series tasks, these methods require to load the whole LLM in both training and reference. This high computational demands limit practical applications in resource-constrained settings, like edge-computing and IoT devices. To address this issue, we propose Knowledge Pruning (KP), a novel paradigm for time series learning in this paper. For a specific downstream task, we argue that the world knowledge learned by LLMs is much redundant and only the related knowledge termed as "pertinent knowledge" is useful. Unlike other methods, our KP targets to prune the redundant knowledge and only distill the pertinent knowledge into the target model. This reduces model size and computational costs significantly. Additionally, different from existing LLM based approaches, our KP does not require to load the LLM in the process of training and testing, further easing computational burdens. With our proposed KP, a lightweight network can effectively learn the pertinent knowledge, achieving satisfactory performances with a low computation cost. To verify the effectiveness of our KP, two fundamental tasks on edge-computing devices are investigated in our experiments, where eight diverse environments or benchmarks with different networks are used to verify the generalization of our KP.
Through experiments, our KP demonstrates effective learning of pertinent knowledge, achieving notable performance improvements in regression (19.7% on average) and classification (up to 13.7%) tasks, showcasing state-of-the-art results.

------------

`[2406.08862] Cognitively Inspired Energy-Based World Models <https://arxiv.org/abs/2406.08862>`__ 认知启发的基于能量的世界模型

::

    Thu, 13 Jun 2024 06:54:37 GMT
    Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam, Aman Chadha, Jundong Li, Tariq Iqbal

One of the predominant methods for training world models is autoregressive prediction in the output space of the next element of a sequence. In Natural Language Processing (NLP), this takes the form of Large Language Models (LLMs) predicting the next token; in Computer Vision (CV), this takes the form of autoregressive models predicting the next frame/token/pixel. However, this approach differs from human cognition in several respects. First, human predictions about the future actively influence internal cognitive processes.
Second, humans naturally evaluate the plausibility of predictions regarding future states. Based on this capability, and third, by assessing when predictions are sufficient, humans allocate a dynamic amount of time to make a prediction. This adaptive process is analogous to System 2 thinking in psychology. All these capabilities are fundamental to the success of humans at high-level reasoning and planning. Therefore, to address the limitations of traditional autoregressive models lacking these human-like capabilities, we introduce Energy-Based World Models (EBWM). EBWM involves training an Energy-Based Model (EBM) to predict the compatibility of a given context and a predicted future state. In doing so, EBWM enables models to achieve all three facets of human cognition described. Moreover, we developed a variant of the traditional autoregressive transformer tailored for Energy-Based models, termed the Energy-Based Transformer (EBT). Our results demonstrate that EBWM scales better with data and GPU Hours than traditional autoregressive transformers in CV, and that EBWM offers promising early scaling in NLP. Consequently, this approach offers an exciting path toward training future models capable of System 2 thinking and intelligently searching across state spaces.

------------

`[2406.09179] Unlearning with Control: Assessing Real-world Utility for Large Language Model Unlearning <https://arxiv.org/abs/2406.09179>`__ 带控制的遗忘:评估大型语言模型遗忘的现实效用

::

    Thu, 13 Jun 2024 14:41:00 GMT
    Qizhou Wang, Bo Han, Puning Yang, Jianing Zhu, Tongliang Liu, Masashi Sugiyama

The compelling goal of eradicating undesirable data behaviors, while preserving usual model functioning, underscores the significance of machine unlearning within the domain of large language models (LLMs). Recent research has begun to approach LLM unlearning via gradient ascent (GA) -- increasing the prediction risk for those training strings targeted to be unlearned, thereby erasing their parameterized responses. Despite their simplicity and efficiency, we suggest that GA-based methods face the propensity towards excessive unlearning, resulting in various undesirable model behaviors, such as catastrophic forgetting, that diminish their practical utility. In this paper, we suggest a set of metrics that can capture multiple facets of real-world utility and propose several controlling methods that can regulate the extent of excessive unlearning. Accordingly, we suggest a general framework to better reflect the practical efficacy of various unlearning methods -- we begin by controlling the unlearning procedures/unlearned models such that no excessive unlearning occurs and follow by the evaluation for unlearning efficacy. Our experimental analysis on established benchmarks revealed that GA-based methods are far from perfect in practice, as strong unlearning is at the high cost of hindering the model utility. We conclude that there is still a long way towards practical and effective LLM unlearning, and more efforts are required in this field.

------------

`[2406.09288] Zero-Shot Learning Over Large Output Spaces : Utilizing Indirect Knowledge Extraction from Large Language Models <https://arxiv.org/abs/2406.09288>`__ 大输出空间上的零样本学习:利用从大型语言模型中间接提取知识

::

    Thu, 13 Jun 2024 16:26:37 GMT
    Jinbin Zhang, Nasib Ullah, Rohit Babbar

Extreme Multi-label Learning (XMC) is a task that allocates the most relevant labels for an instance from a predefined label set. Extreme Zero-shot XMC (EZ-XMC) is a special setting of XMC wherein no supervision is provided; only the instances (raw text of the document) and the predetermined label set are given. The scenario is designed to address cold-start problems in categorization and recommendation. Traditional state-of-the-art methods extract pseudo labels from the document title or segments. These labels from the document are used to train a zero-shot bi-encoder model. The main issue with these generated labels is their misalignment with the tagging task. In this work, we propose a framework to train a small bi-encoder model via the feedback from the large language model (LLM), the bi-encoder model encodes the document and labels into embeddings for retrieval. Our approach leverages the zero-shot ability of LLM to assess the correlation between labels and the document instead of using the low-quality labels extracted from the document itself. Our method also guarantees fast inference without the involvement of LLM. The performance of our approach outperforms the SOTA methods on various datasets while retaining a similar training time for large datasets.

------------

`[2406.08665] Exploring Fuzzing as Data Augmentation for Neural Test Generation <https://arxiv.org/abs/2406.08665>`__ 探索模糊测试作为神经测试生成的数据增强

::

    Wed, 12 Jun 2024 22:09:27 GMT
    Yifeng He, Jicheng Wang, Yuyang Rong, Hao Chen

Testing is an essential part of modern software engineering to build reliable programs. As testing the software is important but expensive, automatic test case generation methods have become popular in software development. Unlike traditional search-based coverage-guided test generation like fuzzing, neural test generation backed by large language models can write tests that are semantically meaningful and can be understood by other maintainers. However, compared to regular code corpus, unit tests in the datasets are limited in amount and diversity. In this paper, we present a novel data augmentation technique **FuzzAug**, that combines the advantages of fuzzing and large language models. FuzzAug not only keeps valid program semantics in the augmented data, but also provides more diverse inputs to the function under test, helping the model to associate correct inputs embedded with the function's dynamic behaviors with the function under test. We evaluate FuzzAug's benefits by using it on a neural test generation dataset to train state-of-the-art code generation models. By augmenting the training set, our model generates test cases with $11\%$ accuracy increases. Models trained with FuzzAug generate unit test functions with double the branch coverage compared to those without it. FuzzAug can be used across various datasets to train advanced code generation models, enhancing their utility in automated software testing. Our work shows the benefits of using dynamic analysis results to enhance neural test generation. Code and data will be publicly available.

------------

`[2406.08824] LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions <https://arxiv.org/abs/2406.08824>`__ llm驱动的机器人有可能实施歧视、暴力和非法行为

::

    Thu, 13 Jun 2024 05:31:49 GMT
    Rumaisa Azeem, Andrew Hundt, Masoumeh Mansouri, Martim Brand\~ao

Members of the Human-Robot Interaction (HRI) and Artificial Intelligence (AI) communities have proposed Large Language Models (LLMs) as a promising resource for robotics tasks such as natural language interactions, doing household and workplace tasks, approximating `common sense reasoning', and modeling humans.
However, recent research has raised concerns about the potential for LLMs to produce discriminatory outcomes and unsafe behaviors in real-world robot experiments and applications. To address these concerns, we conduct an HRI-based evaluation of discrimination and safety criteria on several highly-rated LLMs. Our evaluation reveals that LLMs currently lack robustness when encountering people across a diverse range of protected identity characteristics (e.g., race, gender, disability status, nationality, religion, and their intersections), producing biased outputs consistent with directly discriminatory outcomes -- e.g. `gypsy' and `mute' people are labeled untrustworthy, but not `european' or `able-bodied' people. Furthermore, we test models in settings with unconstrained natural language (open vocabulary) inputs, and find they fail to act safely, generating responses that accept dangerous, violent, or unlawful instructions -- such as incident-causing misstatements, taking people's mobility aids, and sexual predation. Our results underscore the urgent need for systematic, routine, and comprehensive risk assessments and assurances to improve outcomes and ensure LLMs only operate on robots when it is safe, effective, and just to do so. Data and code will be made available.

------------

`[2406.09401] MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations <https://arxiv.org/abs/2406.09401>`__ MMScan:具有层次基础语言标注的多模态3D场景数据集

::

    Thu, 13 Jun 2024 17:59:30 GMT
    Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, Jiangmiao Pang

With the emergence of LLMs and their integration with other data modalities, multi-modal 3D perception attracts more attention due to its connectivity to the physical world and makes rapid progress. However, limited by existing datasets, previous works mainly focus on understanding object properties or inter-object spatial relationships in a 3D scene. To tackle this problem, this paper builds the first largest ever multi-modal 3D scene dataset and benchmark with hierarchical grounded language annotations, MMScan. It is constructed based on a top-down logic, from region to object level, from a single target to inter-target relationships, covering holistic aspects of spatial and attribute understanding. The overall pipeline incorporates powerful VLMs via carefully designed prompts to initialize the annotations efficiently and further involve humans' correction in the loop to ensure the annotations are natural, correct, and comprehensive. Built upon existing 3D scanning data, the resulting multi-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects and 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding and question-answering benchmarks. We evaluate representative baselines on our benchmarks, analyze their capabilities in different aspects, and showcase the key problems to be addressed in the future. Furthermore, we use this high-quality dataset to train state-of-the-art 3D visual grounding and LLMs and obtain remarkable performance improvement both on existing benchmarks and in-the-wild evaluation. Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.

------------

`[2406.09412] Explore the Limits of Omni-modal Pretraining at Scale <https://arxiv.org/abs/2406.09412>`__ 探索大规模全模态预训练的局限性

::

    Thu, 13 Jun 2024 17:59:53 GMT
    Yiyuan Zhang, Handong Li, Jing Liu, Xiangyu Yue

We propose to build omni-modal intelligence, which is capable of understanding any modality and learning universal representations. In specific, we propose a scalable pretraining paradigm, named Multimodal Context (MiCo), which can scale up the numbers of modalities and amount of data, together with the model parameters, in the pretraining process. With MiCo, the pretrained models show significant emergent abilities in multimodal learning, which are evaluated on the following tasks: i) single-modality perception benchmarks of 10 different modalities, ii) 25 cross-modality understanding tasks of retrieval, question-answering, captioning, and iii) 18 multimodal large language model benchmarks. Our models establish 37 new records for state-of-the-art performance. We hope that our research could contribute to the development of omni-modal intelligence. Code and Models are at https://github.com/invictus717/MiCo

------------

`[2406.09067] How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models <https://arxiv.org/abs/2406.09067>`__ 基于transformer的视觉编码器中的表示结构如何?视觉-语言模型中的多目标表示分析

::

    Thu, 13 Jun 2024 12:54:20 GMT
    Tarun Khajuria, Braian Olmiro Dias, Jaan Aru

Forming and using symbol-like structured representations for reasoning has been considered essential for generalising over novel inputs. The primary tool that allows generalisation outside training data distribution is the ability to abstract away irrelevant information into a compact form relevant to the task.
An extreme form of such abstract representations is symbols. Humans make use of symbols to bind information while abstracting away irrelevant parts to utilise the information consistently and meaningfully. This work estimates the state of such structured representations in vision encoders. Specifically, we evaluate image encoders in large vision-language pre-trained models to address the question of which desirable properties their representations lack by applying the criteria of symbolic structured reasoning described for LLMs to the image models. We test the representation space of image encoders like VIT, BLIP, CLIP, and FLAVA to characterise the distribution of the object representations in these models. In particular, we create decoding tasks using multi-object scenes from the COCO dataset, relating the token space to its input content for various objects in the scene. We use these tasks to characterise the network's token and layer-wise information modelling. Our analysis highlights that the CLS token, used for the downstream task, only focuses on a few objects necessary for the trained downstream task. Still, other individual objects are well-modelled separately by the tokens in the network originating from those objects. We further observed a widespread distribution of scene information.
This demonstrates that information is far more entangled in tokens than optimal for representing objects similar to symbols. Given these symbolic properties, we show the network dynamics that cause failure modes of these models on basic downstream tasks in a multi-object scene.

------------

`[2309.17057] Tell Me a Story! Narrative-Driven XAI with Large Language Models <https://arxiv.org/abs/2309.17057>`__ 给我讲个故事!叙事驱动的XAI与大型语言模型

::

    replaced with revised version Wed, 12 Jun 2024 18:18:37 GMT
    Submission history From: James Hinns [view email]
    [v1] Fri, 29 Sep 2023 08:40:08 UTC (2,348 KB)
    [v2] Wed, 12 Jun 2024 18:18:37 UTC (2,362 KB)
    David Martens, James Hinns, Camille Dams, Mark Vergouwen and Theodoros Evgeniou

In many AI applications today, the predominance of black-box machine learning models, due to their typically higher accuracy, amplifies the need for Explainable AI (XAI). Existing XAI approaches, such as the widely used SHAP values or counterfactual (CF) explanations, are arguably often too technical for users to understand and act upon. To enhance comprehension of explanations of AI decisions and the overall user experience, we introduce XAIstories, which leverage Large Language Models to provide narratives about how AI predictions are made: SHAPstories do so based on SHAP explanations, while CFstories do so for CF explanations. We study the impact of our approach on users' experience and understanding of AI predictions. Our results are striking: over 90% of the surveyed general audience finds the narratives generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories in communicating explanations to a general audience, with 83% of data scientists indicating they are likely to use SHAPstories for this purpose. In an image classification setting, CFstories are considered more or equally convincing as the users' own crafted stories by more than 75% of the participants. CFstories additionally bring a tenfold speed gain in creating a narrative. We also find that SHAPstories help users to more accurately summarize and understand AI decisions, in a credit scoring setting we test, correctly answering comprehension questions significantly more often than they do when only SHAP values are provided. The results thereby suggest that XAIstories may significantly help explaining and understanding AI predictions, ultimately supporting better decision-making in various applications.

------------

`[2403.03744] MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models <https://arxiv.org/abs/2403.03744>`__ MedSafetyBench:评估和改进大型语言模型的医疗安全性

::

    replaced with revised version Thu, 13 Jun 2024 15:58:33 GMT
    Submission history From: Tessa Han [view email]
    [v1] Wed, 6 Mar 2024 14:34:07 UTC (41 KB)
    [v2] Wed, 1 May 2024 12:24:04 UTC (386 KB)
    [v3] Tue, 14 May 2024 00:30:54 UTC (386 KB)
    [v4] Thu, 13 Jun 2024 15:58:33 UTC (376 KB)
    Tessa Han, Aounon Kumar, Chirag Agarwal, Himabindu Lakkaraju

As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. To address this gap, we first define the notion of medical safety in LLMs based on the Principles of Medical Ethics set forth by the American Medical Association. We then leverage this understanding to introduce MedSafetyBench, the first benchmark dataset specifically designed to measure the medical safety of LLMs. We demonstrate the utility of MedSafetyBench by using it to evaluate and improve the medical safety of LLMs. Our results show that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using MedSafetyBench improves their medical safety. By introducing this new benchmark dataset, our work enables a systematic study of the state of medical safety in LLMs and motivates future work in this area, thereby mitigating the safety risks of LLMs in medicine.

------------

`[2406.06870] What's in an embedding? Would a rose by any embedding smell as sweet? <https://arxiv.org/abs/2406.06870>`__ 嵌入中有什么?任何一种嵌入的玫瑰闻起来都那么香吗?

::

    replaced with revised version Wed, 12 Jun 2024 18:38:13 GMT
    Submission history From: Venkat Venkatasubramanian [view email]
    [v1] Tue, 11 Jun 2024 01:10:40 UTC (267 KB)
    [v2] Wed, 12 Jun 2024 18:38:13 UTC (268 KB)
    Venkat Venkatasubramanian

Large Language Models (LLMs) are often criticized for lacking true "understanding" and an ability to "reason" with their knowledge, being seen merely as advanced autocomplete systems. We believe that this perspective might be missing an important insight. We suggest that LLMs do develop a kind of empirical "understanding" that is "geometry"-like, which seems quite sufficient for a range of applications in NLP, computer vision, coding assistance, etc. However, this "geometric" understanding, built from incomplete and noisy data, makes them unreliable, difficult to generalize, and lacking in inference capabilities and explanations, similar to the challenges faced by heuristics-based expert systems decades ago.
To overcome these limitations, we suggest that LLMs should be integrated with an "algebraic" representation of knowledge that includes symbolic AI elements used in expert systems. This integration aims to create large knowledge models (LKMs) that not only possess "deep" knowledge grounded in first principles, but also have the ability to reason and explain, mimicking human expert capabilities. To harness the full potential of generative AI safely and effectively, a paradigm shift from LLMs to the more comprehensive LKMs is needed.

------------

`[2406.07394] Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B <https://arxiv.org/abs/2406.07394>`__ 利用LLaMa-3 8B通过蒙特卡罗树自求精访问GPT-4级数学奥林匹克解

::

    replaced with revised version Thu, 13 Jun 2024 07:19:06 GMT
    Submission history From: Di Zhang [view email]
    [v1] Tue, 11 Jun 2024 16:01:07 UTC (106 KB)
    [v2] Thu, 13 Jun 2024 07:19:06 UTC (106 KB)
    Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang

This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.

------------

`[2305.12057] Accurate Knowledge Distillation with n-best Reranking <https://arxiv.org/abs/2305.12057>`__ 基于n-best重排序的精确知识蒸馏

::

    replaced with revised version Wed, 12 Jun 2024 18:28:01 GMT
    Submission history From: Hendra Setiawan [view email]
    [v1] Sat, 20 May 2023 01:53:03 UTC (116 KB)
    [v2] Tue, 14 Nov 2023 21:02:57 UTC (7,711 KB)
    [v3] Sun, 21 Apr 2024 22:19:51 UTC (7,715 KB)
    [v4] Wed, 12 Jun 2024 18:28:01 UTC (7,715 KB)
    Hendra Setiawan

We propose utilizing n-best reranking to enhance Sequence-Level Knowledge Distillation (Kim and Rush, 2016) where we extract pseudo-labels for student model's training data from top n-best hypotheses and leverage a diverse set of models with different inductive biases, objective functions or architectures, including some publicly-available large language models, to pick the highest-quality hypotheses as labels. The effectiveness of our proposal is validated through experiments on the WMT'21 German-English and Chinese-English translation tasks. Our results demonstrate that utilizing pseudo-labels generated by our n-best reranker leads to a significantly more accurate student model. In fact, our best student model achieves comparable accuracy to a large translation model from (Tran et al., 2021) with 4.7 billion parameters, while having two orders of magnitude fewer parameters.

------------

`[2305.13669] The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models <https://arxiv.org/abs/2305.13669>`__ 知识对齐问题:为大型语言模型连接人类和外部知识

::

    replaced with revised version Thu, 13 Jun 2024 03:44:03 GMT
    Submission history From: Shuo Zhang [view email]
    [v1] Tue, 23 May 2023 04:22:50 UTC (10,587 KB)
    [v2] Fri, 3 Nov 2023 17:40:21 UTC (7,610 KB)
    [v3] Thu, 13 Jun 2024 03:44:03 UTC (8,201 KB)
    Shuo Zhang, Liangming Pan, Junzhou Zhao, William Yang Wang

Large language models often necessitate grounding on external knowledge to generate faithful and reliable answers. Yet even with the correct groundings in the reference, they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings. In this work, we formulate this knowledge alignment problem and introduce MixAlign, a framework that interacts with both the human user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic knowledge alignment and, if necessary, further enhances this alignment through human user clarifications. Experimental results highlight the crucial role of knowledge alignment in boosting model performance and mitigating hallucination, with improvements noted up to 22.2% and 27.1% respectively. We also demonstrate the effectiveness of MixAlign in improving knowledge alignment by producing high-quality, user-centered clarifications.

------------

`[2309.00237] Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes <https://arxiv.org/abs/2309.00237>`__ 基于合成临床笔记构建的可公开共享临床大型语言模型

::

    replaced with revised version Thu, 13 Jun 2024 05:04:33 GMT
    Submission history From: Sunjun Kweon [view email]
    [v1] Fri, 1 Sep 2023 04:01:20 UTC (600 KB)
    [v2] Wed, 6 Sep 2023 18:11:15 UTC (600 KB)
    [v3] Thu, 13 Jun 2024 05:04:33 UTC (8,228 KB)
    Sunjun Kweon, Junu Kim, Jiyoun Kim, Sujeong Im, Eunbyeol Cho, Seongsu Bae, Jungwoo Oh, Gyubok Lee, Jong Hak Moon, Seng Chan You, Seungjin Baek, Chang Hoon Han, Yoon Bin Jung, Yohan Jo, Edward Choi

The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructing high-performing clinical language models. This conclusion is supported by detailed evaluations conducted by both GPT-4 and medical professionals. All resources including weights, codes, and data used in the development of Asclepius are made publicly accessible for future research. (this https URL)

------------

`[2311.03099] Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch <https://arxiv.org/abs/2311.03099>`__ 语言模型是超级马里奥:从同源模型中吸收能力就像免费的午餐

::

    replaced with revised version Thu, 13 Jun 2024 11:56:04 GMT
    Submission history From: Bowen Yu [view email]
    [v1] Mon, 6 Nov 2023 13:43:07 UTC (31,034 KB)
    [v2] Sun, 4 Feb 2024 16:28:06 UTC (31,174 KB)
    [v3] Thu, 13 Jun 2024 11:56:04 UTC (28,206 KB)
    Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li

In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard.

------------

`[2311.08348] MC$^2$: Towards Transparent and Culturally-Aware NLP for Minority Languages in China <https://arxiv.org/abs/2311.08348>`__ MC$^2$:面向中国少数民族语言的透明和文化意识的自然语言处理

::

    replaced with revised version Thu, 13 Jun 2024 04:36:11 GMT
    Submission history From: Chen Zhang [view email]
    [v1] Tue, 14 Nov 2023 17:45:50 UTC (7,716 KB)
    [v2] Thu, 13 Jun 2024 04:36:11 UTC (8,534 KB)
    Chen Zhang, Mingxu Tao, Quzhe Huang, Jiuheng Lin, Zhibin Chen, Yansong Feng

Current large language models demonstrate deficiencies in understanding low-resource languages, particularly the minority languages in China. This limitation stems from the scarcity of available pre-training data. To address this accessibility challenge, we present MC$^2$, a Multilingual Corpus of Minority Languages in China, which is the largest open-source corpus of its kind so far. MC$^2$ includes four underrepresented languages: Tibetan, Uyghur, Kazakh, and Mongolian. Notably, we focus on the less common writing systems of Kazakh and Mongolian, i.e., Kazakh Arabic script and traditional Mongolian script, respectively, which have been long neglected in previous corpus construction efforts. Recognizing the prevalence of language contamination within existing corpora, we adopt a quality-centric solution for collecting MC$^2$, prioritizing accuracy while enhancing diversity. Furthermore, we underscore the importance of attending to the multiplicity of writing systems, which is closely related to the cultural awareness of the resulting models. The MC$^2$ corpus and related models are made public to the community.

------------

`[2402.04333] LESS: Selecting Influential Data for Targeted Instruction Tuning <https://arxiv.org/abs/2402.04333>`__ LESS:选择有影响力的数据进行定向指令调优

::

    replaced with revised version Thu, 13 Jun 2024 03:42:02 GMT
    Submission history From: Mengzhou Xia [view email]
    [v1] Tue, 6 Feb 2024 19:18:04 UTC (1,784 KB)
    [v2] Tue, 20 Feb 2024 02:24:09 UTC (1,784 KB)
    [v3] Thu, 13 Jun 2024 03:42:02 UTC (1,803 KB)
    Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, Danqi Chen

Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application.

------------

`[2402.10986] FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models <https://arxiv.org/abs/2402.10986>`__ FinTral: GPT-4级多模态金融大型语言模型族

::

    replaced with revised version Thu, 13 Jun 2024 17:24:50 GMT
    Submission history From: Gagan Bhatia [view email]
    [v1] Fri, 16 Feb 2024 05:05:12 UTC (16,907 KB)
    [v2] Thu, 13 Jun 2024 17:24:50 UTC (16,907 KB)
    [v3] Fri, 14 Jun 2024 13:26:47 UTC (16,907 KB)
    Gagan Bhatia, El Moatez Billah Nagoudi, Hasan Cavusoglu, Muhammad Abdul-Mageed

We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to excel in real-time analysis and decision-making in diverse financial contexts. The GitHub repository for FinTral is available at \url{this https URL}.

------------

`[2402.19167] Teaching Large Language Models an Unseen Language on the Fly <https://arxiv.org/abs/2402.19167>`__ 教学大型语言模型是一种无形的语言

::

    replaced with revised version Thu, 13 Jun 2024 04:58:21 GMT
    Submission history From: Chen Zhang [view email]
    [v1] Thu, 29 Feb 2024 13:50:47 UTC (8,513 KB)
    [v2] Thu, 13 Jun 2024 04:58:21 UTC (8,440 KB)
    Chen Zhang, Xiao Liu, Jiuheng Lin, Yansong Feng

Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones, for which there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce DiPMT++, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and 5K parallel sentences only, DiPMT++ significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. We also validate the effectiveness of our framework on Kalamang, another unseen language. Furthermore, we demonstrate the practical utility of DiPMT++ in aiding humans in translating completely unseen languages, which could contribute to the preservation of linguistic diversity.

------------

`[2403.00835] CLLMs: Consistency Large Language Models <https://arxiv.org/abs/2403.00835>`__ CLLMs:一致性大型语言模型

::

    replaced with revised version Thu, 13 Jun 2024 08:41:28 GMT
    Submission history From: Lanxiang Hu [view email]
    [v1] Wed, 28 Feb 2024 20:17:04 UTC (1,239 KB)
    [v2] Tue, 5 Mar 2024 08:01:01 UTC (1,238 KB)
    [v3] Fri, 8 Mar 2024 00:13:31 UTC (1,239 KB)
    [v4] Thu, 13 Jun 2024 08:41:28 UTC (1,240 KB)
    Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, Hao Zhang

Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.

------------

`[2403.01924] To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering <https://arxiv.org/abs/2403.01924>`__ 生成还是检索?人工语境在医学开放领域问答中的有效性

::

    replaced with revised version Thu, 13 Jun 2024 08:42:05 GMT
    Submission history From: Giacomo Frisoni [view email]
    [v1] Mon, 4 Mar 2024 10:41:52 UTC (4,464 KB)
    [v2] Thu, 13 Jun 2024 08:42:05 UTC (4,466 KB)
    Giacomo Frisoni, Alessio Cocchieri, Alex Presepi, Gianluca Moro, Zaiqiao Meng

Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, "to generate or to retrieve" is the modern equivalent of Hamlet's dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new state-of-the-art in the open-book setting of each testbed, allowing a small-scale reader to outcompete zero-shot closed-book 175B baselines while using up to 706$\times$ fewer parameters. Our findings reveal that generated passages are more effective than retrieved ones in attaining higher accuracy.

------------

`[2403.03304] Large Language Models for Document-Level Event-Argument Data Augmentation for Challenging Role Types <https://arxiv.org/abs/2403.03304>`__ 面向挑战性角色类型的文档级事件参数数据增强的大型语言模型

::

    replaced with revised version Wed, 12 Jun 2024 19:21:33 GMT
    Submission history From: Joseph Gatto [view email]
    [v1] Tue, 5 Mar 2024 20:07:42 UTC (9,775 KB)
    [v2] Wed, 12 Jun 2024 19:21:33 UTC (9,182 KB)
    Joseph Gatto, Parker Seegmiller, Omar Sharif, Sarah M. Preum

Event Argument Extraction (EAE) is an extremely difficult information extraction problem -- with significant limitations in few-shot cross-domain (FSCD) settings. A common solution to FSCD modeling is data augmentation. Unfortunately, existing augmentation methods are not well-suited to a variety of real-world EAE contexts including (i) The need to model long documents (10+ sentences) (ii) The need to model zero and few-shot roles (i.e. event roles with little to no training representation). In this work, we introduce two novel LLM-powered data augmentation frameworks for synthesizing extractive document-level EAE samples using zero in-domain training data. Our highest performing methods provide a 16-pt increase in F1 score on extraction of zero shot role types.
To better facilitate analysis of cross-domain EAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect to roles observed in the source domain. Our experiments show that LLM-based augmentation can boost RDF1 performance by up to 11 F1 points compared to baseline methods.

------------

`[2405.09805] SecureLLM: Using Compositionality to Build Provably Secure Language Models for Private, Sensitive, and Secret Data <https://arxiv.org/abs/2405.09805>`__ SecureLLM:利用组合性为隐私、敏感和秘密数据构建可证明安全的语言模型

::

    replaced with revised version Thu, 13 Jun 2024 16:54:51 GMT
    Submission history From: Andrei Barbu [view email]
    [v1] Thu, 16 May 2024 04:25:53 UTC (6,894 KB)
    [v2] Thu, 13 Jun 2024 16:54:51 UTC (6,894 KB)
    Abdulrahman Alabdulkareem and Christian M Arnold and Yerim Lee and Pieter M Feenstra and Boris Katz and Andrei Barbu

Traditional security mechanisms isolate resources from users who should not access them. We reflect the compositional nature of such security mechanisms back into the structure of LLMs to build a provably secure LLM; that we term SecureLLM. Other approaches to LLM safety attempt to protect against bad actors or bad outcomes, but can only do so to an extent making them inappropriate for sensitive data. SecureLLM blends access security with fine-tuning methods. Each data silo has associated with it a separate fine-tuning and a user has access only to the collection of fine-tunings that they have permission for. The model must then perform on compositional tasks at the intersection of those data silos with the combination of those individual fine-tunings. While applicable to any task like document QA or making API calls, in this work we concern ourselves with models that learn the layouts of new SQL databases to provide natural-language-to-SQL translation capabilities. Existing fine-tuning composition methods fail in this challenging environment, as they are not well-equipped for handling compositional tasks. Compositionality remains a challenge for LLMs. We contribute both a difficult new compositional natural-language-to-SQL translation task and a new perspective on LLM security that allows models to be deployed to secure environments today.

------------

`[2405.20215] TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models <https://arxiv.org/abs/2405.20215>`__ TS-Align:面向大规模语言模型可扩展迭代微调的师生协作框架

::

    replaced with revised version Thu, 13 Jun 2024 03:35:22 GMT
    Submission history From: Chen Zhang [view email]
    [v1] Thu, 30 May 2024 16:17:40 UTC (498 KB)
    [v2] Thu, 13 Jun 2024 03:35:22 UTC (498 KB)
    [v3] Fri, 14 Jun 2024 08:23:33 UTC (498 KB)
    Chen Zhang, Chengguang Tang, Dading Chong, Ke Shi, Guohua Tang, Feng Jiang, Haizhou Li

Mainstream approaches to aligning large language models (LLMs) heavily rely on human preference data, particularly when models require periodic updates. The standard process for iterative alignment of LLMs involves collecting new human feedback for each update. However, the data collection process is costly and challenging to scale. To address this issue, we introduce the "TS-Align" framework, which fine-tunes a policy model using pairwise feedback data automatically mined from its outputs. This automatic mining process is efficiently accomplished through the collaboration between a large-scale teacher model and a small-scale student model. The policy fine-tuning process can be iteratively repeated using on-policy generations within our proposed teacher-student collaborative framework. Through extensive experiments, we demonstrate that our final aligned policy outperforms the base policy model with an average win rate of 69.7% across seven conversational or instruction-following datasets. Furthermore, we show that the ranking capability of the teacher is effectively distilled into the student through our pipeline, resulting in a small-scale yet effective reward model for policy model alignment.

------------

`[2406.00343] Beyond Metrics: Evaluating LLMs' Effectiveness in Culturally Nuanced, Low-Resource Real-World Scenarios <https://arxiv.org/abs/2406.00343>`__ 超越指标:评估llm在文化细微差别、低资源现实世界场景中的有效性

::

    replaced with revised version Thu, 13 Jun 2024 17:53:45 GMT
    Submission history From: Millicent Ochieng [view email]
    [v1] Sat, 1 Jun 2024 07:36:59 UTC (6,640 KB)
    [v2] Thu, 13 Jun 2024 17:53:45 UTC (6,640 KB)
    Millicent Ochieng, Varun Gumma, Sunayana Sitaram, Jindong Wang, Vishrav Chaudhary, Keshet Ronen, Kalika Bali, Jacki O'Neill

The deployment of Large Language Models (LLMs) in real-world applications presents both opportunities and challenges, particularly in multilingual and code-mixed communication settings. This research evaluates the performance of seven leading LLMs in sentiment analysis on a dataset derived from multilingual and code-mixed WhatsApp chats, including Swahili, English and Sheng. Our evaluation includes both quantitative analysis using metrics like F1 score and qualitative assessment of LLMs' explanations for their predictions. We find that, while Mistral-7b and Mixtral-8x7b achieved high F1 scores, they and other LLMs such as GPT-3.5-Turbo, Llama-2-70b, and Gemma-7b struggled with understanding linguistic and contextual nuances, as well as lack of transparency in their decision-making process as observed from their explanations. In contrast, GPT-4 and GPT-4-Turbo excelled in grasping diverse linguistic inputs and managing various contextual information, demonstrating high consistency with human alignment and transparency in their decision-making process. The LLMs however, encountered difficulties in incorporating cultural nuance especially in non-English settings with GPT-4s doing so inconsistently. The findings emphasize the necessity of continuous improvement of LLMs to effectively tackle the challenges of culturally nuanced, low-resource real-world settings and the need for developing evaluation benchmarks for capturing these issues.

------------

`[2406.03339] The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches <https://arxiv.org/abs/2406.03339>`__ 评估LLM应用的挑战:自动化、人工和基于LLM方法的分析

::

    replaced with revised version Thu, 13 Jun 2024 15:13:40 GMT
    Submission history From: Bhashithe Abeysinghe [view email]
    [v1] Wed, 5 Jun 2024 14:55:10 UTC (262 KB)
    [v2] Thu, 13 Jun 2024 15:13:40 UTC (585 KB)
    Bhashithe Abeysinghe and Ruhan Circi

Chatbots have been an interesting application of natural language generation since its inception. With novel transformer based Generative AI methods, building chatbots have become trivial. Chatbots which are targeted at specific domains for example medicine and psychology are implemented rapidly. This however, should not distract from the need to evaluate the chatbot responses. Especially because the natural language generation community does not entirely agree upon how to effectively evaluate such applications. With this work we discuss the issue further with the increasingly popular LLM based evaluations and how they correlate with human evaluations. Additionally, we introduce a comprehensive factored evaluation mechanism that can be utilized in conjunction with both human and LLM-based evaluations. We present the results of an experimental evaluation conducted using this scheme in one of our chatbot implementations which consumed educational reports, and subsequently compare automated, traditional human evaluation, factored human evaluation, and factored LLM evaluation. Results show that factor based evaluation produces better insights on which aspects need to be improved in LLM applications and further strengthens the argument to use human evaluation in critical spaces where main functionality is not direct retrieval.

------------

`[2406.03589] Ranking Manipulation for Conversational Search Engines <https://arxiv.org/abs/2406.03589>`__ 对话式搜索引擎的排名操纵

::

    replaced with revised version Thu, 13 Jun 2024 01:12:56 GMT
    Submission history From: Samuel Pfrommer [view email]
    [v1] Wed, 5 Jun 2024 19:14:21 UTC (7,039 KB)
    [v2] Thu, 13 Jun 2024 01:12:56 UTC (7,039 KB)
    Samuel Pfrommer, Yatong Bai, Tanmay Gautam, Somayeh Sojoudi

Major search engine providers are rapidly incorporating Large Language Model (LLM)-generated content in response to user queries. These conversational search engines operate by loading retrieved website text into the LLM context for summarization and interpretation. Recent research demonstrates that LLMs are highly vulnerable to jailbreaking and prompt injection attacks, which disrupt the safety and quality goals of LLMs using adversarial strings. This work investigates the impact of prompt injections on the ranking order of sources referenced by conversational search engines. To this end, we introduce a focused dataset of real-world consumer product websites and formalize conversational search ranking as an adversarial problem. Experimentally, we analyze conversational search rankings in the absence of adversarial injections and show that different LLMs vary significantly in prioritizing product name, document content, and context position. We then present a tree-of-attacks-based jailbreaking technique which reliably promotes low-ranked products. Importantly, these attacks transfer effectively to state-of-the-art conversational search engines such as this http URL. Given the strong financial incentive for website owners to boost their search ranking, we argue that our problem formulation is of critical importance for future robustness work.

------------

`[2406.05644] How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States <https://arxiv.org/abs/2406.05644>`__ 对齐和越狱是如何工作的:通过中间隐藏状态解释LLM的安全性

::

    replaced with revised version Thu, 13 Jun 2024 05:39:31 GMT
    Submission history From: Zhenhong Zhou [view email]
    [v1] Sun, 9 Jun 2024 05:04:37 UTC (39,168 KB)
    [v2] Thu, 13 Jun 2024 05:39:31 UTC (39,145 KB)
    Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Yongbin Li

Large language models (LLMs) rely on safety alignment to avoid responding to malicious user inputs. Unfortunately, jailbreak can circumvent safety guardrails, resulting in LLMs generating harmful content and raising concerns about LLM safety. Due to language models with intensive parameters often regarded as black boxes, the mechanisms of alignment and jailbreak are challenging to elucidate. In this paper, we employ weak classifiers to explain LLM safety through the intermediate hidden states. We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers. Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations. Jailbreak disturbs the transformation of early unethical classification into negative emotions. We conduct experiments on models from 7B to 70B across various model families to prove our conclusion. Overall, our paper indicates the intrinsical mechanism of LLM safety and how jailbreaks circumvent safety guardrails, offering a new perspective on LLM safety and reducing concerns. Our code is available at this https URL.

------------

`[2406.06144] Language Models Resist Alignment <https://arxiv.org/abs/2406.06144>`__ 语言模型抵抗对齐

::

    replaced with revised version Thu, 13 Jun 2024 06:46:14 GMT
    Submission history From: Jiaming Ji [view email]
    [v1] Mon, 10 Jun 2024 10:03:16 UTC (183 KB)
    [v2] Thu, 13 Jun 2024 06:46:14 UTC (176 KB)
    Jiaming Ji, Kaile Wang, Tianyi Qiu, Boyuan Chen, Jiayi Zhou, Changye Li, Hantao Lou, Yaodong Yang

Large language models (LLMs) may exhibit undesirable behaviors. Recent efforts have focused on aligning these models to prevent harmful generation. Despite these efforts, studies have shown that even a well-conducted alignment process can be easily circumvented, whether intentionally or accidentally. Do alignment fine-tuning have robust effects on models, or are merely superficial? In this work, we answer this question through both theoretical and empirical means. Empirically, we demonstrate the elasticity of post-alignment models, i.e., the tendency to revert to the behavior distribution formed during the pre-training phase upon further fine-tuning. Using compression theory, we formally derive that such fine-tuning process disproportionately undermines alignment compared to pre-training, potentially by orders of magnitude. We conduct experimental validations to confirm the presence of elasticity across models of varying types and sizes. Specifically, we find that model performance declines rapidly before reverting to the pre-training distribution, after which the rate of decline drops significantly. We further reveal that elasticity positively correlates with increased model size and the expansion of pre-training data. Our discovery signifies the importance of taming the inherent elasticity of LLMs, thereby overcoming the resistance of LLMs to alignment finetuning.

------------

`[2406.06579] From Redundancy to Relevance: Enhancing Explainability in Multimodal Large Language Models <https://arxiv.org/abs/2406.06579>`__ 从冗余到相关:增强多模态大型语言模型的可解释性

::

    replaced with revised version Thu, 13 Jun 2024 10:29:45 GMT
    Submission history From: Xiao-Feng Zhang [view email]
    [v1] Tue, 4 Jun 2024 13:52:54 UTC (2,766 KB)
    [v2] Thu, 13 Jun 2024 10:29:45 UTC (2,762 KB)
    Xiaofeng Zhang, Chen Shen, Xiaosong Yuan, Shaotian Yan, Liang Xie, Wenxiao Wang, Chaochen Gu, Hao Tang, Jieping Ye

Recently, multimodal large language models have exploded with an endless variety, most of the popular Large Vision Language Models (LVLMs) depend on sequential visual representation, where images are converted into hundreds or thousands of tokens before being input into the Large Language Model (LLM) along with language prompts. The black-box design hinders the interpretability of visual-language models, especially regarding more complex reasoning tasks. To explore the interaction process between image and text in complex reasoning tasks, we introduce the information flow method to visualize the interaction mechanism. By analyzing the dynamic flow of the information flow, we find that the information flow appears to converge in the shallow layer. Further investigation revealed a redundancy of the image token in the shallow layer. Consequently, a truncation strategy was introduced to aggregate image tokens within these shallow layers. This approach has been validated through experiments across multiple models, yielding consistent improvements.

------------

`[2406.08101] CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems <https://arxiv.org/abs/2406.08101>`__ CoXQL:会话XAI系统中解析解释请求的数据集

::

    replaced with revised version Thu, 13 Jun 2024 03:16:47 GMT
    Submission history From: Qianli Wang [view email]
    [v1] Wed, 12 Jun 2024 11:27:10 UTC (628 KB)
    [v2] Thu, 13 Jun 2024 03:16:47 UTC (619 KB)
    Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian M\"oller

Conversational explainable artificial intelligence (ConvXAI) systems based on large language models (LLMs) have garnered significant interest from the research community in natural language processing (NLP) and human-computer interaction (HCI). Such systems can provide answers to user questions about explanations in dialogues, have the potential to enhance users' comprehension and offer more information about the decision-making and generation processes of LLMs. Currently available ConvXAI systems are based on intent recognition rather than free chat, as this has been found to be more precise and reliable in identifying users' intentions. However, the recognition of intents still presents a challenge in the case of ConvXAI, since little training data exist and the domain is highly specific, as there is a broad range of XAI methods to map requests onto. In order to bridge this gap, we present CoXQL, the first dataset for user intent recognition in ConvXAI, covering 31 intents, seven of which require filling multiple slots. Subsequently, we enhance an existing parsing approach by incorporating template validations, and conduct an evaluation of several LLMs on CoXQL using different parsing strategies. We conclude that the improved parsing approach (MP+) surpasses the performance of previous approaches. We also discover that intents with multiple slots remain highly challenging for LLMs.

------------

`[2406.08316] Is Programming by Example solved by LLMs? <https://arxiv.org/abs/2406.08316>`__ llm能解决实例编程吗?

::

    replaced with revised version Thu, 13 Jun 2024 12:59:06 GMT
    Submission history From: Wen-Ding Li [view email]
    [v1] Wed, 12 Jun 2024 15:16:40 UTC (3,086 KB)
    [v2] Thu, 13 Jun 2024 12:59:06 UTC (3,086 KB)
    Wen-Ding Li, Kevin Ellis

Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have `solved' PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.

------------

----------
Index (64)
----------

`[2406.08713] Batch-Instructed Gradient for Prompt Evolution:Systematic Prompt Optimization for Enhanced Text-to-Image Synthesis <https://arxiv.org/abs/2406.08713>`__ 批次指导梯度快速演化:增强文本到图像合成的系统提示优化

`[2406.08751] 3D Building Generation in Minecraft via Large Language Models <https://arxiv.org/abs/2406.08751>`__ 基于大型语言模型的Minecraft 3D建筑生成

`[2406.09363] ElicitationGPT: Text Elicitation Mechanisms via Language Models <https://arxiv.org/abs/2406.09363>`__ elicitationongpt:基于语言模型的文本诱导机制

`[2406.08582] Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods <https://arxiv.org/abs/2406.08582>`__ 使用QLoRA探索llm中的事实记忆和风格模仿:一项实验研究和质量评估方法

`[2406.08660] Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification <https://arxiv.org/abs/2406.08660>`__ 微调的"小" LLMs(仍然)在文本分类中明显优于零样本生成式AI模型

`[2406.08673] HelpSteer2: Open-source dataset for training top-performing reward models <https://arxiv.org/abs/2406.08673>`__ HelpSteer2:用于训练表现最好的奖励模型的开源数据集

`[2406.08680] Analyzing Large Language Models for Classroom Discussion Assessment <https://arxiv.org/abs/2406.08680>`__ 面向课堂讨论评价的大型语言模型分析

`[2406.08707] mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus <https://arxiv.org/abs/2406.08707>`__ mOSCAR:大规模多语言多模态文档级语料库

`[2406.08726] Standard Language Ideology in AI-Generated Language <https://arxiv.org/abs/2406.08726>`__ 人工智能生成语言中的标准语言意识形态

`[2406.08754] StructuralSleight: Automated Jailbreak Attacks on Large Language Models Utilizing Uncommon Text-Encoded Structure <https://arxiv.org/abs/2406.08754>`__ StructuralSleight:利用不常见的文本编码结构对大型语言模型进行自动化越狱攻击

`[2406.08811] Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large Language Models <https://arxiv.org/abs/2406.08811>`__ 技能混合:学习优化数据使用以微调大型语言模型

`[2406.08842] ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions <https://arxiv.org/abs/2406.08842>`__ ContraSolver:通过解决内部偏好矛盾实现语言模型的自我对齐

`[2406.08848] An Approach to Build Zero-Shot Slot-Filling System for Industry-Grade Conversational Assistants <https://arxiv.org/abs/2406.08848>`__ 一种面向工业级会话助手的零样本槽填充系统构建方法

`[2406.08860] Plan, Generate and Complicate: Improving Low-resource Dialogue State Tracking via Easy-to-Difficult Zero-shot Data Augmentation <https://arxiv.org/abs/2406.08860>`__ 规划、生成和复杂化:通过易-难的零样本数据增强改进低资源对话状态跟踪

`[2406.08903] Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models <https://arxiv.org/abs/2406.08903>`__ Delta-CoMe:大型语言模型混合精度的免训练增量压缩

`[2406.08922] Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors <https://arxiv.org/abs/2406.08922>`__ 导航阴影:揭开现代AI内容检测器的有效干扰

`[2406.09008] LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language Models <https://arxiv.org/abs/2406.09008>`__ LLM阅读茶叶:用大型语言模型自动评估主题模型

`[2406.09012] Bayesian Statistical Modeling with Predictors from LLMs <https://arxiv.org/abs/2406.09012>`__ 基于llm预测变量的贝叶斯统计模型

`[2406.09043] Language Models are Crossword Solvers <https://arxiv.org/abs/2406.09043>`__ 语言模型是填字游戏求解器

`[2406.09098] SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large Language Models <https://arxiv.org/abs/2406.09098>`__ SciKnowEval:评估大型语言模型的多层次科学知识

`[2406.09103] Chain-of-Though (CoT) prompting strategies for medical error detection and correction <https://arxiv.org/abs/2406.09103>`__ 面向医疗差错检测与纠正的CoT激励策略

`[2406.09140] Investigating the translation capabilities of Large Language Models trained on parallel data only <https://arxiv.org/abs/2406.09140>`__ 仅研究在并行数据上训练的大型语言模型的翻译能力

`[2406.09155] DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation <https://arxiv.org/abs/2406.09155>`__ DefAn:用于llm幻觉评估的确定答案数据集

`[2406.09205] ReadCtrl: Personalizing text generation with readability-controlled instruction learning <https://arxiv.org/abs/2406.09205>`__ ReadCtrl:具有可读性控制的教学学习个性化文本生成

`[2406.09265] Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs <https://arxiv.org/abs/2406.09265>`__ 共享问题:在llm中跨语言和任务分析神经元

`[2406.09282] On the Effects of Heterogeneous Data Sources on Speech-to-Text Foundation Models <https://arxiv.org/abs/2406.09282>`__ 异构数据源对语音-文本基础模型的影响

`[2406.09289] Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models <https://arxiv.org/abs/2406.09289>`__ 理解越狱成功:大型语言模型潜空间动力学研究

`[2406.09325] REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space <https://arxiv.org/abs/2406.09325>`__ REVS:基于词汇空间排序编辑的语言模型敏感信息遗忘

`[2406.09330] Learning from Natural Language Explanations for Generalizable Entity Matching <https://arxiv.org/abs/2406.09330>`__ 基于自然语言解释的泛化实体匹配学习

`[2406.09345] DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding <https://arxiv.org/abs/2406.09345>`__ DiscreteSLU:基于自监督离散语音单元的口语理解大型语言模型

`[2406.08765] LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices <https://arxiv.org/abs/2406.08765>`__ 边缘计算设备上基于llm的时序数据分析知识剪枝

`[2406.08862] Cognitively Inspired Energy-Based World Models <https://arxiv.org/abs/2406.08862>`__ 认知启发的基于能量的世界模型

`[2406.09179] Unlearning with Control: Assessing Real-world Utility for Large Language Model Unlearning <https://arxiv.org/abs/2406.09179>`__ 带控制的遗忘:评估大型语言模型遗忘的现实效用

`[2406.09288] Zero-Shot Learning Over Large Output Spaces : Utilizing Indirect Knowledge Extraction from Large Language Models <https://arxiv.org/abs/2406.09288>`__ 大输出空间上的零样本学习:利用从大型语言模型中间接提取知识

`[2406.08665] Exploring Fuzzing as Data Augmentation for Neural Test Generation <https://arxiv.org/abs/2406.08665>`__ 探索模糊测试作为神经测试生成的数据增强

`[2406.08824] LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions <https://arxiv.org/abs/2406.08824>`__ llm驱动的机器人有可能实施歧视、暴力和非法行为

`[2406.09401] MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations <https://arxiv.org/abs/2406.09401>`__ MMScan:具有层次基础语言标注的多模态3D场景数据集

`[2406.09412] Explore the Limits of Omni-modal Pretraining at Scale <https://arxiv.org/abs/2406.09412>`__ 探索大规模全模态预训练的局限性

`[2406.09067] How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models <https://arxiv.org/abs/2406.09067>`__ 基于transformer的视觉编码器中的表示结构如何?视觉-语言模型中的多目标表示分析

`[2309.17057] Tell Me a Story! Narrative-Driven XAI with Large Language Models <https://arxiv.org/abs/2309.17057>`__ 给我讲个故事!叙事驱动的XAI与大型语言模型

`[2403.03744] MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models <https://arxiv.org/abs/2403.03744>`__ MedSafetyBench:评估和改进大型语言模型的医疗安全性

`[2406.06870] What's in an embedding? Would a rose by any embedding smell as sweet? <https://arxiv.org/abs/2406.06870>`__ 嵌入中有什么?任何一种嵌入的玫瑰闻起来都那么香吗?

`[2406.07394] Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B <https://arxiv.org/abs/2406.07394>`__ 利用LLaMa-3 8B通过蒙特卡罗树自求精访问GPT-4级数学奥林匹克解

`[2305.12057] Accurate Knowledge Distillation with n-best Reranking <https://arxiv.org/abs/2305.12057>`__ 基于n-best重排序的精确知识蒸馏

`[2305.13669] The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models <https://arxiv.org/abs/2305.13669>`__ 知识对齐问题:为大型语言模型连接人类和外部知识

`[2309.00237] Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes <https://arxiv.org/abs/2309.00237>`__ 基于合成临床笔记构建的可公开共享临床大型语言模型

`[2311.03099] Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch <https://arxiv.org/abs/2311.03099>`__ 语言模型是超级马里奥:从同源模型中吸收能力就像免费的午餐

`[2311.08348] MC$^2$: Towards Transparent and Culturally-Aware NLP for Minority Languages in China <https://arxiv.org/abs/2311.08348>`__ MC$^2$:面向中国少数民族语言的透明和文化意识的自然语言处理

`[2402.04333] LESS: Selecting Influential Data for Targeted Instruction Tuning <https://arxiv.org/abs/2402.04333>`__ LESS:选择有影响力的数据进行定向指令调优

`[2402.10986] FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models <https://arxiv.org/abs/2402.10986>`__ FinTral: GPT-4级多模态金融大型语言模型族

`[2402.19167] Teaching Large Language Models an Unseen Language on the Fly <https://arxiv.org/abs/2402.19167>`__ 教学大型语言模型是一种无形的语言

`[2403.00835] CLLMs: Consistency Large Language Models <https://arxiv.org/abs/2403.00835>`__ CLLMs:一致性大型语言模型

`[2403.01924] To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering <https://arxiv.org/abs/2403.01924>`__ 生成还是检索?人工语境在医学开放领域问答中的有效性

`[2403.03304] Large Language Models for Document-Level Event-Argument Data Augmentation for Challenging Role Types <https://arxiv.org/abs/2403.03304>`__ 面向挑战性角色类型的文档级事件参数数据增强的大型语言模型

`[2405.09805] SecureLLM: Using Compositionality to Build Provably Secure Language Models for Private, Sensitive, and Secret Data <https://arxiv.org/abs/2405.09805>`__ SecureLLM:利用组合性为隐私、敏感和秘密数据构建可证明安全的语言模型

`[2405.20215] TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models <https://arxiv.org/abs/2405.20215>`__ TS-Align:面向大规模语言模型可扩展迭代微调的师生协作框架

`[2406.00343] Beyond Metrics: Evaluating LLMs' Effectiveness in Culturally Nuanced, Low-Resource Real-World Scenarios <https://arxiv.org/abs/2406.00343>`__ 超越指标:评估llm在文化细微差别、低资源现实世界场景中的有效性

`[2406.03339] The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches <https://arxiv.org/abs/2406.03339>`__ 评估LLM应用的挑战:自动化、人工和基于LLM方法的分析

`[2406.03589] Ranking Manipulation for Conversational Search Engines <https://arxiv.org/abs/2406.03589>`__ 对话式搜索引擎的排名操纵

`[2406.05644] How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States <https://arxiv.org/abs/2406.05644>`__ 对齐和越狱是如何工作的:通过中间隐藏状态解释LLM的安全性

`[2406.06144] Language Models Resist Alignment <https://arxiv.org/abs/2406.06144>`__ 语言模型抵抗对齐

`[2406.06579] From Redundancy to Relevance: Enhancing Explainability in Multimodal Large Language Models <https://arxiv.org/abs/2406.06579>`__ 从冗余到相关:增强多模态大型语言模型的可解释性

`[2406.08101] CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems <https://arxiv.org/abs/2406.08101>`__ CoXQL:会话XAI系统中解析解释请求的数据集

`[2406.08316] Is Programming by Example solved by LLMs? <https://arxiv.org/abs/2406.08316>`__ llm能解决实例编程吗?

