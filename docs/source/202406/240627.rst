240627
========

----------
Survey (1)
----------

`[2406.01171] Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization <https://arxiv.org/abs/2406.01171>`__ llm中的两个人物故事:角色扮演和个性化的调查

::

    replaced with revised version Wed, 26 Jun 2024 09:37:48 GMT
    Submission history From: Yu-Min Tseng [view email]
    [v1] Mon, 3 Jun 2024 10:08:23 UTC (2,837 KB)
    [v2] Wed, 26 Jun 2024 09:37:48 UTC (2,323 KB)
    Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Wei-Lin Chen, Chao-Wei Huang, Yu Meng, Yun-Nung Chen

The concept of persona, originally adopted in dialogue literature, has re-surged as a promising framework for tailoring large language models (LLMs) to specific context (e.g., personalized search, LLM-as-a-judge). However, the growing research on leveraging persona in LLMs is relatively disorganized and lacks a systematic taxonomy. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. Additionally, we introduce existing methods for LLM personality evaluation. To the best of our knowledge, we present the first survey for role-playing and personalization in LLMs under the unified view of persona. We continuously maintain a paper collection to foster future endeavors: this https URL

------------

-------------
Benchmark (7)
-------------

`[2406.17789] Spanish and LLM Benchmarks: is MMLU Lost in Translation? <https://arxiv.org/abs/2406.17789>`__ 西班牙语和LLM基准:MMLU是否迷失在翻译中?

::

    Tue, 28 May 2024 11:13:40 GMT
    Irene Plaza, Nina Melero, Cristina del Pozo, Javier Conde and Pedro Reviriego, Marina Mayor-Rocher, Mar\'ia Grandury

The evaluation of Large Language Models (LLMs) is a key element in their continuous improvement process and many benchmarks have been developed to assess the performance of LLMs in different tasks and topics. As LLMs become adopted worldwide, evaluating them in languages other than English is increasingly important. However, most LLM benchmarks are simply translated using an automated tool and then run in the target language. This means that the results depend not only on the LLM performance in that language but also on the quality of the translation. In this paper, we consider the case of the well-known Massive Multitask Language Understanding (MMLU) benchmark. Selected categories of the benchmark are translated into Spanish using Azure Translator and ChatGPT4 and run on ChatGPT4. Next, the results are processed to identify the test items that produce different answers in Spanish and English. Those are then analyzed manually to understand if the automatic translation caused the change. The results show that a significant fraction of the failing items can be attributed to mistakes in the translation of the benchmark. These results make a strong case for improving benchmarks in languages other than English by at least revising the translations of the items and preferably by adapting the tests to the target language by experts.

------------

`[2406.18321] MathOdyssey: Benchmarking Mathematical Problem-Solving Skills in Large Language Models Using Odyssey Math Data <https://arxiv.org/abs/2406.18321>`__ MathOdyssey:使用Odyssey Math数据对大型语言模型的数学解决问题能力进行基准测试

::

    Wed, 26 Jun 2024 13:02:35 GMT
    Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, Kai Zou

Large language models (LLMs) have significantly advanced natural language understanding and demonstrated strong problem-solving abilities. Despite these successes, most LLMs still struggle with solving mathematical problems due to the intricate reasoning required. This paper investigates the mathematical problem-solving capabilities of LLMs using the newly developed "MathOdyssey" dataset. The dataset includes diverse mathematical problems at high school and university levels, created by experts from notable institutions to rigorously test LLMs in advanced problem-solving scenarios and cover a wider range of subject areas. By providing the MathOdyssey dataset as a resource to the AI community, we aim to contribute to the understanding and improvement of AI capabilities in complex mathematical problem-solving. We conduct benchmarking on open-source models, such as Llama-3 and DBRX-Instruct, and closed-source models from the GPT series and Gemini models. Our results indicate that while LLMs perform well on routine and moderately difficult tasks, they face significant challenges with Olympiad-level problems and complex university-level questions. Our analysis shows a narrowing performance gap between open-source and closed-source models, yet substantial challenges remain, particularly with the most demanding problems. This study highlights the ongoing need for research to enhance the mathematical reasoning of LLMs.
The dataset, results, and code are publicly available.

------------

`[2406.18326] PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models <https://arxiv.org/abs/2406.18326>`__ PaCoST:大型语言模型基准污染检测的配对置信度显著性测试

::

    Wed, 26 Jun 2024 13:12:40 GMT
    Huixuan Zhang, Yun Lin, Xiaojun Wan

Large language models (LLMs) are known to be trained on vast amounts of data, which may unintentionally or intentionally include data from commonly used benchmarks. This inclusion can lead to cheatingly high scores on model leaderboards, yet result in disappointing performance in real-world applications. To address this benchmark contamination problem, we first propose a set of requirements that practical contamination detection methods should follow. Following these proposed requirements, we introduce PaCoST, a Paired Confidence Significance Testing to effectively detect benchmark contamination in LLMs. Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the model is significantly more confident under the original benchmark. We validate the effectiveness of PaCoST and apply it on popular open-source models and benchmarks. We find that almost all models and benchmarks we tested are suspected contaminated more or less. We finally call for new LLM evaluation methods.

------------

`[2402.18060] Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions <https://arxiv.org/abs/2402.18060>`__ 大型语言模型在回答和解释具有挑战性的医学问题方面的基准测试

::

    replaced with revised version Tue, 25 Jun 2024 21:17:09 GMT
    Submission history From: Hanjie Chen [view email]
    [v1] Wed, 28 Feb 2024 05:44:41 UTC (494 KB)
    [v2] Thu, 29 Feb 2024 16:31:57 UTC (494 KB)
    [v3] Wed, 13 Mar 2024 16:44:45 UTC (494 KB)
    [v4] Tue, 25 Jun 2024 21:17:09 UTC (726 KB)
    Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze

LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations. However, medical board exam or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises simulated clinical questions. Both datasets are structured as multiple-choice question-answering tasks, accompanied by expert-written explanations. We evaluate seven LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. Human and automatic evaluations of model-generated explanations provide insights into the promise and deficiency of LLMs for explainable medical QA.

------------

`[2405.00716] Large Language Models in the Clinic: A Comprehensive Benchmark <https://arxiv.org/abs/2405.00716>`__ 临床中的大型语言模型:综合基准

::

    replaced with revised version Wed, 26 Jun 2024 17:48:18 GMT
    Submission history From: Hongjian Zhou [view email]
    [v1] Thu, 25 Apr 2024 15:51:06 UTC (124 KB)
    [v2] Tue, 25 Jun 2024 17:23:22 UTC (392 KB)
    [v3] Wed, 26 Jun 2024 17:48:18 UTC (392 KB)
    Andrew Liu, Hongjian Zhou, Yining Hua, Omid Rohanian, Anshul Thakur, Lei Clifton, David A. Clifton

The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench. We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and complex clinical tasks that are close to real-world practice, i.e., referral QA, treatment recommendation, hospitalization (long document) summarization, patient education, pharmacology QA and drug interaction for emerging drugs. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs.

------------

`[2406.17681] VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation <https://arxiv.org/abs/2406.17681>`__ VarBench:基于动态变量扰动的鲁棒语言模型基准测试

::

    replaced with revised version Wed, 26 Jun 2024 15:21:49 GMT
    Submission history From: Kun Qian [view email]
    [v1] Tue, 25 Jun 2024 16:13:53 UTC (826 KB)
    [v2] Wed, 26 Jun 2024 15:21:49 UTC (826 KB)
    Kun Qian, Shunji Wan, Claudia Tang, Youzhi Wang, Xuanming Zhang, Maximillian Chen, Zhou Yu

As large language models achieve impressive scores on traditional benchmarks, an increasing number of researchers are becoming concerned about benchmark data leakage during pre-training, commonly known as the data contamination problem. To ensure fair evaluation, recent benchmarks release only the training and validation sets, keeping the test set labels closed-source. They require anyone wishing to evaluate his language model to submit the model's predictions for centralized processing and then publish the model's result on their leaderboard. However, this submission process is inefficient and prevents effective error analysis. To address this issue, we propose to variabilize benchmarks and evaluate language models dynamically. Specifically, we extract variables from each test case and define a value range for each variable. For each evaluation, we sample new values from these value ranges to create unique test cases, thus ensuring a fresh evaluation each time. We applied this variable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and TruthfulQA, which cover mathematical generation and multiple-choice tasks. Our experimental results demonstrate that this approach provides a more accurate assessment of the true capabilities of language models, effectively mitigating the contamination problem.

------------

`[2406.15877] BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions <https://arxiv.org/abs/2406.15877>`__ BigCodeBench:对不同函数调用和复杂指令的代码生成进行基准测试

::

    replaced with revised version Wed, 26 Jun 2024 17:05:14 GMT
    Submission history From: Terry Yue Zhuo [view email]
    [v1] Sat, 22 Jun 2024 15:52:04 UTC (1,396 KB)
    [v2] Wed, 26 Jun 2024 17:05:14 UTC (1,393 KB)
    Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra

Automated software engineering has been greatly empowered by the recent advances in Large Language Models (LLMs) for programming. While current benchmarks have shown that LLMs can perform various software engineering tasks like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks. Solving challenging and practical programming tasks requires the capability of utilizing diverse function calls as tools to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding complex instructions. Fulfilling both of these characteristics can pose a great challenge for LLMs. To assess how well LLMs can solve challenging and practical programming tasks, we introduce Bench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained programming tasks. To evaluate LLMs rigorously, each programming task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of Bench, Benchi, that automatically transforms the original docstrings into short instructions only with essential information. Our extensive evaluation of 60 LLMs shows that LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%. The results underscore the need for further advancements in this area.

------------

---------------
Accelerate (11)
---------------

`[2406.17808] Training-Free Exponential Extension of Sliding Window Context with Cascading KV Cache <https://arxiv.org/abs/2406.17808>`__ 

::

    Mon, 24 Jun 2024 03:59:17 GMT
    Jeffrey Willette, Heejun Lee, Youngwan Lee, Myeongjae Jeon, Sung Ju Hwang

The context window within a transformer provides a form of active memory for the current task, which can be useful for few-shot learning and conditional generation, both which depend heavily on previous context tokens. However, as the context length grows, the computational cost increases quadratically.
Recent works have shown that saving a few initial tokens along with a fixed-sized sliding window leads to stable streaming generation with linear complexity in transformer-based Large Language Models (LLMs). However, they make suboptimal use of the fixed window by naively evicting all tokens unconditionally from the key-value (KV) cache once they reach the end of the window, resulting in tokens being forgotten and no longer able to affect subsequent predictions. To overcome this limitation, we propose a novel mechanism for storing longer sliding window contexts with the same total cache size by keeping separate cascading sub-cache buffers whereby each subsequent buffer conditionally accepts a fraction of the relatively more important tokens evicted from the previous buffer. Our method results in a dynamic KV cache that can store tokens from the more distant past than a fixed, static sliding window approach. Our experiments show improvements of 5.6% on long context generation (LongBench), 1.2% in streaming perplexity (PG19), and 0.6% in language understanding (MMLU STEM) using LLMs given the same fixed cache size.
Additionally, we provide an efficient implementation that improves the KV cache latency from 1.33ms per caching operation to 0.54ms, a 59% speedup over previous work.

------------

`[2406.18002] Decoding with Limited Teacher Supervision Requires Understanding When to Trust the Teacher <https://arxiv.org/abs/2406.18002>`__ 在有限的教师监督下解码需要理解何时信任教师

::

    Wed, 26 Jun 2024 01:16:12 GMT
    Hyunjong Ok, Jegwang Ryu, Jaeho Lee

How can sLLMs efficiently utilize the supervision of LLMs to improve their generative quality? This question has been well studied in scenarios where there is no restriction on the number of LLM supervisions one can use, giving birth to many decoding algorithms that utilize supervision without further training. However, it is still unclear what is an effective strategy under the limited supervision scenario, where we assume that no more than a few tokens can be generated by LLMs. To this end, we develop an algorithm to effectively aggregate the sLLM and LLM predictions on initial tokens so that the generated tokens can more accurately condition the subsequent token generation by sLLM only. Critically, we find that it is essential to adaptively overtrust or disregard the LLM prediction based on the confidence of the sLLM. Through our experiments on a wide range of models and datasets, we demonstrate that our method provides a consistent improvement over conventional decoding strategies.

------------

`[2406.18060] AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning <https://arxiv.org/abs/2406.18060>`__ AdaZeta:面向内存高效的大型语言模型微调的自适应零阶张量-训练自适应

::

    Wed, 26 Jun 2024 04:33:13 GMT
    Yifan Yang, Kai Zhen, Ershad Banijamal, Athanasios Mouchtaris, Zheng Zhang

Fine-tuning large language models (LLMs) has achieved remarkable performance across various natural language processing tasks, yet it demands more and more memory as model sizes keep growing. To address this issue, the recently proposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs using only forward passes, thereby avoiding the need for a backpropagation graph. However, significant performance drops and a high risk of divergence have limited their widespread adoption. In this paper, we propose the Adaptive Zeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed to improve the performance and convergence of the ZO methods. To enhance dimension-dependent ZO estimation accuracy, we introduce a fast-forward, low-parameter tensorized adapter. To tackle the frequently observed divergence issue in large-scale ZO fine-tuning tasks, we propose an adaptive query number schedule that guarantees convergence. Detailed theoretical analysis and extensive experimental results on Roberta-Large and Llama-2-7B models substantiate the efficacy of our AdaZeta framework in terms of accuracy, memory efficiency, and convergence speed.

------------

`[2406.18139] LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference <https://arxiv.org/abs/2406.18139>`__ LOOK-M: KV缓存中面向高效多模态长上下文推理的Look-Once优化

::

    Wed, 26 Jun 2024 07:44:24 GMT
    Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, Li Yuan

Long-context Multimodal Large Language Models (MLLMs) demand substantial computational resources for inference as the growth of their multimodal Key-Value (KV) cache, in response to increasing input lengths, challenges memory and time efficiency. Unlike single-modality LLMs that manage only textual contexts, the KV cache of long-context MLLMs includes representations from multiple images with temporal and spatial relationships and related textual contexts. The predominance of image tokens means traditional optimizations for LLMs' KV caches are unsuitable for multimodal long-context settings, and no prior works have addressed this challenge. In this work, we introduce LOOK-M, a pioneering, fine-tuning-free approach that efficiently reduces the multimodal KV cache size while maintaining performance comparable to a full cache. We observe that during prompt prefill, the model prioritizes more textual attention over image features, and based on the multimodal interaction observation, a new proposed text-prior method is explored to compress the KV cache. Furthermore, to mitigate the degradation of image contextual information, we propose several compensatory strategies using KV pairs merging. LOOK-M demonstrates that with a significant reduction in KV Cache memory usage, such as reducing it by 80% in some cases, it not only achieves up to 1.5x faster decoding but also maintains or even enhances performance across a variety of long context multimodal tasks.

------------

`[2406.18200] SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding <https://arxiv.org/abs/2406.18200>`__ SEED:通过预定的推测解码加速推理树构建

::

    Wed, 26 Jun 2024 09:33:41 GMT
    Zhenglin Wang, Jialong Wu, Yilong Lai, Congzhi Zhang, Deyu Zhou

Large Language Models (LLMs) demonstrate remarkable emergent abilities across various tasks, yet fall short of complex reasoning and planning tasks. The tree-search-based reasoning methods address this by surpassing the capabilities of chain-of-thought prompting, encouraging exploration of intermediate steps.
However, such methods introduce significant inference latency due to the systematic exploration and evaluation of multiple thought paths. This paper introduces SeeD, a novel and efficient inference framework to optimize runtime speed and GPU memory management concurrently. By employing a scheduled speculative execution, SeeD efficiently handles multiple iterations for the thought generation and the state evaluation, leveraging a rounds-scheduled strategy to manage draft model dispatching. Extensive experimental evaluations on three reasoning datasets demonstrate superior speedup performance of SeeD, providing a viable path for batched inference in training-free speculative decoding.

------------

`[2406.14066] Optimizing Speculative Decoding for Serving Large Language Models Using Goodput <https://arxiv.org/abs/2406.14066>`__ 使用Goodput优化推测解码以服务于大型语言模型

::

    replaced with revised version Tue, 25 Jun 2024 20:53:16 GMT
    Xiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung, Zhijie Deng, Ion Stoica, Hao Zhang

Categories

------------

`[2312.06149] Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding <https://arxiv.org/abs/2312.06149>`__ 解锁预期文本生成:大型语言模型解码的约束方法

::

    replaced with revised version Tue, 25 Jun 2024 18:01:24 GMT
    Submission history From: Lifu Tu [view email]
    [v1] Mon, 11 Dec 2023 06:35:33 UTC (612 KB)
    [v2] Mon, 19 Feb 2024 04:05:17 UTC (621 KB)
    [v3] Tue, 25 Jun 2024 18:01:24 UTC (645 KB)
    Lifu Tu, Semih Yavuz, Jin Qu, Jiacheng Xu, Rui Meng, Caiming Xiong, Yingbo Zhou

Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 2020), and factual correctness in question-answering (Gao et al., 2023).

------------

`[2402.13720] Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding <https://arxiv.org/abs/2402.13720>`__ 

::

    replaced with revised version Wed, 26 Jun 2024 04:52:02 GMT
    Submission history From: Weilin Zhao [view email]
    [v1] Wed, 21 Feb 2024 11:31:28 UTC (1,539 KB)
    [v2] Wed, 26 Jun 2024 04:52:02 UTC (8,914 KB)
    Weilin Zhao, Yuxiang Huang, Xu Han, Wang Xu, Chaojun Xiao, Xinrong Zhang, Yewei Fang, Kaihuo Zhang, Zhiyuan Liu, Maosong Sun

Speculative decoding is a widely used method that accelerates the generation process of large language models (LLMs) with no compromise in model performance. It achieves this goal by using an existing smaller model for drafting and then employing the target LLM to verify the draft in a low-cost parallel manner. Under such a drafting-verification framework, drafting efficiency has become a bottleneck in the final speedup of speculative decoding. Therefore, generating longer drafts at less cost can lead to better decoding speedup. To achieve this, we introduce Ouroboros, which can generate draft phrases to parallelize the drafting process and meanwhile lengthen drafts in a training-free manner. The experimental results on various typical text generation tasks show that Ouroboros can achieve speedups of up to $2.4\times$ over speculative decoding and $3.9\times$ over vanilla decoding, without fine-tuning draft and target models.

------------

`[2405.11966] Multiple-Choice Questions are Efficient and Robust LLM Evaluators <https://arxiv.org/abs/2405.11966>`__ 选择题是高效和稳健的LLM评估器

::

    replaced with revised version Wed, 26 Jun 2024 07:16:42 GMT
    Submission history From: Ziyin Zhang [view email]
    [v1] Mon, 20 May 2024 11:47:13 UTC (156 KB)
    [v2] Tue, 21 May 2024 15:16:46 UTC (156 KB)
    [v3] Wed, 12 Jun 2024 16:05:40 UTC (455 KB)
    [v4] Wed, 26 Jun 2024 07:16:42 UTC (456 KB)
    Ziyin Zhang and Zhaokun Jiang and Lizhen Xu and Hongkun Hao and Rui Wang

We present GSM-MC, a multiple-choice (MC) dataset constructed by collecting answers and incorrect predictions on GSM8K from 60 open-source models. Through extensive experiments, we show that LLMs' performance on the MC version of this popular benchmark is strongly correlated with their performance on the original version and is quite robust to distractor choices and option orders, while the evaluation time is reduced by a factor of up to 30. Following similar procedures, we introduce MATH-MC, constructed from MATH, and PythonIO, a new program reasoning MC dataset constructed from HumanEval and MBPP. Experimental results indicate that LLMs' performance on these MC benchmarks leaves much room for improvement. Our data and code are available at this https URL.

------------

`[2406.02532] SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices <https://arxiv.org/abs/2406.02532>`__ SpecExec:消费设备上交互式LLM推理的大规模并行推测解码

::

    replaced with revised version Tue, 25 Jun 2024 19:35:06 GMT
    Submission history From: Ruslan Svirschevski [view email]
    [v1] Tue, 4 Jun 2024 17:53:36 UTC (442 KB)
    [v2] Tue, 25 Jun 2024 19:35:06 UTC (442 KB)
    Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin

As large language models gain widespread adoption, running them efficiently becomes crucial. Recent works on LLM inference use speculative decoding to achieve extreme speedups. However, most of these works implicitly design their algorithms for high-end datacenter hardware. In this work, we ask the opposite question: how fast can we run LLMs on consumer machines? Consumer GPUs can no longer fit the largest available models (50B+ parameters) and must offload them to RAM or SSD. When running with offloaded parameters, the inference engine can process batches of hundreds or thousands of tokens at the same time as just one token, making it a natural fit for speculative decoding. We propose SpecExec (Speculative Execution), a simple parallel decoding method that can generate up to 20 tokens per target model iteration for popular LLM families. It utilizes the high spikiness of the token probabilities distribution in modern LLMs and a high degree of alignment between model output probabilities. SpecExec takes the most probable tokens continuation from the draft model to build a "cache" tree for the target model, which then gets validated in a single pass. Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens per second with 16-bit weights.

------------

`[2306.11903] Deep Fusion: Efficient Network Training via Pre-trained Initializations <https://arxiv.org/abs/2306.11903>`__ 

::

    replaced with revised version Wed, 26 Jun 2024 12:16:57 GMT
    Submission history From: Hanna Mazzawi [view email]
    [v1] Tue, 20 Jun 2023 21:30:54 UTC (308 KB)
    [v2] Wed, 7 Feb 2024 17:18:09 UTC (2,298 KB)
    [v3] Wed, 26 Jun 2024 12:16:57 UTC (2,289 KB)
    Hanna Mazzawi, Xavi Gonzalvo, Michael Wunder, Sammy Jerome, Benoit Dherin

In recent years, deep learning has made remarkable progress in a wide range of domains, with a particularly notable impact on natural language processing tasks. One of the challenges associated with training deep neural networks in the context of LLMs is the need for large amounts of computational resources and time. To mitigate this, network growing algorithms offer potential cost savings, but their underlying mechanisms are poorly understood. We present two notable contributions in this paper. First, we present Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks. Second, we propose a theoretical framework using backward error analysis to illustrate the dynamics of mid-training network growth. Our experiments show how Deep Fusion is a practical and effective approach that not only accelerates the training process but also reduces computational requirements, maintaining or surpassing traditional training methods' performance in various NLP tasks and T5 model sizes. Finally, we validate our theoretical framework, which guides the optimal use of Deep Fusion, showing that with carefully optimized training dynamics, it significantly reduces both training time and resource consumption.

------------

-----------------------
In-Context Learning (1)
-----------------------

`[2406.18501] Is In-Context Learning a Type of Gradient-Based Learning? Evidence from the Inverse Frequency Effect in Structural Priming <https://arxiv.org/abs/2406.18501>`__ 情境学习是一种基于梯度的学习吗?来自结构启动中的逆频率效应的证据

::

    Wed, 26 Jun 2024 17:06:41 GMT
    Zhenghao Zhou, Robert Frank, R. Thomas McCoy

Large language models (LLMs) have shown the emergent capability of in-context learning (ICL). One line of research has explained ICL as functionally performing gradient descent. In this paper, we introduce a new way of diagnosing whether ICL is functionally equivalent to gradient-based learning.
Our approach is based on the inverse frequency effect (IFE) -- a phenomenon in which an error-driven learner is expected to show larger updates when trained on infrequent examples than frequent ones. The IFE has previously been studied in psycholinguistics because humans show this effect in the context of structural priming (the tendency for people to produce sentence structures they have encountered recently); the IFE has been used as evidence that human structural priming must involve error-driven learning mechanisms. In our experiments, we simulated structural priming within ICL and found that LLMs display the IFE, with the effect being stronger in larger models. We conclude that ICL is indeed a type of gradient-based learning, supporting the hypothesis that a gradient component is implicitly computed in the forward pass during ICL. Our results suggest that both humans and LLMs make use of gradient-based, error-driven processing mechanisms.

------------

-------------
Reasoning (8)
-------------

`[2406.17873] Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback <https://arxiv.org/abs/2406.17873>`__ 通过关系元组、验证和动态反馈提高大型语言模型的算术推理能力

::

    Tue, 25 Jun 2024 18:21:00 GMT
    Zhongtao Miao, Kaiyan Zhao, Yoshimasa Tsuruoka

Current representations used in reasoning steps of large language models can mostly be categorized into two main types: (1) natural language, which is difficult to verify; and (2) non-natural language, usually programming code, which is difficult for people who are unfamiliar with coding to read. In this paper, we propose to use a semi-structured form to represent reasoning steps of large language models. Specifically, we use relation tuples, which are not only human-readable but also machine-friendly and easier to verify than natural language. We implement a framework that includes three main components: (1) introducing relation tuples into the reasoning steps of large language models; (2) implementing an automatic verification process of reasoning steps with a local code interpreter based on relation tuples; and (3) integrating a simple and effective dynamic feedback mechanism, which we found helpful for self-improvement of large language models. The experimental results on various arithmetic datasets demonstrate the effectiveness of our method in improving the arithmetic reasoning ability of large language models. The source code is available at https://github.com/gpgg/art.

------------

`[2406.17961] NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization <https://arxiv.org/abs/2406.17961>`__ NormTab:通过表格数据规范化改进llm中的符号推理

::

    Tue, 25 Jun 2024 22:40:03 GMT
    Md Mahadi Hasan Nahid, Davood Rafiei

In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in parsing textual data and generating code. However, their performance in tasks involving tabular data, especially those requiring symbolic reasoning, faces challenges due to the structural variance and inconsistency in table cell values often found in web tables. In this paper, we introduce NormTab, a novel framework aimed at enhancing the symbolic reasoning performance of LLMs by normalizing web tables. We study table normalization as a stand-alone, one-time preprocessing step using LLMs to support symbolic reasoning on tabular data. Our experimental evaluation, conducted on challenging web table datasets such as WikiTableQuestion and TabFact, demonstrates that leveraging NormTab significantly improves symbolic reasoning performance, showcasing the importance and effectiveness of web table normalization for enhancing LLM-based symbolic reasoning tasks.

------------

`[2406.18200] SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding <https://arxiv.org/abs/2406.18200>`__ SEED:通过预定的推测解码加速推理树构建

::

    Wed, 26 Jun 2024 09:33:41 GMT
    Zhenglin Wang, Jialong Wu, Yilong Lai, Congzhi Zhang, Deyu Zhou

Large Language Models (LLMs) demonstrate remarkable emergent abilities across various tasks, yet fall short of complex reasoning and planning tasks. The tree-search-based reasoning methods address this by surpassing the capabilities of chain-of-thought prompting, encouraging exploration of intermediate steps.
However, such methods introduce significant inference latency due to the systematic exploration and evaluation of multiple thought paths. This paper introduces SeeD, a novel and efficient inference framework to optimize runtime speed and GPU memory management concurrently. By employing a scheduled speculative execution, SeeD efficiently handles multiple iterations for the thought generation and the state evaluation, leveraging a rounds-scheduled strategy to manage draft model dispatching. Extensive experimental evaluations on three reasoning datasets demonstrate superior speedup performance of SeeD, providing a viable path for batched inference in training-free speculative decoding.

------------

`[2312.12009] Active Preference Inference using Language Models and Probabilistic Reasoning <https://arxiv.org/abs/2312.12009>`__ 基于语言模型和概率推理的主动偏好推理

::

    replaced with revised version Wed, 26 Jun 2024 15:00:52 GMT
    Submission history From: Wasu Top Piriyakulkij [view email]
    [v1] Tue, 19 Dec 2023 09:58:54 UTC (440 KB)
    [v2] Wed, 26 Jun 2024 15:00:52 UTC (440 KB)
    Wasu Top Piriyakulkij, Volodymyr Kuleshov, Kevin Ellis

Actively inferring user preferences, for example by asking good questions, is important for any human-facing decision-making system. Active inference allows such systems to adapt and personalize themselves to nuanced individual preferences. To enable this ability for instruction-tuned large language models (LLMs), one may prompt them to ask users questions to infer their preferences, transforming the language models into more robust, interactive systems. However, out of the box, these models are not efficient at extracting preferences: the questions they generate are not informative, requiring a high number of user interactions and impeding the usability of the downstream system. In this work, we introduce an inference-time algorithm that helps LLMs quickly infer preferences by using more informative questions. Our algorithm uses a probabilistic model whose conditional distributions are defined by prompting an LLM, and returns questions that optimize expected entropy and expected model change. Results in a simplified interactive web shopping setting with real product items show that an LLM equipped with our entropy reduction algorithm outperforms baselines with the same underlying LLM on task performance while using fewer user interactions.

------------

`[2401.09395] Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions <https://arxiv.org/abs/2401.09395>`__ 陷入推理的流沙，远离AGI峰会:通过本体指导的干预评估llm的数学和编码能力

::

    replaced with revised version Wed, 26 Jun 2024 11:43:39 GMT
    Submission history From: Soujanya Poria [view email]
    [v1] Wed, 17 Jan 2024 18:13:07 UTC (1,504 KB)
    [v2] Mon, 19 Feb 2024 01:50:42 UTC (1,693 KB)
    [v3] Wed, 26 Jun 2024 11:43:39 UTC (9,581 KB)
    [v4] Thu, 27 Jun 2024 05:23:50 UTC (9,581 KB)
    Pengfei Hong, Deepanway Ghosal, Navonil Majumder, Somak Aditya, Rada Mihalcea, Soujanya Poria

Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust problem solving skills and structured reasoning abilities in many areas, as defined by our ontology. We open source the datasets and source codes at: this https URL.

------------

`[2404.15515] ToM-LM: Delegating Theory of Mind Reasoning to External Symbolic Executors in Large Language Models <https://arxiv.org/abs/2404.15515>`__ ToM-LM:将心智理论推理委托给大型语言模型中的外部符号执行者

::

    replaced with revised version Wed, 26 Jun 2024 15:57:22 GMT
    Submission history From: Weizhi Tang [view email]
    [v1] Tue, 23 Apr 2024 20:59:03 UTC (696 KB)
    [v2] Thu, 25 Apr 2024 19:33:24 UTC (347 KB)
    [v3] Wed, 26 Jun 2024 15:57:22 UTC (455 KB)
    Weizhi Tang, Vaishak Belle

Theory of Mind (ToM) refers to the ability of individuals to attribute mental states to others. While Large Language Models (LLMs) have shown some promise with ToM ability, they still struggle with complex ToM reasoning. Our approach leverages an external symbolic executor, specifically the SMCDEL model checker, and fine-tuning to improve the ToM reasoning ability of LLMs. In our approach, an LLM is first fine-tuned through pairs of natural language and symbolic formulation representation of ToM problems and is then instructed to generate the symbolic formulation with a one-shot in-context example. The generated symbolic formulation is then executed by the SMCDEL model checker to perform transparent and verifiable ToM reasoning and give the final result. We demonstrate that our approach, ToM-LM, shows a significant improvement over all the constructed baselines. Our study proposes a novel view about externalizing a particular component of ToM reasoning, mainly reasoning about beliefs, and suggests generalizing it to other aspects of ToM reasoning.

------------

`[2406.17294] Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models <https://arxiv.org/abs/2406.17294>`__ math - lava:多模态大型语言模型的自助数学推理

::

    replaced with revised version Wed, 26 Jun 2024 16:43:27 GMT
    Submission history From: Wenhao Shi [view email]
    [v1] Tue, 25 Jun 2024 05:43:21 UTC (1,974 KB)
    [v2] Wed, 26 Jun 2024 16:43:27 UTC (1,974 KB)
    Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, Roy Ka-Wei Lee

Large language models (LLMs) have demonstrated impressive reasoning capabilities, particularly in textual mathematical problem-solving. However, existing open-source image instruction fine-tuning datasets, containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap, we address the lack of high-quality, diverse multimodal mathematical datasets by collecting 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs, creating the MathV360K dataset, which enhances both the breadth and depth of multimodal mathematical questions. We introduce Math-LLaVA, a LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced generalizability, showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs' mathematical reasoning abilities. The code and data are available at: \url{this https URL}.

------------

`[2405.16265] MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time <https://arxiv.org/abs/2405.16265>`__ MindStar:在推理时增强预训练llm的数学推理

::

    replaced with revised version Wed, 26 Jun 2024 14:01:15 GMT
    Submission history From: Jikun Kang [view email]
    [v1] Sat, 25 May 2024 15:07:33 UTC (1,366 KB)
    [v2] Mon, 17 Jun 2024 13:37:39 UTC (1,540 KB)
    [v3] Fri, 21 Jun 2024 22:41:08 UTC (1,541 KB)
    [v4] Wed, 26 Jun 2024 14:01:15 UTC (1,550 KB)
    Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan He, Feng Wen, Jianye Hao, Jun Yao

Although Large Language Models (LLMs) achieve remarkable performance across various tasks, they often struggle with complex reasoning tasks, such as answering mathematical questions. Recent efforts to address this issue have primarily focused on leveraging mathematical datasets through supervised fine-tuning or self-improvement techniques. However, these methods often depend on high-quality datasets that are difficult to prepare, or they require substantial computational resources for fine-tuning. Inspired by findings that LLMs know how to produce the right answer but struggle to select the correct reasoning path, we propose a purely inference-based searching method -- MindStar (M*). This method formulates reasoning tasks as searching problems and proposes two search ideas to identify the optimal reasoning paths. We evaluate the M* framework on both the GSM8K and MATH datasets, comparing its performance with existing open and closed-source LLMs. Our results demonstrate that M* significantly enhances the reasoning abilities of open-source models, such as Llama-2-13B and Mistral-7B, and achieves comparable performance to GPT-3.5 and Grok-1, but with substantially reduced model size and computational costs.

------------

-----------
ToolUse (2)
-----------

`[2406.18495] WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs <https://arxiv.org/abs/2406.18495>`__ WildGuard:为安全风险、越狱和llm拒绝打开一站式审核工具

::

    Wed, 26 Jun 2024 16:58:20 GMT
    Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, Nouha Dziri

We introduce WildGuard -- an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WildGuard serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks and in evaluating models' refusals, a key measure for evaluating safety behaviors in model responses.
To address these challenges, we construct WildGuardMix, a large-scale and carefully balanced multi-task safety moderation dataset with 92K labeled examples that cover vanilla (direct) prompts and adversarial jailbreaks, paired with various refusal and compliance responses. WildGuardMix is a combination of WildGuardTrain, the training data of WildGuard, and WildGuardTest, a high-quality human-annotated moderation test set with 5K labeled items covering broad risk scenarios. Through extensive evaluations on WildGuardTest and ten existing public benchmarks, we show that WildGuard establishes state-of-the-art performance in open-source safety moderation across all the three tasks compared to ten strong existing open-source moderation models (e.g., up to 26.4% improvement on refusal detection). Importantly, WildGuard matches and sometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt harmfulness identification). WildGuard serves as a highly effective safety moderator in an LLM interface, reducing the success rate of jailbreak attacks from 79.8% to 2.4%.

------------

`[2406.17215] Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of Daline <https://arxiv.org/abs/2406.17215>`__ 用以前未见过的工具使大型语言模型执行电力系统模拟:Daline的一个案例

::

    replaced with revised version Wed, 26 Jun 2024 05:45:28 GMT
    Submission history From: Mengshuo Jia [view email]
    [v1] Tue, 25 Jun 2024 02:05:26 UTC (355 KB)
    [v2] Wed, 26 Jun 2024 05:45:28 UTC (355 KB)
    Mengshuo Jia, Zeyu Cui, Gabriela Hug

The integration of experiment technologies with large language models (LLMs) is transforming scientific research, offering AI capabilities beyond specialized problem-solving to becoming research assistants for human scientists. In power systems, simulations are essential for research. However, LLMs face significant challenges in power system simulations due to limited pre-existing knowledge and the complexity of power grids. To address this issue, this work proposes a modular framework that integrates expertise from both the power system and LLM domains. This framework enhances LLMs' ability to perform power system simulations on previously unseen tools. Validated using 34 simulation tasks in Daline, a (optimal) power flow simulation and linearization toolbox not yet exposed to LLMs, the proposed framework improved GPT-4o's simulation coding accuracy from 0% to 96.07%, also outperforming the ChatGPT-4o web interface's 33.8% accuracy (with the entire knowledge base uploaded). These results highlight the potential of LLMs as research assistants in power systems.

------------

-----------------------
Retrieval-Augmented (6)
-----------------------

`[2406.17969] Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective <https://arxiv.org/abs/2406.17969>`__ 鼓励还是抑制单语义?从特征去相关的角度重新审视单语义

::

    Tue, 25 Jun 2024 22:51:08 GMT
    Hanqi Yan, Yanzheng Xiang, Guangyi Chen, Yifei Wang, Lin Gui, Yulan He

To better interpret the intrinsic mechanism of large language models (LLMs), recent studies focus on monosemanticity on its basic units. A monosemantic neuron is dedicated to a single and specific concept, which forms a one-to-one correlation between neurons and concepts. Despite extensive research in monosemanticity probing, it remains unclear whether monosemanticity is beneficial or harmful to model capacity. To explore this question, we revisit monosemanticity from the feature decorrelation perspective and advocate for its encouragement. We experimentally observe that the current conclusion by wang2024learning, which suggests that decreasing monosemanticity enhances model performance, does not hold when the model changes. Instead, we demonstrate that monosemanticity consistently exhibits a positive correlation with model capacity, in the preference alignment process. Consequently, we apply feature correlation as a proxy for monosemanticity and incorporate a feature decorrelation regularizer into the dynamic preference optimization process. The experiments show that our method not only enhances representation diversity and activation sparsity but also improves preference alignment performance.

------------

`[2406.17987] Multi-step Knowledge Retrieval and Inference over Unstructured Data <https://arxiv.org/abs/2406.17987>`__ 非结构化数据的多步知识检索与推理

::

    Wed, 26 Jun 2024 00:00:45 GMT
    Aditya Kalyanpur, Kailash Saravanakumar, Victor Barres, CJ McFate, Lori Moon, Nati Seifu, Maksim Eremeev, Jose Barrera, Eric Brown, David Ferrucci

The advent of Large Language Models (LLMs) and Generative AI has revolutionized natural language applications across various domains. However, high-stakes decision-making tasks in fields such as medical, legal and finance require a level of precision, comprehensiveness, and logical consistency that pure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to deliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI platform to tackle these problems. The platform integrates fine-tuned LLMs for knowledge extraction and alignment with a robust symbolic reasoning engine for logical inference, planning and interactive constraint solving. We describe Cora, a Collaborative Research Assistant built on this platform, that is designed to perform complex research and discovery tasks in high-stakes domains. This paper discusses the multi-step inference challenges inherent in such domains, critiques the limitations of existing LLM-based methods, and demonstrates how Cora's neuro-symbolic approach effectively addresses these issues. We provide an overview of the system architecture, key algorithms for knowledge extraction and formal reasoning, and present preliminary evaluation results that highlight Cora's superior performance compared to well-known LLM and RAG baselines.

------------

`[2406.18034] LLMs for Doctors: Leveraging Medical LLMs to Assist Doctors, Not Replace Them <https://arxiv.org/abs/2406.18034>`__ 医生的llm:利用医学llm来帮助医生，而不是取代他们

::

    Wed, 26 Jun 2024 03:08:24 GMT
    Wenya Xie, Qingying Xiao, Yu Zheng, Xidong Wang, Junying Chen, Ke Ji, Anningzhe Gao, Xiang Wan, Feng Jiang, Benyou Wang

The recent success of Large Language Models (LLMs) has had a significant impact on the healthcare field, providing patients with medical advice, diagnostic information, and more. However, due to a lack of professional medical knowledge, patients are easily misled by generated erroneous information from LLMs, which may result in serious medical problems. To address this issue, we focus on tuning the LLMs to be medical assistants who collaborate with more experienced doctors. We first conduct a two-stage survey by inspiration-feedback to gain a broad understanding of the real needs of doctors for medical assistants. Based on this, we construct a Chinese medical dataset called DoctorFLAN to support the entire workflow of doctors, which includes 92K Q\&A samples from 22 tasks and 27 specialists. Moreover, we evaluate LLMs in doctor-oriented scenarios by constructing the DoctorFLAN-\textit{test} containing 550 single-turn Q\&A and DotaBench containing 74 multi-turn conversations. The evaluation results indicate that being a medical assistant still poses challenges for existing open-source models, but DoctorFLAN can help them significantly. It demonstrates that the doctor-oriented dataset and benchmarks we construct can complement existing patient-oriented work and better promote medical LLMs research.

------------

`[2406.18064] Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need <https://arxiv.org/abs/2406.18064>`__ 评估检索增强生成的答案质量:一个强大的LLM就是你所需要的

::

    Wed, 26 Jun 2024 04:49:41 GMT
    Yang Wang, Alberto Garcia Hernandez, Roman Kyslyi, Nicholas Kersting

We present a comprehensive evaluation of answer quality in Retrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel grading system that is designed to assess correctness, completeness, and honesty. We further map the grading of quality aspects aforementioned into a binary score, indicating an accept or reject decision, mirroring the intuitive "thumbs-up" or "thumbs-down" gesture commonly used in chat applications. This approach suits factual business settings where a clear decision opinion is essential. Our assessment applies vRAG-Eval to two Large Language Models (LLMs), evaluating the quality of answers generated by a vanilla RAG application. We compare these evaluations with human expert judgments and find a substantial alignment between GPT-4's assessments and those of human experts, reaching 83% agreement on accept or reject decisions. This study highlights the potential of LLMs as reliable evaluators in closed-domain, closed-ended settings, particularly when human evaluations require significant resources.

------------

`[2406.18134] Assessing "Implicit" Retrieval Robustness of Large Language Models <https://arxiv.org/abs/2406.18134>`__ 大型语言模型的“隐式”检索鲁棒性评估

::

    Wed, 26 Jun 2024 07:38:24 GMT
    Xiaoyu Shen, Rexhina Blloshmi, Dawei Zhu, Jiahuan Pei, Wei Zhang

Retrieval-augmented generation has gained popularity as a framework to enhance large language models with external knowledge. However, its effectiveness hinges on the retrieval robustness of the model. If the model lacks retrieval robustness, its performance is constrained by the accuracy of the retriever, resulting in significant compromises when the retrieved context is irrelevant. In this paper, we evaluate the "implicit" retrieval robustness of various large language models, instructing them to directly output the final answer without explicitly judging the relevance of the retrieved context. Our findings reveal that fine-tuning on a mix of gold and distracting context significantly enhances the model's robustness to retrieval inaccuracies, while still maintaining its ability to extract correct answers when retrieval is accurate. This suggests that large language models can implicitly handle relevant or irrelevant retrieved context by learning solely from the supervision of the final answer in an end-to-end manner. Introducing an additional process for explicit relevance judgment can be unnecessary and disrupts the end-to-end approach.

------------

`[2406.17415] Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels <https://arxiv.org/abs/2406.17415>`__ 分层量化:一种对整数比特级以外的llm进行量化的实用有效方法

::

    replaced with revised version Wed, 26 Jun 2024 08:00:18 GMT
    Submission history From: Razvan Dumitru [view email]
    [v1] Tue, 25 Jun 2024 09:37:15 UTC (3,927 KB)
    [v2] Wed, 26 Jun 2024 08:00:18 UTC (3,927 KB)
    Razvan-Gabriel Dumitru, Vikas Yadav, Rishabh Maheshwary, Paul-Ioan Clotan, Sathwik Tejaswi Madhusudhan, Mihai Surdeanu

We present a simple variable quantization approach that quantizes different layers of a large language model (LLM) at different bit levels. Specifically, we quantize the most important layers to higher bit precision and less important layers to lower bits to achieve floating point quantization levels. We propose two effective strategies to measure the importance of layers within LLMs: the first measures the importance of a layer based on how different its output embeddings are from the input embeddings (the higher the better); the second estimates the importance of a layer using the number of layer weights that are much larger than average (the smaller the better). We show that quantizing different layers at varying bits according to our importance scores results in minimal performance drop with a far more compressed model size. Finally, we present several practical key takeaways from our variable layer-wise quantization experiments: (a) LLM performance under variable quantization remains close to the original model until 25-50% of layers are moved in lower quantization using our proposed ordering but only until 5-10% if moved using no specific ordering; (b) Quantizing LLMs to lower bits performs substantially better than pruning unless extreme quantization (2-bit) is used; and (c) Layer-wise quantization to lower bits works better in the case of larger LLMs with more layers compared to smaller LLMs with fewer layers. The code used to run the experiments is available at: this https URL.

------------

---------
Agent (7)
---------

`[2406.17962] SimsChat: A Customisable Persona-Driven Role-Playing Agent <https://arxiv.org/abs/2406.17962>`__ SimsChat:可定制的角色驱动的角色扮演代理

::

    Tue, 25 Jun 2024 22:44:17 GMT
    Bohao Yang, Dong Liu, Chen Tang, Chenghao Xiao, Kun Zhao, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, Chenghua Lin

Large Language Models (LLMs) possess the remarkable capability to understand human instructions and generate high-quality text, enabling them to act as agents that simulate human behaviours. This capability allows LLMs to emulate human beings in a more advanced manner, beyond merely replicating simple human behaviours. However, there is a lack of exploring into leveraging LLMs to craft characters from several aspects. In this work, we introduce the Customisable Conversation Agent Framework, which employs LLMs to simulate real-world characters that can be freely customised according to different user preferences. The customisable framework is helpful for designing customisable characters and role-playing agents according to human's preferences. We first propose the SimsConv dataset, which comprises 68 different customised characters, 1,360 multi-turn role-playing dialogues, and encompasses 13,971 interaction dialogues in total. The characters are created from several real-world elements, such as career, aspiration, trait, and skill. Building on these foundations, we present SimsChat, a freely customisable role-playing agent. It incorporates different real-world scenes and topic-specific character interaction dialogues, simulating characters' life experiences in various scenarios and topic-specific interactions with specific emotions. Experimental results show that our proposed framework achieves desirable performance and provides helpful guideline for building better simulacra of human beings in the future. Our data and code are available at https://github.com/Bernard-Yang/SimsChat.

------------

`[2406.18082] Octo-planner: On-device Language Model for Planner-Action Agents <https://arxiv.org/abs/2406.18082>`__ Octo-planner:规划器-行动智能体的设备上语言模型

::

    Wed, 26 Jun 2024 05:40:10 GMT
    Wei Chen, Zhiyuan Li, Zhen Guo, Yikang Shen

AI agents have become increasingly significant in various domains, enabling autonomous decision-making and problem-solving. To function effectively, these agents require a planning process that determines the best course of action and then executes the planned actions. In this paper, we present an efficient on-device Planner-Action framework that separates planning and action execution into two distinct components: a planner agent based on Phi-3 Mini, a 3.8 billion parameter LLM optimized for edge devices, and an action agent using the Octopus model for function execution. The planner agent first responds to user queries by decomposing tasks into a sequence of sub-steps, which are then executed by the action agent. To optimize performance on resource-constrained devices, we employ model fine-tuning instead of in-context learning, reducing computational costs and energy consumption while improving response times. Our approach involves using GPT-4 to generate diverse planning queries and responses based on available functions, with subsequent validations to ensure data quality. We fine-tune the Phi-3 Mini model on this curated dataset, achieving a 97\% success rate in our in-domain test environment. To address multi-domain planning challenges, we developed a multi-LoRA training method that merges weights from LoRAs trained on distinct function subsets. This approach enables flexible handling of complex, multi-domain queries while maintaining computational efficiency on resource-constrained devices. To support further research, we have open-sourced our model weights at \url{https://huggingface.co/NexaAIDev/octopus-planning}. For the demo, please refer to \url{https://www.nexa4ai.com/octo-planner}.

------------

`[2406.18532] Symbolic Learning Enables Self-Evolving Agents <https://arxiv.org/abs/2406.18532>`__ 符号学习使智能体能够自我进化

::

    Wed, 26 Jun 2024 17:59:18 GMT
    Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang

The AI community has been exploring a pathway to artificial general intelligence (AGI) by developing "language agents", which are complex large language models (LLMs) pipelines involving both prompting techniques and tool usage methods. While language agents have demonstrated impressive capabilities for many real-world tasks, a fundamental limitation of current language agents research is that they are model-centric, or engineering-centric. That's to say, the progress on prompts, tools, and pipelines of language agents requires substantial manual engineering efforts from human experts rather than automatically learning from data. We believe the transition from model-centric, or engineering-centric, to data-centric, i.e., the ability of language agents to autonomously learn and evolve in environments, is the key for them to possibly achieve AGI.
In this work, we introduce agent symbolic learning, a systematic framework that enables language agents to optimize themselves on their own in a data-centric way using symbolic optimizers. Specifically, we consider agents as symbolic networks where learnable weights are defined by prompts, tools, and the way they are stacked together. Agent symbolic learning is designed to optimize the symbolic network within language agents by mimicking two fundamental algorithms in connectionist learning: back-propagation and gradient descent. Instead of dealing with numeric weights, agent symbolic learning works with natural language simulacrums of weights, loss, and gradients. We conduct proof-of-concept experiments on both standard benchmarks and complex real-world tasks and show that agent symbolic learning enables language agents to update themselves after being created and deployed in the wild, resulting in "self-evolving agents".

------------

`[2406.18505] Mental Modeling of Reinforcement Learning Agents by Language Models <https://arxiv.org/abs/2406.18505>`__ 基于语言模型的强化学习智能体心智建模

::

    Wed, 26 Jun 2024 17:14:45 GMT
    Wenhao Lu, Xufeng Zhao, Josua Spisak, Jae Hee Lee, Stefan Wermter

Can emergent language models faithfully model the intelligence of decision-making agents? Though modern language models exhibit already some reasoning ability, and theoretically can potentially express any probable distribution over tokens, it remains underexplored how the world knowledge these pretrained models have memorized can be utilized to comprehend an agent's behaviour in the physical world. This study empirically examines, for the first time, how well large language models (LLMs) can build a mental model of agents, termed agent mental modelling, by reasoning about an agent's behaviour and its effect on states from agent interaction history. This research may unveil the potential of leveraging LLMs for elucidating RL agent behaviour, addressing a key challenge in eXplainable reinforcement learning (XRL). To this end, we propose specific evaluation metrics and test them on selected RL task datasets of varying complexity, reporting findings on agent mental model establishment.
Our results disclose that LLMs are not yet capable of fully mental modelling agents through inference alone without further innovations. This work thus provides new insights into the capabilities and limitations of modern LLMs.

------------

`[2406.16620] OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer <https://arxiv.org/abs/2406.16620>`__ OmAgent:任务分治的复杂视频理解多模态Agent框架

::

    Mon, 24 Jun 2024 13:05:39 GMT
    Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, Kyusong Lee

Recent advancements in Large Language Models (LLMs) have expanded their capabilities to multimodal contexts, including comprehensive video understanding. However, processing extensive videos such as 24-hour CCTV footage or full-length films presents significant challenges due to the vast data and processing demands. Traditional methods, like extracting key frames or converting frames to text, often result in substantial information loss. To address these shortcomings, we develop OmAgent, efficiently stores and retrieves relevant video frames for specific queries, preserving the detailed content of videos. Additionally, it features an Divide-and-Conquer Loop capable of autonomous reasoning, dynamically invoking APIs and tools to enhance query processing and accuracy. This approach ensures robust video understanding, significantly reducing information loss. Experimental results affirm OmAgent's efficacy in handling various types of videos and complex tasks. Moreover, we have endowed it with greater autonomy and a robust tool-calling system, enabling it to accomplish even more intricate tasks.

------------

`[2404.05569] 360$^\circ$REA: Towards A Reusable Experience Accumulation with 360{\deg} Assessment for Multi-Agent System <https://arxiv.org/abs/2404.05569>`__ 360$^\circ$REA:基于360度评估的多智能体系统可重用经验积累

::

    replaced with revised version Wed, 26 Jun 2024 11:42:10 GMT
    Submission history From: Hao Li [view email]
    [v1] Mon, 8 Apr 2024 14:43:13 UTC (671 KB)
    [v2] Wed, 26 Jun 2024 11:42:10 UTC (672 KB)
    Shen Gao, Hao Li, Chengrui Huang, Quan Tu, Zhiliang Tian, Minlie Huang, Shuo Shang

Large language model agents have demonstrated remarkable advancements across various complex tasks. Recent works focus on optimizing the agent team or employing self-reflection to iteratively solve complex tasks. Since these agents are all based on the same LLM, only conducting self-evaluation or removing underperforming agents does not substantively enhance the capability of the agents. We argue that a comprehensive evaluation and accumulating experience from evaluation feedback is an effective approach to improving system performance. In this paper, we propose Reusable Experience Accumulation with 360$^\circ$ Assessment (360$^\circ$REA), a hierarchical multi-agent framework inspired by corporate organizational practices. The framework employs a novel 360$^\circ$ performance assessment method for multi-perspective performance evaluation with fine-grained assessment. To enhance the capability of agents in addressing complex tasks, we introduce dual-level experience pool for agents to accumulate experience through fine-grained assessment. Extensive experiments on complex task datasets demonstrate the effectiveness of 360$^\circ$REA.

------------

`[2406.14711] MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate <https://arxiv.org/abs/2406.14711>`__ 多智能体协作攻击:基于辩论的大型语言模型协作对抗攻击研究

::

    replaced with revised version Wed, 26 Jun 2024 16:05:20 GMT
    Submission history From: Alfonso Amayuelas [view email]
    [v1] Thu, 20 Jun 2024 20:09:37 UTC (1,655 KB)
    [v2] Wed, 26 Jun 2024 16:05:20 UTC (1,655 KB)
    Alfonso Amayuelas, Xianjun Yang, Antonis Antoniades, Wenyue Hua, Liangming Pan, William Wang

Large Language Models (LLMs) have shown exceptional results on current benchmarks when working individually. The advancement in their capabilities, along with a reduction in parameter size and inference times, has facilitated the use of these models as agents, enabling interactions among multiple models to execute complex tasks. Such collaborations offer several advantages, including the use of specialized models (e.g. coding), improved confidence through multiple computations, and enhanced divergent thinking, leading to more diverse outputs. Thus, the collaborative use of language models is expected to grow significantly in the coming years. In this work, we evaluate the behavior of a network of models collaborating through debate under the influence of an adversary. We introduce pertinent metrics to assess the adversary's effectiveness, focusing on system accuracy and model agreement. Our findings highlight the importance of a model's persuasive ability in influencing others. Additionally, we explore inference-time methods to generate more compelling arguments and evaluate the potential of prompt-based mitigation as a defensive strategy.

------------

----------
Other (88)
----------

`[2406.17840] Human-Object Interaction from Human-Level Instructions <https://arxiv.org/abs/2406.17840>`__ 基于人级指令的人-物交互

::

    Tue, 25 Jun 2024 17:46:28 GMT
    Zhen Wu, Jiaman Li, C. Karen Liu

Intelligent agents need to autonomously navigate and interact within contextual environments to perform a wide range of daily tasks based on human-level instructions. These agents require a foundational understanding of the world, incorporating common sense and knowledge, to interpret such instructions. Moreover, they must possess precise low-level skills for movement and interaction to execute the detailed task plans derived from these instructions. In this work, we address the task of synthesizing continuous human-object interactions for manipulating large objects within contextual environments, guided by human-level instructions. Our goal is to generate synchronized object motion, full-body human motion, and detailed finger motion, all essential for realistic interactions. Our framework consists of a large language model (LLM) planning module and a low-level motion generator. We use LLMs to deduce spatial object relationships and devise a method for accurately determining their positions and orientations in target scene layouts.
Additionally, the LLM planner outlines a detailed task plan specifying a sequence of sub-tasks. This task plan, along with the target object poses, serves as input for our low-level motion generator, which seamlessly alternates between navigation and interaction modules. We present the first complete system that can synthesize object motion, full-body motion, and finger motion simultaneously from human-level instructions. Our experiments demonstrate the effectiveness of our high-level planner in generating plausible target layouts and our low-level motion generator in synthesizing realistic interactions for diverse objects. Please refer to our project page for more results: https://hoifhli.github.io/.

------------

`[2406.18346] AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations <https://arxiv.org/abs/2406.18346>`__ 通过从人类反馈中强化学习的人工智能对齐?矛盾与局限

::

    Wed, 26 Jun 2024 13:42:13 GMT
    Adam Dahlgren Lindstr\"om, Leila Methnani, Lea Krause, Petter Ericson, \'I\~nigo Mart\'inez de Rituerto de Troya, Dimitri Coelho Mollo, Roel Dobbe

This paper critically evaluates the attempts to align Artificial Intelligence (AI) systems, especially Large Language Models (LLMs), with human values and intentions through Reinforcement Learning from Feedback (RLxF) methods, involving either human feedback (RLHF) or AI feedback (RLAIF). Specifically, we show the shortcomings of the broadly pursued alignment goals of honesty, harmlessness, and helpfulness. Through a multidisciplinary sociotechnical critique, we examine both the theoretical underpinnings and practical implementations of RLxF techniques, revealing significant limitations in their approach to capturing the complexities of human ethics and contributing to AI safety. We highlight tensions and contradictions inherent in the goals of RLxF.
In addition, we discuss ethically-relevant issues that tend to be neglected in discussions about alignment and RLxF, among which the trade-offs between user-friendliness and deception, flexibility and interpretability, and system safety. We conclude by urging researchers and practitioners alike to critically assess the sociotechnical ramifications of RLxF, advocating for a more nuanced and reflective approach to its application in AI development.

------------

`[2406.17803] Understanding the Role of User Profile in the Personalization of Large Language Models <https://arxiv.org/abs/2406.17803>`__ 理解用户画像在大型语言模型个性化中的作用

::

    Sat, 22 Jun 2024 14:32:35 GMT
    Bin Wu, Zhengyan Shi, Hossein A. Rahmani, Varsha Ramineni, Emine Yilmaz

Utilizing user profiles to personalize Large Language Models (LLMs) has been shown to enhance the performance on a wide range of tasks. However, the precise role of user profiles and their effect mechanism on LLMs remains unclear. This study first confirms that the effectiveness of user profiles is primarily due to personalization information rather than semantic information. Furthermore, we investigate how user profiles affect the personalization of LLMs. Within the user profile, we reveal that it is the historical personalized response produced or approved by users that plays a pivotal role in personalizing LLMs.
This discovery unlocks the potential of LLMs to incorporate a greater number of user profiles within the constraints of limited input length. As for the position of user profiles, we observe that user profiles integrated into different positions of the input context do not contribute equally to personalization. Instead, where the user profile that is closer to the beginning affects more on the personalization of LLMs. Our findings reveal the role of user profiles for the personalization of LLMs, and showcase how incorporating user profiles impacts performance providing insight to leverage user profiles effectively.

------------

`[2406.17805] Can LLMs Generate Visualizations with Dataless Prompts? <https://arxiv.org/abs/2406.17805>`__ llm可以生成具有无数据提示的可视化吗?

::

    Sat, 22 Jun 2024 22:59:09 GMT
    Darius Coelho, Harshit Barot, Naitik Rathod, Klaus Mueller

Recent advancements in large language models have revolutionized information access, as these models harness data available on the web to address complex queries, becoming the preferred information source for many users. In certain cases, queries are about publicly available data, which can be effectively answered with data visualizations. In this paper, we investigate the ability of large language models to provide accurate data and relevant visualizations in response to such queries. Specifically, we investigate the ability of GPT-3 and GPT-4 to generate visualizations with dataless prompts, where no data accompanies the query. We evaluate the results of the models by comparing them to visualization cheat sheets created by visualization experts.

------------

`[2406.17806] MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries? <https://arxiv.org/abs/2406.17806>`__ MOSSBench:你的多模态语言模型对安全查询过度敏感吗?

::

    Sat, 22 Jun 2024 23:26:07 GMT
    Xirui Li, Hengguang Zhou, Ruochen Wang, Tianyi Zhou, Minhao Cheng, Cho-Jui Hsieh

Humans are prone to cognitive distortions -- biased thinking patterns that lead to exaggerated responses to specific stimuli, albeit in very different contexts. This paper demonstrates that advanced Multimodal Large Language Models (MLLMs) exhibit similar tendencies. While these models are designed to respond queries under safety mechanism, they sometimes reject harmless queries in the presence of certain visual stimuli, disregarding the benign nature of their contexts. As the initial step in investigating this behavior, we identify three types of stimuli that trigger the oversensitivity of existing MLLMs: Exaggerated Risk, Negated Harm, and Counterintuitive Interpretation. To systematically evaluate MLLMs' oversensitivity to these stimuli, we propose the Multimodal OverSenSitivity Benchmark (MOSSBench). This toolkit consists of 300 manually collected benign multimodal queries, cross-verified by third-party reviewers (AMT). Empirical studies using MOSSBench on 20 MLLMs reveal several insights: (1). Oversensitivity is prevalent among SOTA MLLMs, with refusal rates reaching up to 76% for harmless queries. (2). Safer models are more oversensitive: increasing safety may inadvertently raise caution and conservatism in the model's responses. (3). Different types of stimuli tend to cause errors at specific stages -- perception, intent reasoning, and safety judgement -- in the response process of MLLMs. These findings highlight the need for refined safety mechanisms that balance caution with contextually appropriate responses, improving the reliability of MLLMs in real-world applications. We make our project available at https://turningpoint-ai.github.io/MOSSBench/.

------------

`[2406.17807] Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary <https://arxiv.org/abs/2406.17807>`__ 非完全信息牌游戏解说策略的增强——关丹解说大型语言模型研究

::

    Sun, 23 Jun 2024 11:58:26 GMT
    Meiling Tao.Xuechen Liang, Yiling Tao, Tianyu Shi

Recent advancements in large language models (LLMs) have unlocked the potential for generating high-quality game commentary. However, producing insightful and engaging commentary for complex games with incomplete information remains a significant challenge. In this paper, we introduce a novel commentary method that combine Reinforcement Learning (RL) and LLMs, tailored specifically for the Chinese card game \textit{Guandan}. Our system leverages RL to generate intricate card-playing scenarios and employs LLMs to generate corresponding commentary text, effectively emulating the strategic analysis and narrative prowess of professional commentators. The framework comprises a state commentary guide, a Theory of Mind (ToM)-based strategy analyzer, and a style retrieval module, which seamlessly collaborate to deliver detailed and context-relevant game commentary in the Chinese language environment. We empower LLMs with ToM capabilities and refine both retrieval and information filtering mechanisms. This facilitates the generation of personalized commentary content. Our experimental results showcase the substantial enhancement in performance achieved by the proposed commentary framework when applied to open-source LLMs, surpassing the performance of GPT-4 across multiple evaluation metrics.

------------

`[2406.17923] PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning <https://arxiv.org/abs/2406.17923>`__ PAFT:一种有效的LLM微调并行训练范式

::

    Tue, 25 Jun 2024 20:11:37 GMT
    Shiva Kumar Pentyala, Zhichao Wang, Bin Bi, Kiran Ramnath, Xiang-Bo Mao, Regunathan Radhakrishnan, Sitaram Asur, Na (Claire) Cheng

Large language models (LLMs) have shown remarkable abilities in diverse natural language processing (NLP) tasks. The LLMs generally undergo supervised fine-tuning (SFT) followed by preference alignment to be usable in downstream applications. However, this sequential training pipeline leads to alignment tax that degrades the LLM performance.
This paper introduces PAFT, a new PArallel training paradigm for effective LLM Fine-Tuning, which independently performs SFT and preference alignment (e.g., DPO and ORPO, etc.) with the same pre-trained model on respective datasets. The model produced by SFT and the model from preference alignment are then merged into a final model by parameter fusing for use in downstream applications. This work reveals important findings that preference alignment like DPO naturally results in a sparse model while SFT leads to a natural dense model which needs to be sparsified for effective model merging. This paper introduces an effective interference resolution which reduces the redundancy by sparsifying the delta parameters. The LLM resulted from the new training paradigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard.
Comprehensive evaluation shows the effectiveness of the parallel training paradigm.

------------

`[2406.17947] Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias <https://arxiv.org/abs/2406.17947>`__ 他们指的是“我们”吗?解释群体间偏见中的指代表达

::

    Tue, 25 Jun 2024 21:47:53 GMT
    Venkata S Govindarajan, Matianyu Zang, Kyle Mahowald, David Beaver, Junyi Jessy Li

The variations between in-group and out-group speech (intergroup bias) are subtle and could underlie many social phenomena like stereotype perpetuation and implicit bias. In this paper, we model the intergroup bias as a tagging task on English sports comments from forums dedicated to fandom for NFL teams.
We curate a unique dataset of over 6 million game-time comments from opposing perspectives (the teams in the game), each comment grounded in a non-linguistic description of the events that precipitated these comments (live win probabilities for each team). Expert and crowd annotations justify modeling the bias through tagging of implicit and explicit referring expressions and reveal the rich, contextual understanding of language and the world required for this task. For large-scale analysis of intergroup variation, we use LLMs for automated tagging, and discover that some LLMs perform best when prompted with linguistic descriptions of the win probability at the time of the comment, rather than numerical probability. Further, large-scale tagging of comments using LLMs uncovers linear variations in the form of referent across win probabilities that distinguish in-group and out-group utterances. Code and data are available at https://github.com/venkatasg/intergroup-nfl .

------------

`[2406.17967] Unmasking the Imposters: In-Domain Detection of Human vs. Machine-Generated Tweets <https://arxiv.org/abs/2406.17967>`__ 揭秘冒名顶替者:人工与机器生成推文的域内检测

::

    Tue, 25 Jun 2024 22:49:17 GMT
    Bryan E. Tuck and Rakesh M. Verma

The rapid development of large language models (LLMs) has significantly improved the generation of fluent and convincing text, raising concerns about their misuse on social media platforms. We present a methodology using Twitter datasets to examine the generative capabilities of four LLMs: Llama 3, Mistral, Qwen2, and GPT4o. We evaluate 7B and 8B parameter base-instruction models of the three open-source LLMs and validate the impact of further fine-tuning and "uncensored" versions. Our findings show that "uncensored" models with additional in-domain fine-tuning dramatically reduce the effectiveness of automated detection methods. This study addresses a gap by exploring smaller open-source models and the effects of "uncensoring," providing insights into how fine-tuning and content moderation influence machine-generated text detection.

------------

`[2406.17975] Inherent Challenges of Post-Hoc Membership Inference for Large Language Models <https://arxiv.org/abs/2406.17975>`__ 

::

    Tue, 25 Jun 2024 23:12:07 GMT
    Matthieu Meeus, Shubham Jain, Marek Rei, Yves-Alexandre de Montjoye

Large Language Models (LLMs) are often trained on vast amounts of undisclosed data, motivating the development of post-hoc Membership Inference Attacks (MIAs) to gain insight into their training data composition. However, in this paper, we identify inherent challenges in post-hoc MIA evaluation due to potential distribution shifts between collected member and non-member datasets.
Using a simple bag-of-words classifier, we demonstrate that datasets used in recent post-hoc MIAs suffer from significant distribution shifts, in some cases achieving near-perfect distinction between members and non-members. This implies that previously reported high MIA performance may be largely attributable to these shifts rather than model memorization. We confirm that randomized, controlled setups eliminate such shifts and thus enable the development and fair evaluation of new MIAs. However, we note that such randomized setups are rarely available for the latest LLMs, making post-hoc data collection still required to infer membership for real-world LLMs. As a potential solution, we propose a Regression Discontinuity Design (RDD) approach for post-hoc data collection, which substantially mitigates distribution shifts. Evaluating various MIA methods on this RDD setup yields performance barely above random guessing, in stark contrast to previously reported results.
Overall, our findings highlight the challenges in accurately measuring LLM memorization and the need for careful experimental design in (post-hoc) membership inference tasks.

------------

`[2406.17990] Explicit Diversity Conditions for Effective Question Answer Generation with Large Language Models <https://arxiv.org/abs/2406.17990>`__ 基于大型语言模型的有效问答生成的显式多样性条件

::

    Wed, 26 Jun 2024 00:12:08 GMT
    Vikas Yadav and Hyuk Joon Kwon and Vijay Srinivasan and Hongxia Jin

Question Answer Generation (QAG) is an effective data augmentation technique to improve the accuracy of question answering systems, especially in low-resource domains. While recent pretrained and large language model-based QAG methods have made substantial progress, they face the critical issue of redundant QA pair generation, affecting downstream QA systems. Implicit diversity techniques such as sampling and diverse beam search are proven effective solutions but often yield smaller diversity. We present explicit diversity conditions for QAG, focusing on spatial aspects, question types, and entities, substantially increasing diversity in QA generation. Our work emphasizes the need of explicit diversity conditions for generating diverse question-answer synthetic data by showing significant improvements in downstream QA task over existing widely adopted implicit diversity techniques.
In particular, generated QA pairs from explicit diversity conditions when used to train the downstream QA model results in an average 4.1% exact match and 4.5% F1 improvement over QAG from implicit sampling techniques on SQuADDU. Our work emphasizes the need for explicit diversity conditions even more in low-resource datasets (SubjQA), where average downstream QA performance improvements are around 12% EM.

------------

`[2406.17992] Catching Chameleons: Detecting Evolving Disinformation Generated using Large Language Models <https://arxiv.org/abs/2406.17992>`__ 捕捉变色龙:检测使用大型语言模型生成的不断演化的虚假信息

::

    Wed, 26 Jun 2024 00:21:39 GMT
    Bohan Jiang, Chengshuai Zhao, Zhen Tan, Huan Liu

Despite recent advancements in detecting disinformation generated by large language models (LLMs), current efforts overlook the ever-evolving nature of this disinformation. In this work, we investigate a challenging yet practical research problem of detecting evolving LLM-generated disinformation.
Disinformation evolves constantly through the rapid development of LLMs and their variants. As a consequence, the detection model faces significant challenges. First, it is inefficient to train separate models for each disinformation generator. Second, the performance decreases in scenarios when evolving LLM-generated disinformation is encountered in sequential order. To address this problem, we propose DELD (Detecting Evolving LLM-generated Disinformation), a parameter-efficient approach that jointly leverages the general fact-checking capabilities of pre-trained language models (PLM) and the independent disinformation generation characteristics of various LLMs. In particular, the learned characteristics are concatenated sequentially to facilitate knowledge accumulation and transformation. DELD addresses the issue of label scarcity by integrating the semantic embeddings of disinformation with trainable soft prompts to elicit model-specific knowledge. Our experiments show that \textit{DELD} significantly outperforms state-of-the-art methods.
Moreover, our method provides critical insights into the unique patterns of disinformation generation across different LLMs, offering valuable perspectives in this line of research.

------------

`[2406.18027] Automated Clinical Data Extraction with Knowledge Conditioned LLMs <https://arxiv.org/abs/2406.18027>`__ 基于知识条件llm的自动临床数据提取

::

    Wed, 26 Jun 2024 02:49:28 GMT
    Diya Li, Asim Kadav, Aijing Gao, Rui Li, Richard Bourgon

The extraction of lung lesion information from clinical and medical imaging reports is crucial for research on and clinical care of lung-related diseases.
Large language models (LLMs) can be effective at interpreting unstructured text in reports, but they often hallucinate due to a lack of domain-specific knowledge, leading to reduced accuracy and posing challenges for use in clinical settings. To address this, we propose a novel framework that aligns generated internal knowledge with external knowledge through in-context learning (ICL). Our framework employs a retriever to identify relevant units of internal or external knowledge and a grader to evaluate the truthfulness and helpfulness of the retrieved internal-knowledge rules, to align and update the knowledge bases. Our knowledge-conditioned approach also improves the accuracy and reliability of LLM outputs by addressing the extraction task in two stages: (i) lung lesion finding detection and primary structured field parsing, followed by (ii) further parsing of lesion description text into additional structured fields. Experiments with expert-curated test datasets demonstrate that this ICL approach can increase the F1 score for key fields (lesion size, margin and solidity) by an average of 12.9% over existing ICL methods.

------------

`[2406.18045] PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry <https://arxiv.org/abs/2406.18045>`__ pharmacgpt:生物制药和化学领域特定大型语言模型

::

    Wed, 26 Jun 2024 03:43:09 GMT
    Linqing Chen, Weilei Wang, Zilong Bai, Peng Xu, Yan Fang, Jie Fang, Wentao Wu, Lizhi Zhou, Ruiji Zhang, Yubin Xia, Chaobo Xu, Ran Hu, Licong Xu, Qijun Cai, Haoran Hua, Jing Sun, Jin Liu, Tian Qiu, Haowen Liu, Meng Hu, Xiuwen Li, Fei Gao, Yufu Wang, Lin Tie, Chaochao Wang, Jianping Lu, Cheng Sun, Yixin Wang, Shengjie Yang, Yuancheng Li, Lu Jin, Lisha Zhang, Fu Bian, Changyang Tu

Large language models (LLMs) have revolutionized Natural Language Processing (NLP) by by minimizing the need for complex feature engineering. However, the application of LLMs in specialized domains like biopharmaceuticals and chemistry remains largely unexplored. These fields are characterized by intricate terminologies, specialized knowledge, and a high demand for precision areas where general purpose LLMs often fall short. In this study, we introduce PharmGPT, a suite of multilingual LLMs with 13 billion and 70 billion parameters, specifically trained on a comprehensive corpus of hundreds of billions of tokens tailored to the Bio-Pharmaceutical and Chemical sectors. Our evaluation shows that PharmGPT matches or surpasses existing general models on key benchmarks, such as NAPLEX, demonstrating its exceptional capability in domain-specific tasks. This advancement establishes a new benchmark for LLMs in the Bio-Pharmaceutical and Chemical fields, addressing the existing gap in specialized language modeling. Furthermore, this suggests a promising path for enhanced research and development in these specialized areas, paving the way for more precise and effective applications of NLP in specialized domains.

------------

`[2406.18049] Improving Entity Recognition Using Ensembles of Deep Learning and Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction from Multiple Sources <https://arxiv.org/abs/2406.18049>`__ 

::

    Wed, 26 Jun 2024 03:56:21 GMT
    Yiming Li, Deepthi Viswaroopan, William He, Jianfu Li, Xu Zuo, Hua Xu, Cui Tao

Adverse event (AE) extraction following COVID-19 vaccines from text data is crucial for monitoring and analyzing the safety profiles of immunizations.
Traditional deep learning models are adept at learning intricate feature representations and dependencies in sequential data, but often require extensive labeled data. In contrast, large language models (LLMs) excel in understanding contextual information, but exhibit unstable performance on named entity recognition tasks, possibly due to their broad but unspecific training.
This study aims to evaluate the effectiveness of LLMs and traditional deep learning models in AE extraction, and to assess the impact of ensembling these models on performance. In this study, we utilized reports and posts from the VAERS (n=621), Twitter (n=9,133), and Reddit (n=131) as our corpora. Our goal was to extract three types of entities: "vaccine", "shot", and "ae". We explored and fine-tuned (except GPT-4) multiple LLMs, including GPT-2, GPT-3.5, GPT-4, and Llama-2, as well as traditional deep learning models like RNN and BioBERT. To enhance performance, we created ensembles of the three models with the best performance. For evaluation, we used strict and relaxed F1 scores to evaluate the performance for each entity type, and micro-average F1 was used to assess the overall performance. The ensemble model achieved the highest performance in "vaccine", "shot", and "ae" with strict F1-scores of 0.878, 0.930, and 0.925, respectively, along with a micro-average score of 0.903. In conclusion, this study demonstrates the effectiveness and robustness of ensembling fine-tuned traditional deep learning models and LLMs, for extracting AE-related information. This study contributes to the advancement of biomedical natural language processing, providing valuable insights into improving AE extraction from text data for pharmacovigilance and public health surveillance.

------------

`[2406.18078] Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction <https://arxiv.org/abs/2406.18078>`__ 基于伪标签评分器自训练的方面情感四元组预测

::

    Wed, 26 Jun 2024 05:30:21 GMT
    Yice Zhang, Jie Zeng, Weiming Hu, Ziyi Wang, Shiwei Chen, Ruifeng Xu

Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect term, aspect category, opinion term, sentiment polarity) for a given review, which is the most representative and challenging task in aspect-based sentiment analysis. A key challenge in the ASQP task is the scarcity of labeled data, which limits the performance of existing methods. To tackle this issue, we propose a self-training framework with a pseudo-label scorer, wherein a scorer assesses the match between reviews and their pseudo-labels, aiming to filter out mismatches and thereby enhance the effectiveness of self-training. We highlight two critical aspects to ensure the scorer's effectiveness and reliability: the quality of the training dataset and its model architecture. To this end, we create a human-annotated comparison dataset and train a generative model on it using ranking-based objectives. Extensive experiments on public ASQP datasets reveal that using our scorer can greatly and consistently improve the effectiveness of self-training. Moreover, we explore the possibility of replacing humans with large language models for comparison dataset annotation, and experiments demonstrate its feasibility. We release our code and data at https://github.com/HITSZ-HLT/ST-w-Scorer-ABSA .

------------

`[2406.18088] LLM-Driven Multimodal Opinion Expression Identification <https://arxiv.org/abs/2406.18088>`__ llm驱动的多模态观点表达识别

::

    Wed, 26 Jun 2024 05:52:47 GMT
    Bonian Jia and Huiyao Chen and Yueheng Sun and Meishan Zhang and Min Zhang

Opinion Expression Identification (OEI) is essential in NLP for applications ranging from voice assistants to depression diagnosis. This study extends OEI to encompass multimodal inputs, underlining the significance of auditory cues in delivering emotional subtleties beyond the capabilities of text. We introduce a novel multimodal OEI (MOEI) task, integrating text and speech to mirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we construct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is applied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template for the OEI task to take full advantage of the generative power of large language models (LLMs). Advancing further, we propose an LLM-driven method STOEI, which combines speech and text modal to identify opinion expressions.
Our experiments demonstrate that MOEI significantly improves the performance while our method outperforms existing methods by 9.20\% and obtains SOTA results.

------------

`[2406.18116] BADGE: BADminton report Generation and Evaluation with LLM <https://arxiv.org/abs/2406.18116>`__ 徽章:与LLM一起生成和评估羽毛球报告

::

    Wed, 26 Jun 2024 07:07:52 GMT
    Shang-Hsuan Chiang, Lin-Wei Chao, Kuang-Da Wang, Chih-Chuan Wang, Wen-Chih Peng

Badminton enjoys widespread popularity, and reports on matches generally include details such as player names, game scores, and ball types, providing audiences with a comprehensive view of the games. However, writing these reports can be a time-consuming task. This challenge led us to explore whether a Large Language Model (LLM) could automate the generation and evaluation of badminton reports. We introduce a novel framework named BADGE, designed for this purpose using LLM. Our method consists of two main phases: Report Generation and Report Evaluation. Initially, badminton-related data is processed by the LLM, which then generates a detailed report of the match. We tested different Input Data Types, In-Context Learning (ICL), and LLM, finding that GPT-4 performs best when using CSV data type and the Chain of Thought prompting. Following report generation, the LLM evaluates and scores the reports to assess their quality. Our comparisons between the scores evaluated by GPT-4 and human judges show a tendency to prefer GPT-4 generated reports.
Since the application of LLM in badminton reporting remains largely unexplored, our research serves as a foundational step for future advancements in this area. Moreover, our method can be extended to other sports games, thereby enhancing sports promotion. For more details, please refer to https://github.com/AndyChiangSH/BADGE.

------------

`[2406.18120] ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech Recognition Using LLMs <https://arxiv.org/abs/2406.18120>`__ ArzEn-LLM:使用llm进行代码转换的埃及阿拉伯语-英语翻译和语音识别

::

    Wed, 26 Jun 2024 07:19:51 GMT
    Ahmed Heakl, Youssef Zaghloul, Mennatullah Ali, Rania Hossam, Walid Gomaa

Motivated by the widespread increase in the phenomenon of code-switching between Egyptian Arabic and English in recent times, this paper explores the intricacies of machine translation (MT) and automatic speech recognition (ASR) systems, focusing on translating code-switched Egyptian Arabic-English to either English or Egyptian Arabic. Our goal is to present the methodologies employed in developing these systems, utilizing large language models such as LLama and Gemma. In the field of ASR, we explore the utilization of the Whisper model for code-switched Egyptian Arabic recognition, detailing our experimental procedures including data preprocessing and training techniques. Through the implementation of a consecutive speech-to-text translation system that integrates ASR with MT, we aim to overcome challenges posed by limited resources and the unique characteristics of the Egyptian Arabic dialect.
Evaluation against established metrics showcases promising results, with our methodologies yielding a significant improvement of $56\%$ in English translation over the state-of-the-art and $9.3\%$ in Arabic translation. Since code-switching is deeply inherent in spoken languages, it is crucial that ASR systems can effectively handle this phenomenon. This capability is crucial for enabling seamless interaction in various domains, including business negotiations, cultural exchanges, and academic discourse. Our models and code are available as open-source resources. Code: \url{http://github.com/ahmedheakl/arazn-llm}}, Models: \url{http://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e}.

------------

`[2406.18122] Poisoned LangChain: Jailbreak LLMs by LangChain <https://arxiv.org/abs/2406.18122>`__ 

::

    Wed, 26 Jun 2024 07:21:02 GMT
    Ziqiu Wang, Jun Liu, Shengkai Zhang and Yang Yang

With the development of natural language processing (NLP), large language models (LLMs) are becoming increasingly popular. LLMs are integrating more into everyday life, raising public concerns about their security vulnerabilities.
Consequently, the security of large language models is becoming critically important. Currently, the techniques for attacking and defending against LLMs are continuously evolving. One significant method type of attack is the jailbreak attack, which designed to evade model safety mechanisms and induce the generation of inappropriate content. Existing jailbreak attacks primarily rely on crafting inducement prompts for direct jailbreaks, which are less effective against large models with robust filtering and high comprehension abilities. Given the increasing demand for real-time capabilities in large language models, real-time updates and iterations of new knowledge have become essential. Retrieval-Augmented Generation (RAG), an advanced technique to compensate for the model's lack of new knowledge, is gradually becoming mainstream. As RAG enables the model to utilize external knowledge bases, it provides a new avenue for jailbreak attacks.
In this paper, we conduct the first work to propose the concept of indirect jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on this, we further design a novel method of indirect jailbreak attack, termed Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to interact with large language models, thereby causing the large models to generate malicious non-compliant dialogues.We tested this method on six different large language models across three major categories of jailbreak issues. The experiments demonstrate that PLC successfully implemented indirect jailbreak attacks under three different scenarios, achieving success rates of 88.56%, 79.04%, and 82.69% respectively.

------------

`[2406.18125] ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets and Large Language Models <https://arxiv.org/abs/2406.18125>`__ ResumeAtlas:基于大规模数据集和大型语言模型的简历分类

::

    Wed, 26 Jun 2024 07:25:18 GMT
    Ahmed Heakl, Youssef Mohamed, Noran Mohamed, Ali Sharkaway, Ahmed Zaky

The increasing reliance on online recruitment platforms coupled with the adoption of AI technologies has highlighted the critical need for efficient resume classification methods. However, challenges such as small datasets, lack of standardized resume templates, and privacy concerns hinder the accuracy and effectiveness of existing classification models. In this work, we address these challenges by presenting a comprehensive approach to resume classification. We curated a large-scale dataset of 13,389 resumes from diverse sources and employed Large Language Models (LLMs) such as BERT and Gemma1.1 2B for classification. Our results demonstrate significant improvements over traditional machine learning approaches, with our best model achieving a top-1 accuracy of 92\% and a top-5 accuracy of 97.5\%. These findings underscore the importance of dataset quality and advanced model architectures in enhancing the accuracy and robustness of resume classification systems, thus advancing the field of online recruitment practices.

------------

`[2406.18133] ConvoCache: Smart Re-Use of Chatbot Responses <https://arxiv.org/abs/2406.18133>`__ ConvoCache:智能重用聊天机器人的响应

::

    Wed, 26 Jun 2024 07:35:10 GMT
    Conor Atkins, Ian Wood, Mohamed Ali Kaafar, Hassan Asghar, Nardine Basta, Michal Kepkowski

We present ConvoCache, a conversational caching system that solves the problem of slow and expensive generative AI models in spoken chatbots.
ConvoCache finds a semantically similar prompt in the past and reuses the response. In this paper we evaluate ConvoCache on the DailyDialog dataset. We find that ConvoCache can apply a UniEval coherence threshold of 90% and respond to 89% of prompts using the cache with an average latency of 214ms, replacing LLM and voice synthesis that can take over 1s. To further reduce latency we test prefetching and find limited usefulness. Prefetching with 80% of a request leads to a 63% hit rate, and a drop in overall coherence. ConvoCache can be used with any chatbot to reduce costs by reducing usage of generative AI by up to 89%.

------------

`[2406.18164] NeBuLa: A discourse aware Minecraft Builder <https://arxiv.org/abs/2406.18164>`__ 星云:一个话语感知的Minecraft建造者

::

    Wed, 26 Jun 2024 08:24:44 GMT
    Akshay Chaturvedi, Kate Thompson, Nicholas Asher

When engaging in collaborative tasks, humans efficiently exploit the semantic structure of a conversation to optimize verbal and nonverbal interactions. But in recent "language to code" or "language to action" models, this information is lacking. We show how incorporating the prior discourse and nonlinguistic context of a conversation situated in a nonlinguistic environment can improve the "language to action" component of such interactions. We fine tune an LLM to predict actions based on prior context; our model, NeBuLa, doubles the net-action F1 score over the baseline on this task of Jayannavar et al.(2020).
We also investigate our model's ability to construct shapes and understand location descriptions using a synthetic dataset.

------------

`[2406.18173] UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs <https://arxiv.org/abs/2406.18173>`__ UIO-LLMs:长上下文LLMs的无偏增量优化

::

    Wed, 26 Jun 2024 08:44:36 GMT
    Wenhao Li and Mingbao Lin and Yunshan Zhong and Shuicheng Yan and Rongrong Ji

Managing long texts is challenging for large language models (LLMs) due to limited context window sizes. This study introduces UIO-LLMs, an unbiased incremental optimization approach for memory-enhanced transformers under long-context settings. We initially conceptualize the process as a streamlined encoder-decoder framework where the weights-shared encoder and decoder respectively encapsulate a context segment into memories and leverage these memories to predict outputs of the subsequent segment. Subsequently, by treating our memory-enhanced transformers as fully-connected recurrent neural networks (RNNs), we refine the training process using the Truncated Backpropagation Through Time (TBPTT) algorithm, which incorporates innovative incremental optimization techniques. These techniques not only diminish time complexity but also address the bias in gradient computation through an unbiased optimization process. UIO-LLMs successfully handle long context, such as extending the context window of Llama2-7b-chat from 4K to 100K tokens with minimal 2% additional parameters, while keeping the inference cost nearly linear as context length increases.

------------

`[2406.18187] Selective Prompting Tuning for Personalized Conversations with LLMs <https://arxiv.org/abs/2406.18187>`__ 与llm进行个性化对话的选择性提示调优

::

    Wed, 26 Jun 2024 09:03:52 GMT
    Qiushi Huang, Xubo Liu, Tom Ko, Bo Wu, Wenwu Wang, Yu Zhang, Lilian Tang

In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models' (LLMs) improved response coherence, effective persona integration remains a challenge.
In this work, we first study two common approaches for personalizing LLMs: textual prompting and direct fine-tuning. We observed that textual prompting often struggles to yield responses that are similar to the ground truths in datasets, while direct fine-tuning tends to produce repetitive or overly generic replies. To alleviate those issues, we propose \textbf{S}elective \textbf{P}rompt \textbf{T}uning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs.
Additionally, we propose context-prompt contrastive learning and prompt fusion learning to encourage the SPT to enhance the diversity of personalized conversations. Experiments on the CONVAI2 dataset demonstrate that SPT significantly enhances response diversity by up to 90\%, along with improvements in other critical performance indicators. Those results highlight the efficacy of SPT in fostering engaging and personalized dialogue generation.
The SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available for further exploration.

------------

`[2406.18192] Methodology of Adapting Large English Language Models for Specific Cultural Contexts <https://arxiv.org/abs/2406.18192>`__ 

::

    Wed, 26 Jun 2024 09:16:08 GMT
    Wenjing Zhang and Siqi Xiao and Xuejiao Lei and Ning Wang and Huazheng Zhang and Meijuan An and Bikun Yang and Zhaoxiang Liu and Kai Wang and Shiguo Lian

The rapid growth of large language models(LLMs) has emerged as a prominent trend in the field of artificial intelligence. However, current state-of-the-art LLMs are predominantly based on English. They encounter limitations when directly applied to tasks in specific cultural domains, due to deficiencies in domain-specific knowledge and misunderstandings caused by differences in cultural values. To address this challenge, our paper proposes a rapid adaptation method for large models in specific cultural contexts, which leverages instruction-tuning based on specific cultural knowledge and safety values data. Taking Chinese as the specific cultural context and utilizing the LLaMA3-8B as the experimental English LLM, the evaluation results demonstrate that the adapted LLM significantly enhances its capabilities in domain-specific knowledge and adaptability to safety values, while maintaining its original expertise advantages.

------------

`[2406.18219] A Closer Look into Mixture-of-Experts in Large Language Models <https://arxiv.org/abs/2406.18219>`__ 对大型语言模型中专家混合的仔细研究

::

    Wed, 26 Jun 2024 10:07:57 GMT
    Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, Jie Fu

Mixture-of-experts (MoE) is gaining increasing attention due to its unique properties and remarkable performance, especially for language tasks. By sparsely activating a subset of parameters for each token, MoE architecture could increase the model size without sacrificing computational efficiency, achieving a better trade-off between performance and training costs. However, the underlying mechanism of MoE still lacks further exploration, and its modularization degree remains questionable. In this paper, we make an initial attempt to understand the inner workings of MoE-based large language models.
Concretely, we comprehensively study the parametric and behavioral features of three recent MoE-based models and reveal some intriguing observations, including (1) Neurons act like fine-grained experts. (2) The router of MoE usually selects experts with larger output norms. (3) The expert diversity increases as the layer increases, while the last layer is an outlier. Based on the observations, we also provide suggestions for a broad spectrum of MoE practitioners, such as router design and expert allocation. We hope this work could shed light on future research on the MoE framework and other modular architectures. Code is available at https://github.com/kamanphoebe/Look-into-MoEs.

------------

`[2406.18221] Enhancing Data Privacy in Large Language Models through Private Association Editing <https://arxiv.org/abs/2406.18221>`__ 通过私有关联编辑增强大型语言模型的数据隐私性

::

    Wed, 26 Jun 2024 10:08:47 GMT
    Davide Venditti, Elena Sofia Ruzzetti, Giancarlo A. Xompero, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, Fabio Massimo Zanzotto

Large Language Models (LLMs) are powerful tools with extensive applications, but their tendency to memorize private information raises significant concerns as private data leakage can easily happen. In this paper, we introduce Private Association Editing (PAE), a novel defense approach for private data leakage.
PAE is designed to effectively remove Personally Identifiable Information (PII) without retraining the model. Our approach consists of a four-step procedure: detecting memorized PII, applying PAE cards to mitigate memorization of private data, verifying resilience to targeted data extraction (TDE) attacks, and ensuring consistency in the post-edit LLMs. The versatility and efficiency of PAE, which allows for batch modifications, significantly enhance data privacy in LLMs. Experimental results demonstrate the effectiveness of PAE in mitigating private data leakage. We believe PAE will serve as a critical tool in the ongoing effort to protect data privacy in LLMs, encouraging the development of safer models for real-world applications.

------------

`[2406.18256] LLaMIPa: An Incremental Discourse Parser <https://arxiv.org/abs/2406.18256>`__ LLaMIPa:一个增量篇章解析器

::

    Wed, 26 Jun 2024 11:08:17 GMT
    Kate Thompson, Akshay Chaturvedi, Julie Hunter, Nicholas Asher

This paper provides the first discourse parsing experiments with a large language model (LLM) finetuned on corpora annotated in the style of SDRT (Asher, 1993; Asher and Lascarides, 2003). The result is a discourse parser, LLaMIPa (LLaMA Incremental Parser), which is able to more fully exploit discourse context, leading to substantial performance gains over approaches that use encoder-only models to provide local, context-sensitive representations of discourse units. Furthermore, it is able to process discourse data incrementally, which is essential for the eventual use of discourse information in downstream tasks.

------------

`[2406.18259] Detecting Machine-Generated Texts: Not Just "AI vs Humans" and Explainability is Complicated <https://arxiv.org/abs/2406.18259>`__ 检测机器生成的文本:不仅仅是“人工智能vs人类”和可解释性是复杂的

::

    Wed, 26 Jun 2024 11:11:47 GMT
    Jiazhou Ji, Ruizhe Li, Shujun Li, Jie Guo, Weidong Qiu, Zheng Huang, Chiyu Chen, Xiaoyu Jiang, Xinru Lu

As LLMs rapidly advance, increasing concerns arise regarding risks about actual authorship of texts we see online and in real world. The task of distinguishing LLM-authored texts is complicated by the nuanced and overlapping behaviors of both machines and humans. In this paper, we challenge the current practice of considering LLM-generated text detection a binary classification task of differentiating human from AI. Instead, we introduce a novel ternary text classification scheme, adding an "undecided" category for texts that could be attributed to either source, and we show that this new category is crucial to understand how to make the detection result more explainable to lay users.
This research shifts the paradigm from merely classifying to explaining machine-generated texts, emphasizing need for detectors to provide clear and understandable explanations to users. Our study involves creating four new datasets comprised of texts from various LLMs and human authors. Based on new datasets, we performed binary classification tests to ascertain the most effective SOTA detection methods and identified SOTA LLMs capable of producing harder-to-detect texts. We constructed a new dataset of texts generated by two top-performing LLMs and human authors, and asked three human annotators to produce ternary labels with explanation notes. This dataset was used to investigate how three top-performing SOTA detectors behave in new ternary classification context. Our results highlight why "undecided" category is much needed from the viewpoint of explainability. Additionally, we conducted an analysis of explainability of the three best-performing detectors and the explanation notes of the human annotators, revealing insights about the complexity of explainable detection of machine-generated texts. Finally, we propose guidelines for developing future detection systems with improved explanatory power.

------------

`[2406.18266] "Vorbe\c{s}ti Rom\^ane\c{s}te?" A Recipe to Train Powerful Romanian LLMs with English Instructions <https://arxiv.org/abs/2406.18266>`__ “Vorbe \ c{年代}ti罗\ ^一\ c{年代}te ?”一个用英语指令训练强大的罗马尼亚llm的菜谱

::

    Wed, 26 Jun 2024 11:39:51 GMT
    Mihai Masala, Denis C. Ilie-Ablachim, Alexandru Dima, Dragos Corlatescu, Miruna Zavelca, Ovio Olaru, Simina Terian-Dan, Andrei Terian-Dan, Marius Leordeanu, Horia Velicu, Marius Popescu, Mihai Dascalu, Traian Rebedea

In recent years, Large Language Models (LLMs) have achieved almost human-like performance on various tasks. While some LLMs have been trained on multilingual data, most of the training data is in English; hence, their performance in English greatly exceeds other languages. To our knowledge, we are the first to collect and translate a large collection of texts, instructions, and benchmarks and train, evaluate, and release open-source LLMs tailored for Romanian. We evaluate our methods on four different categories, including academic benchmarks, MT-Bench (manually translated), and a professionally built historical, cultural, and social benchmark adapted to Romanian. We argue for the usefulness and high performance of RoLLMs by obtaining state-of-the-art results across the board. We publicly release all resources (i.e., data, training and evaluation code, models) to support and encourage research on Romanian LLMs while concurrently creating a generalizable recipe, adequate for other low or less-resourced languages.

------------

`[2406.18294] Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs <https://arxiv.org/abs/2406.18294>`__ 分层上下文剪枝:用存储库级预训练代码llm优化真实世界的代码补全

::

    Wed, 26 Jun 2024 12:26:16 GMT
    Lei Zhang, Yunshui Li, Jiaming Li, Xiaobo Xia, Jiaxi Yang, Run Luo, Minzheng Wang, Longze Chen, Junhao Liu, Min Yang

Some recently developed code large language models (Code LLMs) have been pre-trained on repository-level code data (Repo-Code LLMs), enabling these models to recognize repository structures and utilize cross-file information for code completion. However, in real-world development scenarios, simply concatenating the entire code repository often exceeds the context window limits of these Repo-Code LLMs, leading to significant performance degradation.
In this study, we conducted extensive preliminary experiments and analyses on six Repo-Code LLMs. The results indicate that maintaining the topological dependencies of files and increasing the code file content in the completion prompts can improve completion accuracy; pruning the specific implementations of functions in all dependent files does not significantly reduce the accuracy of completions. Based on these findings, we proposed a strategy named Hierarchical Context Pruning (HCP) to construct completion prompts with high informational code content. The HCP models the code repository at the function level, maintaining the topological dependencies between code files while removing a large amount of irrelevant code content, significantly reduces the input length for repository-level code completion. We applied the HCP strategy in experiments with six Repo-Code LLMs, and the results demonstrate that our proposed method can significantly enhance completion accuracy while substantially reducing the length of input. Our code and data are available at https://github.com/Hambaobao/HCP-Coder.

------------

`[2406.18297] FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning <https://arxiv.org/abs/2406.18297>`__ 

::

    Wed, 26 Jun 2024 12:31:31 GMT
    Yufeng Li, Rrubaa Panchendrarajan, Arkaitz Zubiaga

The rapid dissemination of information through social media and the Internet has posed a significant challenge for fact-checking, among others in identifying check-worthy claims that fact-checkers should pay attention to, i.e. filtering claims needing fact-checking from a large pool of sentences.
This challenge has stressed the need to focus on determining the priority of claims, specifically which claims are worth to be fact-checked. Despite advancements in this area in recent years, the application of large language models (LLMs), such as GPT, has only recently drawn attention in studies.
However, many open-source LLMs remain underexplored. Therefore, this study investigates the application of eight prominent open-source LLMs with fine-tuning and prompt engineering to identify check-worthy statements from political transcriptions. Further, we propose a two-step data pruning approach to automatically identify high-quality training data instances for effective learning. The efficiency of our approach is demonstrated through evaluations on the English language dataset as part of the check-worthiness estimation task of CheckThat! 2024. Further, the experiments conducted with data pruning demonstrate that competitive performance can be achieved with only about 44\% of the training data. Our team ranked first in the check-worthiness estimation task in the English language.

------------

`[2406.18305] S3: A Simple Strong Sample-effective Multimodal Dialog System <https://arxiv.org/abs/2406.18305>`__ S3:一个简单高效的多模态对话系统

::

    Wed, 26 Jun 2024 12:45:43 GMT
    Elisei Rykov, Egor Malkershin, and Alexander Panchenko

In this work, we present a conceptually simple yet powerful baseline for the multimodal dialog task, an S3 model, that achieves near state-of-the-art results on two compelling leaderboards: MMMU and AI Journey Contest 2023. The system is based on a pre-trained large language model, pre-trained modality encoders for image and audio, and a trainable modality projector. The proposed effective data mixture for training such an architecture demonstrates that a multimodal model based on a strong language model and trained on a small amount of multimodal data can perform efficiently in the task of multimodal dialog.

------------

`[2406.18312] AI-native Memory: A Pathway from LLMs Towards AGI <https://arxiv.org/abs/2406.18312>`__ ai原生内存:从llm到AGI的路径

::

    Wed, 26 Jun 2024 12:51:37 GMT
    Jingbo Shang, Zai Zheng, Xiang Ying, Felix Tao, Mindverse Team

Large language models (LLMs) have demonstrated the world with the sparks of artificial general intelligence (AGI). One opinion, especially from some startups working on LLMs, argues that an LLM with nearly unlimited context length can realize AGI. However, they might be too optimistic about the long-context capability of (existing) LLMs -- (1) Recent literature has shown that their effective context length is significantly smaller than their claimed context length; and (2) Our reasoning-in-a-haystack experiments further demonstrate that simultaneously finding the relevant information from a long context and conducting (simple) reasoning is nearly impossible. In this paper, we envision a pathway from LLMs to AGI through the integration of \emph{memory}. We believe that AGI should be a system where LLMs serve as core processors. In addition to raw data, the memory in this system would store a large number of important conclusions derived from reasoning processes.
Compared with retrieval-augmented generation (RAG) that merely processing raw data, this approach not only connects semantically related information closer, but also simplifies complex inferences at the time of querying. As an intermediate stage, the memory will likely be in the form of natural language descriptions, which can be directly consumed by users too. Ultimately, every agent/person should have its own large personal model, a deep neural network model (thus \emph{AI-native}) that parameterizes and compresses all types of memory, even the ones cannot be described by natural languages. Finally, we discuss the significant potential of AI-native memory as the transformative infrastructure for (proactive) engagement, personalization, distribution, and social in the AGI era, as well as the incurred privacy and security challenges with preliminary solutions.

------------

`[2406.18365] Themis: Towards Flexible and Interpretable NLG Evaluation <https://arxiv.org/abs/2406.18365>`__ Themis:迈向灵活可解释的NLG评估

::

    Wed, 26 Jun 2024 14:04:29 GMT
    Xinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, Xiaojun Wan

The evaluation of natural language generation (NLG) tasks is a significant and longstanding research issue. With the recent emergence of powerful large language models (LLMs), some studies have turned to LLM-based automatic evaluation methods, which demonstrate great potential to become a new evaluation paradigm following traditional string-based and model-based metrics.
However, despite the improved performance of existing methods, they still possess some deficiencies, such as dependency on references and limited evaluation flexibility. Therefore, in this paper, we meticulously construct a large-scale NLG evaluation corpus NLG-Eval with human and GPT-4 annotations to alleviate the lack of relevant data in this field. Furthermore, we propose Themis, an LLM dedicated to NLG evaluation, which has been trained with our designed multi-perspective consistency and rating-oriented preference alignment methods. Themis can conduct flexible and interpretable evaluations without references, and it exhibits superior evaluation performance on various NLG tasks, simultaneously generalizing well to unseen tasks and surpassing other evaluation models, including GPT-4.

------------

`[2406.18400] Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers <https://arxiv.org/abs/2406.18400>`__ LLMs会梦见大象吗(当被告知不要梦见时)?《变形金刚》中的潜在概念联想和联想记忆

::

    Wed, 26 Jun 2024 14:49:54 GMT
    Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam

Large Language Models (LLMs) have the capacity to store and recall facts.
Through experimentation with open-source models, we observe that this ability to retrieve facts can be easily manipulated by changing contexts, even without altering their factual meanings. These findings highlight that LLMs might behave like an associative memory model where certain tokens in the contexts serve as clues to retrieving facts. We mathematically explore this property by studying how transformers, the building blocks of LLMs, can complete such memory tasks. We study a simple latent concept association problem with a one-layer transformer and we show theoretically and empirically that the transformer gathers information using self-attention and uses the value matrix for associative memory.

------------

`[2406.18403] LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks <https://arxiv.org/abs/2406.18403>`__ LLMs而不是人类法官?20个NLP评估任务的大规模实证研究

::

    Wed, 26 Jun 2024 14:56:13 GMT
    Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fern\'andez, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, Andr\'e F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, Alberto Testoni

There is an increasing trend towards evaluating NLP models with LLM-generated judgments instead of human judgments. In the absence of a comparison against human data, this raises concerns about the validity of these evaluations; in case they are conducted with proprietary models, this also raises concerns over reproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with human annotations, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show that each LLM exhibits a large variance across datasets in its correlation to human judgments. We conclude that LLMs are not yet ready to systematically replace human judges in NLP.

------------

`[2406.18406] IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons <https://arxiv.org/abs/2406.18406>`__ 

::

    Wed, 26 Jun 2024 14:57:38 GMT
    Dan Shi, Renren Jin, Tianhao Shen, Weilong Dong, Xinwei Wu, Deyi Xiong

It is widely acknowledged that large language models (LLMs) encode a vast reservoir of knowledge after being trained on mass data. Recent studies disclose knowledge conflicts in LLM generation, wherein outdated or incorrect parametric knowledge (i.e., encoded knowledge) contradicts new knowledge provided in the context. To mitigate such knowledge conflicts, we propose a novel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to capitalize on neurons that are crucial in processing contextual cues.
Specifically, IRCAN first identifies neurons that significantly contribute to context processing, utilizing a context-aware attribution score derived from integrated gradients. Subsequently, the identified context-aware neurons are strengthened via reweighting. In doing so, we steer LLMs to generate context-sensitive outputs with respect to the new knowledge provided in the context. Extensive experiments conducted across a variety of models and tasks demonstrate that IRCAN not only achieves remarkable improvements in handling knowledge conflicts but also offers a scalable, plug-andplay solution that can be integrated seamlessly with existing models.

------------

`[2406.18449] Cascading Large Language Models for Salient Event Graph Generation <https://arxiv.org/abs/2406.18449>`__ 面向显著事件图生成的大规模语言模型级联

::

    Wed, 26 Jun 2024 15:53:54 GMT
    Xingwei Tan, Yuxiang Zhou, Gabriele Pergola, Yulan He

Generating event graphs from long documents is challenging due to the inherent complexity of multiple tasks involved such as detecting events, identifying their relationships, and reconciling unstructured input with structured graphs. Recent studies typically consider all events with equal importance, failing to distinguish salient events crucial for understanding narratives. This paper presents CALLMSAE, a CAscading Large Language Model framework for SAlient Event graph generation, which leverages the capabilities of LLMs and eliminates the need for costly human annotations. We first identify salient events by prompting LLMs to generate summaries, from which salient events are identified. Next, we develop an iterative code refinement prompting strategy to generate event relation graphs, removing hallucinated relations and recovering missing edges. Fine-tuning contextualised graph generation models on the LLM-generated graphs outperforms the models trained on CAEVO-generated data. Experimental results on a human-annotated test set show that the proposed method generates salient and more accurate graphs, outperforming competitive baselines.

------------

`[2406.18460] Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation <https://arxiv.org/abs/2406.18460>`__ 面向开放域人机对话的大型语言模型角色扮演零样本提示

::

    Wed, 26 Jun 2024 16:10:53 GMT
    Ahmed Njifenjou, Virgile Sucal, Bassam Jabaian and Fabrice Lef\`evre

Recently, various methods have been proposed to create open-domain conversational agents with Large Language Models (LLMs). These models are able to answer user queries, but in a one-way Q&A format rather than a true conversation. Fine-tuning on particular datasets is the usual way to modify their style to increase conversational ability, but this is expensive and usually only available in a few languages. In this study, we explore role-play zero-shot prompting as an efficient and cost-effective solution for open-domain conversation, using capable multilingual LLMs (Beeching et al., 2023) trained to obey instructions. We design a prompting system that, when combined with an instruction-following model - here Vicuna (Chiang et al., 2023) - produces conversational agents that match and even surpass fine-tuned models in human evaluation in French in two different tasks.

------------

`[2406.18510] WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models <https://arxiv.org/abs/2406.18510>`__ 大规模组队:从狂野的越狱到(相对)更安全的语言模型

::

    Wed, 26 Jun 2024 17:31:22 GMT
    Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, Nouha Dziri

We introduce WildTeaming, an automatic LLM safety red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes multiple tactics for systematic exploration of novel jailbreaks. Compared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with LLMs, our work investigates jailbreaks from chatbot users who were not specifically instructed to break the system. WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in up to 4.6x more diverse and successful adversarial attacks compared to state-of-the-art jailbreak methods.
While many datasets exist for jailbreak evaluation, very few open-source datasets exist for jailbreak training, as safety training data has been closed even when model weights are open. With WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs. To mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) harmful queries (vanilla & adversarial) and 2) benign queries that resemble harmful queries in form but contain no harm. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the scaling effects of data and the interplay of data properties and model capabilities during safety training. Through extensive experiments, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All components of WildJailbeak contribute to achieving balanced safety behaviors of models.

------------

`[2406.18512] "Is ChatGPT a Better Explainer than My Professor?": Evaluating the Explanation Capabilities of LLMs in Conversation Compared to a Human Baseline <https://arxiv.org/abs/2406.18512>`__ “ChatGPT比我的教授更能解释问题吗?”:与人类基线相比，评估llm在对话中的解释能力

::

    Wed, 26 Jun 2024 17:33:51 GMT
    Grace Li, Milad Alshomary, Smaranda Muresan

Explanations form the foundation of knowledge sharing and build upon communication principles, social dynamics, and learning theories. We focus specifically on conversational approaches for explanations because the context is highly adaptive and interactive. Our research leverages previous work on explanatory acts, a framework for understanding the different strategies that explainers and explainees employ in a conversation to both explain, understand, and engage with the other party. We use the 5-Levels dataset was constructed from the WIRED YouTube series by Wachsmuth et al., and later annotated by Booshehri et al. with explanatory acts. These annotations provide a framework for understanding how explainers and explainees structure their response when crafting a response.
With the rise of generative AI in the past year, we hope to better understand the capabilities of Large Language Models (LLMs) and how they can augment expert explainer's capabilities in conversational settings. To achieve this goal, the 5-Levels dataset (We use Booshehri et al.'s 2023 annotated dataset with explanatory acts.) allows us to audit the ability of LLMs in engaging in explanation dialogues. To evaluate the effectiveness of LLMs in generating explainer responses, we compared 3 different strategies, we asked human annotators to evaluate 3 different strategies: human explainer response, GPT4 standard response, GPT4 response with Explanation Moves.

------------

`[2406.18521] CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs <https://arxiv.org/abs/2406.18521>`__ CharXiv:绘制多模态llm中现实图表理解的差距

::

    Wed, 26 Jun 2024 17:50:11 GMT
    Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, Danqi Chen

Chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an over-optimistic measure of progress. We demonstrate that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions can deteriorate performance by up to 34.5%. In this work, we propose CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from arXiv papers. CharXiv includes two types of questions: 1) descriptive questions about examining basic chart elements and 2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. Our results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%.
All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. We hope CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress. Project page and leaderboard: https://charxiv.github.io/

------------

`[2406.18528] PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation <https://arxiv.org/abs/2406.18528>`__ 

::

    Wed, 26 Jun 2024 17:56:29 GMT
    Christoph Leiter, Steffen Eger

Large language models (LLMs) have revolutionized the field of NLP. Notably, their in-context learning capabilities also enable their use as evaluation metrics for natural language generation, making them particularly advantageous in low-resource scenarios and time-restricted applications. In this work, we introduce PrExMe, a large-scale prompt exploration for metrics, where we evaluate more than 720 prompt templates for open-source LLM-based metrics on machine translation (MT) and summarization datasets, totalling over 6.6M evaluations. This extensive comparison (1) serves as a benchmark of the performance of recent open-source LLMs as metrics and (2) explores the stability and variability of different prompting strategies. We discover that, on the one hand, there are scenarios for which prompts are stable. For instance, some LLMs show idiosyncratic preferences and favor to grade generated texts with textual labels while others prefer to return numeric scores. On the other hand, the stability of prompts and model rankings can be susceptible to seemingly innocuous changes. For example, changing the requested output format from "0 to 100" to "-1 to +1" can strongly affect the rankings in our evaluation. Our study contributes to understanding the impact of different prompting approaches on LLM-based metrics for MT and summarization evaluation, highlighting the most stable prompting patterns and potential limitations.

------------

`[2406.17812] Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars <https://arxiv.org/abs/2406.17812>`__ 面向科学的可扩展人工智能:视角、方法和范例

::

    Mon, 24 Jun 2024 20:29:29 GMT
    Wesley Brewer, Aditya Kashi, Sajal Dash, Aristeidis Tsaris, Junqi Yin, Mallikarjun Shankar, Feiyi Wang

In a post-ChatGPT world, this paper explores the potential of leveraging scalable artificial intelligence for scientific discovery. We propose that scaling up artificial intelligence on high-performance computing platforms is essential to address such complex problems. This perspective focuses on scientific use cases like cognitive simulations, large language models for scientific inquiry, medical image analysis, and physics-informed approaches.
The study outlines the methodologies needed to address such challenges at scale on supercomputers or the cloud and provides exemplars of such approaches applied to solve a variety of scientific problems.

------------

`[2406.17972] LABOR-LLM: Language-Based Occupational Representations with Large Language Models <https://arxiv.org/abs/2406.17972>`__ LABOR-LLM:基于大型语言模型的职业表示

::

    Tue, 25 Jun 2024 23:07:18 GMT
    Tianyu Du, Ayush Kanodia, Herman Brunborg, Keyon Vafa, Susan Athey

Many empirical studies of labor market questions rely on estimating relatively simple predictive models using small, carefully constructed longitudinal survey datasets based on hand-engineered features. Large Language Models (LLMs), trained on massive datasets, encode vast quantities of world knowledge and can be used for the next job prediction problem. However, while an off-the-shelf LLM produces plausible career trajectories when prompted, the probability with which an LLM predicts a particular job transition conditional on career history will not, in general, align with the true conditional probability in a given population. Recently, Vafa et al. (2024) introduced a transformer-based "foundation model", CAREER, trained using a large, unrepresentative resume dataset, that predicts transitions between jobs; it further demonstrated how transfer learning techniques can be used to leverage the foundation model to build better predictive models of both transitions and wages that reflect conditional transition probabilities found in nationally representative survey datasets. This paper considers an alternative where the fine-tuning of the CAREER foundation model is replaced by fine-tuning LLMs. For the task of next job prediction, we demonstrate that models trained with our approach outperform several alternatives in terms of predictive performance on the survey data, including traditional econometric models, CAREER, and LLMs with in-context learning, even though the LLM can in principle predict job titles that are not allowed in the survey data. Further, we show that our fine-tuned LLM-based models' predictions are more representative of the career trajectories of various workforce subpopulations than off-the-shelf LLM models and CAREER. We conduct experiments and analyses that highlight the sources of the gains in the performance of our models for representative predictions.

------------

`[2406.17781] Large Language Models estimate fine-grained human color-concept associations <https://arxiv.org/abs/2406.17781>`__ 大型语言模型估计细粒度人类颜色-概念关联

::

    Sat, 4 May 2024 04:19:15 GMT
    Kushin Mukherjee, Timothy T. Rogers, Karen B. Schloss

Concepts, both abstract and concrete, elicit a distribution of association strengths across perceptual color space, which influence aspects of visual cognition ranging from object recognition to interpretation of information visualizations. While prior work has hypothesized that color-concept associations may be learned from the cross-modal statistical structure of experience, it has been unclear whether natural environments possess such structure or, if so, whether learning systems are capable of discovering and exploiting it without strong prior constraints. We addressed these questions by investigating the ability of GPT-4, a multimodal large language model, to estimate human-like color-concept associations without any additional training.
Starting with human color-concept association ratings for 71 color set spanning perceptual color space (\texttt{UW-71}) and concepts that varied in abstractness, we assessed how well association ratings generated by GPT-4 could predict human ratings. GPT-4 ratings were correlated with human ratings, with performance comparable to state-of-the-art methods for automatically estimating color-concept associations from images. Variability in GPT-4's performance across concepts could be explained by specificity of the concept's color-concept association distribution. This study suggests that high-order covariances between language and perception, as expressed in the natural environment of the internet, contain sufficient information to support learning of human-like color-concept associations, and provides an existence proof that a learning system can encode such associations without initial constraints. The work further shows that GPT-4 can be used to efficiently estimate distributions of color associations for a broad range of concepts, potentially serving as a critical tool for designing effective and intuitive information visualizations.

------------

`[2406.17915] Semi-supervised classification of dental conditions in panoramic radiographs using large language model and instance segmentation: A real-world dataset evaluation <https://arxiv.org/abs/2406.17915>`__ 基于大型语言模型和实例分割的全景x光片牙齿状况半监督分类:真实世界数据集评估

::

    Tue, 25 Jun 2024 19:56:12 GMT
    Bernardo Silva, Jefferson Fontinele, Carolina Let\'icia Zilli Vieira, Jo\~ao Manuel R.S. Tavares, Patricia Ramos Cury, Luciano Oliveira

Dental panoramic radiographs offer vast diagnostic opportunities, but training supervised deep learning networks for automatic analysis of those radiology images is hampered by a shortage of labeled data. Here, a different perspective on this problem is introduced. A semi-supervised learning framework is proposed to classify thirteen dental conditions on panoramic radiographs, with a particular emphasis on teeth. Large language models were explored to annotate the most common dental conditions based on dental reports.
Additionally, a masked autoencoder was employed to pre-train the classification neural network, and a Vision Transformer was used to leverage the unlabeled data. The analyses were validated using two of the most extensive datasets in the literature, comprising 8,795 panoramic radiographs and 8,029 paired reports and images. Encouragingly, the results consistently met or surpassed the baseline metrics for the Matthews correlation coefficient. A comparison of the proposed solution with human practitioners, supported by statistical analysis, highlighted its effectiveness and performance limitations; based on the degree of agreement among specialists, the solution demonstrated an accuracy level comparable to that of a junior specialist.

------------

`[2406.17957] Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment <https://arxiv.org/abs/2406.17957>`__ 通过学习单调对齐提高llm语音合成的鲁棒性

::

    Tue, 25 Jun 2024 22:18:52 GMT
    Paarth Neekhara, Shehzeen Hussain, Subhankar Ghosh, Jason Li, Rafael Valle, Rohan Badlani, Boris Ginsburg

Large Language Model (LLM) based text-to-speech (TTS) systems have demonstrated remarkable capabilities in handling large speech datasets and generating natural speech for new speakers. However, LLM-based TTS models are not robust as the generated output can contain repeating words, missing words and mis-aligned speech (referred to as hallucinations or attention errors), especially when the text contains multiple occurrences of the same token. We examine these challenges in an encoder-decoder transformer model and find that certain cross-attention heads in such models implicitly learn the text and speech alignment when trained for predicting speech tokens for a given text. To make the alignment more robust, we propose techniques utilizing CTC loss and attention priors that encourage monotonic cross-attention over the text tokens.
Our guided attention training technique does not introduce any new learnable parameters and significantly improves robustness of LLM-based TTS models.

------------

`[2406.18069] Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals <https://arxiv.org/abs/2406.18069>`__ 

::

    Wed, 26 Jun 2024 04:54:45 GMT
    Zengding Liu, Chen Chen, Jiannong Cao, Minglei Pan, Jikui Liu, Nan Li, Fen Miao, and Ye Li

Large language models (LLMs) have captured significant interest from both academia and industry due to their impressive performance across various textual tasks. However, the potential of LLMs to analyze physiological time-series data remains an emerging research field. Particularly, there is a notable gap in the utilization of LLMs for analyzing wearable biosignals to achieve cuffless blood pressure (BP) measurement, which is critical for the management of cardiovascular diseases. This paper presents the first work to explore the capacity of LLMs to perform cuffless BP estimation based on wearable biosignals. We extracted physiological features from electrocardiogram (ECG) and photoplethysmogram (PPG) signals and designed context-enhanced prompts by combining these features with BP domain knowledge and user information. Subsequently, we adapted LLMs to BP estimation tasks through instruction tuning. To evaluate the proposed approach, we conducted assessments of ten advanced LLMs using a comprehensive public dataset of wearable biosignals from 1,272 participants. The experimental results demonstrate that the optimally fine-tuned LLM significantly surpasses conventional task-specific baselines, achieving an estimation error of 0.00 $\pm$ 9.25 mmHg for systolic BP and 1.29 $\pm$ 6.37 mmHg for diastolic BP. Notably, the ablation studies highlight the benefits of our context enhancement strategy, leading to an 8.9% reduction in mean absolute error for systolic BP estimation. This paper pioneers the exploration of LLMs for cuffless BP measurement, providing a potential solution to enhance the accuracy of cuffless BP measurement.

------------

`[2406.18087] EHR-Based Mobile and Web Platform for Chronic Disease Risk Prediction Using Large Language Multimodal Models <https://arxiv.org/abs/2406.18087>`__ 使用大型语言多模态模型的基于ehr的移动和Web平台慢性病风险预测

::

    Wed, 26 Jun 2024 05:51:08 GMT
    Chun-Chieh Liao, Wei-Ting Kuo, I-Hsuan Hu, Yen-Chen Shih, Jun-En Ding, Feng Liu, Fang-Ming Hung

Traditional diagnosis of chronic diseases involves in-person consultations with physicians to identify the disease. However, there is a lack of research focused on predicting and developing application systems using clinical notes and blood test values. We collected five years of Electronic Health Records (EHRs) from Taiwan's hospital database between 2017 and 2021 as an AI database.
Furthermore, we developed an EHR-based chronic disease prediction platform utilizing Large Language Multimodal Models (LLMMs), successfully integrating with frontend web and mobile applications for prediction. This prediction platform can also connect to the hospital's backend database, providing physicians with real-time risk assessment diagnostics. The demonstration link can be found at https://www.youtube.com/watch?v=oqmL9DEDFgA.

------------

`[2406.18115] Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps <https://arxiv.org/abs/2406.18115>`__ 基于3D语义地图的未见动态环境中的开放词汇移动操作

::

    Wed, 26 Jun 2024 07:06:42 GMT
    Dicong Qiu, Wenzong Ma, Zhenfu Pan, Hui Xiong, Junwei Liang

Open-Vocabulary Mobile Manipulation (OVMM) is a crucial capability for autonomous robots, especially when faced with the challenges posed by unknown and dynamic environments. This task requires robots to explore and build a semantic understanding of their surroundings, generate feasible plans to achieve manipulation goals, adapt to environmental changes, and comprehend natural language instructions from humans. To address these challenges, we propose a novel framework that leverages the zero-shot detection and grounded recognition capabilities of pretraining visual-language models (VLMs) combined with dense 3D entity reconstruction to build 3D semantic maps. Additionally, we utilize large language models (LLMs) for spatial region abstraction and online planning, incorporating human instructions and spatial semantic context. We have built a 10-DoF mobile manipulation robotic platform JSR-1 and demonstrated in real-world robot experiments that our proposed framework can effectively capture spatial semantics and process natural language user instructions for zero-shot OVMM tasks under dynamic environment settings, with an overall navigation and task success rate of 80.95% and 73.33% over 105 episodes, and better SFT and SPL by 157.18% and 19.53% respectively compared to the baseline.
Furthermore, the framework is capable of replanning towards the next most probable candidate location based on the spatial semantic context derived from the 3D semantic map when initial plans fail, keeping an average success rate of 76.67%.

------------

`[2406.18193] MammothModa: Multi-Modal Large Language Model <https://arxiv.org/abs/2406.18193>`__ MammothModa:多模态大型语言模型

::

    Wed, 26 Jun 2024 09:17:27 GMT
    Qi She and Junwen Pan and Xin Wan and Rui Zhang and Dawei Lu and Kai Huang

In this report, we introduce MammothModa, yet another multi-modal large language model (MLLM) designed to achieve state-of-the-art performance starting from an elementary baseline. We focus on three key design insights: (i) Integrating Visual Capabilities while Maintaining Complex Language Understanding: In addition to the vision encoder, we incorporated the Visual Attention Experts into the LLM to enhance its visual capabilities. (ii) Extending Context Window for High-Resolution and Long-Duration Visual Feature: We explore the Visual Merger Module to effectively reduce the token number of high-resolution images and incorporated frame position ids to avoid position interpolation. (iii) High-Quality Bilingual Datasets: We meticulously curated and filtered a high-quality bilingual multimodal dataset to reduce visual hallucinations. With above recipe we build MammothModa that consistently outperforms the state-of-the-art models, e.g., LLaVA-series, across main real-world visual language benchmarks without bells and whistles.

------------

`[2406.18379] MALSIGHT: Exploring Malicious Source Code and Benign Pseudocode for Iterative Binary Malware Summarization <https://arxiv.org/abs/2406.18379>`__ MALSIGHT:基于迭代式二进制恶意代码摘要的恶意源代码和良性伪代码探索

::

    Wed, 26 Jun 2024 14:21:09 GMT
    Haolang Lu, Hongrui Peng, Guoshun Nan, Jiaoyang Cui, Cheng Wang, Weifei Jin

Binary malware summarization aims to automatically generate human-readable descriptions of malware behaviors from executable files, facilitating tasks like malware cracking and detection. Previous methods based on Large Language Models (LLMs) have shown great promise. However, they still face significant issues, including poor usability, inaccurate explanations, and incomplete summaries, primarily due to the obscure pseudocode structure and the lack of malware training summaries. Further, calling relationships between functions, which involve the rich interactions within a binary malware, remain largely underexplored. To this end, we propose MALSIGHT, a novel code summarization framework that can iteratively generate descriptions of binary malware by exploring malicious source code and benign pseudocode. Specifically, we construct the first malware summaries, MalS and MalP, using an LLM and manually refine this dataset with human effort. At the training stage, we tune our proposed MalT5, a novel LLM-based code model, on the MalS dataset and a benign pseudocode dataset. Then, at the test stage, we iteratively feed the pseudocode functions into MalT5 to obtain the summary. Such a procedure facilitates the understanding of pseudocode structure and captures the intricate interactions between functions, thereby benefiting the usability, accuracy, and completeness of summaries. Additionally, we propose a novel evaluation benchmark, BLEURT-sum, to measure the quality of summaries. Experiments on three datasets show the effectiveness of the proposed MALSIGHT. Notably, our proposed MalT5, with only 0.77B parameters, delivers comparable performance to much larger ChatGPT3.5.

------------

`[2406.18118] SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance <https://arxiv.org/abs/2406.18118>`__ SafeAligner:基于响应视差引导的针对越狱攻击的安全对齐

::

    Wed, 26 Jun 2024 07:15:44 GMT
    Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang

As the development of large language models (LLMs) rapidly advances, securing these models effectively without compromising their utility has become a pivotal area of research. However, current defense strategies against jailbreak attacks (i.e., efforts to bypass security protocols) often suffer from limited adaptability, restricted general capability, and high cost. To address these challenges, we introduce SafeAligner, a methodology implemented at the decoding stage to fortify defenses against jailbreak attacks. We begin by developing two specialized models: the Sentinel Model, which is trained to foster safety, and the Intruder Model, designed to generate riskier responses. SafeAligner leverages the disparity in security levels between the responses from these models to differentiate between harmful and beneficial tokens, effectively guiding the safety alignment by altering the output token distribution of the target model. Extensive experiments show that SafeAligner can increase the likelihood of beneficial tokens, while reducing the occurrence of harmful ones, thereby ensuring secure alignment with minimal loss to generality.

------------

`[2406.18382] Adversarial Search Engine Optimization for Large Language Models <https://arxiv.org/abs/2406.18382>`__ 面向大型语言模型的对抗性搜索引擎优化

::

    Wed, 26 Jun 2024 14:24:51 GMT
    Fredrik Nestaas, Edoardo Debenedetti, Florian Tram\`er

Large Language Models (LLMs) are increasingly used in applications where the model selects from competing third-party content, such as in LLM-powered search engines or chatbot plugins. In this paper, we introduce Preference Manipulation Attacks, a new class of attacks that manipulate an LLM's selections to favor the attacker. We demonstrate that carefully crafted website content or plugin documentations can trick an LLM to promote the attacker products and discredit competitors, thereby increasing user traffic and monetization. We show this leads to a prisoner's dilemma, where all parties are incentivized to launch attacks, but the collective effect degrades the LLM's outputs for everyone. We demonstrate our attacks on production LLM search engines (Bing and Perplexity) and plugin APIs (for GPT-4 and Claude). As LLMs are increasingly used to rank third-party content, we expect Preference Manipulation Attacks to emerge as a significant threat.

------------

`[2312.02706] Large Knowledge Model: Perspectives and Challenges <https://arxiv.org/abs/2312.02706>`__ 

::

    replaced with revised version Wed, 26 Jun 2024 16:11:55 GMT
    Submission history From: Huajun Chen [view email]
    [v1] Tue, 5 Dec 2023 12:07:30 UTC (153 KB)
    [v2] Wed, 26 Jun 2024 16:11:55 UTC (7,299 KB)
    Huajun Chen

Humankind's understanding of the world is fundamentally linked to our perception and cognition, with \emph{human languages} serving as one of the major carriers of \emph{world knowledge}. In this vein, \emph{Large Language Models} (LLMs) like ChatGPT epitomize the pre-training of extensive, sequence-based world knowledge into neural networks, facilitating the processing and manipulation of this knowledge in a parametric space. This article explores large models through the lens of "knowledge". We initially investigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in enhancing LLMs, covering aspects like knowledge-augmented language model, structure-inducing pre-training, knowledgeable prompts, structured CoT, knowledge editing, semantic tools for LLM and knowledgeable AI agents. Subsequently, we examine how LLMs can boost traditional symbolic knowledge bases, encompassing aspects like using LLM as KG builder and controller, structured knowledge pretraining, and LLM-enhanced symbolic reasoning. Considering the intricate nature of human knowledge, we advocate for the creation of \emph{Large Knowledge Models} (LKM), specifically engineered to manage diversified spectrum of knowledge structures. This promising undertaking would entail several key challenges, such as disentangling knowledge base from language models, cognitive alignment with human knowledge, integration of perception and cognition, and building large commonsense models for interacting with physical world, among others. We finally propose a five-"A" principle to distinguish the concept of LKM.

------------

`[2402.17709] Case-Based or Rule-Based: How Do Transformers Do the Math? <https://arxiv.org/abs/2402.17709>`__ 基于案例还是基于规则:transformer如何计算?

::

    replaced with revised version Wed, 26 Jun 2024 09:25:07 GMT
    Submission history From: Yi Hu [view email]
    [v1] Tue, 27 Feb 2024 17:41:58 UTC (5,760 KB)
    [v2] Wed, 26 Jun 2024 09:25:07 UTC (22,617 KB)
    Yi Hu, Xiaojuan Tang, Haotong Yang, Muhan Zhang

Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar cases seen in the training corpus for help. We define these two different reasoning mechanisms as "rule-based reasoning" and "case-based reasoning". Since rule-based reasoning is essential for acquiring systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to use rules explicitly helps them learn rule-based reasoning and generalize better in length.

------------

`[2311.14096] Cultural Bias and Cultural Alignment of Large Language Models <https://arxiv.org/abs/2311.14096>`__ 大型语言模型的文化偏见和文化对齐

::

    replaced with revised version Wed, 26 Jun 2024 15:26:44 GMT
    Submission history From: Yan Tao [view email]
    [v1] Thu, 23 Nov 2023 16:45:56 UTC (840 KB)
    [v2] Wed, 26 Jun 2024 15:26:44 UTC (492 KB)
    Yan Tao, Olga Viberg, Ryan S. Baker, Rene F. Kizilcec

Culture fundamentally shapes people's reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people's authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models' responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For recent models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models' output for 71-81% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI.

------------

`[2311.18702] CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation <https://arxiv.org/abs/2311.18702>`__ CritiqueLLM:面向大型语言模型生成评估的信息批判生成模型

::

    replaced with revised version Wed, 26 Jun 2024 07:44:11 GMT
    Submission history From: Pei Ke [view email]
    [v1] Thu, 30 Nov 2023 16:52:42 UTC (236 KB)
    [v2] Wed, 26 Jun 2024 07:44:11 UTC (340 KB)
    Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang

Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.

------------

`[2312.09542] Marathon: A Race Through the Realm of Long Context with Large Language Models <https://arxiv.org/abs/2312.09542>`__ 马拉松:用大型语言模型在长上下文领域中的比赛

::

    replaced with revised version Wed, 26 Jun 2024 04:48:11 GMT
    Submission history From: Lei Zhang [view email]
    [v1] Fri, 15 Dec 2023 05:30:14 UTC (309 KB)
    [v2] Wed, 26 Jun 2024 04:48:11 UTC (548 KB)
    Lei Zhang, Yunshui Li, Ziqiang Liu, Jiaxi yang, Junhao Liu, Longze Chen, Run Luo and Min Yang

With the advancement of large language models (LLMs) and the expansion of their context windows, existing long-context benchmarks fall short in effectively evaluating the models' comprehension and reasoning abilities in extended texts. Moreover, conventional benchmarks relying on F1 metrics often inaccurately score responses: they may undervalue correct answers that differ from the reference responses and overvalue incorrect ones that resemble the reference texts. In response to these limitations, we introduce Marathon, a novel evaluation benchmark that adopts a multiple-choice question format. It is specifically designed to overcome the constraints of previous benchmarks and provide a rapid, precise, and unbiased appraisal of the long-context comprehension skills of large language models. We conducted comprehensive evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and assessed the effectiveness of various optimization strategies tailored for long-context generation. We anticipate that the Marathon benchmark and its associated leaderboard will enable a more precise and equitable evaluation of LLMs' capabilities in understanding and reasoning over extended contexts. Marathon is available at this https URL.

------------

`[2402.04678] FaithLM: Towards Faithful Explanations for Large Language Models <https://arxiv.org/abs/2402.04678>`__ FaithLM:大型语言模型的忠实解释

::

    replaced with revised version Wed, 26 Jun 2024 07:43:11 GMT
    Submission history From: Yu-Neng Chuang [view email]
    [v1] Wed, 7 Feb 2024 09:09:14 UTC (491 KB)
    [v2] Sun, 23 Jun 2024 01:13:25 UTC (1,427 KB)
    [v3] Wed, 26 Jun 2024 07:43:11 UTC (1,427 KB)
    Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Ruixiang Tang, Shaochen Zhong, Fan Yang, Mengnan Du, Xuanting Cai, and Xia Hu

Large Language Models (LLMs) have become proficient in addressing complex tasks by leveraging their extensive internal knowledge and reasoning capabilities. However, the black-box nature of these models complicates the task of explaining their decision-making processes. While recent advancements demonstrate the potential of leveraging LLMs to self-explain their predictions through natural language (NL) explanations, their explanations may not accurately reflect the LLMs' decision-making process due to a lack of fidelity optimization on the derived explanations. Measuring the fidelity of NL explanations is a challenging issue, as it is difficult to manipulate the input context to mask the semantics of these explanations. To this end, we introduce FaithLM to explain the decision of LLMs with NL explanations. Specifically, FaithLM designs a method for evaluating the fidelity of NL explanations by incorporating the contrary explanations to the query process. Moreover, FaithLM conducts an iterative process to improve the fidelity of derived explanations. Experiment results on three datasets from multiple domains demonstrate that FaithLM can significantly improve the fidelity of derived explanations, which also provides a better alignment with the ground-truth explanations.

------------

`[2402.10663] Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL <https://arxiv.org/abs/2402.10663>`__ 通过人工无关的文本到sql融合提高演示多样性

::

    replaced with revised version Wed, 26 Jun 2024 06:54:35 GMT
    Submission history From: Dingzirui Wang [view email]
    [v1] Fri, 16 Feb 2024 13:13:18 UTC (310 KB)
    [v2] Wed, 19 Jun 2024 08:02:23 UTC (410 KB)
    [v3] Wed, 26 Jun 2024 06:54:35 UTC (406 KB)
    Dingzirui Wang, Longxu Dou, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che

Currently, the in-context learning method based on large language models (LLMs) has become the mainstream of text-to-SQL research. Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool. However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead. Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL. We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments. Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost. Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on several mainstream datasets, which proves the effectiveness of Fused.

------------

`[2402.13561] Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment <https://arxiv.org/abs/2402.13561>`__ 认知视觉语言映射器:增强视觉知识对齐促进多模态理解

::

    replaced with revised version Wed, 26 Jun 2024 07:05:21 GMT
    Submission history From: Yunxin Li [view email]
    [v1] Wed, 21 Feb 2024 06:34:46 UTC (8,218 KB)
    [v2] Wed, 26 Jun 2024 07:05:21 UTC (8,220 KB)
    Yunxin Li, Xinyu Chen, Baotian Hu, Haoyuan Shi, Min Zhang

Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively. The codes are available at this https URL

------------

`[2405.11407] Can Public LLMs be used for Self-Diagnosis of Medical Conditions ? <https://arxiv.org/abs/2405.11407>`__ 公共llm可以用于医疗状况的自我诊断吗?

::

    replaced with revised version Wed, 26 Jun 2024 01:12:11 GMT
    Nikil Sharan Prabahar Balasubramanian, Sagnik Dakshit

Categories

------------

`[2405.14159] Super Tiny Language Models <https://arxiv.org/abs/2405.14159>`__ 超微型语言模型

::

    replaced with revised version Wed, 26 Jun 2024 08:41:06 GMT
    Submission history From: Dylan Hillier [view email]
    [v1] Thu, 23 May 2024 04:12:49 UTC (553 KB)
    [v2] Wed, 26 Jun 2024 08:41:06 UTC (1,191 KB)
    Dylan Hillier, Leon Guertler, Cheston Tan, Palaash Agrawal, Chen Ruirui, Bobby Cheng

The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. This paper introduces a series of research efforts focused on Super Tiny Language Models (STLMs), which aim to deliver high performance with significantly reduced parameter counts. We explore innovative techniques such as byte-level tokenization with a pooling mechanism, weight tying, and efficient training strategies. These methods aim to significantly reduce reduce the parameter count compared to traditional models -- in future works, we aim to build on these in a way that maintains and improves upon the performance of base transformer models. This series of papers will explore into various subproblems, including tokenizer-free models, self-play based training, and alternative training objectives. We will target models with 10M, 50M, and 100M parameters. Our ultimate goal is to make high-performance language models more accessible and practical for a wide range of applications.

------------

`[2406.01179] Are AI-Generated Text Detectors Robust to Adversarial Perturbations? <https://arxiv.org/abs/2406.01179>`__ 人工智能生成的文本检测器对对抗性扰动鲁棒吗?

::

    replaced with revised version Wed, 26 Jun 2024 12:43:56 GMT
    Submission history From: Guanhua Huang [view email]
    [v1] Mon, 3 Jun 2024 10:21:48 UTC (317 KB)
    [v2] Wed, 26 Jun 2024 12:43:56 UTC (317 KB)
    Guanhua Huang, Yuchen Zhang, Zhe Li, Yongjian You, Mingze Wang, Zhouwang Yang

The widespread use of large language models (LLMs) has sparked concerns about the potential misuse of AI-generated text, as these models can produce content that closely resembles human-generated text. Current detectors for AI-generated text (AIGT) lack robustness against adversarial perturbations, with even minor changes in characters or words causing a reversal in distinguishing between human-created and AI-generated text. This paper investigates the robustness of existing AIGT detection methods and introduces a novel detector, the Siamese Calibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction network to add and remove noise from text, extracting a semantic representation that is robust to local perturbations. We also propose a siamese calibration technique to train the model to make equally confidence predictions under different noise, which improves the model's robustness against adversarial perturbations. Experiments on four publicly available datasets show that the SCRN outperforms all baseline methods, achieving 6.5\%-18.25\% absolute accuracy improvement over the best baseline method under adversarial attacks. Moreover, it exhibits superior generalizability in cross-domain, cross-genre, and mixed-source scenarios. The code is available at \url{this https URL}.

------------

`[2406.04175] Confabulation: The Surprising Value of Large Language Model Hallucinations <https://arxiv.org/abs/2406.04175>`__ 虚构:大型语言模型幻觉的惊人价值

::

    replaced with revised version Tue, 25 Jun 2024 18:37:19 GMT
    Submission history From: Eamon Duede [view email]
    [v1] Thu, 6 Jun 2024 15:32:29 UTC (65 KB)
    [v2] Tue, 25 Jun 2024 18:37:19 UTC (65 KB)
    Peiqi Sui, Eamon Duede, Sophie Wu, Richard Jean So

This paper presents a systematic defense of large language model (LLM) hallucinations or 'confabulations' as a potential resource instead of a categorically negative pitfall. The standard view is that confabulations are inherently problematic and AI research should eliminate this flaw. In this paper, we argue and empirically demonstrate that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication. In other words, it has potential value. Specifically, we analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs. This finding reveals a tension in our usually dismissive understandings of confabulation. It suggests, counter-intuitively, that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation.

------------

`[2406.04625] Key-Element-Informed sLLM Tuning for Document Summarization <https://arxiv.org/abs/2406.04625>`__ 基于关键元素的sLLM文档摘要调优

::

    replaced with revised version Wed, 26 Jun 2024 02:22:11 GMT
    Submission history From: Sangwon Ryu [view email]
    [v1] Fri, 7 Jun 2024 04:19:01 UTC (157 KB)
    [v2] Wed, 26 Jun 2024 02:22:11 UTC (158 KB)
    Sangwon Ryu, Heejin Do, Yunsu Kim, Gary Geunbae Lee, Jungseul Ok

Remarkable advances in large language models (LLMs) have enabled high-quality text summarization. However, this capability is currently accessible only through LLMs of substantial size or proprietary LLMs with usage fees. In response, smaller-scale LLMs (sLLMs) of easy accessibility and low costs have been extensively studied, yet they often suffer from missing key information and entities, i.e., low relevance, in particular, when input documents are long. We hence propose a key-element-informed instruction tuning for summarization, so-called KEITSum, which identifies key elements in documents and instructs sLLM to generate summaries capturing these key elements. Experimental results on dialogue and news datasets demonstrate that sLLM with KEITSum indeed provides high-quality summarization with higher relevance and less hallucinations, competitive to proprietary LLM.

------------

`[2406.10190] CHIRON: Rich Character Representations in Long-Form Narratives <https://arxiv.org/abs/2406.10190>`__ CHIRON:在长篇叙事中丰富的人物表现

::

    replaced with revised version Wed, 26 Jun 2024 14:22:18 GMT
    Submission history From: Alexander Gurung [view email]
    [v1] Fri, 14 Jun 2024 17:23:57 UTC (8,109 KB)
    [v2] Wed, 26 Jun 2024 14:22:18 UTC (7,963 KB)
    Alexander Gurung and Mirella Lapata

Characters are integral to long-form narratives, but are poorly understood by existing story analysis and generation systems. While prior work has simplified characters via graph-based methods and brief character descriptions, we aim to better tackle the problem of representing complex characters by taking inspiration from advice given to professional writers. We propose CHIRON, a new `character sheet' based representation that organizes and filters textual information about characters. We construct CHIRON sheets in two steps: a Generation Module that prompts an LLM for character information via question-answering and a Validation Module that uses automated reasoning and a domain-specific entailment model to eliminate false facts about a character. We validate CHIRON via the downstream task of masked-character prediction, where our experiments show CHIRON is better and more flexible than comparable summary-based baselines. We also show that metrics derived from CHIRON can be used to automatically infer character-centricity in stories, and that these metrics align with human judgments.

------------

`[2406.10552] Large Language Model Enhanced Clustering for News Event Detection <https://arxiv.org/abs/2406.10552>`__ 面向新闻事件检测的大型语言模型增强聚类

::

    replaced with revised version Wed, 26 Jun 2024 17:42:59 GMT
    Submission history From: Adane Tarekegn [view email]
    [v1] Sat, 15 Jun 2024 08:13:47 UTC (846 KB)
    [v2] Wed, 26 Jun 2024 17:42:59 UTC (863 KB)
    [v3] Fri, 28 Jun 2024 09:16:28 UTC (859 KB)
    Adane Nega Tarekegn

The news landscape is continuously evolving, with an ever-increasing volume of information from around the world. Automated event detection within this vast data repository is essential for monitoring, identifying, and categorizing significant news occurrences across diverse platforms. This paper presents an event detection framework that leverages Large Language Models (LLMs) combined with clustering analysis to detect news events from the Global Database of Events, Language, and Tone (GDELT). The framework enhances event clustering through both pre-event detection tasks (keyword extraction and text embedding) and post-event detection tasks (event summarization and topic labelling). We also evaluate the impact of various textual embeddings on the quality of clustering outcomes, ensuring robust news categorization. Additionally, we introduce a novel Cluster Stability Assessment Index (CSAI) to assess the validity and robustness of clustering results. CSAI utilizes multiple feature vectors to provide a new way of measuring clustering quality. Our experiments indicate that the use of LLM embedding in the event detection framework has significantly improved the results, demonstrating greater robustness in terms of CSAI scores. Moreover, post-event detection tasks generate meaningful insights, facilitating effective interpretation of event clustering results. Overall, our experimental results indicate that the proposed framework offers valuable insights and could enhance the accuracy in news analysis and reporting.

------------

`[2406.10794] Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis <https://arxiv.org/abs/2406.10794>`__ LLMs中的越狱攻击理解:表示空间分析

::

    replaced with revised version Wed, 26 Jun 2024 13:50:32 GMT
    Submission history From: Yuping Lin [view email]
    [v1] Sun, 16 Jun 2024 03:38:48 UTC (8,658 KB)
    [v2] Wed, 26 Jun 2024 13:50:32 UTC (8,658 KB)
    Yuping Lin, Pengfei He, Han Xu, Yue Xing, Makoto Yamada, Hui Liu, Jiliang Tang

Large language models (LLMs) are susceptible to a type of attack known as jailbreaking, which misleads LLMs to output harmful contents. Although there are diverse jailbreak attack strategies, there is no unified understanding on why some methods succeed and others fail. This paper explores the behavior of harmful and harmless prompts in the LLM's representation space to investigate the intrinsic properties of successful jailbreak attacks. We hypothesize that successful attacks share some similar properties: They are effective in moving the representation of the harmful prompt towards the direction to the harmless prompts. We leverage hidden representations into the objective of existing jailbreak attacks to move the attacks along the acceptance direction, and conduct experiments to validate the above hypothesis using the proposed objective. We hope this study provides new insights into understanding how LLMs understand harmfulness information.

------------

`[2406.11162] How Good are LLMs at Relation Extraction under Low-Resource Scenario? Comprehensive Evaluation <https://arxiv.org/abs/2406.11162>`__ llm在低资源场景下的关系提取效果如何?综合评价

::

    replaced with revised version Wed, 26 Jun 2024 01:43:15 GMT
    Submission history From: Yuanhang Zheng [view email]
    [v1] Mon, 17 Jun 2024 03:02:04 UTC (282 KB)
    [v2] Wed, 26 Jun 2024 01:43:15 UTC (283 KB)
    Dawulie Jinensibieke, Mieradilijiang Maimaiti, Wentao Xiao, Yuanhang Zheng, Xiaobo Wang

Relation Extraction (RE) serves as a crucial technology for transforming unstructured text into structured information, especially within the framework of Knowledge Graph development. Its importance is emphasized by its essential role in various downstream tasks. Besides the conventional RE methods which are based on neural networks and pre-trained language models, large language models (LLMs) are also utilized in the research field of RE. However, on low-resource languages (LRLs), both conventional RE methods and LLM-based methods perform poorly on RE due to the data scarcity issues. To this end, this paper constructs low-resource relation extraction datasets in 10 LRLs in three regions (Central Asia, Southeast Asia and Middle East). The corpora are constructed by translating the original publicly available English RE datasets (NYT10, FewRel and CrossRE) using an effective multilingual machine translation. Then, we use the language perplexity (PPL) to filter out the low-quality data from the translated datasets. Finally, we conduct an empirical study and validate the performance of several open-source LLMs on these generated LRL RE datasets.

------------

`[2406.11709] Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging <https://arxiv.org/abs/2406.11709>`__ 指示，而不是辅助:基于llm的多轮规划和分层提问，用于苏格拉底式代码调试

::

    replaced with revised version Tue, 25 Jun 2024 21:17:41 GMT
    Submission history From: Priyanka Kargupta [view email]
    [v1] Mon, 17 Jun 2024 16:28:21 UTC (773 KB)
    [v2] Tue, 25 Jun 2024 21:17:41 UTC (774 KB)
    Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani-Tur, Jiawei Han

Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student's conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes -- all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct's state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct's ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning.

------------

`[2406.16253] LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing <https://arxiv.org/abs/2406.16253>`__ LLMs协助NLP研究人员:评论论文(元)评审

::

    replaced with revised version Tue, 25 Jun 2024 18:04:38 GMT
    Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Ranran Zhang, Vipul Gupta, Yinghui Li, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Jiayang Cheng, Zhaowei Wang, Ying Su, Raj Sanjay Shah, Ruohao Guo, Jing Gu, Haoran Li, Kangda Wei, Zihao Wang, Lu Cheng, Surangika Ranathunga, Meng Fang, Jie Fu, Fei Liu, Ruihong Huang, Eduardo Blanco, Yixin Cao, Rui Zhang, Philip S. Yu, Wenpeng Yin

Categories

------------

`[2406.16801] RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale <https://arxiv.org/abs/2406.16801>`__ RES-Q:评估存储库规模的大型语言模型系统的代码编辑

::

    replaced with revised version Tue, 25 Jun 2024 18:04:23 GMT
    Submission history From: Beck LaBash [view email]
    [v1] Mon, 24 Jun 2024 17:08:17 UTC (330 KB)
    [v2] Tue, 25 Jun 2024 18:04:23 UTC (330 KB)
    Beck LaBash, August Rosedale, Alex Reents, Lucas Negritto, Colin Wiel

The instruction-following ability of Large Language Models (LLMs) has cultivated a class of LLM-based systems capable of approaching complex tasks such as making edits to large code repositories. Due to the high sensitivity and unpredictability of LLM behavior in response to changes in prompting, robust evaluation tools are needed to drive future iteration of these systems. We propose RES-Q, a natural language instruction-based benchmark for evaluating $\textbf{R}$epository $\textbf{E}$diting $\textbf{S}$ystems, which consists of 100 handcrafted repository editing tasks derived from real GitHub commits. Given an edit instruction and a code repository, RES-Q evaluates an LLM system's ability to interpret the instruction, navigate the repository to gather relevant information, and construct an appropriate edit that satisfies the specified criteria. We argue that evaluating LLMs in this way addresses issues with traditional benchmarks and provides a more holistic assessment of a model's abilities. We evaluate various state-of-the-art LLMs as language agents in a repository-editing system built on Qurrent OS, our language agent development software. Despite their 1% pass@1 performance difference on HumanEval, we find Claude Sonnet 3.5 outperforms GPT-4o by 12% pass@1 on RES-Q, indicating RES-Q's capacity to differentiate model capability as traditional benchmarks approach saturation. We further investigate token efficiency, performance relationships with existing benchmarks, and interesting disparities between closed and open-source LLMs. Code and dataset are available at this https URL.

------------

`[2406.17588] LongIns: A Challenging Long-context Instruction-based Exam for LLMs <https://arxiv.org/abs/2406.17588>`__ LongIns:一种具有挑战性的基于长上下文指令的LLMs考试

::

    replaced with revised version Wed, 26 Jun 2024 13:28:04 GMT
    Submission history From: Shawn Gavin [view email]
    [v1] Tue, 25 Jun 2024 14:31:26 UTC (1,553 KB)
    [v2] Wed, 26 Jun 2024 13:28:04 UTC (1,553 KB)
    Shawn Gavin, Tuney Zheng, Jiaheng Liu, Quehry Que, Noah Wang, Jian Yang, Chenchen Zhang, Wenhao Huang, Wenhu Chen, Ge Zhang

The long-context capabilities of large language models (LLMs) have been a hot topic in recent years. To evaluate the performance of LLMs in different scenarios, various assessment benchmarks have emerged. However, as most of these benchmarks focus on identifying key information to answer questions, which mainly requires the retrieval ability of LLMs, these benchmarks can partially represent the reasoning performance of LLMs from large amounts of information. Meanwhile, although LLMs often claim to have context windows of 32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual supported length of these LLMs. To address these issues, we propose the LongIns benchmark dataset, a challenging long-context instruction-based exam for LLMs, which is built based on the existing instruction datasets. Specifically, in our LongIns, we introduce three evaluation settings: Global Instruction & Single Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction & Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations on existing LLMs and have the following important findings: (1). The top-performing GPT-4 with 128k context length performs poorly on the evaluation context window of 16k in our LongIns. (2). For the multi-hop reasoning ability of many existing LLMs, significant efforts are still needed under short context windows (less than 4k).

------------

`[2402.06918] Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought <https://arxiv.org/abs/2402.06918>`__ 用成对比较方法生成思维链以寻找最有前途的中间思维

::

    replaced with revised version Wed, 26 Jun 2024 05:47:52 GMT
    Submission history From: Zhen-Yu Zhang [view email]
    [v1] Sat, 10 Feb 2024 09:51:03 UTC (118 KB)
    [v2] Wed, 26 Jun 2024 05:47:52 UTC (157 KB)
    Zhen-Yu Zhang, Siwei Han, Huaxiu Yao, Gang Niu, Masashi Sugiyama

To improve the ability of the large language model (LLMs) to tackle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, enabling problem solving from simple to complex. State-of-the-art methods for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we use pairwise-comparison evaluation instead of point-wise scoring to search for promising intermediate thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair, allowing us to identify the most promising thoughts through an iterative process. To further alleviate the noise in the comparison, we incorporate techniques from ensemble learning and dueling bandits, proposing two variants of the algorithm. Experiments on three real-world tasks demonstrate the effectiveness of our proposed algorithm and verify the rationale of the pairwise comparison mechanism.

------------

`[2404.15778] BASS: Batched Attention-optimized Speculative Sampling <https://arxiv.org/abs/2404.15778>`__ BASS:分批注意力优化的推测采样

::

    replaced with revised version Wed, 26 Jun 2024 17:29:46 GMT
    Submission history From: Haifeng Qian [view email]
    [v1] Wed, 24 Apr 2024 09:57:11 UTC (448 KB)
    [v2] Wed, 26 Jun 2024 17:29:46 UTC (450 KB)
    Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha, Mingyue Shang, Sanjay Krishna Gouda, Ramesh Nallapati, Sudipta Sengupta, Xiaofei Ma, Anoop Deoras

Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15X speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, our system is able to generate sequences with HumanEval Pass@First of 43% and Pass@All of 61%, far exceeding what's feasible with single-sequence speculative decoding. Our peak GPU utilization during decoding reaches as high as 15.8%, more than 3X the highest of that of regular decoding and around 10X of single-sequence speculative decoding.

------------

`[2406.17542] CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained Models using Greedy Coordinate Descent <https://arxiv.org/abs/2406.17542>`__ CDQuant:使用贪婪坐标下降对大型预训练模型进行精确的训练后权重量化

::

    replaced with revised version Wed, 26 Jun 2024 07:44:42 GMT
    Submission history From: Pranav Nair [view email]
    [v1] Tue, 25 Jun 2024 13:29:14 UTC (54 KB)
    [v2] Wed, 26 Jun 2024 07:44:42 UTC (54 KB)
    Pranav Ajit Nair and Arun Sai Suggala

Large language models (LLMs) have recently demonstrated remarkable performance across diverse language tasks. But their deployment is often constrained by their substantial computational and storage requirements. Quantization has emerged as a key technique for addressing this challenge, enabling the compression of large models with minimal impact on performance. The recent GPTQ algorithm, a post-training quantization (PTQ) method, has proven highly effective for compressing LLMs, sparking a wave of research that leverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the PTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ with improved performance. CDQuant uses coordinate descent to minimize the layer-wise reconstruction loss to achieve high-quality quantized weights. Our algorithm is easy to implement and scales efficiently to models with hundreds of billions of parameters. Through extensive evaluation on the PaLM2 model family, we demonstrate that CDQuant consistently outperforms GPTQ across diverse model sizes and quantization levels. In particular, for INT2 quantization of PaLM2-Otter, CDQuant achieves a 10% reduction in perplexity compared to GPTQ.

------------

`[2402.05935] SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models <https://arxiv.org/abs/2402.05935>`__ SPHINX-X:多模态大型语言模型族的数据和参数扩展

::

    replaced with revised version Wed, 26 Jun 2024 07:59:03 GMT
    Submission history From: Renrui Zhang [view email]
    [v1] Thu, 8 Feb 2024 18:59:48 UTC (12,924 KB)
    [v2] Wed, 26 Jun 2024 07:59:03 UTC (12,945 KB)
    Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li, Yu Qiao, Peng Gao

We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. Code and models are released at this https URL

------------

`[2405.05905] Truthful Aggregation of LLMs with an Application to Online Advertising <https://arxiv.org/abs/2405.05905>`__ llm与在线广告应用的真实聚合

::

    replaced with revised version Wed, 26 Jun 2024 06:29:32 GMT
    Submission history From: Ermis Soumalias Mr. [view email]
    [v1] Thu, 9 May 2024 17:01:31 UTC (806 KB)
    [v2] Tue, 25 Jun 2024 16:31:31 UTC (1,625 KB)
    [v3] Wed, 26 Jun 2024 06:29:32 UTC (1,625 KB)
    Ermis Soumalias, Michael J. Curry, Sven Seuken

Online platforms generate hundreds of billions of dollars in revenue per year by showing advertisements alongside their own content. Currently, these platforms are integrating Large Language Models (LLMs) into their services. This makes revenue generation from LLM-generated content the next major challenge in online advertising. We consider a scenario where advertisers aim to influence the responses of an LLM to align with their interests, while platforms seek to maximize advertiser value and ensure user satisfaction. We introduce an auction mechanism for this problem that operates without LLM fine-tuning or access to model weights and provably converges to the output of the optimally fine-tuned LLM for the platform's objective as computational resources increase. Our mechanism ensures that truthful reporting is a dominant strategy for advertisers and it aligns each advertiser's utility with their contribution to social welfare - an essential feature for long-term viability. Additionally, it can incorporate contextual information about the advertisers, significantly accelerating convergence. Via experiments with a publicly available LLM, we show that our mechanism significantly boosts advertiser value and platform revenue, with low computational overhead. While our motivating application is online advertising, our mechanism can be applied in any setting with monetary transfers, making it a general-purpose solution for truthfully aggregating the preferences of self-interested agents over LLM-generated replies.

------------

`[2406.16990] AND: Audio Network Dissection for Interpreting Deep Acoustic Models <https://arxiv.org/abs/2406.16990>`__ 以及:用于解释深度声学模型的音频网络解剖

::

    replaced with revised version Wed, 26 Jun 2024 17:36:53 GMT
    Submission history From: Tungyu Wu [view email]
    [v1] Mon, 24 Jun 2024 06:02:07 UTC (4,398 KB)
    [v2] Wed, 26 Jun 2024 17:36:53 UTC (4,398 KB)
    Tung-Yu Wu, Yu-Xiang Lin, and Tsui-Wei Weng

Neuron-level interpretations aim to explain network behaviors and properties by investigating neurons responsive to specific perceptual or structural input patterns. Although there is emerging work in the vision and language domains, none is explored for acoustic models. To bridge the gap, we introduce $\textit{AND}$, the first $\textbf{A}$udio $\textbf{N}$etwork $\textbf{D}$issection framework that automatically establishes natural language explanations of acoustic neurons based on highly-responsive audio. $\textit{AND}$ features the use of LLMs to summarize mutual acoustic features and identities among audio. Extensive experiments are conducted to verify $\textit{AND}$'s precise and informative descriptions. In addition, we demonstrate a potential use of $\textit{AND}$ for audio machine unlearning by conducting concept-specific pruning based on the generated descriptions. Finally, we highlight two acoustic model behaviors with analysis by $\textit{AND}$: (i) models discriminate audio with a combination of basic acoustic features rather than high-level abstract concepts; (ii) training strategies affect model behaviors and neuron interpretability -- supervised training guides neurons to gradually narrow their attention, while self-supervised learning encourages neurons to be polysemantic for exploring high-level features.

------------

`[2404.03192] Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers <https://arxiv.org/abs/2404.03192>`__ 大型语言模型排名公平吗?关于llm作为排名者公平性的实证研究

::

    replaced with revised version Tue, 25 Jun 2024 20:54:16 GMT
    Submission history From: Xuyang Wu [view email]
    [v1] Thu, 4 Apr 2024 04:23:19 UTC (8,149 KB)
    [v2] Tue, 25 Jun 2024 20:54:16 UTC (8,150 KB)
    Yuan Wang, Xuyang Wu, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang

The integration of Large Language Models (LLMs) in information retrieval has raised a critical reevaluation of fairness in the text-ranking models. LLMs, such as GPT models and Llama2, have shown effectiveness in natural language understanding tasks, and prior works (e.g., RankGPT) have also demonstrated that the LLMs exhibit better performance than the traditional ranking models in the ranking task. However, their fairness remains largely unexplored. This paper presents an empirical study evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes. Our analysis delves into how these LLMs handle queries and documents related to these attributes, aiming to uncover biases in their ranking algorithms. We assess fairness from both user and content perspectives, contributing an empirical benchmark for evaluating LLMs as the fair ranker.

------------

`[2404.10237] Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models <https://arxiv.org/abs/2404.10237>`__ Med-MoE:面向轻量级医学视觉-语言模型的领域专家混合

::

    replaced with revised version Wed, 26 Jun 2024 06:04:51 GMT
    Submission history From: Songtao Jiang [view email]
    [v1] Tue, 16 Apr 2024 02:35:17 UTC (20,069 KB)
    [v2] Wed, 26 Jun 2024 06:04:51 UTC (33,383 KB)
    Songtao Jiang, Tuo Zheng, Yan Zhang, Yeying Jin, Li Yuan and Zuozhu Liu

Recent advancements in general-purpose or domain-specific multimodal large language models (LLMs) have witnessed remarkable progress for medical decision-making. However, they are designated for specific classification or generative tasks, and require model training or finetuning on large-scale datasets with sizeable parameters and tremendous computing, hindering their clinical utility across diverse resource-constrained scenarios in practice. In this paper, we propose a novel and lightweight framework Med-MoE (Mixture-of-Experts) that tackles both discriminative and generative multimodal medical tasks. The learning of Med-MoE consists of three steps: multimodal medical alignment, instruction tuning and routing, and domain-specific MoE tuning. After aligning multimodal medical images with LLM tokens, we then enable the model for different multimodal medical tasks with instruction tuning, together with a trainable router tailored for expert selection across input modalities. Finally, the model is tuned by integrating the router with multiple domain-specific experts, which are selectively activated and further empowered by meta expert. Comprehensive experiments on both open- and close-end medical question answering (Med-VQA) and image classification tasks across datasets such as VQA-RAD, SLAKE and Path-VQA demonstrate that our model can achieve performance superior to or on par with state-of-the-art baselines, while only requiring approximately 30\%-50\% of activated model parameters. Extensive analysis and ablations corroborate the effectiveness and practical utility of our method.

------------

`[2406.14898] Safely Learning with Private Data: A Federated Learning Framework for Large Language Model <https://arxiv.org/abs/2406.14898>`__ 私有数据安全学习:大型语言模型联邦学习框架

::

    replaced with revised version Wed, 26 Jun 2024 04:28:38 GMT
    Submission history From: JiaYing Zheng [view email]
    [v1] Fri, 21 Jun 2024 06:43:15 UTC (9,546 KB)
    [v2] Wed, 26 Jun 2024 04:28:38 UTC (9,495 KB)
    JiaYing Zheng, HaiNan Zhang, LingXiang Wang, WangJie Qiu, HongWei Zheng, ZhiMing Zheng

Private data, being larger and quality-higher than public data, can greatly improve large language models (LLM). However, due to privacy concerns, this data is often dispersed in multiple silos, making its secure utilization for LLM training a challenge. Federated learning (FL) is an ideal solution for training models with distributed private data, but traditional frameworks like FedAvg are unsuitable for LLM due to their high computational demands on clients. An alternative, split learning, offloads most training parameters to the server while training embedding and output layers locally, making it more suitable for LLM. Nonetheless, it faces significant challenges in security and efficiency. Firstly, the gradients of embeddings are prone to attacks, leading to potential reverse engineering of private data. Furthermore, the server's limitation of handle only one client's training request at a time hinders parallel training, severely impacting training efficiency. In this paper, we propose a Federated Learning framework for LLM, named FL-GLM, which prevents data leakage caused by both server-side and peer-client attacks while improving training efficiency. Specifically, we first place the input block and output block on local client to prevent embedding gradient attacks from server. Secondly, we employ key-encryption during client-server communication to prevent reverse engineering attacks from peer-clients. Lastly, we employ optimization methods like client-batching or server-hierarchical, adopting different acceleration methods based on the actual computational capabilities of the server. Experimental results on NLU and generation tasks demonstrate that FL-GLM achieves comparable metrics to centralized chatGLM model, validating the effectiveness of our federated learning framework.

------------

`[2405.02466] ProFLingo: A Fingerprinting-based Intellectual Property Protection Scheme for Large Language Models <https://arxiv.org/abs/2405.02466>`__ ProFLingo:一种基于指纹的大型语言模型知识产权保护方案

::

    replaced with revised version Wed, 26 Jun 2024 16:22:43 GMT
    Submission history From: Heng Jin [view email]
    [v1] Fri, 3 May 2024 20:00:40 UTC (1,134 KB)
    [v2] Wed, 26 Jun 2024 16:22:43 UTC (988 KB)
    Heng Jin and Chaoyu Zhang and Shanghao Shi and Wenjing Lou and Y. Thomas Hou

Large language models (LLMs) have attracted significant attention in recent years. Due to their "Large" nature, training LLMs from scratch consumes immense computational resources. Since several major players in the artificial intelligence (AI) field have open-sourced their original LLMs, an increasing number of individual researchers and smaller companies are able to build derivative LLMs based on these open-sourced models at much lower costs. However, this practice opens up possibilities for unauthorized use or reproduction that may not comply with licensing agreements, and fine-tuning can change the model's behavior, thus complicating the determination of model ownership. Current intellectual property (IP) protection schemes for LLMs are either designed for white-box settings or require additional modifications to the original model, which restricts their use in real-world settings.
In this paper, we propose ProFLingo, a black-box fingerprinting-based IP protection scheme for LLMs. ProFLingo generates queries that elicit specific responses from an original model, thereby establishing unique fingerprints. Our scheme assesses the effectiveness of these queries on a suspect model to determine whether it has been derived from the original model. ProFLingo offers a non-invasive approach, which neither requires knowledge of the suspect model nor modifications to the base model or its training process. To the best of our knowledge, our method represents the first black-box fingerprinting technique for IP protection for LLMs. Our source code and generated queries are available at: this https URL.

------------

----------
Index (88)
----------

`[2406.17840] Human-Object Interaction from Human-Level Instructions <https://arxiv.org/abs/2406.17840>`__ 基于人级指令的人-物交互

`[2406.18346] AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations <https://arxiv.org/abs/2406.18346>`__ 通过从人类反馈中强化学习的人工智能对齐?矛盾与局限

`[2406.17803] Understanding the Role of User Profile in the Personalization of Large Language Models <https://arxiv.org/abs/2406.17803>`__ 理解用户画像在大型语言模型个性化中的作用

`[2406.17805] Can LLMs Generate Visualizations with Dataless Prompts? <https://arxiv.org/abs/2406.17805>`__ llm可以生成具有无数据提示的可视化吗?

`[2406.17806] MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries? <https://arxiv.org/abs/2406.17806>`__ MOSSBench:你的多模态语言模型对安全查询过度敏感吗?

`[2406.17807] Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary <https://arxiv.org/abs/2406.17807>`__ 非完全信息牌游戏解说策略的增强——关丹解说大型语言模型研究

`[2406.17923] PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning <https://arxiv.org/abs/2406.17923>`__ PAFT:一种有效的LLM微调并行训练范式

`[2406.17947] Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias <https://arxiv.org/abs/2406.17947>`__ 他们指的是“我们”吗?解释群体间偏见中的指代表达

`[2406.17967] Unmasking the Imposters: In-Domain Detection of Human vs. Machine-Generated Tweets <https://arxiv.org/abs/2406.17967>`__ 揭秘冒名顶替者:人工与机器生成推文的域内检测

`[2406.17975] Inherent Challenges of Post-Hoc Membership Inference for Large Language Models <https://arxiv.org/abs/2406.17975>`__

`[2406.17990] Explicit Diversity Conditions for Effective Question Answer Generation with Large Language Models <https://arxiv.org/abs/2406.17990>`__ 基于大型语言模型的有效问答生成的显式多样性条件

`[2406.17992] Catching Chameleons: Detecting Evolving Disinformation Generated using Large Language Models <https://arxiv.org/abs/2406.17992>`__ 捕捉变色龙:检测使用大型语言模型生成的不断演化的虚假信息

`[2406.18027] Automated Clinical Data Extraction with Knowledge Conditioned LLMs <https://arxiv.org/abs/2406.18027>`__ 基于知识条件llm的自动临床数据提取

`[2406.18045] PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry <https://arxiv.org/abs/2406.18045>`__ pharmacgpt:生物制药和化学领域特定大型语言模型

`[2406.18049] Improving Entity Recognition Using Ensembles of Deep Learning and Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction from Multiple Sources <https://arxiv.org/abs/2406.18049>`__

`[2406.18078] Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction <https://arxiv.org/abs/2406.18078>`__ 基于伪标签评分器自训练的方面情感四元组预测

`[2406.18088] LLM-Driven Multimodal Opinion Expression Identification <https://arxiv.org/abs/2406.18088>`__ llm驱动的多模态观点表达识别

`[2406.18116] BADGE: BADminton report Generation and Evaluation with LLM <https://arxiv.org/abs/2406.18116>`__ 徽章:与LLM一起生成和评估羽毛球报告

`[2406.18120] ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech Recognition Using LLMs <https://arxiv.org/abs/2406.18120>`__ ArzEn-LLM:使用llm进行代码转换的埃及阿拉伯语-英语翻译和语音识别

`[2406.18122] Poisoned LangChain: Jailbreak LLMs by LangChain <https://arxiv.org/abs/2406.18122>`__

`[2406.18125] ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets and Large Language Models <https://arxiv.org/abs/2406.18125>`__ ResumeAtlas:基于大规模数据集和大型语言模型的简历分类

`[2406.18133] ConvoCache: Smart Re-Use of Chatbot Responses <https://arxiv.org/abs/2406.18133>`__ ConvoCache:智能重用聊天机器人的响应

`[2406.18164] NeBuLa: A discourse aware Minecraft Builder <https://arxiv.org/abs/2406.18164>`__ 星云:一个话语感知的Minecraft建造者

`[2406.18173] UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs <https://arxiv.org/abs/2406.18173>`__ UIO-LLMs:长上下文LLMs的无偏增量优化

`[2406.18187] Selective Prompting Tuning for Personalized Conversations with LLMs <https://arxiv.org/abs/2406.18187>`__ 与llm进行个性化对话的选择性提示调优

`[2406.18192] Methodology of Adapting Large English Language Models for Specific Cultural Contexts <https://arxiv.org/abs/2406.18192>`__

`[2406.18219] A Closer Look into Mixture-of-Experts in Large Language Models <https://arxiv.org/abs/2406.18219>`__ 对大型语言模型中专家混合的仔细研究

`[2406.18221] Enhancing Data Privacy in Large Language Models through Private Association Editing <https://arxiv.org/abs/2406.18221>`__ 通过私有关联编辑增强大型语言模型的数据隐私性

`[2406.18256] LLaMIPa: An Incremental Discourse Parser <https://arxiv.org/abs/2406.18256>`__ LLaMIPa:一个增量篇章解析器

`[2406.18259] Detecting Machine-Generated Texts: Not Just "AI vs Humans" and Explainability is Complicated <https://arxiv.org/abs/2406.18259>`__ 检测机器生成的文本:不仅仅是“人工智能vs人类”和可解释性是复杂的

`[2406.18266] "Vorbe\c{s}ti Rom\^ane\c{s}te?" A Recipe to Train Powerful Romanian LLMs with English Instructions <https://arxiv.org/abs/2406.18266>`__ “Vorbe \ c{年代}ti罗\ ^一\ c{年代}te ?”一个用英语指令训练强大的罗马尼亚llm的菜谱

`[2406.18294] Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs <https://arxiv.org/abs/2406.18294>`__ 分层上下文剪枝:用存储库级预训练代码llm优化真实世界的代码补全

`[2406.18297] FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning <https://arxiv.org/abs/2406.18297>`__

`[2406.18305] S3: A Simple Strong Sample-effective Multimodal Dialog System <https://arxiv.org/abs/2406.18305>`__ S3:一个简单高效的多模态对话系统

`[2406.18312] AI-native Memory: A Pathway from LLMs Towards AGI <https://arxiv.org/abs/2406.18312>`__ ai原生内存:从llm到AGI的路径

`[2406.18365] Themis: Towards Flexible and Interpretable NLG Evaluation <https://arxiv.org/abs/2406.18365>`__ Themis:迈向灵活可解释的NLG评估

`[2406.18400] Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers <https://arxiv.org/abs/2406.18400>`__ LLMs会梦见大象吗(当被告知不要梦见时)?《变形金刚》中的潜在概念联想和联想记忆

`[2406.18403] LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks <https://arxiv.org/abs/2406.18403>`__ LLMs而不是人类法官?20个NLP评估任务的大规模实证研究

`[2406.18406] IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons <https://arxiv.org/abs/2406.18406>`__

`[2406.18449] Cascading Large Language Models for Salient Event Graph Generation <https://arxiv.org/abs/2406.18449>`__ 面向显著事件图生成的大规模语言模型级联

`[2406.18460] Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation <https://arxiv.org/abs/2406.18460>`__ 面向开放域人机对话的大型语言模型角色扮演零样本提示

`[2406.18510] WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models <https://arxiv.org/abs/2406.18510>`__ 大规模组队:从狂野的越狱到(相对)更安全的语言模型

`[2406.18512] "Is ChatGPT a Better Explainer than My Professor?": Evaluating the Explanation Capabilities of LLMs in Conversation Compared to a Human Baseline <https://arxiv.org/abs/2406.18512>`__ “ChatGPT比我的教授更能解释问题吗?”:与人类基线相比，评估llm在对话中的解释能力

`[2406.18521] CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs <https://arxiv.org/abs/2406.18521>`__ CharXiv:绘制多模态llm中现实图表理解的差距

`[2406.18528] PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation <https://arxiv.org/abs/2406.18528>`__

`[2406.17812] Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars <https://arxiv.org/abs/2406.17812>`__ 面向科学的可扩展人工智能:视角、方法和范例

`[2406.17972] LABOR-LLM: Language-Based Occupational Representations with Large Language Models <https://arxiv.org/abs/2406.17972>`__ LABOR-LLM:基于大型语言模型的职业表示

`[2406.17781] Large Language Models estimate fine-grained human color-concept associations <https://arxiv.org/abs/2406.17781>`__ 大型语言模型估计细粒度人类颜色-概念关联

`[2406.17915] Semi-supervised classification of dental conditions in panoramic radiographs using large language model and instance segmentation: A real-world dataset evaluation <https://arxiv.org/abs/2406.17915>`__ 基于大型语言模型和实例分割的全景x光片牙齿状况半监督分类:真实世界数据集评估

`[2406.17957] Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment <https://arxiv.org/abs/2406.17957>`__ 通过学习单调对齐提高llm语音合成的鲁棒性

`[2406.18069] Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals <https://arxiv.org/abs/2406.18069>`__

`[2406.18087] EHR-Based Mobile and Web Platform for Chronic Disease Risk Prediction Using Large Language Multimodal Models <https://arxiv.org/abs/2406.18087>`__ 使用大型语言多模态模型的基于ehr的移动和Web平台慢性病风险预测

`[2406.18115] Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps <https://arxiv.org/abs/2406.18115>`__ 基于3D语义地图的未见动态环境中的开放词汇移动操作

`[2406.18193] MammothModa: Multi-Modal Large Language Model <https://arxiv.org/abs/2406.18193>`__ MammothModa:多模态大型语言模型

`[2406.18379] MALSIGHT: Exploring Malicious Source Code and Benign Pseudocode for Iterative Binary Malware Summarization <https://arxiv.org/abs/2406.18379>`__ MALSIGHT:基于迭代式二进制恶意代码摘要的恶意源代码和良性伪代码探索

`[2406.18118] SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance <https://arxiv.org/abs/2406.18118>`__ SafeAligner:基于响应视差引导的针对越狱攻击的安全对齐

`[2406.18382] Adversarial Search Engine Optimization for Large Language Models <https://arxiv.org/abs/2406.18382>`__ 面向大型语言模型的对抗性搜索引擎优化

`[2312.02706] Large Knowledge Model: Perspectives and Challenges <https://arxiv.org/abs/2312.02706>`__

`[2402.17709] Case-Based or Rule-Based: How Do Transformers Do the Math? <https://arxiv.org/abs/2402.17709>`__ 基于案例还是基于规则:transformer如何计算?

`[2311.14096] Cultural Bias and Cultural Alignment of Large Language Models <https://arxiv.org/abs/2311.14096>`__ 大型语言模型的文化偏见和文化对齐

`[2311.18702] CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation <https://arxiv.org/abs/2311.18702>`__ CritiqueLLM:面向大型语言模型生成评估的信息批判生成模型

`[2312.09542] Marathon: A Race Through the Realm of Long Context with Large Language Models <https://arxiv.org/abs/2312.09542>`__ 马拉松:用大型语言模型在长上下文领域中的比赛

`[2402.04678] FaithLM: Towards Faithful Explanations for Large Language Models <https://arxiv.org/abs/2402.04678>`__ FaithLM:大型语言模型的忠实解释

`[2402.10663] Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL <https://arxiv.org/abs/2402.10663>`__ 通过人工无关的文本到sql融合提高演示多样性

`[2402.13561] Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment <https://arxiv.org/abs/2402.13561>`__ 认知视觉语言映射器:增强视觉知识对齐促进多模态理解

`[2405.11407] Can Public LLMs be used for Self-Diagnosis of Medical Conditions ? <https://arxiv.org/abs/2405.11407>`__ 公共llm可以用于医疗状况的自我诊断吗?

`[2405.14159] Super Tiny Language Models <https://arxiv.org/abs/2405.14159>`__ 超微型语言模型

`[2406.01179] Are AI-Generated Text Detectors Robust to Adversarial Perturbations? <https://arxiv.org/abs/2406.01179>`__ 人工智能生成的文本检测器对对抗性扰动鲁棒吗?

`[2406.04175] Confabulation: The Surprising Value of Large Language Model Hallucinations <https://arxiv.org/abs/2406.04175>`__ 虚构:大型语言模型幻觉的惊人价值

`[2406.04625] Key-Element-Informed sLLM Tuning for Document Summarization <https://arxiv.org/abs/2406.04625>`__ 基于关键元素的sLLM文档摘要调优

`[2406.10190] CHIRON: Rich Character Representations in Long-Form Narratives <https://arxiv.org/abs/2406.10190>`__ CHIRON:在长篇叙事中丰富的人物表现

`[2406.10552] Large Language Model Enhanced Clustering for News Event Detection <https://arxiv.org/abs/2406.10552>`__ 面向新闻事件检测的大型语言模型增强聚类

`[2406.10794] Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis <https://arxiv.org/abs/2406.10794>`__ LLMs中的越狱攻击理解:表示空间分析

`[2406.11162] How Good are LLMs at Relation Extraction under Low-Resource Scenario? Comprehensive Evaluation <https://arxiv.org/abs/2406.11162>`__ llm在低资源场景下的关系提取效果如何?综合评价

`[2406.11709] Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging <https://arxiv.org/abs/2406.11709>`__ 指示，而不是辅助:基于llm的多轮规划和分层提问，用于苏格拉底式代码调试

`[2406.16253] LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing <https://arxiv.org/abs/2406.16253>`__ LLMs协助NLP研究人员:评论论文(元)评审

`[2406.16801] RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale <https://arxiv.org/abs/2406.16801>`__ RES-Q:评估存储库规模的大型语言模型系统的代码编辑

`[2406.17588] LongIns: A Challenging Long-context Instruction-based Exam for LLMs <https://arxiv.org/abs/2406.17588>`__ LongIns:一种具有挑战性的基于长上下文指令的LLMs考试

`[2402.06918] Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought <https://arxiv.org/abs/2402.06918>`__ 用成对比较方法生成思维链以寻找最有前途的中间思维

`[2404.15778] BASS: Batched Attention-optimized Speculative Sampling <https://arxiv.org/abs/2404.15778>`__ BASS:分批注意力优化的推测采样

`[2406.17542] CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained Models using Greedy Coordinate Descent <https://arxiv.org/abs/2406.17542>`__ CDQuant:使用贪婪坐标下降对大型预训练模型进行精确的训练后权重量化

`[2402.05935] SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models <https://arxiv.org/abs/2402.05935>`__ SPHINX-X:多模态大型语言模型族的数据和参数扩展

`[2405.05905] Truthful Aggregation of LLMs with an Application to Online Advertising <https://arxiv.org/abs/2405.05905>`__ llm与在线广告应用的真实聚合

`[2406.16990] AND: Audio Network Dissection for Interpreting Deep Acoustic Models <https://arxiv.org/abs/2406.16990>`__ 以及:用于解释深度声学模型的音频网络解剖

`[2404.03192] Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers <https://arxiv.org/abs/2404.03192>`__ 大型语言模型排名公平吗?关于llm作为排名者公平性的实证研究

`[2404.10237] Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models <https://arxiv.org/abs/2404.10237>`__ Med-MoE:面向轻量级医学视觉-语言模型的领域专家混合

`[2406.14898] Safely Learning with Private Data: A Federated Learning Framework for Large Language Model <https://arxiv.org/abs/2406.14898>`__ 私有数据安全学习:大型语言模型联邦学习框架

`[2405.02466] ProFLingo: A Fingerprinting-based Intellectual Property Protection Scheme for Large Language Models <https://arxiv.org/abs/2405.02466>`__ ProFLingo:一种基于指纹的大型语言模型知识产权保护方案

