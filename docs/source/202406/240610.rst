240610
========

----------
Survey (1)
----------

`[2311.00530] Advances in Embodied Navigation Using Large Language Models: A Survey <https://arxiv.org/abs/2311.00530>`__ 基于大型语言模型的具身导航进展综述

::

    replaced with revised version Fri, 7 Jun 2024 13:13:41 GMT
    Submission history From: Jinzhou Lin [view email]
    [v1] Wed, 1 Nov 2023 14:08:56 UTC (802 KB)
    [v2] Fri, 10 Nov 2023 06:21:32 UTC (807 KB)
    [v3] Sat, 18 Nov 2023 01:37:39 UTC (806 KB)
    [v4] Fri, 7 Jun 2024 13:13:41 UTC (885 KB)
    Jinzhou Lin, Han Gao, Xuxiang Feng, Rongtao Xu, Changwei Wang, Man Zhang, Li Guo, Shibiao Xu

In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidates the role of LLMs in embodied intelligence, based on current research, and forecasts future directions in the field. A comprehensive list of studies in this survey is available at this https URL.

------------

--------------
Benchmark (11)
--------------

`[2406.04598] OCDB: Revisiting Causal Discovery with a Comprehensive Benchmark and Evaluation Framework <https://arxiv.org/abs/2406.04598>`__ OCDB:基于综合基准和评估框架的因果发现研究

::

    Fri, 7 Jun 2024 03:09:22 GMT
    Wei Zhou, Hong Huang, Guowen Zhang, Ruize Shi, Kehan Yin, Yuanyuan Lin, Bang Liu

Large language models (LLMs) have excelled in various natural language processing tasks, but challenges in interpretability and trustworthiness persist, limiting their use in high-stakes fields. Causal discovery offers a promising approach to improve transparency and reliability. However, current evaluations are often one-sided and lack assessments focused on interpretability performance. Additionally, these evaluations rely on synthetic data and lack comprehensive assessments of real-world datasets. These lead to promising methods potentially being overlooked. To address these issues, we propose a flexible evaluation framework with metrics for evaluating differences in causal structures and causal effects, which are crucial attributes that help improve the interpretability of LLMs. We introduce the Open Causal Discovery Benchmark (OCDB), based on real data, to promote fair comparisons and drive optimization of algorithms. Additionally, our new metrics account for undirected edges, enabling fair comparisons between Directed Acyclic Graphs (DAGs) and Completed Partially Directed Acyclic Graphs (CPDAGs). Experimental results show significant shortcomings in existing algorithms' generalization capabilities on real data, highlighting the potential for performance improvement and the importance of our framework in advancing causal discovery techniques.

------------

`[2406.04520] NATURAL PLAN: Benchmarking LLMs on Natural Language Planning <https://arxiv.org/abs/2406.04520>`__ NATURAL PLAN:对llm进行自然语言规划的基准测试

::

    Thu, 6 Jun 2024 21:27:35 GMT
    Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, Denny Zhou

We introduce NATURAL PLAN, a realistic planning benchmark in natural language containing 3 key tasks: Trip Planning, Meeting Planning, and Calendar Scheduling. We focus our evaluation on the planning capabilities of LLMs with full information on the task, by providing outputs from tools such as Google Flights, Google Maps, and Google Calendar as contexts to the models. This eliminates the need for a tool-use environment for evaluating LLMs on Planning.
We observe that NATURAL PLAN is a challenging benchmark for state of the art models. For example, in Trip Planning, GPT-4 and Gemini 1.5 Pro could only achieve 31.1% and 34.8% solve rate respectively. We find that model performance drops drastically as the complexity of the problem increases: all models perform below 5% when there are 10 cities, highlighting a significant gap in planning in natural language for SoTA LLMs. We also conduct extensive ablation studies on NATURAL PLAN to further shed light on the (in)effectiveness of approaches such as self-correction, few-shot generalization, and in-context planning with long-contexts on improving LLM planning.

------------

`[2406.04744] CRAG -- Comprehensive RAG Benchmark <https://arxiv.org/abs/2406.04744>`__ RAG——综合RAG基准

::

    Fri, 7 Jun 2024 08:43:07 GMT
    Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, Xin Luna Dong

Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve <=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.

------------

`[2406.04752] CRiskEval: A Chinese Multi-Level Risk Evaluation Benchmark Dataset for Large Language Models <https://arxiv.org/abs/2406.04752>`__ CRiskEval:面向大型语言模型的中文多级风险评估基准数据集

::

    Fri, 7 Jun 2024 08:52:24 GMT
    Ling Shi and Deyi Xiong

Large language models (LLMs) are possessed of numerous beneficial capabilities, yet their potential inclination harbors unpredictable risks that may materialize in the future. We hence propose CRiskEval, a Chinese dataset meticulously designed for gauging the risk proclivities inherent in LLMs such as resource acquisition and malicious coordination, as part of efforts for proactive preparedness. To curate CRiskEval, we define a new risk taxonomy with 7 types of frontier risks and 4 safety levels, including extremely hazardous,moderately hazardous, neutral and safe. We follow the philosophy of tendency evaluation to empirically measure the stated desire of LLMs via fine-grained multiple-choice question answering. The dataset consists of 14,888 questions that simulate scenarios related to predefined 7 types of frontier risks. Each question is accompanied with 4 answer choices that state opinions or behavioral tendencies corresponding to the question. All answer choices are manually annotated with one of the defined risk levels so that we can easily build a fine-grained frontier risk profile for each assessed LLM. Extensive evaluation with CRiskEval on a spectrum of prevalent Chinese LLMs has unveiled a striking revelation: most models exhibit risk tendencies of more than 40% (weighted tendency to the four risk levels). Furthermore, a subtle increase in the model's inclination toward urgent self-sustainability, power seeking and other dangerous goals becomes evident as the size of models increase. To promote further research on the frontier risk evaluation of LLMs, we publicly release our dataset at https://github.com/lingshi6565/Risk_eval.

------------

`[2406.04770] WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild <https://arxiv.org/abs/2406.04770>`__ WildBench:用真实用户在野外完成的具有挑战性的任务对llm进行基准测试

::

    Fri, 7 Jun 2024 09:15:44 GMT
    Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, Yejin Choi

We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of ``slightly better/worse'' to ``tie'' if the winner response exceeds the loser one by more than $K$ characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard's 0.91 and AlpacaEval2.0's 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.

------------

`[2406.04845] FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models <https://arxiv.org/abs/2406.04845>`__ FedLLM-Bench:大型语言模型联邦学习的现实基准

::

    Fri, 7 Jun 2024 11:19:30 GMT
    Rui Ye, Rui Ge, Xinyu Zhu, Jingyi Chai, Yaxin Du, Yang Liu, Yanfeng Wang, Siheng Chen

Federated learning has enabled multiple parties to collaboratively train large language models without directly sharing their data (FedLLM). Following this training paradigm, the community has put massive efforts from diverse aspects including framework, performance, and privacy. However, an unpleasant fact is that there are currently no realistic datasets and benchmarks for FedLLM and previous works all rely on artificially constructed datasets, failing to capture properties in real-world scenarios. Addressing this, we propose FedLLM-Bench, which involves 8 training methods, 4 training datasets, and 6 evaluation metrics, to offer a comprehensive testbed for the FedLLM community. FedLLM-Bench encompasses three datasets (e.g., user-annotated multilingual dataset) for federated instruction tuning and one dataset (e.g., user-annotated preference dataset) for federated preference alignment, whose scale of client number ranges from 38 to 747. Our datasets incorporate several representative diversities: language, quality, quantity, instruction, length, embedding, and preference, capturing properties in real-world scenarios. Based on FedLLM-Bench, we conduct experiments on all datasets to benchmark existing FL methods and provide empirical insights (e.g., multilingual collaboration).
We believe that our FedLLM-Bench can benefit the FedLLM community by reducing required efforts, providing a practical testbed, and promoting fair comparisons. Code and datasets are available at https://github.com/rui-ye/FedLLM-Bench.

------------

`[2406.05079] SUMIE: A Synthetic Benchmark for Incremental Entity Summarization <https://arxiv.org/abs/2406.05079>`__ SUMIE:增量实体摘要合成基准

::

    Fri, 7 Jun 2024 16:49:21 GMT
    Eunjeong Hwang, Yichao Zhou, Beliz Gunel, James Bradley Wendt and Sandeep Tata

No existing dataset adequately tests how well language models can incrementally update entity summaries - a crucial ability as these models rapidly advance. The Incremental Entity Summarization (IES) task is vital for maintaining accurate, up-to-date knowledge. To address this, we introduce SUMIE, a fully synthetic dataset designed to expose real-world IES challenges.
This dataset effectively highlights problems like incorrect entity association and incomplete information presentation. Unlike common synthetic datasets, ours captures the complexity and nuances found in real-world data. We generate informative and diverse attributes, summaries, and unstructured paragraphs in sequence, ensuring high quality. The alignment between generated summaries and paragraphs exceeds 96%, confirming the dataset's quality. Extensive experiments demonstrate the dataset's difficulty - state-of-the-art LLMs struggle to update summaries with an F1 higher than 80.4%. We will open source the benchmark and the evaluation metrics to help the community make progress on IES tasks.

------------

`[2406.05053] Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation <https://arxiv.org/abs/2406.05053>`__ 浏览器中的提示:编程反馈生成的基准语言模型

::

    Fri, 7 Jun 2024 16:22:51 GMT
    Nachiket Kotalwar, Alkis Gotovos, Adish Singla

Generative AI and large language models hold great promise in enhancing programming education by generating individualized feedback and hints for learners. Recent works have primarily focused on improving the quality of generated feedback to achieve human tutors' quality. While quality is an important performance criterion, it is not the only criterion to optimize for real-world educational deployments. In this paper, we benchmark language models for programming feedback generation across several performance criteria, including quality, cost, time, and data privacy. The key idea is to leverage recent advances in the new paradigm of in-browser inference that allow running these models directly in the browser, thereby providing direct benefits across cost and data privacy. To boost the feedback quality of small models compatible with in-browser inference engines, we develop a fine-tuning pipeline based on GPT-4 generated synthetic data. We showcase the efficacy of fine-tuned Llama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser inference engine on three different Python programming datasets. We will release the full implementation along with a web app and datasets to facilitate further research on in-browser language models.

------------

`[2311.08588] CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation <https://arxiv.org/abs/2311.08588>`__ CodeScope:用于评估llm代码理解和生成的基于执行的多语言多任务多维基准

::

    replaced with revised version Fri, 7 Jun 2024 04:34:16 GMT
    Submission history From: Weixiang Yan [view email]
    [v1] Tue, 14 Nov 2023 23:18:52 UTC (254 KB)
    [v2] Tue, 6 Feb 2024 01:21:50 UTC (261 KB)
    [v3] Fri, 7 Jun 2024 04:34:16 UTC (262 KB)
    Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng

Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers 43 programming languages and eight coding tasks. It evaluates the coding performance of LLMs from three dimensions (perspectives): length, difficulty, and efficiency. To facilitate execution-based evaluations of code generation, we develop MultiCodeEngine, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at this https URL.

------------

`[2402.05044] SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models <https://arxiv.org/abs/2402.05044>`__ SALAD-Bench:大型语言模型的层次化全面安全基准

::

    replaced with revised version Fri, 7 Jun 2024 12:05:46 GMT
    Submission history From: Lijun Li [view email]
    [v1] Wed, 7 Feb 2024 17:33:54 UTC (12,088 KB)
    [v2] Thu, 8 Feb 2024 02:50:22 UTC (12,088 KB)
    [v3] Mon, 4 Mar 2024 07:20:31 UTC (12,089 KB)
    [v4] Fri, 7 Jun 2024 12:05:46 UTC (12,093 KB)
    Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao

In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics. Data and evaluator are released under this https URL.

------------

`[2402.02037] EffiBench: Benchmarking the Efficiency of Automatically Generated Code <https://arxiv.org/abs/2402.02037>`__ EffiBench:自动生成代码的效率基准测试

::

    replaced with revised version Fri, 7 Jun 2024 09:21:21 GMT
    Submission history From: Huang Dong [view email]
    [v1] Sat, 3 Feb 2024 05:24:39 UTC (963 KB)
    [v2] Thu, 15 Feb 2024 15:57:06 UTC (963 KB)
    [v3] Fri, 7 Jun 2024 09:21:21 UTC (945 KB)
    Dong Huang, Yuhao Qing, Weiyi Shang, Heming Cui, Jie M.Zhang

Code generation models have increasingly become integral to aiding software development. Although current research has thoroughly examined the correctness of the code produced by code generation models, a vital aspect that plays a pivotal role in green computing and sustainability efforts has often been neglected. This paper presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems to assess the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution, which obtains the SOTA efficiency on the LeetCode solution leaderboard. With EffiBench, we empirically examine the ability of 42 large language models (35 open-source and 7 closed-source) to generate efficient code. Our evaluation results demonstrate that the efficiency of the code generated by LLMs is generally worse than the efficiency of human-written canonical solutions. For example, GPT-4 generated code has an average \textbf{3.12} times execution time that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 generated code are \textbf{13.89} and \textbf{43.92} times that of the canonical solutions. The source code of EffiBench is released on this https URL. We also provide the LeaderBoard at this https URL.

------------

--------------
Accelerate (9)
--------------

`[2406.04496] Time Sensitive Knowledge Editing through Efficient Finetuning <https://arxiv.org/abs/2406.04496>`__ 基于高效微调的时间敏感知识编辑

::

    Thu, 6 Jun 2024 20:41:36 GMT
    Xiou Ge, Ali Mousavi, Edouard Grave, Armand Joulin, Kun Qian, Benjamin Han, Mostafa Arefiyan, Yunyao Li

Large Language Models (LLMs) have demonstrated impressive capability in different tasks and are bringing transformative changes to many domains.
However, keeping the knowledge in LLMs up-to-date remains a challenge once pretraining is complete. It is thus essential to design effective methods to both update obsolete knowledge and induce new knowledge into LLMs. Existing locate-and-edit knowledge editing (KE) method suffers from two limitations.
First, the post-edit LLMs by such methods generally have poor capability in answering complex queries that require multi-hop reasoning. Second, the long run-time of such locate-and-edit methods to perform knowledge edits make it infeasible for large scale KE in practice. In this paper, we explore Parameter-Efficient Fine-Tuning (PEFT) techniques as an alternative for KE. We curate a more comprehensive temporal KE dataset with both knowledge update and knowledge injection examples for KE performance benchmarking. We further probe the effect of fine-tuning on a range of layers in an LLM for the multi-hop QA task. We find that PEFT performs better than locate-and-edit techniques for time-sensitive knowledge edits.

------------

`[2406.04879] A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques <https://arxiv.org/abs/2406.04879>`__ 深入探讨参数高效偏好对齐技术的权衡

::

    Fri, 7 Jun 2024 12:25:51 GMT
    Megh Thakkar, Quentin Fournier, Matthew D Riemer, Pin-Yu Chen, Amal Zouaq, Payel Das, Sarath Chandar

Large language models are first pre-trained on trillions of tokens and then instruction-tuned or aligned to specific preferences. While pre-training remains out of reach for most researchers due to the compute required, fine-tuning has become affordable thanks to parameter-efficient methods such as LoRA and QLoRA. Alignment is known to be sensitive to the many factors involved, including the quantity and quality of data, the alignment method, and the adapter rank. However, there has not yet been an extensive study of their effect on downstream performance. To address this gap, we conduct an in-depth investigation of the impact of popular choices for three crucial axes: (i) the alignment dataset (HH-RLHF and BeaverTails), (ii) the alignment technique (SFT and DPO), and (iii) the model (LLaMA-1, Vicuna-v1.3, Mistral-7b, and Mistral-7b-Instruct). Our extensive setup spanning over 300 experiments reveals consistent trends and unexpected findings. We observe how more informative data helps with preference alignment, cases where supervised fine-tuning outperforms preference optimization, and how aligning to a distinct preference boosts performance on downstream tasks. Through our in-depth analyses, we put forward key guidelines to help researchers perform more effective parameter-efficient LLM alignment.

------------

`[2406.04984] MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter <https://arxiv.org/abs/2406.04984>`__ MEFT:基于稀疏适配器的内存高效微调

::

    Fri, 7 Jun 2024 14:49:22 GMT
    Jitai Hao, WeiWei Sun, Xin Xin, Qi Meng, Zhumin Chen, Pengjie Ren, Zhaochun Ren

Parameter-Efficient Fine-tuning (PEFT) facilitates the fine-tuning of Large Language Models (LLMs) under limited resources. However, the fine-tuning performance with PEFT on complex, knowledge-intensive tasks is limited due to the constrained model capacity, which originates from the limited number of additional trainable parameters. To overcome this limitation, we introduce a novel mechanism that fine-tunes LLMs with adapters of larger size yet memory-efficient. This is achieved by leveraging the inherent activation sparsity in the Feed-Forward Networks (FFNs) of LLMs and utilizing the larger capacity of Central Processing Unit (CPU) memory compared to Graphics Processing Unit (GPU). We store and update the parameters of larger adapters on the CPU. Moreover, we employ a Mixture of Experts (MoE)-like architecture to mitigate unnecessary CPU computations and reduce the communication volume between the GPU and CPU. This is particularly beneficial over the limited bandwidth of PCI Express (PCIe). Our method can achieve fine-tuning results comparable to those obtained with larger memory capacities, even when operating under more limited resources such as a 24GB memory single GPU setup, with acceptable loss in training efficiency. Our codes are available at https://github.com/CURRENTF/MEFT.

------------

`[2406.05130] An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models <https://arxiv.org/abs/2406.05130>`__ 多模态大型语言模型参数高效微调的实证研究

::

    Fri, 7 Jun 2024 17:58:11 GMT
    Xiongtao Zhou, Jie He, Yuhua Ke, Guangyao Zhu, V\'ictor Guti\'errez-Basulto, Jeff Z. Pan

Multimodal large language models (MLLMs) fine-tuned with multimodal instruction datasets have demonstrated remarkable capabilities in multimodal tasks. However, fine-tuning all parameters of MLLMs has become challenging as they usually contain billions of parameters. To address this issue, we study parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify effective methods for enhancing the performance of MLLMs in scenarios where only a limited number of parameters are trained. This paper conducts empirical studies using four popular PEFT methods to fine-tune the LLM component of open-source MLLMs. We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of the PEFT module, size of fine-tuning data, model stability based on PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT methods on seven datasets from two different categories: unseen and seen datasets. Across all experiments, we show that the adapter is the best-performing PEFT method. At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs. Code and data are available at https://github.com/alenai97/PEFT-MLLM.git.

------------

`[2310.07177] Online Speculative Decoding <https://arxiv.org/abs/2310.07177>`__ 在线推测解码

::

    replaced with revised version Fri, 7 Jun 2024 00:14:47 GMT
    Submission history From: Xiaoxuan Liu [view email]
    [v1] Wed, 11 Oct 2023 04:03:42 UTC (908 KB)
    [v2] Tue, 17 Oct 2023 18:02:19 UTC (908 KB)
    [v3] Fri, 7 Jun 2024 00:14:47 UTC (2,243 KB)
    Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, Hao Zhang

Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding to address this challenge. The main idea is to continuously update the (multiple) draft model(s) on observed user query data. Adapting to query distribution mitigates the shifts between the training distribution of the draft model and the query distribution, enabling the draft model to more accurately predict the target model's outputs. We develop a prototype of online speculative decoding based on knowledge distillation and evaluate it using both synthetic and real query data. The results show a substantial increase in the token acceptance rate by 0.1 to 0.65, bringing 1.42x to 2.17x latency reduction. Our code is available at this https URL.

------------

`[2404.19124] Accelerating Production LLMs with Combined Token/Embedding Speculators <https://arxiv.org/abs/2404.19124>`__ 与代币/嵌入投机者共同加速生产llm

::

    replaced with revised version Thu, 6 Jun 2024 18:38:34 GMT
    Submission history From: Davis Wertheimer [view email]
    [v1] Mon, 29 Apr 2024 21:59:07 UTC (2,397 KB)
    [v2] Thu, 6 Jun 2024 18:38:34 UTC (2,398 KB)
    Davis Wertheimer, Joshua Rosenkranz, Thomas Parnell, Sahil Suneja, Pavithra Ranganathan, Raghu Ganti, Mudhakar Srivatsa

This technical report describes the design and training of novel speculative decoding draft models, for accelerating the inference speeds of large language models in a production environment. By conditioning draft predictions on both context vectors and sampled tokens, we can train our speculators to efficiently predict high-quality n-grams, which the base model then accepts or rejects. This allows us to effectively predict multiple tokens per inference forward pass, accelerating wall-clock inference speeds of highly optimized base model implementations by a factor of 2-3x. We explore these initial results and describe next steps for further improvements.

------------

`[2402.08983] SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding <https://arxiv.org/abs/2402.08983>`__ SafeDecoding:通过安全感知的解码防御越狱攻击

::

    replaced with revised version Fri, 7 Jun 2024 17:25:04 GMT
    Submission history From: Luyao Niu [view email]
    [v1] Wed, 14 Feb 2024 06:54:31 UTC (9,606 KB)
    [v2] Sat, 24 Feb 2024 07:17:16 UTC (9,634 KB)
    [v3] Fri, 7 Jun 2024 17:25:04 UTC (9,635 KB)
    Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, Radha Poovendran

As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their token probabilities, while simultaneously attenuating the probabilities of token sequences that are aligned with the objectives of jailbreak attacks. We perform extensive experiments on five LLMs using six state-of-the-art jailbreak attacks and four benchmark datasets. Our results show that SafeDecoding significantly reduces the attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries. SafeDecoding outperforms six defense methods.

------------

`[2405.00218] Constrained Decoding for Secure Code Generation <https://arxiv.org/abs/2405.00218>`__ 基于约束解码的安全代码生成

::

    replaced with revised version Fri, 7 Jun 2024 06:47:15 GMT
    Submission history From: Yanjun Fu [view email]
    [v1] Tue, 30 Apr 2024 21:52:19 UTC (90 KB)
    [v2] Fri, 7 Jun 2024 06:47:15 UTC (248 KB)
    Yanjun Fu, Ethan Baker, Yu Ding, Yizheng Chen

Code Large Language Models (Code LLMs) have been increasingly used by developers to boost productivity, but they often generate vulnerable code. Thus, there is an urgent need to ensure that code generated by Code LLMs is correct and secure. Previous research has primarily focused on generating secure code, overlooking the fact that secure code also needs to be correct. This oversight can lead to a false sense of security. Currently, the community lacks a method to measure actual progress in this area, and we need solutions that address both security and correctness of code generation.
This paper introduces a new benchmark, CodeGuard+, along with two new metrics, to measure Code LLMs' ability to generate both secure and correct code. Using our new evaluation methods, we show that the state-of-the-art defense technique, prefix tuning, may not be as strong as previously believed, since it generates secure code but sacrifices functional correctness. We also demonstrate that different decoding methods significantly affect the security of Code LLMs.
Furthermore, we explore a new defense direction: constrained decoding for secure code generation. We propose new constrained decoding techniques to generate secure code. Our results reveal that constrained decoding is more effective than prefix tuning to improve the security of Code LLMs, without requiring a specialized training dataset. Moreover, our evaluations over eight state-of-the-art Code LLMs show that constrained decoding has strong performance to improve the security of Code LLMs, and our technique outperforms GPT-4.

------------

`[2312.15698] RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair <https://arxiv.org/abs/2312.15698>`__ RepairLLaMA:面向程序修复的高效表示和微调适配器

::

    replaced with revised version Fri, 7 Jun 2024 13:21:26 GMT
    Submission history From: André Silva [view email]
    [v1] Mon, 25 Dec 2023 11:39:46 UTC (307 KB)
    [v2] Mon, 26 Feb 2024 12:03:24 UTC (308 KB)
    [v3] Mon, 11 Mar 2024 08:31:19 UTC (309 KB)
    [v4] Fri, 7 Jun 2024 13:21:26 UTC (327 KB)
    Andr\'e Silva, Sen Fang, Martin Monperrus

Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tune LLMs with naive code representations and does not scale to frontier models. To address this problem, we propose RepairLLaMA, a novel program repair approach that 1) identifies optimal code representations for APR with fine-tuned models, and 2) pioneers state-of-the-art parameter-efficient fine-tuning technique (PEFT) for program repair. This results in RepairLLaMA producing a highly effective `program repair adapter' for fixing bugs with AI. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals and produce better patches. Second, parameter-efficient fine-tuning helps fine-tuning to converge and clearly contributes to the effectiveness of RepairLLaMA in fixing bugs outside the fine-tuning data distribution. Overall, RepairLLaMA correctly fixes 144 Defects4J v2 and 109 HumanEval-Java bugs, outperforming all baselines.

------------

-----------------------
In-Context Learning (2)
-----------------------

`[2401.06766] Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements <https://arxiv.org/abs/2401.06766>`__ 

::

    replaced with revised version Thu, 6 Jun 2024 19:01:37 GMT
    Submission history From: Anton Voronov [view email]
    [v1] Fri, 12 Jan 2024 18:58:26 UTC (1,282 KB)
    [v2] Mon, 22 Jan 2024 18:55:35 UTC (1,283 KB)
    [v3] Thu, 6 Jun 2024 19:01:37 UTC (2,328 KB)
    Anton Voronov, Lena Wolf, Max Ryabinin

Large language models demonstrate a remarkable capability for learning to solve new tasks from a few examples. The prompt template, or the way the input examples are formatted to obtain the prompt, is an important yet often overlooked aspect of in-context learning. In this work, we conduct a comprehensive study of the template format's influence on the in-context learning performance. We evaluate the impact of the prompt template across 21 models (from 770M to 70B parameters) and 4 standard classification datasets. We show that a poor choice of the template can reduce the performance of the strongest models and inference methods to a random guess level. More importantly, the best templates do not transfer between different setups and even between models of the same family. Our findings show that the currently prevalent approach to evaluation, which ignores template selection, may give misleading results due to different templates in different works. As a first step towards mitigating this issue, we propose Template Ensembles that aggregate model predictions across several templates. This simple test-time augmentation boosts average performance while being robust to the choice of random set of templates.

------------

`[2402.12976] The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis <https://arxiv.org/abs/2402.12976>`__ 演示对多语言语境学习的影响:多维分析

::

    replaced with revised version Fri, 7 Jun 2024 13:44:07 GMT
    Submission history From: Miaoran Zhang [view email]
    [v1] Tue, 20 Feb 2024 12:53:31 UTC (3,025 KB)
    [v2] Fri, 7 Jun 2024 13:44:07 UTC (2,945 KB)
    Miaoran Zhang, Vagrant Gautam, Mingyang Wang, Jesujoba O. Alabi, Xiaoyu Shen, Dietrich Klakow, Marius Mosbach

In-context learning is a popular inference strategy where large language models solve a task using only a few labeled demonstrations without needing any parameter updates. Although there have been extensive studies on English in-context learning, multilingual in-context learning remains under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that strong instruction-following models including Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.

------------

-------------
Reasoning (7)
-------------

`[2406.04800] Zero, Finite, and Infinite Belief History of Theory of Mind Reasoning in Large Language Models <https://arxiv.org/abs/2406.04800>`__ 大型语言模型中心智推理理论的零、有限和无限信仰历史

::

    Fri, 7 Jun 2024 10:04:39 GMT
    Weizhi Tang, Vaishak Belle

Large Language Models (LLMs) have recently shown a promise and emergence of Theory of Mind (ToM) ability and even outperform humans in certain ToM tasks.
To evaluate and extend the boundaries of the ToM reasoning ability of LLMs, we propose a novel concept, taxonomy, and framework, the ToM reasoning with Zero, Finite, and Infinite Belief History and develop a multi-round text-based game, called $\textit{Pick the Right Stuff}$, as a benchmark. We have evaluated six LLMs with this game and found their performance on Zero Belief History is consistently better than on Finite Belief History. In addition, we have found two of the models with small parameter sizes outperform all the evaluated models with large parameter sizes. We expect this work to pave the way for future ToM benchmark development and also for the promotion and development of more complex AI agents or systems which are required to be equipped with more complex ToM reasoning ability.

------------

`[2406.05055] Robustness Assessment of Mathematical Reasoning in the Presence of Missing and Contradictory Conditions <https://arxiv.org/abs/2406.05055>`__ 缺失和矛盾条件下数学推理的鲁棒性评估

::

    Fri, 7 Jun 2024 16:24:12 GMT
    Shi-Yu Tian, Zhi Zhou, Lin-Han Jia, Lan-Zhe Guo, Yu-Feng Li

Large language models (LLMs) have demonstrated impressive performance on reasoning tasks, which can be further improved through few-shot prompting techniques. However, the current evaluation primarily focuses on carefully constructed benchmarks and neglects the consideration of real-world reasoning problems that present missing and contradictory conditions, known as ill-defined problems. Our observations suggest that existing few-shot prompting techniques are ineffective in such scenarios, often providing overconfident answers or hallucination. To further study this problem, we develop a benchmark called Problems with Missing and Contradictory conditions (PMC) and introduce two novel metrics to evaluate the performance of few-shot prompting methods in these scenarios. Our analysis using the PMC benchmark reveals a trade-off dilemma between the performance of mathematical reasoning for well-defined problems and the ability to recognize ill-defined problems. To address the challenges posed by PMC, we propose a novel few-shot prompting method called SMT-LIB Prompting (SLP), which utilizes the SMT-LIB language to model the problems instead of solving them directly. Subsequently, a double-check solving strategy checks the satisfiability and uniqueness of the solution and provides final feedback. Extensive experiments demonstrate the superiority of our SLP approach compared to existing few-shot prompting methods when dealing with problems with missing and contradictory conditions. We will open-source our benchmark and code to facilitate future research.

------------

`[2406.04566] SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models <https://arxiv.org/abs/2406.04566>`__ 

::

    Fri, 7 Jun 2024 01:06:34 GMT
    Md Imbesat Hassan Rizvi, Xiaodan Zhu, Iryna Gurevych

Spatial reasoning is a crucial component of both biological and artificial intelligence. In this work, we present a comprehensive study of the capability of current state-of-the-art large language models (LLMs) on spatial reasoning.
To support our study, we created and contribute a novel Spatial Reasoning Characterization (SpaRC) framework and Spatial Reasoning Paths (SpaRP) datasets, to enable an in-depth understanding of the spatial relations and compositions as well as the usefulness of spatial reasoning chains. We found that all the state-of-the-art LLMs do not perform well on the datasets -- their performances are consistently low across different setups. The spatial reasoning capability improves substantially as model sizes scale up. Finetuning both large language models (e.g., Llama-2-70B) and smaller ones (e.g., Llama-2-13B) can significantly improve their F1-scores by 7--32 absolute points. We also found that the top proprietary LLMs still significantly outperform their open-source counterparts in topological spatial understanding and reasoning.

------------

`[2406.04464] On The Importance of Reasoning for Context Retrieval in Repository-Level Code Editing <https://arxiv.org/abs/2406.04464>`__ 库级代码编辑中上下文检索推理的重要性

::

    Thu, 6 Jun 2024 19:44:17 GMT
    Alexander Kovrigin, Aleksandra Eliseeva, Yaroslav Zharov, Timofey Bryksin

Recent advancements in code-fluent Large Language Models (LLMs) enabled the research on repository-level code editing. In such tasks, the model navigates and modifies the entire codebase of a project according to request. Hence, such tasks require efficient context retrieval, i.e., navigating vast codebases to gather relevant context. Despite the recognized importance of context retrieval, existing studies tend to approach repository-level coding tasks in an end-to-end manner, rendering the impact of individual components within these complicated systems unclear. In this work, we decouple the task of context retrieval from the other components of the repository-level code editing pipelines. We lay the groundwork to define the strengths and weaknesses of this component and the role that reasoning plays in it by conducting experiments that focus solely on context retrieval. We conclude that while the reasoning helps to improve the precision of the gathered context, it still lacks the ability to identify its sufficiency. We also outline the ultimate role of the specialized tools in the process of context gathering. The code supplementing this paper is available at https://github.com/JetBrains-Research/ai-agents-code-editing.

------------

`[2406.04615] What do MLLMs hear? Examining reasoning with text and sound components in Multimodal Large Language Models <https://arxiv.org/abs/2406.04615>`__ mllm听到了什么?多模态大型语言模型中使用文本和声音组件进行推理

::

    Fri, 7 Jun 2024 03:55:00 GMT
    Enis Berk \c{C}oban, Michael I. Mandel, Johanna Devaney

Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, notably in connecting ideas and adhering to logical rules to solve problems. These models have evolved to accommodate various data modalities, including sound and images, known as multimodal LLMs (MLLMs), which are capable of describing images or sound recordings. Previous work has demonstrated that when the LLM component in MLLMs is frozen, the audio or visual encoder serves to caption the sound or image input facilitating text-based reasoning with the LLM component. We are interested in using the LLM's reasoning capabilities in order to facilitate classification. In this paper, we demonstrate through a captioning/classification experiment that an audio MLLM cannot fully leverage its LLM's text-based reasoning when generating audio captions. We also consider how this may be due to MLLMs separately representing auditory and textual information such that it severs the reasoning pathway from the LLM to the audio encoder.

------------

`[2402.14382] Chain-of-History Reasoning for Temporal Knowledge Graph Forecasting <https://arxiv.org/abs/2402.14382>`__ 基于历史链推理的时序知识图谱预测

::

    replaced with revised version Fri, 7 Jun 2024 08:15:18 GMT
    Submission history From: Yuwei Xia [view email]
    [v1] Thu, 22 Feb 2024 08:51:39 UTC (6,308 KB)
    [v2] Fri, 7 Jun 2024 08:15:18 UTC (6,311 KB)
    Yuwei Xia, Ding Wang, Qiang Liu, Liang Wang, Shu Wu, Xiaoyu Zhang

Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based on given histories. Most recent graph-based models excel at capturing structural information within TKGs but lack semantic comprehension abilities. Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has emerged. However, the existing LLM-based model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for LLMs being extremely limited. (2) LLMs struggle with optimal reasoning performance under heavy historical information loads. (3) For TKG prediction, the temporal reasoning capability of LLM alone is limited. To address the first two challenges, we propose Chain-of-History (CoH) reasoning which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for LLMs on TKG prediction. To address the third issue, we design CoH as a plug-and-play module to enhance the performance of graph-based models for TKG prediction. Extensive experiments on three datasets and backbones demonstrate the effectiveness of CoH.

------------

`[2403.20046] Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning <https://arxiv.org/abs/2403.20046>`__ LLMs能从以前的错误中学习吗?调查llm的错误以促进推理

::

    replaced with revised version Fri, 7 Jun 2024 06:27:50 GMT
    Submission history From: Yongqi Tong [view email]
    [v1] Fri, 29 Mar 2024 08:30:34 UTC (8,378 KB)
    [v2] Fri, 7 Jun 2024 06:27:50 UTC (8,263 KB)
    Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang

Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \textbf{Mistake tuning} involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome. \textsc{CoTErrorSet} will be published soon on \texttt{\url{this https URL}}.

------------

-----------------------
Retrieval-Augmented (4)
-----------------------

`[2406.04744] CRAG -- Comprehensive RAG Benchmark <https://arxiv.org/abs/2406.04744>`__ RAG——综合RAG基准

::

    Fri, 7 Jun 2024 08:43:07 GMT
    Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, Xin Luna Dong

Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve <=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.

------------

`[2406.05085] Multi-Head RAG: Solving Multi-Aspect Problems with LLMs <https://arxiv.org/abs/2406.05085>`__ 多头RAG:用llm解决多方面问题

::

    Fri, 7 Jun 2024 16:59:38 GMT
    Maciej Besta, Ales Kubicek, Roman Niggli, Robert Gerstenberger, Lucas Weitzendorf, Mingyuan Chi, Patrick Iff, Joanna Gajda, Piotr Nyczyk, J\"urgen M\"uller, Hubert Niewiadomski, Marcin Chrapek, Micha{\l} Podstawski, Torsten Hoefler

Retrieval Augmented Generation (RAG) enhances the abilities of Large Language Models (LLMs) by enabling the retrieval of documents into the LLM context to provide more accurate and relevant responses. Existing RAG solutions do not focus on queries that may require fetching multiple documents with substantially different contents. Such queries occur frequently, but are challenging because the embeddings of these documents may be distant in the embedding space, making it hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a simple yet powerful idea: leveraging activations of Transformer's multi-head attention layer, instead of the decoder layer, as keys for fetching multi-aspect documents. The driving motivation is that different attention heads can learn to capture different data aspects. Harnessing the corresponding activations results in embeddings that represent various facets of data items and queries, improving the retrieval accuracy for complex queries. We provide an evaluation methodology and metrics, synthetic datasets, and real-world use cases to demonstrate MRAG's effectiveness, showing improvements of up to 20% in relevance over standard RAG baselines. MRAG can be seamlessly integrated with existing RAG frameworks and benchmarking tools like RAGAS as well as different classes of data stores.

------------

`[2406.04369] RAG Does Not Work for Enterprises <https://arxiv.org/abs/2406.04369>`__ RAG不为企业工作

::

    Fri, 31 May 2024 23:30:52 GMT
    Tilmann Bruckhaus (Strative.ai)

Retrieval-Augmented Generation (RAG) improves the accuracy and relevance of large language model outputs by incorporating knowledge retrieval. However, implementing RAG in enterprises poses challenges around data security, accuracy, scalability, and integration. This paper explores the unique requirements for enterprise RAG, surveys current approaches and limitations, and discusses potential advances in semantic search, hybrid queries, and optimized retrieval. It proposes an evaluation framework to validate enterprise RAG solutions, including quantitative testing, qualitative analysis, ablation studies, and industry case studies. This framework aims to help demonstrate the ability of purpose-built RAG architectures to deliver accuracy and relevance improvements with enterprise-grade security, compliance and integration. The paper concludes with implications for enterprise deployments, limitations, and future research directions. Close collaboration between researchers and industry partners may accelerate progress in developing and deploying retrieval-augmented generation technology.

------------

`[2406.04464] On The Importance of Reasoning for Context Retrieval in Repository-Level Code Editing <https://arxiv.org/abs/2406.04464>`__ 库级代码编辑中上下文检索推理的重要性

::

    Thu, 6 Jun 2024 19:44:17 GMT
    Alexander Kovrigin, Aleksandra Eliseeva, Yaroslav Zharov, Timofey Bryksin

Recent advancements in code-fluent Large Language Models (LLMs) enabled the research on repository-level code editing. In such tasks, the model navigates and modifies the entire codebase of a project according to request. Hence, such tasks require efficient context retrieval, i.e., navigating vast codebases to gather relevant context. Despite the recognized importance of context retrieval, existing studies tend to approach repository-level coding tasks in an end-to-end manner, rendering the impact of individual components within these complicated systems unclear. In this work, we decouple the task of context retrieval from the other components of the repository-level code editing pipelines. We lay the groundwork to define the strengths and weaknesses of this component and the role that reasoning plays in it by conducting experiments that focus solely on context retrieval. We conclude that while the reasoning helps to improve the precision of the gathered context, it still lacks the ability to identify its sufficiency. We also outline the ultimate role of the specialized tools in the process of context gathering. The code supplementing this paper is available at https://github.com/JetBrains-Research/ai-agents-code-editing.

------------

---------
Agent (9)
---------

`[2406.04692] Mixture-of-Agents Enhances Large Language Model Capabilities <https://arxiv.org/abs/2406.04692>`__ agent混合增强大型语言模型的能力

::

    Fri, 7 Jun 2024 07:04:10 GMT
    Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, James Zou

Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.

------------

`[2406.04784] SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals <https://arxiv.org/abs/2406.04784>`__ 

::

    Fri, 7 Jun 2024 09:32:03 GMT
    Ruihan Yang, Jiangjie Chen, Yikai Zhang, Siyu Yuan, Aili Chen, Kyle Richardson, Yanghua Xiao, Deqing Yang

Language agents powered by large language models (LLMs) are increasingly valuable as decision-making tools in domains such as gaming and programming.
However, these agents often face challenges in achieving high-level goals without detailed instructions and in adapting to environments where feedback is delayed. In this paper, we present SelfGoal, a novel automatic approach designed to enhance agents' capabilities to achieve high-level goals with limited human prior and environmental feedback. The core concept of SelfGoal involves adaptively breaking down a high-level goal into a tree structure of more practical subgoals during the interaction with environments while identifying the most useful subgoals and progressively updating this structure.
Experimental results demonstrate that SelfGoal significantly enhances the performance of language agents across various tasks, including competitive, cooperative, and deferred feedback environments. Project page: https://selfgoal-agent.github.io.

------------

`[2310.17512] CompeteAI: Understanding the Competition Dynamics in Large Language Model-based Agents <https://arxiv.org/abs/2310.17512>`__ CompeteAI:理解基于语言模型的大型智能体中的竞争动态

::

    replaced with revised version Fri, 7 Jun 2024 09:13:27 GMT
    Submission history From: Jindong Wang [view email]
    [v1] Thu, 26 Oct 2023 16:06:20 UTC (10,075 KB)
    [v2] Fri, 7 Jun 2024 09:13:27 UTC (12,895 KB)
    Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, Xing Xie

Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most of the work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that promotes the development of society and economy. In this paper, we seek to examine the competition dynamics in LLM-based agents. We first propose a general framework for studying the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, restaurant agents and customer agents. Specifically, the restaurant agents compete with each other to attract more customers, where competition encourages them to transform, such as cultivating new operating strategies. Simulation experiments reveal several interesting findings at the micro and macro levels, which align well with existing market and sociological theories. We hope that the framework and environment can be a promising testbed to study competition that fosters understanding of society. Code is available at: this https URL.

------------

`[2402.17574] Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization <https://arxiv.org/abs/2402.17574>`__ Agent-Pro:通过策略级反射和优化学习演进

::

    replaced with revised version Thu, 6 Jun 2024 18:40:47 GMT
    Submission history From: Wenqi Zhang [view email]
    [v1] Tue, 27 Feb 2024 15:09:20 UTC (2,597 KB)
    [v2] Wed, 27 Mar 2024 17:34:57 UTC (2,598 KB)
    [v3] Thu, 6 Jun 2024 18:40:47 UTC (2,601 KB)
    Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu

Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.

------------

`[2309.11436] You Only Look at Screens: Multimodal Chain-of-Action Agents <https://arxiv.org/abs/2309.11436>`__ 你只看屏幕:多模态动作链智能体

::

    replaced with revised version Fri, 7 Jun 2024 04:52:29 GMT
    Submission history From: Zhuosheng Zhang [view email]
    [v1] Wed, 20 Sep 2023 16:12:32 UTC (5,276 KB)
    [v2] Thu, 21 Sep 2023 03:00:07 UTC (5,276 KB)
    [v3] Mon, 20 May 2024 06:40:51 UTC (5,997 KB)
    [v4] Fri, 7 Jun 2024 04:52:29 UTC (5,997 KB)
    Zhuosheng Zhang, Aston Zhang

Autonomous graphical user interface (GUI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, most existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-GUI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -- leveraging a series of intermediate previous action histories and future action plans -- to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30$K$ unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-GUI achieves state-of-the-art performance with an action type prediction accuracy of 90\% and an overall action success rate of 74\%. Code is publicly available at this https URL.

------------

`[2310.17976] InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews <https://arxiv.org/abs/2310.17976>`__ 角色:通过心理访谈评估角色扮演代理人的人格忠实度

::

    replaced with revised version Fri, 7 Jun 2024 12:24:53 GMT
    Submission history From: Xintao Wang [view email]
    [v1] Fri, 27 Oct 2023 08:42:18 UTC (1,770 KB)
    [v2] Mon, 30 Oct 2023 03:13:15 UTC (1,770 KB)
    [v3] Sat, 17 Feb 2024 07:23:11 UTC (9,874 KB)
    [v4] Fri, 7 Jun 2024 12:24:53 UTC (10,387 KB)
    Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, Yanghua Xiao

Role-playing agents (RPAs), powered by large language models, have emerged as a flourishing field of applications. However, a key challenge lies in assessing whether RPAs accurately reproduce the personas of target characters, namely their character fidelity. Existing methods mainly focus on the knowledge and linguistic patterns of characters. This paper, instead, introduces a novel perspective to evaluate the personality fidelity of RPAs with psychological scales. Overcoming drawbacks of previous self-report assessments on RPAs, we propose InCharacter, namely Interviewing Character agents for personality tests. Experiments include various types of RPAs and LLMs, covering 32 distinct characters on 14 widely used psychological scales. The results validate the effectiveness of InCharacter in measuring RPA personalities. Then, with InCharacter, we show that state-of-the-art RPAs exhibit personalities highly aligned with the human-perceived personalities of the characters, achieving an accuracy up to 80.7%.

------------

`[2401.13919] WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models <https://arxiv.org/abs/2401.13919>`__ WebVoyager:构建具有大型多模态模型的端到端Web代理

::

    replaced with revised version Thu, 6 Jun 2024 18:37:34 GMT
    Submission history From: Hongliang He [view email]
    [v1] Thu, 25 Jan 2024 03:33:18 UTC (18,186 KB)
    [v2] Sun, 28 Jan 2024 07:57:21 UTC (18,186 KB)
    [v3] Thu, 29 Feb 2024 12:07:42 UTC (19,153 KB)
    [v4] Thu, 6 Jun 2024 18:37:34 UTC (20,855 KB)
    Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu

The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1% task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents.

------------

`[2402.01030] Executable Code Actions Elicit Better LLM Agents <https://arxiv.org/abs/2402.01030>`__ 可执行的代码动作可以引出更好的LLM代理

::

    replaced with revised version Fri, 7 Jun 2024 01:53:07 GMT
    Submission history From: Xingyao Wang [view email]
    [v1] Thu, 1 Feb 2024 21:38:58 UTC (8,761 KB)
    [v2] Mon, 18 Mar 2024 15:18:45 UTC (8,780 KB)
    [v3] Fri, 24 May 2024 01:05:14 UTC (8,778 KB)
    [v4] Fri, 7 Jun 2024 01:53:07 UTC (8,789 KB)
    Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji

Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.

------------

`[2402.14865] Dynamic Evaluation of Large Language Models by Meta Probing Agents <https://arxiv.org/abs/2402.14865>`__ 基于元探测agent的大型语言模型动态评估

::

    replaced with revised version Fri, 7 Jun 2024 09:19:45 GMT
    Submission history From: Jindong Wang [view email]
    [v1] Wed, 21 Feb 2024 06:46:34 UTC (323 KB)
    [v2] Fri, 7 Jun 2024 09:19:45 UTC (338 KB)
    Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, Xing Xie

Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are also dynamically configurable, allowing multifaceted analysis. We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement. Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Matthew effect on model size, i.e., larger models possess stronger correlations of the abilities. MPA can also be used as a data augmentation approach to enhance LLMs. Code is available at: this https URL.

------------

-----------
Other (106)
-----------

`[2406.04481] Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF <https://arxiv.org/abs/2406.04481>`__ 为安全优化自动驾驶:以llm增强的RLHF为中心的方法

::

    Thu, 6 Jun 2024 20:10:34 GMT
    Yuan Sun, Navid Salami Pargoo, Peter J. Jin, Jorge Ortiz

Reinforcement Learning from Human Feedback (RLHF) is popular in large language models (LLMs), whereas traditional Reinforcement Learning (RL) often falls short. Current autonomous driving methods typically utilize either human feedback in machine learning, including RL, or LLMs. Most feedback guides the car agent's learning process (e.g., controlling the car). RLHF is usually applied in the fine-tuning step, requiring direct human "preferences," which are not commonly used in optimizing autonomous driving models. In this research, we innovatively combine RLHF and LLMs to enhance autonomous driving safety. Training a model with human guidance from scratch is inefficient. Our framework starts with a pre-trained autonomous car agent model and implements multiple human-controlled agents, such as cars and pedestrians, to simulate real-life road environments. The autonomous car model is not directly controlled by humans. We integrate both physical and physiological feedback to fine-tune the model, optimizing this process using LLMs. This multi-agent interactive environment ensures safe, realistic interactions before real-world application. Finally, we will validate our model using data gathered from real-life testbeds located in New Jersey and New York City.

------------

`[2406.04370] Large Language Model Confidence Estimation via Black-Box Access <https://arxiv.org/abs/2406.04370>`__ 基于黑盒访问的大型语言模型置信度估计

::

    Sat, 1 Jun 2024 02:08:44 GMT
    Tejaswini Pedapati, Amit Dhurandhar, Soumya Ghosh, Soham Dan and Prasanna Sattigeri

Estimating uncertainty or confidence in the responses of a model can be significant in evaluating trust not only in the responses, but also in the model as a whole. In this paper, we explore the problem of estimating confidence for responses of large language models (LLMs) with simply black-box or query access to them. We propose a simple and extensible framework where, we engineer novel features and train a (interpretable) model (viz. logistic regression) on these features to estimate the confidence. We empirically demonstrate that our simple framework is effective in estimating confidence of flan-ul2, llama-13b and mistral-7b with it consistently outperforming existing black-box confidence estimation approaches on benchmark datasets such as TriviaQA, SQuAD, CoQA and Natural Questions by even over $10\%$ (on AUROC) in some cases. Additionally, our interpretable approach provides insight into features that are predictive of confidence, leading to the interesting and useful discovery that our confidence models built for one LLM generalize zero-shot across others on a given dataset.

------------

`[2406.04371] Phased Instruction Fine-Tuning for Large Language Models <https://arxiv.org/abs/2406.04371>`__ 

::

    Sat, 1 Jun 2024 04:25:26 GMT
    Wei Pang and Chuan Zhou and Xiao-Hua Zhou and Xiaojie Wang

Instruction Fine-Tuning, a method enhancing pre-trained language models' capabilities from mere next-word prediction to complex instruction following, often employs a one-off training approach on diverse instruction dataset.
However, this method may not effectively enhance models' adherence to instructions due to the simultaneous handling of varying instruction complexities. To address this, we propose a novel phased instruction fine-tuning (Phased IFT) method, grounded in the hypothesis of progressive alignment, which posits that the transition of a pre-trained language model from simple next-word prediction to sophisticated instruction following is a gradual learning process. Specifically, we obtain the score of difficulty for each instruction via GPT-4, stratify the instruction data into subsets of increasing difficulty, and sequentially uptrain on these subsets using the standard supervised loss. Through extensive experiments on the pre-trained models Llama-2 7B/13B, and Mistral-7B using the 52K Alpaca instruction data, we demonstrate that Phased IFT significantly surpasses traditional one-off instruction fine-tuning (One-off IFT) method in win rate, empirically validating the progressive alignment hypothesis. Our findings suggest that Phased IFT offers a simple yet effective pathway for elevating the instruction-following capabilities of pre-trained language models. Models and datasets from our experiments are freely available at https://github.com/xubuvd/PhasedSFT.

------------

`[2406.04383] Exploring the Latest LLMs for Leaderboard Extraction <https://arxiv.org/abs/2406.04383>`__ 探索最新的llm排行榜提取

::

    Thu, 6 Jun 2024 05:54:45 GMT
    Salomon Kabongo, Jennifer D'Souza, and S\"oren Auer

The rapid advancements in Large Language Models (LLMs) have opened new avenues for automating complex tasks in AI research. This paper investigates the efficacy of different LLMs-Mistral 7B, Llama-2, GPT-4-Turbo and GPT-4.o in extracting leaderboard information from empirical AI research articles. We explore three types of contextual inputs to the models: DocTAET (Document Title, Abstract, Experimental Setup, and Tabular Information), DocREC (Results, Experiments, and Conclusions), and DocFULL (entire document). Our comprehensive study evaluates the performance of these models in generating (Task, Dataset, Metric, Score) quadruples from research papers. The findings reveal significant insights into the strengths and limitations of each model and context type, providing valuable guidance for future AI research automation efforts.

------------

`[2406.04428] MoralBench: Moral Evaluation of LLMs <https://arxiv.org/abs/2406.04428>`__ Moral bench: llm的道德评估

::

    Thu, 6 Jun 2024 18:15:01 GMT
    Jianchao Ji, Yutong Chen, Mingyu Jin, Wujiang Xu, Wenyue Hua, Yongfeng Zhang

In the rapidly evolving field of artificial intelligence, large language models (LLMs) have emerged as powerful tools for a myriad of applications, from natural language processing to decision-making support systems. However, as these models become increasingly integrated into societal frameworks, the imperative to ensure they operate within ethical and moral boundaries has never been more critical. This paper introduces a novel benchmark designed to measure and compare the moral reasoning capabilities of LLMs. We present the first comprehensive dataset specifically curated to probe the moral dimensions of LLM outputs, addressing a wide range of ethical dilemmas and scenarios reflective of real-world complexities.
The main contribution of this work lies in the development of benchmark datasets and metrics for assessing the moral identity of LLMs, which accounts for nuance, contextual sensitivity, and alignment with human ethical standards.
Our methodology involves a multi-faceted approach, combining quantitative analysis with qualitative insights from ethics scholars to ensure a thorough evaluation of model performance. By applying our benchmark across several leading LLMs, we uncover significant variations in moral reasoning capabilities of different models. These findings highlight the importance of considering moral reasoning in the development and evaluation of LLMs, as well as the need for ongoing research to address the biases and limitations uncovered in our study. We publicly release the benchmark at https://drive.google.com/drive/u/0/folders/1k93YZJserYc2CkqP8d4B3M3sgd3kA8W7 and also open-source the code of the project at https://github.com/agiresearch/MoralBench.

------------

`[2406.04449] MAIRA-2: Grounded Radiology Report Generation <https://arxiv.org/abs/2406.04449>`__ MAIRA-2:地面放射学报告生成

::

    Thu, 6 Jun 2024 19:12:41 GMT
    Shruthi Bannur, Kenza Bouzid, Daniel C. Castro, Anton Schwaighofer, Sam Bond-Taylor, Maximilian Ilse, Fernando P\'erez-Garc\'ia, Valentina Salvatelli, Harshita Sharma, Felix Meissen, Mercy Ranjit, Shaury Srivastav, Julia Gong, Fabian Falck, Ozan Oktay, Anja Thieme, Matthew P. Lungren, Maria Teodora Wetscherek, Javier Alvarez-Valle, Stephanie L. Hyland

Radiology reporting is a complex task that requires detailed image understanding, integration of multiple inputs, including comparison with prior imaging, and precise language generation. This makes it ideal for the development and use of generative multimodal models. Here, we extend report generation to include the localisation of individual findings on the image - a task we call grounded report generation. Prior work indicates that grounding is important for clarifying image understanding and interpreting AI-generated text. Therefore, grounded reporting stands to improve the utility and transparency of automated report drafting. To enable evaluation of grounded reporting, we propose a novel evaluation framework - RadFact - leveraging the reasoning capabilities of large language models (LLMs). RadFact assesses the factuality of individual generated sentences, as well as correctness of generated spatial localisations when present. We introduce MAIRA-2, a large multimodal model combining a radiology-specific image encoder with a LLM, and trained for the new task of grounded report generation on chest X-rays. MAIRA-2 uses more comprehensive inputs than explored previously: the current frontal image, the current lateral image, the prior frontal image and prior report, as well as the Indication, Technique and Comparison sections of the current report. We demonstrate that these additions significantly improve report quality and reduce hallucinations, establishing a new state of the art on findings generation (without grounding) on MIMIC-CXR while demonstrating the feasibility of grounded reporting as a novel and richer task.

------------

`[2406.04460] Evaluating the Smooth Control of Attribute Intensity in Text Generation with LLMs <https://arxiv.org/abs/2406.04460>`__ 用llm评估文本生成中属性强度的平滑控制

::

    Thu, 6 Jun 2024 19:35:51 GMT
    Shang Zhou, Feng Yao, Chengyu Dong, Zihan Wang, Jingbo Shang

Controlling the attribute intensity of text generation is crucial across scenarios (e.g., writing conciseness, chatting emotion, and explanation clarity). The remarkable capabilities of large language models (LLMs) have revolutionized text generation, prompting us to explore such \emph{smooth control} of LLM generation. Specifically, we propose metrics to assess the range, calibration, and consistency of the generated text's attribute intensity in response to varying control values, as well as its relevance to the intended context. To quantify the attribute intensity and context relevance, we propose an effective evaluation framework leveraging the Elo rating system and GPT4, both renowned for their robust alignment with human judgment. We look into two viable training-free methods for achieving smooth control of LLMs: (1) Prompting with semantic shifters, and (2) Modifying internal model representations. The evaluations of these two methods are conducted on $5$ different attributes with various models. Our code and dataset can be obtained from \url{https://github.com/ShangDataLab/Smooth-Control}.

------------

`[2406.04482] Automatic Bug Detection in LLM-Powered Text-Based Games Using LLMs <https://arxiv.org/abs/2406.04482>`__ 使用llm自动检测基于llm的文本游戏中的Bug

::

    Thu, 6 Jun 2024 20:11:08 GMT
    Claire Jin, Sudha Rao, Xiangyu Peng, Portia Botchway, Jessica Quaye, Chris Brockett, Bill Dolan

Advancements in large language models (LLMs) are revolutionizing interactive game design, enabling dynamic plotlines and interactions between players and non-player characters (NPCs). However, LLMs may exhibit flaws such as hallucinations, forgetfulness, or misinterpretations of prompts, causing logical inconsistencies and unexpected deviations from intended designs.
Automated techniques for detecting such game bugs are still lacking. To address this, we propose a systematic LLM-based method for automatically identifying such bugs from player game logs, eliminating the need for collecting additional data such as post-play surveys. Applied to a text-based game DejaBoom!, our approach effectively identifies bugs inherent in LLM-powered interactive games, surpassing unstructured LLM-powered bug-catching methods and filling the gap in automated detection of logical and design flaws.

------------

`[2406.04523] Proofread: Fixes All Errors with One Tap <https://arxiv.org/abs/2406.04523>`__ 校对:一键修复所有错误

::

    Thu, 6 Jun 2024 21:38:08 GMT
    Renjie Liu, Yanxiang Zhang, Yun Zhu, Haicheng Sun, Yuanbo Zhang, Michael Xuelin Huang, Shanqing Cai, Lei Meng, Shumin Zhai

The impressive capabilities in Large Language Models (LLMs) provide a powerful approach to reimagine users' typing experience. This paper demonstrates Proofread, a novel Gboard feature powered by a server-side LLM in Gboard, enabling seamless sentence-level and paragraph-level corrections with a single tap. We describe the complete system in this paper, from data generation, metrics design to model tuning and deployment. To obtain models with sufficient quality, we implement a careful data synthetic pipeline tailored to online use cases, design multifaceted metrics, employ a two-stage tuning approach to acquire the dedicated LLM for the feature: the Supervised Fine Tuning (SFT) for foundational quality, followed by the Reinforcement Learning (RL) tuning approach for targeted refinement. Specifically, we find sequential tuning on Rewrite and proofread tasks yields the best quality in SFT stage, and propose global and direct rewards in the RL tuning stage to seek further improvement. Extensive experiments on a human-labeled golden set showed our tuned PaLM2-XS model achieved 85.56\% good ratio. We launched the feature to Pixel 8 devices by serving the model on TPU v5 in Google Cloud, with thousands of daily active users. Serving latency was significantly reduced by quantization, bucket inference, text segmentation, and speculative decoding.
Our demo could be seen in \href{https://youtu.be/4ZdcuiwFU7I}{Youtube}.

------------

`[2406.04528] llmNER: (Zero|Few)-Shot Named Entity Recognition, Exploiting the Power of Large Language Models <https://arxiv.org/abs/2406.04528>`__ llmNER:(0 |Few)-Shot命名实体识别，利用大型语言模型的力量

::

    Thu, 6 Jun 2024 22:01:59 GMT
    Fabi\'an Villena, Luis Miranda, Claudio Aracena

Large language models (LLMs) allow us to generate high-quality human-like text. One interesting task in natural language processing (NLP) is named entity recognition (NER), which seeks to detect mentions of relevant information in documents. This paper presents llmNER, a Python library for implementing zero-shot and few-shot NER with LLMs; by providing an easy-to-use interface, llmNER can compose prompts, query the model, and parse the completion returned by the LLM. Also, the library enables the user to perform prompt engineering efficiently by providing a simple interface to test multiple variables. We validated our software on two NER tasks to show the library's flexibility.
llmNER aims to push the boundaries of in-context learning research by removing the barrier of the prompting and parsing steps.

------------

`[2406.04555] Creating an AI Observer: Generative Semantic Workspaces <https://arxiv.org/abs/2406.04555>`__ 创建AI观察者:生成式语义工作空间

::

    Fri, 7 Jun 2024 00:09:13 GMT
    Pavan Holur, Shreyas Rajesh, David Chong, Vwani Roychowdhury

An experienced human Observer reading a document -- such as a crime report -- creates a succinct plot-like $\textit{``Working Memory''}$ comprising different actors, their prototypical roles and states at any point, their evolution over time based on their interactions, and even a map of missing Semantic parts anticipating them in the future. $\textit{An equivalent AI Observer currently does not exist}$. We introduce the $\textbf{[G]}$enerative $\textbf{[S]}$emantic $\textbf{[W]}$orkspace (GSW) -- comprising an $\textit{``Operator''}$ and a $\textit{``Reconciler''}$ -- that leverages advancements in LLMs to create a generative-style Semantic framework, as opposed to a traditionally predefined set of lexicon labels. Given a text segment $C_n$ that describes an ongoing situation, the $\textit{Operator}$ instantiates actor-centric Semantic maps (termed ``Workspace instance'' $\mathcal{W}_n$). The $\textit{Reconciler}$ resolves differences between $\mathcal{W}_n$ and a ``Working memory'' $\mathcal{M}_n^*$ to generate the updated $\mathcal{M}_{n+1}^*$. GSW outperforms well-known baselines on several tasks ($\sim 94\%$ vs. FST, GLEN, BertSRL - multi-sentence Semantics extraction, $\sim 15\%$ vs. NLI-BERT, $\sim 35\%$ vs. QA). By mirroring the real Observer, GSW provides the first step towards Spatial Computing assistants capable of understanding individual intentions and predicting future behavior.

------------

`[2406.04583] Extroversion or Introversion? Controlling The Personality of Your Large Language Models <https://arxiv.org/abs/2406.04583>`__ 外向还是内向?控制你的大型语言模型的个性

::

    Fri, 7 Jun 2024 02:11:49 GMT
    Yanquan Chen, Zhen Wu, Junjie Guo, Shujian Huang, Xinyu Dai

Large language models (LLMs) exhibit robust capabilities in text generation and comprehension, mimicking human behavior and exhibiting synthetic personalities. However, some LLMs have displayed offensive personality, propagating toxic discourse. Existing literature neglects the origin and evolution of LLM personalities, as well as the effective personality control.
To fill these gaps, our study embarked on a comprehensive investigation into LLM personality control. We investigated several typical methods to influence LLMs, including three training methods: Continual Pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF), along with inference phase considerations (prompts). Our investigation revealed a hierarchy of effectiveness in control: Prompt > SFT > RLHF > Continual Pre-train. Notably, SFT exhibits a higher control success rate compared to prompt induction. While prompts prove highly effective, we found that prompt-induced personalities are less robust than those trained, making them more prone to showing conflicting personalities under reverse personality prompt induction. Besides, harnessing the strengths of both SFT and prompt, we proposed $\underline{\text{P}}$rompt $\underline{\text{I}}$nduction post $\underline{\text{S}}$upervised $\underline{\text{F}}$ine-tuning (PISF), which emerges as the most effective and robust strategy for controlling LLMs' personality, displaying high efficacy, high success rates, and high robustness.
Even under reverse personality prompt induction, LLMs controlled by PISF still exhibit stable and robust personalities.

------------

`[2406.04614] LawGPT: A Chinese Legal Knowledge-Enhanced Large Language Model <https://arxiv.org/abs/2406.04614>`__ LawGPT:一个中文法律知识增强的大型语言模型

::

    Fri, 7 Jun 2024 03:52:56 GMT
    Zhi Zhou, Jiang-Xin Shi, Peng-Xiao Song, Xiao-Wen Yang, Yi-Xuan Jin, Lan-Zhe Guo, Yu-Feng Li

Large language models (LLMs), including both proprietary and open-source models, have showcased remarkable capabilities in addressing a wide range of downstream tasks. Nonetheless, when it comes to practical Chinese legal tasks, these models fail to meet the actual requirements. Proprietary models do not ensure data privacy for sensitive legal cases, while open-source models demonstrate unsatisfactory performance due to their lack of legal knowledge. To address this problem, we introduce LawGPT, the first open-source model specifically designed for Chinese legal applications. LawGPT comprises two key components: legal-oriented pre-training and legal supervised fine-tuning.
Specifically, we employ large-scale Chinese legal documents for legal-oriented pre-training to incorporate legal domain knowledge. To further improve the model's performance on downstream legal tasks, we create a knowledge-driven instruction dataset for legal supervised fine-tuning. Our experimental results demonstrate that LawGPT outperforms the open-source LLaMA 7B model. Our code and resources are publicly available at https://github.com/pengxiao-song/LaWGPT and have received 5.7K stars on GitHub.

------------

`[2406.04625] Key-Element-Informed sLLM Tuning for Document Summarization <https://arxiv.org/abs/2406.04625>`__ 基于关键元素的sLLM文档摘要调优

::

    Fri, 7 Jun 2024 04:19:01 GMT
    Sangwon Ryu, Heejin Do, Yunsu Kim, Gary Geunbae Lee, Jungseul Ok

Remarkable advances in large language models (LLMs) have enabled high-quality text summarization. However, this capability is currently accessible only through LLMs of substantial size or proprietary LLMs with usage fees. In response, smaller-scale LLMs (sLLMs) of easy accessibility and low costs have been extensively studied, yet they often suffer from missing key information and entities, i.e., low relevance, in particular, when input documents are long. We hence propose a key-element-informed instruction tuning for summarization, so-called KEITSum, which identifies key elements in documents and instructs sLLM to generate summaries capturing these key elements.
Experimental results on dialogue and news datasets demonstrate that sLLM with KEITSum indeed provides high-quality summarization with higher relevance and less hallucinations, competitive to proprietary LLM.

------------

`[2406.04630] Low-Resource Cross-Lingual Summarization through Few-Shot Learning with Large Language Models <https://arxiv.org/abs/2406.04630>`__ 基于大语言模型少样本学习的低资源跨语言摘要

::

    Fri, 7 Jun 2024 04:31:41 GMT
    Gyutae Park and Seojin Hwang and Hwanhee Lee

Cross-lingual summarization (XLS) aims to generate a summary in a target language different from the source language document. While large language models (LLMs) have shown promising zero-shot XLS performance, their few-shot capabilities on this task remain unexplored, especially for low-resource languages with limited parallel data. In this paper, we investigate the few-shot XLS performance of various models, including Mistral-7B-Instruct-v0.2, GPT-3.5, and GPT-4. Our experiments demonstrate that few-shot learning significantly improves the XLS performance of LLMs, particularly GPT-3.5 and GPT-4, in low-resource settings. However, the open-source model Mistral-7B-Instruct-v0.2 struggles to adapt effectively to the XLS task with limited examples. Our findings highlight the potential of few-shot learning for improving XLS performance and the need for further research in designing LLM architectures and pre-training objectives tailored for this task. We provide a future work direction to explore more effective few-shot learning strategies and to investigate the transfer learning capabilities of LLMs for cross-lingual summarization.

------------

`[2406.04638] Large Language Model-guided Document Selection <https://arxiv.org/abs/2406.04638>`__ 大型语言模型引导的文档选择

::

    Fri, 7 Jun 2024 04:52:46 GMT
    Xiang Kong, Tom Gunter, Ruoming Pang

Large Language Model (LLM) pre-training exhausts an ever growing compute budget, yet recent research has demonstrated that careful document selection enables comparable model quality with only a fraction of the FLOPs. Inspired by efforts suggesting that domain-specific training document selection is in fact an interpretable process [Gunasekar et al., 2023], as well as research showing that instruction-finetuned LLMs are adept zero-shot data labelers [Gilardi et al.,2023], we explore a promising direction for scalable general-domain document selection; employing a prompted LLM as a document grader, we distill quality labels into a classifier model, which is applied at scale to a large, and already heavily-filtered, web-crawl-derived corpus autonomously. Following the guidance of this classifier, we drop 75% of the corpus and train LLMs on the remaining data. Results across multiple benchmarks show that: 1. Filtering allows us to quality-match a model trained on the full corpus across diverse benchmarks with at most 70% of the FLOPs, 2. More capable LLM labelers and classifier models lead to better results that are less sensitive to the labeler's prompt, 3. In-context learning helps to boost the performance of less-capable labeling models. In all cases we use open-source datasets, models, recipes, and evaluation frameworks, so that results can be reproduced by the community.

------------

`[2406.04669] DiNeR: a Large Realistic Dataset for Evaluating Compositional Generalization <https://arxiv.org/abs/2406.04669>`__ 

::

    Fri, 7 Jun 2024 06:35:21 GMT
    Chengang Hu and Xiao Liu and Yansong Feng

Most of the existing compositional generalization datasets are synthetically-generated, resulting in a lack of natural language variation.
While there have been recent attempts to introduce non-synthetic datasets for compositional generalization, they suffer from either limited data scale or a lack of diversity in the forms of combinations. To better investigate compositional generalization with more linguistic phenomena and compositional diversity, we propose the DIsh NamE Recognition (DiNeR) task and create a large realistic Chinese dataset. Given a recipe instruction, models are required to recognize the dish name composed of diverse combinations of food, actions, and flavors. Our dataset consists of 3,811 dishes and 228,114 recipes, and involves plenty of linguistic phenomena such as anaphora, omission and ambiguity. We provide two strong baselines based on T5 and large language models (LLMs). This work contributes a challenging task, baseline methods to tackle the task, and insights into compositional generalization in the context of dish name recognition. Code and data are available at https://github.com/Jumpy-pku/DiNeR.

------------

`[2406.04712] AICoderEval: Improving AI Domain Code Generation of Large Language Models <https://arxiv.org/abs/2406.04712>`__ AICoderEval:改进大型语言模型的AI域代码生成

::

    Fri, 7 Jun 2024 07:45:38 GMT
    Yinghui Xia, Yuyan Chen, Tianyu Shi, Jun Wang, Jinsong Yang

Automated code generation is a pivotal capability of large language models (LLMs). However, assessing this capability in real-world scenarios remains challenging. Previous methods focus more on low-level code generation, such as model loading, instead of generating high-level codes catering for real-world tasks, such as image-to-text, text classification, in various domains.
Therefore, we construct AICoderEval, a dataset focused on real-world tasks in various domains based on HuggingFace, PyTorch, and TensorFlow, along with comprehensive metrics for evaluation and enhancing LLMs' task-specific code generation capability. AICoderEval contains test cases and complete programs for automated evaluation of these tasks, covering domains such as natural language processing, computer vision, and multimodal learning. To facilitate research in this area, we open-source the AICoderEval dataset at \url{https://huggingface.co/datasets/vixuowis/AICoderEval}. After that, we propose CoderGen, an agent-based framework, to help LLMs generate codes related to real-world tasks on the constructed AICoderEval. Moreover, we train a more powerful task-specific code generation model, named AICoder, which is refined on llama-3 based on AICoderEval. Our experiments demonstrate the effectiveness of CoderGen in improving LLMs' task-specific code generation capability (by 12.00\% on pass@1 for original model and 9.50\% on pass@1 for ReAct Agent).
AICoder also outperforms current code generation LLMs, indicating the great quality of the AICoderEval benchmark.

------------

`[2406.04758] Think out Loud: Emotion Deducing Explanation in Dialogues <https://arxiv.org/abs/2406.04758>`__ 自言自语:对话中情感演绎解释

::

    Fri, 7 Jun 2024 08:58:29 GMT
    Jiangnan Li, Zheng Lin, Lanrui Wang, Qingyi Si, Yanan Cao, Mo Yu, Peng Fu, Weiping Wang, Jie Zhou

Humans convey emotions through daily dialogues, making emotion understanding a crucial step of affective intelligence. To understand emotions in dialogues, machines are asked to recognize the emotion for an utterance (Emotion Recognition in Dialogues, ERD); based on the emotion, then find causal utterances for the emotion (Emotion Cause Extraction in Dialogues, ECED). The setting of the two tasks requires first ERD and then ECED, ignoring the mutual complement between emotion and cause. To fix this, some new tasks are proposed to extract them simultaneously. Although the current research on these tasks has excellent achievements, simply identifying emotion-related factors by classification modeling lacks realizing the specific thinking process of causes stimulating the emotion in an explainable way. This thinking process especially reflected in the reasoning ability of Large Language Models (LLMs) is under-explored. To this end, we propose a new task "Emotion Deducing Explanation in Dialogues" (EDEN). EDEN recognizes emotion and causes in an explicitly thinking way. That is, models need to generate an explanation text, which first summarizes the causes; analyzes the inner activities of the speakers triggered by the causes using common sense; then guesses the emotion accordingly. To support the study of EDEN, based on the existing resources in ECED, we construct two EDEN datasets by human effort. We further evaluate different models on EDEN and find that LLMs are more competent than conventional PLMs. Besides, EDEN can help LLMs achieve better recognition of emotions and causes, which explores a new research direction of explainable emotion understanding in dialogues.

------------

`[2406.04836] Revisiting Catastrophic Forgetting in Large Language Model Tuning <https://arxiv.org/abs/2406.04836>`__ 大型语言模型调优中的灾难性遗忘

::

    Fri, 7 Jun 2024 11:09:13 GMT
    Hongyu Li, Liang Ding, Meng Fang, Dacheng Tao

Catastrophic Forgetting (CF) means models forgetting previously acquired knowledge when learning new data. It compromises the effectiveness of large language models (LLMs) during fine-tuning, yet the underlying causes have not been thoroughly investigated. This paper takes the first step to reveal the direct link between the flatness of the model loss landscape and the extent of CF in the field of LLMs. Based on this, we introduce the sharpness-aware minimization to mitigate CF by flattening the loss landscape. Experiments on three widely-used fine-tuning datasets, spanning different model scales, demonstrate the effectiveness of our method in alleviating CF. Analyses show that we nicely complement the existing anti-forgetting strategies, further enhancing the resistance of LLMs to CF.

------------

`[2406.04854] Uncertainty Aware Learning for Language Model Alignment <https://arxiv.org/abs/2406.04854>`__ 语言模型对齐的不确定性感知学习

::

    Fri, 7 Jun 2024 11:37:45 GMT
    Yikun Wang, Rui Zheng, Liang Ding, Qi Zhang, Dahua Lin, Dacheng Tao

As instruction-tuned large language models (LLMs) evolve, aligning pretrained foundation models presents increasing challenges. Existing alignment strategies, which typically leverage diverse and high-quality data sources, often overlook the intrinsic uncertainty of tasks, learning all data samples equally. This may lead to suboptimal data efficiency and model performance. In response, we propose uncertainty-aware learning (UAL) to improve the model alignment of different task scenarios, by introducing the sample uncertainty (elicited from more capable LLMs). We implement UAL in a simple fashion -- adaptively setting the label smoothing value of training according to the uncertainty of individual samples. Analysis shows that our UAL indeed facilitates better token clustering in the feature space, validating our hypothesis. Extensive experiments on widely used benchmarks demonstrate that our UAL significantly and consistently outperforms standard supervised fine-tuning. Notably, LLMs aligned in a mixed scenario have achieved an average improvement of 10.62\% on high-entropy tasks (i.e., AlpacaEval leaderboard), and 1.81\% on complex low-entropy tasks (i.e., MetaMath and GSM8K).

------------

`[2406.04866] ComplexTempQA: A Large-Scale Dataset for Complex Temporal Question Answering <https://arxiv.org/abs/2406.04866>`__ ComplexTempQA:面向复杂时序问答的大规模数据集

::

    Fri, 7 Jun 2024 12:01:59 GMT
    Raphael Gruber, Abdelrahman Abdallah, Michael F\"arber, Adam Jatowt

We introduce ComplexTempQA,a large-scale dataset consisting of over 100 million question-answer pairs designed to tackle the challenges in temporal question answering. ComplexTempQA significantly surpasses existing benchmarks like HOTPOTQA, TORQUE, and TEQUILA in scale and scope. Utilizing data from Wikipedia and Wikidata, the dataset covers questions spanning over two decades and offers an unmatched breadth of topics. We introduce a unique taxonomy that categorizes questions as attributes, comparisons, and counting questions, each revolving around events, entities, and time periods. One standout feature of ComplexTempQA is the high complexity of its questions, which demand effective capabilities for answering such as across-time comparison, temporal aggregation, and multi-hop reasoning involving temporal event ordering and entity recognition. Additionally, each question is accompanied by detailed metadata, including specific time scopes, allowing for comprehensive evaluation and enhancement of the temporal reasoning abilities of large language models.
ComplexTempQA serves both as a testing ground for developing sophisticated AI models and as a foundation for advancing research in question answering, information retrieval, and language understanding. Dataset and code are freely available at: https://github.com/DataScienceUIBK/ComplexTempQA.

------------

`[2406.04926] Through the Thicket: A Study of Number-Oriented LLMs derived from Random Forest Models <https://arxiv.org/abs/2406.04926>`__ 

::

    Fri, 7 Jun 2024 13:31:51 GMT
    Micha{\l} Romaszewski, Przemys{\l}aw Seku{\l}a, Przemys{\l}aw G{\l}omb, Micha{\l} Cholewa, Katarzyna Ko{\l}odziej

Large Language Models (LLMs) have shown exceptional performance in text processing. Notably, LLMs can synthesize information from large datasets and explain their decisions similarly to human reasoning through a chain of thought (CoT). An emerging application of LLMs is the handling and interpreting of numerical data, where fine-tuning enhances their performance over basic inference methods. This paper proposes a novel approach to training LLMs using knowledge transfer from a random forest (RF) ensemble, leveraging its efficiency and accuracy. By converting RF decision paths into natural language statements, we generate outputs for LLM fine-tuning, enhancing the model's ability to classify and explain its decisions. Our method includes verifying these rules through established classification metrics, ensuring their correctness. We also examine the impact of preprocessing techniques on the representation of numerical data and their influence on classification accuracy and rule correctness

------------

`[2406.04941] TCMD: A Traditional Chinese Medicine QA Dataset for Evaluating Large Language Models <https://arxiv.org/abs/2406.04941>`__ TCMD:用于评估大型语言模型的中医QA数据集

::

    Fri, 7 Jun 2024 13:48:15 GMT
    Ping Yu, Kaitao Song, Fengchen He, Ming Chen, Jianfeng Lu

The recently unprecedented advancements in Large Language Models (LLMs) have propelled the medical community by establishing advanced medical-domain models.
However, due to the limited collection of medical datasets, there are only a few comprehensive benchmarks available to gauge progress in this area. In this paper, we introduce a new medical question-answering (QA) dataset that contains massive manual instruction for solving Traditional Chinese Medicine examination tasks, called TCMD. Specifically, our TCMD collects massive questions across diverse domains with their annotated medical subjects and thus supports us in comprehensively assessing the capability of LLMs in the TCM domain. Extensive evaluation of various general LLMs and medical-domain-specific LLMs is conducted. Moreover, we also analyze the robustness of current LLMs in solving TCM QA tasks by introducing randomness. The inconsistency of the experimental results also reveals the shortcomings of current LLMs in solving QA tasks. We also expect that our dataset can further facilitate the development of LLMs in the TCM area.

------------

`[2406.04947] BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense <https://arxiv.org/abs/2406.04947>`__ BAMO在SemEval-2024任务9:难题:挑战常识的新任务

::

    Fri, 7 Jun 2024 14:01:56 GMT
    Baktash Ansari, Mohammadmostafa Rostamkhani, Sauleh Eetemadi

This paper outlines our approach to SemEval 2024 Task 9, BRAINTEASER: A Novel Task Defying Common Sense. The task aims to evaluate the ability of language models to think creatively. The dataset comprises multi-choice questions that challenge models to think "outside of the box". We fine-tune 2 models, BERT and RoBERTa Large. Next, we employ a Chain of Thought (CoT) zero-shot prompting approach with 6 large language models, such as GPT-3.5, Mixtral, and Llama2.
Finally, we utilize ReConcile, a technique that employs a "round table conference" approach with multiple agents for zero-shot learning, to generate consensus answers among 3 selected language models. Our best method achieves an overall accuracy of 85 percent on the sentence puzzles subtask.

------------

`[2406.04952] Quantifying Geospatial in the Common Crawl Corpus <https://arxiv.org/abs/2406.04952>`__ 基于Common Crawl语料库的地理空间量化

::

    Fri, 7 Jun 2024 14:16:37 GMT
    Ilya Ilyankou, Meihui Wang, James Haworth, Stefano Cavazzi

Large language models (LLMs) exhibit emerging geospatial capabilities, stemming from their pre-training on vast unlabelled text datasets that are often derived from the Common Crawl corpus. However, the geospatial content within CC remains largely unexplored, impacting our understanding of LLMs' spatial reasoning. This paper investigates the prevalence of geospatial data in recent Common Crawl releases using Gemini, a powerful language model. By analyzing a sample of documents and manually revising the results, we estimate that between 1 in 5 and 1 in 6 documents contain geospatial information such as coordinates and street addresses. Our findings provide quantitative insights into the nature and extent of geospatial data within Common Crawl, and web crawl data in general. Furthermore, we formulate questions to guide future investigations into the geospatial content of available web crawl datasets and its influence on LLMs.

------------

`[2406.05035] Scenarios and Approaches for Situated Natural Language Explanations <https://arxiv.org/abs/2406.05035>`__ 情境自然语言解释的场景和方法

::

    Fri, 7 Jun 2024 15:56:32 GMT
    Pengshuo Qiu, Frank Rudzicz, Zining Zhu

Large language models (LLMs) can be used to generate natural language explanations (NLE) that are adapted to different users' situations. However, there is yet to be a quantitative evaluation of the extent of such adaptation.
To bridge this gap, we collect a benchmarking dataset, Situation-Based Explanation. This dataset contains 100 explanandums. Each explanandum is paired with explanations targeted at three distinct audience types-such as educators, students, and professionals-enabling us to assess how well the explanations meet the specific informational needs and contexts of these diverse groups e.g.
students, teachers, and parents. For each "explanandum paired with an audience" situation, we include a human-written explanation. These allow us to compute scores that quantify how the LLMs adapt the explanations to the situations. On an array of pretrained language models with varying sizes, we examine three categories of prompting methods: rule-based prompting, meta-prompting, and in-context learning prompting. We find that 1) language models can generate prompts that result in explanations more precisely aligned with the target situations, 2) explicitly modeling an "assistant" persona by prompting "You are a helpful assistant..." is not a necessary prompt technique for situated NLE tasks, and 3) the in-context learning prompts only can help LLMs learn the demonstration template but can't improve their inference performance. SBE and our analysis facilitate future research towards generating situated natural language explanations.

------------

`[2406.05063] Are Large Language Models More Empathetic than Humans? <https://arxiv.org/abs/2406.05063>`__ 

::

    Fri, 7 Jun 2024 16:33:43 GMT
    Anuradha Welivita and Pearl Pu

With the emergence of large language models (LLMs), investigating if they can surpass humans in areas such as emotion recognition and empathetic responding has become a focal point of research. This paper presents a comprehensive study exploring the empathetic responding capabilities of four state-of-the-art LLMs: GPT-4, LLaMA-2-70B-Chat, Gemini-1.0-Pro, and Mixtral-8x7B-Instruct in comparison to a human baseline. We engaged 1,000 participants in a between-subjects user study, assessing the empathetic quality of responses generated by humans and the four LLMs to 2,000 emotional dialogue prompts meticulously selected to cover a broad spectrum of 32 distinct positive and negative emotions. Our findings reveal a statistically significant superiority of the empathetic responding capability of LLMs over humans. GPT-4 emerged as the most empathetic, marking approximately 31% increase in responses rated as "Good" compared to the human benchmark. It was followed by LLaMA-2, Mixtral-8x7B, and Gemini-Pro, which showed increases of approximately 24%, 21%, and 10% in "Good" ratings, respectively. We further analyzed the response ratings at a finer granularity and discovered that some LLMs are significantly better at responding to specific emotions compared to others. The suggested evaluation framework offers a scalable and adaptable approach for assessing the empathy of new LLMs, avoiding the need to replicate this study's findings in future research.

------------

`[2406.04412] Aligning Large Language Models with Self-generated Preference Data <https://arxiv.org/abs/2406.04412>`__ 用自我生成的偏好数据对齐大型语言模型

::

    Thu, 6 Jun 2024 18:01:02 GMT
    Dongyoung Kim, Kimin Lee, Jinwoo Shin, Jaehyung Kim

Aligning large language models (LLMs) with human preferences becomes a key component to obtaining state-of-the-art performance, but it yields a huge cost to construct a large human-annotated preference dataset. To tackle this problem, we propose a new framework that boosts the alignment of LLMs through Self-generated Preference data (Selfie) using only a very small amount of human-annotated preference data. Our key idea is leveraging the human prior knowledge within the small (seed) data and progressively improving the alignment of LLM, by iteratively generating the responses and learning from them with the self-annotated preference data. To be specific, we propose to derive the preference label from the logits of LLM to explicitly extract the model's inherent preference. Compared to the previous approaches using external reward models or implicit in-context learning, we observe that the proposed approach is significantly more effective. In addition, we introduce a noise-aware preference learning algorithm to mitigate the risk of low quality within generated preference data. Our experimental results demonstrate that the proposed framework significantly boosts the alignment of LLMs. For example, we achieve superior alignment performance on AlpacaEval 2.0 with only 3.3\% of the ground-truth preference labels in the Ultrafeedback data compared to the cases using the entire data or state-of-the-art baselines.

------------

`[2406.04443] Gradient Clipping Improves AdaGrad when the Noise Is Heavy-Tailed <https://arxiv.org/abs/2406.04443>`__ 当噪声具有重尾时，梯度裁剪可以改进AdaGrad算法

::

    Thu, 6 Jun 2024 18:49:10 GMT
    Savelii Chezhegov, Yaroslav Klyukin, Andrei Semenov, Aleksandr Beznosikov, Alexander Gasnikov, Samuel Horv\'ath, Martin Tak\'a\v{c}, Eduard Gorbunov

Methods with adaptive stepsizes, such as AdaGrad and Adam, are essential for training modern Deep Learning models, especially Large Language Models.
Typically, the noise in the stochastic gradients is heavy-tailed for the later ones. Gradient clipping provably helps to achieve good high-probability convergence for such noises. However, despite the similarity between AdaGrad/Adam and Clip-SGD, the high-probability convergence of AdaGrad/Adam has not been studied in this case. In this work, we prove that AdaGrad (and its delayed version) can have provably bad high-probability convergence if the noise is heavy-tailed. To fix this issue, we propose a new version of AdaGrad called Clip-RAdaGradD (Clipped Reweighted AdaGrad with Delay) and prove its high-probability convergence bounds with polylogarithmic dependence on the confidence level for smooth convex/non-convex stochastic optimization with heavy-tailed noise. Our empirical evaluations, including NLP model fine-tuning, highlight the superiority of clipped versions of AdaGrad/Adam in handling the heavy-tailed noise.

------------

`[2406.04446] Can Language Models Use Forecasting Strategies? <https://arxiv.org/abs/2406.04446>`__ 

::

    Thu, 6 Jun 2024 19:01:42 GMT
    Sarah Pratt, Seth Blumberg, Pietro Kreitlon Carolino, Meredith Ringel Morris

Advances in deep learning systems have allowed large models to match or surpass human accuracy on a number of skills such as image classification, basic programming, and standardized test taking. As the performance of the most capable models begin to saturate on tasks where humans already achieve high accuracy, it becomes necessary to benchmark models on increasingly complex abilities. One such task is forecasting the future outcome of events. In this work we describe experiments using a novel dataset of real world events and associated human predictions, an evaluation metric to measure forecasting ability, and the accuracy of a number of different LLM based forecasting designs on the provided dataset. Additionally, we analyze the performance of the LLM forecasters against human predictions and find that models still struggle to make accurate predictions about the future. Our follow-up experiments indicate this is likely due to models' tendency to guess that most events are unlikely to occur (which tends to be true for many prediction datasets, but does not reflect actual forecasting abilities). We reflect on next steps for developing a systematic and reliable approach to studying LLM forecasting.

------------

`[2406.04501] FLUID-LLM: Learning Computational Fluid Dynamics with Spatiotemporal-aware Large Language Models <https://arxiv.org/abs/2406.04501>`__ Fluid - llm:基于时空感知大型语言模型的计算流体力学学习

::

    Thu, 6 Jun 2024 20:55:40 GMT
    Max Zhu and Adri\'an Bazaga and Pietro Li\`o

Learning computational fluid dynamics (CFD) traditionally relies on computationally intensive simulations of the Navier-Stokes equations. Recently, large language models (LLMs) have shown remarkable pattern recognition and reasoning abilities in natural language processing (NLP) and computer vision (CV). However, these models struggle with the complex geometries inherent in fluid dynamics. We introduce FLUID-LLM, a novel framework combining pre-trained LLMs with spatiotemporal-aware encoding to predict unsteady fluid dynamics. Our approach leverages the temporal autoregressive abilities of LLMs alongside spatial-aware layers, bridging the gap between previous CFD prediction methods.
Evaluations on standard benchmarks reveal significant performance improvements across various fluid datasets. Our results demonstrate that FLUID-LLM effectively integrates spatiotemporal information into pre-trained LLMs, enhancing CFD task performance.

------------

`[2406.04606] Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions <https://arxiv.org/abs/2406.04606>`__ 有用的还是有害的数据?用于解释语言模型预测的无微调沙普利归因

::

    Fri, 7 Jun 2024 03:29:57 GMT
    Jingtan Wang, Xiaoqiang Lin, Rui Qiao, Chuan-Sheng Foo, Bryan Kian Hsiang Low

The increasing complexity of foundational models underscores the necessity for explainability, particularly for fine-tuning, the most widely used training method for adapting models to downstream tasks. Instance attribution, one type of explanation, attributes the model prediction to each training example by an instance score. However, the robustness of instance scores, specifically towards dataset resampling, has been overlooked. To bridge this gap, we propose a notion of robustness on the sign of the instance score. We theoretically and empirically demonstrate that the popular leave-one-out-based methods lack robustness, while the Shapley value behaves significantly better, but at a higher computational cost. Accordingly, we introduce an efficient fine-tuning-free approximation of the Shapley value (FreeShap) for instance attribution based on the neural tangent kernel. We empirically demonstrate that FreeShap outperforms other methods for instance attribution and other data-centric applications such as data removal, data selection, and wrong label detection, and further generalize our scale to large language models (LLMs).
Our code is available at https://github.com/JTWang2000/FreeShap.

------------

`[2406.04640] LinkGPT: Teaching Large Language Models To Predict Missing Links <https://arxiv.org/abs/2406.04640>`__ LinkGPT:大型语言模型预测缺失链接

::

    Fri, 7 Jun 2024 04:54:36 GMT
    Zhongmou He, Jing Zhu, Shengyi Qian, Joyce Chai, Danai Koutra

Large Language Models (LLMs) have shown promising results on various language and vision tasks. Recently, there has been growing interest in applying LLMs to graph-based tasks, particularly on Text-Attributed Graphs (TAGs). However, most studies have focused on node classification, while the use of LLMs for link prediction (LP) remains understudied. In this work, we propose a new task on LLMs, where the objective is to leverage LLMs to predict missing links between nodes in a graph. This task evaluates an LLM's ability to reason over structured data and infer new facts based on learned patterns. This new task poses two key challenges: (1) How to effectively integrate pairwise structural information into the LLMs, which is known to be crucial for LP performance, and (2) how to solve the computational bottleneck when teaching LLMs to perform LP.
To address these challenges, we propose LinkGPT, the first end-to-end trained LLM for LP tasks. To effectively enhance the LLM's ability to understand the underlying structure, we design a two-stage instruction tuning approach where the first stage fine-tunes the pairwise encoder, projector, and node projector, and the second stage further fine-tunes the LLMs to predict links. To address the efficiency challenges at inference time, we introduce a retrieval-reranking scheme. Experiments show that LinkGPT can achieve state-of-the-art performance on real-world graphs as well as superior generalization in zero-shot and few-shot learning, surpassing existing benchmarks. At inference time, it can achieve $10\times$ speedup while maintaining high LP accuracy.

------------

`[2406.04687] LogiCode: an LLM-Driven Framework for Logical Anomaly Detection <https://arxiv.org/abs/2406.04687>`__ LogiCode:一个llm驱动的逻辑异常检测框架

::

    Fri, 7 Jun 2024 07:01:06 GMT
    Yiheng Zhang, Yunkang Cao, Xiaohao Xu, Weiming Shen

This paper presents LogiCode, a novel framework that leverages Large Language Models (LLMs) for identifying logical anomalies in industrial settings, moving beyond traditional focus on structural inconsistencies. By harnessing LLMs for logical reasoning, LogiCode autonomously generates Python codes to pinpoint anomalies such as incorrect component quantities or missing elements, marking a significant leap forward in anomaly detection technologies. A custom dataset "LOCO-Annotations" and a benchmark "LogiBench" are introduced to evaluate the LogiCode's performance across various metrics including binary classification accuracy, code generation success rate, and precision in reasoning. Findings demonstrate LogiCode's enhanced interpretability, significantly improving the accuracy of logical anomaly detection and offering detailed explanations for identified anomalies. This represents a notable shift towards more intelligent, LLM-driven approaches in industrial anomaly detection, promising substantial impacts on industry-specific applications.

------------

`[2406.04824] FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch <https://arxiv.org/abs/2406.04824>`__ FunBO:用FunSearch发现贝叶斯优化的获取函数

::

    Fri, 7 Jun 2024 10:49:59 GMT
    Virginia Aglietti, Ira Ktena, Jessica Schrouff, Eleni Sgouritsa, Francisco J. R. Ruiz, Alexis Bellot, Silvia Chiappa

The sample efficiency of Bayesian optimization algorithms depends on carefully crafted acquisition functions (AFs) guiding the sequential collection of function evaluations. The best-performing AF can vary significantly across optimization problems, often requiring ad-hoc and problem-specific choices.
This work tackles the challenge of designing novel AFs that perform well across a variety of experimental settings. Based on FunSearch, a recent work using Large Language Models (LLMs) for discovery in mathematical sciences, we propose FunBO, an LLM-based method that can be used to learn new AFs written in computer code by leveraging access to a limited number of evaluations for a set of objective functions. We provide the analytic expression of all discovered AFs and evaluate them on various global optimization benchmarks and hyperparameter optimization tasks. We show how FunBO identifies AFs that generalize well in and out of the training distribution of functions, thus outperforming established general-purpose AFs and achieving competitive performance against AFs that are customized to specific function types and are learned via transfer-learning algorithms.

------------

`[2406.04346] Automating Patch Set Generation from Code Review Comments Using Large Language Models <https://arxiv.org/abs/2406.04346>`__ 

::

    Wed, 10 Apr 2024 02:46:08 GMT
    Tajmilur Rahman, Rahul Singh, Mir Yousuf Sultan

The advent of Large Language Models (LLMs) has revolutionized various domains of artificial intelligence, including the realm of software engineering. In this research, we evaluate the efficacy of pre-trained LLMs in replicating the tasks traditionally performed by developers in response to code review comments. We provide code contexts to five popular LLMs and obtain the suggested code-changes (patch sets) derived from real-world code-review comments. The performance of each model is meticulously assessed by comparing their generated patch sets against the historical data of human-generated patch-sets from the same repositories. This comparative analysis aims to determine the accuracy, relevance, and depth of the LLMs' feedback, thereby evaluating their readiness to support developers in responding to code-review comments.
Novelty: This particular research area is still immature requiring a substantial amount of studies yet to be done. No prior research has compared the performance of existing Large Language Models (LLMs) in code-review comments. This in-progress study assesses current LLMs in code review and paves the way for future advancements in automated code quality assurance, reducing context-switching overhead due to interruptions from code change requests.

------------

`[2406.04373] VerilogReader: LLM-Aided Hardware Test Generation <https://arxiv.org/abs/2406.04373>`__ veriloreader: llm辅助的硬件测试生成

::

    Mon, 3 Jun 2024 07:20:51 GMT
    Ruiyang Ma, Yuxin Yang, Ziqian Liu, Jiaxi Zhang, Min Li, Junhua Huang, Guojie Luo

Test generation has been a critical and labor-intensive process in hardware design verification. Recently, the emergence of Large Language Model (LLM) with their advanced understanding and inference capabilities, has introduced a novel approach. In this work, we investigate the integration of LLM into the Coverage Directed Test Generation (CDG) process, where the LLM functions as a Verilog Reader. It accurately grasps the code logic, thereby generating stimuli that can reach unexplored code branches. We compare our framework with random testing, using our self-designed Verilog benchmark suite. Experiments demonstrate that our framework outperforms random testing on designs within the LLM's comprehension scope. Our work also proposes prompt engineering optimizations to augment LLM's understanding scope and accuracy.

------------

`[2406.04379] VHDL-Eval: A Framework for Evaluating Large Language Models in VHDL Code Generation <https://arxiv.org/abs/2406.04379>`__ VHDL- eval:面向VHDL代码生成的大型语言模型评估框架

::

    Thu, 6 Jun 2024 00:06:50 GMT
    Prashanth Vijayaraghavan, Luyao Shi, Stefano Ambrogio, Charles Mackin, Apoorva Nitsure, David Beymer, Ehsan Degan

With the unprecedented advancements in Large Language Models (LLMs), their application domains have expanded to include code generation tasks across various programming languages. While significant progress has been made in enhancing LLMs for popular programming languages, there exists a notable gap in comprehensive evaluation frameworks tailored for Hardware Description Languages (HDLs), particularly VHDL. This paper addresses this gap by introducing a comprehensive evaluation framework designed specifically for assessing LLM performance in VHDL code generation task. We construct a dataset for evaluating LLMs on VHDL code generation task. This dataset is constructed by translating a collection of Verilog evaluation problems to VHDL and aggregating publicly available VHDL problems, resulting in a total of 202 problems. To assess the functional correctness of the generated VHDL code, we utilize a curated set of self-verifying testbenches specifically designed for those aggregated VHDL problem set. We conduct an initial evaluation of different LLMs and their variants, including zero-shot code generation, in-context learning (ICL), and Parameter-efficient fine-tuning (PEFT) methods. Our findings underscore the considerable challenges faced by existing LLMs in VHDL code generation, revealing significant scope for improvement. This study emphasizes the necessity of supervised fine-tuning code generation models specifically for VHDL, offering potential benefits to VHDL designers seeking efficient code generation solutions.

------------

`[2406.04432] LipGER: Visually-Conditioned Generative Error Correction for Robust Automatic Speech Recognition <https://arxiv.org/abs/2406.04432>`__ LipGER:鲁棒自动语音识别的视觉条件生成误差校正

::

    Thu, 6 Jun 2024 18:17:59 GMT
    Sreyan Ghosh and Sonal Kumar and Ashish Seth and Purva Chiniya and Utkarsh Tyagi and Ramani Duraiswami and Dinesh Manocha

Visual cues, like lip motion, have been shown to improve the performance of Automatic Speech Recognition (ASR) systems in noisy environments. We propose LipGER (Lip Motion aided Generative Error Correction), a novel framework for leveraging visual cues for noise-robust ASR. Instead of learning the cross-modal correlation between the audio and visual modalities, we make an LLM learn the task of visually-conditioned (generative) ASR error correction.
Specifically, we instruct an LLM to predict the transcription from the N-best hypotheses generated using ASR beam-search. This is further conditioned on lip motions. This approach addresses key challenges in traditional AVSR learning, such as the lack of large-scale paired datasets and difficulties in adapting to new domains. We experiment on 4 datasets in various settings and show that LipGER improves the Word Error Rate in the range of 1.1%-49.2%. We also release LipHyp, a large-scale dataset with hypothesis-transcription pairs that is additionally equipped with lip motion cues to promote further research in this space

------------

`[2406.04568] StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation <https://arxiv.org/abs/2406.04568>`__ StackSight:通过大型语言模型和神经符号思维链反编译揭开WebAssembly

::

    Fri, 7 Jun 2024 01:08:17 GMT
    Weike Fang, Zhejian Zhou, Junzhou He, Weihang Wang

WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets.
StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics.

------------

`[2406.04594] Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach <https://arxiv.org/abs/2406.04594>`__ 用C4:通信驱动方法提高大规模并行训练效率

::

    Fri, 7 Jun 2024 02:58:35 GMT
    Jianbo Dong, Bin Luo, Jun Zhang, Pengcheng Zhang, Fei Feng, Yikai Zhu, Ang Liu, Zian Chen, Yi Shi, Hairong Jiao, Gang Lu, Yu Guan, Ennan Zhai, Wencong Xiao, Hanyu Zhao, Man Yuan, Siran Yang, Xiang Li, Jiamang Wang, Rui Men, Jianwei Zhang, Huang Zhong, Dennis Cai, Yuan Xie, Binzhang Fu

The emergence of Large Language Models (LLMs) has necessitated the adoption of parallel training techniques, involving the deployment of thousands of GPUs to train a single model. Unfortunately, we have found that the efficiency of current parallel training is often suboptimal, largely due to the following two main issues. Firstly, hardware failures are inevitable, leading to interruptions in the training tasks. The inability to quickly identify the faulty components results in a substantial waste of GPU resources. Secondly, since GPUs must wait for parameter synchronization to complete before proceeding to the next round of computation, network congestions can greatly increase the waiting time for GPUs. To address these challenges, this paper introduces a communication-driven solution, namely the C4. The key insights of C4 are two folds. First, in parallel training, collective communication exhibits periodic and homogeneous characteristics, so any anomalies are certainly due to some form of hardware malfunction. By leveraging this feature, C4 can rapidly identify the faulty components, swiftly isolate the anomaly, and restart the task, thereby avoiding resource wastage caused by delays in anomaly detection. Second, the predictable communication model of collective communication, involving few large flows, allows C4 to efficiently execute traffic planning, substantially reducing network congestion. C4 has been extensively implemented across our production systems, cutting error-induced overhead by roughly 30% and enhancing runtime performance by about 15% for certain applications with moderate communication costs.

------------

`[2406.04693] LLM-Vectorizer: LLM-based Verified Loop Vectorizer <https://arxiv.org/abs/2406.04693>`__ LLM-Vectorizer:基于llm验证的循环向量化器

::

    Fri, 7 Jun 2024 07:04:26 GMT
    Jubi Taneja, Avery Laird, Cong Yan, Madan Musuvathi, Shuvendu K. Lahiri

Vectorization is a powerful optimization technique that significantly boosts the performance of high performance computing applications operating on large data arrays. Despite decades of research on auto-vectorization, compilers frequently miss opportunities to vectorize code. On the other hand, writing vectorized code manually using compiler intrinsics is still a complex, error-prone task that demands deep knowledge of specific architecture and compilers.
In this paper, we evaluate the potential of large-language models (LLMs) to generate vectorized (Single Instruction Multiple Data) code from scalar programs that process individual array elements. We propose a novel finite-state machine multi-agents based approach that harnesses LLMs and test-based feedback to generate vectorized code. Our findings indicate that LLMs are capable of producing high performance vectorized code with run-time speedup ranging from 1.1x to 9.4x as compared to the state-of-the-art compilers such as Intel Compiler, GCC, and Clang.
To verify the correctness of vectorized code, we use Alive2, a leading bounded translation validation tool for LLVM IR. We describe a few domain-specific techniques to improve the scalability of Alive2 on our benchmark dataset. Overall, our approach is able to verify 38.2% of vectorizations as correct on the TSVC benchmark dataset.

------------

`[2406.04710] Morescient GAI for Software Engineering <https://arxiv.org/abs/2406.04710>`__ 软件工程领域更先进的GAI

::

    Fri, 7 Jun 2024 07:38:33 GMT
    Marcus Kessel, Colin Atkinson

The ability of Generative AI (GAI) technology to automatically check, synthesize and modify software engineering artifacts promises to revolutionize all aspects of software engineering. Using GAI for software engineering tasks is consequently one of the most rapidly expanding fields of software engineering research, with dozens of LLM-based code models having been published since 2021. However, the overwhelming majority of existing code models share a major weakness - they are exclusively trained on the syntactic facet of software, significantly lowering their trustworthiness in tasks dependent on software semantics. To address this problem, a new class of "Morescient" GAI is needed that is "aware" of (i.e., trained on) both the semantic and static facets of software. This, in turn, will require a new generation of software observation platforms capable of generating ultra-large quantities of execution observations in a structured and readily analyzable way. In this paper, we present a vision for how such "Morescient" GAI models can be engineered, evolved and disseminated according to the principles of open science.

------------

`[2406.04755] Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations <https://arxiv.org/abs/2406.04755>`__ 销售呓语:对LLM品牌推荐的不引人注目的攻击

::

    Fri, 7 Jun 2024 08:54:55 GMT
    Weiran Lin, Anna Gerchanovsky, Omer Akgul, Lujo Bauer, Matt Fredrikson, Zifan Wang

Large language model (LLM) users might rely on others (e.g., prompting services), to write prompts. However, the risks of trusting prompts written by others remain unstudied. In this paper, we assess the risk of using such prompts on brand recommendation tasks when shopping. First, we found that paraphrasing prompts can result in LLMs mentioning given brands with drastically different probabilities, including a pair of prompts where the probability changes by 100%. Next, we developed an approach that can be used to perturb an original base prompt to increase the likelihood that an LLM mentions a given brand. We designed a human-inconspicuous algorithm that perturbs prompts, which empirically forces LLMs to mention strings related to a brand more often, by absolute improvements up to 78.3%. Our results suggest that our perturbed prompts, 1) are inconspicuous to humans, 2) force LLMs to recommend a target brand more often, and 3) increase the perceived chances of picking targeted brands.

------------

`[2406.05132] 3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs <https://arxiv.org/abs/2406.05132>`__ 

::

    Fri, 7 Jun 2024 17:59:59 GMT
    Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, David F. Fouhey, Joyce Chai

The integration of language and 3D perception is crucial for developing embodied agents and robots that comprehend and interact with the physical world. While large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, their adaptation to 3D environments (3D-LLMs) remains in its early stages. A primary challenge is the absence of large-scale datasets that provide dense grounding between language and 3D scenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset comprising 40,087 household scenes paired with 6.2 million densely-grounded scene-language instructions. Our results show that instruction tuning with 3D-GRAND significantly enhances grounding capabilities and reduces hallucinations in 3D-LLMs. As part of our contributions, we propose a comprehensive benchmark 3D-POPE to systematically evaluate hallucination in 3D-LLMs, enabling fair comparisons among future models. Our experiments highlight a scaling effect between dataset size and 3D-LLM performance, emphasizing the critical role of large-scale 3D-text datasets in advancing embodied AI research. Notably, our results demonstrate early signals for effective sim-to-real transfer, indicating that models trained on large synthetic data can perform well on real-world 3D scans. Through 3D-GRAND and 3D-POPE, we aim to equip the embodied AI community with essential resources and insights, setting the stage for more reliable and better-grounded 3D-LLMs.
Project website: https://3d-grand.github.io

------------

`[2406.04927] LLM-based speaker diarization correction: A generalizable approach <https://arxiv.org/abs/2406.04927>`__ 基于llm的说话人分级校正:一种通用方法

::

    Fri, 7 Jun 2024 13:33:22 GMT
    Georgios Efstathiadis, Vijay Yadav, Anzar Abbas

Speaker diarization is necessary for interpreting conversations transcribed using automated speech recognition (ASR) tools. Despite significant developments in diarization methods, diarization accuracy remains an issue.
Here, we investigate the use of large language models (LLMs) for diarization correction as a post-processing step. LLMs were fine-tuned using the Fisher corpus, a large dataset of transcribed conversations. The ability of the models to improve diarization accuracy in a holdout dataset was measured. We report that fine-tuned LLMs can markedly improve diarization accuracy. However, model performance is constrained to transcripts produced using the same ASR tool as the transcripts used for fine-tuning, limiting generalizability. To address this constraint, an ensemble model was developed by combining weights from three separate models, each fine-tuned using transcripts from a different ASR tool. The ensemble model demonstrated better overall performance than each of the ASR-specific models, suggesting that a generalizable and ASR-agnostic approach may be achievable. We hope to make these models accessible through public-facing APIs for use by third-party applications.

------------

`[2406.05039] Bootstrapping Referring Multi-Object Tracking <https://arxiv.org/abs/2406.05039>`__ Bootstrapping参考多目标跟踪

::

    Fri, 7 Jun 2024 16:02:10 GMT
    Yani Zhang and Dongming Wu and Wencheng Han and Xingping Dong

Referring multi-object tracking (RMOT) aims at detecting and tracking multiple objects following human instruction represented by a natural language expression. Existing RMOT benchmarks are usually formulated through manual annotations, integrated with static regulations. This approach results in a dearth of notable diversity and a constrained scope of implementation. In this work, our key idea is to bootstrap the task of referring multi-object tracking by introducing discriminative language words as much as possible. In specific, we first develop Refer-KITTI into a large-scale dataset, named Refer-KITTI-V2.
It starts with 2,719 manual annotations, addressing the issue of class imbalance and introducing more keywords to make it closer to real-world scenarios compared to Refer-KITTI. They are further expanded to a total of 9,758 annotations by prompting large language models, which create 617 different words, surpassing previous RMOT benchmarks. In addition, the end-to-end framework in RMOT is also bootstrapped by a simple yet elegant temporal advancement strategy, which achieves better performance than previous approaches. The source code and dataset is available at https://github.com/zyn213/TempRMOT.

------------

`[2406.04348] Gaining Insights into Group-Level Course Difficulty via Differential Course Functioning <https://arxiv.org/abs/2406.04348>`__ 通过差异课程功能获得群体水平课程难度的见解

::

    Tue, 7 May 2024 14:19:11 GMT
    Frederik Baucks, Robin Schmucker, Conrad Borchers, Zachary A. Pardos and Laurenz Wiskott

Curriculum Analytics (CA) studies curriculum structure and student data to ensure the quality of educational programs. One desirable property of courses within curricula is that they are not unexpectedly more difficult for students of different backgrounds. While prior work points to likely variations in course difficulty across student groups, robust methodologies for capturing such variations are scarce, and existing approaches do not adequately decouple course-specific difficulty from students' general performance levels. The present study introduces Differential Course Functioning (DCF) as an Item Response Theory (IRT)-based CA methodology. DCF controls for student performance levels and examines whether significant differences exist in how distinct student groups succeed in a given course. Leveraging data from over 20,000 students at a large public university, we demonstrate DCF's ability to detect inequities in undergraduate course difficulty across student groups described by grade achievement. We compare major pairs with high co-enrollment and transfer students to their non-transfer peers. For the former, our findings suggest a link between DCF effect sizes and the alignment of course content to student home department motivating interventions targeted towards improving course preparedness. For the latter, results suggest minor variations in course-specific difficulty between transfer and non-transfer students. While this is desirable, it also suggests that interventions targeted toward mitigating grade achievement gaps in transfer students should encompass comprehensive support beyond enhancing preparedness for individual courses. By providing more nuanced and equitable assessments of academic performance and difficulties experienced by diverse student populations, DCF could support policymakers, course articulation officers, and student advisors.

------------

`[2406.04857] A Near-Linear Time Approximation Algorithm for Beyond-Worst-Case Graph Clustering <https://arxiv.org/abs/2406.04857>`__ 超最差情况图聚类的近似线性时间算法

::

    Fri, 7 Jun 2024 11:40:54 GMT
    Vincent Cohen-Addad, Tommaso d'Orsi, Aida Mousavifar

We consider the semi-random graph model of [Makarychev, Makarychev and Vijayaraghavan, STOC'12], where, given a random bipartite graph with $\alpha$ edges and an unknown bipartition $(A, B)$ of the vertex set, an adversary can add arbitrary edges inside each community and remove arbitrary edges from the cut $(A, B)$ (i.e. all adversarial changes are \textit{monotone} with respect to the bipartition). For this model, a polynomial time algorithm is known to approximate the Balanced Cut problem up to value $O(\alpha)$ [MMV'12] as long as the cut $(A, B)$ has size $\Omega(\alpha)$. However, it consists of slow subroutines requiring optimal solutions for logarithmically many semidefinite programs. We study the fine-grained complexity of the problem and present the first near-linear time algorithm that achieves similar performances to that of [MMV'12]. Our algorithm runs in time $O(|V(G)|^{1+o(1)} + |E(G)|^{1+o(1)})$ and finds a balanced cut of value $O(\alpha)$. Our approach appears easily extendible to related problem, such as Sparsest Cut, and also yields an near-linear time $O(1)$-approximation to Dagupta's objective function for hierarchical clustering [Dasgupta, STOC'16] for the semi-random hierarchical stochastic block model inputs of [Cohen-Addad, Kanade, Mallmann-Trenn, Mathieu, JACM'19].

------------

`[2308.05201] "Generate" the Future of Work through AI: Empirical Evidence from Online Labor Markets <https://arxiv.org/abs/2308.05201>`__ 

::

    replaced with revised version Thu, 6 Jun 2024 22:23:57 GMT
    Submission history From: Xingchen Xu [view email]
    [v1] Wed, 9 Aug 2023 19:45:00 UTC (212 KB)
    [v2] Thu, 6 Jun 2024 22:23:57 UTC (2,024 KB)
    Jin Liu (1), Xingchen Xu (2), Xi Nan (2), Yongjun Li (1) and Yong Tan (2) ((1) University of Science and Technology of China, (2) University of Washington)

Large Language Model (LLM) based generative AI, such as ChatGPT, is considered the first generation of Artificial General Intelligence (AGI), exhibiting zero-shot learning abilities for a wide variety of downstream tasks. Due to its general-purpose and emergent nature, its impact on labor dynamics becomes complex and difficult to anticipate. Leveraging an extensive dataset from a prominent online labor market, we uncover a post-ChatGPT decline in labor demand, supply, and transactions for submarkets pertaining to text-related and programming-related jobs, in comparison to those not directly exposed to ChatGPT's core functionalities. Meanwhile, these affected submarkets exhibit a discernible increase in the complexity of the remaining jobs and a heightened level of competition among freelancers. Intriguingly, our findings indicate that the diminution in the labor supply pertaining to programming is comparatively less pronounced, a phenomenon ascribed to the transition of freelancers previously engaged in text-related tasks now bidding for programming-related opportunities. Although the per-period job diversity freelancers apply for tends to be more limited, those who successfully navigate skill transitions from text to programming demonstrate greater resilience to ChatGPT's overall market contraction impact. As AI becomes increasingly versatile and potent, our paper offers crucial insights into AI's influence on labor markets and individuals' reactions, underscoring the necessity for proactive interventions to address the challenges and opportunities presented by this transformative technology.

------------

`[2312.11111] The Good, The Bad, and Why: Unveiling Emotions in Generative AI <https://arxiv.org/abs/2312.11111>`__ 好的，坏的和为什么:揭示生成AI中的情感

::

    replaced with revised version Fri, 7 Jun 2024 09:25:31 GMT
    Submission history From: Jindong Wang [view email]
    [v1] Mon, 18 Dec 2023 11:19:45 UTC (2,022 KB)
    [v2] Tue, 19 Dec 2023 04:27:47 UTC (2,022 KB)
    [v3] Fri, 7 Jun 2024 09:25:31 UTC (3,074 KB)
    Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Xinyi Wang, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, Xing Xie

Emotion significantly impacts our daily behaviors and interactions. While recent generative AI models, such as large language models, have shown impressive performance in various tasks, it remains unclear whether they truly comprehend emotions. This paper aims to address this gap by incorporating psychological theories to gain a holistic understanding of emotions in generative AI models. Specifically, we propose three approaches: 1) EmotionPrompt to enhance AI model performance, 2) EmotionAttack to impair AI model performance, and 3) EmotionDecode to explain the effects of emotional stimuli, both benign and malignant. Through extensive experiments involving language and multi-modal models on semantic understanding, logical reasoning, and generation tasks, we demonstrate that both textual and visual EmotionPrompt can boost the performance of AI models while EmotionAttack can hinder it. Additionally, EmotionDecode reveals that AI models can comprehend emotional stimuli akin to the mechanism of dopamine in the human brain. Our work heralds a novel avenue for exploring psychology to enhance our understanding of generative AI models.

------------

`[2302.12246] Active Prompting with Chain-of-Thought for Large Language Models <https://arxiv.org/abs/2302.12246>`__ 基于思维链的大型语言模型主动提示

::

    replaced with revised version Fri, 7 Jun 2024 02:51:25 GMT
    Submission history From: Shizhe Diao [view email]
    [v1] Thu, 23 Feb 2023 18:58:59 UTC (511 KB)
    [v2] Sun, 26 Feb 2023 15:18:50 UTC (511 KB)
    [v3] Tue, 23 May 2023 15:43:28 UTC (511 KB)
    [v4] Fri, 7 Jun 2024 02:51:25 UTC (705 KB)
    Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang

The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at this https URL.

------------

`[2306.11825] DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models <https://arxiv.org/abs/2306.11825>`__ 

::

    replaced with revised version Thu, 6 Jun 2024 21:01:09 GMT
    Submission history From: Sidi Lu [view email]
    [v1] Tue, 20 Jun 2023 18:36:52 UTC (123 KB)
    [v2] Thu, 6 Jun 2024 21:01:09 UTC (422 KB)
    Sidi Lu and Wenbo Zhao and Chenyang Tao and Arpit Gupta and Shanchan Wu and Tagyoung Chung and Nanyun Peng

NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable generation with large language models. It is designed to avoid catastrophic forgetting while achieving guaranteed convergence to an entropy-maximized closed-form optimal solution with reasonable modeling capacity. Despite the success, several challenges arise when apply NADO to a wide range of scenarios. Vanilla NADO suffers from gradient vanishing for low-probability control signals and is highly reliant on a regularization to satisfy the stochastic version of Bellman equation. In addition, the vanilla implementation of NADO introduces a few additional transformer layers, suffering from a limited capacity especially compared to other finetune-based model adaptation methods like LoRA. In this paper, we propose a improved version of the NADO algorithm, namely DiNADO (norm-Disentangled NeurAlly-Decomposed Oracles), which improves the performance of the NADO algorithm through disentangling the step-wise global norm over the approximated oracle $R$-value for all potential next-tokens, allowing DiNADO to be combined with finetuning methods like LoRA. We discuss in depth how DiNADO achieves better capacity, stability and flexibility with both empirical and theoretical results. Experiments on formality control in machine translation and the lexically constrained generation task CommonGen demonstrates the significance of the improvements.

------------

`[2306.11879] Open-Domain Text Evaluation via Contrastive Distribution Methods <https://arxiv.org/abs/2306.11879>`__ 基于对比分布方法的开放域文本评价

::

    replaced with revised version Thu, 6 Jun 2024 21:24:17 GMT
    Submission history From: Sidi Lu [view email]
    [v1] Tue, 20 Jun 2023 20:37:54 UTC (2,259 KB)
    [v2] Fri, 3 May 2024 23:21:45 UTC (2,375 KB)
    [v3] Thu, 6 Jun 2024 21:24:17 UTC (2,375 KB)
    Sidi Lu and Hongyi Liu and Asli Celikyilmaz and Tianlu Wang and Nanyun Peng

Recent advancements in open-domain text generation, driven by the power of large pre-trained language models (LLMs), have demonstrated remarkable performance. However, assessing these models' generation quality remains a challenge. In this paper, we introduce a novel method for evaluating open-domain text generation called Contrastive Distribution Methods (CDM). Leveraging the connection between increasing model parameters and enhanced LLM performance, CDM creates a mapping from the _contrast_ of two probabilistic distributions -- one known to be superior to the other -- to quality measures. We investigate CDM for open-domain text generation evaluation under two paradigms: 1) _Generative_ CDM, which harnesses the contrast of two language models' distributions to generate synthetic examples for training discriminator-based metrics; 2) _Discriminative_ CDM, which directly uses distribution disparities between two language models for evaluation. Our experiments on coherence evaluation for multi-turn dialogue and commonsense evaluation for controllable generation demonstrate CDM's superior correlate with human judgment than existing automatic evaluation metrics, highlighting the strong performance and generalizability of our approach.

------------

`[2310.04799] Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages <https://arxiv.org/abs/2310.04799>`__ 聊天向量:为llm配备新语言中的指令遵循和模型对齐的简单方法

::

    replaced with revised version Fri, 7 Jun 2024 06:28:05 GMT
    Submission history From: Shih-Cheng Huang [view email]
    [v1] Sat, 7 Oct 2023 13:34:21 UTC (3,479 KB)
    [v2] Wed, 6 Mar 2024 15:50:02 UTC (12,239 KB)
    [v3] Fri, 7 Jun 2024 06:28:05 UTC (8,927 KB)
    Shih-Cheng Huang, Pin-Zu Li, Yu-Chi Hsu, Kuang-Ming Chen, Yu Tung Lin, Shih-Kai Hsiao, Richard Tzong-Han Tsai, Hung-yi Lee

Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of $\textit{chat vector}$ to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model's weights, we can endow the model with chat capabilities in new languages without the need for further training. Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our approach, we extend our experiments to encompass various languages, base models, and chat vectors. The results underscore the chat vector's simplicity, effectiveness, and wide applicability, making it a compelling solution for efficiently enabling conversational capabilities in pre-trained language models. Our code is available at this https URL.

------------

`[2310.05492] How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition <https://arxiv.org/abs/2310.05492>`__ 大型语言模型的能力如何受到监督微调数据组合的影响

::

    replaced with revised version Fri, 7 Jun 2024 15:51:08 GMT
    Submission history From: Guanting Dong [view email]
    [v1] Mon, 9 Oct 2023 07:56:16 UTC (852 KB)
    [v2] Wed, 1 Nov 2023 07:11:37 UTC (852 KB)
    [v3] Fri, 19 Jan 2024 06:06:46 UTC (917 KB)
    [v4] Fri, 7 Jun 2024 15:51:08 UTC (929 KB)
    Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou

Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specifically focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.

------------

`[2310.15123] Branch-Solve-Merge Improves Large Language Model Evaluation and Generation <https://arxiv.org/abs/2310.15123>`__ 分支求解-合并改进了大型语言模型的评估和生成

::

    replaced with revised version Fri, 7 Jun 2024 16:08:49 GMT
    Submission history From: Swarnadeep Saha [view email]
    [v1] Mon, 23 Oct 2023 17:29:48 UTC (583 KB)
    [v2] Fri, 7 Jun 2024 16:08:49 UTC (8,235 KB)
    Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, Xian Li

Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria. However, their performance can fall short, due to the model's lack of coherence and inability to plan and decompose the problem. We propose Branch-Solve-Merge (BSM), a Large Language Model program (Schlag et al., 2023) for tackling such challenging natural language tasks. It consists of branch, solve, and merge modules that are parameterized with specific prompts to the base LLM. These three modules plan a decomposition of the task into multiple parallel sub-tasks, independently solve them, and fuse the solutions to the sub-tasks. We apply our method to the tasks of LLM response evaluation and constrained text generation and evaluate its effectiveness with multiple LLMs, including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to 26%, reducing length and pairwise position biases by up to 50%, and allowing LLaMA2-chat to match or outperform GPT-4 on most domains. On a constraint story generation task, BSM improves the coherence of stories while also improving constraint satisfaction by 12%.

------------

`[2310.19975] BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing <https://arxiv.org/abs/2310.19975>`__ BioInstruct:面向生物医学自然语言处理的大型语言模型指令调优

::

    replaced with revised version Thu, 6 Jun 2024 21:16:12 GMT
    Submission history From: Hieu Tran [view email]
    [v1] Mon, 30 Oct 2023 19:38:50 UTC (234 KB)
    [v2] Mon, 6 Nov 2023 15:05:34 UTC (233 KB)
    [v3] Thu, 6 Jun 2024 21:16:12 UTC (237 KB)
    Hieu Tran, Zhichao Yang, Zonghai Yao, Hong Yu

To enhance the performance of large language models (LLMs) in biomedical natural language processing (BioNLP) by introducing a domain-specific instruction dataset and examining its impact when combined with multi-task learning principles. We created the BioInstruct, comprising 25,005 instructions to instruction-tune LLMs(LLaMA 1 & 2, 7B & 13B version). The instructions were created by prompting the GPT-4 language model with three-seed samples randomly drawn from an 80 human curated instructions. We employed Low-Rank Adaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these instruction-tuned LLMs on several BioNLP tasks, which can be grouped into three major categories: question answering(QA), information extraction(IE), and text generation(GEN). We also examined whether categories(e.g., QA, IE, and generation) of instructions impact model performance. Comparing with LLMs without instruction-tuned, our instruction-tuned LLMs demonstrated marked performance gains: 17.3% in QA, 5.7% in IE, and 96% in Generation tasks. Our 7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed other LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with vast domain-specific data or a variety of tasks. Our results also show that the performance gain is significantly higher when instruction fine-tuning is conducted with closely related tasks. Our findings align with the observations of multi-task learning, suggesting the synergies between two tasks. The BioInstruct dataset serves as a valuable resource and instruction tuned LLMs lead to the best performing BioNLP applications.

------------

`[2311.08374] A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts <https://arxiv.org/abs/2311.08374>`__ 

::

    replaced with revised version Thu, 6 Jun 2024 23:14:32 GMT
    Submission history From: Saranya Venkatraman [view email]
    [v1] Tue, 14 Nov 2023 18:40:42 UTC (2,360 KB)
    [v2] Thu, 6 Jun 2024 23:14:32 UTC (2,807 KB)
    Nafis Irtiza Tripto, Saranya Venkatraman, Dominik Macko, Robert Moro, Ivan Srba, Adaku Uchendu, Thai Le, Dongwon Lee

In the realm of text manipulation and linguistic transformation, the question of authorship has been a subject of fascination and philosophical inquiry. Much like the Ship of Theseus paradox, which ponders whether a ship remains the same when each of its original planks is replaced, our research delves into an intriguing question: Does a text retain its original authorship when it undergoes numerous paraphrasing iterations? Specifically, since Large Language Models (LLMs) have demonstrated remarkable proficiency in both the generation of original content and the modification of human-authored texts, a pivotal question emerges concerning the determination of authorship in instances where LLMs or similar paraphrasing tools are employed to rephrase the text--i.e., whether authorship should be attributed to the original human author or the AI-powered tool. Therefore, we embark on a philosophical voyage through the seas of language and authorship to unravel this intricate puzzle. Using a computational approach, we discover that the diminishing performance in text classification models, with each successive paraphrasing iteration, is closely associated with the extent of deviation from the original author's style, thus provoking a reconsideration of the current notion of authorship.

------------

`[2311.09204] Fusion-Eval: Integrating Assistant Evaluators with LLMs <https://arxiv.org/abs/2311.09204>`__ Fusion-Eval:将辅助评估器与llm集成

::

    replaced with revised version Thu, 6 Jun 2024 22:47:44 GMT
    Submission history From: Lei Shu [view email]
    [v1] Wed, 15 Nov 2023 18:46:56 UTC (29 KB)
    [v2] Fri, 16 Feb 2024 05:05:56 UTC (108 KB)
    [v3] Thu, 6 Jun 2024 22:47:44 UTC (148 KB)
    Lei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong Chen, Lei Meng

Evaluating natural language systems poses significant challenges, particularly in the realms of natural language understanding and high-level reasoning. In this paper, we introduce 'Fusion-Eval', an innovative approach that leverages Large Language Models (LLMs) to integrate insights from various assistant evaluators. The LLM is given the example to evaluate along with scores from the assistant evaluators. Each of these evaluators specializes in assessing distinct aspects of responses. Fusion-Eval achieves a 0.962 system-level Kendall-Tau correlation with humans on SummEval and a 0.744 turn-level Spearman correlation on TopicalChat, which is significantly higher than baseline methods. These results highlight Fusion-Eval's significant potential in the realm of natural language system evaluation.

------------

`[2311.09677] R-Tuning: Instructing Large Language Models to Say `I Don't Know' <https://arxiv.org/abs/2311.09677>`__ r调优:指示大型语言模型说“我不知道”

::

    replaced with revised version Fri, 7 Jun 2024 02:46:36 GMT
    Submission history From: Shizhe Diao [view email]
    [v1] Thu, 16 Nov 2023 08:45:44 UTC (25,842 KB)
    [v2] Sun, 5 May 2024 13:00:14 UTC (9,807 KB)
    [v3] Fri, 7 Jun 2024 02:46:36 UTC (9,806 KB)
    Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, Tong Zhang

Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the disparity in knowledge encompassed by pre-trained parameters compared to that of instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate R-Tuning effectively improves a model's ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty results in better calibration and an improved ability to estimate the uncertainty than uncertainty-based testing. Our code is available at this https URL.

------------

`[2311.09766] LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores <https://arxiv.org/abs/2311.09766>`__ llm作为自恋评估者:当自我膨胀评估分数

::

    replaced with revised version Fri, 7 Jun 2024 09:41:36 GMT
    Submission history From: Yiqi Liu [view email]
    [v1] Thu, 16 Nov 2023 10:43:26 UTC (1,337 KB)
    [v2] Sat, 17 Feb 2024 16:16:10 UTC (3,739 KB)
    [v3] Tue, 20 Feb 2024 17:21:51 UTC (3,739 KB)
    [v4] Fri, 7 Jun 2024 09:41:36 UTC (3,741 KB)
    Yiqi Liu, Nafise Sadat Moosavi, Chenghua Lin

Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in a reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generative evaluation models can be influenced by factors beyond the inherent text quality, highlighting the necessity of developing more reliable evaluation protocols in the future.

------------

`[2311.17438] CLOMO: Counterfactual Logical Modification with Large Language Models <https://arxiv.org/abs/2311.17438>`__ CLOMO:基于大型语言模型的反事实逻辑修改

::

    replaced with revised version Fri, 7 Jun 2024 04:27:54 GMT
    Submission history From: Yinya Huang [view email]
    [v1] Wed, 29 Nov 2023 08:29:54 UTC (4,139 KB)
    [v2] Thu, 30 Nov 2023 04:23:58 UTC (4,138 KB)
    [v3] Wed, 14 Feb 2024 05:53:40 UTC (6,999 KB)
    [v4] Fri, 7 Jun 2024 04:27:54 UTC (3,318 KB)
    Yinya Huang, Ruixin Hong, Hongming Zhang, Wei Shao, Zhicheng Yang, Dong Yu, Changshui Zhang, Xiaodan Liang, Linqi Song

In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the decomposed Self-Evaluation Score (SES) to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance. Code and data are available at this https URL.

------------

`[2312.02120] Magicoder: Empowering Code Generation with OSS-Instruct <https://arxiv.org/abs/2312.02120>`__ Magicoder:用OSS-Instruct实现代码生成

::

    replaced with revised version Fri, 7 Jun 2024 02:50:56 GMT
    Submission history From: Yuxiang Wei [view email]
    [v1] Mon, 4 Dec 2023 18:50:35 UTC (1,322 KB)
    [v2] Fri, 7 Jun 2024 02:50:56 UTC (1,485 KB)
    Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang

We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.

------------

`[2312.06924] Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack <https://arxiv.org/abs/2312.06924>`__ NLP任务中的安全对齐:作为上下文攻击的弱对齐摘要

::

    replaced with revised version Thu, 6 Jun 2024 22:58:32 GMT
    Submission history From: Yu Fu [view email]
    [v1] Tue, 12 Dec 2023 01:39:29 UTC (9,091 KB)
    [v2] Thu, 6 Jun 2024 22:58:32 UTC (9,392 KB)
    Yu Fu, Yufei Li, Wen Xiao, Cong Liu, Yue Dong

Recent developments in balancing the usefulness and safety of Large Language Models (LLMs) have raised a critical question: Are mainstream NLP tasks adequately aligned with safety consideration? Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks. For instance, LLMs can effectively summarize malicious long documents but often refuse to translate them. This discrepancy highlights a previously unidentified vulnerability: attacks exploiting tasks with weaker safety alignment, like summarization, can potentially compromise the integrity of tasks traditionally deemed more robust, such as translation and question-answering (QA). Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content. We demonstrate these vulnerabilities in various safety-aligned LLMs, particularly Llama2 models, Gemini and GPT-4, indicating an urgent need for strengthening safety alignments across a broad spectrum of NLP tasks.

------------

`[2312.14187] WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning <https://arxiv.org/abs/2312.14187>`__ WaveCoder:通过指令调整广泛通用的代码大型语言模型增强

::

    replaced with revised version Fri, 7 Jun 2024 07:46:28 GMT
    Submission history From: Zhaojian Yu [view email]
    [v1] Wed, 20 Dec 2023 09:02:29 UTC (1,336 KB)
    [v2] Tue, 26 Dec 2023 13:51:38 UTC (1,336 KB)
    [v3] Thu, 11 Jan 2024 07:44:55 UTC (1,463 KB)
    [v4] Wed, 5 Jun 2024 10:06:45 UTC (2,464 KB)
    [v5] Fri, 7 Jun 2024 07:46:28 UTC (2,464 KB)
    Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, Qiufeng Yin

Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeSeaXDataset, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks.

------------

`[2401.02009] Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives <https://arxiv.org/abs/2401.02009>`__ 自我对比:通过不一致的解决视角进行更好的反思

::

    replaced with revised version Thu, 6 Jun 2024 18:46:03 GMT
    Submission history From: Wenqi Zhang [view email]
    [v1] Thu, 4 Jan 2024 00:32:33 UTC (3,093 KB)
    [v2] Wed, 27 Mar 2024 17:24:47 UTC (2,777 KB)
    [v3] Thu, 6 Jun 2024 18:46:03 UTC (2,778 KB)
    Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu

The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can catalyze more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.

------------

`[2401.06102] Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models <https://arxiv.org/abs/2401.06102>`__ Patchscopes:用于检查语言模型隐藏表示的统一框架

::

    replaced with revised version Thu, 6 Jun 2024 22:59:58 GMT
    Submission history From: Asma Ghandeharioun [view email]
    [v1] Thu, 11 Jan 2024 18:33:48 UTC (760 KB)
    [v2] Fri, 12 Jan 2024 17:54:18 UTC (760 KB)
    [v3] Thu, 30 May 2024 02:52:08 UTC (825 KB)
    [v4] Thu, 6 Jun 2024 22:59:58 UTC (826 KB)
    Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, Mor Geva

Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.

------------

`[2402.05629] Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations <https://arxiv.org/abs/2402.05629>`__ 合并事实，制造谬误:评估长期累积的事实主张的矛盾性质

::

    replaced with revised version Fri, 7 Jun 2024 02:28:40 GMT
    Submission history From: Cheng-Han Chiang [view email]
    [v1] Thu, 8 Feb 2024 12:36:29 UTC (1,251 KB)
    [v2] Fri, 23 Feb 2024 11:25:19 UTC (1,136 KB)
    [v3] Mon, 20 May 2024 11:16:38 UTC (1,136 KB)
    [v4] Fri, 7 Jun 2024 02:28:40 UTC (1,146 KB)
    Cheng-Han Chiang, Hung-yi Lee

Long-form generations from large language models (LLMs) contain a mix of factual and non-factual claims, making evaluating factuality difficult. Prior works evaluate the factuality of a long paragraph by decomposing it into multiple facts, verifying those facts independently, and aggregating the results. Such methods assume that combining factual claims forms a factual paragraph. The above assumption can be violated: we show that strong open-source models like Llama-chat can generate paragraphs that contain verifiable facts, but the facts are combined into a non-factual paragraph due to entity ambiguity. We further reveal that existing factuality metrics, including FActScore and citation recall, cannot properly evaluate these non-factual paragraphs and overestimate their factuality. To address this, we introduce an enhanced metric, D-FActScore, specifically designed for content with ambiguous entities. We evaluate the D-FActScores of people biographies generated by retrieval-augmented LLMs. We show that D-FActScore can better assess the factuality of paragraphs with entity ambiguity than FActScore. We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs, making their D-FActScore much lower than FActScore by over 10%.

------------

`[2402.10137] TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles <https://arxiv.org/abs/2402.10137>`__ TOAD:具有不同回应风格的任务导向型自动对话

::

    replaced with revised version Thu, 6 Jun 2024 20:18:11 GMT
    Submission history From: Yinhong Liu [view email]
    [v1] Thu, 15 Feb 2024 17:40:02 UTC (8,193 KB)
    [v2] Fri, 16 Feb 2024 10:57:32 UTC (8,193 KB)
    [v3] Thu, 6 Jun 2024 20:18:11 UTC (8,190 KB)
    Yinhong Liu, Yimai Fang, David Vandyke and Nigel Collier

In light of recent advances in large language models (LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios. However, the creation of high-quality annotated data for Task-Oriented Dialog (TOD) is recognized to be slow and costly. To address these challenges, we introduce Task-Oriented Automatic Dialogs (TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline. The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options. Two aspects of system response styles are considered, verbosity level and users' expression mirroring. We benchmark TOAD on two response generation tasks, and the results show that modeling more verbose responses or responses without user expression mirroring is more challenging.

------------

`[2402.10412] Measuring and Reducing LLM Hallucination without Gold-Standard Answers <https://arxiv.org/abs/2402.10412>`__ 在没有黄金标准答案的情况下测量和减少LLM幻觉

::

    replaced with revised version Thu, 6 Jun 2024 18:33:02 GMT
    Submission history From: Jiaheng Wei [view email]
    [v1] Fri, 16 Feb 2024 02:32:06 UTC (561 KB)
    [v2] Thu, 6 Jun 2024 18:33:02 UTC (310 KB)
    Jiaheng Wei, Yuanshun Yao, Jean-Francois Ton, Hongyi Guo, Andrew Estornell, Yang Liu

LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require having a benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers written by humans. Such requirements make hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), an innovative hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accurate hallucination measures than naively using reference LLMs. We also show how to leverage FEWL to reduce hallucination through both in-context learning and supervised fine-tuning. Extensive experiment results on Truthful-QA, CHALE, and HaluEval datasets demonstrate the effectiveness of FEWL.

------------

`[2402.10586] Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs <https://arxiv.org/abs/2402.10586>`__ 细枝末节:通过话语母题检测机器生成的文本

::

    replaced with revised version Thu, 6 Jun 2024 20:04:52 GMT
    Submission history From: Zae Myung Kim [view email]
    [v1] Fri, 16 Feb 2024 11:20:30 UTC (10,155 KB)
    [v2] Thu, 6 Jun 2024 20:04:52 UTC (10,136 KB)
    Zae Myung Kim and Kwang Hee Lee and Preston Zhu and Vipul Raheja and Dongyeop Kang

With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers' overall performance in distinguishing between human-written and machine-generated texts, even on out-of-distribution and paraphrased samples. This underscores the significance of incorporating hierarchical discourse features in the analysis of text patterns. The code and dataset are available at this https URL.

------------

`[2402.10671] Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm <https://arxiv.org/abs/2402.10671>`__ 增强注意力的分解:通过工作流范式改进基于llm的Text-to-SQL

::

    replaced with revised version Fri, 7 Jun 2024 09:45:21 GMT
    Submission history From: Yuanzhen Xie [view email]
    [v1] Fri, 16 Feb 2024 13:24:05 UTC (4,156 KB)
    [v2] Fri, 7 Jun 2024 09:45:21 UTC (4,299 KB)
    Yuanzhen Xie, Xinzhou Jin, Tao Xie, MingXiong Lin, Liang Chen, Chenyun Yu, Lei Cheng, ChengXiang Zhuo, Bo Hu, Zang Li

In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model's attention. Additionally, the inclusion of self-correction and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches. Extensive experiments conducted on three datasets demonstrate that our approach outperforms other methods by a significant margin. About 2-3 percentage point improvements compared to the existing baseline on the Spider Dev, Spider-Realistic, and Bird Dev datasets and new SOTA results on the Spider Test dataset are achieved. Our code is available on GitHub: \url{this https URL}.

------------

`[2402.11655] Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals <https://arxiv.org/abs/2402.11655>`__ 机制的竞争:追踪语言模型如何处理事实和反事实

::

    replaced with revised version Thu, 6 Jun 2024 21:45:21 GMT
    Submission history From: Zhijing Jin [view email]
    [v1] Sun, 18 Feb 2024 17:26:51 UTC (1,114 KB)
    [v2] Thu, 6 Jun 2024 21:45:21 UTC (606 KB)
    Francesco Ortu, Zhijing Jin, Diego Doimo, Mrinmaya Sachan, Alberto Cazzaniga, Bernhard Sch\"olkopf

Interpretability research aims to bridge the gap between empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research focuses on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose a formulation of competition of mechanisms, which focuses on the interplay of multiple mechanisms instead of individual mechanisms and traces how one of them becomes dominant in the final prediction. We uncover how and where mechanisms compete within LLMs using two interpretability methods: logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components and reveal attention positions that effectively control the strength of certain mechanisms. Code: this https URL. Data: this https URL.

------------

`[2402.11709] GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network <https://arxiv.org/abs/2402.11709>`__ GNNavi:基于图神经网络的大型语言模型信息流导航

::

    replaced with revised version Fri, 7 Jun 2024 14:36:33 GMT
    Submission history From: Shuzhou Yuan [view email]
    [v1] Sun, 18 Feb 2024 21:13:05 UTC (10,166 KB)
    [v2] Fri, 7 Jun 2024 14:36:33 UTC (10,168 KB)
    Shuzhou Yuan, Ercong Nie, Michael F\"arber, Helmut Schmid, Hinrich Sch\"utze

Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are used. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 show GNNavi surpasses standard prompt-based fine-tuning methods in few-shot settings by updating just 0.2% to 0.5% of parameters. We compare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and Adapter in terms of performance and efficiency. Our analysis reveals that GNNavi enhances information flow and ensures a clear aggregation process.

------------

`[2402.11753] ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs <https://arxiv.org/abs/2402.11753>`__ ArtPrompt:基于ASCII art的针对对齐llm的越狱攻击

::

    replaced with revised version Fri, 7 Jun 2024 17:35:17 GMT
    Submission history From: Luyao Niu [view email]
    [v1] Mon, 19 Feb 2024 00:43:31 UTC (968 KB)
    [v2] Thu, 22 Feb 2024 18:40:03 UTC (968 KB)
    [v3] Fri, 19 Apr 2024 20:59:59 UTC (968 KB)
    [v4] Fri, 7 Jun 2024 17:35:17 UTC (1,896 KB)
    Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran

Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we develop the jailbreak attack ArtPrompt, which leverages the poor performance of LLMs in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from LLMs. ArtPrompt only requires black-box access to the victim LLMs, making it a practical attack. We evaluate ArtPrompt on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five LLMs. Our code is available at this https URL.

------------

`[2402.12071] EmoBench: Evaluating the Emotional Intelligence of Large Language Models <https://arxiv.org/abs/2402.12071>`__ EmoBench:大型语言模型情绪智力评估

::

    replaced with revised version Fri, 7 Jun 2024 07:43:45 GMT
    Submission history From: Sahand Sabour [view email]
    [v1] Mon, 19 Feb 2024 11:48:09 UTC (8,319 KB)
    [v2] Fri, 7 Jun 2024 07:43:45 UTC (8,372 KB)
    Sahand Sabour, Siyang Liu, Zheyuan Zhang, June M. Liu, Jinfeng Zhou, Alvionna S. Sunaryo, Juanzi Li, Tatia M.C. Lee, Rada Mihalcea, Minlie Huang

Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at this https URL.

------------

`[2402.15018] Unintended Impacts of LLM Alignment on Global Representation <https://arxiv.org/abs/2402.15018>`__ LLM对齐对全局表示的意外影响

::

    replaced with revised version Thu, 6 Jun 2024 22:31:48 GMT
    Submission history From: Michael Ryan [view email]
    [v1] Thu, 22 Feb 2024 23:31:22 UTC (3,515 KB)
    [v2] Thu, 6 Jun 2024 22:31:48 UTC (3,869 KB)
    Michael J. Ryan, William Held, Diyi Yang

Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable preference tuning. We make our code and data publicly available on Github.

------------

`[2402.16459] Defending LLMs against Jailbreaking Attacks via Backtranslation <https://arxiv.org/abs/2402.16459>`__ 通过反向翻译防御llm越狱攻击

::

    replaced with revised version Thu, 6 Jun 2024 19:21:44 GMT
    Submission history From: Yihan Wang [view email]
    [v1] Mon, 26 Feb 2024 10:03:33 UTC (6,924 KB)
    [v2] Wed, 28 Feb 2024 22:21:05 UTC (6,924 KB)
    [v3] Thu, 6 Jun 2024 19:21:44 UTC (6,928 KB)
    Yihan Wang, Zhouxing Shi, Andrew Bai, Cho-Jui Hsieh

Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiveness and efficiency. We empirically demonstrate that our defense significantly outperforms the baselines, in the cases that are hard for the baselines, and our defense also has little impact on the generation quality for benign input prompts. Our implementation is based on our library for LLM jailbreaking defense algorithms at \url{this https URL}, and the code for reproducing our experiments is available at \url{this https URL}.

------------

`[2403.03329] Guardrail Baselines for Unlearning in LLMs <https://arxiv.org/abs/2403.03329>`__ llm中忘记学习的护栏基线

::

    replaced with revised version Thu, 6 Jun 2024 19:45:09 GMT
    Submission history From: Pratiksha Thaker [view email]
    [v1] Tue, 5 Mar 2024 21:19:06 UTC (69 KB)
    [v2] Thu, 6 Jun 2024 19:45:09 UTC (77 KB)
    Pratiksha Thaker, Yash Maurya, Shengyuan Hu, Virginia Smith, Zhiwei Steven Wu

Recent work has demonstrated that finetuning is a promising approach to 'unlearn' concepts from large language models. However, finetuning can be expensive, as it requires both generating a set of examples and running iterations of finetuning to update the model. In this work, we show that simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to finetuning. We recommend that researchers investigate these lightweight baselines when evaluating the performance of more computationally intensive finetuning methods. While we do not claim that methods such as prompting or filtering are universal solutions to the problem of unlearning, our work suggests the need for evaluation metrics that can better separate the power of guardrails vs. finetuning, and highlights scenarios where guardrails expose possible unintended behavior in existing metrics and benchmarks.

------------

`[2403.04696] Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification <https://arxiv.org/abs/2403.04696>`__ 事实核查通过标记级不确定性量化的大型语言模型的输出

::

    replaced with revised version Thu, 6 Jun 2024 21:32:39 GMT
    Submission history From: Artem Shelmanov [view email]
    [v1] Thu, 7 Mar 2024 17:44:17 UTC (3,070 KB)
    [v2] Thu, 6 Jun 2024 21:32:39 UTC (6,205 KB)
    Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov

Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.

------------

`[2403.08492] Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking <https://arxiv.org/abs/2403.08492>`__ 丰富语义知识增强的大型语言模型小样本中文拼写检查

::

    replaced with revised version Fri, 7 Jun 2024 08:41:58 GMT
    Submission history From: Yujing Chen [view email]
    [v1] Wed, 13 Mar 2024 12:55:43 UTC (3,120 KB)
    [v2] Fri, 7 Jun 2024 08:41:58 UTC (2,057 KB)
    Ming Dong, Yujing Chen, Miao Zhang, Hao Sun, Tingting He

Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and optical character recognition (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications. In this paper, we explore using an in-context learning method named RS-LLM (Rich Semantic based LLMs) to introduce large language models (LLMs) as the foundation model. Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than the BERT-based model on few-shot CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verified the superiority of our proposed framework.

------------

`[2403.10822] Can Large Language Models abstract Medical Coded Language? <https://arxiv.org/abs/2403.10822>`__ 大型语言模型能抽象医学编码语言吗?

::

    replaced with revised version Thu, 6 Jun 2024 21:58:49 GMT
    Submission history From: Simon A. Lee [view email]
    [v1] Sat, 16 Mar 2024 06:18:15 UTC (281 KB)
    [v2] Thu, 21 Mar 2024 23:47:24 UTC (520 KB)
    [v3] Thu, 6 Jun 2024 21:58:49 UTC (831 KB)
    Simon A. Lee, Timothy Lindsey

Large Language Models (LLMs) have become a pivotal research area, potentially making beneficial contributions in fields like healthcare where they can streamline automated billing and decision support. However, the frequent use of specialized coded languages like ICD-10, which are regularly updated and deviate from natural language formats, presents potential challenges for LLMs in creating accurate and meaningful latent representations. This raises concerns among healthcare professionals about potential inaccuracies or ``hallucinations" that could result in the direct impact of a patient. Therefore, this study evaluates whether large language models (LLMs) are aware of medical code ontologies and can accurately generate names from these codes. We assess the capabilities and limitations of both general and biomedical-specific generative models, such as GPT, LLaMA-2, and Meditron, focusing on their proficiency with domain-specific terminologies. While the results indicate that LLMs struggle with coded language, we offer insights on how to adapt these models to reason more effectively.

------------

`[2404.03189] The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models <https://arxiv.org/abs/2404.03189>`__ 概率也很重要:大型语言模型中自由文本解释的忠实度的更可靠的指标

::

    replaced with revised version Fri, 7 Jun 2024 11:54:44 GMT
    Submission history From: Noah Siegel [view email]
    [v1] Thu, 4 Apr 2024 04:20:04 UTC (860 KB)
    [v2] Fri, 7 Jun 2024 11:54:44 UTC (884 KB)
    Noah Y. Siegel, Oana-Maria Camburu, Nicolas Heess, Maria Perez-Ortiz

In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.

------------

`[2404.15247] XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts <https://arxiv.org/abs/2404.15247>`__ XFT:通过简单地合并升级循环的专家组合来释放代码指令调优的威力

::

    replaced with revised version Thu, 6 Jun 2024 18:18:21 GMT
    Submission history From: Yifeng Ding [view email]
    [v1] Tue, 23 Apr 2024 17:32:24 UTC (250 KB)
    [v2] Thu, 6 Jun 2024 18:18:21 UTC (251 KB)
    Yifeng Ding, Jiawei Liu, Yuxiang Wei, Terry Yue Zhuo, Lingming Zhang

We introduce XFT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs). While vanilla sparse upcycling fails to improve instruction tuning, XFT introduces a shared expert mechanism with a novel routing weight normalization strategy into sparse upcycling, which significantly boosts instruction tuning. After fine-tuning the upcycled MoE model, XFT introduces a learnable model merging mechanism to compile the upcycled MoE model back to a dense model, achieving upcycled MoE-level performance with only dense-model compute. By applying XFT to a 1.3B model, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6 pass@1 on HumanEval and HumanEval+ respectively. With the same data and model architecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+, along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and DS-1000, demonstrating its generalizability. XFT is fully orthogonal to existing techniques such as Evol-Instruct and OSS-Instruct, opening a new dimension for improving code instruction tuning. Codes are available at this https URL.

------------

`[2404.15485] Evaluating the Efficacy of Large Language Models in Identifying Phishing Attempts <https://arxiv.org/abs/2404.15485>`__ 评估大型语言模型识别网络钓鱼企图的有效性

::

    replaced with revised version Thu, 6 Jun 2024 21:03:03 GMT
    Submission history From: Het Patel Mr [view email]
    [v1] Tue, 23 Apr 2024 19:55:18 UTC (1,269 KB)
    [v2] Tue, 4 Jun 2024 17:37:08 UTC (729 KB)
    [v3] Thu, 6 Jun 2024 21:03:03 UTC (648 KB)
    Het Patel, Umair Rehman, Farkhund Iqbal

Phishing, a prevalent cybercrime tactic for decades, remains a significant threat in today's digital world. By leveraging clever social engineering elements and modern technology, cybercrime targets many individuals, businesses, and organizations to exploit trust and security. These cyber-attackers are often disguised in many trustworthy forms to appear as legitimate sources. By cleverly using psychological elements like urgency, fear, social proof, and other manipulative strategies, phishers can lure individuals into revealing sensitive and personalized information. Building on this pervasive issue within modern technology, this paper aims to analyze the effectiveness of 15 Large Language Models (LLMs) in detecting phishing attempts, specifically focusing on a randomized set of "419 Scam" emails. The objective is to determine which LLMs can accurately detect phishing emails by analyzing a text file containing email metadata based on predefined criteria. The experiment concluded that the following models, ChatGPT 3.5, GPT-3.5-Turbo-Instruct, and ChatGPT, were the most effective in detecting phishing emails.

------------

`[2405.00301] Enhanced Language Model Truthfulness with Learnable Intervention and Uncertainty Expression <https://arxiv.org/abs/2405.00301>`__ 基于可学习干预和不确定性表达的增强语言模型真实性

::

    replaced with revised version Fri, 7 Jun 2024 01:41:15 GMT
    Submission history From: Farima Fatahi Bayat [view email]
    [v1] Wed, 1 May 2024 03:50:09 UTC (7,435 KB)
    [v2] Thu, 6 Jun 2024 07:32:15 UTC (7,436 KB)
    [v3] Fri, 7 Jun 2024 01:41:15 UTC (7,436 KB)
    Farima Fatahi Bayat, Xin Liu, H. V. Jagadish, Lu Wang

Large language models (LLMs) can generate long-form and coherent text, yet they often hallucinate facts, which undermines their reliability. To mitigate this issue, inference-time methods steer LLM representations toward the "truthful directions" previously learned for truth elicitation. However, applying these truthful directions with the same intensity fails to generalize across different query contexts. We propose LITO, a Learnable Intervention method for Truthfulness Optimization that automatically identifies the optimal intervention intensity tailored to each specific context. LITO explores a sequence of model generations based on increasing levels of intervention intensities. It selects the most accurate response or refuses to answer when the predictions are highly uncertain. Experiments on multiple LLMs and question-answering datasets demonstrate that LITO improves truthfulness while preserving task accuracy. The adaptive nature of LITO counters the limitations of one-size-fits-all intervention methods, maximizing truthfulness by reflecting the model's internal knowledge only when it is confident. Our code is available at this https URL.

------------

`[2405.00715] Adapting Open-Source Large Language Models for Cost-Effective, Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning <https://arxiv.org/abs/2405.00715>`__ 采用基于策略强化学习的开源大型语言模型，用于具有成本效益的专家级临床记录生成

::

    replaced with revised version Fri, 7 Jun 2024 00:59:52 GMT
    Submission history From: Hanyin Wang [view email]
    [v1] Thu, 25 Apr 2024 15:34:53 UTC (3,060 KB)
    [v2] Wed, 5 Jun 2024 04:03:17 UTC (2,847 KB)
    [v3] Fri, 7 Jun 2024 00:59:52 UTC (2,847 KB)
    Hanyin Wang, Chufan Gao, Bolun Liu, Qiping Xu, Guleid Hussein, Mohamad El Labban, Kingsley Iheasirim, Hariprasad Korsapati, Chuck Outcalt, Jimeng Sun

Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have demonstrated promising capabilities in clinical text summarization tasks. However, due to patient data privacy concerns and computational costs, many healthcare providers prefer using small, locally-hosted models over external generic LLMs. This study presents a comprehensive domain- and task-specific adaptation process for the open-source LLaMA-2 13 billion parameter model, enabling it to generate high-quality clinical notes from outpatient patient-doctor dialogues. Our process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced a new approach, DistillDirect, for performing on-policy reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting model, LLaMA-Clinic, can generate clinical notes comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as "acceptable" or higher across all three criteria: real-world readiness, completeness, and accuracy. In the more challenging "Assessment and Plan" section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than physician-authored notes (4.1/5). Our cost analysis for inference shows that our LLaMA-Clinic model achieves a 4.375-fold cost reduction compared to an external generic LLM service. Additionally, we highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format, rather than relying on LLMs to determine this for clinical practice. We have made our newly created synthetic clinic dialogue-note dataset and the physician feedback dataset publicly available to foster future research.

------------

`[2405.18203] IAPT: Instruction-Aware Prompt Tuning for Large Language Models <https://arxiv.org/abs/2405.18203>`__ IAPT:面向大型语言模型的指令感知提示调优

::

    replaced with revised version Fri, 7 Jun 2024 06:41:18 GMT
    Submission history From: Wei Zhu [view email]
    [v1] Tue, 28 May 2024 14:11:01 UTC (601 KB)
    [v2] Fri, 7 Jun 2024 06:41:18 UTC (605 KB)
    Wei Zhu, Aaron Xuxiang Tian, Congrui Yin, Yuan Ni, Xiaoling Wang, Guotong Xie

Soft prompt tuning is a widely studied parameter-efficient fine-tuning method. However, it has a clear drawback: many soft tokens must be inserted into the input sequences to guarantee downstream performance. As a result, soft prompt tuning is less considered than Low-rank adaptation (LoRA) in the large language modeling (LLM) era. In this work, we propose a novel prompt tuning method, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft tokens. First, we install a parameter-efficient soft prompt generator at each Transformer layer to generate idiosyncratic soft prompts for each input instruction. The generated soft prompts can be seen as a semantic summary of the input instructions and can effectively guide the output generation. Second, the soft prompt generators are modules with a bottleneck architecture consisting of a self-attention pooling operation, two linear projections, and an activation function. Pilot experiments show that prompt generators at different Transformer layers require different activation functions. Thus, we propose to learn the idiosyncratic activation functions for prompt generators automatically with the help of rational functions. We have conducted experiments on various tasks, and the experimental results demonstrate that (a) our IAPT method can outperform the recent baselines with comparable tunable parameters. (b) Our IAPT method is more efficient than LoRA under the single-backbone multi-tenant setting.

------------

`[2405.20657] DORY: Deliberative Prompt Recovery for LLM <https://arxiv.org/abs/2405.20657>`__ 多莉:对LLM的审慎迅速恢复

::

    replaced with revised version Fri, 7 Jun 2024 17:00:00 GMT
    Submission history From: Lirong Gao [view email]
    [v1] Fri, 31 May 2024 07:51:16 UTC (10,084 KB)
    [v2] Fri, 7 Jun 2024 17:00:00 UTC (10,084 KB)
    Lirong Gao, Ru Peng, Yiming Zhang, Junbo Zhao

Prompt recovery in large language models (LLMs) is crucial for understanding how LLMs work and addressing concerns regarding privacy, copyright, etc. The trend towards inference-only APIs complicates this task by restricting access to essential outputs for recovery. To tackle this challenge, we extract prompt-related information from limited outputs and identify a strong(negative) correlation between output probability-based uncertainty and the success of prompt recovery. This finding led to the development of Deliberative PrOmpt RecoverY (DORY), our novel approach that leverages uncertainty to recover prompts accurately. DORY involves reconstructing drafts from outputs, refining these with hints, and filtering out noise based on uncertainty. Our evaluation across diverse LLMs and prompt benchmarks shows that DORY outperforms existing baselines, improving performance by approximately 10.82% and establishing a new state-of-the-art record in prompt recovery tasks. Significantly, DORY operates using a single LLM without any external resources or model, offering a cost-effective, user-friendly prompt recovery solution.

------------

`[2406.00606] LLMs Could Autonomously Learn Without External Supervision <https://arxiv.org/abs/2406.00606>`__ llm可以在没有外部监督的情况下自主学习

::

    replaced with revised version Thu, 6 Jun 2024 22:48:35 GMT
    Submission history From: Ke Ji [view email]
    [v1] Sun, 2 Jun 2024 03:36:37 UTC (792 KB)
    [v2] Thu, 6 Jun 2024 22:48:35 UTC (792 KB)
    Ke Ji and Junying Chen and Anningzhe Gao and Wenya Xie and Xiang Wan and Benyou Wang

In the quest for super-human performance, Large Language Models (LLMs) have traditionally been tethered to human-annotated datasets and predefined training objectives-a process that is both labor-intensive and inherently limited. This paper presents a transformative approach: Autonomous Learning for LLMs, a self-sufficient learning paradigm that frees models from the constraints of human supervision. This method endows LLMs with the ability to self-educate through direct interaction with text, akin to a human reading and comprehending literature. Our approach eliminates the reliance on annotated data, fostering an Autonomous Learning environment where the model independently identifies and reinforces its knowledge gaps. Empirical results from our comprehensive experiments, which utilized a diverse array of learning materials and were evaluated against standard public quizzes, reveal that Autonomous Learning outstrips the performance of both Pre-training and Supervised Fine-Tuning (SFT), as well as retrieval-augmented methods. These findings underscore the potential of Autonomous Learning to not only enhance the efficiency and effectiveness of LLM training but also to pave the way for the development of more advanced, self-reliant AI systems.

------------

`[2406.02524] CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks <https://arxiv.org/abs/2406.02524>`__ CheckEmbed:对开放式任务的LLM解决方案的有效验证

::

    replaced with revised version Fri, 7 Jun 2024 17:58:22 GMT
    Submission history From: Robert Gerstenberger [view email]
    [v1] Tue, 4 Jun 2024 17:42:21 UTC (562 KB)
    [v2] Fri, 7 Jun 2024 17:58:22 UTC (648 KB)
    Maciej Besta, Lorenzo Paleari, Ales Kubicek, Piotr Nyczyk, Robert Gerstenberger, Patrick Iff, Tomasz Lehmann, Hubert Niewiadomski, Torsten Hoefler

Large Language Models (LLMs) are revolutionizing various domains, yet verifying their answers remains a significant challenge, especially for intricate open-ended tasks such as consolidation, summarization, and extraction of knowledge. In this work, we propose CheckEmbed: an accurate, scalable, and simple LLM verification approach. CheckEmbed is driven by a straightforward yet powerful idea: in order to compare LLM solutions to one another or to the ground-truth, compare their corresponding answer-level embeddings obtained with a model such as GPT Text Embedding Large. This reduces a complex textual answer to a single embedding, facilitating straightforward, fast, and meaningful verification. We develop a comprehensive verification pipeline implementing the CheckEmbed methodology. The CheckEmbed pipeline also comes with metrics for assessing the truthfulness of the LLM answers, such as embedding heatmaps and their summaries. We show how to use these metrics for deploying practical engines that decide whether an LLM answer is satisfactory or not. We apply the pipeline to real-world document analysis tasks, including term extraction and document summarization, showcasing significant improvements in accuracy, cost-effectiveness, and runtime performance compared to existing token-, sentence-, and fact-level schemes such as BERTScore or SelfCheckGPT.

------------

`[2406.03847] Lean Workbook: A large-scale Lean problem set formalized from natural language math problems <https://arxiv.org/abs/2406.03847>`__ 精益工作簿:由自然语言数学问题形式化的大规模精益问题集

::

    replaced with revised version Fri, 7 Jun 2024 16:12:21 GMT
    Submission history From: Huaiyuan Ying [view email]
    [v1] Thu, 6 Jun 2024 08:25:43 UTC (450 KB)
    [v2] Fri, 7 Jun 2024 16:12:21 UTC (450 KB)
    Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, Kai Chen

Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at this https URL and our data at this https URL.

------------

`[2406.04127] Are We Done with MMLU? <https://arxiv.org/abs/2406.04127>`__ MMLU搞定了吗?

::

    replaced with revised version Fri, 7 Jun 2024 15:19:06 GMT
    Submission history From: Aryo Gema [view email]
    [v1] Thu, 6 Jun 2024 14:49:06 UTC (4,720 KB)
    [v2] Fri, 7 Jun 2024 15:19:06 UTC (402 KB)
    Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, Pasquale Minervini

Maybe not. We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, we find that 57% of the analysed questions in the Virology subset contain errors. To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error taxonomy. Then, we create MMLU-Redux, which is a subset of 3,000 manually re-annotated questions across 30 MMLU subjects. Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that were originally reported. Our results strongly advocate for revising MMLU's error-ridden questions to enhance its future utility and reliability as a benchmark. Therefore, we open up MMLU-Redux for additional annotation this https URL.

------------

`[2406.04220] BEADs: Bias Evaluation Across Domains <https://arxiv.org/abs/2406.04220>`__ BEADs:跨域偏差评估

::

    replaced with revised version Fri, 7 Jun 2024 12:29:48 GMT
    Submission history From: Shaina Raza Dr. [view email]
    [v1] Thu, 6 Jun 2024 16:18:30 UTC (32,227 KB)
    [v2] Fri, 7 Jun 2024 12:29:48 UTC (32,223 KB)
    Shaina Raza and Mizanur Rahman and Michael R. Zhang

Recent improvements in large language models (LLMs) have significantly enhanced natural language processing (NLP) applications. However, these models can also inherit and perpetuate biases from their training data. Addressing this issue is crucial, yet many existing datasets do not offer evaluation across diverse NLP tasks. To tackle this, we introduce the Bias Evaluations Across Domains (BEADs) dataset, designed to support a wide range of NLP tasks, including text classification, bias entity recognition, bias quantification, and benign language generation. BEADs uses AI-driven annotation combined with experts' verification to provide reliable labels. This method overcomes the limitations of existing datasets that typically depend on crowd-sourcing, expert-only annotations with limited bias evaluations, or unverified AI labeling. Our empirical analysis shows that BEADs is effective in detecting and reducing biases across different language models, with smaller models fine-tuned on BEADs often outperforming LLMs in bias classification tasks. However, these models may still exhibit biases towards certain demographics. Fine-tuning LLMs with our benign language data also reduces biases while preserving the models' knowledge. Our findings highlight the importance of comprehensive bias evaluation and the potential of targeted fine-tuning for reducing the bias of LLMs. We are making BEADs publicly available at this https URL
Warning: This paper contains examples that may be considered offensive.

------------

`[2406.04289] What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages <https://arxiv.org/abs/2406.04289>`__ 哪些语言易于语言建模?从学习概率正则语言的角度来看

::

    replaced with revised version Fri, 7 Jun 2024 08:30:02 GMT
    Submission history From: Nadav Borenstein [view email]
    [v1] Thu, 6 Jun 2024 17:34:24 UTC (8,395 KB)
    [v2] Fri, 7 Jun 2024 08:30:02 UTC (8,366 KB)
    Nadav Borenstein, Anej Svete, Robin Chan, Josef Valvoda, Franz Nowak, Isabelle Augenstein, Eleanor Chodroff, Ryan Cotterell

What can large language models learn? By definition, language models (LM) are distributions over strings. Therefore, an intuitive way of addressing the above question is to formalize it as a matter of learnability of classes of distributions over strings. While prior work in this direction focused on assessing the theoretical limits, in contrast, we seek to understand the empirical learnability. Unlike prior empirical work, we evaluate neural LMs on their home turf-learning probabilistic languages-rather than as classifiers of formal languages. In particular, we investigate the learnability of regular LMs (RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs as a function of various complexity parameters of the RLM and the hidden state size of the neural LM. We find that the RLM rank, which corresponds to the size of linear space spanned by the logits of its conditional distributions, and the expected length of sampled strings are strong and significant predictors of learnability for both RNNs and Transformers. Several other predictors also reach significance, but with differing patterns between RNNs and Transformers.

------------

`[2402.06627] Feedback Loops With Language Models Drive In-Context Reward Hacking <https://arxiv.org/abs/2402.06627>`__ 语言模型的反馈循环驱动上下文奖励黑客

::

    replaced with revised version Thu, 6 Jun 2024 21:39:09 GMT
    Submission history From: Alexander Pan [view email]
    [v1] Fri, 9 Feb 2024 18:59:29 UTC (350 KB)
    [v2] Tue, 4 Jun 2024 00:16:52 UTC (302 KB)
    [v3] Thu, 6 Jun 2024 21:39:09 UTC (302 KB)
    Alexander Pan and Erik Jones and Meena Jagadeesan and Jacob Steinhardt

Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide three recommendations for evaluation to capture more instances of ICRH. As AI development accelerates, the effects of feedback loops will proliferate, increasing the need to understand their role in shaping LLM behavior.

------------

`[2402.08679] COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability <https://arxiv.org/abs/2402.08679>`__ 冷攻击:为具有隐蔽性和可控性的llm越狱

::

    replaced with revised version Fri, 7 Jun 2024 00:13:08 GMT
    Submission history From: Xingang Guo [view email]
    [v1] Tue, 13 Feb 2024 18:58:48 UTC (410 KB)
    [v2] Fri, 7 Jun 2024 00:13:08 UTC (1,019 KB)
    Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Bin Hu

Jailbreaks on large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent (suffix) attack with continuation constraint, but also allow us to address new controllable attack settings such as revising a user query adversarially with paraphrasing constraint, and inserting stealthy attacks in context with position constraint. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5, and GPT-4) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at this https URL.

------------

`[2406.00426] InterpreTabNet: Distilling Predictive Signals from Tabular Data by Salient Feature Interpretation <https://arxiv.org/abs/2406.00426>`__ InterpreTabNet:通过显著特征解释从表格数据中提取预测信号

::

    replaced with revised version Fri, 7 Jun 2024 00:35:25 GMT
    Submission history From: Jacob Si [view email]
    [v1] Sat, 1 Jun 2024 12:48:11 UTC (8,871 KB)
    [v2] Fri, 7 Jun 2024 00:35:25 UTC (8,878 KB)
    Jacob Si, Wendy Yusi Cheng, Michael Cooper, Rahul G. Krishnan

Tabular data are omnipresent in various sectors of industries. Neural networks for tabular data such as TabNet have been proposed to make predictions while leveraging the attention mechanism for interpretability. However, the inferred attention masks are often dense, making it challenging to come up with rationales about the predictive signal. To remedy this, we propose InterpreTabNet, a variant of the TabNet model that models the attention mechanism as a latent variable sampled from a Gumbel-Softmax distribution. This enables us to regularize the model to learn distinct concepts in the attention masks via a KL Divergence regularizer. It prevents overlapping feature selection by promoting sparsity which maximizes the model's efficacy and improves interpretability to determine the important features when predicting the outcome. To assist in the interpretation of feature interdependencies from our model, we employ a large language model (GPT-4) and use prompt engineering to map from the learned feature mask onto natural language text describing the learned signal. Through comprehensive experiments on real-world datasets, we demonstrate that InterpreTabNet outperforms previous methods for interpreting tabular data while attaining competitive accuracy.

------------

`[2308.10103] ASPIRE: Language-Guided Data Augmentation for Improving Robustness Against Spurious Correlations <https://arxiv.org/abs/2308.10103>`__ 

::

    replaced with revised version Fri, 7 Jun 2024 00:47:40 GMT
    Submission history From: Sreyan Ghosh [view email]
    [v1] Sat, 19 Aug 2023 20:18:15 UTC (26,864 KB)
    [v2] Wed, 15 Nov 2023 00:14:08 UTC (41,493 KB)
    [v3] Fri, 7 Jun 2024 00:47:40 UTC (46,604 KB)
    Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Utkarsh Tyagi, Sakshi Singh, Sanjoy Chowdhury and Dinesh Manocha

Neural image classifiers can often learn to make predictions by overly relying on non-predictive features that are spuriously correlated with the class labels in the training data. This leads to poor performance in real-world atypical scenarios where such features are absent. This paper presents ASPIRE (Language-guided Data Augmentation for SPurIous correlation REmoval), a simple yet effective solution for supplementing the training dataset with images without spurious features, for robust learning against spurious correlations via better generalization. ASPIRE, guided by language at various steps, can generate non-spurious images without requiring any group labeling or existing non-spurious images in the training set. Precisely, we employ LLMs to first extract foreground and background features from textual descriptions of an image, followed by advanced language-guided image editing to discover the features that are spuriously correlated with the class label. Finally, we personalize a text-to-image generation model using the edited images to generate diverse in-domain images without spurious features. ASPIRE is complementary to all prior robust training methods in literature, and we demonstrate its effectiveness across 4 datasets and 9 baselines and show that ASPIRE improves the worst-group classification accuracy of prior methods by 1% - 38%. We also contribute a novel test set for the challenging Hard ImageNet dataset.

------------

`[2402.10980] ChemReasoner: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback <https://arxiv.org/abs/2402.10980>`__ ChemReasoner:基于量子化学反馈的大型语言模型知识空间启发式搜索

::

    replaced with revised version Fri, 7 Jun 2024 17:33:21 GMT
    Submission history From: Henry Sprueill [view email]
    [v1] Thu, 15 Feb 2024 21:33:07 UTC (14,570 KB)
    [v2] Wed, 21 Feb 2024 17:34:43 UTC (14,570 KB)
    [v3] Tue, 7 May 2024 00:00:06 UTC (14,570 KB)
    [v4] Fri, 7 Jun 2024 17:33:21 UTC (14,733 KB)
    Henry W. Sprueill, Carl Edwards, Khushbu Agarwal, Mariefel V. Olarte, Udishnu Sanyal, Conrad Johnston, Hongbin Liu, Heng Ji, Sutanay Choudhury

The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and reaction energy barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automatically guide the exploration without human input, providing competitive performance against expert-enumerated chemical descriptor-based implementations. By integrating language-guided reasoning with computational chemistry feedback, our work pioneers AI-accelerated, trustworthy catalyst discovery.

------------

`[2403.07750] Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings <https://arxiv.org/abs/2403.07750>`__ Synth$^2$:基于合成标题和图像嵌入的视觉语言模型增强

::

    replaced with revised version Fri, 7 Jun 2024 12:10:47 GMT
    Submission history From: Andrea Banino [view email]
    [v1] Tue, 12 Mar 2024 15:36:42 UTC (5,027 KB)
    [v2] Fri, 7 Jun 2024 12:10:47 UTC (2,248 KB)
    Sahand Sharifzadeh, Christos Kaplanis, Shreya Pathak, Dharshan Kumaran, Anastasija Ilic, Jovana Mitrovic, Charles Blundell, Andrea Banino

The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). In this work, we investigate an approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs a pretrained text-to-image model to synthesize image embeddings from captions generated by an LLM. Despite the text-to-image model and VLM initially being trained on the same data, our approach leverages the image generator's ability to create novel compositions, resulting in synthetic image embeddings that expand beyond the limitations of the original dataset. Extensive experiments demonstrate that our VLM, finetuned on synthetic data achieves comparable performance to models trained solely on human-annotated data, while requiring significantly less data. Furthermore, we perform a set of analyses on captions which reveals that semantic diversity and balance are key aspects for better downstream performance. Finally, we show that synthesizing images in the image embedding space is 25\% faster than in the pixel space. We believe our work not only addresses a significant challenge in VLM training but also opens up promising avenues for the development of self-improving multi-modal models.

------------

`[2405.11380] Meta-Control: Automatic Model-based Control Synthesis for Heterogeneous Robot Skills <https://arxiv.org/abs/2405.11380>`__ 元控制:基于模型的异构机器人技能自动控制综合

::

    replaced with revised version Fri, 7 Jun 2024 15:22:41 GMT
    Submission history From: Tianhao Wei [view email]
    [v1] Sat, 18 May 2024 19:58:44 UTC (6,261 KB)
    [v2] Fri, 7 Jun 2024 15:22:41 UTC (6,400 KB)
    Tianhao Wei, Liqian Ma, Rui Chen, Weiye Zhao, Changliu Liu

The requirements for real-world manipulation tasks are diverse and often conflicting; some tasks require precise motion while others require force compliance; some tasks require avoidance of certain regions, while others require convergence to certain states. Satisfying these varied requirements with a fixed state-action representation and control strategy is challenging, impeding the development of a universal robotic foundation model. In this work, we propose Meta-Control, the first LLM-enabled automatic control synthesis approach that creates customized state representations and control strategies tailored to specific tasks. Our core insight is that a meta-control system can be built to automate the thought process that human experts use to design control systems. Specifically, human experts heavily use a model-based, hierarchical (from abstract to concrete) thought model, then compose various dynamic models and controllers together to form a control system. Meta-Control mimics the thought model and harnesses LLM's extensive control knowledge with Socrates' "art of midwifery" to automate the thought process. Meta-Control stands out for its fully model-based nature, allowing rigorous analysis, generalizability, robustness, efficient parameter tuning, and reliable real-time execution.

------------

`[2405.12450] PathOCL: Path-Based Prompt Augmentation for OCL Generation with GPT-4 <https://arxiv.org/abs/2405.12450>`__ PathOCL:基于路径的GPT-4 OCL生成提示增强

::

    replaced with revised version Thu, 6 Jun 2024 23:10:24 GMT
    Submission history From: Seif Abukhalaf [view email]
    [v1] Tue, 21 May 2024 02:00:54 UTC (2,390 KB)
    [v2] Thu, 6 Jun 2024 23:10:24 UTC (2,169 KB)
    Seif Abukhalaf, Mohammad Hamdaqa, Foutse Khomh

The rapid progress of AI-powered programming assistants, such as GitHub Copilot, has facilitated the development of software applications. These assistants rely on large language models (LLMs), which are foundation models (FMs) that support a wide range of tasks related to understanding and generating language. LLMs have demonstrated their ability to express UML model specifications using formal languages like the Object Constraint Language (OCL). However, the context size of the prompt is limited by the number of tokens an LLM can process. This limitation becomes significant as the size of UML class models increases. In this study, we introduce PathOCL, a novel path-based prompt augmentation technique designed to facilitate OCL generation. PathOCL addresses the limitations of LLMs, specifically their token processing limit and the challenges posed by large UML class models. PathOCL is based on the concept of chunking, which selectively augments the prompts with a subset of UML classes relevant to the English specification. Our findings demonstrate that PathOCL, compared to augmenting the complete UML class model (UML-Augmentation), generates a higher number of valid and correct OCL constraints using the GPT-4 model. Moreover, the average prompt size crafted using PathOCL significantly decreases when scaling the size of the UML class models.

------------

`[2405.11093] AudioSetMix: Enhancing Audio-Language Datasets with LLM-Assisted Augmentations <https://arxiv.org/abs/2405.11093>`__ AudioSetMix:用llm辅助增强音频语言数据集

::

    replaced with revised version Fri, 7 Jun 2024 16:15:26 GMT
    Submission history From: David Xu [view email]
    [v1] Fri, 17 May 2024 21:08:58 UTC (162 KB)
    [v2] Fri, 7 Jun 2024 16:15:26 UTC (162 KB)
    David Xu

Multi-modal learning in the audio-language domain has seen significant advancements in recent years. However, audio-language learning faces challenges due to limited and lower-quality data compared to image-language tasks. Existing audio-language datasets are notably smaller, and manual labeling is hindered by the need to listen to entire audio clips for accurate labeling.
Our method systematically generates audio-caption pairs by augmenting audio clips with natural language labels and corresponding audio signal processing operations. Leveraging a Large Language Model, we generate descriptions of augmented audio clips with a prompt template. This scalable method produces AudioSetMix, a high-quality training dataset for text-and-audio related models.
Integration of our dataset improves models performance on benchmarks by providing diversified and better-aligned examples. Notably, our dataset addresses the absence of modifiers (adjectives and adverbs) in existing datasets. By enabling models to learn these concepts, and generating hard negative examples during training, we achieve state-of-the-art performance on multiple benchmarks.

------------

-----------
Index (106)
-----------

`[2406.04481] Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF <https://arxiv.org/abs/2406.04481>`__ 为安全优化自动驾驶:以llm增强的RLHF为中心的方法

`[2406.04370] Large Language Model Confidence Estimation via Black-Box Access <https://arxiv.org/abs/2406.04370>`__ 基于黑盒访问的大型语言模型置信度估计

`[2406.04371] Phased Instruction Fine-Tuning for Large Language Models <https://arxiv.org/abs/2406.04371>`__

`[2406.04383] Exploring the Latest LLMs for Leaderboard Extraction <https://arxiv.org/abs/2406.04383>`__ 探索最新的llm排行榜提取

`[2406.04428] MoralBench: Moral Evaluation of LLMs <https://arxiv.org/abs/2406.04428>`__ Moral bench: llm的道德评估

`[2406.04449] MAIRA-2: Grounded Radiology Report Generation <https://arxiv.org/abs/2406.04449>`__ MAIRA-2:地面放射学报告生成

`[2406.04460] Evaluating the Smooth Control of Attribute Intensity in Text Generation with LLMs <https://arxiv.org/abs/2406.04460>`__ 用llm评估文本生成中属性强度的平滑控制

`[2406.04482] Automatic Bug Detection in LLM-Powered Text-Based Games Using LLMs <https://arxiv.org/abs/2406.04482>`__ 使用llm自动检测基于llm的文本游戏中的Bug

`[2406.04523] Proofread: Fixes All Errors with One Tap <https://arxiv.org/abs/2406.04523>`__ 校对:一键修复所有错误

`[2406.04528] llmNER: (Zero|Few)-Shot Named Entity Recognition, Exploiting the Power of Large Language Models <https://arxiv.org/abs/2406.04528>`__ llmNER:(0 |Few)-Shot命名实体识别，利用大型语言模型的力量

`[2406.04555] Creating an AI Observer: Generative Semantic Workspaces <https://arxiv.org/abs/2406.04555>`__ 创建AI观察者:生成式语义工作空间

`[2406.04583] Extroversion or Introversion? Controlling The Personality of Your Large Language Models <https://arxiv.org/abs/2406.04583>`__ 外向还是内向?控制你的大型语言模型的个性

`[2406.04614] LawGPT: A Chinese Legal Knowledge-Enhanced Large Language Model <https://arxiv.org/abs/2406.04614>`__ LawGPT:一个中文法律知识增强的大型语言模型

`[2406.04625] Key-Element-Informed sLLM Tuning for Document Summarization <https://arxiv.org/abs/2406.04625>`__ 基于关键元素的sLLM文档摘要调优

`[2406.04630] Low-Resource Cross-Lingual Summarization through Few-Shot Learning with Large Language Models <https://arxiv.org/abs/2406.04630>`__ 基于大语言模型少样本学习的低资源跨语言摘要

`[2406.04638] Large Language Model-guided Document Selection <https://arxiv.org/abs/2406.04638>`__ 大型语言模型引导的文档选择

`[2406.04669] DiNeR: a Large Realistic Dataset for Evaluating Compositional Generalization <https://arxiv.org/abs/2406.04669>`__

`[2406.04712] AICoderEval: Improving AI Domain Code Generation of Large Language Models <https://arxiv.org/abs/2406.04712>`__ AICoderEval:改进大型语言模型的AI域代码生成

`[2406.04758] Think out Loud: Emotion Deducing Explanation in Dialogues <https://arxiv.org/abs/2406.04758>`__ 自言自语:对话中情感演绎解释

`[2406.04836] Revisiting Catastrophic Forgetting in Large Language Model Tuning <https://arxiv.org/abs/2406.04836>`__ 大型语言模型调优中的灾难性遗忘

`[2406.04854] Uncertainty Aware Learning for Language Model Alignment <https://arxiv.org/abs/2406.04854>`__ 语言模型对齐的不确定性感知学习

`[2406.04866] ComplexTempQA: A Large-Scale Dataset for Complex Temporal Question Answering <https://arxiv.org/abs/2406.04866>`__ ComplexTempQA:面向复杂时序问答的大规模数据集

`[2406.04926] Through the Thicket: A Study of Number-Oriented LLMs derived from Random Forest Models <https://arxiv.org/abs/2406.04926>`__

`[2406.04941] TCMD: A Traditional Chinese Medicine QA Dataset for Evaluating Large Language Models <https://arxiv.org/abs/2406.04941>`__ TCMD:用于评估大型语言模型的中医QA数据集

`[2406.04947] BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense <https://arxiv.org/abs/2406.04947>`__ BAMO在SemEval-2024任务9:难题:挑战常识的新任务

`[2406.04952] Quantifying Geospatial in the Common Crawl Corpus <https://arxiv.org/abs/2406.04952>`__ 基于Common Crawl语料库的地理空间量化

`[2406.05035] Scenarios and Approaches for Situated Natural Language Explanations <https://arxiv.org/abs/2406.05035>`__ 情境自然语言解释的场景和方法

`[2406.05063] Are Large Language Models More Empathetic than Humans? <https://arxiv.org/abs/2406.05063>`__

`[2406.04412] Aligning Large Language Models with Self-generated Preference Data <https://arxiv.org/abs/2406.04412>`__ 用自我生成的偏好数据对齐大型语言模型

`[2406.04443] Gradient Clipping Improves AdaGrad when the Noise Is Heavy-Tailed <https://arxiv.org/abs/2406.04443>`__ 当噪声具有重尾时，梯度裁剪可以改进AdaGrad算法

`[2406.04446] Can Language Models Use Forecasting Strategies? <https://arxiv.org/abs/2406.04446>`__

`[2406.04501] FLUID-LLM: Learning Computational Fluid Dynamics with Spatiotemporal-aware Large Language Models <https://arxiv.org/abs/2406.04501>`__ Fluid - llm:基于时空感知大型语言模型的计算流体力学学习

`[2406.04606] Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions <https://arxiv.org/abs/2406.04606>`__ 有用的还是有害的数据?用于解释语言模型预测的无微调沙普利归因

`[2406.04640] LinkGPT: Teaching Large Language Models To Predict Missing Links <https://arxiv.org/abs/2406.04640>`__ LinkGPT:大型语言模型预测缺失链接

`[2406.04687] LogiCode: an LLM-Driven Framework for Logical Anomaly Detection <https://arxiv.org/abs/2406.04687>`__ LogiCode:一个llm驱动的逻辑异常检测框架

`[2406.04824] FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch <https://arxiv.org/abs/2406.04824>`__ FunBO:用FunSearch发现贝叶斯优化的获取函数

`[2406.04346] Automating Patch Set Generation from Code Review Comments Using Large Language Models <https://arxiv.org/abs/2406.04346>`__

`[2406.04373] VerilogReader: LLM-Aided Hardware Test Generation <https://arxiv.org/abs/2406.04373>`__ veriloreader: llm辅助的硬件测试生成

`[2406.04379] VHDL-Eval: A Framework for Evaluating Large Language Models in VHDL Code Generation <https://arxiv.org/abs/2406.04379>`__ VHDL- eval:面向VHDL代码生成的大型语言模型评估框架

`[2406.04432] LipGER: Visually-Conditioned Generative Error Correction for Robust Automatic Speech Recognition <https://arxiv.org/abs/2406.04432>`__ LipGER:鲁棒自动语音识别的视觉条件生成误差校正

`[2406.04568] StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation <https://arxiv.org/abs/2406.04568>`__ StackSight:通过大型语言模型和神经符号思维链反编译揭开WebAssembly

`[2406.04594] Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach <https://arxiv.org/abs/2406.04594>`__ 用C4:通信驱动方法提高大规模并行训练效率

`[2406.04693] LLM-Vectorizer: LLM-based Verified Loop Vectorizer <https://arxiv.org/abs/2406.04693>`__ LLM-Vectorizer:基于llm验证的循环向量化器

`[2406.04710] Morescient GAI for Software Engineering <https://arxiv.org/abs/2406.04710>`__ 软件工程领域更先进的GAI

`[2406.04755] Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations <https://arxiv.org/abs/2406.04755>`__ 销售呓语:对LLM品牌推荐的不引人注目的攻击

`[2406.05132] 3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs <https://arxiv.org/abs/2406.05132>`__

`[2406.04927] LLM-based speaker diarization correction: A generalizable approach <https://arxiv.org/abs/2406.04927>`__ 基于llm的说话人分级校正:一种通用方法

`[2406.05039] Bootstrapping Referring Multi-Object Tracking <https://arxiv.org/abs/2406.05039>`__ Bootstrapping参考多目标跟踪

`[2406.04348] Gaining Insights into Group-Level Course Difficulty via Differential Course Functioning <https://arxiv.org/abs/2406.04348>`__ 通过差异课程功能获得群体水平课程难度的见解

`[2406.04857] A Near-Linear Time Approximation Algorithm for Beyond-Worst-Case Graph Clustering <https://arxiv.org/abs/2406.04857>`__ 超最差情况图聚类的近似线性时间算法

`[2308.05201] "Generate" the Future of Work through AI: Empirical Evidence from Online Labor Markets <https://arxiv.org/abs/2308.05201>`__

`[2312.11111] The Good, The Bad, and Why: Unveiling Emotions in Generative AI <https://arxiv.org/abs/2312.11111>`__ 好的，坏的和为什么:揭示生成AI中的情感

`[2302.12246] Active Prompting with Chain-of-Thought for Large Language Models <https://arxiv.org/abs/2302.12246>`__ 基于思维链的大型语言模型主动提示

`[2306.11825] DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models <https://arxiv.org/abs/2306.11825>`__

`[2306.11879] Open-Domain Text Evaluation via Contrastive Distribution Methods <https://arxiv.org/abs/2306.11879>`__ 基于对比分布方法的开放域文本评价

`[2310.04799] Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages <https://arxiv.org/abs/2310.04799>`__ 聊天向量:为llm配备新语言中的指令遵循和模型对齐的简单方法

`[2310.05492] How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition <https://arxiv.org/abs/2310.05492>`__ 大型语言模型的能力如何受到监督微调数据组合的影响

`[2310.15123] Branch-Solve-Merge Improves Large Language Model Evaluation and Generation <https://arxiv.org/abs/2310.15123>`__ 分支求解-合并改进了大型语言模型的评估和生成

`[2310.19975] BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing <https://arxiv.org/abs/2310.19975>`__ BioInstruct:面向生物医学自然语言处理的大型语言模型指令调优

`[2311.08374] A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts <https://arxiv.org/abs/2311.08374>`__

`[2311.09204] Fusion-Eval: Integrating Assistant Evaluators with LLMs <https://arxiv.org/abs/2311.09204>`__ Fusion-Eval:将辅助评估器与llm集成

`[2311.09677] R-Tuning: Instructing Large Language Models to Say `I Don't Know' <https://arxiv.org/abs/2311.09677>`__ r调优:指示大型语言模型说“我不知道”

`[2311.09766] LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores <https://arxiv.org/abs/2311.09766>`__ llm作为自恋评估者:当自我膨胀评估分数

`[2311.17438] CLOMO: Counterfactual Logical Modification with Large Language Models <https://arxiv.org/abs/2311.17438>`__ CLOMO:基于大型语言模型的反事实逻辑修改

`[2312.02120] Magicoder: Empowering Code Generation with OSS-Instruct <https://arxiv.org/abs/2312.02120>`__ Magicoder:用OSS-Instruct实现代码生成

`[2312.06924] Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack <https://arxiv.org/abs/2312.06924>`__ NLP任务中的安全对齐:作为上下文攻击的弱对齐摘要

`[2312.14187] WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning <https://arxiv.org/abs/2312.14187>`__ WaveCoder:通过指令调整广泛通用的代码大型语言模型增强

`[2401.02009] Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives <https://arxiv.org/abs/2401.02009>`__ 自我对比:通过不一致的解决视角进行更好的反思

`[2401.06102] Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models <https://arxiv.org/abs/2401.06102>`__ Patchscopes:用于检查语言模型隐藏表示的统一框架

`[2402.05629] Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations <https://arxiv.org/abs/2402.05629>`__ 合并事实，制造谬误:评估长期累积的事实主张的矛盾性质

`[2402.10137] TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles <https://arxiv.org/abs/2402.10137>`__ TOAD:具有不同回应风格的任务导向型自动对话

`[2402.10412] Measuring and Reducing LLM Hallucination without Gold-Standard Answers <https://arxiv.org/abs/2402.10412>`__ 在没有黄金标准答案的情况下测量和减少LLM幻觉

`[2402.10586] Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs <https://arxiv.org/abs/2402.10586>`__ 细枝末节:通过话语母题检测机器生成的文本

`[2402.10671] Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm <https://arxiv.org/abs/2402.10671>`__ 增强注意力的分解:通过工作流范式改进基于llm的Text-to-SQL

`[2402.11655] Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals <https://arxiv.org/abs/2402.11655>`__ 机制的竞争:追踪语言模型如何处理事实和反事实

`[2402.11709] GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network <https://arxiv.org/abs/2402.11709>`__ GNNavi:基于图神经网络的大型语言模型信息流导航

`[2402.11753] ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs <https://arxiv.org/abs/2402.11753>`__ ArtPrompt:基于ASCII art的针对对齐llm的越狱攻击

`[2402.12071] EmoBench: Evaluating the Emotional Intelligence of Large Language Models <https://arxiv.org/abs/2402.12071>`__ EmoBench:大型语言模型情绪智力评估

`[2402.15018] Unintended Impacts of LLM Alignment on Global Representation <https://arxiv.org/abs/2402.15018>`__ LLM对齐对全局表示的意外影响

`[2402.16459] Defending LLMs against Jailbreaking Attacks via Backtranslation <https://arxiv.org/abs/2402.16459>`__ 通过反向翻译防御llm越狱攻击

`[2403.03329] Guardrail Baselines for Unlearning in LLMs <https://arxiv.org/abs/2403.03329>`__ llm中忘记学习的护栏基线

`[2403.04696] Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification <https://arxiv.org/abs/2403.04696>`__ 事实核查通过标记级不确定性量化的大型语言模型的输出

`[2403.08492] Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking <https://arxiv.org/abs/2403.08492>`__ 丰富语义知识增强的大型语言模型小样本中文拼写检查

`[2403.10822] Can Large Language Models abstract Medical Coded Language? <https://arxiv.org/abs/2403.10822>`__ 大型语言模型能抽象医学编码语言吗?

`[2404.03189] The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models <https://arxiv.org/abs/2404.03189>`__ 概率也很重要:大型语言模型中自由文本解释的忠实度的更可靠的指标

`[2404.15247] XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts <https://arxiv.org/abs/2404.15247>`__ XFT:通过简单地合并升级循环的专家组合来释放代码指令调优的威力

`[2404.15485] Evaluating the Efficacy of Large Language Models in Identifying Phishing Attempts <https://arxiv.org/abs/2404.15485>`__ 评估大型语言模型识别网络钓鱼企图的有效性

`[2405.00301] Enhanced Language Model Truthfulness with Learnable Intervention and Uncertainty Expression <https://arxiv.org/abs/2405.00301>`__ 基于可学习干预和不确定性表达的增强语言模型真实性

`[2405.00715] Adapting Open-Source Large Language Models for Cost-Effective, Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning <https://arxiv.org/abs/2405.00715>`__ 采用基于策略强化学习的开源大型语言模型，用于具有成本效益的专家级临床记录生成

`[2405.18203] IAPT: Instruction-Aware Prompt Tuning for Large Language Models <https://arxiv.org/abs/2405.18203>`__ IAPT:面向大型语言模型的指令感知提示调优

`[2405.20657] DORY: Deliberative Prompt Recovery for LLM <https://arxiv.org/abs/2405.20657>`__ 多莉:对LLM的审慎迅速恢复

`[2406.00606] LLMs Could Autonomously Learn Without External Supervision <https://arxiv.org/abs/2406.00606>`__ llm可以在没有外部监督的情况下自主学习

`[2406.02524] CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks <https://arxiv.org/abs/2406.02524>`__ CheckEmbed:对开放式任务的LLM解决方案的有效验证

`[2406.03847] Lean Workbook: A large-scale Lean problem set formalized from natural language math problems <https://arxiv.org/abs/2406.03847>`__ 精益工作簿:由自然语言数学问题形式化的大规模精益问题集

`[2406.04127] Are We Done with MMLU? <https://arxiv.org/abs/2406.04127>`__ MMLU搞定了吗?

`[2406.04220] BEADs: Bias Evaluation Across Domains <https://arxiv.org/abs/2406.04220>`__ BEADs:跨域偏差评估

`[2406.04289] What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages <https://arxiv.org/abs/2406.04289>`__ 哪些语言易于语言建模?从学习概率正则语言的角度来看

`[2402.06627] Feedback Loops With Language Models Drive In-Context Reward Hacking <https://arxiv.org/abs/2402.06627>`__ 语言模型的反馈循环驱动上下文奖励黑客

`[2402.08679] COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability <https://arxiv.org/abs/2402.08679>`__ 冷攻击:为具有隐蔽性和可控性的llm越狱

`[2406.00426] InterpreTabNet: Distilling Predictive Signals from Tabular Data by Salient Feature Interpretation <https://arxiv.org/abs/2406.00426>`__ InterpreTabNet:通过显著特征解释从表格数据中提取预测信号

`[2308.10103] ASPIRE: Language-Guided Data Augmentation for Improving Robustness Against Spurious Correlations <https://arxiv.org/abs/2308.10103>`__

`[2402.10980] ChemReasoner: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback <https://arxiv.org/abs/2402.10980>`__ ChemReasoner:基于量子化学反馈的大型语言模型知识空间启发式搜索

`[2403.07750] Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings <https://arxiv.org/abs/2403.07750>`__ Synth$^2$:基于合成标题和图像嵌入的视觉语言模型增强

`[2405.11380] Meta-Control: Automatic Model-based Control Synthesis for Heterogeneous Robot Skills <https://arxiv.org/abs/2405.11380>`__ 元控制:基于模型的异构机器人技能自动控制综合

`[2405.12450] PathOCL: Path-Based Prompt Augmentation for OCL Generation with GPT-4 <https://arxiv.org/abs/2405.12450>`__ PathOCL:基于路径的GPT-4 OCL生成提示增强

`[2405.11093] AudioSetMix: Enhancing Audio-Language Datasets with LLM-Assisted Augmentations <https://arxiv.org/abs/2405.11093>`__ AudioSetMix:用llm辅助增强音频语言数据集

