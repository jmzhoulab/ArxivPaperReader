240605
========

----------
Survey (2)
----------

`[2312.17617] Large Language Models for Generative Information Extraction: A Survey <https://arxiv.org/abs/2312.17617>`__ 面向生成信息抽取的大型语言模型综述

::

    replaced with revised version Tue, 4 Jun 2024 13:20:27 GMT
    Submission history From: Derong Xu [view email]
    [v1] Fri, 29 Dec 2023 14:25:22 UTC (898 KB)
    [v2] Tue, 4 Jun 2024 13:20:27 UTC (7,485 KB)
    Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Yang Wang, Enhong Chen

Information extraction (IE) aims to extract structural knowledge (such as entities, relations, and events) from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation, allowing for generalization across various domains and tasks. As a result, numerous works have been proposed to harness abilities of LLMs and offer viable solutions for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and learning paradigms, then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related resources at: \url{this https URL}.

------------

`[2401.07851] Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding <https://arxiv.org/abs/2401.07851>`__ 解锁大型语言模型推理中的效率:推测解码综合综述

::

    replaced with revised version Tue, 4 Jun 2024 17:08:37 GMT
    Submission history From: Heming Xia [view email]
    [v1] Mon, 15 Jan 2024 17:26:50 UTC (2,017 KB)
    [v2] Tue, 20 Feb 2024 10:24:57 UTC (1,345 KB)
    [v3] Tue, 4 Jun 2024 17:08:37 UTC (1,348 KB)
    Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui

To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.

------------

-------------
Benchmark (8)
-------------

`[2406.02106] MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset <https://arxiv.org/abs/2406.02106>`__ MARS:基于多任务评估数据集的语言模型形而上学推理能力基准测试

::

    Tue, 4 Jun 2024 08:35:04 GMT
    Weiqi Wang, Yangqiu Song

To enable Large Language Models (LLMs) to function as conscious agents with generalizable reasoning capabilities, it is crucial that they possess the reasoning ability to comprehend situational changes (transitions) in distribution triggered by environmental factors or actions from other agents.
Despite its fundamental significance, this ability remains underexplored due to the complexity of modeling infinite possible changes in an event and their associated distributions, coupled with the lack of benchmark data with situational transitions. Addressing these gaps, we propose a novel formulation of reasoning with distributional changes as a three-step discriminative process, termed as MetAphysical ReaSoning. We then introduce the first-ever benchmark, MARS, comprising three tasks corresponding to each step. These tasks systematically assess LLMs' capabilities in reasoning the plausibility of (i) changes in actions, (ii) states caused by changed actions, and (iii) situational transitions driven by changes in action. Extensive evaluations with 20 (L)LMs of varying sizes and methods indicate that all three tasks in this process pose significant challenges, even for state-of-the-art LLMs and LMs after fine-tuning. Further analyses reveal potential causes for the underperformance of LLMs and demonstrate that pre-training them on large-scale conceptualization taxonomies can potentially enhance their metaphysical reasoning capabilities. Our data and models are publicly accessible at https://github.com/HKUST-KnowComp/MARS.

------------

`[2406.02472] Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding <https://arxiv.org/abs/2406.02472>`__ 用大型语言模型分析时序复杂事件?时间、长上下文理解的基准

::

    Tue, 4 Jun 2024 16:42:17 GMT
    Zhihan Zhang, Yixin Cao, Chenchen Ye, Yunshan Ma, Lizi Liao, Tat-Seng Chua

The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events. We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.

------------

`[2406.01607] Recent advances in text embedding: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark <https://arxiv.org/abs/2406.01607>`__ 文本嵌入的最新进展:MTEB基准上表现最好的方法的全面综述

::

    Mon, 27 May 2024 09:52:54 GMT
    Hongliu Cao

Text embedding methods have become increasingly popular in both industrial and academic fields due to their critical role in a variety of natural language processing tasks. The significance of universal text embeddings has been further highlighted with the rise of Large Language Models (LLMs) applications such as Retrieval-Augmented Systems (RAGs). While previous models have attempted to be general-purpose, they often struggle to generalize across tasks and domains. However, recent advancements in training data quantity, quality and diversity; synthetic data generation from LLMs as well as using LLMs as backbones encourage great improvements in pursuing universal text embeddings.
In this paper, we provide an overview of the recent advances in universal text embedding models with a focus on the top performing text embeddings on Massive Text Embedding Benchmark (MTEB). Through detailed comparison and analysis, we highlight the key contributions and limitations in this area, and propose potentially inspiring future research directions.

------------

`[2401.05604] REBUS: A Robust Evaluation Benchmark of Understanding Symbols <https://arxiv.org/abs/2401.05604>`__ REBUS:一个强大的符号理解评估基准

::

    replaced with revised version Mon, 3 Jun 2024 23:49:45 GMT
    Submission history From: Andrew Gritsevskiy [view email]
    [v1] Thu, 11 Jan 2024 00:30:28 UTC (4,457 KB)
    [v2] Mon, 3 Jun 2024 23:49:45 UTC (4,606 KB)
    Andrew Gritsevskiy, Arjun Panickssery, Aaron Kirtland, Derik Kauffman, Hans Gundlach, Irina Gritsevskaya, Joe Cavanagh, Jonathan Chiang, Lydia La Roux, Michelle Hung

We propose a new benchmark evaluating the performance of multimodal large language models on rebus puzzles. The dataset covers 333 original examples of image-based wordplay, cluing 13 categories such as movies, composers, major cities, and food. To achieve good performance on the benchmark of identifying the clued word or phrase, models must combine image recognition and string manipulation with hypothesis testing, multi-step reasoning, and an understanding of human cognition, making for a complex, multimodal evaluation of capabilities. We find that GPT-4o significantly outperforms all other models, followed by proprietary models outperforming all other evaluated models. However, even the best model has a final accuracy of only 42\%, which goes down to just 7\% on hard puzzles, highlighting the need for substantial improvements in reasoning. Further, models rarely understand all parts of a puzzle, and are almost always incapable of retroactively explaining the correct answer. Our benchmark can therefore be used to identify major shortcomings in the knowledge and reasoning of multimodal large language models.

------------

`[2402.11034] PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering <https://arxiv.org/abs/2402.11034>`__ PAT-Questions:基于当前锚定时态问答的自更新基准

::

    replaced with revised version Mon, 3 Jun 2024 18:36:59 GMT
    Submission history From: Jannat Ara Meem [view email]
    [v1] Fri, 16 Feb 2024 19:26:09 UTC (681 KB)
    [v2] Mon, 3 Jun 2024 18:36:59 UTC (2,559 KB)
    Jannat Ara Meem, Muhammad Shihab Rashid, Yue Dong and Vagelis Hristidis

Existing work on Temporal Question Answering (TQA) has predominantly focused on questions anchored to specific timestamps or events (e.g. "Who was the US president in 1970?"). Little work has studied questions whose temporal context is relative to the present time (e.g. "Who was the previous US president?"). We refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses unique challenges: (1) large language models (LLMs) may have outdated knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are hard to reason, (3) multi-hop reasoning may be required, and (4) the gold answers of benchmarks must be continuously updated. To address these challenges, we introduce the PAT-Questions benchmark, which includes single and multi-hop temporal questions. The answers in PAT-Questions can be automatically refreshed by re-running SPARQL queries on a knowledge graph, if available. We evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model (TEMPREASON-T5) on PAT-Questions through direct prompting and retrieval-augmented generation (RAG). The results highlight the limitations of existing solutions in PATQA and motivate the need for new methods to improve PATQA reasoning capabilities.

------------

`[2402.13109] CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models <https://arxiv.org/abs/2402.13109>`__ CIF-Bench:用于评估大型语言模型泛化能力的中文指令遵循基准

::

    replaced with revised version Tue, 4 Jun 2024 14:26:30 GMT
    Submission history From: Yizhi Li [view email]
    [v1] Tue, 20 Feb 2024 16:02:12 UTC (237 KB)
    [v2] Tue, 4 Jun 2024 14:26:30 UTC (239 KB)
    Yizhi LI, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Zekun Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, Wangchunshu Zhou, Yiming Liang, Lei Zhang, Lei Ma, Jiajun Zhang, Zuowen Li, Stephen W. Huang, Chenghua Lin, Jie Fu

The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate data contamination, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our evaluation of 28 selected LLMs reveals a noticeable performance gap, with the best model scoring only 52.9%, highlighting the limitations of LLMs in less familiar language and task contexts. This work not only uncovers the current limitations of LLMs in handling Chinese language tasks but also sets a new standard for future LLM generalizability research, pushing towards the development of more adaptable, culturally informed, and linguistically diverse models.

------------

`[2402.15813] Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method <https://arxiv.org/abs/2402.15813>`__ 衡量llm的议价能力:一个基准和买方增强方法

::

    replaced with revised version Tue, 4 Jun 2024 08:12:04 GMT
    Submission history From: Tian Xia [view email]
    [v1] Sat, 24 Feb 2024 13:36:58 UTC (468 KB)
    [v2] Thu, 29 Feb 2024 13:04:11 UTC (468 KB)
    [v3] Tue, 4 Jun 2024 08:12:04 UTC (7,332 KB)
    Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang, Yang Yang, Rui Wang

Bargaining is an important and unique part of negotiation between humans. As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents' bargaining abilities remains an open problem. For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes. It allows us to quantitatively assess an agent's performance in the Bargain task. We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various LLM agents' bargaining abilities. We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance. To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer's offers, and an LLM Narrator to create natural language sentences for generated offers. Experimental results show that OG-Narrator improves the buyer's deal rates from 26.67% to 88.88% and brings a ten times multiplication of profits on all baselines, even a model that has not been aligned.

------------

`[2406.01359] R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models <https://arxiv.org/abs/2406.01359>`__ R2C2-Coder:代码大型语言模型的真实库级代码补全能力增强和基准测试

::

    replaced with revised version Tue, 4 Jun 2024 03:13:43 GMT
    Submission history From: Jiaheng Liu [view email]
    [v1] Mon, 3 Jun 2024 14:24:29 UTC (13,688 KB)
    [v2] Tue, 4 Jun 2024 03:13:43 UTC (13,732 KB)
    Ken Deng, Jiaheng Liu, He Zhu, Congnan Liu, Jingxin Li, Jiakai Wang, Peng Zhao, Chenchen Zhang, Yanan Wu, Xueqiao Yin, Yuanxing Zhang, Wenbo Su, Bangyu Xiang, Tiezheng Ge, Bo Zheng

Code completion models have made significant progress in recent years. Recently, repository-level code completion has drawn more attention in modern software development, and several baseline methods and benchmarks have been proposed. However, existing repository-level code completion methods often fall short of fully using the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies. Besides, the existing benchmarks usually focus on limited code completion scenarios, which cannot reflect the repository-level code completion abilities well of existing methods. To address these limitations, we propose the R2C2-Coder to enhance and benchmark the real-world repository-level code completion abilities of code Large Language Models, where the R2C2-Coder includes a code prompt construction method R2C2-Enhance and a well-designed benchmark R2C2-Bench. Specifically, first, in R2C2-Enhance, we first construct the candidate retrieval pool and then assemble the completion prompt by retrieving from the retrieval pool for each completion cursor position. Second, based on R2C2 -Enhance, we can construct a more challenging and diverse R2C2-Bench with training, validation and test splits, where a context perturbation strategy is proposed to simulate the real-world repository-level code completion well. Extensive results on multiple benchmarks demonstrate the effectiveness of our R2C2-Coder.

------------

---------------
Accelerate (16)
---------------

`[2406.01721] Rotation and Permutation for Advanced Outlier Management and Efficient Quantization of LLMs <https://arxiv.org/abs/2406.01721>`__ 旋转和排列用于高级异常值管理和llm的高效量化

::

    Mon, 3 Jun 2024 18:27:44 GMT
    Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, Ying Wei

Quantizing large language models (LLMs) presents significant challenges, primarily due to outlier activations that compromise the efficiency of low-bit representation. Traditional approaches mainly focus on solving Normal Outliers-activations with consistently high magnitudes across all tokens.
However, these techniques falter when dealing with Massive Outliers, which are significantly higher in value and often cause substantial performance losses during low-bit quantization. In this study, we propose DuQuant, an innovative quantization strategy employing rotation and permutation transformations to more effectively eliminate both types of outliers. Initially, DuQuant constructs rotation matrices informed by specific outlier dimensions, redistributing these outliers across adjacent channels within different rotation blocks. Subsequently, a zigzag permutation is applied to ensure a balanced distribution of outliers among blocks, minimizing block-wise variance.
An additional rotation further enhances the smoothness of the activation landscape, thereby improving model performance. DuQuant streamlines the quantization process and demonstrates superior outlier management, achieving top-tier results in multiple tasks with various LLM architectures even under 4-bit weight-activation quantization. Our code is available at https://github.com/Hsu1023/DuQuant.

------------

`[2406.02069] PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling <https://arxiv.org/abs/2406.02069>`__ PyramidKV:基于金字塔信息漏斗的动态KV缓存压缩

::

    Tue, 4 Jun 2024 07:51:30 GMT
    Zefan Cai., Yichi Zhang, Bofei Gao, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu and Wen Xiao

In this study, we investigate whether attention-based information flow inside large language models (LLMs) is aggregated through noticeable patterns for long context processing. Our observations reveal that LLMs aggregate information through Pyramidal Information Funneling where attention is scattering widely in lower layers, progressively consolidating within specific contexts, and ultimately focusin on critical tokens (a.k.a massive activation or attention sink) in higher layers. Motivated by these insights, we developed PyramidKV, a novel and effective KV cache compression method. This approach dynamically adjusts the KV cache size across different layers, allocating more cache in lower layers and less in higher ones, diverging from traditional methods that maintain a uniform KV cache size. Our experimental evaluations, utilizing the LongBench benchmark, show that PyramidKV matches the performance of models with a full KV cache while retaining only 12% of the KV cache, thus significantly reducing memory usage. In scenarios emphasizing memory efficiency, where only 0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache compression techniques achieving up to a 20.5 absolute accuracy improvement on TREC.

------------

`[2406.02120] Diver: Large Language Model Decoding with Span-Level Mutual Information Verification <https://arxiv.org/abs/2406.02120>`__ 潜水员:基于跨级互信息验证的大型语言模型解码

::

    Tue, 4 Jun 2024 09:02:22 GMT
    Jinliang Lu, Chen Wang and Jiajun Zhang

Large language models (LLMs) have shown impressive capabilities in adapting to various tasks when provided with task-specific instructions. However, LLMs using standard decoding strategies often struggle with deviations from the inputs. Intuitively, compliant LLM outputs should reflect the information present in the input, which can be measured by point-wise mutual information (PMI) scores. Therefore, we propose Diver, a novel approach that enhances LLM Decoding through span-level PMI verification. During inference, Diver first identifies divergence steps that may lead to multiple candidate spans.
Subsequently, it calculates the PMI scores by assessing the log-likelihood gains of the input if the candidate spans are generated. Finally, the optimal span is selected based on the PMI re-ranked output distributions. We evaluate our method across various downstream tasks, and empirical results demonstrate that Diver significantly outperforms existing decoding methods in both performance and versatility.

------------

`[2406.02532] SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices <https://arxiv.org/abs/2406.02532>`__ SpecExec:消费设备上交互式LLM推理的大规模并行推测解码

::

    Tue, 4 Jun 2024 17:53:36 GMT
    Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin

As large language models gain widespread adoption, running them efficiently becomes crucial. Recent works on LLM inference use speculative decoding to achieve extreme speedups. However, most of these works implicitly design their algorithms for high-end datacenter hardware. In this work, we ask the opposite question: how fast can we run LLMs on consumer machines? Consumer GPUs can no longer fit the largest available models (50B+ parameters) and must offload them to RAM or SSD. When running with offloaded parameters, the inference engine can process batches of hundreds or thousands of tokens at the same time as just one token, making it a natural fit for speculative decoding. We propose SpecExec (Speculative Execution), a simple parallel decoding method that can generate up to 20 tokens per target model iteration for popular LLM families. It utilizes the high spikiness of the token probabilities distribution in modern LLMs and a high degree of alignment between model output probabilities. SpecExec takes the most probable tokens continuation from the draft model to build a "cache" tree for the target model, which then gets validated in a single pass. Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens per second with 16-bit weights.

------------

`[2406.02214] SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining <https://arxiv.org/abs/2406.02214>`__ SLTrain:一种参数和内存高效的稀疏低秩预训练方法

::

    Tue, 4 Jun 2024 11:14:21 GMT
    Andi Han, Jiaxiang Li, Wei Huang, Mingyi Hong, Akiko Takeda, Pratik Jawanpuria, Bamdev Mishra

Large language models (LLMs) have shown impressive capabilities across various tasks. However, training LLMs from scratch requires significant computational power and extensive memory capacity. Recent studies have explored low-rank structures on weights for efficient fine-tuning in terms of parameters and memory, either through low-rank adaptation or factorization. While effective for fine-tuning, low-rank structures are generally less suitable for pretraining because they restrict parameters to a low-dimensional subspace. In this work, we propose to parameterize the weights as a sum of low-rank and sparse matrices for pretraining, which we call SLTrain. The low-rank component is learned via matrix factorization, while for the sparse component, we employ a simple strategy of uniformly selecting the sparsity support at random and learning only the non-zero entries with the fixed support. While being simple, the random fixed-support sparse learning strategy significantly enhances pretraining when combined with low-rank learning. Our results show that SLTrain adds minimal extra parameters and memory costs compared to pretraining with low-rank parameterization, yet achieves substantially better performance, which is comparable to full-rank training. Remarkably, when combined with quantization and per-layer updates, SLTrain can reduce memory requirements by up to 73% when pretraining the LLaMA 7B model.

------------

`[2406.02542] Loki: Low-Rank Keys for Efficient Sparse Attention <https://arxiv.org/abs/2406.02542>`__ Loki:高效稀疏注意力的低秩键

::

    Tue, 4 Jun 2024 17:58:03 GMT
    Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, Abhinav Bhatele

Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.

------------

`[2406.02368] Large Language Models Make Sample-Efficient Recommender Systems <https://arxiv.org/abs/2406.02368>`__ 大型语言模型形成样本高效的推荐系统

::

    Tue, 4 Jun 2024 14:46:25 GMT
    Jianghao Lin, Xinyi Dai, Rong Shan, Bo Chen, Ruiming Tang, Yong Yu, Weinan Zhang

Large language models (LLMs) have achieved remarkable progress in the field of natural language processing (NLP), demonstrating remarkable abilities in producing text that resembles human language for various tasks. This opens up new opportunities for employing them in recommender systems (RSs). In this paper, we specifically examine the sample efficiency of LLM-enhanced recommender systems, which pertains to the model's capacity to attain superior performance with a limited quantity of training data. Conventional recommendation models (CRMs) often need a large amount of training data because of the sparsity of features and interactions. Hence, we propose and verify our core viewpoint: Large Language Models Make Sample-Efficient Recommender Systems. We propose a simple yet effective framework (i.e., Laser) to validate the viewpoint from two aspects: (1) LLMs themselves are sample-efficient recommenders; and (2) LLMs, as feature generators and encoders, make CRMs more sample-efficient. Extensive experiments on two public datasets show that Laser requires only a small fraction of training samples to match or even surpass CRMs that are trained on the entire training set, demonstrating superior sample efficiency.

------------

`[2310.07075] Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding <https://arxiv.org/abs/2310.07075>`__ 不要微调，解码:通过受限解码无语法错误的工具使用

::

    replaced with revised version Tue, 4 Jun 2024 15:50:22 GMT
    Submission history From: Kexun Zhang [view email]
    [v1] Tue, 10 Oct 2023 23:37:53 UTC (505 KB)
    [v2] Sun, 26 May 2024 02:16:18 UTC (609 KB)
    [v3] Tue, 4 Jun 2024 15:50:22 UTC (609 KB)
    Kexun Zhang, Hongqiao Chen, Lei Li, William Wang

Instruction-tuned large language models (LLMs) excel at many tasks but often fail to use external tools due to complicated and unfamiliar syntax constraints. While extensive fine-tuning and prompting can mitigate the issue, these approaches are expensive and hard to generalize. Furthermore, because syntax constraints are only learned implicitly during fine-tuning, models still make frequent syntax errors. Motivated by the fact that these constraints can be better satisfied explicitly with constrained decoding, we propose TOOLDEC, a decoding algorithm using finite state machines to force LLMs to follow tool syntax. Our experiments show that TOOLDEC eliminates all syntax errors, achieving significantly better performance on various base models and benchmarks. More surprisingly, when applied to generalist out-of-the-box LLMs such as Mistral-Instruct, TOOLDEC improves its accuracy in tool use from the initial 0% to an impressive 52%, matching the performance of specialized fine-tuned models such as ToolLLM.

------------

`[2311.08263] Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster <https://arxiv.org/abs/2311.08263>`__ 快速思维链:通过并行解码一瞥未来使答案更快

::

    replaced with revised version Tue, 4 Jun 2024 03:10:34 GMT
    Submission history From: Hongxuan Zhang [view email]
    [v1] Tue, 14 Nov 2023 15:56:18 UTC (210 KB)
    [v2] Tue, 4 Jun 2024 03:10:34 UTC (833 KB)
    Hongxuan Zhang, Zhining Liu, Yao Zhao, Jiaqi Zheng, Chenyi Zhuang, Jinjie Gu and Guihai Chen

In this work, we propose FastCoT, a model-agnostic framework based on parallel decoding without any further training of an auxiliary model or modification to the LLM itself. FastCoT uses a size-varying context window whose size changes with position to conduct parallel decoding and auto-regressive decoding simultaneously, thus fully utilizing GPU computation resources. In FastCoT, the parallel decoding part provides the LLM with a quick glance of the future composed of approximate tokens, which could lead to faster answers compared to regular autoregressive decoding used by causal transformers. We also provide an implementation of parallel decoding within LLM, which supports KV-cache generation and batch processing. Through extensive experiments, we demonstrate that FastCoT saves inference time by nearly 20% with only a negligible performance drop compared to the regular approach. Additionally, we show that the context window size exhibits considerable robustness for different tasks.

------------

`[2401.07851] Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding <https://arxiv.org/abs/2401.07851>`__ 解锁大型语言模型推理中的效率:推测解码综合综述

::

    replaced with revised version Tue, 4 Jun 2024 17:08:37 GMT
    Submission history From: Heming Xia [view email]
    [v1] Mon, 15 Jan 2024 17:26:50 UTC (2,017 KB)
    [v2] Tue, 20 Feb 2024 10:24:57 UTC (1,345 KB)
    [v3] Tue, 4 Jun 2024 17:08:37 UTC (1,348 KB)
    Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui

To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.

------------

`[2401.12200] APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference <https://arxiv.org/abs/2401.12200>`__ APT:面向高效训练和推理的自适应剪枝和调优预训练语言模型

::

    replaced with revised version Tue, 4 Jun 2024 06:39:23 GMT
    Submission history From: Bowen Zhao [view email]
    [v1] Mon, 22 Jan 2024 18:39:40 UTC (468 KB)
    [v2] Tue, 4 Jun 2024 06:39:23 UTC (1,608 KB)
    Bowen Zhao, Hannaneh Hajishirzi, Qingqing Cao

Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively prunes and tunes parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models' performance with 70% parameters remained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces large LMs memory training footprint by up to 70%.

------------

`[2402.06126] Learn To be Efficient: Build Structured Sparsity in Large Language Models <https://arxiv.org/abs/2402.06126>`__ 学习高效:在大型语言模型中构建结构化稀疏性

::

    replaced with revised version Mon, 3 Jun 2024 18:28:58 GMT
    Submission history From: Haizhong Zheng [view email]
    [v1] Fri, 9 Feb 2024 01:18:16 UTC (1,329 KB)
    [v2] Tue, 13 Feb 2024 16:38:03 UTC (1,329 KB)
    [v3] Mon, 3 Jun 2024 18:28:58 UTC (2,954 KB)
    Haizhong Zheng, Xiaoyan Bai, Xueshen Liu, Z. Morley Mao, Beidi Chen, Fan Lai, Atul Prakash

Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. However, existing methods only focus on utilizing this naturally formed activation sparsity in a post-training setting, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity. To achieve this, we introduce a novel training algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like LLaMA using non-ReLU activations. Extensive evaluation on language understanding, language generation, and instruction tuning tasks show that LTE consistently outperforms SOTA baselines. Along with our hardware-aware custom kernel implementation, LTE reduces LLaMA2-7B inference latency by 25% at 50% sparsity.

------------

`[2403.15447] Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression <https://arxiv.org/abs/2403.15447>`__ 压缩信任解码:高效llm在压缩下的可信性审查

::

    replaced with revised version Tue, 4 Jun 2024 05:40:12 GMT
    Submission history From: Junyuan Hong [view email]
    [v1] Mon, 18 Mar 2024 01:38:19 UTC (1,676 KB)
    [v2] Mon, 3 Jun 2024 14:49:00 UTC (1,676 KB)
    [v3] Tue, 4 Jun 2024 05:40:12 UTC (1,677 KB)
    Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian Bartoldson, Ajay Jaiswal, Kaidi Xu, Bhavya Kailkhura, Dan Hendrycks, Dawn Song, Zhangyang Wang, Bo Li

Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of three (3) leading LLMs using five (5) SoTA compression techniques across eight (8) trustworthiness dimensions. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% sparsity. Moreover, employing quantization within a moderate bit range could unexpectedly improve certain trustworthiness dimensions such as ethics and fairness. Conversely, extreme quantization to very low bit levels (3 bits) tends to reduce trustworthiness significantly. This increased risk cannot be uncovered by looking at benign performance alone, in turn, mandating comprehensive trustworthiness evaluation in practice. These findings culminate in practical recommendations for simultaneously achieving high utility, efficiency, and trustworthiness in LLMs. Code and models are available at this https URL.

------------

`[2405.10637] Layer-Condensed KV Cache for Efficient Inference of Large Language Models <https://arxiv.org/abs/2405.10637>`__ 面向大型语言模型高效推理的层压缩KV缓存

::

    replaced with revised version Tue, 4 Jun 2024 00:08:10 GMT
    Submission history From: Haoyi Wu [view email]
    [v1] Fri, 17 May 2024 08:59:46 UTC (8,097 KB)
    [v2] Tue, 4 Jun 2024 00:08:10 UTC (8,097 KB)
    Haoyi Wu, Kewei Tu

Huge memory consumption has been a major bottleneck for deploying high-throughput large language models in real-world applications. In addition to the large number of parameters, the key-value (KV) cache for the attention mechanism in the transformer architecture consumes a significant amount of memory, especially when the number of layers is large for deep language models. In this paper, we propose a novel method that only computes and caches the KVs of a small number of layers, thus significantly saving memory consumption and improving inference throughput. Our experiments on large language models show that our method achieves up to 26$\times$ higher throughput than standard transformers and competitive performance in language modeling and downstream tasks. In addition, our method is orthogonal to existing transformer memory-saving techniques, so it is straightforward to integrate them with our model, achieving further improvement in inference efficiency. Our code is available at this https URL.

------------

`[2305.16617] Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model <https://arxiv.org/abs/2305.16617>`__ 基于贝叶斯代理模型的llm生成文本高效检测

::

    replaced with revised version Tue, 4 Jun 2024 07:05:48 GMT
    Submission history From: Yibo Miao [view email]
    [v1] Fri, 26 May 2023 04:23:10 UTC (706 KB)
    [v2] Tue, 28 May 2024 16:10:59 UTC (513 KB)
    [v3] Tue, 4 Jun 2024 07:05:48 UTC (557 KB)
    Yibo Miao, Hongcheng Gao, Hao Zhang, Zhijie Deng

The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries.

------------

`[2406.00965] Efficient Behavior Tree Planning with Commonsense Pruning and Heuristic <https://arxiv.org/abs/2406.00965>`__ 基于常识剪枝和启发式的高效行为树规划

::

    replaced with revised version Tue, 4 Jun 2024 01:41:24 GMT
    Submission history From: Xinglin Chen [view email]
    [v1] Mon, 3 Jun 2024 03:38:56 UTC (2,496 KB)
    [v2] Tue, 4 Jun 2024 01:41:24 UTC (2,620 KB)
    Xinglin Chen, Yishuai Cai, Yunxin Mao, Minglong Li, Zhou Yang, Wen Shanghua, Wenjing Yang, Weixia Xu, Ji Wang

Behavior Tree (BT) planning is crucial for autonomous robot behavior control, yet its application in complex scenarios is hampered by long planning times. Pruning and heuristics are common techniques to accelerate planning, but it is difficult to design general pruning strategies and heuristic functions for BT planning problems. This paper proposes improving BT planning efficiency for everyday service robots leveraging commonsense reasoning provided by Large Language Models (LLMs), leading to model-free pre-planning action space pruning and heuristic generation. This approach takes advantage of the modularity and interpretability of BT nodes, represented by predicate logic, to enable LLMs to predict the task-relevant action predicates and objects, and even the optimal path, without an explicit action model. We propose the Heuristic Optimal Behavior Tree Expansion Algorithm (HOBTEA) with two heuristic variants and provide a formal comparison and discussion of their efficiency and optimality. We introduce a learnable and transferable commonsense library to enhance the LLM's reasoning performance without fine-tuning. The action space expansion based on the commonsense library can further increase the success rate of planning. Experiments show the theoretical bounds of commonsense pruning and heuristic, and demonstrate the actual performance of LLM learning and reasoning with the commonsense library. Results in four datasets showcase the practical effectiveness of our approach in everyday service robot applications.

------------

-----------------------
In-Context Learning (4)
-----------------------

`[2406.01860] Eliciting the Priors of Large Language Models using Iterated In-Context Learning <https://arxiv.org/abs/2406.01860>`__ 利用迭代上下文学习获取大型语言模型的先验信息

::

    Tue, 4 Jun 2024 00:09:43 GMT
    Jian-Qiao Zhu, Thomas L. Griffiths

As Large Language Models (LLMs) are increasingly deployed in real-world settings, understanding the knowledge they implicitly use when making decisions is critical. One way to capture this knowledge is in the form of Bayesian prior distributions. We develop a prompt-based workflow for eliciting prior distributions from LLMs. Our approach is based on iterated learning, a Markov chain Monte Carlo method in which successive inferences are chained in a way that supports sampling from the prior distribution. We validated our method in settings where iterated learning has previously been used to estimate the priors of human participants -- causal learning, proportion estimation, and predicting everyday quantities. We found that priors elicited from GPT-4 qualitatively align with human priors in these settings. We then used the same method to elicit priors from GPT-4 for a variety of speculative events, such as the timing of the development of superhuman AI.

------------

`[2406.01808] In-Context Learning of Physical Properties: Few-Shot Adaptation to Out-of-Distribution Molecular Graphs <https://arxiv.org/abs/2406.01808>`__ 物理性质的上下文学习:分布外分子图的少样本适应

::

    Mon, 3 Jun 2024 21:59:21 GMT
    Grzegorz Kaszuba, Amirhossein D. Naghdi, Dario Massa, Stefanos Papanikolaou, Andrzej Jaszkiewicz, Piotr Sankowski

Large language models manifest the ability of few-shot adaptation to a sequence of provided examples. This behavior, known as in-context learning, allows for performing nontrivial machine learning tasks during inference only.
In this work, we address the question: can we leverage in-context learning to predict out-of-distribution materials properties? However, this would not be possible for structure property prediction tasks unless an effective method is found to pass atomic-level geometric features to the transformer model. To address this problem, we employ a compound model in which GPT-2 acts on the output of geometry-aware graph neural networks to adapt in-context information.
To demonstrate our model's capabilities, we partition the QM9 dataset into sequences of molecules that share a common substructure and use them for in-context learning. This approach significantly improves the performance of the model on out-of-distribution examples, surpassing the one of general graph neural network models.

------------

`[2406.02550] Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks <https://arxiv.org/abs/2406.02550>`__ 学习grok:模块算术任务中情境学习和技能构成的出现

::

    Tue, 4 Jun 2024 17:59:36 GMT
    Tianyu He, Darshil Doshi, Aritra Das, Andrey Gromov

Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions $z = a \, x + b \, y \;\mathrm{mod}\; p$ labeled by the vector $(a, b) \in \mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is \emph{transient}, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing the highly structured representations in both phases; and discuss the learnt algorithm.

------------

`[2305.13016] Iterative Forward Tuning Boosts In-Context Learning in Language Models <https://arxiv.org/abs/2305.13016>`__ 迭代前向调优促进语言模型的上下文学习

::

    replaced with revised version Tue, 4 Jun 2024 05:49:01 GMT
    Submission history From: Jiaxi Yang [view email]
    [v1] Mon, 22 May 2023 13:18:17 UTC (1,591 KB)
    [v2] Tue, 30 May 2023 05:47:19 UTC (1,591 KB)
    [v3] Tue, 4 Jun 2024 05:49:01 UTC (1,955 KB)
    Jiaxi Yang, Binyuan Hui, Min Yang, Bailin Wang, Bowen Li, Binhua Li, Fei Huang, Yongbin Li

Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.

------------

-------------
Reasoning (6)
-------------

`[2406.02030] Multimodal Reasoning with Multimodal Knowledge Graph <https://arxiv.org/abs/2406.02030>`__ 基于多模态知识图谱的多模态推理

::

    Tue, 4 Jun 2024 07:13:23 GMT
    Junlin Lee and Yequan Wang and Jing Li and Min Zhang

Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs.
Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, MR-MKG achieves superior performance while training on only a small fraction of parameters, approximately 2.25% of the LLM's parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR-MKG method outperforms previous state-of-the-art models.

------------

`[2406.02106] MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset <https://arxiv.org/abs/2406.02106>`__ MARS:基于多任务评估数据集的语言模型形而上学推理能力基准测试

::

    Tue, 4 Jun 2024 08:35:04 GMT
    Weiqi Wang, Yangqiu Song

To enable Large Language Models (LLMs) to function as conscious agents with generalizable reasoning capabilities, it is crucial that they possess the reasoning ability to comprehend situational changes (transitions) in distribution triggered by environmental factors or actions from other agents.
Despite its fundamental significance, this ability remains underexplored due to the complexity of modeling infinite possible changes in an event and their associated distributions, coupled with the lack of benchmark data with situational transitions. Addressing these gaps, we propose a novel formulation of reasoning with distributional changes as a three-step discriminative process, termed as MetAphysical ReaSoning. We then introduce the first-ever benchmark, MARS, comprising three tasks corresponding to each step. These tasks systematically assess LLMs' capabilities in reasoning the plausibility of (i) changes in actions, (ii) states caused by changed actions, and (iii) situational transitions driven by changes in action. Extensive evaluations with 20 (L)LMs of varying sizes and methods indicate that all three tasks in this process pose significant challenges, even for state-of-the-art LLMs and LMs after fine-tuning. Further analyses reveal potential causes for the underperformance of LLMs and demonstrate that pre-training them on large-scale conceptualization taxonomies can potentially enhance their metaphysical reasoning capabilities. Our data and models are publicly accessible at https://github.com/HKUST-KnowComp/MARS.

------------

`[2406.02301] mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models <https://arxiv.org/abs/2406.02301>`__ mCoT:语言模型推理一致性的多语言指令调优

::

    Tue, 4 Jun 2024 13:30:45 GMT
    Huiyuan Lai, Malvina Nissim

Large language models (LLMs) with Chain-of-thought (CoT) have recently emerged as a powerful technique for eliciting reasoning to improve various downstream tasks. As most research mainly focuses on English, with few explorations in a multilingual context, the question of how reliable this reasoning capability is in different languages is still open. To address it directly, we study multilingual reasoning consistency across multiple languages, using popular open-source LLMs. First, we compile the first large-scale multilingual math reasoning dataset, mCoT-MATH, covering eleven diverse languages. Then, we introduce multilingual CoT instruction tuning to boost reasoning capability across languages, thereby improving model consistency. While existing LLMs show substantial variation across the languages we consider, and especially low performance for lesser resourced languages, our 7B parameter model mCoT achieves impressive consistency across languages, and superior or comparable performance to close- and open-source models even of much larger sizes.

------------

`[2406.02061] Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models <https://arxiv.org/abs/2406.02061>`__ Alice in Wonderland:在最先进的大型语言模型中显示完全推理分解的简单任务

::

    Tue, 4 Jun 2024 07:43:33 GMT
    Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, Jenia Jitsev

Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale. These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models. We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans. The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical "reasoning"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible. Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs, Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/AIW

------------

`[2311.08516] LLMs cannot find reasoning errors, but can correct them given the error location <https://arxiv.org/abs/2311.08516>`__ llm不能发现推理错误，但可以根据错误位置纠正它们

::

    replaced with revised version Tue, 4 Jun 2024 10:25:13 GMT
    Submission history From: Gladys Tyen [view email]
    [v1] Tue, 14 Nov 2023 20:12:38 UTC (7,191 KB)
    [v2] Tue, 9 Jan 2024 03:32:32 UTC (7,191 KB)
    [v3] Tue, 4 Jun 2024 10:25:13 UTC (7,319 KB)
    Gladys Tyen, Hassan Mansoor, Victor C\u{a}rbune, Peter Chen, Tony Mak

While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023b; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we show that poor self-correction performance stems from LLMs' inability to find logical mistakes, rather than their ability to correct a known mistake. Firstly, we benchmark several state-of-the-art LLMs on their mistake-finding ability and demonstrate that they generally struggle with the task, even in highly objective, unambiguous cases. Secondly, we test the correction abilities of LLMs -- separately from mistake finding -- using a backtracking setup that feeds ground truth mistake location information to the model. We show that this boosts downstream task performance across our 5 reasoning tasks, indicating that LLMs' correction abilities are robust. Finally, we show that it is possible to obtain mistake location information without ground truth labels or in-domain training data. We train a small classifier with out-of-domain data, which exhibits stronger mistake-finding performance than prompting a large model. We release our dataset of LLM-generated logical mistakes, BIG-Bench Mistake, to enable further research into locating LLM reasoning mistakes.

------------

`[2406.00922] MEDIQ: Question-Asking LLMs for Adaptive and Reliable Clinical Reasoning <https://arxiv.org/abs/2406.00922>`__ MEDIQ:询问LLMs，以获得适应性和可靠的临床推理

::

    replaced with revised version Tue, 4 Jun 2024 13:55:05 GMT
    Submission history From: Shuyue Stella Li [view email]
    [v1] Mon, 3 Jun 2024 01:32:52 UTC (8,552 KB)
    [v2] Tue, 4 Jun 2024 13:55:05 UTC (8,551 KB)
    Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson, Pang Wei Koh, Yulia Tsvetkov

In high-stakes domains like clinical reasoning, AI assistants powered by large language models (LLMs) are yet to be reliable and safe. We identify a key obstacle towards reliability: existing LLMs are trained to answer any question, even with incomplete context in the prompt or insufficient parametric knowledge. We propose to change this paradigm to develop more careful LLMs that ask follow-up questions to gather necessary and sufficient information and respond reliably. We introduce MEDIQ, a framework to simulate realistic clinical interactions, which incorporates a Patient System and an adaptive Expert System. The Patient may provide incomplete information in the beginning; the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details from the Patient via follow-up questions. To evaluate MEDIQ, we convert MEDQA and CRAFT-MD -- medical benchmarks for diagnostic question answering -- into an interactive setup. We develop a reliable Patient system and prototype several Expert systems, first showing that directly prompting state-of-the-art LLMs to ask questions degrades the quality of clinical reasoning, indicating that adapting LLMs to interactive information-seeking settings is nontrivial. We then augment the Expert with a novel abstention module to better estimate model confidence and decide whether to ask more questions, thereby improving diagnostic accuracy by 20.3%; however, performance still lags compared to an (unrealistic in practice) upper bound when full information is given upfront. Further analyses reveal that interactive performance can be improved by filtering irrelevant contexts and reformatting conversations. Overall, our paper introduces a novel problem towards LLM reliability, a novel MEDIQ framework, and highlights important future directions to extend the information-seeking abilities of LLM assistants in critical domains.

------------

-----------
ToolUse (4)
-----------

`[2405.20956] A Robot Walks into a Bar: Can Language Models Serve as Creativity Support Tools for Comedy? An Evaluation of LLMs' Humour Alignment with Comedians <https://arxiv.org/abs/2405.20956>`__ 机器人走进酒吧:语言模型可以作为喜剧的创造力支持工具吗?LLMs与喜剧演员的幽默一致性评估

::

    replaced with revised version Mon, 3 Jun 2024 21:01:50 GMT
    Submission history From: Piotr Mirowski [view email]
    [v1] Fri, 31 May 2024 15:55:51 UTC (313 KB)
    [v2] Mon, 3 Jun 2024 21:01:50 UTC (313 KB)
    Piotr Wojciech Mirowski, Juliette Love, Kory W. Mathewson, Shakir Mohamed

We interviewed twenty professional comedians who perform live shows in front of audiences and who use artificial intelligence in their artistic process as part of 3-hour workshops on ``AI x Comedy'' conducted at the Edinburgh Festival Fringe in August 2023 and online. The workshop consisted of a comedy writing session with large language models (LLMs), a human-computer interaction questionnaire to assess the Creativity Support Index of AI as a writing tool, and a focus group interrogating the comedians' motivations for and processes of using AI, as well as their ethical concerns about bias, censorship and copyright. Participants noted that existing moderation strategies used in safety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by erasing minority groups and their perspectives, and qualified this as a form of censorship. At the same time, most participants felt the LLMs did not succeed as a creativity support tool, by producing bland and biased comedy tropes, akin to ``cruise ship comedy material from the 1950s, but a bit less racist''. Our work extends scholarship about the subtle difference between, one the one hand, harmful speech, and on the other hand, ``offensive'' language as a practice of resistance, satire and ``punching up''. We also interrogate the global value alignment behind such language models, and discuss the importance of community-based value alignment and data ownership to build AI tools that better suit artists' needs.

------------

`[2310.07075] Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding <https://arxiv.org/abs/2310.07075>`__ 不要微调，解码:通过受限解码无语法错误的工具使用

::

    replaced with revised version Tue, 4 Jun 2024 15:50:22 GMT
    Submission history From: Kexun Zhang [view email]
    [v1] Tue, 10 Oct 2023 23:37:53 UTC (505 KB)
    [v2] Sun, 26 May 2024 02:16:18 UTC (609 KB)
    [v3] Tue, 4 Jun 2024 15:50:22 UTC (609 KB)
    Kexun Zhang, Hongqiao Chen, Lei Li, William Wang

Instruction-tuned large language models (LLMs) excel at many tasks but often fail to use external tools due to complicated and unfamiliar syntax constraints. While extensive fine-tuning and prompting can mitigate the issue, these approaches are expensive and hard to generalize. Furthermore, because syntax constraints are only learned implicitly during fine-tuning, models still make frequent syntax errors. Motivated by the fact that these constraints can be better satisfied explicitly with constrained decoding, we propose TOOLDEC, a decoding algorithm using finite state machines to force LLMs to follow tool syntax. Our experiments show that TOOLDEC eliminates all syntax errors, achieving significantly better performance on various base models and benchmarks. More surprisingly, when applied to generalist out-of-the-box LLMs such as Mistral-Instruct, TOOLDEC improves its accuracy in tool use from the initial 0% to an impressive 52%, matching the performance of specialized fine-tuned models such as ToolLLM.

------------

`[2312.04455] Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use <https://arxiv.org/abs/2312.04455>`__ 加强注意力:增强大型语言模型的上下文感知，以有效地使用工具

::

    replaced with revised version Tue, 4 Jun 2024 07:33:12 GMT
    Submission history From: Yuhan Chen [view email]
    [v1] Thu, 7 Dec 2023 17:24:51 UTC (1,209 KB)
    [v2] Sun, 11 Feb 2024 13:01:16 UTC (1,771 KB)
    [v3] Fri, 1 Mar 2024 07:56:37 UTC (1,771 KB)
    [v4] Tue, 4 Jun 2024 07:33:12 UTC (2,077 KB)
    Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li and Rui Yan

In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchmark, our method elevates a 7B model to achieve state-of-the-art performance, comparable to that of GPT-4. On other benchmarks and some RAG tasks, which also demand a thorough understanding of contextual content, Attention Buckets also exhibited notable enhancements in performance.

------------

`[2402.09615] API Pack: A Massive Multi-Programming Language Dataset for API Call Generation <https://arxiv.org/abs/2402.09615>`__ API包:用于API调用生成的大规模多编程语言数据集

::

    replaced with revised version Mon, 3 Jun 2024 22:38:04 GMT
    Submission history From: Zhen Guo [view email]
    [v1] Wed, 14 Feb 2024 23:09:15 UTC (1,561 KB)
    [v2] Fri, 16 Feb 2024 13:58:38 UTC (1,561 KB)
    [v3] Fri, 31 May 2024 17:31:38 UTC (708 KB)
    [v4] Mon, 3 Jun 2024 22:38:04 UTC (711 KB)
    Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda

We introduce API Pack, a massive multi-programming language dataset containing more than 1 million instruction-API call pairs to improve the API call generation capabilities of large language models. By fine-tuning CodeLlama-13B on 20,000 Python instances from API Pack, we enable it to outperform GPT-3.5 and GPT-4 in generating unseen API calls. Fine-tuning on API Pack also facilitates cross-programming language generalization by leveraging a large amount of data in one language and small amounts of data from other languages. Scaling the training data to 1 million instances further improves the model's ability to generalize to new APIs not used in training. To facilitate further research, we open-source the API Pack dataset, trained model, and associated source code at this https URL.

------------

-----------------------
Retrieval-Augmented (6)
-----------------------

`[2406.01876] GRAM: Generative Retrieval Augmented Matching of Data Schemas in the Context of Data Security <https://arxiv.org/abs/2406.01876>`__ GRAM:数据安全背景下数据模式的生成式检索增强匹配

::

    Tue, 4 Jun 2024 01:08:00 GMT
    Xuanqing Liu, Luyang Kong, Runhui Wang, Patrick Song, Austin Nevins, Henrik Johnson, Nimish Amlathe, Davor Golac

Schema matching constitutes a pivotal phase in the data ingestion process for contemporary database systems. Its objective is to discern pairwise similarities between two sets of attributes, each associated with a distinct data table. This challenge emerges at the initial stages of data analytics, such as when incorporating a third-party table into existing databases to inform business insights. Given its significance in the realm of database systems, schema matching has been under investigation since the 2000s. This study revisits this foundational problem within the context of large language models. Adhering to increasingly stringent data security policies, our focus lies on the zero-shot and few-shot scenarios: the model should analyze only a minimal amount of customer data to execute the matching task, contrasting with the conventional approach of scrutinizing the entire data table. We emphasize that the zero-shot or few-shot assumption is imperative to safeguard the identity and privacy of customer data, even at the potential cost of accuracy.
The capability to accurately match attributes under such stringent requirements distinguishes our work from previous literature in this domain.

------------

`[2406.02135] Robust Interaction-based Relevance Modeling for Online E-Commerce and LLM-based Retrieval <https://arxiv.org/abs/2406.02135>`__ 基于鲁棒交互的在线电子商务相关建模与llm检索

::

    Tue, 4 Jun 2024 09:24:04 GMT
    Ben Chen, Huangyu Dai, Xiang Ma, Wen Jiang, Wei Ning

Semantic relevance calculation is crucial for e-commerce search engines, as it ensures that the items selected closely align with customer intent.
Inadequate attention to this aspect can detrimentally affect user experience and engagement. Traditional text-matching techniques are prevalent but often fail to capture the nuances of search intent accurately, so neural networks now have become a preferred solution to processing such complex text matching.
Existing methods predominantly employ representation-based architectures, which strike a balance between high traffic capacity and low latency. However, they exhibit significant shortcomings in generalization and robustness when compared to interaction-based architectures. In this work, we introduce a robust interaction-based modeling paradigm to address these shortcomings. It encompasses 1) a dynamic length representation scheme for expedited inference, 2) a professional terms recognition method to identify subjects and core attributes from complex sentence structures, and 3) a contrastive adversarial training protocol to bolster the model's robustness and matching capabilities.
Extensive offline evaluations demonstrate the superior robustness and effectiveness of our approach, and online A/B testing confirms its ability to improve relevance in the same exposure position, resulting in more clicks and conversions. To the best of our knowledge, this method is the first interaction-based approach for large e-commerce search relevance calculation.
Notably, we have deployed it for the entire search traffic on alibaba.com, the largest B2B e-commerce platform in the world.

------------

`[2402.03181] C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models <https://arxiv.org/abs/2402.03181>`__ C-RAG:检索增强语言模型的认证生成风险

::

    replaced with revised version Tue, 4 Jun 2024 04:51:08 GMT
    Submission history From: Mintong Kang [view email]
    [v1] Mon, 5 Feb 2024 16:46:16 UTC (14,509 KB)
    [v2] Mon, 12 Feb 2024 22:19:17 UTC (14,509 KB)
    [v3] Sun, 3 Mar 2024 18:13:54 UTC (14,509 KB)
    [v4] Tue, 4 Jun 2024 04:51:08 UTC (7,891 KB)
    Mintong Kang, Nezihe Merve G\"urel, Ning Yu, Dawn Song, Bo Li

Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.

------------

`[2405.20680] Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2405.20680>`__ 检索增强大型语言模型中检索器不一致性的解开和缓解

::

    replaced with revised version Tue, 4 Jun 2024 11:51:53 GMT
    Submission history From: Mingda Li [view email]
    [v1] Fri, 31 May 2024 08:22:49 UTC (5,343 KB)
    [v2] Mon, 3 Jun 2024 06:20:18 UTC (5,343 KB)
    [v3] Tue, 4 Jun 2024 11:51:53 UTC (5,343 KB)
    Mingda Li, Xinyu Li, Yifan Chen, Wenfeng Xuan, Weinan Zhang

Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their superiority in terms of factuality, they do not consistently outperform the original retrieval-free Language Models (LMs). Our experiments reveal that this example-level performance inconsistency exists not only between retrieval-augmented and retrieval-free LM but also among different retrievers. To understand this phenomenon, we investigate the degeneration behavior of RALMs and theoretically decompose it into four categories. Further analysis based on our decomposition reveals that the innate difference in knowledge sources and the unpredictable degeneration of the reader model contribute most to the inconsistency. Drawing from our analysis, we introduce Ensemble of Retrievers (EoR), a trainable framework that can adaptively retrieve from different knowledge sources and effectively decrease unpredictable reader errors. Our experiments on Open Domain Question Answering show that EoR substantially improves performance over the RALM with a single retriever by considerably reducing inconsistent behaviors.

------------

`[2402.07092] Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation <https://arxiv.org/abs/2402.07092>`__ 基于llm -认知数据增强的会话密集检索泛化

::

    replaced with revised version Tue, 4 Jun 2024 02:09:15 GMT
    Submission history From: Haonan Chen [view email]
    [v1] Sun, 11 Feb 2024 03:27:22 UTC (1,011 KB)
    [v2] Mon, 19 Feb 2024 05:37:46 UTC (581 KB)
    [v3] Tue, 4 Jun 2024 02:09:15 UTC (1,013 KB)
    Haonan Chen, Zhicheng Dou, Kelong Mao, Jiongnan Liu, Ziliang Zhao

Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space. A contrastive learning objective is then employed to train a better conversational context encoder. Extensive experiments conducted on four public datasets, under both normal and zero-shot settings, demonstrate the effectiveness, generalizability, and applicability of ConvAug. The code is released at this https URL.

------------

`[2402.13542] ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling <https://arxiv.org/abs/2402.13542>`__ ARL2:基于自引导自适应相关标记的黑盒大型语言模型检索器对齐

::

    replaced with revised version Tue, 4 Jun 2024 05:17:24 GMT
    Submission history From: Yue Yu [view email]
    [v1] Wed, 21 Feb 2024 05:41:34 UTC (9,241 KB)
    [v2] Tue, 4 Jun 2024 05:17:24 UTC (9,241 KB)
    Lingxi Zhang, Yue Yu, Kuan Wang, Chao Zhang

Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities. Our code will be published at \url{this https URL}.

------------

---------
Agent (4)
---------

`[2406.01637] Teams of LLM Agents can Exploit Zero-Day Vulnerabilities <https://arxiv.org/abs/2406.01637>`__ LLM代理团队可以利用零日漏洞

::

    Sun, 2 Jun 2024 16:25:26 GMT
    Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang

LLM agents have become increasingly sophisticated, especially in the realm of cybersecurity. Researchers have shown that LLM agents can exploit real-world vulnerabilities when given a description of the vulnerability and toy capture-the-flag problems. However, these agents still perform poorly on real-world vulnerabilities that are unknown to the agent ahead of time (zero-day vulnerabilities).
In this work, we show that teams of LLM agents can exploit real-world, zero-day vulnerabilities. Prior agents struggle with exploring many different vulnerabilities and long-range planning when used alone. To resolve this, we introduce HPTSA, a system of agents with a planning agent that can launch subagents. The planning agent explores the system and determines which subagents to call, resolving long-term planning issues when trying different vulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and show that our team of agents improve over prior work by up to 4.5$\times$.

------------

`[2406.01893] Large Language Model-Enabled Multi-Agent Manufacturing Systems <https://arxiv.org/abs/2406.01893>`__ 大型语言模型支持的多智能体制造系统

::

    Tue, 4 Jun 2024 01:57:37 GMT
    Jonghan Lim, Birgit Vogel-Heuser, Ilya Kovalenko

Traditional manufacturing faces challenges adapting to dynamic environments and quickly responding to manufacturing changes. The use of multi-agent systems has improved adaptability and coordination but requires further advancements in rapid human instruction comprehension, operational adaptability, and coordination through natural language integration. Large language models like GPT-3.5 and GPT-4 enhance multi-agent manufacturing systems by enabling agents to communicate in natural language and interpret human instructions for decision-making. This research introduces a novel framework where large language models enhance the capabilities of agents in manufacturing, making them more adaptable, and capable of processing context-specific instructions. A case study demonstrates the practical application of this framework, showing how agents can effectively communicate, understand tasks, and execute manufacturing processes, including precise G-code allocation among agents. The findings highlight the importance of continuous large language model integration into multi-agent manufacturing systems and the development of sophisticated agent communication protocols for a more flexible manufacturing system.

------------

`[2404.07677] ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs <https://arxiv.org/abs/2404.07677>`__ ODA:用于集成llm和知识图谱的观察驱动代理

::

    replaced with revised version Tue, 4 Jun 2024 07:16:14 GMT
    Submission history From: Lei Sun [view email]
    [v1] Thu, 11 Apr 2024 12:16:16 UTC (1,946 KB)
    [v2] Tue, 4 Jun 2024 07:16:14 UTC (1,949 KB)
    Lei Sun, Zhengwei Tao, Youdi Li, Hiroshi Arakawa

The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM's analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via global observation, which enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%.

------------

`[2402.04247] Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science <https://arxiv.org/abs/2402.04247>`__ 维护优先于自主:LLM代理对科学的风险

::

    replaced with revised version Mon, 3 Jun 2024 21:45:53 GMT
    Submission history From: Xiangru Tang [view email]
    [v1] Tue, 6 Feb 2024 18:54:07 UTC (12,836 KB)
    [v2] Wed, 7 Feb 2024 14:26:02 UTC (12,836 KB)
    [v3] Mon, 3 Jun 2024 21:45:53 UTC (12,838 KB)
    Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein

Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, these agents, called scientific LLM agents, also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This perspective paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.

------------

-----------
Other (109)
-----------

`[2406.02057] Tabular and Deep Learning for the Whittle Index <https://arxiv.org/abs/2406.02057>`__ Whittle索引的表格和深度学习

::

    Tue, 4 Jun 2024 07:41:15 GMT
    Francisco Robledo Rela\~no (LMAP, UPPA, UPV / EHU), Vivek Borkar (EE-IIT), Urtzi Ayesta (IRIT-RMESS, UPV/EHU, CNRS), Konstantin Avrachenkov (Inria)

The Whittle index policy is a heuristic that has shown remarkably good performance (with guaranteed asymptotic optimality) when applied to the class of problems known as Restless Multi-Armed Bandit Problems (RMABPs). In this paper we present QWI and QWINN, two reinforcement learning algorithms, respectively tabular and deep, to learn the Whittle index for the total discounted criterion. The key feature is the use of two time-scales, a faster one to update the state-action Q -values, and a relatively slower one to update the Whittle indices. In our main theoretical result we show that QWI, which is a tabular implementation, converges to the real Whittle indices. We then present QWINN, an adaptation of QWI algorithm using neural networks to compute the Q -values on the faster time-scale, which is able to extrapolate information from one state to another and scales naturally to large state-space environments. For QWINN, we show that all local minima of the Bellman error are locally stable equilibria, which is the first result of its kind for DQN-based schemes. Numerical computations show that QWI and QWINN converge faster than the standard Q -learning algorithm, neural-network based approximate Q-learning and other state of the art algorithms.

------------

`[2406.01749] Towards Harnessing Large Language Models for Comprehension of Conversational Grounding <https://arxiv.org/abs/2406.01749>`__ 利用大型语言模型来理解会话基础

::

    Mon, 3 Jun 2024 19:34:39 GMT
    Kristiina Jokinen, Phillip Schneider, Taiga Mori

Conversational grounding is a collaborative mechanism for establishing mutual knowledge among participants engaged in a dialogue. This experimental study analyzes information-seeking conversations to investigate the capabilities of large language models in classifying dialogue turns related to explicit or implicit grounding and predicting grounded knowledge elements. Our experimental results reveal challenges encountered by large language models in the two tasks and discuss ongoing research efforts to enhance large language model-based conversational grounding comprehension through pipeline architectures and knowledge bases. These initiatives aim to develop more effective dialogue systems that are better equipped to handle the intricacies of grounded knowledge in conversations.

------------

`[2406.01771] LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback <https://arxiv.org/abs/2406.01771>`__ 超越英语的llm:用跨语言反馈扩展llm的多语言能力

::

    Mon, 3 Jun 2024 20:25:12 GMT
    Wen Lai, Mohsen Mesgar, Alexander Fraser

To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low-resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages.
We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages.

------------

`[2406.01775] OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models <https://arxiv.org/abs/2406.01775>`__ OLoRA:大型语言模型的标准正交低秩自适应

::

    Mon, 3 Jun 2024 20:37:27 GMT
    Kerim B\"uy\"ukaky\"uz

The advent of large language models (LLMs) has revolutionized natural language processing, enabling unprecedented capabilities in understanding and generating human-like text. However, the computational cost and convergence times associated with fine-tuning these models remain significant challenges.
Low-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these issues by introducing efficient fine-tuning techniques with a reduced number of trainable parameters. In this paper, we present OLoRA, an enhancement to the LoRA method that leverages orthonormal matrix initialization through QR decomposition. OLoRA significantly accelerates the convergence of LLM training while preserving the efficiency benefits of LoRA, such as the number of trainable parameters and GPU memory footprint. Our empirical evaluations demonstrate that OLoRA not only converges faster but also exhibits improved performance compared to standard LoRA across a variety of language modeling tasks. This advancement opens new avenues for more efficient and accessible fine-tuning of LLMs, potentially enabling broader adoption and innovation in natural language applications.

------------

`[2406.01806] Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation <https://arxiv.org/abs/2406.01806>`__ 上下文序列似然:自然语言生成的增强置信度分数

::

    Mon, 3 Jun 2024 21:55:07 GMT
    Zhen Lin, Shubhendu Trivedi, Jimeng Sun

The advent of large language models (LLMs) has dramatically advanced the state-of-the-art in numerous natural language generation tasks. For LLMs to be applied reliably, it is essential to have an accurate measure of their confidence. Currently, the most commonly used confidence score function is the likelihood of the generated sequence, which, however, conflates semantic and syntactic components. For instance, in question-answering (QA) tasks, an awkward phrasing of the correct answer might result in a lower probability prediction. Additionally, different tokens should be weighted differently depending on the context. In this work, we propose enhancing the predicted sequence probability by assigning different weights to various tokens using attention values elicited from the base LLM. By employing a validation set, we can identify the relevant attention heads, thereby significantly improving the reliability of the vanilla sequence probability confidence measure. We refer to this new score as the Contextualized Sequence Likelihood (CSL). CSL is easy to implement, fast to compute, and offers considerable potential for further improvement with task-specific prompts. Across several QA datasets and a diverse array of LLMs, CSL has demonstrated significantly higher reliability than state-of-the-art baselines in predicting generation quality, as measured by the AUROC or AUARC.

------------

`[2406.01855] TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability <https://arxiv.org/abs/2406.01855>`__ TruthEval:评估LLM真实性和可靠性的数据集

::

    Tue, 4 Jun 2024 00:01:35 GMT
    Aisha Khatun and Daniel G. Brown

Large Language Model (LLM) evaluation is currently one of the most important areas of research, with existing benchmarks proving to be insufficient and not completely representative of LLMs' various capabilities. We present a curated collection of challenging statements on sensitive topics for LLM benchmarking called TruthEval. These statements were curated by hand and contain known truth values. The categories were chosen to distinguish LLMs' abilities from their stochastic nature. We perform some initial analyses using this dataset and find several instances of LLMs failing in simple tasks showing their inability to understand simple questions.

------------

`[2406.01931] Dishonesty in Helpful and Harmless Alignment <https://arxiv.org/abs/2406.01931>`__ 有益无害的不诚实行为

::

    Tue, 4 Jun 2024 03:31:09 GMT
    Youcheng Huang, Jingkun Tang, Duanyu Feng, Zheng Zhang, Wenqiang Lei, Jiancheng Lv, Anthony G. Cohn

People tell lies when seeking rewards. Large language models (LLMs) are aligned to human values with reinforcement learning where they get rewards if they satisfy human preference. We find that this also induces dishonesty in helpful and harmless alignment where LLMs tell lies in generating harmless responses. Using the latest interpreting tools, we detect dishonesty, show how LLMs can be harmful if their honesty is increased, and analyze such conflicts at the parameter-level. Given these preliminaries and the hypothesis that reward-seeking stimulates dishonesty, we theoretically show that the dishonesty can in-turn decrease the alignment performances and augment reward-seeking alignment with representation regularization. Extensive results, including GPT-4 annotated win-rates, perplexities, and cases studies demonstrate that we can train more honest, helpful, and harmless LLMs. We will make all our codes and results be open-sourced upon this paper's acceptance.

------------

`[2406.01940] Process-Driven Autoformalization in Lean 4 <https://arxiv.org/abs/2406.01940>`__ 精益4中的过程驱动的自动形式化

::

    Tue, 4 Jun 2024 03:48:08 GMT
    Jianqiao Lu, Zhengying Liu, Yingjia Wan, Yinya Huang, Haiming Wang, Zhicheng Yang, Jing Tang, Zhijiang Guo

Autoformalization, the conversion of natural language mathematics into formal languages, offers significant potential for advancing mathematical reasoning.
However, existing efforts are limited to formal languages with substantial online corpora and struggle to keep pace with rapidly evolving languages like Lean 4. To bridge this gap, we propose a new benchmark \textbf{Form}alization for \textbf{L}ean~\textbf{4} (\textbf{\name}) designed to evaluate the autoformalization capabilities of large language models (LLMs). This benchmark encompasses a comprehensive assessment of questions, answers, formal statements, and proofs. Additionally, we introduce a \textbf{P}rocess-\textbf{S}upervised \textbf{V}erifier (\textbf{PSV}) model that leverages the precise feedback from Lean 4 compilers to enhance autoformalization. Our experiments demonstrate that the PSV method improves autoformalization, enabling higher accuracy using less filtered training data.
Furthermore, when fine-tuned with data containing detailed process information, PSV can leverage the data more effectively, leading to more significant improvements in autoformalization for Lean 4. Our dataset and code are available at \url{https://github.com/rookie-joe/PDA}.

------------

`[2406.01943] Enhancing Trust in LLMs: Algorithms for Comparing and Interpreting LLMs <https://arxiv.org/abs/2406.01943>`__ 增强对llm的信任:比较和解释llm的算法

::

    Tue, 4 Jun 2024 03:54:53 GMT
    Nik Bear Brown

This paper surveys evaluation techniques to enhance the trustworthiness and understanding of Large Language Models (LLMs). As reliance on LLMs grows, ensuring their reliability, fairness, and transparency is crucial. We explore algorithmic methods and metrics to assess LLM performance, identify weaknesses, and guide development towards more trustworthy applications. Key evaluation metrics include Perplexity Measurement, NLP metrics (BLEU, ROUGE, METEOR, BERTScore, GLEU, Word Error Rate, Character Error Rate), Zero-Shot and Few-Shot Learning Performance, Transfer Learning Evaluation, Adversarial Testing, and Fairness and Bias Evaluation. We introduce innovative approaches like LLMMaps for stratified evaluation, Benchmarking and Leaderboards for competitive assessment, Stratified Analysis for in-depth understanding, Visualization of Blooms Taxonomy for cognitive level accuracy distribution, Hallucination Score for quantifying inaccuracies, Knowledge Stratification Strategy for hierarchical analysis, and Machine Learning Models for Hierarchy Generation.
Human Evaluation is highlighted for capturing nuances that automated metrics may miss. These techniques form a framework for evaluating LLMs, aiming to enhance transparency, guide development, and establish user trust. Future papers will describe metric visualization and demonstrate each approach on practical examples.

------------

`[2406.01981] Zyda: A 1.3T Dataset for Open Language Modeling <https://arxiv.org/abs/2406.01981>`__ Zyda:开放语言建模的1.3T数据集

::

    Tue, 4 Jun 2024 05:47:17 GMT
    Yury Tokpanov, Beren Millidge, Paolo Glorioso, Jonathan Pilault, Adam Ibrahim, James Whittington, Quentin Anthony

The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.

------------

`[2406.01983] RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models <https://arxiv.org/abs/2406.01983>`__ RKLD:基于反向kl散度的知识蒸馏用于大型语言模型中个人信息遗忘

::

    Tue, 4 Jun 2024 05:51:43 GMT
    Bichen Wang, Yuzhe Zi, Yixin Sun, Yanyan Zhao, Bing Qin

With the passage of the Right to Be Forgotten (RTBF) regulations and the scaling up of language model training datasets, research on model unlearning in large language models (LLMs) has become more crucial. Before the era of LLMs, machine unlearning research focused mainly on classification tasks in models with small parameters. In these tasks, the content to be forgotten or retained is clear and straightforward. However, as parameter sizes have grown and tasks have become more complex, balancing forget quality and model utility has become more challenging, especially in scenarios involving personal data instead of classification results. Existing methods based on gradient ascent and its variants often struggle with this balance, leading to unintended information loss or partial forgetting. To address this challenge, we propose RKLD, a novel \textbf{R}everse \textbf{KL}-Divergence-based Knowledge \textbf{D}istillation unlearning algorithm for LLMs targeting the unlearning of personal information.
Through RKLD, we achieve significant forget quality and effectively maintain the model utility in our experiments.

------------

`[2406.02002] Position Debiasing Fine-Tuning for Causal Perception in Long-Term Dialogue <https://arxiv.org/abs/2406.02002>`__ 长期对话中因果感知的位置去偏差微调

::

    Tue, 4 Jun 2024 06:33:13 GMT
    Shixuan Fan, Wei Wei, Wendi Li, Xian-Ling Mao, Wenfeng Xie, Dangyang Chen

The core of the dialogue system is to generate relevant, informative, and human-like responses based on extensive dialogue history. Recently, dialogue generation domain has seen mainstream adoption of large language models (LLMs), due to its powerful capability in generating utterances. However, there is a natural deficiency for such models, that is, inherent position bias, which may lead them to pay more attention to the nearby utterances instead of causally relevant ones, resulting in generating irrelevant and generic responses in long-term dialogue. To alleviate such problem, in this paper, we propose a novel method, named Causal Perception long-term Dialogue framework (CPD), which employs perturbation-based causal variable discovery method to extract casually relevant utterances from the dialogue history and enhances model causal perception during fine-tuning. Specifically, a local-position awareness method is proposed in CPD for inter-sentence position correlation elimination, which helps models extract causally relevant utterances based on perturbations. Then, a casual-perception fine-tuning strategy is also proposed, to enhance the capability of discovering the causal invariant factors, by differently perturbing causally relevant and non-casually relevant ones for response generation. Experimental results on two datasets prove that our proposed method can effectively alleviate the position bias for multiple LLMs and achieve significant progress compared with existing baselines.

------------

`[2406.02018] Why Would You Suggest That? Human Trust in Language Model Responses <https://arxiv.org/abs/2406.02018>`__ 你为什么这么说?人类对语言模型反应的信任

::

    Tue, 4 Jun 2024 06:57:47 GMT
    Manasi Sharma, Ho Chit Siu, Rohan Paleja, Jaime D. Pe\~na

The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount. Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance. Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses. Position and faithfulness of these explanations are also important factors. However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation. Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems.

------------

`[2406.02044] QROA: A Black-Box Query-Response Optimization Attack on LLMs <https://arxiv.org/abs/2406.02044>`__ QROA:一种针对llm的黑盒查询响应优化攻击

::

    Tue, 4 Jun 2024 07:27:36 GMT
    Hussein Jawad, Nicolas J.-B. BRUNEL (LaMME)

Large Language Models (LLMs) have surged in popularity in recent months, yet they possess concerning capabilities for generating harmful content when manipulated. This study introduces the Query-Response Optimization Attack (QROA), an optimization-based strategy designed to exploit LLMs through a black-box, query-only interaction. QROA adds an optimized trigger to a malicious instruction to compel the LLM to generate harmful content. Unlike previous approaches, QROA does not require access to the model's logit information or any other internal data and operates solely through the standard query-response interface of LLMs. Inspired by deep Q-learning and Greedy coordinate descent, the method iteratively updates tokens to maximize a designed reward function. We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\%. We also tested the model against Llama2-chat, the fine-tuned version of Llama2 designed to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger seed. This study demonstrates the feasibility of generating jailbreak attacks against deployed LLMs in the public domain using black-box optimization methods, enabling more comprehensive safety testing of LLMs.

------------

`[2406.02050] Analyzing Social Biases in Japanese Large Language Models <https://arxiv.org/abs/2406.02050>`__ 日语大型语言模型中的社会偏见分析

::

    Tue, 4 Jun 2024 07:31:06 GMT
    Hitomi Yanaka, Han Namgi, Ryoma Kumon, Jie Lu, Masashi Takeshita, Ryo Sekizawa, Taisei Kato, Hiromi Arai

With the development of Large Language Models (LLMs), social biases in the LLMs have become a crucial issue. While various benchmarks for social biases have been provided across languages, the extent to which Japanese LLMs exhibit social biases has not been fully investigated. In this study, we construct the Japanese Bias Benchmark dataset for Question Answering (JBBQ) based on the English bias benchmark BBQ, and analyze social biases in Japanese LLMs. The results show that while current Japanese LLMs improve their accuracies on JBBQ by instruction-tuning, their bias scores become larger. In addition, augmenting their prompts with warning about social biases reduces the effect of biases in some models.

------------

`[2406.02060] I've got the "Answer"! Interpretation of LLMs Hidden States in Question Answering <https://arxiv.org/abs/2406.02060>`__ 我找到了“答案”!答疑中的llm隐藏状态解读

::

    Tue, 4 Jun 2024 07:43:12 GMT
    Valeriya Goloviznina and Evgeny Kotelnikov

Interpretability and explainability of AI are becoming increasingly important in light of the rapid development of large language models (LLMs). This paper investigates the interpretation of LLMs in the context of the knowledge-based question answering. The main hypothesis of the study is that correct and incorrect model behavior can be distinguished at the level of hidden states.
The quantized models LLaMA-2-7B-Chat, Mistral-7B, Vicuna-7B and the MuSeRC question-answering dataset are used to test this hypothesis. The results of the analysis support the proposed hypothesis. We also identify the layers which have a negative effect on the model's behavior. As a prospect of practical application of the hypothesis, we propose to train such "weak" layers additionally in order to improve the quality of the task solution.

------------

`[2406.02079] Assessing the Performance of Chinese Open Source Large Language Models in Information Extraction Tasks <https://arxiv.org/abs/2406.02079>`__ 中文开源大型语言模型在信息抽取任务中的性能评估

::

    Tue, 4 Jun 2024 08:00:40 GMT
    Yida Cai, Hao Sun, Hsiu-Yuan Huang, Yunfang Wu

Information Extraction (IE) plays a crucial role in Natural Language Processing (NLP) by extracting structured information from unstructured text, thereby facilitating seamless integration with various real-world applications that rely on structured data. Despite its significance, recent experiments focusing on English IE tasks have shed light on the challenges faced by Large Language Models (LLMs) in achieving optimal performance, particularly in sub-tasks like Named Entity Recognition (NER). In this paper, we delve into a comprehensive investigation of the performance of mainstream Chinese open-source LLMs in tackling IE tasks, specifically under zero-shot conditions where the models are not fine-tuned for specific tasks. Additionally, we present the outcomes of several few-shot experiments to further gauge the capability of these models. Moreover, our study includes a comparative analysis between these open-source LLMs and ChatGPT, a widely recognized language model, on IE performance. Through meticulous experimentation and analysis, we aim to provide insights into the strengths, limitations, and potential enhancements of existing Chinese open-source LLMs in the domain of Information Extraction within the context of NLP.

------------

`[2406.02100] Exploring Mathematical Extrapolation of Large Language Models with Synthetic Data <https://arxiv.org/abs/2406.02100>`__ 用合成数据探索大型语言模型的数学外推

::

    Tue, 4 Jun 2024 08:30:37 GMT
    Haolong Li, Yu Ma, Yinqi Zhang, Chen Ye, Jie Chen

Large Language Models (LLMs) have shown excellent performance in language understanding, text generation, code synthesis, and many other tasks, while they still struggle in complex multi-step reasoning problems, such as mathematical reasoning. In this paper, through a newly proposed arithmetical puzzle problem, we show that the model can perform well on multi-step reasoning tasks via fine-tuning on high-quality synthetic data. Experimental results with the open-llama-3B model on three different test datasets show that not only the model can reach a zero-shot pass@1 at 0.44 on the in-domain dataset, it also demonstrates certain generalization capabilities on the out-of-domain datasets.
Specifically, this paper has designed two out-of-domain datasets in the form of extending the numerical range and the composing components of the arithmetical puzzle problem separately. The fine-tuned models have shown encouraging performance on these two far more difficult tasks with the zero-shot pass@1 at 0.33 and 0.35, respectively.

------------

`[2406.02110] UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models <https://arxiv.org/abs/2406.02110>`__ UniOQA:基于大型语言模型的知识图谱统一问答框架

::

    Tue, 4 Jun 2024 08:36:39 GMT
    Zhuoyang Li, Liran Deng, Hui Liu, Qiaoqiao Liu and Junzhao Du

OwnThink stands as the most extensive Chinese open-domain knowledge graph introduced in recent times. Despite prior attempts in question answering over OwnThink (OQA), existing studies have faced limitations in model representation capabilities, posing challenges in further enhancing overall accuracy in question answering. In this paper, we introduce UniOQA, a unified framework that integrates two complementary parallel workflows. Unlike conventional approaches, UniOQA harnesses large language models (LLMs) for precise question answering and incorporates a direct-answer-prediction process as a cost-effective complement. Initially, to bolster representation capacity, we fine-tune an LLM to translate questions into the Cypher query language (CQL), tackling issues associated with restricted semantic understanding and hallucinations. Subsequently, we introduce the Entity and Relation Replacement algorithm to ensure the executability of the generated CQL. Concurrently, to augment overall accuracy in question answering, we further adapt the Retrieval-Augmented Generation (RAG) process to the knowledge graph.
Ultimately, we optimize answer accuracy through a dynamic decision algorithm.
Experimental findings illustrate that UniOQA notably advances SpCQL Logical Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new state-of-the-art results on this benchmark. Through ablation experiments, we delve into the superior representation capacity of UniOQA and quantify its performance breakthrough.

------------

`[2406.02134] The current status of large language models in summarizing radiology report impressions <https://arxiv.org/abs/2406.02134>`__ 大型语言模型在总结放射学报告印象中的现状

::

    Tue, 4 Jun 2024 09:23:30 GMT
    Danqing Hu, Shanyuan Zhang, Qing Liu, Xiaofeng Zhu, Bing Liu

Large language models (LLMs) like ChatGPT show excellent capabilities in various natural language processing tasks, especially for text generation. The effectiveness of LLMs in summarizing radiology report impressions remains unclear. In this study, we explore the capability of eight LLMs on the radiology report impression summarization. Three types of radiology reports, i.e., CT, PET-CT, and Ultrasound reports, are collected from Peking University Cancer Hospital and Institute. We use the report findings to construct the zero-shot, one-shot, and three-shot prompts with complete example reports to generate the impressions. Besides the automatic quantitative evaluation metrics, we define five human evaluation metrics, i.e., completeness, correctness, conciseness, verisimilitude, and replaceability, to evaluate the semantics of the generated impressions. Two thoracic surgeons (ZSY and LB) and one radiologist (LQ) compare the generated impressions with the reference impressions and score each impression under the five human evaluation metrics.
Experimental results show that there is a gap between the generated impressions and reference impressions. Although the LLMs achieve comparable performance in completeness and correctness, the conciseness and verisimilitude scores are not very high. Using few-shot prompts can improve the LLMs' performance in conciseness and verisimilitude, but the clinicians still think the LLMs can not replace the radiologists in summarizing the radiology impressions.

------------

`[2406.02143] Reinforcement Tuning for Detecting Stances and Debunking Rumors Jointly with Large Language Models <https://arxiv.org/abs/2406.02143>`__ 联合大型语言模型检测立场和揭穿谣言的强化调优

::

    Tue, 4 Jun 2024 09:31:18 GMT
    Ruichao Yang, Wei Gao, Jing Ma, Hongzhan Lin, Bo Wang

Learning multi-task models for jointly detecting stance and verifying rumors poses challenges due to the need for training data of stance at post level and rumor veracity at claim level, which are difficult to obtain. To address this issue, we leverage large language models (LLMs) as the foundation annotators for the joint stance detection (SD) and rumor verification (RV) tasks, dubbed as JSDRV. We introduce a novel reinforcement tuning framework to enhance the joint predictive capabilities of LLM-based SD and RV components. Specifically, we devise a policy for selecting LLM-annotated data at the two levels, employing a hybrid reward mechanism to choose high-quality labels for effective LLM fine-tuning on both tasks. Results demonstrate that JSDRV improves the capabilities of LLMs in the joint tasks, not only outperforming state-of-the-art methods but also generalizing to non-LLMs accommodated as task models.

------------

`[2406.02148] Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models <https://arxiv.org/abs/2406.02148>`__ 协同事件理解:基于大型语言模型的跨文档事件共指消解协同方法

::

    Tue, 4 Jun 2024 09:35:47 GMT
    Qingkai Min, Qipeng Guo, Xiangkun Hu, Songfang Huang, Zheng Zhang, Yue Zhang

Cross-document event coreference resolution (CDECR) involves clustering event mentions across multiple documents that refer to the same real-world events.
Existing approaches utilize fine-tuning of small language models (SLMs) like BERT to address the compatibility among the contexts of event mentions.
However, due to the complexity and diversity of contexts, these models are prone to learning simple co-occurrences. Recently, large language models (LLMs) like ChatGPT have demonstrated impressive contextual understanding, yet they encounter challenges in adapting to specific information extraction (IE) tasks.
In this paper, we propose a collaborative approach for CDECR, leveraging the capabilities of both a universally capable LLM and a task-specific SLM. The collaborative strategy begins with the LLM accurately and comprehensively summarizing events through prompting. Then, the SLM refines its learning of event representations based on these insights during fine-tuning. Experimental results demonstrate that our approach surpasses the performance of both the large and small language models individually, forming a complementary advantage. Across various datasets, our approach achieves state-of-the-art performance, underscoring its effectiveness in diverse scenarios.

------------

`[2406.02224] FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models <https://arxiv.org/abs/2406.02224>`__ FedMKT:大型和小型语言模型的联邦互知识迁移

::

    Tue, 4 Jun 2024 11:36:09 GMT
    Tao Fan, Guoqiang Ma, Yan Kang, Hanlin Gu, Lixin Fan, Qiang Yang

Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small language models (SLMs) at downstream clients. However, a significant gap remains in the simultaneous mutual enhancement of both the server's LLM and clients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient federated mutual knowledge transfer framework for large and small language models. This framework is designed to adaptively transfer knowledge from the server's LLM to clients' SLMs while concurrently enriching the LLM with clients' unique domain insights. We facilitate token alignment using minimum edit distance (MinED) and then selective mutual knowledge transfer between client-side SLMs and a server-side LLM, aiming to collectively enhance their performance. Through extensive experiments across three distinct scenarios, heterogeneous, homogeneous, and one-to-one, we evaluate the effectiveness of FedMKT using various public LLMs and SLMs on a range of NLP text generation tasks. Empirical results demonstrate significant performance improvements in clients' SLMs with the aid of the LLM. Furthermore, the LLM optimized by FedMKT achieves a performance comparable to that achieved through direct fine-tuning based on clients' data, highlighting the effectiveness and adaptability of FedMKT.

------------

`[2406.02267] Prompting Large Language Models with Human Error Markings for Self-Correcting Machine Translation <https://arxiv.org/abs/2406.02267>`__ 基于人工错误标记的大型语言模型的自纠错机器翻译

::

    Tue, 4 Jun 2024 12:43:47 GMT
    Nathaniel Berger, Stefan Riezler, Miriam Exel, Matthias Huck

While large language models (LLMs) pre-trained on massive amounts of unpaired language data have reached the state-of-the-art in machine translation (MT) of general domain texts, post-editing (PE) is still required to correct errors and to enhance term translation quality in specialized domains. In this paper we present a pilot study of enhancing translation memories (TM) produced by PE (source segments, machine translations, and reference translations, henceforth called PE-TM) for the needs of correct and consistent term translation in technical domains.
We investigate a light-weight two-step scenario where, at inference time, a human translator marks errors in the first translation step, and in a second step a few similar examples are extracted from the PE-TM to prompt an LLM. Our experiment shows that the additional effort of augmenting translations with human error markings guides the LLM to focus on a correction of the marked errors, yielding consistent improvements over automatic PE (APE) and MT from scratch.

------------

`[2406.02325] Technical Language Processing for Telecommunications Specifications <https://arxiv.org/abs/2406.02325>`__ 电信规范的技术语言处理

::

    Tue, 4 Jun 2024 13:57:22 GMT
    Felipe A. Rodriguez Y.

Large Language Models (LLMs) are continuously being applied in a more diverse set of contexts. At their current state, however, even state-of-the-art LLMs such as Generative Pre-Trained Transformer 4 (GTP-4) have challenges when extracting information from real-world technical documentation without a heavy preprocessing. One such area with real-world technical documentation is telecommunications engineering, which could greatly benefit from domain-specific LLMs. The unique format and overall structure of telecommunications internal specifications differs greatly from standard English and thus it is evident that the application of out-of-the-box Natural Language Processing (NLP) tools is not a viable option. In this article, we outline the limitations of out-of-the-box NLP tools for processing technical information generated by telecommunications experts, and expand the concept of Technical Language Processing (TLP) to the telecommunication domain.
Additionally, we explore the effect of domain-specific LLMs in the work of Specification Engineers, emphasizing the potential benefits of adopting domain-specific LLMs to speed up the training of experts in different telecommunications fields.

------------

`[2406.02350] LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing <https://arxiv.org/abs/2406.02350>`__ LlamaCare:用于增强医疗保健知识共享的大型医疗语言模型

::

    Tue, 4 Jun 2024 14:24:53 GMT
    Maojun Sun

Large language models (LLMs) have shown amazing capabilities in knowledge memorization and present. However, when it comes to domain-specific knowledge and downstream tasks like medical, general LLMs are often unable to give precise answers. In addition, when people want LLMs to answer classification questions, they usually go through instruction tuning first, however, LLMs do not always give a direct index of the categorization after instruction tuning.
In this paper, we proposed LlamaCare, a fine-tuned medical language model, and Extended Classification Integration(ECI), a module to handle classification problems of LLMs. Our contributions are : (i) We fine-tuned a large language model of medical knowledge with very low carbon emissions and achieved similar performance with ChatGPT by a 24G GPU. (ii) We solved the problem of redundant categorical answers and improved the performance of LLMs by proposing a new module called Extended Classification Integration. (iii) We released our processed data for one-shot and few-shot training for some benchmarks such as PubMedQA and USMLE 1-3 step. Our method achieves a close effect with the state-of-the-art model in benchmarks while costing lower GPU resources compared to LLMs with the same quantity of parameters. Our models, codes, and datasets can be found in https://github.com/Stephen-SMJ/LLamaCare

------------

`[2406.02376] Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs <https://arxiv.org/abs/2406.02376>`__ 高压缩比下保留关键信息:基于查询引导的llm压缩器

::

    Tue, 4 Jun 2024 14:53:24 GMT
    Zhiwei Cao, Qian Cao, Yu Lu, Ningxin Peng, Luyang Huang, Shanbo Cheng, Jinsong Su

The growing popularity of Large Language Models has sparked interest in context compression for Large Language Models (LLMs). However, the performance of previous methods degrades dramatically as compression ratios increase, sometimes even falling to the closed-book level. This decline can be attributed to the loss of key information during the compression process. Our preliminary study supports this hypothesis, emphasizing the significance of retaining key information to maintain model performance under high compression ratios. As a result, we introduce Query-Guided Compressor (QGC), which leverages queries to guide the context compression process, effectively preserving key information within the compressed context. Additionally, we employ a dynamic compression strategy. We validate the effectiveness of our proposed QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets.
Experimental results show that QGC can consistently perform well even at high compression ratios, which also offers significant benefits in terms of inference cost and throughput.

------------

`[2406.02378] On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept <https://arxiv.org/abs/2406.02378>`__ LLMs内在的自校正能力:不确定性与潜在概念

::

    Tue, 4 Jun 2024 14:55:43 GMT
    Guangliang Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, Kristen Johnson, Jiliang Tang, Rongrong Wang

Large Language Models (LLMs) can improve their responses when instructed to do so, a capability known as self-correction. When these instructions lack specific details about the issues in the response, this is referred to as leveraging the intrinsic self-correction capability. The empirical success of self-correction can be found in various applications, e.g., text detoxification and social bias mitigation. However, leveraging this self-correction capability may not always be effective, as it has the potential to revise an initially correct response into an incorrect one. In this paper, we endeavor to understand how and why leveraging the self-correction capability is effective.
We identify that appropriate instructions can guide LLMs to a convergence state, wherein additional self-correction steps do not yield further performance improvements. We empirically demonstrate that model uncertainty and activated latent concepts jointly characterize the effectiveness of self-correction. Furthermore, we provide a mathematical formulation indicating that the activated latent concept drives the convergence of the model uncertainty and self-correction performance. Our analysis can also be generalized to the self-correction behaviors observed in Vision-Language Models (VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from our principle in terms of selecting effective fine-tuning samples. Such initial success demonstrates the potential extensibility for better instruction tuning and safety alignment.

------------

`[2406.02394] Multiple Choice Questions and Large Languages Models: A Case Study with Fictional Medical Data <https://arxiv.org/abs/2406.02394>`__ 多项选择题和大型语言模型:基于虚构医疗数据的案例研究

::

    Tue, 4 Jun 2024 15:08:56 GMT
    Maxime Griot, Jean Vanderdonckt, Demet Yuksel, Coralie Hemptinne

Large Language Models (LLMs) like ChatGPT demonstrate significant potential in the medical field, often evaluated using multiple-choice questions (MCQs) similar to those found on the USMLE. Despite their prevalence in medical education, MCQs have limitations that might be exacerbated when assessing LLMs.
To evaluate the effectiveness of MCQs in assessing the performance of LLMs, we developed a fictional medical benchmark focused on a non-existent gland, the Glianorex. This approach allowed us to isolate the knowledge of the LLM from its test-taking abilities. We used GPT-4 to generate a comprehensive textbook on the Glianorex in both English and French and developed corresponding multiple-choice questions in both languages. We evaluated various open-source, proprietary, and domain-specific LLMs using these questions in a zero-shot setting. The models achieved average scores around 67%, with minor performance differences between larger and smaller models. Performance was slightly higher in English than in French. Fine-tuned medical models showed some improvement over their base versions in English but not in French. The uniformly high performance across models suggests that traditional MCQ-based benchmarks may not accurately measure LLMs' clinical knowledge and reasoning abilities, instead highlighting their pattern recognition skills. This study underscores the need for more robust evaluation methods to better assess the true capabilities of LLMs in medical contexts.

------------

`[2406.02481] Hiding Text in Large Language Models: Introducing Unconditional Token Forcing Confusion <https://arxiv.org/abs/2406.02481>`__ 在大型语言模型中隐藏文本:引入无条件标记强制混淆

::

    Tue, 4 Jun 2024 16:49:06 GMT
    Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki

With the help of simple fine-tuning, one can artificially embed hidden text into large language models (LLMs). This text is revealed only when triggered by a specific query to the LLM. Two primary applications are LLM fingerprinting and steganography. In the context of LLM fingerprinting, a unique text identifier (fingerprint) is embedded within the model to verify licensing compliance. In the context of steganography, the LLM serves as a carrier for hidden messages that can be disclosed through a designated trigger.
Our work demonstrates that embedding hidden text in the LLM via fine-tuning, though seemingly secure due to the vast number of potential triggers (any sequence of characters or tokens could serve as a trigger), is susceptible to extraction through analysis of the LLM's output decoding process. We propose a novel approach to extraction called Unconditional Token Forcing. It is premised on the hypothesis that iteratively feeding each token from the LLM's vocabulary into the model should reveal sequences with abnormally high token probabilities, indicating potential embedded text candidates. Additionally, our experiments show that when the first token of a hidden fingerprint is used as an input, the LLM not only produces an output sequence with high token probabilities, but also repetitively generates the fingerprint itself. We also present a method to hide text in such a way that it is resistant to Unconditional Token Forcing, which we named Unconditional Token Forcing Confusion.

------------

`[2406.02524] CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks <https://arxiv.org/abs/2406.02524>`__ CheckEmbed:对开放式任务的LLM解决方案的有效验证

::

    Tue, 4 Jun 2024 17:42:21 GMT
    Maciej Besta, Lorenzo Paleari, Ales Kubicek, Piotr Nyczyk, Robert Gerstenberger, Patrick Iff, Tomasz Lehmann, Hubert Niewiadomski, Torsten Hoefler

Large Language Models (LLMs) are revolutionizing various domains, yet verifying their answers remains a significant challenge, especially for intricate open-ended tasks such as consolidation, summarization, and extraction of knowledge. In this work, we propose CheckEmbed: an accurate, scalable, and simple LLM verification approach. CheckEmbed is driven by a straightforward yet powerful idea: in order to compare LLM solutions to one another or to the ground-truth, compare their corresponding answer-level embeddings obtained with a model such as GPT Text Embedding Large. This reduces a complex textual answer to a single embedding, facilitating straightforward, fast, and meaningful verification. We develop a comprehensive verification pipeline implementing the CheckEmbed methodology. The CheckEmbed pipeline also comes with metrics for assessing the truthfulness of the LLM answers, such as embedding heatmaps and their summaries. We show how to use these metrics for deploying practical engines that decide whether an LLM answer is satisfactory or not. We apply the pipeline to real-world document analysis tasks, including term extraction and document summarization, showcasing significant improvements in accuracy, cost-effectiveness, and runtime performance compared to existing token-, sentence-, and fact-level schemes such as BERTScore or SelfCheckGPT.

------------

`[2406.02528] Scalable MatMul-free Language Modeling <https://arxiv.org/abs/2406.02528>`__ 可扩展的无matmull语言建模

::

    Tue, 4 Jun 2024 17:50:34 GMT
    Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, Jason K. Eshraghian

Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at \url{https://github.com/ridgerchu/matmulfreellm}.

------------

`[2406.02536] Mitigate Position Bias in Large Language Models via Scaling a Single Dimension <https://arxiv.org/abs/2406.02536>`__ 通过扩展单一维度缓解大型语言模型中的位置偏差

::

    Tue, 4 Jun 2024 17:55:38 GMT
    Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, Lili Qiu

Large Language Models (LLMs) are increasingly applied in various real-world scenarios due to their excellent generalization capabilities and robust generative abilities. However, they exhibit position bias, also known as "lost in the middle", a phenomenon that is especially pronounced in long-context scenarios, which indicates the placement of the key information in different positions of a prompt can significantly affect accuracy. This paper first explores the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. It further identifies that, in addition to position embeddings, causal attention mask also contributes to position bias by creating position-specific hidden states. Based on these insights, we propose a method to mitigate position bias by scaling this positional hidden states. Experiments on the NaturalQuestions Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using various models including RoPE models, context windowextended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% by modifying just one dimension of hidden states. Our code is available at https://aka.ms/PositionalHidden.

------------

`[2406.01638] TimeCMA: Towards LLM-Empowered Time Series Forecasting via Cross-Modality Alignment <https://arxiv.org/abs/2406.01638>`__ TimeCMA:基于跨模态对齐的llm授权时间序列预测

::

    Mon, 3 Jun 2024 00:27:29 GMT
    Chenxi Liu, Qianxiong Xu, Hao Miao, Sun Yang, Lingzheng Zhang, Cheng Long, Ziyue Li, Rui Zhao

The widespread adoption of scalable mobile sensing has led to large amounts of time series data for real-world applications. A fundamental application is multivariate time series forecasting (MTSF), which aims to predict future time series values based on historical observations. Existing MTSF methods suffer from limited parameterization and small-scale training data. Recently, Large language models (LLMs) have been introduced in time series, which achieve promising forecasting performance but incur heavy computational costs. To solve these challenges, we propose TimeCMA, an LLM-empowered framework for time series forecasting with cross-modality alignment. We design a dual-modality encoding module with two branches, where the time series encoding branch extracts relatively low-quality yet pure embeddings of time series through an inverted Transformer. In addition, the LLM-empowered encoding branch wraps the same time series as prompts to obtain high-quality yet entangled prompt embeddings via a Pre-trained LLM. Then, we design a cross-modality alignment module to retrieve high-quality and pure time series embeddings from the prompt embeddings. Moreover, we develop a time series forecasting module to decode the aligned embeddings while capturing dependencies among multiple variables for forecasting. Notably, we tailor the prompt to encode sufficient temporal information into a last token and design the last token embedding storage to reduce computational costs. Extensive experiments on real data offer insight into the accuracy and efficiency of the proposed framework.

------------

`[2406.02128] Iteration Head: A Mechanistic Study of Chain-of-Thought <https://arxiv.org/abs/2406.02128>`__ 迭代头:思维链的机械性研究

::

    Tue, 4 Jun 2024 09:11:46 GMT
    Vivien Cabannes, Charles Arnal, Wassim Bouaziz, Alice Yang, Francois Charton, Julia Kempe

Chain-of-Thought (CoT) reasoning is known to improve Large Language Models both empirically and in terms of theoretical approximation power. However, our understanding of the inner workings and conditions of apparition of CoT capabilities remains limited. This paper helps fill this gap by demonstrating how CoT reasoning emerges in transformers in a controlled and interpretable setting. In particular, we observe the appearance of a specialized attention mechanism dedicated to iterative reasoning, which we coined "iteration heads".
We track both the emergence and the precise working of these iteration heads down to the attention level, and measure the transferability of the CoT skills to which they give rise between tasks.

------------

`[2406.02290] A Study of Optimizations for Fine-tuning Large Language Models <https://arxiv.org/abs/2406.02290>`__ 大型语言模型微调优化研究

::

    Tue, 4 Jun 2024 13:05:47 GMT
    Arjun Singh, Nikhil Pandey, Anup Shirgaonkar, Pavan Manoj, Vijay Aski

Fine-tuning large language models is a popular choice among users trying to adapt them for specific applications. However, fine-tuning these models is a demanding task because the user has to examine several factors, such as resource budget, runtime, model size and context length among others. A specific challenge is that fine-tuning is memory intensive, imposing constraints on the required hardware memory and context length of training data that can be handled. In this work, we share a detailed study on a variety of fine-tuning optimizations across different fine-tuning scenarios. In particular, we assess Gradient Checkpointing, Low Rank Adaptation, DeepSpeed's ZeRO Redundancy Optimizer and Flash Attention. With a focus on memory and runtime, we examine the impact of different optimization combinations on GPU memory usage and execution runtime during fine-tuning phase. We provide recommendation on best default optimization for balancing memory and runtime across diverse model sizes. We share effective strategies for fine-tuning very large models with tens or hundreds of billions of parameters and enabling large context lengths during fine-tuning. Furthermore, we propose the appropriate optimization mixtures for fine-tuning under GPU resource limitations.

------------

`[2406.02356] Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy Arithmetic Tasks <https://arxiv.org/abs/2406.02356>`__ 语言模型可以轻松地完成艰巨的算术任务，很难完成简单的算术任务

::

    Tue, 4 Jun 2024 14:34:39 GMT
    Andrew Gambardella, Yusuke Iwasawa, Yutaka Matsuo

The ability (and inability) of large language models (LLMs) to perform arithmetic tasks has been the subject of much theoretical and practical debate.
We show that LLMs are frequently able to correctly and confidently predict the first digit of n-digit by m-digit multiplication tasks without using chain of thought reasoning, despite these tasks require compounding operations to solve.
Simultaneously, LLMs in practice often fail to correctly or confidently predict the last digit of an n-digit by m-digit multiplication, a task equivalent to 1-digit by 1-digit multiplication which can be easily learned or memorized. We show that the latter task can be solved more robustly when the LLM is conditioned on all of the correct higher-order digits, which on average increases the confidence of the correct last digit on 5-digit by 5-digit multiplication tasks using Llama 2-13B by over 230% (0.13 to 0.43) and Mistral-7B by 150% (0.22 to 0.55).

------------

`[2406.02395] GrootVL: Tree Topology is All You Need in State Space Model <https://arxiv.org/abs/2406.02395>`__ GrootVL:树拓扑是状态空间模型中所需要的全部

::

    Tue, 4 Jun 2024 15:09:29 GMT
    Yicheng Xiao, Lin Song, Shaoli Huang, Jiangshan Wang, Siyu Song, Yixiao Ge, Xiu Li, Ying Shan

The state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency. However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies. To address this issue, we propose the GrootVL network, which first dynamically generates a tree topology based on spatial relationships and input features. Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities. Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. GrootVL is a versatile multimodal framework that can be applied to both visual and textual tasks. Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation.
Besides, by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost.

------------

`[2406.02479] Applying Fine-Tuned LLMs for Reducing Data Needs in Load Profile Analysis <https://arxiv.org/abs/2406.02479>`__ 应用微调llm减少负载剖面分析中的数据需求

::

    Sun, 2 Jun 2024 23:18:11 GMT
    Yi Hu, Hyeonjin Kim, Kai Ye, Ning Lu

This paper presents a novel method for utilizing fine-tuned Large Language Models (LLMs) to minimize data requirements in load profile analysis, demonstrated through the restoration of missing data in power system load profiles. A two-stage fine-tuning strategy is proposed to adapt a pre-trained LLMs, i.e., GPT-3.5, for missing data restoration tasks. Through empirical evaluation, we demonstrate the effectiveness of the fine-tuned model in accurately restoring missing data, achieving comparable performance to state-of-the-art specifically designed models such as BERT-PIN. Key findings include the importance of prompt engineering and the optimal utilization of fine-tuning samples, highlighting the efficiency of few-shot learning in transferring knowledge from general user cases to specific target users.
Furthermore, the proposed approach demonstrates notable cost-effectiveness and time efficiency compared to training models from scratch, making it a practical solution for scenarios with limited data availability and computing resources.
This research has significant potential for application to other power system load profile analysis tasks. Consequently, it advances the use of LLMs in power system analytics, offering promising implications for enhancing the resilience and efficiency of power distribution systems.

------------

`[2406.02500] Demystifying the Compression of Mixture-of-Experts Through a Unified Framework <https://arxiv.org/abs/2406.02500>`__ 通过统一的框架揭开了专家混合压缩的神秘面纱

::

    Tue, 4 Jun 2024 17:18:40 GMT
    Shwai He, Daize Dong, Liang Ding, Ang Li

Scaling large language models has revolutionized the performance across diverse domains, yet the continual growth in model size poses significant challenges for real-world deployment. The Mixture of Experts (MoE) approach addresses this by dynamically selecting and activating only a subset of experts, significantly reducing computational costs while maintaining high performance. However, MoE introduces potential redundancy (e.g., parameters) and extra costs (e.g., communication overhead). Despite numerous compression techniques developed for mitigating the redundancy in dense models, the compression of MoE remains under-explored. We first bridge this gap with a cutting-edge unified framework that not only seamlessly integrates mainstream compression methods but also helps systematically understand MoE compression.
This framework approaches compression from two perspectives: Expert Slimming which compresses individual experts and Expert Trimming which removes structured modules. Within this framework, we explore the optimization space unexplored by existing methods,and further introduce aggressive Expert Trimming techniques, i.e., Layer Drop and Block Drop, to eliminate redundancy at larger scales. Based on these insights,we present a comprehensive recipe to guide practitioners in compressing MoE effectively. Extensive experimental results demonstrate the effectiveness of the compression methods under our framework and the proposed recipe, achieving a 6.05x speedup and only 20.0GB memory usage while maintaining over 92% of performance on Mixtral-8x7B.

------------

`[2406.02543] To Believe or Not to Believe Your LLM <https://arxiv.org/abs/2406.02543>`__ 信不信你的LLM

::

    Tue, 4 Jun 2024 17:58:18 GMT
    Yasin Abbasi Yadkori, Ilja Kuzborskij, Andr\'as Gy\"orgy, Csaba Szepesv\'ari

We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses.
Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.

------------

`[2401.11708] Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs <https://arxiv.org/abs/2401.11708>`__ 掌握文本到图像扩散:多模态llm的再现、规划和生成

::

    Mon, 22 Jan 2024 06:16:29 GMT
    Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui

Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: https://github.com/YangLing0818/RPG-DiffusionMaster

------------

`[2406.01633] On Overcoming Miscalibrated Conversational Priors in LLM-based Chatbots <https://arxiv.org/abs/2406.01633>`__ 基于llm的聊天机器人会话先验误差克服研究

::

    Sat, 1 Jun 2024 15:54:45 GMT
    Christine Herlihy, Jennifer Neville, Tobias Schnabel, Adith Swaminathan

We explore the use of Large Language Model (LLM-based) chatbots to power recommender systems. We observe that the chatbots respond poorly when they encounter under-specified requests (e.g., they make incorrect assumptions, hedge with a long response, or refuse to answer). We conjecture that such miscalibrated response tendencies (i.e., conversational priors) can be attributed to LLM fine-tuning using annotators -- single-turn annotations may not capture multi-turn conversation utility, and the annotators' preferences may not even be representative of users interacting with a recommender system.
We first analyze public LLM chat logs to conclude that query under-specification is common. Next, we study synthetic recommendation problems with configurable latent item utilities and frame them as Partially Observed Decision Processes (PODP). We find that pre-trained LLMs can be sub-optimal for PODPs and derive better policies that clarify under-specified queries when appropriate. Then, we re-calibrate LLMs by prompting them with learned control messages to approximate the improved policy. Finally, we show empirically that our lightweight learning approach effectively uses logged conversation data to re-calibrate the response strategies of LLM-based chatbots for recommendation tasks.

------------

`[2406.01698] Demystifying Platform Requirements for Diverse LLM Inference Use Cases <https://arxiv.org/abs/2406.01698>`__ 阐明不同LLM推理用例的平台需求

::

    Mon, 3 Jun 2024 18:00:50 GMT
    Abhimanyu Bambhaniya, Ritik Raj, Geonhwa Jeong, Souvik Kundu, Sudarshan Srinivasan, Midhilesh Elavazhagan, Madhu Kumar and Tushar Krishna

Large language models (LLMs) have shown remarkable performance across a wide range of applications, often outperforming human experts. However, deploying these parameter-heavy models efficiently for diverse inference use cases requires carefully designed hardware platforms with ample computing, memory, and network resources. With LLM deployment scenarios and models evolving at breakneck speed, the hardware requirements to meet SLOs remains an open research question. In this work, we present an analytical tool, GenZ, to study the relationship between LLM inference performance and various platform design parameters. Our analysis provides insights into configuring platforms for different LLM workloads and use cases. We quantify the platform requirements to support SOTA LLMs models like LLaMA and GPT-4 under diverse serving settings.
Furthermore, we project the hardware capabilities needed to enable future LLMs potentially exceeding hundreds of trillions of parameters. The trends and insights derived from GenZ can guide AI engineers deploying LLMs as well as computer architects designing next-generation hardware accelerators and platforms. Ultimately, this work sheds light on the platform design considerations for unlocking the full potential of large language models across a spectrum of applications. The source code is available at https://github.com/abhibambhaniya/GenZ-LLM-Analyzer .

------------

`[2406.01882] HoneyGPT: Breaking the Trilemma in Terminal Honeypots with Large Language Model <https://arxiv.org/abs/2406.01882>`__ HoneyGPT:用大型语言模型打破终端蜜罐的三难困境

::

    Tue, 4 Jun 2024 01:31:20 GMT
    Ziyang Wang, Jianzhou You, Haining Wang, Tianwei Yuan, Shichao Lv, Yang Wang, Limin Sun

Honeypots, as a strategic cyber-deception mechanism designed to emulate authentic interactions and bait unauthorized entities, continue to struggle with balancing flexibility, interaction depth, and deceptive capability despite their evolution over decades. Often they also lack the capability of proactively adapting to an attacker's evolving tactics, which restricts the depth of engagement and subsequent information gathering. Under this context, the emergent capabilities of large language models, in tandem with pioneering prompt-based engineering techniques, offer a transformative shift in the design and deployment of honeypot technologies. In this paper, we introduce HoneyGPT, a pioneering honeypot architecture based on ChatGPT, heralding a new era of intelligent honeypot solutions characterized by their cost-effectiveness, high adaptability, and enhanced interactivity, coupled with a predisposition for proactive attacker engagement. Furthermore, we present a structured prompt engineering framework that augments long-term interaction memory and robust security analytics. This framework, integrating thought of chain tactics attuned to honeypot contexts, enhances interactivity and deception, deepens security analytics, and ensures sustained engagement.
The evaluation of HoneyGPT includes two parts: a baseline comparison based on a collected dataset and a field evaluation in real scenarios for four weeks.
The baseline comparison demonstrates HoneyGPT's remarkable ability to strike a balance among flexibility, interaction depth, and deceptive capability. The field evaluation further validates HoneyGPT's efficacy, showing its marked superiority in enticing attackers into more profound interactive engagements and capturing a wider array of novel attack vectors in comparison to existing honeypot technologies.

------------

`[2406.01914] HPE-CogVLM: New Head Pose Grounding Task Exploration on Vision Language Model <https://arxiv.org/abs/2406.01914>`__ HPE-CogVLM:视觉语言模型新的头部姿态接地任务探索

::

    Tue, 4 Jun 2024 02:51:26 GMT
    Yu Tian, Tianqi Shao, Tsukasa Demizu, Xuyang Wu, Hsin-Tai Wu

Head pose estimation (HPE) task requires a sophisticated understanding of 3D spatial relationships and precise numerical output of yaw, pitch, and roll Euler angles. Previous HPE studies are mainly based on Non-large language models (Non-LLMs), which rely on close-up human heads cropped from the full image as inputs and lack robustness in real-world scenario. In this paper, we present a novel framework to enhance the HPE prediction task by leveraging the visual grounding capability of CogVLM. CogVLM is a vision language model (VLM) with grounding capability of predicting object bounding boxes (BBoxes), which enables HPE training and prediction using full image information input. To integrate the HPE task into the VLM, we first cop with the catastrophic forgetting problem in large language models (LLMs) by investigating the rehearsal ratio in the data rehearsal method. Then, we propose and validate a LoRA layer-based model merging method, which keeps the integrity of parameters, to enhance the HPE performance in the framework. The results show our HPE-CogVLM achieves a 31.5\% reduction in Mean Absolute Error for HPE prediction over the current Non-LLM based state-of-the-art in cross-dataset evaluation. Furthermore, we compare our LoRA layer-based model merging method with LoRA fine-tuning only and other merging methods in CogVLM. The results demonstrate our framework outperforms them in all HPE metrics.

------------

`[2406.01967] DrEureka: Language Model Guided Sim-To-Real Transfer <https://arxiv.org/abs/2406.01967>`__ DrEureka:语言模型引导的模拟到现实迁移

::

    Tue, 4 Jun 2024 04:53:05 GMT
    Yecheng Jason Ma, William Liang, Hung-Ju Wang, Sam Wang, Yuke Zhu, Linxi Fan, Osbert Bastani, Dinesh Jayaraman

Transferring policies learned in simulation to the real world is a promising strategy for acquiring robot skills at scale. However, sim-to-real approaches typically rely on manual design and tuning of the task reward function as well as the simulation physics parameters, rendering the process slow and human-labor intensive. In this paper, we investigate using Large Language Models (LLMs) to automate and accelerate sim-to-real design. Our LLM-guided sim-to-real approach, DrEureka, requires only the physics simulation for the target task and automatically constructs suitable reward functions and domain randomization distributions to support real-world transfer. We first demonstrate that our approach can discover sim-to-real configurations that are competitive with existing human-designed ones on quadruped locomotion and dexterous manipulation tasks. Then, we showcase that our approach is capable of solving novel robot tasks, such as quadruped balancing and walking atop a yoga ball, without iterative manual design.

------------

`[2406.02377] XRec: Large Language Models for Explainable Recommendation <https://arxiv.org/abs/2406.02377>`__ 

::

    Tue, 4 Jun 2024 14:55:14 GMT
    Qiyao Ma, Xubin Ren, Chao Huang

Recommender systems help users navigate information overload by providing personalized recommendations aligned with their preferences. Collaborative Filtering (CF) is a widely adopted approach, but while advanced techniques like graph neural networks (GNNs) and self-supervised learning (SSL) have enhanced CF models for better user representations, they often lack the ability to provide explanations for the recommended items. Explainable recommendations aim to address this gap by offering transparency and insights into the recommendation decision-making process, enhancing users' understanding. This work leverages the language capabilities of Large Language Models (LLMs) to push the boundaries of explainable recommender systems. We introduce a model-agnostic framework called XRec, which enables LLMs to provide comprehensive explanations for user behaviors in recommender systems. By integrating collaborative signals and designing a lightweight collaborative adaptor, the framework empowers LLMs to understand complex patterns in user-item interactions and gain a deeper understanding of user preferences. Our extensive experiments demonstrate the effectiveness of XRec, showcasing its ability to generate comprehensive and meaningful explanations that outperform baseline approaches in explainable recommender systems. We open-source our model implementation at https://github.com/HKUDS/XRec.

------------

`[2406.02523] RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots <https://arxiv.org/abs/2406.02523>`__ RoboCasa:通才机器人日常任务的大规模模拟

::

    Tue, 4 Jun 2024 17:41:31 GMT
    Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, Yuke Zhu

Recent advancements in Artificial Intelligence (AI) have largely been propelled by scaling. In Robotics, scaling is hindered by the lack of access to massive robot datasets. We advocate using realistic physical simulation as a means to scale environments, tasks, and datasets for robot learning methods. We present RoboCasa, a large-scale simulation framework for training generalist robots in everyday environments. RoboCasa features realistic and diverse scenes focusing on kitchen environments. We provide thousands of 3D assets across over 150 object categories and dozens of interactable furniture and appliances. We enrich the realism and diversity of our simulation with generative AI tools, such as object assets from text-to-3D models and environment textures from text-to-image models. We design a set of 100 tasks for systematic evaluation, including composite tasks generated by the guidance of large language models.
To facilitate learning, we provide high-quality human demonstrations and integrate automated trajectory generation methods to substantially enlarge our datasets with minimal human burden. Our experiments show a clear scaling trend in using synthetically generated robot data for large-scale imitation learning and show great promise in harnessing simulation data in real-world tasks.
Videos and open-source code are available at https://robocasa.ai/

------------

`[2406.02539] Parrot: Multilingual Visual Instruction Tuning <https://arxiv.org/abs/2406.02539>`__ Parrot:多语言视觉教学调整

::

    Tue, 4 Jun 2024 17:56:28 GMT
    Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye

The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4V has marked a significant step towards artificial general intelligence. Existing methods mainly focus on aligning vision encoders with LLMs through supervised fine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs' inherent ability to react to multiple languages progressively deteriorate as the training process evolves. We empirically find that the imbalanced SFT datasets, primarily composed of English-centric image-text pairs, lead to significantly reduced performance in non-English languages. This is due to the failure of aligning the vision encoder and LLM with multilingual tokens during the SFT process. In this paper, we introduce Parrot, a novel method that utilizes textual guidance to drive visual token alignment at the language level. Parrot makes the visual tokens condition on diverse language inputs and uses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens.
Specifically, to enhance non-English visual tokens alignment, we compute the cross-attention using the initial visual features and textual embeddings, the result of which is then fed into the MoE router to select the most relevant experts. The selected experts subsequently convert the initial visual tokens into language-specific visual tokens. Moreover, considering the current lack of benchmarks for evaluating multilingual capabilities within the field, we collect and make available a Massive Multilingual Multimodal Benchmark which includes 6 languages, 15 categories, and 12,000 questions, named as MMMB. Our method not only demonstrates state-of-the-art performance on multilingual MMBench and MMMB, but also excels across a broad range of multimodal tasks.
Both the source code and the training dataset of Parrot will be made publicly available.

------------

`[2406.01946] Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature <https://arxiv.org/abs/2406.01946>`__ Bileve:利用双层签名保护大型语言模型中的文本出处不受欺骗

::

    Tue, 4 Jun 2024 03:58:14 GMT
    Tong Zhou, Xuandong Zhao, Xiaolin Xu, Shaolei Ren

Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy.
Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability.

------------

`[2406.01631] An LLM-based Recommender System Environment <https://arxiv.org/abs/2406.01631>`__ 一个基于llm的推荐系统环境

::

    Sat, 1 Jun 2024 11:56:08 GMT
    Nathan Corecco, Giorgio Piatti, Luca A. Lanzend\"orfer, Flint Xiaofeng Fan, Roger Wattenhofer

Reinforcement learning (RL) has gained popularity in the realm of recommender systems due to its ability to optimize long-term rewards and guide users in discovering relevant content. However, the successful implementation of RL in recommender systems is challenging because of several factors, including the limited availability of online data for training on-policy methods. This scarcity requires expensive human interaction for online model training.
Furthermore, the development of effective evaluation frameworks that accurately reflect the quality of models remains a fundamental challenge in recommender systems. To address these challenges, we propose a comprehensive framework for synthetic environments that simulate human behavior by harnessing the capabilities of large language models (LLMs). We complement our framework with in-depth ablation studies and demonstrate its effectiveness with experiments on movie and book recommendations. By utilizing LLMs as synthetic users, this work introduces a modular and novel framework for training RL-based recommender systems. The software, including the RL environment, is publicly available.

------------

`[2406.02255] MidiCaps -- A large-scale MIDI dataset with text captions <https://arxiv.org/abs/2406.02255>`__ MidiCaps—具有文本标题的大规模MIDI数据集

::

    Tue, 4 Jun 2024 12:21:55 GMT
    Jan Melechovsky, Abhinaba Roy, Dorien Herremans

Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist, mostly due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting the first large-scale MIDI dataset with text captions that is openly available: MidiCaps. MIDI (Musical Instrument Digital Interface) files are a widely used format for encoding musical information. Their structured format captures the nuances of musical composition and has practical applications by music producers, composers, musicologists, as well as performers. Inspired by recent advancements in captioning techniques applied to various domains, we present a large-scale curated dataset of over 168k MIDI files accompanied by textual descriptions.
Each MIDI caption succinctly describes the musical content, encompassing tempo, chord progression, time signature, instruments present, genre and mood; thereby facilitating multi-modal exploration and analysis. The dataset contains a mix of various genres, styles, and complexities, offering a rich source for training and evaluating models for tasks such as music information retrieval, music understanding and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research in the intersection of music and natural language processing, fostering advancements in both fields.

------------

`[2303.00438] A Framework for Neurosymbolic Robot Action Planning using Large Language Models <https://arxiv.org/abs/2303.00438>`__ 基于大型语言模型的神经符号机器人动作规划框架

::

    replaced with revised version Tue, 4 Jun 2024 12:03:28 GMT
    Submission history From: Alessio Capitanelli M. Eng. [view email]
    [v1] Wed, 1 Mar 2023 11:54:22 UTC (302 KB)
    [v2] Tue, 5 Dec 2023 12:51:15 UTC (3,029 KB)
    [v3] Tue, 4 Jun 2024 12:03:28 UTC (3,137 KB)
    Alessio Capitanelli and Fulvio Mastrogiovanni

Symbolic task planning is a widely used approach to enforce robot autonomy due to its ease of understanding and deployment in robot architectures. However, techniques for symbolic task planning are difficult to scale in real-world, human-robot collaboration scenarios because of the poor performance in complex planning domains or when frequent re-planning is needed. We present a framework, Teriyaki, specifically aimed at bridging the gap between symbolic task planning and machine learning approaches. The rationale is training Large Language Models (LLMs), namely GPT-3, into a neurosymbolic task planner compatible with the Planning Domain Definition Language (PDDL), and then leveraging its generative capabilities to overcome a number of limitations inherent to symbolic task planners. Potential benefits include (i) a better scalability in so far as the planning domain complexity increases, since LLMs' response time linearly scales with the combined length of the input and the output, and (ii) the ability to synthesize a plan action-by-action instead of end-to-end, making each action available for execution as soon as it is generated instead of waiting for the whole plan to be available, which in turn enables concurrent planning and execution. Recently, significant efforts have been devoted by the research community to evaluate the cognitive capabilities of LLMs, with alternate successes. Instead, with Teriyaki we aim to provide an overall planning performance comparable to traditional planners in specific planning domains, while leveraging LLMs capabilities to build a look-ahead predictive planning model. Preliminary results in selected domains show that our method can: (i) solve 95.5% of problems in a test data set of 1,000 samples; (ii) produce plans up to 13.5% shorter than a traditional symbolic planner; (iii) reduce average overall waiting times for a plan availability by up to 61.4%

------------

`[2402.06529] Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity <https://arxiv.org/abs/2402.06529>`__ 内省规划:将机器人的不确定性与固有的任务模糊性对齐

::

    replaced with revised version Tue, 4 Jun 2024 02:25:30 GMT
    Submission history From: Kaiqu Liang [view email]
    [v1] Fri, 9 Feb 2024 16:40:59 UTC (9,496 KB)
    [v2] Sun, 18 Feb 2024 07:01:01 UTC (9,721 KB)
    [v3] Tue, 4 Jun 2024 02:25:30 UTC (8,196 KB)
    Kaiqu Liang, Zixu Zhang, Jaime Fern\'andez Fisac

Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches. Furthermore, we assess the effectiveness of introspective planning in conjunction with conformal prediction, revealing that this combination yields tighter confidence bounds, thereby maintaining statistical success guarantees with fewer superfluous user clarification queries. Code is available at this https URL.

------------

`[2402.09147] Into the Unknown: Self-Learning Large Language Models <https://arxiv.org/abs/2402.09147>`__ 探索未知:自学习大型语言模型

::

    replaced with revised version Tue, 4 Jun 2024 12:44:46 GMT
    Submission history From: Teddy Ferdinan [view email]
    [v1] Wed, 14 Feb 2024 12:56:58 UTC (11,106 KB)
    [v2] Tue, 4 Jun 2024 12:44:46 UTC (11,114 KB)
    Teddy Ferdinan, Jan Koco\'n, Przemys{\l}aw Kazienko

We address the main problem of self-learning LLM: the question of what to learn. We propose a self-learning LLM framework that enables an LLM to independently learn previously unknown knowledge through selfassessment of their own hallucinations. Using the hallucination score, we introduce a new concept of Points in the Unknown (PiUs), along with one extrinsic and three intrinsic methods for automatic PiUs identification. It facilitates the creation of a self-learning loop that focuses exclusively on the knowledge gap in Points in the Unknown, resulting in a reduced hallucination score. We also developed evaluation metrics for gauging an LLM's self-learning capability. Our experiments revealed that 7B-Mistral models that have been finetuned or aligned and RWKV5-Eagle are capable of self-learning considerably well. Our self-learning concept allows more efficient LLM updates and opens new perspectives for knowledge exchange. It may also increase public trust in AI.

------------

`[2303.15422] KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation <https://arxiv.org/abs/2303.15422>`__ 

::

    replaced with revised version Tue, 4 Jun 2024 10:00:56 GMT
    Submission history From: Di Wu [view email]
    [v1] Mon, 27 Mar 2023 17:45:38 UTC (8,110 KB)
    [v2] Wed, 1 Nov 2023 05:00:38 UTC (8,507 KB)
    [v3] Fri, 16 Feb 2024 22:37:42 UTC (8,856 KB)
    [v4] Tue, 4 Jun 2024 10:00:56 UTC (8,858 KB)
    Di Wu, Da Yin, Kai-Wei Chang

Despite the significant advancements in keyphrase extraction and keyphrase generation methods, the predominant approach for evaluation mainly relies on exact matching with human references. This scheme fails to recognize systems that generate keyphrases semantically equivalent to the references or diverse keyphrases that carry practical utility. To better assess the capability of keyphrase systems, we propose KPEval, a comprehensive evaluation framework consisting of four critical aspects: reference agreement, faithfulness, diversity, and utility. For each aspect, we design semantic-based metrics to reflect the evaluation objectives. Meta-evaluation studies demonstrate that our evaluation strategy correlates better with human preferences compared to a range of previously proposed metrics. Using KPEval, we re-evaluate 23 keyphrase systems and discover that (1) established model comparison results have blind-spots especially when considering reference-free evaluation; (2) large language models are underestimated by prior evaluation works; and (3) there is no single best model that can excel in all the aspects.

------------

`[2308.10248] Activation Addition: Steering Language Models Without Optimization <https://arxiv.org/abs/2308.10248>`__ 激活添加:无优化的转向语言模型

::

    replaced with revised version Tue, 4 Jun 2024 10:08:39 GMT
    Submission history From: Gavin Leech [view email]
    [v1] Sun, 20 Aug 2023 12:21:05 UTC (395 KB)
    [v2] Fri, 1 Sep 2023 17:07:29 UTC (395 KB)
    [v3] Mon, 13 Nov 2023 14:05:13 UTC (646 KB)
    [v4] Tue, 4 Jun 2024 10:08:39 UTC (1,035 KB)
    Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, Monte MacDiarmid

Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering and guided decoding. We instead investigate activation engineering: modifying activations at inference-time to predictably alter model behavior. We bias the forward pass with a 'steering vector' implicitly specified through natural language. Past work learned these steering vectors; our Activation Addition (ActAdd) method instead computes them by taking activation differences resulting from pairs of prompts. We demonstrate ActAdd on a range of LLMs (LLaMA-3, OPT, GPT-2, and GPT-J), obtaining SOTA on detoxification and negative-to-positive sentiment control. Our approach yields inference-time control over high-level properties of output like topic and sentiment while preserving performance on off-target tasks. ActAdd takes far less compute and implementation effort than finetuning or RLHF, allows users control through natural language, and its computational overhead (as a fraction of inference time) appears stable or improving over increasing model size.

------------

`[2310.00901] PACIT: Unlocking the Power of Examples for Better In-Context Instruction Tuning <https://arxiv.org/abs/2310.00901>`__ PACIT:解锁示例的力量，以更好地进行上下文指令调优

::

    replaced with revised version Tue, 4 Jun 2024 14:21:11 GMT
    Submission history From: Tianci Xue [view email]
    [v1] Mon, 2 Oct 2023 04:42:53 UTC (544 KB)
    [v2] Thu, 5 Oct 2023 03:04:12 UTC (544 KB)
    [v3] Tue, 4 Jun 2024 14:21:11 UTC (289 KB)
    Tianci Xue, Ziqi Wang, Yixia Li, Yun Chen, Guanhua Chen

Instruction tuning enhances the instruction following ability of large language models by finetuning with supervised instruction data. Previous work proposes in-context instruction tuning (ICIT) where specific positive or negative examples are incorporated into the prompt for better performance. In this work, we propose PACIT, a simple and effective in-context instruction tuning method, inspired by the pedagogical concept of desirable difficulty. The PACIT method unlocks the power of examples by encouraging the model to actively learn to grasp the distinctions between the positive and negative examples instead of merely reading. The model is expected to first verify the correctness of the provided example according to the task description, which is then set as the condition for generating a better response to the task instance. Our extensive experiments prove the effectiveness of PACIT, outperforming ICIT baseline on both in-domain and out-domain tasks up to 9.16 and 3.14 average ROUGE-L scores, respectively. Moreover, PACIT can notably enhance the performance of instruction tuning even when all positive and negative examples are generated with a self-instruct method.

------------

`[2310.10570] On Context Utilization in Summarization with Large Language Models <https://arxiv.org/abs/2310.10570>`__ 大型语言模型摘要中的上下文利用研究

::

    replaced with revised version Tue, 4 Jun 2024 06:56:48 GMT
    Submission history From: Mathieu Ravaut [view email]
    [v1] Mon, 16 Oct 2023 16:45:12 UTC (5,614 KB)
    [v2] Thu, 30 Nov 2023 09:37:20 UTC (8,558 KB)
    [v3] Tue, 20 Feb 2024 05:14:44 UTC (4,543 KB)
    [v4] Tue, 4 Jun 2024 06:56:48 UTC (4,624 KB)
    Mathieu Ravaut, Aixin Sun, Nancy F. Chen, Shafiq Joty

Large language models (LLMs) excel in abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, exceeding 100k tokens. However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source document(s). Besides, in summarization, mapping facts from the source to the summary is not trivial as salient content is usually re-phrased. In this paper, we conduct the first comprehensive study on context utilization and position bias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5 evaluation metrics. We introduce a new evaluation benchmark called MiddleSum on the which we benchmark two alternative inference methods to alleviate position bias: hierarchical summarization and incremental summarization. Our code and data can be found here: this https URL.

------------

`[2311.09213] GENEVA: GENErating and Visualizing branching narratives using LLMs <https://arxiv.org/abs/2311.09213>`__ GENEVA:使用llm生成和可视化分支叙述

::

    replaced with revised version Mon, 3 Jun 2024 21:01:51 GMT
    Submission history From: Sudha Rao [view email]
    [v1] Wed, 15 Nov 2023 18:55:45 UTC (7,969 KB)
    [v2] Mon, 3 Jun 2024 21:01:51 UTC (8,842 KB)
    Jorge Leandro, Sudha Rao, Michael Xu, Weijia Xu, Nebosja Jojic, Chris Brockett, and Bill Dolan

Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The narratives of these may take years to write and typically involve a large creative team. In this work, we demonstrate the potential of large generative text models to assist this process. \textbf{GENEVA}, a prototype tool, generates a rich narrative graph with branching and reconverging storylines that match a high-level narrative description and constraints provided by the designer. A large language model (LLM), GPT-4, is used to generate the branching narrative and to render it in a graph format in a two-step process. We illustrate the use of GENEVA in generating new branching narratives for four well-known stories under different contextual constraints. This tool has the potential to assist in game development, simulations, and other applications with game-like properties.

------------

`[2312.02143] Competition-Level Problems are Effective LLM Evaluators <https://arxiv.org/abs/2312.02143>`__ 

::

    replaced with revised version Tue, 4 Jun 2024 05:49:50 GMT
    Submission history From: Yiming Huang [view email]
    [v1] Mon, 4 Dec 2023 18:58:57 UTC (2,495 KB)
    [v2] Tue, 5 Dec 2023 03:44:19 UTC (2,495 KB)
    [v3] Tue, 4 Jun 2024 05:49:50 UTC (2,530 KB)
    Yiming Huang, Zhenghao Lin, Xiao Liu, Yeyun Gong, Shuai Lu, Fangyu Lei, Yaobo Liang, Yelong Shen, Chen Lin, Nan Duan, Weizhu Chen

Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently. This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills. We first provide a comprehensive evaluation of GPT-4's peiceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered. Surprisingly, the peiceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems. We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification, unfortunately none of them is able to consistently mitigate the challenges. Through our work, we emphasis the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future.

------------

`[2312.06185] KnowGPT: Knowledge Graph based Prompting for Large Language Models <https://arxiv.org/abs/2312.06185>`__ KnowGPT:基于知识图谱的大型语言模型提示

::

    replaced with revised version Tue, 4 Jun 2024 10:10:37 GMT
    Submission history From: Qinggang Zhang [view email]
    [v1] Mon, 11 Dec 2023 07:56:25 UTC (1,229 KB)
    [v2] Mon, 19 Feb 2024 09:28:20 UTC (1,230 KB)
    [v3] Tue, 27 Feb 2024 10:05:04 UTC (1,345 KB)
    [v4] Wed, 13 Mar 2024 07:35:18 UTC (1,279 KB)
    [v5] Tue, 4 Jun 2024 10:10:37 UTC (1,527 KB)
    Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, Xiao Huang

Large Language Models (LLMs) have demonstrated remarkable capabilities in many real-world applications. Nonetheless, LLMs are often criticized for their tendency to produce hallucinations, wherein the models fabricate incorrect statements on tasks beyond their knowledge and perception. To alleviate this issue, researchers have explored leveraging the factual knowledge in knowledge graphs (KGs) to ground the LLM's responses in established facts and principles. However, most state-of-the-art LLMs are closed-source, making it challenging to develop a prompting framework that can efficiently and effectively integrate KGs into LLMs with hard prompts only. Generally, existing KG-enhanced LLMs usually suffer from three critical issues, including huge search space, high API costs, and laborious prompt engineering, that impede their widespread application in practice. To this end, we introduce a novel Knowledge Graph based PrompTing framework, namely KnowGPT, to enhance LLMs with domain knowledge. KnowGPT contains a knowledge extraction module to extract the most informative knowledge from KGs, and a context-aware prompt construction module to automatically convert extracted knowledge into effective prompts. Experiments on three benchmarks demonstrate that KnowGPT significantly outperforms all competitors. Notably, KnowGPT achieves a 92.6% accuracy on OpenbookQA leaderboard, comparable to human-level performance.

------------

`[2401.04218] Distortions in Judged Spatial Relations in Large Language Models <https://arxiv.org/abs/2401.04218>`__ 大型语言模型中判定空间关系的扭曲

::

    replaced with revised version Tue, 4 Jun 2024 12:01:56 GMT
    Submission history From: Nir Fulman [view email]
    [v1] Mon, 8 Jan 2024 20:08:04 UTC (334 KB)
    [v2] Tue, 4 Jun 2024 12:01:56 UTC (315 KB)
    Nir Fulman, Abdulkadir Memduho\u{g}lu, Alexander Zipf

We present a benchmark for assessing the capability of Large Language Models (LLMs) to discern intercardinal directions between geographic locations and apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar to humans, where judgments about individual locations' spatial relationships are influenced by the perceived relationships of the larger groups that contain them. To investigate this, we formulated 14 questions focusing on well-known American cities. Seven questions were designed to challenge the LLMs with scenarios potentially influenced by the orientation of larger geographical units, such as states or countries, while the remaining seven targeted locations were less susceptible to such hierarchical categorization. Among the tested models, GPT-4 exhibited superior performance with 55 percent accuracy, followed by GPT-3.5 at 47 percent, and Llama-2 at 45 percent. The models showed significantly reduced accuracy on tasks with suspected hierarchical bias. For example, GPT-4's accuracy dropped to 33 percent on these tasks, compared to 86 percent on others. However, the models identified the nearest cardinal direction in most cases, reflecting their associative learning mechanism, thereby embodying human-like misconceptions. We discuss avenues for improving the spatial reasoning capabilities of LLMs.

------------

`[2401.12192] Text Embedding Inversion Security for Multilingual Language Models <https://arxiv.org/abs/2401.12192>`__ 多语言语言模型的文本嵌入倒置安全性

::

    replaced with revised version Tue, 4 Jun 2024 13:28:10 GMT
    Submission history From: Yiyi Chen [view email]
    [v1] Mon, 22 Jan 2024 18:34:42 UTC (9,783 KB)
    [v2] Fri, 16 Feb 2024 11:10:57 UTC (12,248 KB)
    [v3] Tue, 4 Jun 2024 13:28:10 UTC (12,249 KB)
    Yiyi Chen and Heather Lent and Johannes Bjerva

Textual data is often represented as real-numbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be susceptible to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages potentially exposed to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and cross-lingual inversion attacks, and explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English-based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for both monolingual and multilingual models. This study is the first to investigate multilingual inversion attacks, shedding light on the differences in attacks and defenses across monolingual and multilingual settings.

------------

`[2401.15042] PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models <https://arxiv.org/abs/2401.15042>`__ PROXYQA:用大型语言模型评估长文本生成的替代框架

::

    replaced with revised version Tue, 4 Jun 2024 12:46:47 GMT
    Submission history From: Haochen Tan [view email]
    [v1] Fri, 26 Jan 2024 18:12:25 UTC (7,869 KB)
    [v2] Mon, 12 Feb 2024 03:32:43 UTC (8,091 KB)
    [v3] Tue, 13 Feb 2024 13:24:49 UTC (8,091 KB)
    [v4] Tue, 4 Jun 2024 12:46:47 UTC (8,093 KB)
    Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, Linqi Song

Large Language Models (LLMs) have succeeded remarkably in understanding long-form contents. However, exploring their capability for generating long-form contents, such as reports and articles, has been relatively unexplored and inadequately assessed by existing benchmarks. The prevalent evaluation methods, which predominantly rely on crowdsourcing, are recognized for their labor-intensive nature and lack of efficiency, whereas automated metrics, such as the ROUGE score, demonstrate discordance with human judgment criteria. In this paper, we propose ProxyQA, an innovative framework dedicated to assessing long-text generation. ProxyQA comprises in-depth human-curated meta-questions spanning various domains, each accompanied by specific proxy-questions with pre-annotated answers. LLMs are tasked to generate extensive content in response to these meta-questions, by engaging an evaluator and incorporating the generated texts as contextual background, ProxyQA assesses the generated content's quality through the evaluator's accuracy in addressing the proxy-questions. We examine multiple LLMs, emphasizing ProxyQA's demanding nature as a high-quality assessment tool. Human evaluation demonstrates that the proxy-question method is notably self-consistent and aligns closely with human evaluative standards. The dataset and leaderboard is available at \url{this https URL}.

------------

`[2401.16475] InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification <https://arxiv.org/abs/2401.16475>`__ InfoLossQA:文本简化中信息损失的表征与恢复

::

    replaced with revised version Tue, 4 Jun 2024 13:36:07 GMT
    Submission history From: Jan Trienes [view email]
    [v1] Mon, 29 Jan 2024 19:00:01 UTC (3,097 KB)
    [v2] Tue, 4 Jun 2024 13:36:07 UTC (3,528 KB)
    Jan Trienes, Sebastian Joseph, J\"org Schl\"otterer, Christin Seifert, Kyle Lo, Wei Xu, Byron C. Wallace, Junyi Jessy Li

Text simplification aims to make technical texts more accessible to laypeople but often results in deletion of information and vagueness. This work proposes InfoLossQA, a framework to characterize and recover simplification-induced information loss in form of question-and-answer (QA) pairs. Building on the theory of Question Under Discussion, the QA pairs are designed to help readers deepen their knowledge of a text. We conduct a range of experiments with this framework. First, we collect a dataset of 1,000 linguist-curated QA pairs derived from 104 LLM simplifications of scientific abstracts of medical studies. Our analyses of this data reveal that information loss occurs frequently, and that the QA pairs give a high-level overview of what information was lost. Second, we devise two methods for this task: end-to-end prompting of open-source and commercial language models, and a natural language inference pipeline. With a novel evaluation framework considering the correctness of QA pairs and their linguistic suitability, our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss.

------------

`[2402.04833] Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning <https://arxiv.org/abs/2402.04833>`__ Long更多的是用于对齐:用于指令微调的简单但难以击败的基线

::

    replaced with revised version Tue, 4 Jun 2024 17:20:01 GMT
    Submission history From: Hao Zhao [view email]
    [v1] Wed, 7 Feb 2024 13:32:11 UTC (1,015 KB)
    [v2] Tue, 4 Jun 2024 17:20:01 UTC (1,277 KB)
    Hao Zhao, Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion

There is a consensus that instruction fine-tuning of LLMs requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a quality scorer. We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses -- that intuitively contain more learnable information and are harder to overfit -- from standard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining competitive on the Open LLM benchmarks that test factual knowledge. We demonstrate this for several LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B-v0.1) and datasets (Alpaca-52k, Evol-Instruct-70k). In addition, a lightweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to obtain competitive results on MT-Bench and the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0, while training on only 1,000 examples and no extra preference data. We also conduct a thorough analysis of our models to ensure that their enhanced performance is not simply due to GPT-4's preference for longer responses. Overall, our findings suggest that fine-tuning on the longest responses should be the default baseline for any work on instruction fine-tuning. We provide our code at this https URL.

------------

`[2402.10693] Exploring Precision and Recall to assess the quality and diversity of LLMs <https://arxiv.org/abs/2402.10693>`__ 探索精度和召回率以评估llm的质量和多样性

::

    replaced with revised version Tue, 4 Jun 2024 11:33:27 GMT
    Submission history From: Alexandre Verine [view email]
    [v1] Fri, 16 Feb 2024 13:53:26 UTC (3,904 KB)
    [v2] Wed, 28 Feb 2024 10:12:34 UTC (3,904 KB)
    [v3] Tue, 4 Jun 2024 11:33:27 UTC (3,914 KB)
    Florian Le Bronnec, Alexandre Verine, Benjamin Negrevergne, Yann Chevaleyre, Alexandre Allauzen

We introduce a novel evaluation framework for Large Language Models (LLMs) such as \textsc{Llama-2} and \textsc{Mistral}, focusing on importing Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals new insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned on instruction dataset or with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges that current LLMs face in generating diverse and high-quality text. We release our code and data.

------------

`[2402.13463] RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models <https://arxiv.org/abs/2402.13463>`__ RefuteBench:大型语言模型的反驳指令遵循评估

::

    replaced with revised version Tue, 4 Jun 2024 07:48:51 GMT
    Submission history From: Jianhao Yan [view email]
    [v1] Wed, 21 Feb 2024 01:39:56 UTC (945 KB)
    [v2] Thu, 22 Feb 2024 06:17:06 UTC (945 KB)
    [v3] Tue, 4 Jun 2024 07:48:51 UTC (945 KB)
    Jianhao Yan, Yun Luo, Yue Zhang

The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses. We further propose a recall-and-repeat prompts as a simple and effective way to enhance the model's responsiveness to feedback.

------------

`[2402.13551] Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions <https://arxiv.org/abs/2402.13551>`__ 

::

    replaced with revised version Tue, 4 Jun 2024 03:26:19 GMT
    Submission history From: Liyan Xu [view email]
    [v1] Wed, 21 Feb 2024 06:14:04 UTC (209 KB)
    [v2] Tue, 4 Jun 2024 03:26:19 UTC (218 KB)
    Liyan Xu, Jiangnan Li, Mo Yu, Jie Zhou

This work introduces an original and practical paradigm for narrative comprehension, stemming from the characteristics that individual passages within narratives tend to be more cohesively related than isolated. Complementary to the common end-to-end paradigm, we propose a fine-grained modeling of narrative context, by formulating a graph dubbed NarCo, which explicitly depicts task-agnostic coherence dependencies that are ready to be consumed by various downstream tasks. In particular, edges in NarCo encompass free-form retrospective questions between context snippets, inspired by human cognitive perception that constantly reinstates relevant events from prior context. Importantly, our graph formalism is practically instantiated by LLMs without human annotations, through our designed two-stage prompting scheme. To examine the graph properties and its utility, we conduct three studies in narratives, each from a unique angle: edge relation efficacy, local context enrichment, and broader application in QA. All tasks could benefit from the explicit coherence captured by NarCo.

------------

`[2402.14007] Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models <https://arxiv.org/abs/2402.14007>`__ 水印能通过翻译吗?论大型语言模型文本水印的跨语言一致性

::

    replaced with revised version Tue, 4 Jun 2024 14:24:15 GMT
    Submission history From: Zhiwei He [view email]
    [v1] Wed, 21 Feb 2024 18:48:38 UTC (9,404 KB)
    [v2] Tue, 4 Jun 2024 14:24:15 UTC (10,749 KB)
    Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang

Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of cross-lingual consistency in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks, decreasing the AUCs to a random-guessing level without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose X-SIR as a defense method against CWRA. Code: this https URL.

------------

`[2402.14355] Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models? <https://arxiv.org/abs/2402.14355>`__ 规则还是故事，对于大型语言模型来说，哪个是更好的常识性表达?

::

    replaced with revised version Tue, 4 Jun 2024 08:05:51 GMT
    Submission history From: Ning Bian [view email]
    [v1] Thu, 22 Feb 2024 07:55:26 UTC (362 KB)
    [v2] Tue, 4 Jun 2024 08:05:51 UTC (363 KB)
    Ning Bian, Xianpei Han, Hongyu Lin, Yaojie Lu, Ben He, Le Sun

Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning. These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities.

------------

`[2402.16006] ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings <https://arxiv.org/abs/2402.16006>`__ ASETF:一种基于翻译后缀嵌入的llm越狱攻击新方法

::

    replaced with revised version Tue, 4 Jun 2024 02:59:58 GMT
    Submission history From: Hao Wang [view email]
    [v1] Sun, 25 Feb 2024 06:46:27 UTC (13,022 KB)
    [v2] Tue, 4 Jun 2024 02:59:58 UTC (8,293 KB)
    Hao Wang, Hao Li, Minlie Huang, Lei Sha

The safety defense methods of Large language models(LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. However, similar to traditional text adversarial attacks, this approach, while effective, is limited by the challenge of the discrete tokens. This gradient based discrete optimization attack requires over 100,000 LLM calls, and due to the unreadable of adversarial suffixes, it can be relatively easily penetrated by common defense methods such as perplexity filters. To cope with this challenge, in this paper, we proposes an Adversarial Suffix Embedding Translation Framework (ASETF), aimed at transforming continuous adversarial suffix embeddings into coherent and understandable text. This method greatly reduces the computational overhead during the attack process and helps to automatically generate multiple adversarial samples, which can be used as data to strengthen LLMs security defense. Experimental evaluations were conducted on Llama2, Vicuna, and other prominent LLMs, employing harmful directives sourced from the Advbench dataset. The results indicate that our method significantly reduces the computation time of adversarial suffixes and achieves a much better attack success rate to existing techniques, while significantly enhancing the textual fluency of the prompts. In addition, our approach can be generalized into a broader method for generating transferable adversarial suffixes that can successfully attack multiple LLMs, even black-box LLMs, such as ChatGPT and Gemini.

------------

`[2402.17649] Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs <https://arxiv.org/abs/2402.17649>`__ 超越脆弱:评估llm中政治世界观的可靠性和一致性

::

    replaced with revised version Mon, 3 Jun 2024 22:25:08 GMT
    Submission history From: Tanise Ceron [view email]
    [v1] Tue, 27 Feb 2024 16:19:37 UTC (9,722 KB)
    [v2] Mon, 3 Jun 2024 22:25:08 UTC (11,276 KB)
    Tanise Ceron, Neele Falk, Ana Bari\'c, Dmitry Nikolaev, Sebastian Pad\'o

Due to the widespread use of large language models (LLMs) in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings (Feng et al., 2023; Motoki et al., 2024). However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs' stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance towards environment protection, social welfare state and liberal society but also (right-wing) law and order, with no consistent preferences in foreign policy and migration.

------------

`[2402.18005] A Sentiment Consolidation Framework for Meta-Review Generation <https://arxiv.org/abs/2402.18005>`__ 面向元评论生成的情感整合框架

::

    replaced with revised version Tue, 4 Jun 2024 16:10:13 GMT
    Submission history From: Miao Li [view email]
    [v1] Wed, 28 Feb 2024 02:40:09 UTC (1,122 KB)
    [v2] Tue, 4 Jun 2024 16:10:13 UTC (1,136 KB)
    Miao Li and Jey Han Lau and Eduard Hovy

Modern natural language generation systems with Large Language Models (LLMs) exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if they truly possess the capability of information consolidation to generate summaries, especially on documents with opinionated information. We focus on meta-review generation, a form of sentiment summarisation for the scientific domain. To make scientific sentiment summarization more grounded, we hypothesize that human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews. Based on the framework, we propose novel prompting methods for LLMs to generate meta-reviews and evaluation metrics to assess the quality of generated meta-reviews. Our framework is validated empirically as we find that prompting LLMs based on the framework -- compared with prompting them with simple instructions -- generates better meta-reviews.

------------

`[2402.18099] Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models <https://arxiv.org/abs/2402.18099>`__ 医学大型语言模型的事实知识编辑和解释能力

::

    replaced with revised version Tue, 4 Jun 2024 14:38:34 GMT
    Submission history From: Derong Xu [view email]
    [v1] Wed, 28 Feb 2024 06:40:57 UTC (944 KB)
    [v2] Tue, 4 Jun 2024 14:38:34 UTC (1,990 KB)
    Derong Xu, Ziheng Zhang, Zhihong Zhu, Zhenxi Lin, Qidong Liu, Xian Wu, Tong Xu, Wanyu Wang, Yuyang Ye, Xiangyu Zhao, Yefeng Zheng, Enhong Chen

Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods face significant challenges in dealing with specialized and complex knowledge in medical domain. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. MedLaSA harnesses the strengths of both adding extra parameters and locate-then-edit methods for medical model editing. We utilize causal tracing to identify the association of knowledge in neurons across different layers, and generate a corresponding scale set from the association value for each piece of knowledge. Subsequently, we incorporate scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge, which allows for the adjustment of the adapter's weight and rank. The more similar the content, the more consistent the scale between them. This ensures precise editing of semantically identical knowledge while avoiding impact on unrelated knowledge. To evaluate the editing impact on the behaviours of LLMs, we propose two model editing studies for medical domain: (1) editing factual knowledge for medical specialization and (2) editing the explanatory ability for complex knowledge. We build two novel medical benchmarking datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting unrelated knowledge.

------------

`[2403.00862] NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism <https://arxiv.org/abs/2403.00862>`__ NewsBench:一个评估中文新闻大型语言模型编辑能力的系统评估框架

::

    replaced with revised version Tue, 4 Jun 2024 14:50:58 GMT
    Submission history From: Miao Li [view email]
    [v1] Thu, 29 Feb 2024 21:05:14 UTC (165 KB)
    [v2] Thu, 21 Mar 2024 10:14:09 UTC (165 KB)
    [v3] Sun, 2 Jun 2024 13:38:01 UTC (134 KB)
    [v4] Tue, 4 Jun 2024 14:50:58 UTC (903 KB)
    Miao Li and Ming-Bin Chen and Bo Tang and Shengbin Hou and Pengyu Wang and Haiying Deng and Zhiyu Li and Feiyu Xiong and Keming Mao and Peng Cheng and Yi Luo

We present NewsBench, a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. Our constructed benchmark dataset is focused on four facets of writing proficiency and six facets of safety adherence, and it comprises manually and carefully designed 1,267 test samples in the types of multiple choice questions and short answer questions for five editorial tasks in 24 news domains. To measure performances, we propose different GPT-4 based automatic evaluation protocols to assess LLM generations for short answer questions in terms of writing proficiency and safety adherence, and both are validated by the high correlations with human evaluations. Based on the systematic evaluation framework, we conduct a comprehensive analysis of ten popular LLMs which can handle Chinese. The experimental results highlight GPT-4 and ERNIE Bot as top performers, yet reveal a relative deficiency in journalistic safety adherence in creative writing tasks. Our findings also underscore the need for enhanced ethical guidance in machine-generated journalistic content, marking a step forward in aligning LLMs with journalistic standards and safety considerations.

------------

`[2403.01069] LLMCRIT: Teaching Large Language Models to Use Criteria <https://arxiv.org/abs/2403.01069>`__ LLMCRIT:教大型语言模型使用标准

::

    replaced with revised version Tue, 4 Jun 2024 15:30:20 GMT
    Submission history From: Weizhe Yuan [view email]
    [v1] Sat, 2 Mar 2024 02:25:55 UTC (9,801 KB)
    [v2] Tue, 4 Jun 2024 15:30:20 UTC (9,814 KB)
    Weizhe Yuan and Pengfei Liu and Matthias Gall\'e

Humans follow criteria when they execute tasks, and these criteria are directly used to assess the quality of task completion. Therefore, having models learn to use criteria to provide feedback can help humans or models to perform tasks better. However, existing research in this field tends to consider only a limited set of criteria or quality assessment aspects. To fill this gap, we propose a general framework that enables large language models (LLMs) to use comprehensive criteria for a task in delivering natural language feedback on task execution. In particular, we present a model-in-the-loop framework that semi-automatically derives criteria from collected guidelines for different writing tasks and constructs in-context demonstrations for each criterion. We choose three tasks from real-world scenarios to operationalize this idea: paper introduction writing, Python code writing, and Reddit post writing, and evaluate our feedback generation framework using different LLMs. The results reveal the fine-grained effects of incorporating criteria and demonstrations and provide valuable insights on how to teach LLMs to use criteria more effectively.

------------

`[2403.06935] Naming, Describing, and Quantifying Visual Objects in Humans and LLMs <https://arxiv.org/abs/2403.06935>`__ 人类和llm中视觉对象的命名、描述和量化

::

    replaced with revised version Tue, 4 Jun 2024 09:49:06 GMT
    Submission history From: Alberto Testoni [view email]
    [v1] Mon, 11 Mar 2024 17:20:12 UTC (9,270 KB)
    [v2] Wed, 13 Mar 2024 09:26:26 UTC (9,270 KB)
    [v3] Tue, 4 Jun 2024 09:49:06 UTC (9,535 KB)
    Alberto Testoni, Juell Sprott, Sandro Pezzelle

While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision & Language Large Language Models (VLLMs) can mimic this crucial feature of language use is an open question. This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy. Furthermore, similar patterns of variation are observed among human speakers for highly context-sensitive expressions, such as the quantifiers 'few' or 'most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work. Our results reveal mixed evidence on the ability of VLLMs to capture human naming preferences at generation time: while some models are good at mimicking human distributions for nouns and attributes, all of them fail to assign quantifiers, a task that requires more accurate, high-level reasoning.

------------

`[2403.09972] Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection <https://arxiv.org/abs/2403.09972>`__ 信任前三思:基于综合答案反射的大型语言模型自我检测

::

    replaced with revised version Tue, 4 Jun 2024 05:42:12 GMT
    Submission history From: Moxin Li [view email]
    [v1] Fri, 15 Mar 2024 02:38:26 UTC (7,374 KB)
    [v2] Tue, 4 Jun 2024 05:42:12 UTC (7,424 KB)
    Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, Tat-Seng Chua

Self-detection for Large Language Model (LLM) seeks to evaluate the LLM output trustability by leveraging LLM's own capabilities, alleviating the output hallucination issue. However, existing self-detection approaches only retrospectively evaluate answers generated by LLM, typically leading to the over-trust in incorrectly generated answers. To tackle this limitation, we propose a novel self-detection paradigm that considers the comprehensive answer space beyond LLM-generated answers. It thoroughly compares the trustability of multiple candidate answers to mitigate the over-trust in LLM-generated incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation. This framework can be seamlessly integrated with existing approaches for superior self-detection. Extensive experiments on six datasets spanning three tasks demonstrate the effectiveness of the proposed framework.

------------

`[2403.13485] An Entropy-based Text Watermarking Detection Method <https://arxiv.org/abs/2403.13485>`__ 一种基于信息熵的文本水印检测方法

::

    replaced with revised version Tue, 4 Jun 2024 10:00:18 GMT
    Submission history From: Yijian Lu [view email]
    [v1] Wed, 20 Mar 2024 10:40:01 UTC (529 KB)
    [v2] Tue, 2 Apr 2024 12:31:27 UTC (530 KB)
    [v3] Tue, 4 Jun 2024 10:00:18 UTC (538 KB)
    Yijian Lu, Aiwei Liu, Dianzhi Yu, Jingjing Li, Irwin King

Currently, text watermarking algorithms for large language models (LLMs) can embed hidden features to texts generated by LLMs to facilitate subsequent detection, thus alleviating the problem of misuse of LLMs. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we proposed that the influence of token entropy should be fully considered in the watermark detection process, that is, the weight of each token during watermark detection should be customized according to its entropy, rather than setting the weights of all tokens to the same value as in previous methods. Specifically, we proposed an Entropy-based Text Watermark Detection (EWD) that gives higher-entropy tokens higher influence weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free and fully automated. In the experiment, we found that our method can achieve better detection performance in low-entropy scenarios, and our method is also general and can be applied to texts with different entropy distributions. Our code and data is available on \url{this https URL}. Additionally, our algorithm could be accessed through MarkLLM\url{this https URL}.

------------

`[2404.03080] Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model <https://arxiv.org/abs/2404.03080>`__ 基于大语言模型的多学科材料科学材料知识图谱的构建与应用

::

    replaced with revised version Tue, 4 Jun 2024 10:04:05 GMT
    Submission history From: Yanpeng Ye [view email]
    [v1] Wed, 3 Apr 2024 21:46:14 UTC (4,005 KB)
    [v2] Tue, 4 Jun 2024 10:04:05 UTC (5,699 KB)
    Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Haofen Wang, Imran Razzak, Tong Xie, Wenjie Zhang

Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges for efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information. To tackle these issues, this article introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural language processing techniques, integrated with large language models to extract and systematically organize a decade's worth of high-quality research into structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration. By implementing network-based algorithms, MKG not only facilitates efficient link prediction but also significantly reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also lays the groundwork for more sophisticated science knowledge graphs.

------------

`[2404.15485] Large Language Models Spot Phishing Emails with Surprising Accuracy: A Comparative Analysis of Performance <https://arxiv.org/abs/2404.15485>`__ 大型语言模型以惊人的准确性发现钓鱼邮件:性能比较分析

::

    replaced with revised version Tue, 4 Jun 2024 17:37:08 GMT
    Submission history From: Het Patel Mr [view email]
    [v1] Tue, 23 Apr 2024 19:55:18 UTC (1,269 KB)
    [v2] Tue, 4 Jun 2024 17:37:08 UTC (729 KB)
    Het Patel, Umair Rehman, Farkhund Iqbal

Phishing, a prevalent cybercrime tactic for decades, remains a significant threat in today's digital world. By leveraging clever social engineering elements and modern technology, cybercrime targets many individuals, businesses, and organizations to exploit trust and security. These cyber-attackers are often disguised in many trustworthy forms to appear as legitimate sources. By cleverly using psychological elements like urgency, fear, social proof, and other manipulative strategies, phishers can lure individuals into revealing sensitive and personalized information. Building on this pervasive issue within modern technology, this paper aims to analyze the effectiveness of 15 Large Language Models (LLMs) in detecting phishing attempts, specifically focusing on a randomized set of "419 Scam" emails. The objective is to determine which LLMs can accurately detect phishing emails by analyzing a text file containing email metadata based on predefined criteria. The experiment concluded that the following models, ChatGPT 3.5, GPT-3.5-Turbo-Instruct, and ChatGPT, were the most effective in detecting phishing emails.

------------

`[2404.17027] Player-Driven Emergence in LLM-Driven Game Narrative <https://arxiv.org/abs/2404.17027>`__ llm驱动的游戏叙事中的玩家驱动涌现

::

    replaced with revised version Mon, 3 Jun 2024 21:27:14 GMT
    Submission history From: Sudha Rao [view email]
    [v1] Thu, 25 Apr 2024 20:39:44 UTC (1,287 KB)
    [v2] Thu, 16 May 2024 21:10:03 UTC (1,287 KB)
    [v3] Mon, 3 Jun 2024 21:27:14 UTC (3,294 KB)
    Xiangyu Peng, Jessica Quaye, Sudha Rao, Weijia Xu, Portia Botchway, Chris Brockett, Nebojsa Jojic, Gabriel DesGarennes, Ken Lobb, Michael Xu, Jorge Leandro, Claire Jin, Bill Dolan

We explore how interaction with large language models (LLMs) can give rise to emergent behaviors, empowering players to participate in the evolution of game narratives. Our testbed is a text-adventure game in which players attempt to solve a mystery under a fixed narrative premise, but can freely interact with non-player characters generated by GPT-4, a large language model. We recruit 28 gamers to play the game and use GPT-4 to automatically convert the game logs into a node-graph representing the narrative in the player's gameplay. We find that through their interactions with the non-deterministic behavior of the LLM, players are able to discover interesting new emergent nodes that were not a part of the original narrative but have potential for being fun and engaging. Players that created the most emergent nodes tended to be those that often enjoy games that facilitate discovery, exploration and experimentation.

------------

`[2405.11577] A Multi-Perspective Analysis of Memorization in Large Language Models <https://arxiv.org/abs/2405.11577>`__ 大型语言模型记忆的多角度分析

::

    replaced with revised version Tue, 4 Jun 2024 15:28:20 GMT
    Submission history From: Bowen Chen [view email]
    [v1] Sun, 19 May 2024 15:00:50 UTC (5,855 KB)
    [v2] Mon, 27 May 2024 04:41:02 UTC (6,220 KB)
    [v3] Thu, 30 May 2024 05:13:19 UTC (9,006 KB)
    [v4] Tue, 4 Jun 2024 15:28:20 UTC (9,933 KB)
    Bowen Chen, Namgi Han, Yusuke Miyao

Large Language Models (LLMs), trained on massive corpora with billions of parameters, show unprecedented performance in various fields. Though surprised by their excellent performances, researchers also noticed some special behaviors of those LLMs. One of those behaviors is memorization, in which LLMs can generate the same content used to train them. Though previous research has discussed memorization, the memorization of LLMs still lacks explanation, especially the cause of memorization and the dynamics of generating them. In this research, we comprehensively discussed memorization from various perspectives and extended the discussion scope to not only just the memorized content but also less and unmemorized content. Through various studies, we found that: (1) Through experiments, we revealed the relation of memorization between model size, continuation size, and context size. Further, we showed how unmemorized sentences transition to memorized sentences. (2) Through embedding analysis, we showed the distribution and decoding dynamics across model size in embedding space for sentences with different memorization scores. The n-gram statistics analysis presents d (3) An analysis over n-gram and entropy decoding dynamics discovered a boundary effect when the model starts to generate memorized sentences or unmemorized sentences. (4)We trained a Transformer model to predict the memorization of different models, showing that it is possible to predict memorizations by context.

------------

`[2405.13516] LIRE: listwise reward enhancement for preference alignment <https://arxiv.org/abs/2405.13516>`__ LIRE:偏好对齐的列表级奖励增强

::

    replaced with revised version Tue, 4 Jun 2024 08:21:05 GMT
    Submission history From: Mingye Zhu [view email]
    [v1] Wed, 22 May 2024 10:21:50 UTC (1,334 KB)
    [v2] Tue, 4 Jun 2024 08:21:05 UTC (1,332 KB)
    Mingye Zhu, Yi Liu, Lei Zhang, Junbo Guo, Zhendong Mao

Recently, tremendous strides have been made to align the generation of Large Language Models (LLMs) with human values to mitigate toxic or unhelpful content. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves effective and is widely adopted by researchers. However, implementing RLHF is complex, and its sensitivity to hyperparameters renders achieving stable performance and scalability challenging. Furthermore, prevailing approaches to preference alignment primarily concentrate on pairwise comparisons, with limited exploration into multi-response scenarios, thereby overlooking the potential richness within the candidate pool. For the above reasons, we propose a new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a gradient-based reward optimization approach that incorporates the offline rewards of multiple responses into a streamlined listwise framework, thus eliminating the need for online sampling during training. LIRE is straightforward to implement, requiring minimal parameter tuning, and seamlessly aligns with the pairwise paradigm while naturally extending to multi-response scenarios. Moreover, we introduce a self-enhancement algorithm aimed at iteratively refining the reward during training. Our experiments demonstrate that LIRE consistently outperforms existing methods across several benchmarks on dialogue and summarization tasks, with good transferability to out-of-distribution data, assessed using proxy reward models and human annotators.

------------

`[2405.16412] KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge <https://arxiv.org/abs/2405.16412>`__ KG-FIT:基于开放世界知识的知识图谱微调

::

    replaced with revised version Tue, 4 Jun 2024 07:35:32 GMT
    Submission history From: Pengcheng Jiang [view email]
    [v1] Sun, 26 May 2024 03:04:26 UTC (2,175 KB)
    [v2] Tue, 4 Jun 2024 07:35:32 UTC (2,175 KB)
    Pengcheng Jiang, Lang Cao, Cao Xiao, Parminder Bhatia, Jimeng Sun, Jiawei Han

Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of 14.4%, 13.5%, and 11.9% in the Hits@10 metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of 12.6%, 6.7%, and 17.7% compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.

------------

`[2405.16433] CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling <https://arxiv.org/abs/2405.16433>`__ CPsyCoun:基于报告的中文心理咨询多轮对话重构与评估框架

::

    replaced with revised version Tue, 4 Jun 2024 16:25:25 GMT
    Submission history From: Chenhao Zhang [view email]
    [v1] Sun, 26 May 2024 05:18:00 UTC (5,722 KB)
    [v2] Tue, 4 Jun 2024 16:25:25 UTC (5,722 KB)
    Chenhao Zhang, Renhao Li, Minghuan Tan, Min Yang, Jingwei Zhu, Di Yang, Jiahao Zhao, Guancheng Ye, Chengming Li, Xiping Hu, Derek F. Wong

Using large language models (LLMs) to assist psychological counseling is a significant but challenging task at present. Attempts have been made on improving empathetic conversations or acting as effective assistants in the treatment with LLMs. However, the existing datasets lack consulting knowledge, resulting in LLMs lacking professional consulting competence. Moreover, how to automatically evaluate multi-turn dialogues within the counseling process remains an understudied area. To bridge the gap, we propose CPsyCoun, a report-based multi-turn dialogue reconstruction and evaluation framework for Chinese psychological counseling. To fully exploit psychological counseling reports, a two-phase approach is devised to construct high-quality dialogues while a comprehensive evaluation benchmark is developed for the effective automatic evaluation of multi-turn psychological consultations. Competitive experimental results demonstrate the effectiveness of our proposed framework in psychological counseling. We open-source the datasets and model for future research at this https URL

------------

`[2405.20477] Automated Focused Feedback Generation for Scientific Writing Assistance <https://arxiv.org/abs/2405.20477>`__ 科学写作辅助的自动聚焦反馈生成

::

    replaced with revised version Tue, 4 Jun 2024 16:03:57 GMT
    Submission history From: Eric Chamoun [view email]
    [v1] Thu, 30 May 2024 20:56:41 UTC (3,153 KB)
    [v2] Tue, 4 Jun 2024 16:03:57 UTC (3,153 KB)
    Eric Chamoun, Michael Schlichktrull, Andreas Vlachos

Scientific writing is a challenging task, particularly for novice researchers who often rely on feedback from experienced peers. Recent work has primarily focused on improving surface form and style rather than manuscript content. In this paper, we propose a novel task: automated focused feedback generation for scientific writing assistance. We present SWIF$^{2}$T: a Scientific WrIting Focused Feedback Tool. It is designed to generate specific, actionable and coherent comments, which identify weaknesses in a scientific paper and/or propose revisions to it. Our approach consists of four components - planner, investigator, reviewer and controller - leveraging multiple Large Language Models (LLMs) to implement them. We compile a dataset of 300 peer reviews citing weaknesses in scientific papers and conduct human evaluation. The results demonstrate the superiority in specificity, reading comprehension, and overall helpfulness of SWIF$^{2}$T's feedback compared to other approaches. In our analysis, we also identified cases where automatically generated reviews were judged better than human ones, suggesting opportunities for integration of AI-generated feedback in scientific writing.

------------

`[2406.00050] An Empirical Analysis on Large Language Models in Debate Evaluation <https://arxiv.org/abs/2406.00050>`__ 大型语言模型在辩论评估中的实证分析

::

    replaced with revised version Tue, 4 Jun 2024 14:51:25 GMT
    Submission history From: Xinyi Liu [view email]
    [v1] Tue, 28 May 2024 18:34:53 UTC (8,693 KB)
    [v2] Tue, 4 Jun 2024 14:51:25 UTC (8,693 KB)
    Xinyi Liu, Pinxin Liu, Hangfeng He

In this study, we investigate the capabilities and inherent biases of advanced large language models (LLMs) such as GPT-3.5 and GPT-4 in the context of debate evaluation. We discover that LLM's performance exceeds humans and surpasses the performance of state-of-the-art methods fine-tuned on extensive datasets in debate evaluation. We additionally explore and analyze biases present in LLMs, including positional bias, lexical bias, order bias, which may affect their evaluative judgments. Our findings reveal a consistent bias in both GPT-3.5 and GPT-4 towards the second candidate response presented, attributed to prompt design. We also uncover lexical biases in both GPT-3.5 and GPT-4, especially when label sets carry connotations such as numerical or sequential, highlighting the critical need for careful label verbalizer selection in prompt design. Additionally, our analysis indicates a tendency of both models to favor the debate's concluding side as the winner, suggesting an end-of-discussion bias.

------------

`[2406.01428] Superhuman performance in urology board questions by an explainable large language model enabled for context integration of the European Association of Urology guidelines: the UroBot study <https://arxiv.org/abs/2406.01428>`__ 通过一个可解释的大型语言模型在泌尿外科委员会问题上的超人表现，使欧洲泌尿外科协会指南的上下文集成成为可能:UroBot研究

::

    replaced with revised version Tue, 4 Jun 2024 05:39:15 GMT
    Submission history From: Martin Hetz [view email]
    [v1] Mon, 3 Jun 2024 15:26:06 UTC (704 KB)
    [v2] Tue, 4 Jun 2024 05:39:15 UTC (1,074 KB)
    Martin J. Hetz, Nicolas Carl, Sarah Haggenm\"uller, Christoph Wies, Maurice Stephan Michel, Frederik Wessels, Titus J. Brinker

Large Language Models (LLMs) are revolutionizing medical Question-Answering (medQA) through extensive use of medical literature. However, their performance is often hampered by outdated training data and a lack of explainability, which limits clinical applicability. This study aimed to create and assess UroBot, a urology-specialized chatbot, by comparing it with state-of-the-art models and the performance of urologists on urological board questions, ensuring full clinician-verifiability. UroBot was developed using OpenAI's GPT-3.5, GPT-4, and GPT-4o models, employing retrieval-augmented generation (RAG) and the latest 2023 guidelines from the European Association of Urology (EAU). The evaluation included ten runs of 200 European Board of Urology (EBU) In-Service Assessment (ISA) questions, with performance assessed by the mean Rate of Correct Answers (RoCA). UroBot-4o achieved an average RoCA of 88.4%, surpassing GPT-4o by 10.8%, with a score of 77.6%. It was also clinician-verifiable and exhibited the highest run agreement as indicated by Fleiss' Kappa (k = 0.979). By comparison, the average performance of urologists on board questions, as reported in the literature, is 68.7%. UroBot's clinician-verifiable nature and superior accuracy compared to both existing models and urologists on board questions highlight its potential for clinical integration. The study also provides the necessary code and instructions for further development of UroBot.

------------

`[2406.01514] Decoupled Alignment for Robust Plug-and-Play Adaptation <https://arxiv.org/abs/2406.01514>`__ 面向健壮即插即用适应的解耦对齐

::

    replaced with revised version Tue, 4 Jun 2024 03:04:09 GMT
    Submission history From: Haozheng Luo [view email]
    [v1] Mon, 3 Jun 2024 16:46:18 UTC (2,711 KB)
    [v2] Tue, 4 Jun 2024 03:04:09 UTC (2,711 KB)
    Haozheng Luo, Jiahao Yu, Wenxin Zhang, Jialong Li, Jerry Yao-Chieh Hu, Xingyu Xing, Han Liu

We introduce a low-resource safety enhancement method for aligning large language models (LLMs) without the need for supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF). Our main idea is to exploit knowledge distillation to extract the alignment information from existing well-aligned LLMs and integrate it into unaligned LLMs in a plug-and-play fashion. Methodology, we employ delta debugging to identify the critical components of knowledge necessary for effective distillation. On the harmful question dataset, our method significantly enhances the average defense success rate by approximately 14.41%, reaching as high as 51.39%, in 17 unaligned pre-trained LLMs, without compromising performance.

------------

`[2310.07579] In-Context Unlearning: Language Models as Few Shot Unlearners <https://arxiv.org/abs/2310.07579>`__ 

::

    replaced with revised version Tue, 4 Jun 2024 12:35:56 GMT
    Submission history From: Martin Pawelczyk [view email]
    [v1] Wed, 11 Oct 2023 15:19:31 UTC (23,361 KB)
    [v2] Thu, 12 Oct 2023 14:15:24 UTC (23,361 KB)
    [v3] Tue, 4 Jun 2024 12:35:56 UTC (24,582 KB)
    Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju

Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.

------------

`[2312.16291] Observable Propagation: Uncovering Feature Vectors in Transformers <https://arxiv.org/abs/2312.16291>`__ 可观察传播:在transformer中发现特征向量

::

    replaced with revised version Tue, 4 Jun 2024 01:26:01 GMT
    Submission history From: Jacob Dunefsky [view email]
    [v1] Tue, 26 Dec 2023 19:00:56 UTC (240 KB)
    [v2] Tue, 4 Jun 2024 01:26:01 UTC (117 KB)
    Jacob Dunefsky and Arman Cohan

A key goal of current mechanistic interpretability research in NLP is to find linear features (also called "feature vectors") for transformers: directions in activation space corresponding to concepts that are used by a given model in its computation. Present state-of-the-art methods for finding linear features require large amounts of labelled data -- both laborious to acquire and computationally expensive to utilize. In this work, we introduce a novel method, called "observable propagation" (in short: ObProp), for finding linear features used by transformer language models in computing a given task -- using almost no data. Our paradigm centers on the concept of "observables", linear functionals corresponding to given tasks. We then introduce a mathematical theory for the analysis of feature vectors, including a similarity metric between feature vectors called the coupling coefficient which estimates the degree to which one feature's output correlates with another's. We use ObProp to perform extensive qualitative investigations into several tasks, including gendered occupational bias, political party prediction, and programming language detection. Our results suggest that ObProp surpasses traditional approaches for finding feature vectors in the low-data regime, and that ObProp can be used to better understand the mechanisms responsible for bias in large language models.

------------

`[2402.01032] Repeat After Me: Transformers are Better than State Space Models at Copying <https://arxiv.org/abs/2402.01032>`__ 跟我重复一遍:transformer在复制方面比状态空间模型更好

::

    replaced with revised version Mon, 3 Jun 2024 22:22:15 GMT
    Submission history From: Samy Jelassi [view email]
    [v1] Thu, 1 Feb 2024 21:44:11 UTC (2,337 KB)
    [v2] Mon, 3 Jun 2024 22:22:15 UTC (2,409 KB)
    Samy Jelassi, David Brandfonbrener, Sham M. Kakade, Eran Malach

Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.

------------

`[2402.02368] Timer: Generative Pre-trained Transformers Are Large Time Series Models <https://arxiv.org/abs/2402.02368>`__ 定时器:生成式预训练transformer是大型时间序列模型

::

    replaced with revised version Tue, 4 Jun 2024 08:08:59 GMT
    Submission history From: Yong Liu [view email]
    [v1] Sun, 4 Feb 2024 06:55:55 UTC (4,529 KB)
    [v2] Tue, 4 Jun 2024 08:08:59 UTC (5,111 KB)
    Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, Mingsheng Long

Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world data-scarce scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progress has been achieved with the emergence of large language models, exhibiting unprecedented abilities such as few-shot generalization, scalability, and task generality, which are however absent in small deep models. To change the status quo of training scenario-specific small models from scratch, this paper aims at the early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet diverse application needs, we convert forecasting, imputation, and anomaly detection of time series into a unified generative task. The outcome of this study is a Time Series Transformer (Timer), which is generative pre-trained by next token prediction and adapted to various downstream tasks with promising capabilities as an LTSM. Code and datasets are available at: this https URL.

------------

`[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks <https://arxiv.org/abs/2402.04396>`__ QuIP#:使用Hadamard非相干性和Lattice码本实现更好的LLM量化

::

    replaced with revised version Tue, 4 Jun 2024 04:51:52 GMT
    Submission history From: Albert Tseng [view email]
    [v1] Tue, 6 Feb 2024 20:52:12 UTC (2,058 KB)
    [v2] Tue, 4 Jun 2024 04:51:52 UTC (843 KB)
    Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, Christopher De Sa

Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves QuIP's (Chee et al., 2023) incoherence processing by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference. Our code can be found at this https URL.

------------

`[2402.06627] Feedback Loops With Language Models Drive In-Context Reward Hacking <https://arxiv.org/abs/2402.06627>`__ 语言模型的反馈循环驱动上下文奖励黑客

::

    replaced with revised version Tue, 4 Jun 2024 00:16:52 GMT
    Submission history From: Alexander Pan [view email]
    [v1] Fri, 9 Feb 2024 18:59:29 UTC (350 KB)
    [v2] Tue, 4 Jun 2024 00:16:52 UTC (302 KB)
    Alexander Pan and Erik Jones and Meena Jagadeesan and Jacob Steinhardt

Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide three recommendations for evaluation to capture more instances of ICRH. As AI development accelerates, the effects of feedback loops will proliferate, increasing the need to understand their role in shaping LLM behavior.

------------

`[2403.02545] Wukong: Towards a Scaling Law for Large-Scale Recommendation <https://arxiv.org/abs/2403.02545>`__ 悟空:面向大规模推荐的尺度法则

::

    replaced with revised version Tue, 4 Jun 2024 04:29:24 GMT
    Submission history From: Buyun Zhang [view email]
    [v1] Mon, 4 Mar 2024 23:40:20 UTC (972 KB)
    [v2] Fri, 8 Mar 2024 03:39:59 UTC (971 KB)
    [v3] Thu, 2 May 2024 21:12:57 UTC (1,201 KB)
    [v4] Tue, 4 Jun 2024 04:29:24 UTC (1,454 KB)
    Buyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Daifeng Guo, Yanli Zhao, Shen Li, Yuchen Hao, Yantao Yao, Guna Lakshminarayanan, Ellie Dingqiao Wen, Jongsoo Park, Maxim Naumov, Wenlin Chen

Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wukong's scalability on an internal, large-scale dataset. The results show that Wukong retains its superiority in quality over state-of-the-art models, while holding the scaling law across two orders of magnitude in model complexity, extending beyond 100 GFLOP/example, where prior arts fall short.

------------

`[2404.14928] Graph Machine Learning in the Era of Large Language Models (LLMs) <https://arxiv.org/abs/2404.14928>`__ 大型语言模型(LLMs)时代的图机器学习

::

    replaced with revised version Tue, 4 Jun 2024 01:31:30 GMT
    Submission history From: Wenqi Fan [view email]
    [v1] Tue, 23 Apr 2024 11:13:39 UTC (23,810 KB)
    [v2] Tue, 4 Jun 2024 01:31:30 UTC (23,848 KB)
    Wenqi Fan, Shijie Wang, Jiani Huang, Zhikai Chen, Yu Song, Wenzhuo Tang, Haitao Mao, Hui Liu, Xiaorui Liu, Dawei Yin, Qing Li

Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.

------------

`[2405.20835] Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs <https://arxiv.org/abs/2405.20835>`__ 离群点和校准集对现代llm的量化影响越来越小

::

    replaced with revised version Mon, 3 Jun 2024 19:35:36 GMT
    Submission history From: Davide Paglieri [view email]
    [v1] Fri, 31 May 2024 14:24:33 UTC (14,201 KB)
    [v2] Mon, 3 Jun 2024 19:35:36 UTC (14,206 KB)
    Davide Paglieri, Saurabh Dash, Tim Rockt\"aschel, Jack Parker-Holder

Post-Training Quantization (PTQ) enhances the efficiency of Large Language Models (LLMs) by enabling faster operation and compatibility with more accessible hardware through reduced memory usage, at the cost of small performance drops. We explore the role of calibration sets in PTQ, specifically their effect on hidden activations in various notable open-source LLMs. Calibration sets are crucial for evaluating activation magnitudes and identifying outliers, which can distort the quantization range and negatively impact performance. Our analysis reveals a marked contrast in quantization effectiveness across models. The older OPT model, upon which much of the quantization literature is based, shows significant performance deterioration and high susceptibility to outliers with varying calibration sets. In contrast, newer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and Mistral 7B demonstrate strong robustness, with Mistral 7B showing near-immunity to outliers and stable activations. These findings suggest a shift in PTQ strategies might be needed. As advancements in pre-training methods reduce the relevance of outliers, there is an emerging need to reassess the fundamentals of current quantization literature. The emphasis should pivot towards optimizing inference speed, rather than primarily focusing on outlier preservation, to align with the evolving characteristics of state-of-the-art LLMs.

------------

`[2406.01124] Latent Logic Tree Extraction for Event Sequence Explanation from LLMs <https://arxiv.org/abs/2406.01124>`__ llm中事件序列解释的潜在逻辑树抽取

::

    replaced with revised version Tue, 4 Jun 2024 06:09:03 GMT
    Submission history From: Zitao Song [view email]
    [v1] Mon, 3 Jun 2024 09:10:42 UTC (390 KB)
    [v2] Tue, 4 Jun 2024 06:09:03 UTC (390 KB)
    Zitao Song, Chao Yang, Chaojie Wang, Bo An, Shuang Li

Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences. Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence. Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees. We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables. In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences. LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution. We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables. The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters. In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations. Empirical demonstrations showcase the promising performance and adaptability of our framework.

------------

`[2309.03613] Evaluating ChatGPT as a Recommender System: A Rigorous Approach <https://arxiv.org/abs/2309.03613>`__ 评估ChatGPT推荐系统:一种严格的方法

::

    replaced with revised version Tue, 4 Jun 2024 14:25:45 GMT
    Submission history From: Dario Di Palma [view email]
    [v1] Thu, 7 Sep 2023 10:13:09 UTC (687 KB)
    [v2] Tue, 4 Jun 2024 14:25:45 UTC (596 KB)
    Dario Di Palma, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, Eugenio Di Sciascio

Large Language Models (LLMs) have recently shown impressive abilities in handling various natural language-related tasks. Among different LLMs, current studies have assessed ChatGPT's superior performance across manifold tasks, especially under the zero/few-shot prompting conditions. Given such successes, the Recommender Systems (RSs) research community have started investigating its potential applications within the recommendation scenario. However, although various methods have been proposed to integrate ChatGPT's capabilities into RSs, current research struggles to comprehensively evaluate such models while considering the peculiarities of generative models. Often, evaluations do not consider hallucinations, duplications, and out-of-the-closed domain recommendations and solely focus on accuracy metrics, neglecting the impact on beyond-accuracy facets. To bridge this gap, we propose a robust evaluation pipeline to assess ChatGPT's ability as an RS and post-process ChatGPT recommendations to account for these aspects. Through this pipeline, we investigate ChatGPT-3.5 and ChatGPT-4 performance in the recommendation task under the zero-shot condition employing the role-playing prompt. We analyze the model's functionality in three settings: the Top-N Recommendation, the cold-start recommendation, and the re-ranking of a list of recommendations, and in three domains: movies, music, and books. The experiments reveal that ChatGPT exhibits higher accuracy than the baselines on books domain. It also excels in re-ranking and cold-start scenarios while maintaining reasonable beyond-accuracy metrics. Furthermore, we measure the similarity between the ChatGPT recommendations and the other recommenders, providing insights about how ChatGPT could be categorized in the realm of recommender systems. The evaluation pipeline is publicly released for future research.

------------

`[2312.08078] Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation <https://arxiv.org/abs/2312.08078>`__ 医学影像中的细粒度图像-文本对齐实现可解释的循环图像报告生成

::

    replaced with revised version Tue, 4 Jun 2024 12:27:38 GMT
    Submission history From: Wenting Chen [view email]
    [v1] Wed, 13 Dec 2023 11:47:28 UTC (4,648 KB)
    [v2] Thu, 14 Dec 2023 02:31:44 UTC (4,648 KB)
    [v3] Fri, 15 Dec 2023 13:22:51 UTC (7,772 KB)
    [v4] Wed, 27 Dec 2023 07:21:12 UTC (7,593 KB)
    [v5] Tue, 4 Jun 2024 12:27:38 UTC (8,206 KB)
    Wenting Chen, Linlin Shen, Jingyang Lin, Jiebo Luo, Xiang Li, Yixuan Yuan

To address these issues, we propose a novel Adaptive patch-word Matching (AdaMatch) model to correlate chest X-ray (CXR) image regions with words in medical reports and apply it to CXR-report generation to provide explainability for the generation process. AdaMatch exploits the fine-grained relation between adaptive patches and words to provide explanations of specific image regions with corresponding words. To capture the abnormal regions of varying sizes and positions, we introduce the Adaptive Patch extraction (AdaPatch) module to acquire the adaptive patches for these regions adaptively. In order to provide explicit explainability for CXR-report generation task, we propose an AdaMatch-based bidirectional large language model for Cyclic CXR-report generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords for CXR images and `keypatches' for medical reports as hints to guide CXR-report generation. Extensive experiments on two publicly available CXR datasets prove the effectiveness of our method and its superior performance to existing methods.

------------

`[2312.14125] VideoPoet: A Large Language Model for Zero-Shot Video Generation <https://arxiv.org/abs/2312.14125>`__ VideoPoet:用于零样本视频生成的大型语言模型

::

    replaced with revised version Tue, 4 Jun 2024 17:25:20 GMT
    Submission history From: Jonathan Huang [view email]
    [v1] Thu, 21 Dec 2023 18:46:41 UTC (39,735 KB)
    [v2] Thu, 14 Mar 2024 18:08:11 UTC (41,255 KB)
    [v3] Fri, 22 Mar 2024 17:06:53 UTC (41,255 KB)
    [v4] Tue, 4 Jun 2024 17:25:20 UTC (41,035 KB)
    Dan Kondratyuk and Lijun Yu and Xiuye Gu and Jos\'e Lezama and Jonathan Huang and Grant Schindler and Rachel Hornung and Vighnesh Birodkar and Jimmy Yan and Ming-Chang Chiu and Krishna Somandepalli and Hassan Akbari and Yair Alon and Yong Cheng and Josh Dillon and Agrim Gupta and Meera Hahn and Anja Hauth and David Hendon and Alonso Martinez and David Minnen and Mikhail Sirotenko and Kihyuk Sohn and Xuan Yang and Hartwig Adam and Ming-Hsuan Yang and Irfan Essa and Huisheng Wang and David A. Ross and Bryan Seybold and Lu Jiang

We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/

------------

`[2402.16906] LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step <https://arxiv.org/abs/2402.16906>`__ LDB:一个逐步验证运行时执行的大型语言模型调试器

::

    replaced with revised version Tue, 4 Jun 2024 06:55:27 GMT
    Submission history From: Lily Zhong [view email]
    [v1] Sun, 25 Feb 2024 00:56:27 UTC (782 KB)
    [v2] Sat, 2 Mar 2024 00:04:53 UTC (782 KB)
    [v3] Sun, 10 Mar 2024 06:16:01 UTC (782 KB)
    [v4] Mon, 29 Apr 2024 02:23:06 UTC (783 KB)
    [v5] Tue, 4 Jun 2024 06:55:27 UTC (786 KB)
    Lily Zhong, Zilong Wang, Jingbo Shang

Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifically, LDB segments the programs into basic blocks and tracks the values of intermediate variables after each block throughout the runtime execution. This allows LLMs to concentrate on simpler code units within the overall execution flow, verify their correctness against the task description block by block, and efficiently pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks, archiving new state-of-the-art performance in code debugging for various LLM selections.

------------

`[2311.05436] Fair Wasserstein Coresets <https://arxiv.org/abs/2311.05436>`__ 公平沃瑟斯坦核心集

::

    replaced with revised version Tue, 4 Jun 2024 15:21:50 GMT
    Submission history From: Niccolò Dalmasso [view email]
    [v1] Thu, 9 Nov 2023 15:21:56 UTC (1,468 KB)
    [v2] Thu, 8 Feb 2024 21:52:44 UTC (1,515 KB)
    [v3] Tue, 4 Jun 2024 15:21:50 UTC (1,531 KB)
    Zikai Xiong, Niccol\`o Dalmasso, Shubham Sharma, Freddy Lecue, Daniele Magazzeni, Vamsi K. Potluru, Tucker Balch, Manuela Veloso

Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored. In this work, we present fair Wasserstein coresets (FWC), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of FWC is equivalent to Lloyd's algorithm for k-medians and k-means clustering. Experiments conducted on both synthetic and real datasets show that FWC: (i) achieves a competitive fairness-utility tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4).

------------

`[2401.03506] DiarizationLM: Speaker Diarization Post-Processing with Large Language Models <https://arxiv.org/abs/2401.03506>`__ DiarizationLM:基于大型语言模型的说话人DiarizationLM后处理

::

    replaced with revised version Tue, 4 Jun 2024 15:08:15 GMT
    Submission history From: Quan Wang [view email]
    [v1] Sun, 7 Jan 2024 14:54:57 UTC (174 KB)
    [v2] Tue, 16 Jan 2024 23:12:55 UTC (175 KB)
    [v3] Mon, 22 Jan 2024 18:53:36 UTC (177 KB)
    [v4] Tue, 6 Feb 2024 22:38:24 UTC (177 KB)
    [v5] Tue, 4 Jun 2024 15:08:15 UTC (176 KB)
    Quan Wang, Yiling Huang, Guanlong Zhao, Evan Clark, Wei Xia, Hank Liao

In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. Various goals can be achieved with the proposed framework, such as improving the readability of the diarized transcript, or reducing the word diarization error rate (WDER). In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset.

------------

-----------
Index (109)
-----------

`[2406.02057] Tabular and Deep Learning for the Whittle Index <https://arxiv.org/abs/2406.02057>`__ Whittle索引的表格和深度学习

`[2406.01749] Towards Harnessing Large Language Models for Comprehension of Conversational Grounding <https://arxiv.org/abs/2406.01749>`__ 利用大型语言模型来理解会话基础

`[2406.01771] LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback <https://arxiv.org/abs/2406.01771>`__ 超越英语的llm:用跨语言反馈扩展llm的多语言能力

`[2406.01775] OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models <https://arxiv.org/abs/2406.01775>`__ OLoRA:大型语言模型的标准正交低秩自适应

`[2406.01806] Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation <https://arxiv.org/abs/2406.01806>`__ 上下文序列似然:自然语言生成的增强置信度分数

`[2406.01855] TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability <https://arxiv.org/abs/2406.01855>`__ TruthEval:评估LLM真实性和可靠性的数据集

`[2406.01931] Dishonesty in Helpful and Harmless Alignment <https://arxiv.org/abs/2406.01931>`__ 有益无害的不诚实行为

`[2406.01940] Process-Driven Autoformalization in Lean 4 <https://arxiv.org/abs/2406.01940>`__ 精益4中的过程驱动的自动形式化

`[2406.01943] Enhancing Trust in LLMs: Algorithms for Comparing and Interpreting LLMs <https://arxiv.org/abs/2406.01943>`__ 增强对llm的信任:比较和解释llm的算法

`[2406.01981] Zyda: A 1.3T Dataset for Open Language Modeling <https://arxiv.org/abs/2406.01981>`__ Zyda:开放语言建模的1.3T数据集

`[2406.01983] RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models <https://arxiv.org/abs/2406.01983>`__ RKLD:基于反向kl散度的知识蒸馏用于大型语言模型中个人信息遗忘

`[2406.02002] Position Debiasing Fine-Tuning for Causal Perception in Long-Term Dialogue <https://arxiv.org/abs/2406.02002>`__ 长期对话中因果感知的位置去偏差微调

`[2406.02018] Why Would You Suggest That? Human Trust in Language Model Responses <https://arxiv.org/abs/2406.02018>`__ 你为什么这么说?人类对语言模型反应的信任

`[2406.02044] QROA: A Black-Box Query-Response Optimization Attack on LLMs <https://arxiv.org/abs/2406.02044>`__ QROA:一种针对llm的黑盒查询响应优化攻击

`[2406.02050] Analyzing Social Biases in Japanese Large Language Models <https://arxiv.org/abs/2406.02050>`__ 日语大型语言模型中的社会偏见分析

`[2406.02060] I've got the "Answer"! Interpretation of LLMs Hidden States in Question Answering <https://arxiv.org/abs/2406.02060>`__ 我找到了“答案”!答疑中的llm隐藏状态解读

`[2406.02079] Assessing the Performance of Chinese Open Source Large Language Models in Information Extraction Tasks <https://arxiv.org/abs/2406.02079>`__ 中文开源大型语言模型在信息抽取任务中的性能评估

`[2406.02100] Exploring Mathematical Extrapolation of Large Language Models with Synthetic Data <https://arxiv.org/abs/2406.02100>`__ 用合成数据探索大型语言模型的数学外推

`[2406.02110] UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models <https://arxiv.org/abs/2406.02110>`__ UniOQA:基于大型语言模型的知识图谱统一问答框架

`[2406.02134] The current status of large language models in summarizing radiology report impressions <https://arxiv.org/abs/2406.02134>`__ 大型语言模型在总结放射学报告印象中的现状

`[2406.02143] Reinforcement Tuning for Detecting Stances and Debunking Rumors Jointly with Large Language Models <https://arxiv.org/abs/2406.02143>`__ 联合大型语言模型检测立场和揭穿谣言的强化调优

`[2406.02148] Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models <https://arxiv.org/abs/2406.02148>`__ 协同事件理解:基于大型语言模型的跨文档事件共指消解协同方法

`[2406.02224] FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models <https://arxiv.org/abs/2406.02224>`__ FedMKT:大型和小型语言模型的联邦互知识迁移

`[2406.02267] Prompting Large Language Models with Human Error Markings for Self-Correcting Machine Translation <https://arxiv.org/abs/2406.02267>`__ 基于人工错误标记的大型语言模型的自纠错机器翻译

`[2406.02325] Technical Language Processing for Telecommunications Specifications <https://arxiv.org/abs/2406.02325>`__ 电信规范的技术语言处理

`[2406.02350] LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing <https://arxiv.org/abs/2406.02350>`__ LlamaCare:用于增强医疗保健知识共享的大型医疗语言模型

`[2406.02376] Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs <https://arxiv.org/abs/2406.02376>`__ 高压缩比下保留关键信息:基于查询引导的llm压缩器

`[2406.02378] On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept <https://arxiv.org/abs/2406.02378>`__ LLMs内在的自校正能力:不确定性与潜在概念

`[2406.02394] Multiple Choice Questions and Large Languages Models: A Case Study with Fictional Medical Data <https://arxiv.org/abs/2406.02394>`__ 多项选择题和大型语言模型:基于虚构医疗数据的案例研究

`[2406.02481] Hiding Text in Large Language Models: Introducing Unconditional Token Forcing Confusion <https://arxiv.org/abs/2406.02481>`__ 在大型语言模型中隐藏文本:引入无条件标记强制混淆

`[2406.02524] CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks <https://arxiv.org/abs/2406.02524>`__ CheckEmbed:对开放式任务的LLM解决方案的有效验证

`[2406.02528] Scalable MatMul-free Language Modeling <https://arxiv.org/abs/2406.02528>`__ 可扩展的无matmull语言建模

`[2406.02536] Mitigate Position Bias in Large Language Models via Scaling a Single Dimension <https://arxiv.org/abs/2406.02536>`__ 通过扩展单一维度缓解大型语言模型中的位置偏差

`[2406.01638] TimeCMA: Towards LLM-Empowered Time Series Forecasting via Cross-Modality Alignment <https://arxiv.org/abs/2406.01638>`__ TimeCMA:基于跨模态对齐的llm授权时间序列预测

`[2406.02128] Iteration Head: A Mechanistic Study of Chain-of-Thought <https://arxiv.org/abs/2406.02128>`__ 迭代头:思维链的机械性研究

`[2406.02290] A Study of Optimizations for Fine-tuning Large Language Models <https://arxiv.org/abs/2406.02290>`__ 大型语言模型微调优化研究

`[2406.02356] Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy Arithmetic Tasks <https://arxiv.org/abs/2406.02356>`__ 语言模型可以轻松地完成艰巨的算术任务，很难完成简单的算术任务

`[2406.02395] GrootVL: Tree Topology is All You Need in State Space Model <https://arxiv.org/abs/2406.02395>`__ GrootVL:树拓扑是状态空间模型中所需要的全部

`[2406.02479] Applying Fine-Tuned LLMs for Reducing Data Needs in Load Profile Analysis <https://arxiv.org/abs/2406.02479>`__ 应用微调llm减少负载剖面分析中的数据需求

`[2406.02500] Demystifying the Compression of Mixture-of-Experts Through a Unified Framework <https://arxiv.org/abs/2406.02500>`__ 通过统一的框架揭开了专家混合压缩的神秘面纱

`[2406.02543] To Believe or Not to Believe Your LLM <https://arxiv.org/abs/2406.02543>`__ 信不信你的LLM

`[2401.11708] Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs <https://arxiv.org/abs/2401.11708>`__ 掌握文本到图像扩散:多模态llm的再现、规划和生成

`[2406.01633] On Overcoming Miscalibrated Conversational Priors in LLM-based Chatbots <https://arxiv.org/abs/2406.01633>`__ 基于llm的聊天机器人会话先验误差克服研究

`[2406.01698] Demystifying Platform Requirements for Diverse LLM Inference Use Cases <https://arxiv.org/abs/2406.01698>`__ 阐明不同LLM推理用例的平台需求

`[2406.01882] HoneyGPT: Breaking the Trilemma in Terminal Honeypots with Large Language Model <https://arxiv.org/abs/2406.01882>`__ HoneyGPT:用大型语言模型打破终端蜜罐的三难困境

`[2406.01914] HPE-CogVLM: New Head Pose Grounding Task Exploration on Vision Language Model <https://arxiv.org/abs/2406.01914>`__ HPE-CogVLM:视觉语言模型新的头部姿态接地任务探索

`[2406.01967] DrEureka: Language Model Guided Sim-To-Real Transfer <https://arxiv.org/abs/2406.01967>`__ DrEureka:语言模型引导的模拟到现实迁移

`[2406.02377] XRec: Large Language Models for Explainable Recommendation <https://arxiv.org/abs/2406.02377>`__

`[2406.02523] RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots <https://arxiv.org/abs/2406.02523>`__ RoboCasa:通才机器人日常任务的大规模模拟

`[2406.02539] Parrot: Multilingual Visual Instruction Tuning <https://arxiv.org/abs/2406.02539>`__ Parrot:多语言视觉教学调整

`[2406.01946] Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature <https://arxiv.org/abs/2406.01946>`__ Bileve:利用双层签名保护大型语言模型中的文本出处不受欺骗

`[2406.01631] An LLM-based Recommender System Environment <https://arxiv.org/abs/2406.01631>`__ 一个基于llm的推荐系统环境

`[2406.02255] MidiCaps -- A large-scale MIDI dataset with text captions <https://arxiv.org/abs/2406.02255>`__ MidiCaps—具有文本标题的大规模MIDI数据集

`[2303.00438] A Framework for Neurosymbolic Robot Action Planning using Large Language Models <https://arxiv.org/abs/2303.00438>`__ 基于大型语言模型的神经符号机器人动作规划框架

`[2402.06529] Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity <https://arxiv.org/abs/2402.06529>`__ 内省规划:将机器人的不确定性与固有的任务模糊性对齐

`[2402.09147] Into the Unknown: Self-Learning Large Language Models <https://arxiv.org/abs/2402.09147>`__ 探索未知:自学习大型语言模型

`[2303.15422] KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation <https://arxiv.org/abs/2303.15422>`__

`[2308.10248] Activation Addition: Steering Language Models Without Optimization <https://arxiv.org/abs/2308.10248>`__ 激活添加:无优化的转向语言模型

`[2310.00901] PACIT: Unlocking the Power of Examples for Better In-Context Instruction Tuning <https://arxiv.org/abs/2310.00901>`__ PACIT:解锁示例的力量，以更好地进行上下文指令调优

`[2310.10570] On Context Utilization in Summarization with Large Language Models <https://arxiv.org/abs/2310.10570>`__ 大型语言模型摘要中的上下文利用研究

`[2311.09213] GENEVA: GENErating and Visualizing branching narratives using LLMs <https://arxiv.org/abs/2311.09213>`__ GENEVA:使用llm生成和可视化分支叙述

`[2312.02143] Competition-Level Problems are Effective LLM Evaluators <https://arxiv.org/abs/2312.02143>`__

`[2312.06185] KnowGPT: Knowledge Graph based Prompting for Large Language Models <https://arxiv.org/abs/2312.06185>`__ KnowGPT:基于知识图谱的大型语言模型提示

`[2401.04218] Distortions in Judged Spatial Relations in Large Language Models <https://arxiv.org/abs/2401.04218>`__ 大型语言模型中判定空间关系的扭曲

`[2401.12192] Text Embedding Inversion Security for Multilingual Language Models <https://arxiv.org/abs/2401.12192>`__ 多语言语言模型的文本嵌入倒置安全性

`[2401.15042] PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models <https://arxiv.org/abs/2401.15042>`__ PROXYQA:用大型语言模型评估长文本生成的替代框架

`[2401.16475] InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification <https://arxiv.org/abs/2401.16475>`__ InfoLossQA:文本简化中信息损失的表征与恢复

`[2402.04833] Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning <https://arxiv.org/abs/2402.04833>`__ Long更多的是用于对齐:用于指令微调的简单但难以击败的基线

`[2402.10693] Exploring Precision and Recall to assess the quality and diversity of LLMs <https://arxiv.org/abs/2402.10693>`__ 探索精度和召回率以评估llm的质量和多样性

`[2402.13463] RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models <https://arxiv.org/abs/2402.13463>`__ RefuteBench:大型语言模型的反驳指令遵循评估

`[2402.13551] Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions <https://arxiv.org/abs/2402.13551>`__

`[2402.14007] Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models <https://arxiv.org/abs/2402.14007>`__ 水印能通过翻译吗?论大型语言模型文本水印的跨语言一致性

`[2402.14355] Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models? <https://arxiv.org/abs/2402.14355>`__ 规则还是故事，对于大型语言模型来说，哪个是更好的常识性表达?

`[2402.16006] ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings <https://arxiv.org/abs/2402.16006>`__ ASETF:一种基于翻译后缀嵌入的llm越狱攻击新方法

`[2402.17649] Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs <https://arxiv.org/abs/2402.17649>`__ 超越脆弱:评估llm中政治世界观的可靠性和一致性

`[2402.18005] A Sentiment Consolidation Framework for Meta-Review Generation <https://arxiv.org/abs/2402.18005>`__ 面向元评论生成的情感整合框架

`[2402.18099] Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models <https://arxiv.org/abs/2402.18099>`__ 医学大型语言模型的事实知识编辑和解释能力

`[2403.00862] NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism <https://arxiv.org/abs/2403.00862>`__ NewsBench:一个评估中文新闻大型语言模型编辑能力的系统评估框架

`[2403.01069] LLMCRIT: Teaching Large Language Models to Use Criteria <https://arxiv.org/abs/2403.01069>`__ LLMCRIT:教大型语言模型使用标准

`[2403.06935] Naming, Describing, and Quantifying Visual Objects in Humans and LLMs <https://arxiv.org/abs/2403.06935>`__ 人类和llm中视觉对象的命名、描述和量化

`[2403.09972] Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection <https://arxiv.org/abs/2403.09972>`__ 信任前三思:基于综合答案反射的大型语言模型自我检测

`[2403.13485] An Entropy-based Text Watermarking Detection Method <https://arxiv.org/abs/2403.13485>`__ 一种基于信息熵的文本水印检测方法

`[2404.03080] Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model <https://arxiv.org/abs/2404.03080>`__ 基于大语言模型的多学科材料科学材料知识图谱的构建与应用

`[2404.15485] Large Language Models Spot Phishing Emails with Surprising Accuracy: A Comparative Analysis of Performance <https://arxiv.org/abs/2404.15485>`__ 大型语言模型以惊人的准确性发现钓鱼邮件:性能比较分析

`[2404.17027] Player-Driven Emergence in LLM-Driven Game Narrative <https://arxiv.org/abs/2404.17027>`__ llm驱动的游戏叙事中的玩家驱动涌现

`[2405.11577] A Multi-Perspective Analysis of Memorization in Large Language Models <https://arxiv.org/abs/2405.11577>`__ 大型语言模型记忆的多角度分析

`[2405.13516] LIRE: listwise reward enhancement for preference alignment <https://arxiv.org/abs/2405.13516>`__ LIRE:偏好对齐的列表级奖励增强

`[2405.16412] KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge <https://arxiv.org/abs/2405.16412>`__ KG-FIT:基于开放世界知识的知识图谱微调

`[2405.16433] CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling <https://arxiv.org/abs/2405.16433>`__ CPsyCoun:基于报告的中文心理咨询多轮对话重构与评估框架

`[2405.20477] Automated Focused Feedback Generation for Scientific Writing Assistance <https://arxiv.org/abs/2405.20477>`__ 科学写作辅助的自动聚焦反馈生成

`[2406.00050] An Empirical Analysis on Large Language Models in Debate Evaluation <https://arxiv.org/abs/2406.00050>`__ 大型语言模型在辩论评估中的实证分析

`[2406.01428] Superhuman performance in urology board questions by an explainable large language model enabled for context integration of the European Association of Urology guidelines: the UroBot study <https://arxiv.org/abs/2406.01428>`__ 通过一个可解释的大型语言模型在泌尿外科委员会问题上的超人表现，使欧洲泌尿外科协会指南的上下文集成成为可能:UroBot研究

`[2406.01514] Decoupled Alignment for Robust Plug-and-Play Adaptation <https://arxiv.org/abs/2406.01514>`__ 面向健壮即插即用适应的解耦对齐

`[2310.07579] In-Context Unlearning: Language Models as Few Shot Unlearners <https://arxiv.org/abs/2310.07579>`__

`[2312.16291] Observable Propagation: Uncovering Feature Vectors in Transformers <https://arxiv.org/abs/2312.16291>`__ 可观察传播:在transformer中发现特征向量

`[2402.01032] Repeat After Me: Transformers are Better than State Space Models at Copying <https://arxiv.org/abs/2402.01032>`__ 跟我重复一遍:transformer在复制方面比状态空间模型更好

`[2402.02368] Timer: Generative Pre-trained Transformers Are Large Time Series Models <https://arxiv.org/abs/2402.02368>`__ 定时器:生成式预训练transformer是大型时间序列模型

`[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks <https://arxiv.org/abs/2402.04396>`__ QuIP#:使用Hadamard非相干性和Lattice码本实现更好的LLM量化

`[2402.06627] Feedback Loops With Language Models Drive In-Context Reward Hacking <https://arxiv.org/abs/2402.06627>`__ 语言模型的反馈循环驱动上下文奖励黑客

`[2403.02545] Wukong: Towards a Scaling Law for Large-Scale Recommendation <https://arxiv.org/abs/2403.02545>`__ 悟空:面向大规模推荐的尺度法则

`[2404.14928] Graph Machine Learning in the Era of Large Language Models (LLMs) <https://arxiv.org/abs/2404.14928>`__ 大型语言模型(LLMs)时代的图机器学习

`[2405.20835] Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs <https://arxiv.org/abs/2405.20835>`__ 离群点和校准集对现代llm的量化影响越来越小

`[2406.01124] Latent Logic Tree Extraction for Event Sequence Explanation from LLMs <https://arxiv.org/abs/2406.01124>`__ llm中事件序列解释的潜在逻辑树抽取

`[2309.03613] Evaluating ChatGPT as a Recommender System: A Rigorous Approach <https://arxiv.org/abs/2309.03613>`__ 评估ChatGPT推荐系统:一种严格的方法

`[2312.08078] Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation <https://arxiv.org/abs/2312.08078>`__ 医学影像中的细粒度图像-文本对齐实现可解释的循环图像报告生成

`[2312.14125] VideoPoet: A Large Language Model for Zero-Shot Video Generation <https://arxiv.org/abs/2312.14125>`__ VideoPoet:用于零样本视频生成的大型语言模型

`[2402.16906] LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step <https://arxiv.org/abs/2402.16906>`__ LDB:一个逐步验证运行时执行的大型语言模型调试器

`[2311.05436] Fair Wasserstein Coresets <https://arxiv.org/abs/2311.05436>`__ 公平沃瑟斯坦核心集

`[2401.03506] DiarizationLM: Speaker Diarization Post-Processing with Large Language Models <https://arxiv.org/abs/2401.03506>`__ DiarizationLM:基于大型语言模型的说话人DiarizationLM后处理

