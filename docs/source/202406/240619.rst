240619
========

----------
Survey (9)
----------

`[2406.12844] Synergizing Foundation Models and Federated Learning: A Survey <https://arxiv.org/abs/2406.12844>`__ 协同基础模型与联邦学习研究综述

::

    Tue, 18 Jun 2024 17:58:09 GMT
    Shenghui Li, Fanghua Ye, Meng Fang, Jiaxu Zhao, Yun-Hin Chan, Edith C.-H. Ngai, Thiemo Voigt

The recent development of Foundation Models (FMs), represented by large language models, vision transformers, and multimodal models, has been making a significant impact on both academia and industry. Compared with small-scale models, FMs have a much stronger demand for high-volume data during the pre-training phase. Although general FMs can be pre-trained on data collected from open sources such as the Internet, domain-specific FMs need proprietary data, posing a practical challenge regarding the amount of data available due to privacy concerns. Federated Learning (FL) is a collaborative learning paradigm that breaks the barrier of data availability from different participants. Therefore, it provides a promising solution to customize and adapt FMs to a wide range of domain-specific tasks using distributed datasets whilst preserving privacy. This survey paper discusses the potentials and challenges of synergizing FL and FMs and summarizes core techniques, future directions, and applications. A periodically updated paper collection on FM-FL is available at https://github.com/lishenghui/awesome-fm-fl.

------------

`[2406.11903] A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges <https://arxiv.org/abs/2406.11903>`__ 金融应用大型语言模型综述:进展、前景与挑战

::

    Sat, 15 Jun 2024 16:11:35 GMT
    Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M. Mulvey, H. Vincent Poor, Qingsong Wen, Stefan Zohren

Recent advances in large language models (LLMs) have unlocked novel opportunities for machine learning applications in the financial domain. These models have demonstrated remarkable capabilities in understanding context, processing vast amounts of data, and generating human-preferred contents. In this survey, we explore the application of LLMs on various financial tasks, focusing on their potential to transform traditional practices and drive innovation. We provide a discussion of the progress and advantages of LLMs in financial contexts, analyzing their advanced technologies as well as prospective capabilities in contextual understanding, transfer learning flexibility, complex emotion detection, etc. We then highlight this survey for categorizing the existing literature into key application areas, including linguistic tasks, sentiment analysis, financial time series, financial reasoning, agent-based modeling, and other applications. For each application area, we delve into specific methodologies, such as textual analysis, knowledge-based analysis, forecasting, data augmentation, planning, decision support, and simulations. Furthermore, a comprehensive collection of datasets, model assets, and useful codes associated with mainstream applications are presented as resources for the researchers and practitioners. Finally, we outline the challenges and opportunities for future research, particularly emphasizing a number of distinctive aspects in this field. We hope our work can help facilitate the adoption and further development of LLMs in the financial sector.

------------

`[2406.00252] Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey <https://arxiv.org/abs/2406.00252>`__ 多模态和多智能体系统满足理性:综述

::

    replaced with revised version Tue, 18 Jun 2024 04:22:39 GMT
    Submission history From: Bowen Jiang [view email]
    [v1] Sat, 1 Jun 2024 01:17:25 UTC (2,332 KB)
    [v2] Wed, 5 Jun 2024 19:39:56 UTC (2,332 KB)
    [v3] Tue, 18 Jun 2024 04:22:39 UTC (4,650 KB)
    Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Weijie J. Su, Camillo J. Taylor, Tanwi Mallick

Rationality is the quality of being guided by reason, characterized by logical thinking and decision-making that align with evidence and logical rules. This quality is essential for effective problem-solving, as it ensures that solutions are well-founded and systematically derived. Despite the advancements of large language models (LLMs) in generating human-like text with remarkable accuracy, they present biases inherited from the training data, inconsistency across different contexts, and difficulty understanding complex scenarios involving multiple layers of context. Therefore, recent research attempts to leverage the strength of multiple agents working collaboratively with various types of data and tools for enhanced consistency and reliability. To that end, this paper aims to understand whether multi-modal and multi-agent systems are advancing toward rationality by surveying the state-of-the-art works, identifying advancements over single-agent and single-modal systems in terms of rationality, and discussing open problems and future directions. We maintain an open repository at this https URL.

------------

`[2301.00234] A Survey on In-context Learning <https://arxiv.org/abs/2301.00234>`__ 语境学习综述

::

    replaced with revised version Tue, 18 Jun 2024 04:19:31 GMT
    Submission history From: Qingxiu Dong [view email]
    [v1] Sat, 31 Dec 2022 15:57:09 UTC (426 KB)
    [v2] Wed, 8 Feb 2023 02:59:46 UTC (439 KB)
    [v3] Thu, 1 Jun 2023 12:23:40 UTC (6,761 KB)
    [v4] Tue, 18 Jun 2024 04:19:31 UTC (8,286 KB)
    Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li and Zhifang Sui

With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.

------------

`[2310.01424] Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey <https://arxiv.org/abs/2310.01424>`__ 

::

    replaced with revised version Tue, 18 Jun 2024 09:14:34 GMT
    Submission history From: Victoria Smith [view email]
    [v1] Wed, 27 Sep 2023 15:15:23 UTC (43 KB)
    [v2] Tue, 18 Jun 2024 09:14:34 UTC (59 KB)
    Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, Adrian Weller

Large Language Models (LLMs) have shown greatly enhanced performance in recent years, attributed to increased size and extensive training data. This advancement has led to widespread interest and adoption across industries and the public. However, training data memorization in Machine Learning models scales with model size, particularly concerning for LLMs. Memorized text sequences have the potential to be directly leaked from LLMs, posing a serious threat to data privacy. Various techniques have been developed to attack LLMs and extract their training data. As these models continue to grow, this issue becomes increasingly critical. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first SoK on data privacy for LLMs. We (i) identify a taxonomy of salient dimensions where attacks differ on LLMs, (ii) systematize existing attacks, using our taxonomy of dimensions to highlight key trends, (iii) survey existing mitigation strategies, highlighting their strengths and limitations, and (iv) identify key gaps, demonstrating open problems and areas for concern.

------------

`[2401.14043] Towards Goal-oriented Prompt Engineering for Large Language Models: A Survey <https://arxiv.org/abs/2401.14043>`__ 面向目标的大型语言模型提示工程综述

::

    replaced with revised version Tue, 18 Jun 2024 02:58:37 GMT
    Submission history From: Haochen Li [view email]
    [v1] Thu, 25 Jan 2024 09:47:55 UTC (204 KB)
    [v2] Tue, 18 Jun 2024 02:58:37 UTC (200 KB)
    Haochen Li, Jonathan Leung, Zhiqi Shen

Large Language Models (LLMs) have shown prominent performance in various downstream tasks and prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not only as an overview of current prompt engineering methods, but also aims to highlight the limitation of designing prompts based on an anthropomorphic assumption that expects LLMs to think like humans. From our review of 36 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework. With four future directions proposed, we hope to further emphasize the power and potential of goal-oriented prompt engineering in all fields.

------------

`[2406.11191] A Survey on Human Preference Learning for Large Language Models <https://arxiv.org/abs/2406.11191>`__ 人类对大型语言模型的偏好学习综述

::

    replaced with revised version Tue, 18 Jun 2024 08:18:33 GMT
    Submission history From: Ruili Jiang [view email]
    [v1] Mon, 17 Jun 2024 03:52:51 UTC (497 KB)
    [v2] Tue, 18 Jun 2024 08:18:33 UTC (497 KB)
    Ruili Jiang, Kehai Chen, Xuefeng Bai, Zhixuan He, Juntao Li, Muyun Yang, Tiejun Zhao, Liqiang Nie, Min Zhang

The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs.

------------

`[2305.19860] A Survey on Large Language Models for Recommendation <https://arxiv.org/abs/2305.19860>`__ 面向推荐的大型语言模型综述

::

    replaced with revised version Tue, 18 Jun 2024 08:07:01 GMT
    Submission history From: Likang Wu [view email]
    [v1] Wed, 31 May 2023 13:51:26 UTC (248 KB)
    [v2] Thu, 1 Jun 2023 03:22:17 UTC (249 KB)
    [v3] Fri, 4 Aug 2023 02:58:15 UTC (256 KB)
    [v4] Fri, 18 Aug 2023 05:56:05 UTC (377 KB)
    [v5] Tue, 18 Jun 2024 08:07:01 UTC (2,086 KB)
    Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, Enhong Chen

Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec), with the latter being systematically sorted out for the first time. Furthermore, we systematically review and analyze existing LLM-based recommendation systems within each paradigm, providing insights into their methodologies, techniques, and performance. Additionally, we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation, this https URL.

------------

`[2406.10252] AutoSurvey: Large Language Models Can Automatically Write Surveys <https://arxiv.org/abs/2406.10252>`__ AutoSurvey:大型语言模型可以自动编写调查问卷

::

    replaced with revised version Tue, 18 Jun 2024 02:11:31 GMT
    Submission history From: Yidong Wang [view email]
    [v1] Mon, 10 Jun 2024 12:56:06 UTC (1,286 KB)
    [v2] Tue, 18 Jun 2024 02:11:31 UTC (1,286 KB)
    Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang

This paper introduces AutoSurvey, a speedy and well-organized methodology for automating the creation of comprehensive literature surveys in rapidly evolving fields like artificial intelligence. Traditional survey paper creation faces challenges due to the vast volume and complexity of information, prompting the need for efficient survey methods. While large language models (LLMs) offer promise in automating this process, challenges such as context window limitations, parametric knowledge constraints, and the lack of evaluation benchmarks remain. AutoSurvey addresses these challenges through a systematic approach that involves initial retrieval and outline generation, subsection drafting by specialized LLMs, integration and refinement, and rigorous evaluation and iteration. Our contributions include a comprehensive solution to the survey problem, a reliable evaluation method, and experimental validation demonstrating AutoSurvey's effectiveness.We open our resources at \url{this https URL}.

------------

--------------
Benchmark (22)
--------------

`[2406.12072] DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs <https://arxiv.org/abs/2406.12072>`__ DTGB:动态文本属性图综合基准

::

    Mon, 17 Jun 2024 20:16:12 GMT
    Jiasheng Zhang, Jialin Chen, Menglin Yang, Aosong Feng, Shuang Liang, Jie Shao, and Rex Ying

Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world scenarios, where each node and edge are associated with text descriptions, and both the graph structure and text descriptions evolve over time. Despite their broad applicability, there is a notable scarcity of benchmark datasets tailored to DyTAGs, which hinders the potential advancement in many research fields. To address this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB), a collection of large-scale, time-evolving graphs from diverse domains, with nodes and edges enriched by dynamically changing text attributes and categories. To facilitate the use of DTGB, we design standardized evaluation procedures based on four real-world use cases: future link prediction, destination node retrieval, edge classification, and textual relation generation. These tasks require models to understand both dynamic graph structures and natural language, highlighting the unique challenges posed by DyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB, evaluating 7 popular dynamic graph learning algorithms and their variants of adapting to text attributes with LLM embeddings, along with 6 powerful large language models (LLMs). Our results show the limitations of existing models in handling DyTAGs. Our analysis also demonstrates the utility of DTGB in investigating the incorporation of structural and textual dynamics. The proposed DTGB fosters research on DyTAGs and their broad applications. It offers a comprehensive benchmark for evaluating and advancing models to handle the interplay between dynamic graph structures and natural language. The dataset and source code are available at https://github.com/zjs123/DTGB.

------------

`[2406.12321] Automatic benchmarking of large multimodal models via iterative experiment programming <https://arxiv.org/abs/2406.12321>`__ 基于迭代实验编程的大型多模态模型自动基准测试

::

    Tue, 18 Jun 2024 06:43:46 GMT
    Alessandro Conti, Enrico Fini, Paolo Rota, Yiming Wang, Massimiliano Mancini, Elisa Ricci

Assessing the capabilities of large multimodal models (LMMs) often requires the creation of ad-hoc evaluations. Currently, building new benchmarks requires tremendous amounts of manual work for each specific analysis. This makes the evaluation process tedious and costly. In this paper, we present APEx, Automatic Programming of Experiments, the first framework for automatic benchmarking of LMMs. Given a research question expressed in natural language, APEx leverages a large language model (LLM) and a library of pre-specified tools to generate a set of experiments for the model at hand, and progressively compile a scientific report. The report drives the testing procedure: based on the current status of the investigation, APEx chooses which experiments to perform and whether the results are sufficient to draw conclusions. Finally, the LLM refines the report, presenting the results to the user in natural language. Thanks to its modularity, our framework is flexible and extensible as new tools become available. Empirically, APEx reproduces the findings of existing studies while allowing for arbitrary analyses and hypothesis testing.

------------

`[2406.12655] Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review <https://arxiv.org/abs/2406.12655>`__ 

::

    Tue, 18 Jun 2024 14:25:34 GMT
    Debalina Ghosh Paul, Hong Zhu and Ian Bayley

With the rapid development of Large Language Models (LLMs), a large number of machine learning models have been developed to assist programming tasks including the generation of program code from natural language input. However, how to evaluate such LLMs for this task is still an open problem despite of the great amount of research efforts that have been made and reported to evaluate and compare them. This paper provides a critical review of the existing work on the testing and evaluation of these tools with a focus on two key aspects: the benchmarks and the metrics used in the evaluations. Based on the review, further research directions are discussed.

------------

`[2406.12009] FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial Information Disclosure <https://arxiv.org/abs/2406.12009>`__ FinTruthQA:评估财务信息披露质量的基准数据集

::

    Mon, 17 Jun 2024 18:25:02 GMT
    Ziyue Xu, Peilin Zhou, Xinyu Shi, Jiageng Wu, Yikang Jiang, Bin Ke, Jie Yang

Accurate and transparent financial information disclosure is crucial in the fields of accounting and finance, ensuring market efficiency and investor confidence. Among many information disclosure platforms, the Chinese stock exchanges' investor interactive platform provides a novel and interactive way for listed firms to disclose information of interest to investors through an online question-and-answer (Q&A) format. However, it is common for listed firms to respond to questions with limited or no substantive information, and automatically evaluating the quality of financial information disclosure on large amounts of Q&A pairs is challenging. This paper builds a benchmark FinTruthQA, that can evaluate advanced natural language processing (NLP) techniques for the automatic quality assessment of information disclosure in financial Q&A data. FinTruthQA comprises 6,000 real-world financial Q&A entries and each Q&A was manually annotated based on four conceptual dimensions of accounting. We benchmarked various NLP techniques on FinTruthQA, including statistical machine learning models, pre-trained language model and their fine-tuned versions, as well as the large language model GPT-4. Experiments showed that existing NLP models have strong predictive ability for real question identification and question relevance tasks, but are suboptimal for answer relevance and answer readability tasks. By establishing this benchmark, we provide a robust foundation for the automatic evaluation of information disclosure, significantly enhancing the transparency and quality of financial reporting. FinTruthQA can be used by auditors, regulators, and financial analysts for real-time monitoring and data-driven decision-making, as well as by researchers for advanced studies in accounting and finance, ultimately fostering greater trust and efficiency in the financial markets.

------------

`[2406.12066] Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks <https://arxiv.org/abs/2406.12066>`__ 在生物医学基准中，语言模型对药物名称的影响令人惊讶地脆弱

::

    Mon, 17 Jun 2024 20:09:24 GMT
    Jack Gallifant, Shan Chen, Pedro Moreira, Nikolaj Munch, Mingye Gao, Jackson Pond, Leo Anthony Celi, Hugo Aerts, Thomas Hartvigsen, Danielle Bitterman

Medical knowledge is context-dependent and requires consistent reasoning across various natural language expressions of semantically equivalent phrases.
This is particularly crucial for drug names, where patients often use brand names like Advil or Tylenol instead of their generic equivalents. To study this, we create a new robustness dataset, RABBITS, to evaluate performance differences on medical benchmarks after swapping brand and generic drug names using physician expert annotations.
We assess both open-source and API-based LLMs on MedQA and MedMCQA, revealing a consistent performance drop ranging from 1-10\%. Furthermore, we identify a potential source of this fragility as the contamination of test data in widely used pre-training datasets. All code is accessible at https://github.com/BittermanLab/RABBITS, and a HuggingFace leaderboard is available at https://huggingface.co/spaces/AIM-Harvard/rabbits-leaderboard.

------------

`[2406.12386] IPEval: A Bilingual Intellectual Property Agency Consultation Evaluation Benchmark for Large Language Models <https://arxiv.org/abs/2406.12386>`__ IPEval:面向大型语言模型的双语知识产权代理咨询评估基准

::

    Tue, 18 Jun 2024 08:18:18 GMT
    Qiyao Wang, Jianguo Huang, Shule Lu, Yuan Lin, Kan Xu, Liang Yang, Hongfei Lin

The rapid development of Large Language Models (LLMs) in vertical domains, including intellectual property (IP), lacks a specific evaluation benchmark for assessing their understanding, application, and reasoning abilities. To fill this gap, we introduce IPEval, the first evaluation benchmark tailored for IP agency and consulting tasks. IPEval comprises 2657 multiple-choice questions across four major dimensions: creation, application, protection, and management of IP. These questions span patent rights (inventions, utility models, designs), trademarks, copyrights, trade secrets, and other related laws.
Evaluation methods include zero-shot, 5-few-shot, and Chain of Thought (CoT) for seven LLM types, predominantly in English or Chinese. Results show superior English performance by models like GPT series and Qwen series, while Chinese-centric LLMs excel in Chinese tests, albeit specialized IP LLMs lag behind general-purpose ones. Regional and temporal aspects of IP underscore the need for LLMs to grasp legal nuances and evolving laws. IPEval aims to accurately gauge LLM capabilities in IP and spur development of specialized models. Website: \url{https://ipeval.github.io/}

------------

`[2406.12546] Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models <https://arxiv.org/abs/2406.12546>`__ 

::

    Tue, 18 Jun 2024 12:24:22 GMT
    Philipp Mondorf, Barbara Plank

Knights and knaves problems represent a classic genre of logical puzzles where characters either tell the truth or lie. The objective is to logically deduce each character's identity based on their statements. The challenge arises from the truth-telling or lying behavior, which influences the logical implications of each statement. Solving these puzzles requires not only direct deductions from individual statements, but the ability to assess the truthfulness of statements by reasoning through various hypothetical scenarios.
As such, knights and knaves puzzles serve as compelling examples of suppositional reasoning. In this paper, we introduce $\textit{TruthQuest}$, a benchmark for suppositional reasoning based on the principles of knights and knaves puzzles. Our benchmark presents problems of varying complexity, considering both the number of characters and the types of logical statements involved. Evaluations on $\textit{TruthQuest}$ show that large language models like Llama 3 and Mixtral-8x7B exhibit significant difficulties solving these tasks. A detailed error analysis of the models' output reveals that lower-performing models exhibit a diverse range of reasoning errors, frequently failing to grasp the concept of truth and lies. In comparison, more proficient models primarily struggle with accurately inferring the logical implications of potentially false statements.

------------

`[2406.12549] MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts <https://arxiv.org/abs/2406.12549>`__ MultiSocial:社交媒体文本机器生成文本检测的多语言基准

::

    Tue, 18 Jun 2024 12:26:09 GMT
    Dominik Macko, Jakub Kopal, Robert Moro, Ivan Srba

Recent LLMs are able to generate high-quality multilingual texts, indistinguishable for humans from authentic human-written ones. Research in machine-generated text detection is however mostly focused on the English language and longer texts, such as news articles, scientific papers or student essays. Social-media texts are usually much shorter and often feature informal language, grammatical errors, or distinct linguistic items (e.g., emoticons, hashtags). There is a gap in studying the ability of existing methods in detection of such texts, reflected also in the lack of existing multilingual benchmark datasets. To fill this gap we propose the first multilingual (22 languages) and multi-platform (5 social media platforms) dataset for benchmarking machine-generated text detection in the social-media domain, called MultiSocial. It contains 472,097 texts, of which about 58k are human-written and approximately the same amount is generated by each of 7 multilingual LLMs. We use this benchmark to compare existing detection methods in zero-shot as well as fine-tuned form. Our results indicate that the fine-tuned detectors have no problem to be trained on social-media texts and that the platform selection for training matters.

------------

`[2406.12572] Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models <https://arxiv.org/abs/2406.12572>`__ Mathador-LM:大型语言模型数学推理动态基准

::

    Tue, 18 Jun 2024 13:02:12 GMT
    Eldar Kurtic, Amir Moeini, Dan Alistarh

We introduce Mathador-LM, a new benchmark for evaluating the mathematical reasoning on large language models (LLMs), combining ruleset interpretation, planning, and problem-solving. This benchmark is inspired by the Mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances dynamically, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. Additionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary models struggle with Mathador-LM, scoring significantly lower than average 5th graders. This stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks.

------------

`[2406.12753] OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI <https://arxiv.org/abs/2406.12753>`__ OlympicArena:面向超级智能AI的多学科认知推理基准测试

::

    Tue, 18 Jun 2024 16:20:53 GMT
    Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, Pengfei Liu

The evolution of Artificial Intelligence (AI) has been significantly accelerated by advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), gradually showcasing potential cognitive reasoning abilities in problem-solving and scientific discovery (i.e., AI4Science) once exclusive to human intellect. To comprehensively evaluate current models' performance in cognitive reasoning abilities, we introduce OlympicArena, which includes 11,163 bilingual problems across both text-only and interleaved text-image modalities. These challenges encompass a wide range of disciplines spanning seven fields and 62 international Olympic competitions, rigorously examined for data leakage. We argue that the challenges in Olympic competition problems are ideal for evaluating AI's cognitive reasoning due to their complexity and interdisciplinary nature, which are essential for tackling complex scientific challenges and facilitating discoveries. Beyond evaluating performance across various disciplines using answer-only criteria, we conduct detailed experiments and analyses from multiple perspectives. We delve into the models' cognitive reasoning abilities, their performance across different modalities, and their outcomes in process-level evaluations, which are vital for tasks requiring complex reasoning with lengthy solutions. Our extensive evaluations reveal that even advanced models like GPT-4o only achieve a 39.97% overall accuracy, illustrating current AI limitations in complex reasoning and multimodal integration. Through the OlympicArena, we aim to advance AI towards superintelligence, equipping it to address more complex challenges in science and beyond. We also provide a comprehensive set of resources to support AI research, including a benchmark dataset, an open-source annotation platform, a detailed evaluation tool, and a leaderboard with automatic submission features.

------------

`[2406.12784] UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions <https://arxiv.org/abs/2406.12784>`__ UBENCH:基于多项选择题的大型语言模型不确定性基准测试

::

    Tue, 18 Jun 2024 16:50:38 GMT
    Xunzhi Wang, Zhuowei Zhang, Qiongyu Li, Gaonan Chen, Mengting Hu, Zhiyu li, Bitong Luo, Hang Gao, Zhixin Han, Haotian Wang

The rapid development of large language models (LLMs) has shown promising practical results. However, their low interpretability often leads to errors in unforeseen circumstances, limiting their utility. Many works have focused on creating comprehensive evaluation systems, but previous benchmarks have primarily assessed problem-solving abilities while neglecting the response's uncertainty, which may result in unreliability. Recent methods for measuring LLM reliability are resource-intensive and unable to test black-box models. To address this, we propose UBENCH, a comprehensive benchmark for evaluating LLM reliability. UBENCH includes 3,978 multiple-choice questions covering knowledge, language, understanding, and reasoning abilities. Experimental results show that UBENCH has achieved state-of-the-art performance, while its single-sampling method significantly saves computational resources compared to baseline methods that require multiple samplings. Additionally, based on UBENCH, we evaluate the reliability of 15 popular LLMs, finding GLM4 to be the most outstanding, closely followed by GPT-4. We also explore the impact of Chain-of-Thought prompts, role-playing prompts, option order, and temperature on LLM reliability, analyzing the varying effects on different LLMs.

------------

`[2406.11939] From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline <https://arxiv.org/abs/2406.11939>`__ 从众包数据到高质量基准:Arena-Hard和BenchBuilder管道

::

    Mon, 17 Jun 2024 17:26:10 GMT
    Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica

The rapid evolution of language models has necessitated the development of more challenging benchmarks. Current static benchmarks often struggle to consistently distinguish between the capabilities of different models and fail to align with real-world user preferences. On the other hand, live crowd-sourced platforms like the Chatbot Arena collect a wide range of natural prompts and user feedback. However, these prompts vary in sophistication and the feedback cannot be applied offline to new models. In order to ensure that benchmarks keep up with the pace of LLM development, we address how one can evaluate benchmarks on their ability to confidently separate models and their alignment with human preference. Under these principles, we developed BenchBuilder, a living benchmark that filters high-quality prompts from live data sources to enable offline evaluation on fresh, challenging prompts.
BenchBuilder identifies seven indicators of a high-quality prompt, such as the requirement for domain knowledge, and utilizes an LLM annotator to select a high-quality subset of prompts from various topic clusters. The LLM evaluation process employs an LLM judge to ensure a fully automated, high-quality, and constantly updating benchmark. We apply BenchBuilder on prompts from the Chatbot Arena to create Arena-Hard-Auto v0.1: 500 challenging user prompts from a wide range of tasks. Arena-Hard-Auto v0.1 offers 3x tighter confidence intervals than MT-Bench and achieves a state-of-the-art 89.1% agreement with human preference rankings, all at a cost of only $25 and without human labelers. The BenchBuilder pipeline enhances evaluation benchmarks and provides a valuable tool for developers, enabling them to extract high-quality benchmarks from extensive data with minimal effort.

------------

`[2406.11915] miniCodeProps: a Minimal Benchmark for Proving Code Properties <https://arxiv.org/abs/2406.11915>`__ miniCodeProps:证明代码属性的最小基准测试

::

    Sun, 16 Jun 2024 21:11:23 GMT
    Evan Lohn and Sean Welleck

Neural networks have shown initial promise in automating mathematical theorem proving in proof assistants such as Lean. The same proof assistants can be used to verify the correctness of code by pairing code with specifications and proofs that the specifications hold. Automating the writing of code, specifications, and proofs could lower the cost of verification, or, ambitiously, enable a machine learning system to output provably correct code.
However, it remains unclear whether current neural theorem provers can automatically verify even relatively simple programs. We present miniCodeProps, a benchmark of 177 program specifications in the Lean proof assistant, aimed at the subproblem of automatically generating a proof for a provided program and specification. miniCodeProps contains specifications about simple, self-contained programs (e.g., lists, natural numbers, binary trees) with varied proof difficulty. Despite its simplicity, miniCodeProps is challenging for current LLM-based provers, which succeed in proving about 25 percent of the specifications. We publicly release miniCodeProps as a benchmark for furthering automated theorem proving in the context of formally verified code.

------------

`[2406.11927] REPOEXEC: Evaluate Code Generation with a Repository-Level Executable Benchmark <https://arxiv.org/abs/2406.11927>`__ 

::

    Mon, 17 Jun 2024 10:45:22 GMT
    Nam Le Hai, Dung Manh Nguyen, Nghi D. Q. Bui

The ability of CodeLLMs to generate executable and functionally correct code at the \textit{repository-level scale }remains largely unexplored. We introduce \methodnamews, a novel benchmark for evaluating code generation at the repository-level scale, emphasizing executability and correctness.
\methodnamews provides an automated system that verifies requirements and incorporates a mechanism for dynamically generating high-coverage test cases to assess the functionality of generated code. Our work explores a controlled scenario where developers specify necessary code dependencies, challenging the model to integrate these accurately. Experiments show that while pretrained LLMs outperform instruction-tuning models in correctness, the latter excel in utilizing provided dependencies and demonstrating debugging capabilities.
\methodnamews aims to provide a comprehensive evaluation of code functionality and alignment with developer intent, paving the way for more reliable and applicable CodeLLMs in real-world scenarios.

------------

`[2406.12635] ScenEval: A Benchmark for Scenario-Based Evaluation of Code Generation <https://arxiv.org/abs/2406.12635>`__ ScenEval:基于场景的代码生成评估基准

::

    Tue, 18 Jun 2024 14:02:20 GMT
    Debalina Ghosh Paul, Hong Zhu and Ian Bayley

In the scenario-based evaluation of machine learning models, a key problem is how to construct test datasets that represent various scenarios. The methodology proposed in this paper is to construct a benchmark and attach metadata to each test case. Then a test system can be constructed with test morphisms that filter the test cases based on metadata to form a dataset.
The paper demonstrates this methodology with large language models for code generation. A benchmark called ScenEval is constructed from problems in textbooks, an online tutorial website and Stack Overflow. Filtering by scenario is demonstrated and the test sets are used to evaluate ChatGPT for Java code generation.
Our experiments found that the performance of ChatGPT decreases with the complexity of the coding task. It is weakest for advanced topics like multi-threading, data structure algorithms and recursive methods. The Java code generated by ChatGPT tends to be much shorter than reference solution in terms of number of lines, while it is more likely to be more complex in both cyclomatic and cognitive complexity metrics, if the generated code is correct.
However, the generated code is more likely to be less complex than the reference solution if the code is incorrect.

------------

`[2406.12742] Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning <https://arxiv.org/abs/2406.12742>`__ 视觉和语言模型中的多图像理解基准测试:感知、知识、推理和多跳推理

::

    Tue, 18 Jun 2024 16:02:18 GMT
    Bingchen Zhao, Yongshuo Zong, Letian Zhang, Timothy Hospedales

The advancement of large language models (LLMs) has significantly broadened the scope of applications in natural language processing, with multi-modal LLMs extending these capabilities to integrate and interpret visual data. However, existing benchmarks for visual language models (VLMs) predominantly focus on single-image inputs, neglecting the crucial aspect of multi-image understanding. In this paper, we introduce a Multi-Image Relational Benchmark MIRB, designed to evaluate VLMs' ability to compare, analyze, and reason across multiple images. Our benchmark encompasses four categories: perception, visual world knowledge, reasoning, and multi-hop reasoning. Through a comprehensive evaluation of a wide range of open-source and closed-source models, we demonstrate that while open-source VLMs were shown to approach the performance of GPT-4V in single-image tasks, a significant performance gap remains in multi-image reasoning tasks. Our findings also reveal that even the state-of-the-art GPT-4V model struggles with our benchmark, underscoring the need for further research and development in this area. We believe our contribution of MIRB could serve as a testbed for developing the next-generation multi-modal models.

------------

`[2312.11865] Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach <https://arxiv.org/abs/2312.11865>`__ 大型语言模型玩星际争霸2:基准和摘要链方法

::

    replaced with revised version Tue, 18 Jun 2024 03:07:37 GMT
    Submission history From: Weiyu Ma [view email]
    [v1] Tue, 19 Dec 2023 05:27:16 UTC (38,368 KB)
    [v2] Mon, 17 Jun 2024 09:04:43 UTC (12,766 KB)
    [v3] Tue, 18 Jun 2024 03:07:37 UTC (12,766 KB)
    Weiyu Ma, Qirui Mi, Yongcheng Zeng, Xue Yan, Yuqiao Wu, Runji Lin, Haifeng Zhang, Jun Wang

StarCraft II is a challenging benchmark for AI agents due to the necessity of both precise micro level operations and strategic macro awareness. Previous works, such as Alphastar and SCC, achieve impressive performance on tackling StarCraft II , however, still exhibit deficiencies in long term strategic planning and strategy interpretability. Emerging large language model (LLM) agents, such as Voyage and MetaGPT, presents the immense potential in solving intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs on StarCraft II, a highly complex RTS this http URL conveniently take full advantage of LLMs` reasoning abilities, we first develop textual StratCraft II environment, called TextStarCraft II, which LLM agent can interact. Secondly, we propose a Chain of Summarization method, including single frame summarization for processing raw observations and multi frame summarization for analyzing game information, providing command recommendations, and generating strategic decisions. Our experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs`s mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in game performance of LLM agents, encompassing aspects like win rate and the impact of Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the relevant knowledge and complex planning abilities needed to address StarCraft II scenarios; 2. Human experts consider the performance of LLM agents to be close to that of an average player who has played StarCraft II for eight years; 3. LLM agents are capable of defeating the built in AI at the Harder(Lv5) difficulty level. We have open sourced the code and released demo videos of LLM agent playing StarCraft II.

------------

`[2310.00746] RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models <https://arxiv.org/abs/2310.00746>`__ RoleLLM:大型语言模型的基准测试、诱导和角色扮演能力增强

::

    replaced with revised version Tue, 18 Jun 2024 13:08:24 GMT
    Submission history From: Zekun Wang [view email]
    [v1] Sun, 1 Oct 2023 17:52:59 UTC (5,895 KB)
    [v2] Wed, 24 Apr 2024 07:56:00 UTC (5,212 KB)
    [v3] Tue, 18 Jun 2024 13:08:24 UTC (6,344 KB)
    Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Stephen W. Huang, Jie Fu, Junran Peng

The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4).

------------

`[2402.15862] SportQA: A Benchmark for Sports Understanding in Large Language Models <https://arxiv.org/abs/2402.15862>`__ SportQA:大型语言模型中体育理解的基准

::

    replaced with revised version Tue, 18 Jun 2024 03:29:51 GMT
    Submission history From: Haotian Xia [view email]
    [v1] Sat, 24 Feb 2024 17:12:10 UTC (9,069 KB)
    [v2] Tue, 18 Jun 2024 03:29:51 UTC (9,656 KB)
    Haotian Xia, Zhengbang Yang, Yuqing Wang, Rhys Tracy, Yun Zhao, Dongdong Huang, Zezhi Chen, Yan Zhu, Yuan-fang Wang, Weining Shen

A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise. The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and enhancing sports understanding in LLMs.

------------

`[2403.01976] SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis <https://arxiv.org/abs/2403.01976>`__ sciassessment: LLM科学文献分析能力的基准测试

::

    replaced with revised version Tue, 18 Jun 2024 05:45:33 GMT
    Submission history From: Hengxing Cai [view email]
    [v1] Mon, 4 Mar 2024 12:19:28 UTC (4,202 KB)
    [v2] Fri, 15 Mar 2024 13:27:31 UTC (8,174 KB)
    [v3] Sat, 15 Jun 2024 15:45:47 UTC (7,855 KB)
    [v4] Tue, 18 Jun 2024 05:45:33 UTC (7,855 KB)
    Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Mingjun Xu, Jin Huang, Fang Xi, Jiaxi Zhuang, Yuqi Yin, Yaqi Li, Changhong Chen, Zheng Cheng, Zifeng Zhao, Linfeng Zhang, Guolin Ke

Recent breakthroughs in Large Language Models (LLMs) have revolutionized natural language understanding and generation, sparking significant interest in applying them to scientific literature analysis. However, existing benchmarks fail to adequately evaluate the proficiency of LLMs in this domain, particularly in scenarios requiring higher-level abilities beyond mere memorization and the handling of multimodal data. In response to this gap, we introduce SciAssess, a benchmark specifically designed for the comprehensive evaluation of LLMs in scientific literature analysis. SciAssess aims to thoroughly assess the efficacy of LLMs by focusing on their capabilities in Memorization (L1), Comprehension (L2), and Analysis \& Reasoning (L3). It encompasses a variety of tasks drawn from diverse scientific fields, including fundamental science, alloy materials, biomedicine, drug discovery, and organic materials. To ensure the reliability of SciAssess, rigorous quality control measures have been implemented, ensuring accuracy, anonymization, and compliance with copyright standards. SciAssess evaluates 11 LLMs, including GPT, Claude, and Gemini, highlighting their strengths and areas for improvement. This evaluation supports the ongoing development of LLM applications in the analysis of scientific literature. SciAssess and its resources are available at \url{this https URL}.

------------

`[2403.18697] The Invalsi Benchmarks: measuring Linguistic and Mathematical understanding of Large Language Models in Italian <https://arxiv.org/abs/2403.18697>`__ Invalsi基准:测量意大利语大型语言模型的语言和数学理解

::

    replaced with revised version Mon, 17 Jun 2024 21:55:24 GMT
    Submission history From: Giovanni Puccetti [view email]
    [v1] Wed, 27 Mar 2024 15:46:25 UTC (103 KB)
    [v2] Mon, 17 Jun 2024 21:55:24 UTC (229 KB)
    Andrea Esuli and Giovanni Puccetti

While Italian is a high resource language, there are few Italian-native benchmarks to evaluate Large Language Models (LLMs) generative abilities in this language. This work presents two new benchmarks: Invalsi MATE to evaluate models performance on mathematical understanding in Italian and Invalsi ITA to evaluate language understanding in Italian. These benchmarks are based on the Invalsi tests, which are administered to students of age between 6 and 18 within the Italian school system and have been validated by several experts in teaching and pedagogy.
We use these benchmarks to evaluate 9 powerful language models showing that current language models are bound by 70% accuracy in mathematical understanding, achieved by Llama 3 70b and by 85% in language understanding. We also compare LLMs with the average performance of Italian students to show that Llama 3 is the only one to perform better than students on Invalsi MATE while most models outperform students on Invalsi ITA.
We will make data and evaluation code openly available to pave the way for the future development of larger and harder benchmarks to evaluate LLMs' mathematical and linguistic understanding in Italian.

------------

`[2404.10508] White Men Lead, Black Women Help? Benchmarking Language Agency Social Biases in LLMs <https://arxiv.org/abs/2404.10508>`__ 白人男性领导，黑人女性帮忙?llm中的语言代理社会偏见基准测试

::

    replaced with revised version Mon, 17 Jun 2024 21:36:46 GMT
    Submission history From: Yixin Wan [view email]
    [v1] Tue, 16 Apr 2024 12:27:54 UTC (1,485 KB)
    [v2] Mon, 17 Jun 2024 21:36:46 UTC (5,118 KB)
    Yixin Wan, Kai-Wei Chang

Language agency is an important aspect of evaluating social biases in texts. While several studies approached agency-related bias in human-written language, very limited research has investigated such biases in Large Language Model (LLM)-generated content. In addition, previous research often relies on string-matching techniques to identify agentic and communal words within texts, which fall short of accurately classifying language agency. We introduce the novel Language Agency Bias Evaluation (LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing agency levels attributed to different demographic groups in model generations. LABE leverages 5,400 template-based prompts, an accurate agency classifier, and corresponding bias metrics to test for gender, racial, and intersectional language agency biases in LLMs on 3 text generation tasks: biographies, professor reviews, and reference letters. To build better and more accurate automated agency classifiers, we also contribute and release the Language Agency Classification (LAC) dataset, consisting of 3,724 agentic and communal sentences. Using LABE, we unveil previously under-explored language agency social biases in 3 recent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) For the same text category, LLM generations demonstrate higher levels of gender bias than human-written texts; (2) On most generation tasks, models show remarkably higher levels of intersectional bias than the other bias aspects. Those who are at the intersection of gender and racial minority groups -- such as Black females -- are consistently described by texts with lower levels of agency; (3) Among the 3 LLMs investigated, Llama3 demonstrates greatest overall bias in language agency; (4) Not only does prompt-based mitigation fail to resolve language agency bias in LLMs, but it frequently leads to the exacerbation of biases in generated texts.

------------

---------------
Accelerate (11)
---------------

`[2406.12274] SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models <https://arxiv.org/abs/2406.12274>`__ SafeInfer:大型语言模型的上下文自适应解码时间安全对齐

::

    Tue, 18 Jun 2024 05:03:23 GMT
    Somnath Banerjee, Soham Tripathy, Sayan Layek, Shanu Kumar, Animesh Mukherjee, Rima Hazra

Safety-aligned language models often exhibit fragile and imbalanced safety mechanisms, increasing the likelihood of generating unsafe content. In addition, incorporating new knowledge through editing techniques to language models can further compromise safety. To address these issues, we propose SafeInfer, a context-adaptive, decoding-time safety alignment strategy for generating safe responses to user queries. SafeInfer comprises two phases: the safety amplification phase, which employs safe demonstration examples to adjust the model's hidden states and increase the likelihood of safer outputs, and the safety-guided decoding phase, which influences token selection based on safety-optimized distributions, ensuring the generated content complies with ethical guidelines. Further, we present HarmEval, a novel benchmark for extensive safety evaluations, designed to address potential misuse scenarios in accordance with the policies of leading AI tech giants.

------------

`[2406.12295] Fast and Slow Generating: An Empirical Study on Large and Small Language Models Collaborative Decoding <https://arxiv.org/abs/2406.12295>`__ 快速和缓慢的生成:大小语言模型协同解码的实证研究

::

    Tue, 18 Jun 2024 05:59:28 GMT
    Kaiyan Zhang, Jianyu Wang, Ning Ding, Biqing Qi, Ermo Hua, Xingtai Lv, Bowen Zhou

Large Language Models (LLMs) demonstrate impressive performance in diverse applications, yet they face significant drawbacks, including high inference latency, expensive training cost, and generation of hallucination.
Collaborative decoding between large and small language models (SLMs) offers a novel approach to address these challenges. Inspired by dual-process cognitive theory, we integrate these methods into a unified framework termed Fast and Slow Generating (FS-GEN). This paper explores several techniques within the FS-GEN framework, including speculative decoding, contrastive decoding, and emulator or proxy fine-tuning. We provide a comprehensive analysis of these methodologies, offering insights into their similarities and differences under this framework. Our study delves into the differential knowledge capabilities of LLMs versus SLMs through the FS-GEN lens, revealing that fewer than 20% of collaborative interactions are required across various methods. These interactions adhere to a scaling law relative to the parameter ratios, thereby facilitating predictable collaboration. Furthermore, we investigate the specific positions where collaboration is most effective from an uncertainty perspective, yielding novel insights that could refine FS-GEN methods. Our findings reveal that the essential difference between models of different sizes lies in the uncertainty of the next token prediction, where interventions by larger models are most needed to assist the smaller ones. Code for Reproduction: https://github.com/TsinghuaC3I/FS-GEN

------------

`[2406.12335] Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters <https://arxiv.org/abs/2406.12335>`__ 注意力分数并不是KV缓存缩减中Token重要性指标所需要的全部:值也很重要

::

    Tue, 18 Jun 2024 07:01:11 GMT
    Zhiyu Guo, Hidetaka Kamigaito, Taro Watanabe

Scaling the context size of large language models (LLMs) enables them to perform various new tasks, e.g., book summarization. However, the memory cost of the Key and Value (KV) cache in attention significantly limits the practical applications of LLMs. Recent works have explored token pruning for KV cache reduction in LLMs, relying solely on attention scores as a token importance indicator. However, our investigation into value vector norms revealed a notably non-uniform pattern questioning their reliance only on attention scores. Inspired by this, we propose a new method: Value-Aware Token Pruning (VATP) which uses both attention scores and the $ \ell_{1} $ norm of value vectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat and Vicuna-v1.5-7B across 16 LongBench tasks demonstrate VATP's superior performance.

------------

`[2406.12125] Efficient Sequential Decision Making with Large Language Models <https://arxiv.org/abs/2406.12125>`__ 

::

    Mon, 17 Jun 2024 22:13:22 GMT
    Dingyang Chen, Qi Zhang, Yinglun Zhu

This paper focuses on extending the success of large language models (LLMs) to sequential decision making. Existing efforts either (i) re-train or finetune LLMs for decision making, or (ii) design prompts for pretrained LLMs. The former approach suffers from the computational burden of gradient updates, and the latter approach does not show promising results. In this paper, we propose a new approach that leverages online model selection algorithms to efficiently incorporate LLMs agents into sequential decision making. Statistically, our approach significantly outperforms both traditional decision making algorithms and vanilla LLM agents. Computationally, our approach avoids the need for expensive gradient updates of LLMs, and throughout the decision making process, it requires only a small number of LLM calls. We conduct extensive experiments to verify the effectiveness of our proposed approach. As an example, on a large-scale Amazon dataset, our approach achieves more than a $6$x performance gain over baselines while calling LLMs in only $1.5$\% of the time steps.

------------

`[2406.12311] Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models <https://arxiv.org/abs/2406.12311>`__ 规模混合:大型语言模型的内存高效token自适应二值化

::

    Tue, 18 Jun 2024 06:32:23 GMT
    Dongwon Jo, Taesu Kim, Yulhwa Kim, Jae-Joon Kim

Binarization, which converts weight parameters to binary values, has emerged as an effective strategy to reduce the size of large language models (LLMs).
However, typical binarization techniques significantly diminish linguistic effectiveness of LLMs. To address this issue, we introduce a novel binarization technique called Mixture of Scales (BinaryMoS). Unlike conventional methods, BinaryMoS employs multiple scaling experts for binary weights, dynamically merging these experts for each token to adaptively generate scaling factors.
This token-adaptive approach boosts the representational power of binarized LLMs by enabling contextual adjustments to the values of binary weights.
Moreover, because this adaptive process only involves the scaling factors rather than the entire weight matrix, BinaryMoS maintains compression efficiency similar to traditional static binarization methods. Our experimental results reveal that BinaryMoS surpasses conventional binarization techniques in various natural language processing tasks and even outperforms 2-bit quantization methods, all while maintaining similar model size to static binarization techniques.

------------

`[2406.10382] Efficient Prompting for LLM-based Generative Internet of Things <https://arxiv.org/abs/2406.10382>`__ 基于llm的生成式物联网的有效提示

::

    replaced with revised version Tue, 18 Jun 2024 01:26:33 GMT
    Submission history From: Burak Kantarci [view email]
    [v1] Fri, 14 Jun 2024 19:24:00 UTC (11,014 KB)
    [v2] Tue, 18 Jun 2024 01:26:33 UTC (11,014 KB)
    Bin Xiao, Burak Kantarci, Jiawen Kang, Dusit Niyato, Mohsen Guizani

Large language models (LLMs) have demonstrated remarkable capacities on various tasks, and integrating the capacities of LLMs into the Internet of Things (IoT) applications has drawn much research attention recently. Due to security concerns, many institutions avoid accessing state-of-the-art commercial LLM services, requiring the deployment and utilization of open-source LLMs in a local network setting. However, open-source LLMs usually have more limitations regarding their performance, such as their arithmetic calculation and reasoning capacities, and practical systems of applying LLMs to IoT have yet to be well-explored. Therefore, we propose a text-based generative IoT (GIoT) system deployed in the local network setting in this study. To alleviate the limitations of LLMs and provide service with competitive performance, we apply prompt engineering methods to enhance the capacities of the open-source LLMs, design a Prompt Management Module and a Post-processing Module to manage the tailored prompts for different tasks and process the results generated by the LLMs. To demonstrate the effectiveness of the proposed system, we discuss a challenging Table Question Answering (Table-QA) task as a case study of the proposed system, as tabular data is usually more challenging than plain text because of their complex structures, heterogeneous data types and sometimes huge sizes. We conduct comprehensive experiments on two popular Table-QA datasets, and the results show that our proposal can achieve competitive performance compared with state-of-the-art LLMs, demonstrating that the proposed LLM-based GIoT system can provide competitive performance with tailored prompting methods and is easily extensible to new tasks without training.

------------

`[2405.04304] Accelerating Speculative Decoding using Dynamic Speculation Length <https://arxiv.org/abs/2405.04304>`__ 利用动态推测长度加速推测解码

::

    replaced with revised version Tue, 18 Jun 2024 12:34:35 GMT
    Submission history From: Jonathan Mamou [view email]
    [v1] Tue, 7 May 2024 13:27:52 UTC (674 KB)
    [v2] Tue, 18 Jun 2024 12:34:35 UTC (677 KB)
    Jonathan Mamou and Oren Pereg and Daniel Korat and Moshe Berchansky and Nadav Timor and Moshe Wasserblat and Roy Schwartz

Speculative decoding is commonly used for reducing the inference latency of large language models. Its effectiveness depends highly on the speculation lookahead (SL)-the number of tokens generated by the draft model at each iteration. In this work we show that the common practice of using the same SL for all iterations static SL is suboptimal. We introduce DISCO (DynamIc SpeCulation lookahead Optimization), a novel method for dynamically selecting the SL. Our experiments with four datasets show that DISCO reaches an average speedup of 10% compared to the best static SL baseline, while generating the exact same text.

------------

`[2405.12604] Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model Against LLM Red-Teaming <https://arxiv.org/abs/2405.12604>`__ 

::

    replaced with revised version Mon, 17 Jun 2024 18:52:45 GMT
    Submission history From: Jiaxu Liu [view email]
    [v1] Tue, 21 May 2024 08:57:44 UTC (4,310 KB)
    [v2] Mon, 17 Jun 2024 18:52:45 UTC (4,310 KB)
    Jiaxu Liu, Xiangyu Yin, Sihao Wu, Jianhong Wang, Meng Fang, Xinping Yi, Xiaowei Huang

With the proliferation of red-teaming strategies for Large Language Models (LLMs), the deficiency in the literature about improving the safety and robustness of LLM defense strategies is becoming increasingly pronounced. This paper introduces the LLM-based \textbf{sentinel} model as a plug-and-play prefix module designed to reconstruct the input prompt with just a few ($<30$) additional tokens, effectively reducing toxicity in responses from target LLMs. The sentinel model naturally overcomes the \textit{parameter inefficiency} and \textit{limited model accessibility} for fine-tuning large target models. We employ an interleaved training regimen using Proximal Policy Optimization (PPO) to optimize both red team and sentinel models dynamically, incorporating a value head-sharing mechanism inspired by the multi-agent centralized critic to manage the complex interplay between agents. Our extensive experiments across text-to-text and text-to-image demonstrate the effectiveness of our approach in mitigating toxic outputs, even when dealing with larger models like \texttt{Llama-2}, \texttt{GPT-3.5} and \texttt{Stable-Diffusion}, highlighting the potential of our framework in enhancing safety and robustness in various applications.

------------

`[2406.11357] Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities <https://arxiv.org/abs/2406.11357>`__ Refiner:有效重构检索内容以提高问答能力

::

    replaced with revised version Tue, 18 Jun 2024 02:44:27 GMT
    Submission history From: Zhonghao Li [view email]
    [v1] Mon, 17 Jun 2024 09:25:10 UTC (975 KB)
    [v2] Tue, 18 Jun 2024 02:44:27 UTC (975 KB)
    Zhonghao Li, Xuming Hu, Aiwei Liu, Kening Zheng, Sirui Huang, Hui Xiong

Large Language Models (LLMs) are limited by their parametric knowledge, leading to hallucinations in knowledge-extensive tasks. To address this, Retrieval-Augmented Generation (RAG) incorporates external document chunks to expand LLM knowledge. Furthermore, compressing information from document chunks through extraction or summarization can improve LLM performance. Nonetheless, LLMs still struggle to notice and utilize scattered key information, a problem known as the "lost-in-the-middle" syndrome. Therefore, we typically need to restructure the content for LLM to recognize the key information. We propose $\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that operates in the post-retrieval process of RAG. $\textit{Refiner}$ leverages a single decoder-only LLM to adaptively extract query-relevant contents verbatim along with the necessary context, and section them based on their interconnectedness, thereby highlights information distinction, and aligns downstream LLMs with the original context effectively. Experiments show that a trained $\textit{Refiner}$ (with 7B parameters) exhibits significant gain to downstream LLM in improving answer accuracy, and outperforms other state-of-the-art advanced RAG and concurrent compressing approaches in various single-hop and multi-hop QA tasks. Notably, $\textit{Refiner}$ achieves a 80.5% tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared to the next best solution. $\textit{Refiner}$ is a plug-and-play solution that can be seamlessly integrated with RAG systems, facilitating its application across diverse open-source frameworks.

------------

`[2402.04513] Online Cascade Learning for Efficient Inference over Streams <https://arxiv.org/abs/2402.04513>`__ 基于在线级联学习的流高效推理

::

    replaced with revised version Mon, 17 Jun 2024 18:54:36 GMT
    Submission history From: Lunyiu Nie [view email]
    [v1] Wed, 7 Feb 2024 01:46:50 UTC (1,975 KB)
    [v2] Fri, 31 May 2024 15:59:34 UTC (2,078 KB)
    [v3] Mon, 17 Jun 2024 18:54:36 UTC (2,047 KB)
    Lunyiu Nie, Zhimin Ding, Erdong Hu, Christopher Jermaine, Swarat Chaudhuri

Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to address this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regression) and ending with a powerful LLM, along with a deferral policy that determines the model to be used on a given input. We formulate the task of learning cascades online as an imitation-learning problem, where smaller models are updated over time imitating the collected LLM demonstrations, and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90% with strong robustness against input distribution shifts, underscoring its efficacy and adaptability in stream processing.

------------

`[2406.11640] Linear Bellman Completeness Suffices for Efficient Online Reinforcement Learning with Few Actions <https://arxiv.org/abs/2406.11640>`__ 线性Bellman完备性足以以较少的动作实现高效的在线强化学习

::

    replaced with revised version Tue, 18 Jun 2024 04:27:49 GMT
    Submission history From: Noah Golowich [view email]
    [v1] Mon, 17 Jun 2024 15:24:49 UTC (81 KB)
    [v2] Tue, 18 Jun 2024 04:27:49 UTC (81 KB)
    Noah Golowich and Ankur Moitra

One of the most natural approaches to reinforcement learning (RL) with function approximation is value iteration, which inductively generates approximations to the optimal value function by solving a sequence of regression problems. To ensure the success of value iteration, it is typically assumed that Bellman completeness holds, which ensures that these regression problems are well-specified. We study the problem of learning an optimal policy under Bellman completeness in the online model of RL with linear function approximation. In the linear setting, while statistically efficient algorithms are known under Bellman completeness (e.g., Jiang et al. (2017); Zanette et al. (2020)), these algorithms all rely on the principle of global optimism which requires solving a nonconvex optimization problem. In particular, it has remained open as to whether computationally efficient algorithms exist. In this paper we give the first polynomial-time algorithm for RL under linear Bellman completeness when the number of actions is any constant.

------------

-----------------------
In-Context Learning (5)
-----------------------

`[2406.11890] Unraveling the Mechanics of Learning-Based Demonstration Selection for In-Context Learning <https://arxiv.org/abs/2406.11890>`__ 

::

    Fri, 14 Jun 2024 03:34:02 GMT
    Hui Liu, Wenya Wang, Hao Sun, Chris Xing Tian, Chenqi Kong, Xin Dong, Haoliang Li

Large Language Models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities from few-shot demonstration exemplars. While recent learning-based demonstration selection methods have proven beneficial to ICL by choosing more useful exemplars, their underlying mechanisms are opaque, hindering efforts to address limitations such as high training costs and poor generalization across tasks. These methods generally assume the selection process captures similarities between the exemplar and the target instance, however, it remains unknown what kinds of similarities are captured and vital to performing ICL. To dive into this question, we analyze the working mechanisms of the learning-based demonstration selection methods and empirically identify two important factors related to similarity measurement: 1) The ability to integrate different levels of task-agnostic text similarities between the input of exemplars and test cases enhances generalization power across different tasks. 2) Incorporating task-specific labels when measuring the similarities significantly improves the performance on each specific task.
We validate these two findings through extensive quantitative and qualitative analyses across ten datasets and various LLMs. Based on our findings, we introduce two effective yet simplified exemplar selection methods catering to task-agnostic and task-specific demands, eliminating the costly LLM inference overhead.

------------

`[2301.00234] A Survey on In-context Learning <https://arxiv.org/abs/2301.00234>`__ 语境学习综述

::

    replaced with revised version Tue, 18 Jun 2024 04:19:31 GMT
    Submission history From: Qingxiu Dong [view email]
    [v1] Sat, 31 Dec 2022 15:57:09 UTC (426 KB)
    [v2] Wed, 8 Feb 2023 02:59:46 UTC (439 KB)
    [v3] Thu, 1 Jun 2023 12:23:40 UTC (6,761 KB)
    [v4] Tue, 18 Jun 2024 04:19:31 UTC (8,286 KB)
    Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li and Zhifang Sui

With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.

------------

`[2402.11750] In-Context Learning Demonstration Selection via Influence Analysis <https://arxiv.org/abs/2402.11750>`__ 基于影响分析的情境学习演示选择

::

    replaced with revised version Mon, 17 Jun 2024 18:34:54 GMT
    Submission history From: Vinay M.S. [view email]
    [v1] Mon, 19 Feb 2024 00:39:31 UTC (164 KB)
    [v2] Mon, 17 Jun 2024 18:34:54 UTC (162 KB)
    Vinay M.S., Minh-Hao Van, Xintao Wu

Large Language Models (LLMs) have showcased their In-Context Learning (ICL) capabilities, enabling few-shot learning without the need for gradient updates. Despite its advantages, the effectiveness of ICL heavily depends on the choice of demonstrations. Selecting the most effective demonstrations for ICL remains a significant research challenge. To tackle this issue, we propose a demonstration selection method named InfICL, which utilizes influence functions to analyze impacts of training samples. By identifying the most influential training samples as demonstrations, InfICL aims to enhance the ICL generalization performance. To keep InfICL cost-effective, we only use the LLM to generate sample input embeddings, avoiding expensive fine-tuning. Through empirical studies on various real-world datasets, we demonstrate advantages of InfICL compared to state-of-the-art baselines.

------------

`[2405.10288] Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction <https://arxiv.org/abs/2405.10288>`__ 基于时间线的句子分解与上下文学习的时序事实抽取

::

    replaced with revised version Tue, 18 Jun 2024 08:22:29 GMT
    Submission history From: Jianhao Chen [view email]
    [v1] Thu, 16 May 2024 17:48:21 UTC (716 KB)
    [v2] Sun, 2 Jun 2024 08:04:26 UTC (716 KB)
    [v3] Tue, 18 Jun 2024 08:22:29 UTC (716 KB)
    Jianhao Chen, Haoyuan Ouyang, Junyang Ren, Wentao Ding, Wei Hu, Yuzhong Qu

Facts extraction is pivotal for constructing knowledge graphs. Recently, the increasing demand for temporal facts in downstream tasks has led to the emergence of the task of temporal fact extraction. In this paper, we specifically address the extraction of temporal facts from natural language text. Previous studies fail to handle the challenge of establishing time-to-fact correspondences in complex sentences. To overcome this hurdle, we propose a timeline-based sentence decomposition strategy using large language models (LLMs) with in-context learning, ensuring a fine-grained understanding of the timeline associated with various facts. In addition, we evaluate the performance of LLMs for direct temporal fact extraction and get unsatisfactory results. To this end, we introduce TSDRE, a method that incorporates the decomposition capabilities of LLMs into the traditional fine-tuning of smaller pre-trained language models (PLMs). To support the evaluation, we construct ComplexTRED, a complex temporal fact extraction dataset. Our experiments show that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and ComplexTRED datasets.

------------

`[2402.00743] Theoretical Understanding of In-Context Learning in Shallow Transformers with Unstructured Data <https://arxiv.org/abs/2402.00743>`__ 基于非结构化数据的浅层transformer中上下文学习的理论理解

::

    replaced with revised version Tue, 18 Jun 2024 13:11:32 GMT
    Submission history From: Yue Xing [view email]
    [v1] Thu, 1 Feb 2024 16:39:45 UTC (4,371 KB)
    [v2] Tue, 18 Jun 2024 13:11:32 UTC (4,232 KB)
    Yue Xing, Xiaofeng Lin, Chenheng Xu, Namjoon Suh, Qifan Song, Guang Cheng

Large language models (LLMs) are powerful models that can learn concepts at the inference stage via in-context learning (ICL). While theoretical studies, e.g., \cite{zhang2023trained}, attempt to explain the mechanism of ICL, they assume the input $x_i$ and the output $y_i$ of each demonstration example are in the same token (i.e., structured data). However, in real practice, the examples are usually text input, and all words, regardless of their logic relationship, are stored in different tokens (i.e., unstructured data \cite{wibisono2023role}). To understand how LLMs learn from the unstructured data in ICL, this paper studies the role of each component in the transformer architecture and provides a theoretical understanding to explain the success of the architecture. In particular, we consider a simple transformer with one/two attention layers and linear regression tasks for the ICL prediction. We observe that (1) a transformer with two layers of (self-)attentions with a look-ahead attention mask can learn from the prompt in the unstructured data, and (2) positional encoding can match the $x_i$ and $y_i$ tokens to achieve a better ICL performance.

------------

--------------
Reasoning (15)
--------------

`[2406.12288] An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs <https://arxiv.org/abs/2406.12288>`__ 神经元激活作为统一透镜来解释llm引发的算术推理思维链的研究

::

    Tue, 18 Jun 2024 05:49:24 GMT
    Daking Rai, Ziyu Yao

Large language models (LLMs) have shown strong arithmetic reasoning capabilities when prompted with Chain-of-Thought (CoT) prompts. However, we have only a limited understanding of how they are processed by LLMs. To demystify it, prior work has primarily focused on ablating different components in the CoT prompt and empirically observing their resulting LLM performance change. Yet, the reason why these components are important to LLM reasoning is not explored. To fill this gap, in this work, we investigate ``neuron activation'' as a lens to provide a unified explanation to observations made by prior work. Specifically, we look into neurons within the feed-forward layers of LLMs that may have activated their arithmetic reasoning capabilities, using Llama2 as an example. To facilitate this investigation, we also propose an approach based on GPT-4 to automatically identify neurons that imply arithmetic reasoning. Our analyses revealed that the activation of reasoning neurons in the feed-forward layers of an LLM can explain the importance of various components in a CoT prompt, and future research can extend it for a more complete understanding.

------------

`[2406.12084] When Reasoning Meets Information Aggregation: A Case Study with Sports Narratives <https://arxiv.org/abs/2406.12084>`__ 当推理遇到信息聚合:体育叙事案例研究

::

    Mon, 17 Jun 2024 20:49:35 GMT
    Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Wenlin Yao, Hassan Foroosh, Dong Yu, Fei Liu

Reasoning is most powerful when an LLM accurately aggregates relevant information. We examine the critical role of information aggregation in reasoning by requiring the LLM to analyze sports narratives. To succeed at this task, an LLM must infer points from actions, identify related entities, attribute points accurately to players and teams, and compile key statistics to draw conclusions. We conduct comprehensive experiments with real NBA basketball data and present SportsGen, a new method to synthesize game narratives. By synthesizing data, we can rigorously evaluate LLMs' reasoning capabilities under complex scenarios with varying narrative lengths and density of information. Our findings show that most models, including GPT-4o, often fail to accurately aggregate basketball scores due to frequent scoring patterns.
Open-source models like Llama-3 further suffer from significant score hallucinations. Finally, the effectiveness of reasoning is influenced by narrative complexity, information density, and domain-specific terms, highlighting the challenges in analytical reasoning tasks.

------------

`[2406.12255] A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning <https://arxiv.org/abs/2406.12255>`__ 基于Hopfieldian视图的思维链推理解释

::

    Tue, 18 Jun 2024 04:07:13 GMT
    Lijie Hu, Liang Liu, Shu Yang, Xin Chen, Hongru Xiao, Mengdi Li, Pan Zhou, Muhammad Asif Ali, and Di Wang

Chain-of-Thought (CoT) holds a significant place in augmenting the reasoning performance for large language models (LLMs). While some studies focus on improving CoT accuracy through methods like retrieval enhancement, yet a rigorous explanation for why CoT achieves such success remains unclear. In this paper, we analyze CoT methods under two different settings by asking the following questions: (1) For zero-shot CoT, why does prompting the model with "let's think step by step" significantly impact its outputs? (2) For few-shot CoT, why does providing examples before questioning the model could substantially improve its reasoning ability? To answer these questions, we conduct a top-down explainable analysis from the Hopfieldian view and propose a Read-and-Control approach for controlling the accuracy of CoT. Through extensive experiments on seven datasets for three different tasks, we demonstrate that our framework can decipher the inner workings of CoT, provide reasoning error localization, and control to come up with the correct reasoning path.

------------

`[2406.12319] PRePair: Pointwise Reasoning Enhance Pairwise Evaluating for Robust Instruction-Following Assessments <https://arxiv.org/abs/2406.12319>`__ 准备:逐点推理增强成对评估，以进行鲁棒的指令遵循评估

::

    Tue, 18 Jun 2024 06:43:04 GMT
    Hawon Jeong, ChaeHun Park, Jimin Hong, Jaegul Choo

Pairwise evaluation using large language models (LLMs) is widely used for evaluating natural language generation (NLG) tasks. However, the reliability of LLMs is often compromised by biases, such as favoring verbosity and authoritative tone. In the study, we focus on the comparison of two LLM-based evaluation approaches, pointwise and pairwise. Our findings demonstrate that pointwise evaluators exhibit more robustness against undesirable preferences.
Further analysis reveals that pairwise evaluators can accurately identify the shortcomings of low-quality outputs even when their judgment is incorrect.
These results indicate that LLMs are more severely influenced by their bias in a pairwise evaluation setup. To mitigate this, we propose a hybrid method that integrates pointwise reasoning into pairwise evaluation. Experimental results show that our method enhances the robustness of pairwise evaluators against adversarial samples while preserving accuracy on normal samples.

------------

`[2406.12331] Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text Understanding <https://arxiv.org/abs/2406.12331>`__ 检索满足推理:面向长文本理解的动态上下文编辑

::

    Tue, 18 Jun 2024 06:54:28 GMT
    Weizhi Fei, Xueyan Niu, Guoqing Xie, Yanhua Zhang, Bo Bai, Lei Deng, Wei Han

Current Large Language Models (LLMs) face inherent limitations due to their pre-defined context lengths, which impede their capacity for multi-hop reasoning within extensive textual contexts. While existing techniques like Retrieval-Augmented Generation (RAG) have attempted to bridge this gap by sourcing external information, they fall short when direct answers are not readily available. We introduce a novel approach that re-imagines information retrieval through dynamic in-context editing, inspired by recent breakthroughs in knowledge editing. By treating lengthy contexts as malleable external knowledge, our method interactively gathers and integrates relevant information, thereby enabling LLMs to perform sophisticated reasoning steps.
Experimental results demonstrate that our method effectively empowers context-limited LLMs, such as Llama2, to engage in multi-hop reasoning with improved performance, which outperforms state-of-the-art context window extrapolation methods and even compares favorably to more advanced commercial long-context models. Our interactive method not only enhances reasoning capabilities but also mitigates the associated training and computational costs, making it a pragmatic solution for enhancing LLMs' reasoning within expansive contexts.

------------

`[2406.12546] Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models <https://arxiv.org/abs/2406.12546>`__ 

::

    Tue, 18 Jun 2024 12:24:22 GMT
    Philipp Mondorf, Barbara Plank

Knights and knaves problems represent a classic genre of logical puzzles where characters either tell the truth or lie. The objective is to logically deduce each character's identity based on their statements. The challenge arises from the truth-telling or lying behavior, which influences the logical implications of each statement. Solving these puzzles requires not only direct deductions from individual statements, but the ability to assess the truthfulness of statements by reasoning through various hypothetical scenarios.
As such, knights and knaves puzzles serve as compelling examples of suppositional reasoning. In this paper, we introduce $\textit{TruthQuest}$, a benchmark for suppositional reasoning based on the principles of knights and knaves puzzles. Our benchmark presents problems of varying complexity, considering both the number of characters and the types of logical statements involved. Evaluations on $\textit{TruthQuest}$ show that large language models like Llama 3 and Mixtral-8x7B exhibit significant difficulties solving these tasks. A detailed error analysis of the models' output reveals that lower-performing models exhibit a diverse range of reasoning errors, frequently failing to grasp the concept of truth and lies. In comparison, more proficient models primarily struggle with accurately inferring the logical implications of potentially false statements.

------------

`[2406.12572] Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models <https://arxiv.org/abs/2406.12572>`__ Mathador-LM:大型语言模型数学推理动态基准

::

    Tue, 18 Jun 2024 13:02:12 GMT
    Eldar Kurtic, Amir Moeini, Dan Alistarh

We introduce Mathador-LM, a new benchmark for evaluating the mathematical reasoning on large language models (LLMs), combining ruleset interpretation, planning, and problem-solving. This benchmark is inspired by the Mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances dynamically, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. Additionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary models struggle with Mathador-LM, scoring significantly lower than average 5th graders. This stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks.

------------

`[2406.12753] OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI <https://arxiv.org/abs/2406.12753>`__ OlympicArena:面向超级智能AI的多学科认知推理基准测试

::

    Tue, 18 Jun 2024 16:20:53 GMT
    Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, Pengfei Liu

The evolution of Artificial Intelligence (AI) has been significantly accelerated by advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), gradually showcasing potential cognitive reasoning abilities in problem-solving and scientific discovery (i.e., AI4Science) once exclusive to human intellect. To comprehensively evaluate current models' performance in cognitive reasoning abilities, we introduce OlympicArena, which includes 11,163 bilingual problems across both text-only and interleaved text-image modalities. These challenges encompass a wide range of disciplines spanning seven fields and 62 international Olympic competitions, rigorously examined for data leakage. We argue that the challenges in Olympic competition problems are ideal for evaluating AI's cognitive reasoning due to their complexity and interdisciplinary nature, which are essential for tackling complex scientific challenges and facilitating discoveries. Beyond evaluating performance across various disciplines using answer-only criteria, we conduct detailed experiments and analyses from multiple perspectives. We delve into the models' cognitive reasoning abilities, their performance across different modalities, and their outcomes in process-level evaluations, which are vital for tasks requiring complex reasoning with lengthy solutions. Our extensive evaluations reveal that even advanced models like GPT-4o only achieve a 39.97% overall accuracy, illustrating current AI limitations in complex reasoning and multimodal integration. Through the OlympicArena, we aim to advance AI towards superintelligence, equipping it to address more complex challenges in science and beyond. We also provide a comprehensive set of resources to support AI research, including a benchmark dataset, an open-source annotation platform, a detailed evaluation tool, and a leaderboard with automatic submission features.

------------

`[2406.12742] Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning <https://arxiv.org/abs/2406.12742>`__ 视觉和语言模型中的多图像理解基准测试:感知、知识、推理和多跳推理

::

    Tue, 18 Jun 2024 16:02:18 GMT
    Bingchen Zhao, Yongshuo Zong, Letian Zhang, Timothy Hospedales

The advancement of large language models (LLMs) has significantly broadened the scope of applications in natural language processing, with multi-modal LLMs extending these capabilities to integrate and interpret visual data. However, existing benchmarks for visual language models (VLMs) predominantly focus on single-image inputs, neglecting the crucial aspect of multi-image understanding. In this paper, we introduce a Multi-Image Relational Benchmark MIRB, designed to evaluate VLMs' ability to compare, analyze, and reason across multiple images. Our benchmark encompasses four categories: perception, visual world knowledge, reasoning, and multi-hop reasoning. Through a comprehensive evaluation of a wide range of open-source and closed-source models, we demonstrate that while open-source VLMs were shown to approach the performance of GPT-4V in single-image tasks, a significant performance gap remains in multi-image reasoning tasks. Our findings also reveal that even the state-of-the-art GPT-4V model struggles with our benchmark, underscoring the need for further research and development in this area. We believe our contribution of MIRB could serve as a testbed for developing the next-generation multi-modal models.

------------

`[2404.11041] On the Empirical Complexity of Reasoning and Planning in LLMs <https://arxiv.org/abs/2404.11041>`__ llm中推理和规划的经验复杂性

::

    replaced with revised version Tue, 18 Jun 2024 02:03:35 GMT
    Submission history From: Zirui Zhao [view email]
    [v1] Wed, 17 Apr 2024 03:34:27 UTC (9,697 KB)
    [v2] Tue, 18 Jun 2024 02:03:35 UTC (8,729 KB)
    Liwei Kang, Zirui Zhao, David Hsu, Wee Sun Lee

Chain-of-thought (CoT), tree-of-thought (ToT), and related techniques work surprisingly well in practice for some complex reasoning tasks with Large Language Models (LLMs), but why? This work seeks the underlying reasons by conducting experimental case studies and linking the performance benefits to well-established sample and computational complexity principles in machine learning. We experimented with 6 reasoning tasks, ranging from grade school math, air travel planning, ..., to Blocksworld. The results suggest that (i) both CoT and ToT benefit significantly from task decomposition, which breaks a complex reasoning task into a sequence of steps with low sample complexity and explicitly outlines the reasoning structure, and (ii) for computationally hard reasoning tasks, the more sophisticated tree structure of ToT outperforms the linear structure of CoT. These findings provide useful guidelines for the use of LLM in solving reasoning tasks in practice.

------------

`[2405.00451] Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning <https://arxiv.org/abs/2405.00451>`__ 蒙特卡洛树搜索通过迭代偏好学习促进推理

::

    replaced with revised version Mon, 17 Jun 2024 22:11:49 GMT
    Submission history From: Yuxi Xie [view email]
    [v1] Wed, 1 May 2024 11:10:24 UTC (379 KB)
    [v2] Mon, 17 Jun 2024 22:11:49 UTC (375 KB)
    Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, Michael Shieh

We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\%$ (+$5.9\%$), $34.7\%$ (+$5.8\%$), and $76.4\%$ (+$15.8\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at this https URL.

------------

`[2401.15269] Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2401.15269>`__ 用检索增强的大型语言模型通过检索和自我反思改进医学推理

::

    replaced with revised version Tue, 18 Jun 2024 02:10:15 GMT
    Submission history From: Minbyul Jeong [view email]
    [v1] Sat, 27 Jan 2024 02:29:42 UTC (487 KB)
    [v2] Wed, 3 Apr 2024 01:27:20 UTC (487 KB)
    [v3] Tue, 18 Jun 2024 02:10:15 UTC (303 KB)
    Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang

Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains.

------------

`[2402.18439] Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication <https://arxiv.org/abs/2402.18439>`__ 超越自然语言:llm利用替代格式来增强推理和沟通

::

    replaced with revised version Tue, 18 Jun 2024 03:06:39 GMT
    Submission history From: Weize Chen [view email]
    [v1] Wed, 28 Feb 2024 16:07:54 UTC (1,936 KB)
    [v2] Tue, 18 Jun 2024 03:06:39 UTC (1,943 KB)
    Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen Qian, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun

Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code is released at \url{this https URL}.

------------

`[2406.11012] Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game <https://arxiv.org/abs/2406.11012>`__ Connecting the Dots:使用New York Times Connections文字游戏评估llm的抽象推理能力

::

    replaced with revised version Tue, 18 Jun 2024 15:02:28 GMT
    Submission history From: Tuhin Chakrabarty Mr [view email]
    [v1] Sun, 16 Jun 2024 17:10:32 UTC (5,032 KB)
    [v2] Tue, 18 Jun 2024 15:02:28 UTC (5,032 KB)
    Prisha Samadarshi, Mariam Mustafa, Anushka Kulkarni, Raven Rothkopf, Tuhin Chakrabarty, Smaranda Muresan

The New York Times Connections game has emerged as a popular and challenging pursuit for word puzzle enthusiasts. We collect 200 Connections games to evaluate the performance of state-of-the-art large language models (LLMs) against expert and novice human players. Our results show that even the best-performing LLM, GPT-4o, which has otherwise shown impressive reasoning abilities on a wide variety of benchmarks, can only fully solve 8% of the games. Compared to GPT-4o, novice and expert players perform better, with expert human players significantly outperforming GPT-4o. To deepen our understanding we create a taxonomy of the knowledge types required to successfully categorize words in the Connections game, revealing that LLMs struggle with associative, encyclopedic, and linguistic knowledge. Our findings establish the New York Times Connections game as a challenging benchmark for evaluating abstract reasoning capabilities in humans and AI systems.

------------

`[2403.19322] Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models <https://arxiv.org/abs/2403.19322>`__ 多模态大型语言模型推理的即插即用基础

::

    replaced with revised version Tue, 18 Jun 2024 05:57:14 GMT
    Submission history From: Yuxuan Liu [view email]
    [v1] Thu, 28 Mar 2024 11:26:30 UTC (2,468 KB)
    [v2] Tue, 18 Jun 2024 05:57:14 UTC (3,905 KB)
    Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Weimo Deng, Ziyong Feng, Yongle Zhao, Yin Xie

The rise of Multimodal Large Language Models (MLLMs), renowned for their advanced instruction-following and reasoning capabilities, has significantly propelled the field of visual reasoning. However, due to limitations in their image tokenization processes, most MLLMs struggle to capture fine details of text and objects in images, especially in high-resolution samples. To overcome this limitation, we introduce P2G, a novel framework for plug-and-play grounding in MLLMs. P2G utilizes the tool-usage potential of MLLMs to employ expert agents for on-the-fly grounding of reasoning into critical visual and textual elements in images, thereby enabling deliberate reasoning through multimodal prompting. Additionally, we develop P2GB, a benchmark designed to evaluate MLLMs' proficiency in understanding inter-object relationships and textual content in challenging high-resolution images. Extensive experiments on visual reasoning tasks demonstrate the superiority of P2G, achieving performance comparable to GPT-4V on P2GB with a 7B backbone. Our work underscores the potential of grounding reasoning with external agents in MLLMs, presenting a promising alternative to mere model scaling.

------------

-----------
ToolUse (8)
-----------

`[2406.12276] CodeNav: Beyond tool-use to using real-world codebases with LLM agents <https://arxiv.org/abs/2406.12276>`__ CodeNav:超越工具使用，使用LLM代理使用真实世界的代码库

::

    Tue, 18 Jun 2024 05:10:38 GMT
    Tanmay Gupta, Luca Weihs, Aniruddha Kembhavi

We present CodeNav, an LLM agent that navigates and leverages previously unseen code repositories to solve user queries. In contrast to tool-use LLM agents that require ``registration'' of all relevant tools via manual descriptions within the LLM context, CodeNav automatically indexes and searches over code blocks in the target codebase, finds relevant code snippets, imports them, and uses them to iteratively generate a solution with execution feedback.
To highlight the core-capabilities of CodeNav, we first showcase three case studies where we use CodeNav for solving complex user queries using three diverse codebases. Next, on three benchmarks, we quantitatively compare the effectiveness of code-use (which only has access to the target codebase) to tool-use (which has privileged access to all tool names and descriptions).
Finally, we study the effect of varying kinds of tool and library descriptions on code-use performance, as well as investigate the advantage of the agent seeing source code as opposed to natural descriptions of code. All code will be made open source under a permissive license.

------------

`[2406.12266] Towards a Client-Centered Assessment of LLM Therapists by Client Simulation <https://arxiv.org/abs/2406.12266>`__ 

::

    Tue, 18 Jun 2024 04:46:55 GMT
    Jiashuo Wang, Yang Xiao, Yanran Li, Changhe Song, Chunpu Xu, Chenhao Tan, Wenjie Li

Although there is a growing belief that LLMs can be used as therapists, exploring LLMs' capabilities and inefficacy, particularly from the client's perspective, is limited. This work focuses on a client-centered assessment of LLM therapists with the involvement of simulated clients, a standard approach in clinical medical education. However, there are two challenges when applying the approach to assess LLM therapists at scale. Ethically, asking humans to frequently mimic clients and exposing them to potentially harmful LLM outputs can be risky and unsafe. Technically, it can be difficult to consistently compare the performances of different LLM therapists interacting with the same client. To this end, we adopt LLMs to simulate clients and propose ClientCAST, a client-centered approach to assessing LLM therapists by client simulation.
Specifically, the simulated client is utilized to interact with LLM therapists and complete questionnaires related to the interaction. Based on the questionnaire results, we assess LLM therapists from three client-centered aspects: session outcome, therapeutic alliance, and self-reported feelings. We conduct experiments to examine the reliability of ClientCAST and use it to evaluate LLMs therapists implemented by Claude-3, GPT-3.5, LLaMA3-70B, and Mixtral 8*7B. Codes are released at https://github.com/wangjs9/ClientCAST.

------------

`[2406.12307] Can Tool-augmented Large Language Models be Aware of Incomplete Conditions? <https://arxiv.org/abs/2406.12307>`__ 工具增强的大型语言模型能意识到不完整的条件吗?

::

    Tue, 18 Jun 2024 06:28:06 GMT
    Seungbin Yang, ChaeHun Park, Taehee Kim, Jaegul Choo

Recent advancements in integrating large language models (LLMs) with tools have allowed the models to interact with real-world environments. However, these tool-augmented LLMs often encounter incomplete scenarios when users provide partial information or the necessary tools are unavailable. Recognizing and managing such scenarios is crucial for LLMs to ensure their reliability, but this exploration remains understudied. This study examines whether LLMs can identify incomplete conditions and appropriately determine when to refrain from using tools. To this end, we address a dataset by manipulating instances from two datasets by removing necessary tools or essential information for tool invocation. We confirm that most LLMs are challenged to identify the additional information required to utilize specific tools and the absence of appropriate tools. Our research can contribute to advancing reliable LLMs by addressing scenarios that commonly arise during interactions between humans and LLMs.

------------

`[2406.12793] ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools <https://arxiv.org/abs/2406.12793>`__ ChatGLM:一个大型语言模型族从GLM-130B到GLM-4所有工具

::

    Tue, 18 Jun 2024 16:58:21 GMT
    Team GLM: Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang

We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through https://github.com/THUDM and https://huggingface.co/THUDM.

------------

`[2402.09334] AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach <https://arxiv.org/abs/2402.09334>`__ AuditLLM:基于Multiprobe方法的大型语言模型审计工具

::

    replaced with revised version Mon, 17 Jun 2024 18:24:41 GMT
    Submission history From: Maryam Amirizaniani [view email]
    [v1] Wed, 14 Feb 2024 17:31:04 UTC (1,121 KB)
    [v2] Mon, 17 Jun 2024 18:24:41 UTC (2,072 KB)
    Maryam Amirizaniani, Elias Martin, Tanya Roosta, Aman Chadha, Chirag Shah

As Large Language Models (LLMs) are integrated into various sectors, ensuring their reliability and safety is crucial. This necessitates rigorous probing and auditing to maintain their effectiveness and trustworthiness in practical applications. Subjecting LLMs to varied iterations of a single query can unveil potential inconsistencies in their knowledge base or functional capacity. However, a tool for performing such audits with a easy to execute workflow, and low technical threshold is lacking. In this demo, we introduce ``AuditLLM,'' a novel tool designed to audit the performance of various LLMs in a methodical way. AuditLLM's primary function is to audit a given LLM by deploying multiple probes derived from a single question, thus detecting any inconsistencies in the model's comprehension or performance. A robust, reliable, and consistent LLM is expected to generate semantically similar responses to variably phrased versions of the same question. Building on this premise, AuditLLM generates easily interpretable results that reflect the LLM's consistency based on a single input question provided by the user. A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues. One could then use the output of AuditLLM to further investigate issues with the aforementioned LLM. To facilitate demonstration and practical uses, AuditLLM offers two key modes: (1) Live mode which allows instant auditing of LLMs by analyzing responses to real-time queries; and (2) Batch mode which facilitates comprehensive LLM auditing by processing multiple queries at once for in-depth analysis. This tool is beneficial for both researchers and general users, as it enhances our understanding of LLMs' capabilities in generating responses, using a standardized auditing platform.

------------

`[2401.08508] EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis <https://arxiv.org/abs/2401.08508>`__ EmoLLMs:面向全面情感分析的一系列情感大型语言模型和标注工具

::

    replaced with revised version Tue, 18 Jun 2024 02:24:03 GMT
    Submission history From: Zhiwei Liu [view email]
    [v1] Tue, 16 Jan 2024 17:11:11 UTC (861 KB)
    [v2] Tue, 18 Jun 2024 02:24:03 UTC (936 KB)
    Zhiwei Liu, Kailai Yang, Tianlin Zhang, Qianqian Xie, Sophia Ananiadou

Sentiment analysis and emotion detection are important research topics in natural language processing (NLP) and benefit many downstream tasks. With the widespread application of LLMs, researchers have started exploring the application of LLMs based on instruction-tuning in the field of sentiment analysis. However, these models only focus on single aspects of affective classification tasks (e.g. sentimental polarity or categorical emotions), and overlook the regression tasks (e.g. sentiment strength or emotion intensity), which leads to poor performance in downstream tasks. The main reason is the lack of comprehensive affective instruction tuning datasets and evaluation benchmarks, which cover various affective classification and regression tasks. Moreover, although emotional information is useful for downstream tasks, existing downstream datasets lack high-quality and comprehensive affective annotations. In this paper, we propose EmoLLMs, the first series of open-sourced instruction-following LLMs for comprehensive affective analysis based on fine-tuning various LLMs with instruction data, the first multi-task affective analysis instruction dataset (AAID) with 234K data samples based on various classification and regression tasks to support LLM instruction tuning, and a comprehensive affective evaluation benchmark (AEB) with 14 tasks from various sources and domains to test the generalization ability of LLMs. We propose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various affective instruction tasks. We compare our model with a variety of LLMs on AEB, where our models outperform all other open-sourced LLMs, and surpass ChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve the ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks, and demonstrates our models can be used as affective annotation tools.

------------

`[2406.11200] AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval <https://arxiv.org/abs/2406.11200>`__ AvaTaR:优化LLM agent的工具辅助知识检索

::

    replaced with revised version Tue, 18 Jun 2024 01:39:57 GMT
    Submission history From: Shirley Wu [view email]
    [v1] Mon, 17 Jun 2024 04:20:02 UTC (13,925 KB)
    [v2] Tue, 18 Jun 2024 01:39:57 UTC (13,925 KB)
    Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N. Ioannidis, Karthik Subbian, Jure Leskovec, James Zou

Large language model (LLM) agents have demonstrated impressive capability in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing the prompting techniques that make LLM agents able to effectively use external tools and knowledge is a heuristic and laborious task. Here, we introduce AvaTaR, a novel and automatic framework that optimizes an LLM agent to effectively use the provided tools and improve its performance on a given task/domain. During optimization, we design a comparator module to iteratively provide insightful and holistic prompts to the LLM agent via reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information. We find AvaTaR consistently outperforms state-of-the-art approaches across all four challenging tasks and exhibits strong generalization ability when applied to novel cases, achieving an average relative improvement of 14% on the Hit@1 metric. Code and dataset are available at this https URL.

------------

`[2402.03130] User Centric Evaluation of Code Generation Tools <https://arxiv.org/abs/2402.03130>`__ 

::

    replaced with revised version Tue, 18 Jun 2024 13:45:05 GMT
    Submission history From: Hong Zhu [view email]
    [v1] Mon, 5 Feb 2024 15:56:19 UTC (557 KB)
    [v2] Tue, 9 Apr 2024 12:37:56 UTC (4,614 KB)
    [v3] Tue, 18 Jun 2024 13:45:05 UTC (3,066 KB)
    Tanha Miah and Hong Zhu

With the rapid advance of machine learning (ML) technology, large language models (LLMs) are increasingly explored as an intelligent tool to generate program code from natural language specifications. However, existing evaluations of LLMs have focused on their capabilities in comparison with humans. It is desirable to evaluate their usability when deciding on whether to use a LLM in software production. This paper proposes a user centric method for this purpose. It includes metadata in the test cases of a benchmark to describe their usages, conducts testing in a multi-attempt process that mimics the uses of LLMs, measures LLM generated solutions on a set of quality attributes that reflect usability, and evaluates the performance based on user experiences in the uses of LLMs as a tool.
The paper also reports a case study with the method in the evaluation of ChatGPT's usability as a code generation tool for the R programming language. Our experiments demonstrated that ChatGPT is highly useful for generating R program code although it may fail on hard programming tasks. The user experiences are good with overall average number of attempts being 1.61 and the average time of completion being 47.02 seconds. Our experiments also found that the weakest aspect of usability is conciseness, which has a score of 3.80 out of 5.

------------

------------------------
Retrieval-Augmented (12)
------------------------

`[2406.12066] Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks <https://arxiv.org/abs/2406.12066>`__ 在生物医学基准中，语言模型对药物名称的影响令人惊讶地脆弱

::

    Mon, 17 Jun 2024 20:09:24 GMT
    Jack Gallifant, Shan Chen, Pedro Moreira, Nikolaj Munch, Mingye Gao, Jackson Pond, Leo Anthony Celi, Hugo Aerts, Thomas Hartvigsen, Danielle Bitterman

Medical knowledge is context-dependent and requires consistent reasoning across various natural language expressions of semantically equivalent phrases.
This is particularly crucial for drug names, where patients often use brand names like Advil or Tylenol instead of their generic equivalents. To study this, we create a new robustness dataset, RABBITS, to evaluate performance differences on medical benchmarks after swapping brand and generic drug names using physician expert annotations.
We assess both open-source and API-based LLMs on MedQA and MedMCQA, revealing a consistent performance drop ranging from 1-10\%. Furthermore, we identify a potential source of this fragility as the contamination of test data in widely used pre-training datasets. All code is accessible at https://github.com/BittermanLab/RABBITS, and a HuggingFace leaderboard is available at https://huggingface.co/spaces/AIM-Harvard/rabbits-leaderboard.

------------

`[2406.12197] Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction <https://arxiv.org/abs/2406.12197>`__ 辩论作为优化:事件抽取的自适应保角预测和多样化检索

::

    Tue, 18 Jun 2024 01:53:49 GMT
    Sijia Wang and Lifu Huang

We propose a multi-agent debate as optimization (DAO) system for event extraction, where the primary objective is to iteratively refine the large language models (LLMs) outputs through debating without parameter tuning. In DAO, we introduce two novel modules: the Diverse-RAG (DRAG) module and the Adaptive Conformal Prediction (AdaCP) module. DRAG systematically retrieves supporting information that best fits the debate discussion, while AdaCP enhances the accuracy and reliability of event extraction by effectively rejecting less promising answers. Experimental results demonstrate a significant reduction in the performance gap between supervised approaches and tuning-free LLM-based methods by 18.1% and 17.8% on ACE05 and 17.9% and 15.2% on CASIE for event detection and argument extraction respectively.

------------

`[2406.12331] Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text Understanding <https://arxiv.org/abs/2406.12331>`__ 检索满足推理:面向长文本理解的动态上下文编辑

::

    Tue, 18 Jun 2024 06:54:28 GMT
    Weizhi Fei, Xueyan Niu, Guoqing Xie, Yanhua Zhang, Bo Bai, Lei Deng, Wei Han

Current Large Language Models (LLMs) face inherent limitations due to their pre-defined context lengths, which impede their capacity for multi-hop reasoning within extensive textual contexts. While existing techniques like Retrieval-Augmented Generation (RAG) have attempted to bridge this gap by sourcing external information, they fall short when direct answers are not readily available. We introduce a novel approach that re-imagines information retrieval through dynamic in-context editing, inspired by recent breakthroughs in knowledge editing. By treating lengthy contexts as malleable external knowledge, our method interactively gathers and integrates relevant information, thereby enabling LLMs to perform sophisticated reasoning steps.
Experimental results demonstrate that our method effectively empowers context-limited LLMs, such as Llama2, to engage in multi-hop reasoning with improved performance, which outperforms state-of-the-art context window extrapolation methods and even compares favorably to more advanced commercial long-context models. Our interactive method not only enhances reasoning capabilities but also mitigates the associated training and computational costs, making it a pragmatic solution for enhancing LLMs' reasoning within expansive contexts.

------------

`[2406.12430] PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers <https://arxiv.org/abs/2406.12430>`__ PlanRAG:作为决策者的生成式大型语言模型的计划-然后检索增强生成

::

    Tue, 18 Jun 2024 09:25:35 GMT
    Myeonghwa Lee, Seonho An, Min-Soo Kim

In this paper, we conduct a study to utilize LLMs as a solution for decision making that requires complex data analysis. We define Decision QA as the task of answering the best decision, $d_{best}$, for a decision-making question $Q$, business rules $R$ and a database $D$. Since there is no benchmark that can examine Decision QA, we propose Decision QA benchmark, DQA. It has two scenarios, Locating and Building, constructed from two video games (Europa Universalis IV and Victoria 3) that have almost the same goal as Decision QA.
To address Decision QA effectively, we also propose a new RAG technique called the iterative plan-then-retrieval augmented generation (PlanRAG). Our PlanRAG-based LM generates the plan for decision making as the first step, and the retriever generates the queries for data analysis as the second step. The proposed method outperforms the state-of-the-art iterative RAG method by 15.8% in the Locating scenario and by 7.4% in the Building scenario, respectively. We release our code and benchmark at https://github.com/myeon9h/PlanRAG.

------------

`[2406.12494] LightPAL: Lightweight Passage Retrieval for Open Domain Multi-Document Summarization <https://arxiv.org/abs/2406.12494>`__ LightPAL:面向开放域多文档摘要的轻量级段落检索

::

    Tue, 18 Jun 2024 10:57:27 GMT
    Masafumi Enomoto, Kunihiro Takeoka, Kosuke Akimoto, Kiril Gashteovski, Masafumi Oyamada

Open-Domain Multi-Document Summarization (ODMDS) is crucial for addressing diverse information needs, which aims to generate a summary as answer to user's query, synthesizing relevant content from multiple documents in a large collection. Existing approaches that first find relevant passages and then generate a summary using a language model are inadequate for ODMDS. This is because open-ended queries often require additional context for the retrieved passages to cover the topic comprehensively, making it challenging to retrieve all relevant passages initially. While iterative retrieval methods have been explored for multi-hop question answering (MQA), they are impractical for ODMDS due to high latency from repeated large language model (LLM) inference for reasoning. To address this issue, we propose LightPAL, a lightweight passage retrieval method for ODMDS that constructs a graph representing passage relationships using an LLM during indexing and employs random walk instead of iterative reasoning and retrieval at inference time. Experiments on ODMDS benchmarks show that LightPAL outperforms baseline retrievers in summary quality while being significantly more efficient than an iterative MQA approach.

------------

`[2406.12566] RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation <https://arxiv.org/abs/2406.12566>`__ RichRAG:在检索增强生成中为多方面查询设计丰富的响应

::

    Tue, 18 Jun 2024 12:52:51 GMT
    Shuting Wang, Xin Xu, Mang Wang, Weipeng Chen, Yutao Zhu, Zhicheng Dou

Retrieval-augmented generation (RAG) effectively addresses issues of static knowledge and hallucination in large language models. Existing studies mostly focus on question scenarios with clear user intents and concise answers.
However, it is prevalent that users issue broad, open-ended queries with diverse sub-intents, for which they desire rich and long-form answers covering multiple relevant aspects. To tackle this important yet underexplored problem, we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect explorer to identify potential sub-aspects of input questions, a multi-faceted retriever to build a candidate pool of diverse external documents related to these sub-aspects, and a generative list-wise ranker, which is a key module to provide the top-k most valuable documents for the final generator. These ranked documents sufficiently cover various query aspects and are aware of the generator's preferences, hence incentivizing it to produce rich and comprehensive responses for users. The training of our ranker involves a supervised fine-tuning stage to ensure the basic coverage of documents, and a reinforcement learning stage to align downstream LLM's preferences to the ranking of documents. Experimental results on two publicly available datasets prove that our framework effectively and efficiently provides comprehensive and satisfying responses to users.

------------

`[2401.15269] Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2401.15269>`__ 用检索增强的大型语言模型通过检索和自我反思改进医学推理

::

    replaced with revised version Tue, 18 Jun 2024 02:10:15 GMT
    Submission history From: Minbyul Jeong [view email]
    [v1] Sat, 27 Jan 2024 02:29:42 UTC (487 KB)
    [v2] Wed, 3 Apr 2024 01:27:20 UTC (487 KB)
    [v3] Tue, 18 Jun 2024 02:10:15 UTC (303 KB)
    Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang

Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains.

------------

`[2402.15301] Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models <https://arxiv.org/abs/2402.15301>`__ 基于检索增强生成的大型语言模型因果图发现

::

    replaced with revised version Tue, 18 Jun 2024 05:51:50 GMT
    Submission history From: Yuzhe Zhang [view email]
    [v1] Fri, 23 Feb 2024 13:02:10 UTC (7,324 KB)
    [v2] Tue, 18 Jun 2024 05:51:50 UTC (63 KB)
    Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, Chen Wang

Causal graph recovery is traditionally done using statistical estimation-based methods or based on individual's knowledge about variables of interests. They often suffer from data collection biases and limitations of individuals' knowledge. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that leverages LLMs to deduce causal relationships in general causal graph recovery tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs extracted from scientific publication database as well as experiment data about factors of interest to achieve this goal. Our method gives a prompting strategy to extract associational relationships among those factors and a mechanism to perform causality verification for these associations. Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets. More importantly, as causality among some factors may change as new research results emerge, our method show sensitivity to new evidence in the literature and can provide useful information for updating causal graphs accordingly.

------------

`[2402.18439] Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication <https://arxiv.org/abs/2402.18439>`__ 超越自然语言:llm利用替代格式来增强推理和沟通

::

    replaced with revised version Tue, 18 Jun 2024 03:06:39 GMT
    Submission history From: Weize Chen [view email]
    [v1] Wed, 28 Feb 2024 16:07:54 UTC (1,936 KB)
    [v2] Tue, 18 Jun 2024 03:06:39 UTC (1,943 KB)
    Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen Qian, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun

Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code is released at \url{this https URL}.

------------

`[2406.11357] Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities <https://arxiv.org/abs/2406.11357>`__ Refiner:有效重构检索内容以提高问答能力

::

    replaced with revised version Tue, 18 Jun 2024 02:44:27 GMT
    Submission history From: Zhonghao Li [view email]
    [v1] Mon, 17 Jun 2024 09:25:10 UTC (975 KB)
    [v2] Tue, 18 Jun 2024 02:44:27 UTC (975 KB)
    Zhonghao Li, Xuming Hu, Aiwei Liu, Kening Zheng, Sirui Huang, Hui Xiong

Large Language Models (LLMs) are limited by their parametric knowledge, leading to hallucinations in knowledge-extensive tasks. To address this, Retrieval-Augmented Generation (RAG) incorporates external document chunks to expand LLM knowledge. Furthermore, compressing information from document chunks through extraction or summarization can improve LLM performance. Nonetheless, LLMs still struggle to notice and utilize scattered key information, a problem known as the "lost-in-the-middle" syndrome. Therefore, we typically need to restructure the content for LLM to recognize the key information. We propose $\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that operates in the post-retrieval process of RAG. $\textit{Refiner}$ leverages a single decoder-only LLM to adaptively extract query-relevant contents verbatim along with the necessary context, and section them based on their interconnectedness, thereby highlights information distinction, and aligns downstream LLMs with the original context effectively. Experiments show that a trained $\textit{Refiner}$ (with 7B parameters) exhibits significant gain to downstream LLM in improving answer accuracy, and outperforms other state-of-the-art advanced RAG and concurrent compressing approaches in various single-hop and multi-hop QA tasks. Notably, $\textit{Refiner}$ achieves a 80.5% tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared to the next best solution. $\textit{Refiner}$ is a plug-and-play solution that can be seamlessly integrated with RAG systems, facilitating its application across diverse open-source frameworks.

------------

`[2406.11200] AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval <https://arxiv.org/abs/2406.11200>`__ AvaTaR:优化LLM agent的工具辅助知识检索

::

    replaced with revised version Tue, 18 Jun 2024 01:39:57 GMT
    Submission history From: Shirley Wu [view email]
    [v1] Mon, 17 Jun 2024 04:20:02 UTC (13,925 KB)
    [v2] Tue, 18 Jun 2024 01:39:57 UTC (13,925 KB)
    Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N. Ioannidis, Karthik Subbian, Jure Leskovec, James Zou

Large language model (LLM) agents have demonstrated impressive capability in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing the prompting techniques that make LLM agents able to effectively use external tools and knowledge is a heuristic and laborious task. Here, we introduce AvaTaR, a novel and automatic framework that optimizes an LLM agent to effectively use the provided tools and improve its performance on a given task/domain. During optimization, we design a comparator module to iteratively provide insightful and holistic prompts to the LLM agent via reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information. We find AvaTaR consistently outperforms state-of-the-art approaches across all four challenging tasks and exhibits strong generalization ability when applied to novel cases, achieving an average relative improvement of 14% on the Hit@1 metric. Code and dataset are available at this https URL.

------------

`[2401.01065] BEV-TSR: Text-Scene Retrieval in BEV Space for Autonomous Driving <https://arxiv.org/abs/2401.01065>`__ BEV- tsr:面向自动驾驶的BEV空间文本-场景检索

::

    replaced with revised version Tue, 18 Jun 2024 04:20:51 GMT
    Submission history From: Tian Gao [view email]
    [v1] Tue, 2 Jan 2024 06:56:23 UTC (807 KB)
    [v2] Tue, 18 Jun 2024 04:20:51 UTC (5,239 KB)
    Tao Tang, Dafeng Wei, Zhengyu Jia, Tian Gao, Changwei Cai, Chengkai Hou, Peng Jia, Kun Zhan, Haiyang Sun, Jingchen Fan, Yixing Zhao, Fu Liu, Xiaodan Liang, Xianpeng Lang, Yang Wang

The rapid development of the autonomous driving industry has led to a significant accumulation of autonomous driving data. Consequently, there comes a growing demand for retrieving data to provide specialized optimization. However, directly applying previous image retrieval methods faces several challenges, such as the lack of global feature representation and inadequate text retrieval ability for complex driving scenes. To address these issues, firstly, we propose the BEV-TSR framework which leverages descriptive text as an input to retrieve corresponding scenes in the Bird's Eye View (BEV) space. Then to facilitate complex scene retrieval with extensive text descriptions, we employ a large language model (LLM) to extract the semantic features of the text inputs and incorporate knowledge graph embeddings to enhance the semantic richness of the language embedding. To achieve feature alignment between the BEV feature and language embedding, we propose Shared Cross-modal Embedding with a set of shared learnable embeddings to bridge the gap between these two modalities, and employ a caption generation task to further enhance the alignment. Furthermore, there lack of well-formed retrieval datasets for effective evaluation. To this end, we establish a multi-level retrieval dataset, nuScenes-Retrieval, based on the widely adopted nuScenes dataset. Experimental results on the multi-level nuScenes-Retrieval show that BEV-TSR achieves state-of-the-art performance, e.g., 85.78% and 87.66% top-1 accuracy on scene-to-text and text-to-scene retrieval respectively. Codes and datasets will be available.

------------

----------
Agent (11)
----------

`[2406.12276] CodeNav: Beyond tool-use to using real-world codebases with LLM agents <https://arxiv.org/abs/2406.12276>`__ CodeNav:超越工具使用，使用LLM代理使用真实世界的代码库

::

    Tue, 18 Jun 2024 05:10:38 GMT
    Tanmay Gupta, Luca Weihs, Aniruddha Kembhavi

We present CodeNav, an LLM agent that navigates and leverages previously unseen code repositories to solve user queries. In contrast to tool-use LLM agents that require ``registration'' of all relevant tools via manual descriptions within the LLM context, CodeNav automatically indexes and searches over code blocks in the target codebase, finds relevant code snippets, imports them, and uses them to iteratively generate a solution with execution feedback.
To highlight the core-capabilities of CodeNav, we first showcase three case studies where we use CodeNav for solving complex user queries using three diverse codebases. Next, on three benchmarks, we quantitatively compare the effectiveness of code-use (which only has access to the target codebase) to tool-use (which has privileged access to all tool names and descriptions).
Finally, we study the effect of varying kinds of tool and library descriptions on code-use performance, as well as investigate the advantage of the agent seeing source code as opposed to natural descriptions of code. All code will be made open source under a permissive license.

------------

`[2406.12216] Is persona enough for personality? Using ChatGPT to reconstruct an agent's latent personality from simple descriptions <https://arxiv.org/abs/2406.12216>`__ 人格是否足够人格?利用ChatGPT从简单的描述中重构agent的潜在人格

::

    Tue, 18 Jun 2024 02:32:57 GMT
    Yongyi Ji, Zhisheng Tang, Mayank Kejriwal

Personality, a fundamental aspect of human cognition, contains a range of traits that influence behaviors, thoughts, and emotions. This paper explores the capabilities of large language models (LLMs) in reconstructing these complex cognitive attributes based only on simple descriptions containing socio-demographic and personality type information. Utilizing the HEXACO personality framework, our study examines the consistency of LLMs in recovering and predicting underlying (latent) personality dimensions from simple descriptions. Our experiments reveal a significant degree of consistency in personality reconstruction, although some inconsistencies and biases, such as a tendency to default to positive traits in the absence of explicit information, are also observed. Additionally, socio-demographic factors like age and number of children were found to influence the reconstructed personality dimensions.
These findings have implications for building sophisticated agent-based simulacra using LLMs and highlight the need for further research on robust personality generation in LLMs.

------------

`[2406.12639] Ask-before-Plan: Proactive Language Agents for Real-World Planning <https://arxiv.org/abs/2406.12639>`__ 计划前问:面向现实世界规划的主动语言代理

::

    Tue, 18 Jun 2024 14:07:28 GMT
    Xuan Zhang, Yang Deng, Zifeng Ren, See-Kiong Ng, Tat-Seng Chua

The evolution of large language models (LLMs) has enhanced the planning capabilities of language agents in diverse real-world scenarios. Despite these advancements, the potential of LLM-powered agents to comprehend ambiguous user instructions for reasoning and decision-making is still under exploration. In this work, we introduce a new task, Proactive Agent Planning, which requires language agents to predict clarification needs based on user-agent conversation and agent-environment interaction, invoke external tools to collect valid information, and generate a plan to fulfill the user's demands. To study this practical problem, we establish a new benchmark dataset, Ask-before-Plan. To tackle the deficiency of LLMs in proactive planning, we propose a novel multi-agent framework, Clarification-Execution-Planning (\texttt{CEP}), which consists of three agents specialized in clarification, execution, and planning.
We introduce the trajectory tuning scheme for the clarification agent and static execution agent, as well as the memory recollection mechanism for the dynamic execution agent. Extensive evaluations and comprehensive analyses conducted on the Ask-before-Plan dataset validate the effectiveness of our proposed framework.

------------

`[2406.12707] Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction <https://arxiv.org/abs/2406.12707>`__ 与类人智能体对话:通过可感知的声音接收和反应进行共情对话

::

    Tue, 18 Jun 2024 15:19:51 GMT
    Haoqiu Yan, Yongxin Zhu, Kai Zheng, Bing Liu, Haoyu Cao, Deqiang Jiang and Linli Xu

Large Language Model (LLM)-enhanced agents become increasingly prevalent in Human-AI communication, offering vast potential from entertainment to professional domains. However, current multi-modal dialogue systems overlook the acoustic information present in speech, which is crucial for understanding human communication nuances. This oversight can lead to misinterpretations of speakers' intentions, resulting in inconsistent or even contradictory responses within dialogues. To bridge this gap, in this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system designed to discern deeper or more subtle meanings beyond the literal interpretations of words through the integration of speech modality perception. Employing LLMs as a cognitive core, PerceptiveAgent perceives acoustic information from input speech and generates empathetic responses based on speaking styles described in natural language. Experimental results indicate that PerceptiveAgent excels in contextual understanding by accurately discerning the speakers' true intentions in scenarios where the linguistic meaning is either contrary to or inconsistent with the speaker's true feelings, producing more nuanced and expressive spoken dialogues. Code is publicly available at: \url{https://github.com/Haoqiu-Yan/PerceptiveAgent}.

------------

`[2406.12708] AgentReview: Exploring Peer Review Dynamics with LLM Agents <https://arxiv.org/abs/2406.12708>`__ 

::

    Tue, 18 Jun 2024 15:22:12 GMT
    Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, Jindong Wang

Peer review is fundamental to the integrity and advancement of scientific publication. Traditional methods of peer review analyses often rely on exploration and statistics of existing peer review data, which do not adequately address the multivariate nature of the process, account for the latent variables, and are further constrained by privacy concerns due to the sensitive nature of the data. We introduce AgentReview, the first large language model (LLM) based peer review simulation framework, which effectively disentangles the impacts of multiple latent factors and addresses the privacy issue. Our study reveals significant insights, including a notable 37.1% variation in paper decisions due to reviewers' biases, supported by sociological theories such as the social influence theory, altruism fatigue, and authority bias. We believe that this study could offer valuable insights to improve the design of peer review mechanisms.

------------

`[2406.12806] Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents <https://arxiv.org/abs/2406.12806>`__ 通过LLM代理的代码分析识别软件系统中的性能敏感配置

::

    Tue, 18 Jun 2024 17:22:48 GMT
    Zehao Wang, Dong Jae Kim, Tse-Hsun Chen

Configuration settings are essential for tailoring software behavior to meet specific performance requirements. However, incorrect configurations are widespread, and identifying those that impact system performance is challenging due to the vast number and complexity of possible settings. In this work, we present PerfSense, a lightweight framework that leverages Large Language Models (LLMs) to efficiently identify performance-sensitive configurations with minimal overhead. PerfSense employs LLM agents to simulate interactions between developers and performance engineers using advanced prompting techniques such as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of seven open-source Java systems demonstrates that PerfSense achieves an average accuracy of 64.77% in classifying performance-sensitive configurations, outperforming both our LLM baseline (50.36%) and the previous state-of-the-art method (61.75%). Notably, our prompt chaining technique improves recall by 10% to 30% while maintaining similar precision levels. Additionally, a manual analysis of 362 misclassifications reveals common issues, including LLMs' misunderstandings of requirements (26.8%). In summary, PerfSense significantly reduces manual effort in classifying performance-sensitive configurations and offers valuable insights for future LLM-based code analysis research.

------------

`[2406.00252] Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey <https://arxiv.org/abs/2406.00252>`__ 多模态和多智能体系统满足理性:综述

::

    replaced with revised version Tue, 18 Jun 2024 04:22:39 GMT
    Submission history From: Bowen Jiang [view email]
    [v1] Sat, 1 Jun 2024 01:17:25 UTC (2,332 KB)
    [v2] Wed, 5 Jun 2024 19:39:56 UTC (2,332 KB)
    [v3] Tue, 18 Jun 2024 04:22:39 UTC (4,650 KB)
    Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Weijie J. Su, Camillo J. Taylor, Tanwi Mallick

Rationality is the quality of being guided by reason, characterized by logical thinking and decision-making that align with evidence and logical rules. This quality is essential for effective problem-solving, as it ensures that solutions are well-founded and systematically derived. Despite the advancements of large language models (LLMs) in generating human-like text with remarkable accuracy, they present biases inherited from the training data, inconsistency across different contexts, and difficulty understanding complex scenarios involving multiple layers of context. Therefore, recent research attempts to leverage the strength of multiple agents working collaboratively with various types of data and tools for enhanced consistency and reliability. To that end, this paper aims to understand whether multi-modal and multi-agent systems are advancing toward rationality by surveying the state-of-the-art works, identifying advancements over single-agent and single-modal systems in terms of rationality, and discussing open problems and future directions. We maintain an open repository at this https URL.

------------

`[2311.09835] ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code <https://arxiv.org/abs/2311.09835>`__ ML-Bench:在库级代码上评估用于机器学习任务的大型语言模型和代理

::

    replaced with revised version Tue, 18 Jun 2024 12:49:41 GMT
    Submission history From: Xiangru Tang [view email]
    [v1] Thu, 16 Nov 2023 12:03:21 UTC (21,189 KB)
    [v2] Wed, 17 Apr 2024 17:13:03 UTC (12,439 KB)
    [v3] Wed, 12 Jun 2024 10:31:57 UTC (6,632 KB)
    [v4] Tue, 18 Jun 2024 12:49:41 UTC (6,750 KB)
    Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Liang Chen, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yin Fang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, Mark Gerstein

Despite Large Language Models (LLMs) like GPT-4 achieving impressive results in function-level code generation, they struggle with repository-scale code understanding (e.g., coming up with the right arguments for calling routines), requiring a deeper comprehension of complex file interactions. Also, recently, people have developed LLM agents that attempt to interact with repository code (e.g., compiling and evaluating its execution), prompting the need to evaluate their performance. These gaps have motivated our development of ML-Bench, a benchmark rooted in real-world programming applications that leverage existing code repositories to perform tasks. Addressing the need for LLMs to interpret long code contexts and translate instructions into precise, executable scripts, ML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories, challenging LLMs to accommodate user-specified arguments and documentation intricacies effectively. To evaluate both LLMs and AI agents, two setups are employed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a predefined deployment environment, and ML-Agent-Bench for testing autonomous agents in an end-to-end task execution within a Linux sandbox environment. Our findings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%, there remains significant scope for improvement, highlighted by issues such as hallucinated outputs and difficulties with bash script generation. Notably, in the more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate, reflecting the efficacy of iterative action and feedback in complex task resolution. Our code, dataset, and models are available at this https URL.

------------

`[2402.01737] Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues <https://arxiv.org/abs/2402.01737>`__ 面向社会感知协商对话的辅助大型语言模型智能体

::

    replaced with revised version Tue, 18 Jun 2024 13:10:16 GMT
    Submission history From: Yuncheng Hua [view email]
    [v1] Mon, 29 Jan 2024 09:07:40 UTC (5,409 KB)
    [v2] Tue, 18 Jun 2024 13:10:16 UTC (3,930 KB)
    Yuncheng Hua, Lizhen Qu, Gholamreza Haffari

We develop assistive agents based on Large Language Models (LLMs) that aid interlocutors in business negotiations. Specifically, we simulate business negotiations by letting two LLM-based agents engage in role play. A third LLM acts as a remediator agent to rewrite utterances violating norms for improving negotiation outcomes. We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL exemplars for the remediator, where we propose a novel select criteria, called value impact, to measure the quality of the negotiation outcomes. We provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different negotiation topics. The source code and the generated dataset will be publicly available upon acceptance.

------------

`[2402.00798] Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents <https://arxiv.org/abs/2402.00798>`__ 形式化- llm:集成形式化语言和自然语言的可控基于llm的智能体

::

    replaced with revised version Tue, 18 Jun 2024 05:44:03 GMT
    Submission history From: Zelong Li [view email]
    [v1] Thu, 1 Feb 2024 17:30:50 UTC (898 KB)
    [v2] Sun, 4 Feb 2024 22:16:48 UTC (898 KB)
    [v3] Tue, 18 Jun 2024 05:44:03 UTC (903 KB)
    Zelong Li, Wenyue Hua, Hao Wang, He Zhu, Yongfeng Zhang

Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans. Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential. The work is open-sourced at this https URL.

------------

`[2406.11200] AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval <https://arxiv.org/abs/2406.11200>`__ AvaTaR:优化LLM agent的工具辅助知识检索

::

    replaced with revised version Tue, 18 Jun 2024 01:39:57 GMT
    Submission history From: Shirley Wu [view email]
    [v1] Mon, 17 Jun 2024 04:20:02 UTC (13,925 KB)
    [v2] Tue, 18 Jun 2024 01:39:57 UTC (13,925 KB)
    Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N. Ioannidis, Karthik Subbian, Jure Leskovec, James Zou

Large language model (LLM) agents have demonstrated impressive capability in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing the prompting techniques that make LLM agents able to effectively use external tools and knowledge is a heuristic and laborious task. Here, we introduce AvaTaR, a novel and automatic framework that optimizes an LLM agent to effectively use the provided tools and improve its performance on a given task/domain. During optimization, we design a comparator module to iteratively provide insightful and holistic prompts to the LLM agent via reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information. We find AvaTaR consistently outperforms state-of-the-art approaches across all four challenging tasks and exhibits strong generalization ability when applied to novel cases, achieving an average relative improvement of 14% on the Hit@1 metric. Code and dataset are available at this https URL.

------------

-----------
Other (149)
-----------

`[2406.11871] Generative AI Voting: Fair Collective Choice is Resilient to LLM Biases and Inconsistencies <https://arxiv.org/abs/2406.11871>`__ 生成式AI投票:公平的集体选择对LLM偏见和不一致有弹性

::

    Fri, 31 May 2024 01:41:48 GMT
    Srijoni Majumdar, Edith Elkind, Evangelos Pournaras

Scaling up deliberative and voting participation is a longstanding endeavor -- a cornerstone for direct democracy and legitimate collective choice. Recent breakthroughs in generative artificial intelligence (AI) and large language models (LLMs) provide unprecedented opportunities, but also alerting risks for digital democracy. AI personal assistants can overcome cognitive bandwidth limitations of humans, providing decision support capabilities or even direct AI representation of human voters at large scale. However, the quality of this representation and what underlying biases manifest when delegating collective decision making to LLMs is an alarming and timely challenge to tackle. By rigorously emulating with high realism more than >50K LLM voting personas in 81 real-world voting elections, we show that different LLMs (GPT 3, GPT 3.5, and Llama2) come with biases and significant inconsistencies in complex preferential ballot formats, compared to simpler and more consistent majoritarian elections. Strikingly, fair voting aggregation methods, such as equal shares, prove to be a win-win: fairer voting outcomes for humans with fairer AI representation. This novel underlying relationship proves paramount for democratic resilience in progressives scenarios with low voters turnout and voter fatigue supported by AI representatives: abstained voters are mitigated by recovering highly representative voting outcomes that are fairer. These insights provide remarkable foundations for science, policymakers and citizens in explaining and mitigating AI risks in democratic innovations.

------------

`[2406.11875] ChatPCG: Large Language Model-Driven Reward Design for Procedural Content Generation <https://arxiv.org/abs/2406.11875>`__ ChatPCG:面向程序内容生成的大型语言模型驱动奖励设计

::

    Fri, 7 Jun 2024 08:18:42 GMT
    In-Chang Baek, Tae-Hwa Park, Jin-Ha Noh, Cheong-Mok Bae, Kyung-Joong Kim

Driven by the rapid growth of machine learning, recent advances in game artificial intelligence (AI) have significantly impacted productivity across various gaming genres. Reward design plays a pivotal role in training game AI models, wherein researchers implement concepts of specific reward functions.
However, despite the presence of AI, the reward design process predominantly remains in the domain of human experts, as it is heavily reliant on their creativity and engineering skills. Therefore, this paper proposes ChatPCG, a large language model (LLM)-driven reward design framework.It leverages human-level insights, coupled with game expertise, to generate rewards tailored to specific game features automatically. Moreover, ChatPCG is integrated with deep reinforcement learning, demonstrating its potential for multiplayer game content generation tasks. The results suggest that the proposed LLM exhibits the capability to comprehend game mechanics and content generation tasks, enabling tailored content generation for a specified game. This study not only highlights the potential for improving accessibility in content generation but also aims to streamline the game AI development process.

------------

`[2406.11911] A Notion of Complexity for Theory of Mind via Discrete World Models <https://arxiv.org/abs/2406.11911>`__ 基于离散世界模型的心智理论复杂性概念

::

    Sun, 16 Jun 2024 16:46:55 GMT
    X. Angelo Huang, Emanuele La Malfa, Samuele Marro, Andrea Asperti, Anthony Cohn, Michael Wooldridge

Theory of Mind (ToM) can be used to assess the capabilities of Large Language Models (LLMs) in complex scenarios where social reasoning is required. While the research community has proposed many ToM benchmarks, their hardness varies greatly, and their complexity is not well defined. This work proposes a framework to measure the complexity of ToM tasks. We quantify a problem's complexity as the number of states necessary to solve it correctly. Our complexity measure also accounts for spurious states of a ToM problem designed to make it apparently harder. We use our method to assess the complexity of five widely adopted ToM benchmarks. On top of this framework, we design a prompting technique that augments the information available to a model with a description of how the environment changes with the agents' interactions. We name this technique Discrete World Models (DWM) and show how it elicits superior performance on ToM tasks.

------------

`[2406.11938] Tracking the perspectives of interacting language models <https://arxiv.org/abs/2406.11938>`__ 交互式语言模型的视角跟踪

::

    Mon, 17 Jun 2024 17:20:16 GMT
    Hayden Helm and Brandon Duderstadt and Youngser Park and Carey E. Priebe

Large language models (LLMs) are capable of producing high quality information at unprecedented rates. As these models continue to entrench themselves in society, the content they produce will become increasingly pervasive in databases that are, in turn, incorporated into the pre-training data, fine-tuning data, retrieval data, etc. of other language models. In this paper we formalize the idea of a communication network of LLMs and introduce a method for representing the perspective of individual models within a collection of LLMs. Given these tools we systematically study information diffusion in the communication network of LLMs in various simulated settings.

------------

`[2406.11980] Prompt Design Matters for Computational Social Science Tasks but in Unpredictable Ways <https://arxiv.org/abs/2406.11980>`__ 提示设计对于计算社会科学任务很重要，但是以不可预测的方式

::

    Mon, 17 Jun 2024 18:01:43 GMT
    Shubham Atreja, Joshua Ashkinaze, Lingyao Li, Julia Mendelsohn, Libby Hemphill

Manually annotating data for computational social science tasks can be costly, time-consuming, and emotionally draining. While recent work suggests that LLMs can perform such annotation tasks in zero-shot settings, little is known about how prompt design impacts LLMs' compliance and accuracy. We conduct a large-scale multi-prompt experiment to test how model selection (ChatGPT, PaLM2, and Falcon7b) and prompt design features (definition inclusion, output type, explanation, and prompt length) impact the compliance and accuracy of LLM-generated annotations on four CSS tasks (toxicity, sentiment, rumor stance, and news frames). Our results show that LLM compliance and accuracy are highly prompt-dependent. For instance, prompting for numerical scores instead of labels reduces all LLMs' compliance and accuracy. The overall best prompting setup is task-dependent, and minor prompt changes can cause large changes in the distribution of generated labels. By showing that prompt design significantly impacts the quality and distribution of LLM-generated annotations, this work serves as both a warning and practical guide for researchers and practitioners.

------------

`[2406.12000] Look Further Ahead: Testing the Limits of GPT-4 in Path Planning <https://arxiv.org/abs/2406.12000>`__ 进一步展望:测试GPT-4在路径规划中的极限

::

    Mon, 17 Jun 2024 18:12:56 GMT
    Mohamed Aghzal, Erion Plaku, Ziyu Yao

Large Language Models (LLMs) have shown impressive capabilities across a wide variety of tasks. However, they still face challenges with long-horizon planning. To study this, we propose path planning tasks as a platform to evaluate LLMs' ability to navigate long trajectories under geometric constraints. Our proposed benchmark systematically tests path-planning skills in complex settings. Using this, we examined GPT-4's planning abilities using various task representations and prompting approaches. We found that framing prompts as Python code and decomposing long trajectory tasks improve GPT-4's path planning effectiveness. However, while these approaches show some promise toward improving the planning ability of the model, they do not obtain optimal paths and fail at generalizing over extended horizons.

------------

`[2406.12043] Grade Score: Quantifying LLM Performance in Option Selection <https://arxiv.org/abs/2406.12043>`__ 

::

    Mon, 17 Jun 2024 19:29:39 GMT
    Dmitri Iourovitski

This study introduces the "Grade Score", a novel metric designed to evaluate the consistency and fairness of Large Language Models (LLMs) when used as multiple-choice judges with respect to order bias and choice consistency. The Grade Score combines Entropy, which measures order bias, and Mode Frequency, which assesses choice stability, offering insights into LLMs' reliability and impartiality. The study explores techniques such as prompt engineering and option sampling strategies to optimize the Grade Score, demonstrating their effectiveness in enhancing LLMs' performance. Results showcase varying performances among LLMs with respect to prompts and highlight the positive impact of including irrelevant options. The study also identifies an emergent behavior in instruction-following models, where they adapt to instructions targeting specific biases, demonstrating their adaptability. The Grade Score facilitates comparisons between LLMs and encourages ongoing research towards optimizing their decision-making processes, with potential implications for improving their reliability and fairness in various applications. All code is available on GitHub https://github.com/IoDmitri/GradeLab

------------

`[2406.12058] WellDunn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions <https://arxiv.org/abs/2406.12058>`__ WellDunn:关于语言模型和大型语言模型在识别健康维度方面的鲁棒性和可解释性

::

    Mon, 17 Jun 2024 19:50:40 GMT
    Seyedali Mohammadi (1), Edward Raff (1 and 2), Jinendra Malekar (3), Vedant Palit (4), Francis Ferraro (1), and Manas Gaur (1) ((1) University of Maryland, Baltimore County, (2) Booz Allen Hamilton, (3) University of South Carolina, (4) Indian Institute of Technology, Kharagpur)

Language Models (LMs) are being proposed for mental health applications where the heightened risk of adverse outcomes means predictive performance may not be a sufficient litmus test of a model's utility in clinical practice. A model that can be trusted for practice should have a correspondence between explanation and clinical determination, yet no prior research has examined the attention fidelity of these models and their effect on ground truth explanations. We introduce an evaluation design that focuses on the robustness and explainability of LMs in identifying Wellness Dimensions (WD). We focus on two mental health and well-being datasets: (a) Multi-label Classification-based MultiWD, and (b) WellXplain for evaluating attention mechanism veracity against expert-labeled explanations. The labels are based on Halbert Dunn's theory of wellness, which gives grounding to our evaluation. We reveal four surprising results about LMs/LLMs: (1) Despite their human-like capabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM fails to deliver any remarkable improvements in performance or explanations. (2) Re-examining LMs' predictions based on a confidence-oriented loss function reveals a significant performance drop. (3) Across all LMs/LLMs, the alignment between attention and explanations remains low, with LLMs scoring a dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific knowledge and undervalue explanations, causing these discrepancies. This study highlights the need for further research into their consistency and explanations in mental health and well-being.

------------

`[2406.12146] Should AI Optimize Your Code? A Comparative Study of Current Large Language Models Versus Classical Optimizing Compilers <https://arxiv.org/abs/2406.12146>`__ AI应该优化你的代码吗?当前大型语言模型与经典优化编译器的比较研究

::

    Mon, 17 Jun 2024 23:26:41 GMT
    Miguel Romero Rosas, Miguel Torres Sanchez, Rudolf Eigenmann

In the contemporary landscape of computer architecture, the demand for efficient parallel programming persists, needing robust optimization techniques. Traditional optimizing compilers have historically been pivotal in this endeavor, adapting to the evolving complexities of modern software systems. The emergence of Large Language Models (LLMs) raises intriguing questions about the potential for AI-driven approaches to revolutionize code optimization methodologies.
This paper presents a comparative analysis between two state-of-the-art Large Language Models, GPT-4.0 and CodeLlama-70B, and traditional optimizing compilers, assessing their respective abilities and limitations in optimizing code for maximum efficiency. Additionally, we introduce a benchmark suite of challenging optimization patterns and an automatic mechanism for evaluating performance and correctness of the code generated by such tools. We used two different prompting methodologies to assess the performance of the LLMs -- Chain of Thought (CoT) and Instruction Prompting (IP). We then compared these results with three traditional optimizing compilers, CETUS, PLUTO and ROSE, across a range of real-world use cases.
A key finding is that while LLMs have the potential to outperform current optimizing compilers, they often generate incorrect code on large code sizes, calling for automated verification methods. Our extensive evaluation across 3 different benchmarks suites shows CodeLlama-70B as the superior optimizer among the two LLMs, capable of achieving speedups of up to 2.1x. Additionally, CETUS is the best among the optimizing compilers, achieving a maximum speedup of 1.9x. We also found no significant difference between the two prompting methods: Chain of Thought (Cot) and Instructing prompting (IP).

------------

`[2406.12172] Navigating the Labyrinth: Evaluating and Enhancing LLMs' Ability to Reason About Search Problems <https://arxiv.org/abs/2406.12172>`__ 迷宫导航:评估和增强llm对搜索问题的推理能力

::

    Tue, 18 Jun 2024 00:44:58 GMT
    Nasim Borazjanizadeh, Roei Herzig, Trevor Darrell, Rogerio Feris, Leonid Karlinsky

Recently, Large Language Models (LLMs) attained impressive performance in math and reasoning benchmarks. However, they still often struggle with logic problems and puzzles that are relatively easy for humans. To further investigate this, we introduce a new benchmark, SearchBench, containing 11 unique search problem types, each equipped with automated pipelines to generate an arbitrary number of instances and analyze the feasibility, correctness, and optimality of LLM-generated solutions. We show that even the most advanced LLMs fail to solve these problems end-to-end in text, e.g. GPT4 solves only 1.4%.
SearchBench problems require considering multiple pathways to the solution as well as backtracking, posing a significant challenge to auto-regressive models.
Instructing LLMs to generate code that solves the problem helps, but only slightly, e.g., GPT4's performance rises to 11.7%. In this work, we show that in-context learning with A* algorithm implementations enhances performance. The full potential of this promoting approach emerges when combined with our proposed Multi-Stage-Multi-Try method, which breaks down the algorithm implementation into two stages and verifies the first stage against unit tests, raising GPT-4's performance above 57%.

------------

`[2406.12203] InterIntent: Investigating Social Intelligence of LLMs via Intention Understanding in an Interactive Game Context <https://arxiv.org/abs/2406.12203>`__ InterIntent:在互动游戏情境中通过意图理解来研究llm的社会智能

::

    Tue, 18 Jun 2024 02:02:15 GMT
    Ziyi Liu, Abhishek Anand, Pei Zhou, Jen-tse Huang, Jieyu Zhao

Large language models (LLMs) have demonstrated the potential to mimic human social intelligence. However, most studies focus on simplistic and static self-report or performance-based tests, which limits the depth and validity of the analysis. In this paper, we developed a novel framework, InterIntent, to assess LLMs' social intelligence by mapping their ability to understand and manage intentions in a game setting. We focus on four dimensions of social intelligence: situational awareness, self-regulation, self-awareness, and theory of mind. Each dimension is linked to a specific game task: intention selection, intention following, intention summarization, and intention guessing. Our findings indicate that while LLMs exhibit high proficiency in selecting intentions, achieving an accuracy of 88\%, their ability to infer the intentions of others is significantly weaker, trailing human performance by 20\%. Additionally, game performance correlates with intention understanding, highlighting the importance of the four components towards success in this game. These findings underline the crucial role of intention understanding in evaluating LLMs' social intelligence and highlight the potential of using social deduction games as a complex testbed to enhance LLM evaluation.
InterIntent contributes a structured approach to bridging the evaluation gap in social intelligence within multiplayer games.

------------

`[2406.12227] Interpretable Catastrophic Forgetting of Large Language Model Fine-tuning via Instruction Vector <https://arxiv.org/abs/2406.12227>`__ 基于指令向量微调的大型语言模型可解释灾难性遗忘

::

    Tue, 18 Jun 2024 03:05:08 GMT
    Gangwei Jiang, Zhaoyi Li, Caigao Jiang, Siqiao Xue, Jun Zhou, Linqi Song, Defu Lian, Ying Wei

Fine-tuning large language models (LLMs) can cause them to lose their general capabilities. However, the intrinsic mechanisms behind such forgetting remain unexplored. In this paper, we begin by examining this phenomenon by focusing on knowledge understanding and instruction following, with the latter identified as the main contributor to forgetting during fine-tuning. Consequently, we propose the Instruction Vector (IV) framework to capture model representations highly related to specific instruction-following capabilities, thereby making it possible to understand model-intrinsic forgetting. Through the analysis of IV dynamics pre and post-training, we suggest that fine-tuning mostly adds specialized reasoning patterns instead of erasing previous skills, which may appear as forgetting. Building on this insight, we develop IV-guided training, which aims to preserve original computation graph, thereby mitigating catastrophic forgetting. Empirical tests on three benchmarks confirm the efficacy of this new approach, supporting the relationship between IVs and forgetting. Our code will be made available soon.

------------

`[2406.12232] "You Gotta be a Doctor, Lin": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations <https://arxiv.org/abs/2406.12232>`__ 

::

    Tue, 18 Jun 2024 03:11:43 GMT
    Huy Nghiem, John Prindle, Jieyu Zhao, Hal Daum\'e III

Social science research has shown that candidates with names indicative of certain races or genders often face discrimination in employment practices.
Similarly, Large Language Models (LLMs) have demonstrated racial and gender biases in various applications. In this study, we utilize GPT-3.5-Turbo and Llama 3-70B-Instruct to simulate hiring decisions and salary recommendations for candidates with 320 first names that strongly signal their race and gender, across over 750,000 prompts. Our empirical results indicate a preference among these models for hiring candidates with White female-sounding names over other demographic groups across 40 occupations. Additionally, even among candidates with identical qualifications, salary recommendations vary by as much as 5% between different subgroups. A comparison with real-world labor data reveals inconsistent alignment with U.S. labor market characteristics, underscoring the necessity of risk investigation of LLM-powered systems.

------------

`[2406.12257] CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models <https://arxiv.org/abs/2406.12257>`__ CleanGen:减轻大型语言模型生成任务的后门攻击

::

    Tue, 18 Jun 2024 04:10:38 GMT
    Yuetai Li, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Dinuka Sahabandu, Bhaskar Ramasubramanian, Radha Poovendran

The remarkable performance of large language models (LLMs) in generation tasks has enabled practitioners to leverage publicly available models to power custom applications, such as chatbots and virtual assistants. However, the data used to train or fine-tune these LLMs is often undisclosed, allowing an attacker to compromise the data and inject backdoors into the models. In this paper, we develop a novel inference time defense, named CleanGen, to mitigate backdoor attacks for generation tasks in LLMs. CleanGenis a lightweight and effective decoding strategy that is compatible with the state-of-the-art (SOTA) LLMs. Our insight behind CleanGen is that compared to other LLMs, backdoored LLMs assign significantly higher probabilities to tokens representing the attacker-desired contents. These discrepancies in token probabilities enable CleanGen to identify suspicious tokens favored by the attacker and replace them with tokens generated by another LLM that is not compromised by the same attacker, thereby avoiding generation of attacker-desired content. We evaluate CleanGen against five SOTA backdoor attacks. Our results show that CleanGen achieves lower attack success rates (ASR) compared to five SOTA baseline defenses for all five backdoor attacks. Moreover, LLMs deploying CleanGen maintain helpfulness in their responses when serving benign user queries with minimal added computational overhead.

------------

`[2406.12259] Adversarial Attacks on Large Language Models in Medicine <https://arxiv.org/abs/2406.12259>`__ 医学大型语言模型的对抗性攻击

::

    Tue, 18 Jun 2024 04:24:30 GMT
    Yifan Yang, Qiao Jin, Furong Huang, Zhiyong Lu

The integration of Large Language Models (LLMs) into healthcare applications offers promising advancements in medical diagnostics, treatment recommendations, and patient care. However, the susceptibility of LLMs to adversarial attacks poses a significant threat, potentially leading to harmful outcomes in delicate medical contexts. This study investigates the vulnerability of LLMs to two types of adversarial attacks in three medical tasks. Utilizing real-world patient data, we demonstrate that both open-source and proprietary LLMs are susceptible to manipulation across multiple tasks.
This research further reveals that domain-specific tasks demand more adversarial data in model fine-tuning than general domain tasks for effective attack execution, especially for more capable models. We discover that while integrating adversarial data does not markedly degrade overall model performance on medical benchmarks, it does lead to noticeable shifts in fine-tuned model weights, suggesting a potential pathway for detecting and countering model attacks. This research highlights the urgent need for robust security measures and the development of defensive mechanisms to safeguard LLMs in medical applications, to ensure their safe and effective deployment in healthcare settings.

------------

`[2406.12374] Problem-Solving in Language Model Networks <https://arxiv.org/abs/2406.12374>`__ 语言模型网络中的问题解决

::

    Tue, 18 Jun 2024 07:59:14 GMT
    Ciaran Regan, Alexandre Gournail, Mizuki Oka

To improve the reasoning and question-answering capabilities of Large Language Models (LLMs), several multi-agent approaches have been introduced.
While these methods enhance performance, the application of collective intelligence-based approaches to complex network structures and the dynamics of agent interactions remain underexplored. This work extends the concept of multi-agent debate to more general network topologies, measuring the question-answering accuracy, influence, consensus, and the effects of bias on the collective. The results show that random networks perform similarly to fully connected networks despite using significantly fewer tokens. Furthermore, a strong consensus among agents in correlates with correct answers, whereas divided responses typically indicate incorrect answers. Analysing the influence of the agents reveals a balance between self-reflection and interconnectedness; self-reflection aids when local interactions are incorrect, and local interactions aid when the agent itself is incorrect. Additionally, bias plays a strong role in system performance with correctly biased hub nodes boosting performance. These insights suggest that using random networks or scale-free networks with knowledgeable agents placed in central positions can enhance the overall performance of multi-agent systems.

------------

`[2406.12670] Stealth edits for provably fixing or attacking large language models <https://arxiv.org/abs/2406.12670>`__ 用于修复或攻击大型语言模型的隐形编辑

::

    Tue, 18 Jun 2024 14:43:18 GMT
    Oliver J. Sutton, Qinghua Zhou, Wei Wang, Desmond J. Higham, Alexander N. Gorban, Alexander Bastounis, Ivan Y. Tyukin

We reveal new methods and the theoretical foundations of techniques for editing large language models. We also show how the new theory can be used to assess the editability of models and to expose their susceptibility to previously unknown malicious attacks. Our theoretical approach shows that a single metric (a specific measure of the intrinsic dimensionality of the model's features) is fundamental to predicting the success of popular editing approaches, and reveals new bridges between disparate families of editing methods. We collectively refer to these approaches as stealth editing methods, because they aim to directly and inexpensively update a model's weights to correct the model's responses to known hallucinating prompts without otherwise affecting the model's behaviour, without requiring retraining. By carefully applying the insight gleaned from our theoretical investigation, we are able to introduce a new network block -- named a jet-pack block -- which is optimised for highly selective model editing, uses only standard network operations, and can be inserted into existing networks. The intrinsic dimensionality metric also determines the vulnerability of a language model to a stealth attack: a small change to a model's weights which changes its response to a single attacker-chosen prompt. Stealth attacks do not require access to or knowledge of the model's training data, therefore representing a potent yet previously unrecognised threat to redistributed foundation models. They are computationally simple enough to be implemented in malware in many cases.
Extensive experimental results illustrate and support the method and its theoretical underpinnings. Demos and source code for editing language models are available at https://github.com/qinghua-zhou/stealth-edits.

------------

`[2406.12018] CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling <https://arxiv.org/abs/2406.12018>`__ CItruS:面向长序列建模的分块指令感知状态移除

::

    Mon, 17 Jun 2024 18:34:58 GMT
    Yu Bai, Xiyuan Zou, Heyan Huang, Sanxing Chen, Marc-Antoine Rondeau, Yang Gao, Jackie Chi Kit Cheung

Long sequence modeling has gained broad interest as large language models (LLMs) continue to advance. Recent research has identified that a large portion of hidden states within the key-value caches of Transformer models can be discarded (also termed evicted) without affecting the perplexity performance in generating long sequences. However, we show that these methods, despite preserving perplexity performance, often drop information that is important for solving downstream tasks, a problem which we call information neglect. To address this issue, we introduce Chunked Instruction-aware State Eviction (CItruS), a novel modeling technique that integrates the attention preferences useful for a downstream task into the eviction process of hidden states. In addition, we design a method for chunked sequence processing to further improve efficiency. Our training-free method exhibits superior performance on long sequence comprehension and retrieval tasks over several strong baselines under the same memory budget, while preserving language modeling perplexity.

------------

`[2406.12023] LiLiuM: eBay's Large Language Models for e-commerce <https://arxiv.org/abs/2406.12023>`__ LiLiuM: eBay电子商务大型语言模型

::

    Mon, 17 Jun 2024 18:45:41 GMT
    Christian Herold and Michael Kozielski and Leonid Ekimov and Pavel Petrushkov and Pierre-Yves Vandenbussche and Shahram Khadivi

We introduce the LiLiuM series of large language models (LLMs): 1B, 7B, and 13B parameter models developed 100% in-house to fit eBay's specific needs in the e-commerce domain. This gives eBay full control over all aspects of the models including license, data, vocabulary, and architecture. We expect these models to be used as a foundation for fine-tuning and instruction-tuning, eliminating dependencies to external models.
The LiLiuM LLMs have been trained on 3 trillion tokens of multilingual text from general and e-commerce domain. They perform similar to the popular LLaMA-2 models on English natural language understanding (NLU) benchmarks. At the same time, we outperform LLaMA-2 on non-English NLU tasks, machine translation and on e-commerce specific downstream tasks.
As part of our data mixture, we utilize the newly released RedPajama-V2 dataset for training and share our insights regarding data filtering and deduplication. We also discuss in detail how to serialize structured data for use in autoregressive language modeling. We provide insights on the effects of including code and parallel machine translation data in pre-training.
Furthermore, we develop our own tokenizer and model vocabulary, customized towards e-commerce. This way, we can achieve up to 34% speed-up in text generation on eBay-specific downstream tasks compared to LLaMA-2.
Finally, in relation to LLM pretraining, we show that checkpoint averaging can further improve over the best individual model checkpoint.

------------

`[2406.12033] Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models <https://arxiv.org/abs/2406.12033>`__ 用大型语言模型揭示和减轻心理健康分析中的偏见

::

    Mon, 17 Jun 2024 19:05:32 GMT
    Yuqing Wang, Yun Zhao, Sara Alessandra Keller, Anne de Hond, Marieke M. van Buchem, Malvika Pillai, Tina Hernandez-Boussard

The advancement of large language models (LLMs) has demonstrated strong capabilities across various applications, including mental health analysis.
However, existing studies have focused on predictive performance, leaving the critical issue of fairness underexplored, posing significant risks to vulnerable populations. Despite acknowledging potential biases, previous works have lacked thorough investigations into these biases and their impacts. To address this gap, we systematically evaluate biases across seven social factors (e.g., gender, age, religion) using ten LLMs with different prompting methods on eight diverse mental health datasets. Our results show that GPT-4 achieves the best overall balance in performance and fairness among LLMs, although it still lags behind domain-specific models like MentalRoBERTa in some cases.
Additionally, our tailored fairness-aware prompts can effectively mitigate bias in mental health predictions, highlighting the great potential for fair analysis in this field.

------------

`[2406.12034] Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts <https://arxiv.org/abs/2406.12034>`__ Self-MoE:基于自专业专家的组合式大型语言模型

::

    Mon, 17 Jun 2024 19:06:54 GMT
    Junmo Kang, Leonid Karlinsky, Hongyin Luo, Zhen Wang, Jacob Hansen, James Glass, David Cox, Rameswar Panda, Rogerio Feris, Alan Ritter

We present Self-MoE, an approach that transforms a monolithic LLM into a compositional, modular system of self-specialized experts, named MiXSE (MiXture of Self-specialized Experts). Our approach leverages self-specialization, which constructs expert modules using self-generated synthetic data, each equipped with a shared base LLM and incorporating self-optimized routing. This allows for dynamic and capability-specific handling of various target tasks, enhancing overall capabilities, without extensive human-labeled data and added parameters. Our empirical results reveal that specializing LLMs may exhibit potential trade-offs in performances on non-specialized tasks. On the other hand, our Self-MoE demonstrates substantial improvements over the base LLM across diverse benchmarks such as knowledge, reasoning, math, and coding. It also consistently outperforms other methods, including instance merging and weight merging, while offering better flexibility and interpretability by design with semantic experts and routing. Our findings highlight the critical role of modularity and the potential of self-improvement in achieving efficient, scalable, and adaptable systems.

------------

`[2406.12036] MedCalc-Bench: Evaluating Large Language Models for Medical Calculations <https://arxiv.org/abs/2406.12036>`__ 

::

    Mon, 17 Jun 2024 19:07:21 GMT
    Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina S Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad W Safranek, Abid A Anwar, Andrew Zhang, Aidan Gilson, Maxwell B Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, and Zhiyong Lu

As opposed to evaluating computation and logic-based reasoning, current bench2 marks for evaluating large language models (LLMs) in medicine are primarily focused on question-answering involving domain knowledge and descriptive rea4 soning. While such qualitative capabilities are vital to medical diagnosis, in real5 world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. To this end, we propose MedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical calculation capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks. Each instance in MedCalc-Bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. While our evaluation results show the potential of LLMs in this area, none of them are effective enough for clinical settings. Common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. We hope our study highlights the quantitative knowledge and reasoning gaps in LLMs within medical settings, encouraging future improvements of LLMs for various clinical calculation tasks.

------------

`[2406.12038] Soft Prompting for Unlearning in Large Language Models <https://arxiv.org/abs/2406.12038>`__ 大型语言模型中遗忘的软提示

::

    Mon, 17 Jun 2024 19:11:40 GMT
    Karuna Bhaila, Minh-Hao Van, Xintao Wu

The widespread popularity of Large Language Models (LLMs), partly due to their unique ability to perform in-context learning, has also brought to light the importance of ethical and safety considerations when deploying these pre-trained models. In this work, we focus on investigating machine unlearning for LLMs motivated by data protection regulations. In contrast to the growing literature on fine-tuning methods to achieve unlearning, we focus on a comparatively lightweight alternative called soft prompting to realize the unlearning of a subset of training data. With losses designed to enforce forgetting as well as utility preservation, our framework \textbf{S}oft \textbf{P}rompting for \textbf{U}n\textbf{l}earning (SPUL) learns prompt tokens that can be appended to an arbitrary query to induce unlearning of specific examples at inference time without updating LLM parameters. We conduct a rigorous evaluation of the proposed method and our results indicate that SPUL can significantly improve the trade-off between utility and forgetting in the context of text classification with LLMs. We further validate our method using multiple LLMs to highlight the scalability of our framework and provide detailed insights into the choice of hyperparameters and the influence of the size of unlearning data. Our implementation is available at \url{https://github.com/karuna-bhaila/llm_unlearning}.

------------

`[2406.12053] InternalInspector $I^2$: Robust Confidence Estimation in LLMs through Internal States <https://arxiv.org/abs/2406.12053>`__ InternalInspector $I^2$:通过内部状态对llm进行鲁棒置信度估计

::

    Mon, 17 Jun 2024 19:46:05 GMT
    Mohammad Beigi, Ying Shen, Runing Yang, Zihao Lin, Qifan Wang, Ankith Mohan, Jianfeng He, Ming Jin, Chang-Tien Lu, Lifu Huang

Despite their vast capabilities, Large Language Models (LLMs) often struggle with generating reliable outputs, frequently producing high-confidence inaccuracies known as hallucinations. Addressing this challenge, our research introduces InternalInspector, a novel framework designed to enhance confidence estimation in LLMs by leveraging contrastive learning on internal states including attention states, feed-forward states, and activation states of all layers. Unlike existing methods that primarily focus on the final activation state, InternalInspector conducts a comprehensive analysis across all internal states of every layer to accurately identify both correct and incorrect prediction processes. By benchmarking InternalInspector against existing confidence estimation methods across various natural language understanding and generation tasks, including factual question answering, commonsense reasoning, and reading comprehension, InternalInspector achieves significantly higher accuracy in aligning the estimated confidence scores with the correctness of the LLM's predictions and lower calibration error. Furthermore, InternalInspector excels at HaluEval, a hallucination detection benchmark, outperforming other internal-based confidence estimation methods in this task.

------------

`[2406.12069] Satyrn: A Platform for Analytics Augmented Generation <https://arxiv.org/abs/2406.12069>`__ Satyrn:一个分析增强生成平台

::

    Mon, 17 Jun 2024 20:14:16 GMT
    Marko Sterbentz, Cameron Barrie, Shubham Shahi, Abhratanu Dutta, Donna Hooshmand, Harper Pack, Kristian J. Hammond

Large language models (LLMs) are capable of producing documents, and retrieval augmented generation (RAG) has shown itself to be a powerful method for improving accuracy without sacrificing fluency. However, not all information can be retrieved from text. We propose an approach that uses the analysis of structured data to generate fact sets that are used to guide generation in much the same way that retrieved documents are used in RAG. This analytics augmented generation (AAG) approach supports the ability to utilize standard analytic techniques to generate facts that are then converted to text and passed to an LLM. We present a neurosymbolic platform, Satyrn that leverages AAG to produce accurate, fluent, and coherent reports grounded in large scale databases. In our experiments, we find that Satyrn generates reports in which over 86% accurate claims while maintaining high levels of fluency and coherence, even when using smaller language models such as Mistral-7B, as compared to GPT-4 Code Interpreter in which just 57% of claims are accurate.

------------

`[2406.12074] COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities <https://arxiv.org/abs/2406.12074>`__ COMMUNITY-CROSS-INSTRUCT:将大型语言模型与在线社区对齐的无监督指令生成

::

    Mon, 17 Jun 2024 20:20:47 GMT
    Zihao He, Rebecca Dorn, Siyi Guo, Minh Duc Chu, Kristina Lerman

Social scientists use surveys to probe the opinions and beliefs of populations, but these methods are slow, costly, and prone to biases. Recent advances in large language models (LLMs) enable creating computational representations or "digital twins" of populations that generate human-like responses mimicking the population's language, styles, and attitudes. We introduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs to online communities to elicit their beliefs. Given a corpus of a community's online discussions, Community-Cross-Instruct automatically generates instruction-output pairs by an advanced LLM to (1) finetune an foundational LLM to faithfully represent that community, and (2) evaluate the alignment of the finetuned model to the community. We demonstrate the method's utility in accurately representing political and fitness communities on Reddit. Unlike prior methods requiring human-authored instructions, Community-Cross-Instruct generates instructions in a fully unsupervised manner, enhancing scalability and generalization across domains. This work enables cost-effective and automated surveying of diverse online communities.

------------

`[2406.12104] End-to-end Text-to-SQL Generation within an Analytics Insight Engine <https://arxiv.org/abs/2406.12104>`__ 在分析洞察引擎中生成端到端的文本到sql

::

    Mon, 17 Jun 2024 21:33:01 GMT
    Karime Maamari, Amine Mhedhbi

Recent advancements in Text-to-SQL have pushed database management systems towards greater democratization of data access. Today's language models are at the core of these advancements. They enable impressive Text-to-SQL generation as experienced in the development of Distyl AI's Analytics Insight Engine. Its early deployment with enterprise customers has highlighted three core challenges. First, data analysts expect support with authoring SQL queries of very high complexity. Second, requests are ad-hoc and, as such, require low latency. Finally, generation requires an understanding of domain-specific terminology and practices.
The design and implementation of our Text-to-SQL generation pipeline, powered by large language models, tackles these challenges. The core tenants of our approach rely on external knowledge that we extract in a pre-processing phase, on retrieving the appropriate external knowledge at query generation time, and on decomposing SQL query generation following a hierarchical CTE-based structure. Finally, an adaptation framework leverages feedback to update the external knowledge, in turn improving query generation over time. We give an overview of our end-to-end approach and highlight the operators generating SQL during inference.

------------

`[2406.12109] Can LLMs Learn Macroeconomic Narratives from Social Media? <https://arxiv.org/abs/2406.12109>`__ 

::

    Mon, 17 Jun 2024 21:37:09 GMT
    Almog Gueta, Amir Feder, Zorik Gekhman, Ariel Goldstein, Roi Reichart

This study empirically tests the $\textit{Narrative Economics}$ hypothesis, which posits that narratives (ideas that are spread virally and affect public beliefs) can influence economic fluctuations. We introduce two curated datasets containing posts from X (formerly Twitter) which capture economy-related narratives (Data will be shared upon paper acceptance). Employing Natural Language Processing (NLP) methods, we extract and summarize narratives from the tweets. We test their predictive power for $\textit{macroeconomic}$ forecasting by incorporating the tweets' or the extracted narratives' representations in downstream financial prediction tasks. Our work highlights the challenges in improving macroeconomic models with narrative data, paving the way for the research community to realistically address this important challenge. From a scientific perspective, our investigation offers valuable insights and NLP tools for narrative extraction and summarization using Large Language Models (LLMs), contributing to future research on the role of narratives in economics.

------------

`[2406.12114] Enhancing Text Classification through LLM-Driven Active Learning and Human Annotation <https://arxiv.org/abs/2406.12114>`__ 通过llm驱动的主动学习和人工标注增强文本分类

::

    Mon, 17 Jun 2024 21:45:48 GMT
    Hamidreza Rouzegar, Masoud Makrehchi

In the context of text classification, the financial burden of annotation exercises for creating training data is a critical issue. Active learning techniques, particularly those rooted in uncertainty sampling, offer a cost-effective solution by pinpointing the most instructive samples for manual annotation. Similarly, Large Language Models (LLMs) such as GPT-3.5 provide an alternative for automated annotation but come with concerns regarding their reliability. This study introduces a novel methodology that integrates human annotators and LLMs within an Active Learning framework. We conducted evaluations on three public datasets. IMDB for sentiment analysis, a Fake News dataset for authenticity discernment, and a Movie Genres dataset for multi-label classification.The proposed framework integrates human annotation with the output of LLMs, depending on the model uncertainty levels. This strategy achieves an optimal balance between cost efficiency and classification performance. The empirical results show a substantial decrease in the costs associated with data annotation while either maintaining or improving model accuracy.

------------

`[2406.12128] AI "News" Content Farms Are Easy to Make and Hard to Detect: A Case Study in Italian <https://arxiv.org/abs/2406.12128>`__ AI“新闻”内容农场很容易制作，但很难检测:意大利语案例研究

::

    Mon, 17 Jun 2024 22:19:00 GMT
    Giovanni Puccetti and Anna Rogers and Chiara Alzetta and Felice Dell'Orletta and Andrea Esuli

Large Language Models (LLMs) are increasingly used as "content farm" models (CFMs), to generate synthetic text that could pass for real news articles. This is already happening even for languages that do not have high-quality monolingual LLMs. We show that fine-tuning Llama (v1), mostly trained on English, on as little as 40K Italian news articles, is sufficient for producing news-like texts that native speakers of Italian struggle to identify as synthetic.
We investigate three LLMs and three methods of detecting synthetic texts (log-likelihood, DetectGPT, and supervised classification), finding that they all perform better than human raters, but they are all impractical in the real world (requiring either access to token likelihood information or a large dataset of CFM texts). We also explore the possibility of creating a proxy CFM: an LLM fine-tuned on a similar dataset to one used by the real "content farm".
We find that even a small amount of fine-tuning data suffices for creating a successful detector, but we need to know which base LLM is used, which is a major challenge.
Our results suggest that there are currently no practical methods for detecting synthetic news-like texts 'in the wild', while generating them is too easy. We highlight the urgency of more NLP research on this problem.

------------

`[2406.12158] LLMs Are Prone to Fallacies in Causal Inference <https://arxiv.org/abs/2406.12158>`__ llm在因果推理中容易出现谬误

::

    Tue, 18 Jun 2024 00:14:07 GMT
    Nitish Joshi, Abulhair Saparov, Yixin Wang, He He

Recent work shows that causal facts can be effectively extracted from LLMs through prompting, facilitating the creation of causal graphs for causal inference tasks. However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize. Thus, this work investigates: Can LLMs infer causal relations from other relational data in text? To disentangle the role of memorized causal facts vs inferred causal relations, we finetune LLMs on synthetic data containing temporal, spatial and counterfactual relations, and measure whether the LLM can then infer causal relations. We find that: (a) LLMs are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. X mentioned before Y implies X causes Y); (b) if the order is randomized, LLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal relation) implies X causes Y. We also find that while LLMs can correctly deduce the absence of causal relations from temporal and spatial relations, they have difficulty inferring causal relations from counterfactuals, questioning their understanding of causality.

------------

`[2406.12159] Exploring the Impact of a Transformer's Latent Space Geometry on Downstream Task Performance <https://arxiv.org/abs/2406.12159>`__ 探索Transformer潜空间几何对下游任务性能的影响

::

    Tue, 18 Jun 2024 00:17:30 GMT
    Anna C. Marbut, John W. Chandler, Travis J. Wheeler

It is generally thought that transformer-based large language models benefit from pre-training by learning generic linguistic knowledge that can be focused on a specific task during fine-tuning. However, we propose that much of the benefit from pre-training may be captured by geometric characteristics of the latent space representations, divorced from any specific linguistic knowledge.
In this work we explore the relationship between GLUE benchmarking task performance and a variety of measures applied to the latent space resulting from BERT-type contextual language models. We find that there is a strong linear relationship between a measure of quantized cell density and average GLUE performance and that these measures may be predictive of otherwise surprising GLUE performance for several non-standard BERT-type models from the literature. These results may be suggestive of a strategy for decreasing pre-training requirements, wherein model initialization can be informed by the geometric characteristics of the model's latent space.

------------

`[2406.12182] Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models <https://arxiv.org/abs/2406.12182>`__ Aqulia-Med LLM:开创全流程开源医学语言模型

::

    Tue, 18 Jun 2024 01:30:07 GMT
    Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou, Donglin Hao, Yonghua Lin

Recently, both closed-source LLMs and open-source communities have made significant strides, outperforming humans in various general domains. However, their performance in specific professional fields such as medicine, especially within the open-source community, remains suboptimal due to the complexity of medical knowledge. We propose Aquila-Med, a bilingual medical LLM based on Aquila, addressing these challenges through continue pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). We construct a large-scale Chinese and English medical dataset for continue pre-training and a high-quality SFT dataset, covering extensive medical specialties. Additionally, we develop a high-quality Direct Preference Optimization (DPO) dataset for further alignment. Aquila-Med achieves notable results across single-turn, multi-turn dialogues, and medical multiple-choice questions, demonstrating the effectiveness of our approach. We open-source the datasets and the entire training process, contributing valuable resources to the research community. Our models and datasets will released at https://huggingface.co/BAAI/AquilaMed-RL.

------------

`[2406.12208] Knowledge Fusion By Evolving Weights of Language Models <https://arxiv.org/abs/2406.12208>`__ 

::

    Tue, 18 Jun 2024 02:12:34 GMT
    Guodong Du, Jing Li, Hanting Liu, Runhua Jiang, Shuyang Yu, Yifei Guo, Sim Kuan Goh, Ho-Kin Tang

Fine-tuning pre-trained language models, particularly large language models, demands extensive computing resources and can result in varying performance outcomes across different domains and datasets. This paper examines the approach of integrating multiple models from diverse training scenarios into a unified model. This unified model excels across various data domains and exhibits the ability to generalize well on out-of-domain data. We propose a knowledge fusion method named Evolver, inspired by evolutionary algorithms, which does not need further training or additional training data. Specifically, our method involves aggregating the weights of different language models into a population and subsequently generating offspring models through mutation and crossover operations. These offspring models are then evaluated against their parents, allowing for the preservation of those models that show enhanced performance on development datasets. Importantly, our model evolving strategy can be seamlessly integrated with existing model merging frameworks, offering a versatile tool for model enhancement. Experimental results on mainstream language models (i.e., encoder-only, decoder-only, encoder-decoder) reveal that Evolver outperforms previous state-of-the-art models by large margins. The code is publicly available at {https://github.com/duguodong7/model-evolution}.

------------

`[2406.12213] LLM-Oracle Machines <https://arxiv.org/abs/2406.12213>`__ LLM-Oracle机器

::

    Tue, 18 Jun 2024 02:25:33 GMT
    Jie Wang

Contemporary AI applications leverage large language models (LLMs) for their knowledge and inference capabilities in natural language processing tasks. This approach aligns with the concept of oracle Turing machines (OTMs). To capture the essence of these computations, including those desired but not yet in practice, we extend the notion of OTMs by employing a cluster of LLMs as the oracle. We present four variants: basic, augmented, fault-avoidance, and $\epsilon$-fault. The first two variants are commonly observed, whereas the latter two are specifically designed to ensure reliable outcomes by addressing LLM hallucinations, biases, and inconsistencies.

------------

`[2406.12221] On-Policy Fine-grained Knowledge Feedback for Hallucination Mitigation <https://arxiv.org/abs/2406.12221>`__ 缓解幻觉的政策细粒度知识反馈

::

    Tue, 18 Jun 2024 02:43:49 GMT
    Xueru Wen, Xinyu Lu, Xinyan Guan, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun

Hallucination occurs when large language models (LLMs) exhibit behavior that deviates from the boundaries of their knowledge during the response generation process. Previous learning-based methods focus on detecting knowledge boundaries and finetuning models with instance-level feedback, but they suffer from inaccurate signals due to off-policy data sampling and coarse-grained feedback. In this paper, we introduce \textit{\b{R}einforcement \b{L}earning \b{f}or \b{H}allucination} (RLFH), a fine-grained feedback-based online reinforcement learning method for hallucination mitigation. Unlike previous learning-based methods, RLFH enables LLMs to explore the boundaries of their internal knowledge and provide on-policy, fine-grained feedback on these explorations. To construct fine-grained feedback for learning reliable generation behavior, RLFH decomposes the outcomes of large models into atomic facts, provides statement-level evaluation signals, and traces back the signals to the tokens of the original responses. Finally, RLFH adopts the online reinforcement algorithm with these token-level rewards to adjust model behavior for hallucination mitigation. For effective on-policy optimization, RLFH also introduces an LLM-based fact assessment framework to verify the truthfulness and helpfulness of atomic facts without human intervention. Experiments on HotpotQA, SQuADv2, and Biography benchmarks demonstrate that RLFH can balance their usage of internal knowledge during the generation process to eliminate the hallucination behavior of LLMs.

------------

`[2406.12223] ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations <https://arxiv.org/abs/2406.12223>`__ ToxiCloakCN:基于隐蔽性扰动的中文攻击性语言检测鲁棒性评估

::

    Tue, 18 Jun 2024 02:44:56 GMT
    Yunze Xiao, Yujia Hu, Kenny Tsu Wei Choo and Roy Ka-wei Lee

Detecting hate speech and offensive language is essential for maintaining a safe and respectful digital environment. This study examines the limitations of state-of-the-art large language models (LLMs) in identifying offensive content within systematically perturbed data, with a focus on Chinese, a language particularly susceptible to such perturbations. We introduce \textsf{ToxiCloakCN}, an enhanced dataset derived from ToxiCN, augmented with homophonic substitutions and emoji transformations, to test the robustness of LLMs against these cloaking perturbations. Our findings reveal that existing models significantly underperform in detecting offensive content when these perturbations are applied. We provide an in-depth analysis of how different types of offensive content are affected by these perturbations and explore the alignment between human and model explanations of offensiveness. Our work highlights the urgent need for more advanced techniques in offensive language detection to combat the evolving tactics used to evade detection mechanisms.

------------

`[2406.12238] PFID: Privacy First Inference Delegation Framework for LLMs <https://arxiv.org/abs/2406.12238>`__ PFID:面向llm的隐私优先推理委托框架

::

    Tue, 18 Jun 2024 03:27:09 GMT
    Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, Jing Xiao

This paper introduces a novel privacy-preservation framework named PFID for LLMs that addresses critical privacy concerns by localizing user data through model sharding and singular value decomposition. When users are interacting with LLM systems, their prompts could be subject to being exposed to eavesdroppers within or outside LLM system providers who are interested in collecting users' input. In this work, we proposed a framework to camouflage user input, so as to alleviate privacy issues. Our framework proposes to place model shards on the client and the public server, we sent compressed hidden states instead of prompts to and from servers. Clients have held back information that can re-privatized the hidden states so that overall system performance is comparable to traditional LLMs services. Our framework was designed to be communication efficient, computation can be delegated to the local client so that the server's computation burden can be lightened. We conduct extensive experiments on machine translation tasks to verify our framework's performance.

------------

`[2406.12263] Defending Against Social Engineering Attacks in the Age of LLMs <https://arxiv.org/abs/2406.12263>`__ 在llm时代防御社会工程攻击

::

    Tue, 18 Jun 2024 04:39:40 GMT
    Lin Ai, Tharindu Kumarage, Amrita Bhattacharjee, Zizhou Liu, Zheng Hui, Michael Davinroy, James Cook, Laura Cassani, Kirill Trapeznikov, Matthias Kirchner, Arslan Basharat, Anthony Hoogs, Joshua Garland, Huan Liu, Julia Hirschberg

The proliferation of Large Language Models (LLMs) poses challenges in detecting and mitigating digital deception, as these models can emulate human conversational patterns and facilitate chat-based social engineering (CSE) attacks. This study investigates the dual capabilities of LLMs as both facilitators and defenders against CSE threats. We develop a novel dataset, SEConvo, simulating CSE scenarios in academic and recruitment contexts, and designed to examine how LLMs can be exploited in these situations. Our findings reveal that, while off-the-shelf LLMs generate high-quality CSE content, their detection capabilities are suboptimal, leading to increased operational costs for defense. In response, we propose ConvoSentinel, a modular defense pipeline that improves detection at both the message and the conversation levels, offering enhanced adaptability and cost-effectiveness. The retrieval-augmented module in ConvoSentinel identifies malicious intent by comparing messages to a database of similar conversations, enhancing CSE detection at all stages. Our study highlights the need for advanced strategies to leverage LLMs in cybersecurity.

------------

`[2406.12269] Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner for Insightful Table Summarization <https://arxiv.org/abs/2406.12269>`__ 基于问题-精确推理的隐式表知识挖掘方法

::

    Tue, 18 Jun 2024 04:55:09 GMT
    Kwangwook Seo, Jinyoung Yeo, Dongha Lee

Implicit knowledge hidden within the explicit table cells, such as data insights, is the key to generating a high-quality table summary. However, unveiling such implicit knowledge is a non-trivial task. Due to the complex nature of structured tables, it is challenging even for large language models (LLMs) to mine the implicit knowledge in an insightful and faithful manner. To address this challenge, we propose a novel table reasoning framework Question-then-Pinpoint. Our work focuses on building a plug-and-play table reasoner that can self-question the insightful knowledge and answer it by faithfully pinpointing evidence on the table to provide explainable guidance for the summarizer. To train a reliable reasoner, we collect table knowledge by guiding a teacher LLM to follow the coarse-to-fine reasoning paths and refine it through two quality enhancement strategies to selectively distill the high-quality knowledge to the reasoner. Extensive experiments on two table summarization datasets, including our newly proposed InsTaSumm, validate the general effectiveness of our framework.

------------

`[2406.12277] What Matters in Learning Facts in Language Models? Multifaceted Knowledge Probing with Diverse Multi-Prompt Datasets <https://arxiv.org/abs/2406.12277>`__ 在语言模型中学习事实有什么重要的?用多样化的多提示数据集进行多方面的知识探索

::

    Tue, 18 Jun 2024 05:11:35 GMT
    Xin Zhao, Naoki Yoshinaga, Daisuke Oba

Large language models (LLMs) face issues in handling factual knowledge, making it vital to evaluate their true ability to understand facts. In this study, we introduce knowledge probing frameworks, BELIEF(-ICL), to evaluate the knowledge understanding ability of not only encoder-based PLMs but also decoder-based PLMs from diverse perspectives. BELIEFs utilize a multi-prompt dataset to evaluate PLM's accuracy, consistency, and reliability in factual knowledge understanding. To provide a more reliable evaluation with BELIEFs, we semi-automatically create MyriadLAMA, which has more diverse prompts than existing datasets. We validate the effectiveness of BELIEFs in correctly and comprehensively evaluating PLM's factual understanding ability through extensive evaluations. We further investigate key factors in learning facts in LLMs, and reveal the limitation of the prompt-based knowledge probing. The dataset is anonymously publicized.

------------

`[2406.12329] SNAP: Unlearning Selective Knowledge in Large Language Models with Negative Instructions <https://arxiv.org/abs/2406.12329>`__ 

::

    Tue, 18 Jun 2024 06:54:05 GMT
    Minseok Choi, Daniel Rim, Dohyun Lee, Jaegul Choo

Instruction-following large language models (LLMs), such as ChatGPT, have become increasingly popular with the general audience, many of whom are incorporating them into their daily routines. However, these LLMs inadvertently disclose personal or copyrighted information, which calls for a machine unlearning method to remove selective knowledge. Previous attempts sought to forget the link between the target information and its associated entities, but it rather led to generating undesirable responses about the target, compromising the end-user experience. In this work, we propose SNAP, an innovative framework designed to selectively unlearn information by 1) training an LLM with negative instructions to generate obliterated responses, 2) augmenting hard positives to retain the original LLM performance, and 3) applying the novel Wasserstein regularization to ensure adequate deviation from the initial weights of the LLM. We evaluate our framework on various NLP benchmarks and demonstrate that our approach retains the original LLM capabilities, while successfully unlearning the specified information.

------------

`[2406.12347] Interpreting Bias in Large Language Models: A Feature-Based Approach <https://arxiv.org/abs/2406.12347>`__ 解释大型语言模型中的偏差:基于特征的方法

::

    Tue, 18 Jun 2024 07:28:15 GMT
    Nirmalendu Prakash and Lee Ka Wei Roy

Large Language Models (LLMs) such as Mistral and LLaMA have showcased remarkable performance across various natural language processing (NLP) tasks.
Despite their success, these models inherit social biases from the diverse datasets on which they are trained. This paper investigates the propagation of biases within LLMs through a novel feature-based analytical approach. Drawing inspiration from causal mediation analysis, we hypothesize the evolution of bias-related features and validate them using interpretability techniques like activation and attribution patching. Our contributions are threefold: (1) We introduce and empirically validate a feature-based method for bias analysis in LLMs, applied to LLaMA-2-7B, LLaMA-3-8B, and Mistral-7B-v0.3 with templates from a professions dataset. (2) We extend our method to another form of gender bias, demonstrating its generalizability. (3) We differentiate the roles of MLPs and attention heads in bias propagation and implement targeted debiasing using a counterfactual dataset. Our findings reveal the complex nature of bias in LLMs and emphasize the necessity for tailored debiasing strategies, offering a deeper understanding of bias mechanisms and pathways for effective mitigation.

------------

`[2406.12381] QOG:Question and Options Generation based on Language Model <https://arxiv.org/abs/2406.12381>`__ QOG:基于语言模型的问题和选项生成

::

    Tue, 18 Jun 2024 08:09:58 GMT
    Jincheng Zhou

Question-Options Generation (QOG) is a task that involves generating a set of question-options pairs given context. This task has various applications, including fine-tuning large models, information retrieval, and automated multiple-choice question generation for education. In this paper, we develop QOG models using three different methods based on fine-tuning sequence-to-sequence language models (LMs). Experiments demonstrate that the end-to-end QOG model is computationally efficient and stable during both training and inference, outperforming other methods. Furthermore, our analysis indicates that our QOG models are competitive on the QOG task compared to the large language model Llama 3-8B.

------------

`[2406.12382] From Instance Training to Instruction Learning: Task Adapters Generation from Instructions <https://arxiv.org/abs/2406.12382>`__ 从实例训练到指令学习:从指令生成任务适配器

::

    Tue, 18 Jun 2024 08:14:28 GMT
    Huanxuan Liao, Yao Xu, Shizhu He, Yuanzhe Zhang, Yanchao Hao, Shengping Liu, Kang Liu, Jun Zhao

Large language models (LLMs) have acquired the ability to solve general tasks by utilizing instruction finetuning (IFT). However, IFT still relies heavily on instance training of extensive task data, which greatly limits the adaptability of LLMs to real-world scenarios where labeled task instances are scarce and broader task generalization becomes paramount. Contrary to LLMs, humans acquire skills and complete tasks not merely through repeated practice but also by understanding and following instructional guidelines. This paper is dedicated to simulating human learning to address the shortcomings of instance training, focusing on instruction learning to enhance cross-task generalization. Within this context, we introduce Task Adapters Generation from Instructions (TAGI), which automatically constructs the task-specific model in a parameter generation manner based on the given task instructions without retraining for unseen tasks. Specifically, we utilize knowledge distillation to enhance the consistency between TAGI developed through Learning with Instruction and task-specific models developed through Training with Instance, by aligning the labels, output logits, and adapter parameters between them. TAGI is endowed with cross-task generalization capabilities through a two-stage training process that includes hypernetwork pretraining and finetuning. We evaluate TAGI on the Super-Natural Instructions and P3 datasets. The experimental results demonstrate that TAGI can match or even outperform traditional meta-trained models and other hypernetwork models, while significantly reducing computational requirements.

------------

`[2406.12397] Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models <https://arxiv.org/abs/2406.12397>`__ 揭示缺陷:探索大型语言模型合成数据的不完善和缓解策略

::

    Tue, 18 Jun 2024 08:38:59 GMT
    Jie Chen, Yupeng Zhang, Bingning Wang, Wayne Xin Zhao, Ji-Rong Wen, Weipeng Chen

Synthetic data has been proposed as a solution to address the issue of high-quality data scarcity in the training of large language models (LLMs).
Studies have shown that synthetic data can effectively improve the performance of LLMs on downstream benchmarks. However, despite its potential benefits, our analysis suggests that there may be inherent flaws in synthetic data. The uniform format of synthetic data can lead to pattern overfitting and cause significant shifts in the output distribution, thereby reducing the model's instruction-following capabilities. Our work delves into these specific flaws associated with question-answer (Q-A) pairs, a prevalent type of synthetic data, and presents a method based on unlearning techniques to mitigate these flaws. The empirical results demonstrate the effectiveness of our approach, which can reverse the instruction-following issues caused by pattern overfitting without compromising performance on benchmarks at relatively low cost. Our work has yielded key insights into the effective use of synthetic data, aiming to promote more robust and efficient LLM training.

------------

`[2406.12399] QueerBench: Quantifying Discrimination in Language Models Toward Queer Identities <https://arxiv.org/abs/2406.12399>`__ 

::

    Tue, 18 Jun 2024 08:40:29 GMT
    Mae Sosto, Alberto Barr\'on-Cede\~no

With the increasing role of Natural Language Processing (NLP) in various applications, challenges concerning bias and stereotype perpetuation are accentuated, which often leads to hate speech and harm. Despite existing studies on sexism and misogyny, issues like homophobia and transphobia remain underexplored and often adopt binary perspectives, putting the safety of LGBTQIA+ individuals at high risk in online spaces. In this paper, we assess the potential harm caused by sentence completions generated by English large language models (LLMs) concerning LGBTQIA+ individuals. This is achieved using QueerBench, our new assessment framework, which employs a template-based approach and a Masked Language Modeling (MLM) task. The analysis indicates that large language models tend to exhibit discriminatory behaviour more frequently towards individuals within the LGBTQIA+ community, reaching a difference gap of 7.2% in the QueerBench score of harmfulness.

------------

`[2406.12403] PDSS: A Privacy-Preserving Framework for Step-by-Step Distillation of Large Language Models <https://arxiv.org/abs/2406.12403>`__ PDSS:大型语言模型分步提炼的隐私保护框架

::

    Tue, 18 Jun 2024 08:48:14 GMT
    Tao Fan, Yan Kang, Weijing Chen, Hanlin Gu, Yuanfeng Song, Lixin Fan, Kai Chen, Qiang Yang

In the context of real-world applications, leveraging large language models (LLMs) for domain-specific tasks often faces two major challenges: domain-specific knowledge privacy and constrained resources. To address these issues, we propose PDSS, a privacy-preserving framework for step-by-step distillation of LLMs. PDSS works on a server-client architecture, wherein client transmits perturbed prompts to the server's LLM for rationale generation. The generated rationales are then decoded by the client and used to enrich the training of task-specific small language model(SLM) within a multi-task learning paradigm. PDSS introduces two privacy protection strategies: the Exponential Mechanism Strategy and the Encoder-Decoder Strategy, balancing prompt privacy and rationale usability. Experiments demonstrate the effectiveness of PDSS in various text generation tasks, enabling the training of task-specific SLM with enhanced performance while prioritizing data privacy protection.

------------

`[2406.12416] Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models <https://arxiv.org/abs/2406.12416>`__ 超越欠对齐:原子偏好增强的大型语言模型事实性调优

::

    Tue, 18 Jun 2024 09:07:30 GMT
    Hongbang Yuan, Yubo Chen, Pengfei Cao, Zhuoran Jin, Kang Liu, Jun Zhao

Large language models (LLMs) have achieved remarkable success but still tend to generate factually erroneous responses, a phenomenon known as hallucination.
A recent trend is to use preference learning to fine-tune models to align with factuality. However, existing work primarily evaluates fine-tuned models on in-domain (ID) datasets and the factuality on out-of-domain (OOD) datasets remains underexplored. In this paper, we conduct a comprehensive evaluation of the factuality of different models tuned by various preference learning algorithms and demonstrate that their performance on OOD datasets either increases minimally or decreases. Subsequently, we reveal that the main cause of model's failure to uphold factuality under a distribution shift is \textbf{under-alignment}, rather than \textbf{over-alignment}, by analyzing the token distribution shift of the models before and after tuning. Finally, we propose \textbf{APEFT} (\textbf{A}tomic \textbf{P}reference \textbf{E}nhanced \textbf{F}actuality \textbf{T}uning), a framework that enhances model's awareness of factuality at the granularity of individual facts. Extensive experiments demonstrate that APEFT improves model performance by an average of $\boldsymbol{3.45\%}$ on both ID and OOD datasets, which is highly effective.

------------

`[2406.12428] PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems <https://arxiv.org/abs/2406.12428>`__ PSLM:基于llm的低延迟口语对话系统文本和语音并行生成

::

    Tue, 18 Jun 2024 09:23:54 GMT
    Kentaro Mitsui, Koh Mitsuda, Toshiaki Wakatsuki, Yukiya Hono, Kei Sawada

Multimodal language models that process both text and speech have a potential for applications in spoken dialogue systems. However, current models face two major challenges in response generation latency: (1) generating a spoken response requires the prior generation of a written response, and (2) speech sequences are significantly longer than text sequences. This study addresses these issues by extending the input and output sequences of the language model to support the parallel generation of text and speech. Our experiments on spoken question answering tasks demonstrate that our approach improves latency while maintaining the quality of response content. Additionally, we show that latency can be further reduced by generating speech in multiple sequences. Demo samples are available at https://rinnakk.github.io/research/publications/PSLM.

------------

`[2406.12468] Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities <https://arxiv.org/abs/2406.12468>`__ 自适应Token偏置:基于偏置关键实体的知识编辑

::

    Tue, 18 Jun 2024 10:18:06 GMT
    Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Yilong Xu, Xueqi Cheng

The parametric knowledge memorized by large language models (LLMs) becomes outdated quickly. In-context editing (ICE) is currently the most effective method for updating the knowledge of LLMs. Recent advancements involve enhancing ICE by modifying the decoding strategy, obviating the need for altering internal model structures or adjusting external prompts. However, this enhancement operates across the entire sequence generation, encompassing a plethora of non-critical tokens. In this work, we introduce $\textbf{A}$daptive $\textbf{T}$oken $\textbf{Bias}$er ($\textbf{ATBias}$), a new decoding technique designed to enhance ICE. It focuses on the tokens that are mostly related to knowledge during decoding, biasing their logits by matching key entities related to new and parametric knowledge. Experimental results show that ATBias significantly enhances ICE performance, achieving up to a 32.3% improvement over state-of-the-art ICE methods while incurring only half the latency. ATBias not only improves the knowledge editing capabilities of ICE but can also be widely applied to LLMs with negligible cost.

------------

`[2406.12480] The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions <https://arxiv.org/abs/2406.12480>`__ llm生成的合成数据用于在线政治讨论中的立场检测的力量

::

    Tue, 18 Jun 2024 10:36:21 GMT
    Stefan Sylvius Wagner and Maike Behrendt and Marc Ziegele and Stefan Harmeling

Stance detection holds great potential for enhancing the quality of online political discussions, as it has shown to be useful for summarizing discussions, detecting misinformation, and evaluating opinion distributions.
Usually, transformer-based models are used directly for stance detection, which require large amounts of data. However, the broad range of debate questions in online political discussion creates a variety of possible scenarios that the model is faced with and thus makes data acquisition for model training difficult. In this work, we show how to leverage LLM-generated synthetic data to train and improve stance detection agents for online political discussions:(i) We generate synthetic data for specific debate questions by prompting a Mistral-7B model and show that fine-tuning with the generated synthetic data can substantially improve the performance of stance detection.
(ii) We examine the impact of combining synthetic data with the most informative samples from an unlabelled dataset. First, we use the synthetic data to select the most informative samples, second, we combine both these samples and the synthetic data for fine-tuning. This approach reduces labelling effort and consistently surpasses the performance of the baseline model that is trained with fully labeled data. Overall, we show in comprehensive experiments that LLM-generated data greatly improves stance detection performance for online political discussions.

------------

`[2406.12548] P-Tailor: Customizing Personality Traits for Language Models via Mixture of Specialized LoRA Experts <https://arxiv.org/abs/2406.12548>`__ P-Tailor:通过混合专业LoRA专家为语言模型定制个性特征

::

    Tue, 18 Jun 2024 12:25:13 GMT
    Yuhao Dan, Jie Zhou, Qin Chen, Junfeng Tian, Liang He

Personalized large language models (LLMs) have attracted great attention in many applications, such as intelligent education and emotional support. Most work focuses on controlling the character settings based on the profile (e.g., age, skill, experience, and so on). Conversely, the psychological theory-based personality traits with implicit expression and behavior are not well modeled, limiting their potential application in more specialized fields such as the psychological counseling agents. In this paper, we propose a mixture of experts (MoE)-based personalized LLMs, named P-tailor, to model the Big Five Personality Traits. Particularly, we learn specialized LoRA experts to represent various traits, such as openness, conscientiousness, extraversion, agreeableness and neuroticism. Then, we integrate P-Tailor with a personality specialization loss, promoting experts to specialize in distinct personality traits, thereby enhancing the efficiency of model parameter utilization. Due to the lack of datasets, we also curate a high-quality personality crafting dataset (PCD) to learn and develop the ability to exhibit different personality traits across various topics. We conduct extensive experiments to verify the great performance and effectiveness of P-Tailor in manipulation of the fine-grained personality traits of LLMs.

------------

`[2406.12570] Applying Ensemble Methods to Model-Agnostic Machine-Generated Text Detection <https://arxiv.org/abs/2406.12570>`__ 应用集成方法进行模型无关的机器生成文本检测

::

    Tue, 18 Jun 2024 12:58:01 GMT
    Ivan Ong, Boon King Quek

In this paper, we study the problem of detecting machine-generated text when the large language model (LLM) it is possibly derived from is unknown. We do so by apply ensembling methods to the outputs from DetectGPT classifiers (Mitchell et al. 2023), a zero-shot model for machine-generated text detection which is highly accurate when the generative (or base) language model is the same as the discriminative (or scoring) language model. We find that simple summary statistics of DetectGPT sub-model outputs yield an AUROC of 0.73 (relative to 0.61) while retaining its zero-shot nature, and that supervised learning methods sharply boost the accuracy to an AUROC of 0.94 but require a training dataset. This suggests the possibility of further generalisation to create a highly-accurate, model-agnostic machine-generated text detector.

------------

`[2406.12585] Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling <https://arxiv.org/abs/2406.12585>`__ 通过将代币生成视为集成的分类，打破了LLM社区的天花板

::

    Tue, 18 Jun 2024 13:17:26 GMT
    Yao-Ching Yu, Chun-Chih Kuo, Ziqi Ye, Yu-Cheng Chang, Yueh-Se Li

Ensembling multiple models has always been an effective approach to push the limits of existing performance and is widely used in classification tasks by simply averaging the classification probability vectors from multiple classifiers to achieve better accuracy. However, in the thriving open-source Large Language Model (LLM) community, ensembling methods are rare and typically limited to ensembling the full-text outputs of LLMs, such as selecting the best output using a ranker, which leads to underutilization of token-level probability information. In this paper, we treat the Generation of each token by LLMs as a Classification (GaC) for ensembling. This approach fully exploits the probability information at each generation step and better prevents LLMs from producing early incorrect tokens that lead to snowballing errors. In experiments, we ensemble state-of-the-art LLMs on several benchmarks, including exams, mathematics and reasoning, and observe that our method breaks the existing community performance ceiling. Furthermore, we observed that most of the tokens in the answer are simple and do not affect the correctness of the final answer. Therefore, we also experimented with ensembling only key tokens, and the results showed better performance with lower latency across benchmarks.

------------

`[2406.12606] Low-Redundant Optimization for Large Language Model Alignment <https://arxiv.org/abs/2406.12606>`__ 

::

    Tue, 18 Jun 2024 13:34:40 GMT
    Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Jingyuan Wang and Ji-Rong Wen

Large language models (LLMs) are still struggling in aligning with human preference in complex tasks and scenarios. They are prone to overfit into the unexpected patterns or superficial styles in the training data. We conduct an empirical study that only selects the top-10\% most updated parameters in LLMs for alignment training, and see improvements in the convergence process and final performance. It indicates the existence of redundant neurons in LLMs for alignment training. To reduce its influence, we propose a low-redundant alignment method named \textbf{ALLO}, focusing on optimizing the most related neurons with the most useful supervised signals. Concretely, we first identify the neurons that are related to the human preference data by a gradient-based strategy, then identify the alignment-related key tokens by reward models for computing loss. Besides, we also decompose the alignment process into the forgetting and learning stages, where we first forget the tokens with unaligned knowledge and then learn aligned knowledge, by updating different ratios of neurons, respectively. Experimental results on 10 datasets have shown the effectiveness of ALLO. Our code and data are available at \url{https://github.com/RUCAIBox/ALLO}.

------------

`[2406.12624] Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges <https://arxiv.org/abs/2406.12624>`__ 评判:评估作为评判的llms的一致性和脆弱性

::

    Tue, 18 Jun 2024 13:49:54 GMT
    Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes

Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges. We leverage TriviaQA as a benchmark for assessing objective knowledge reasoning of LLMs and evaluate them alongside human annotations which we found to have a high inter-annotator agreement. Our study includes 9 judge models and 9 exam taker models -- both base and instruction-tuned. We assess the judge model's alignment across different model sizes, families, and judge prompts. Among other results, our research rediscovers the importance of using Cohen's kappa as a metric of alignment as opposed to simple percent agreement, showing that judges with high percent agreement can still assign vastly different scores. We find that both Llama-3 70B and GPT-4 Turbo have an excellent alignment with humans, but in terms of ranking exam taker models, they are outperformed by both JudgeLM-7B and the lexical judge Contains, which have up to 34 points lower human alignment. Through error analysis and various other studies, including the effects of instruction length and leniency bias, we hope to provide valuable lessons for using LLMs as judges in the future.

------------

`[2406.12641] DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence? <https://arxiv.org/abs/2406.12641>`__ DetectBench:大型语言模型能检测并拼凑隐含证据吗?

::

    Tue, 18 Jun 2024 14:08:01 GMT
    Zhouhong Gu, Lin Zhang, Xiaoxuan Zhu, Jiangjie Chen, Wenhao Huang, Yikai Zhang, Shusen Wang, Zheyu Ye, Yan Gao, Hongwei Feng, Yanghua Xiao

Detecting evidence within the context is a key step in the process of reasoning task. Evaluating and enhancing the capabilities of LLMs in evidence detection will strengthen context-based reasoning performance. This paper proposes a benchmark called DetectBench for verifying the ability to detect and piece together implicit evidence within a long context. DetectBench contains 3,928 multiple-choice questions, with an average of 994 tokens per question.
Each question contains an average of 4.55 pieces of implicit evidence, and solving the problem typically requires 7.62 logical jumps to find the correct answer. To enhance the performance of LLMs in evidence detection, this paper proposes Detective Reasoning Prompt and Finetune. Experiments demonstrate that the existing LLMs' abilities to detect evidence in long contexts are far inferior to humans. However, the Detective Reasoning Prompt effectively enhances the capability of powerful LLMs in evidence detection, while the Finetuning method shows significant effects in enhancing the performance of weaker LLMs. Moreover, when the abilities of LLMs in evidence detection are improved, their final reasoning performance is also enhanced accordingly.

------------

`[2406.12644] Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models <https://arxiv.org/abs/2406.12644>`__ 层次提示分类法:大型语言模型通用评估框架

::

    Tue, 18 Jun 2024 14:12:27 GMT
    Devichand Budagam, Sankalp KJ, Ashutosh Kumar, Vinija Jain, Aman Chadha

Assessing the effectiveness of large language models (LLMs) in addressing diverse tasks is essential for comprehending their strengths and weaknesses.
Conventional evaluation techniques typically apply a single prompting strategy uniformly across datasets, not considering the varying degrees of task complexity. We introduce the Hierarchical Prompting Taxonomy (HPT), a taxonomy that employs a Hierarchical Prompt Framework (HPF) composed of five unique prompting strategies, arranged from the simplest to the most complex, to assess LLMs more precisely and to offer a clearer perspective. This taxonomy assigns a score, called the Hierarchical Prompting Score (HP-Score), to datasets as well as LLMs based on the rules of the taxonomy, providing a nuanced understanding of their ability to solve diverse tasks and offering a universal measure of task complexity. Additionally, we introduce the Adaptive Hierarchical Prompt framework, which automates the selection of appropriate prompting strategies for each task. This study compares manual and adaptive hierarchical prompt frameworks using four instruction-tuned LLMs, namely Llama 3 8B, Phi 3 3.8B, Mistral 7B, and Gemma 7B, across four datasets: BoolQ, CommonSenseQA (CSQA), IWSLT-2017 en-fr (IWSLT), and SamSum. Experiments demonstrate the effectiveness of HPT, providing a reliable way to compare different tasks and LLM capabilities. This paper leads to the development of a universal evaluation metric that can be used to evaluate both the complexity of the datasets and the capabilities of LLMs. The implementation of both manual HPF and adaptive HPF is publicly available.

------------

`[2406.12645] Evaluating Transparency of Machine Generated Fact Checking Explanations <https://arxiv.org/abs/2406.12645>`__ 机器生成事实检查解释的透明度评估

::

    Tue, 18 Jun 2024 14:13:13 GMT
    Rui Xing, Timothy Baldwin, Jey Han Lau

An important factor when it comes to generating fact-checking explanations is the selection of evidence: intuitively, high-quality explanations can only be generated given the right evidence. In this work, we investigate the impact of human-curated vs. machine-selected evidence for explanation generation using large language models. To assess the quality of explanations, we focus on transparency (whether an explanation cites sources properly) and utility (whether an explanation is helpful in clarifying a claim). Surprisingly, we found that large language models generate similar or higher quality explanations using machine-selected evidence, suggesting carefully curated evidence (by humans) may not be necessary. That said, even with the best model, the generated explanations are not always faithful to the sources, suggesting further room for improvement in explanation generation for fact-checking.

------------

`[2406.12665] CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis <https://arxiv.org/abs/2406.12665>`__ CollabStory: Multi-LLM协同故事生成与作者分析

::

    Tue, 18 Jun 2024 14:35:12 GMT
    Saranya Venkatraman, Nafis Irtiza Tripto, Dongwon Lee

The rise of unifying frameworks that enable seamless interoperability of Large Language Models (LLMs) has made LLM-LLM collaboration for open-ended tasks a possibility. Despite this, there have not been efforts to explore such collaborative writing. We take the next step beyond human-LLM collaboration to explore this multi-LLM scenario by generating the first exclusively LLM-generated collaborative stories dataset called CollabStory. We focus on single-author ($N=1$) to multi-author (up to $N=5$) scenarios, where multiple LLMs co-author stories. We generate over 32k stories using open-source instruction-tuned LLMs. Further, we take inspiration from the PAN tasks that have set the standard for human-human multi-author writing tasks and analysis.
We extend their authorship-related tasks for multi-LLM settings and present baselines for LLM-LLM collaboration. We find that current baselines are not able to handle this emerging scenario. Thus, CollabStory is a resource that could help propel an understanding as well as the development of techniques to discern the use of multiple LLMs. This is crucial to study in the context of writing tasks since LLM-LLM collaboration could potentially overwhelm ongoing challenges related to plagiarism detection, credit assignment, maintaining academic integrity in educational settings, and addressing copyright infringement concerns. We make our dataset and code available at \texttt{\url{https://github.com/saranya-venkatraman/multi_llm_story_writing}}.

------------

`[2406.12673] Estimating Knowledge in Large Language Models Without Generating a Single Token <https://arxiv.org/abs/2406.12673>`__ 

::

    Tue, 18 Jun 2024 14:45:50 GMT
    Daniela Gottesman and Mor Geva

To evaluate knowledge in large language models (LLMs), current methods query the model and then evaluate its generated responses. In this work, we ask whether evaluation can be done $\textit{before}$ the model has generated any text. Concretely, is it possible to estimate how knowledgeable a model is about a certain entity, only from its internal computation? We study this question with two tasks: given a subject entity, the goal is to predict (a) the ability of the model to answer common questions about the entity, and (b) the factuality of responses generated by the model about the entity. Experiments with a variety of LLMs show that KEEN, a simple probe trained over internal subject representations, succeeds at both tasks - strongly correlating with both the QA accuracy of the model per-subject and FActScore, a recent factuality metric in open-ended generation. Moreover, KEEN naturally aligns with the model's hedging behavior and faithfully reflects changes in the model's knowledge after fine-tuning. Lastly, we show a more interpretable yet equally performant variant of KEEN, which highlights a small set of tokens that correlates with the model's lack of knowledge. Being simple and lightweight, KEEN can be leveraged to identify gaps and clusters of entity knowledge in LLMs, and guide decisions such as augmenting queries with retrieval.

------------

`[2406.12679] Vernacular? I Barely Know Her: Challenges with Style Control and Stereotyping <https://arxiv.org/abs/2406.12679>`__ 方言吗?我几乎不认识她:风格控制和刻板印象方面的挑战

::

    Tue, 18 Jun 2024 14:51:30 GMT
    Ankit Aich, Tingting Liu, Salvatore Giorgi, Kelsey Isman, Lyle Ungar, Brenda Curtis

Large Language Models (LLMs) are increasingly being used in educational and learning applications. Research has demonstrated that controlling for style, to fit the needs of the learner, fosters increased understanding, promotes inclusion, and helps with knowledge distillation. To understand the capabilities and limitations of contemporary LLMs in style control, we evaluated five state-of-the-art models: GPT-3.5, GPT-4, GPT-4o, Llama-3, and Mistral-instruct- 7B across two style control tasks. We observed significant inconsistencies in the first task, with model performances averaging between 5th and 8th grade reading levels for tasks intended for first-graders, and standard deviations up to 27.6. For our second task, we observed a statistically significant improvement in performance from 0.02 to 0.26.
However, we find that even without stereotypes in reference texts, LLMs often generated culturally insensitive content during their tasks. We provide a thorough analysis and discussion of the results.

------------

`[2406.12680] Measuring Psychological Depth in Language Models <https://arxiv.org/abs/2406.12680>`__ 语言模型中的心理深度测量

::

    Tue, 18 Jun 2024 14:51:54 GMT
    Fabrice Harel-Canada, Hanyu Zhou, Sreya Mupalla, Zeynep Yildiz, Amit Sahai, Nanyun Peng

Evaluations of creative stories generated by large language models (LLMs) often focus on objective properties of the text, such as its style, coherence, and toxicity. While these metrics are indispensable, they do not speak to a story's subjective, psychological impact from a reader's perspective. We introduce the Psychological Depth Scale (PDS), a novel framework rooted in literary theory that measures an LLM's ability to produce authentic and narratively complex stories that provoke emotion, empathy, and engagement. We empirically validate our framework by showing that humans can consistently evaluate stories based on PDS (0.72 Krippendorff's alpha). We also explore techniques for automating the PDS to easily scale future analyses. GPT-4o, combined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an average Spearman correlation of $0.51$ with human judgment while Llama-3-70B scores as high as 0.68 for empathy. Finally, we compared the depth of stories authored by both humans and LLMs. Surprisingly, GPT-4 stories either surpassed or were statistically indistinguishable from highly-rated human-written stories sourced from Reddit. By shifting the focus from text to reader, the Psychological Depth Scale is a validated, automated, and systematic means of measuring the capacity of LLMs to connect with humans through the stories they tell.

------------

`[2406.12687] Using LLMs to Aid Annotation and Collection of Clinically-Enriched Data in Bipolar Disorder and Schizophrenia <https://arxiv.org/abs/2406.12687>`__ 使用llm帮助注释和收集双相情感障碍和精神分裂症的临床丰富数据

::

    Tue, 18 Jun 2024 15:00:24 GMT
    Ankit Aich, Avery Quynh, Pamela Osseyi, Amy Pinkham, Philip Harvey, Brenda Curtis, Colin Depp, Natalie Parde

NLP in mental health has been primarily social media focused. Real world practitioners also have high case loads and often domain specific variables, of which modern LLMs lack context. We take a dataset made by recruiting 644 participants, including individuals diagnosed with Bipolar Disorder (BD), Schizophrenia (SZ), and Healthy Controls (HC). Participants undertook tasks derived from a standardized mental health instrument, and the resulting data were transcribed and annotated by experts across five clinical variables. This paper demonstrates the application of contemporary language models in sequence-to-sequence tasks to enhance mental health research. Specifically, we illustrate how these models can facilitate the deployment of mental health instruments, data collection, and data annotation with high accuracy and scalability. We show that small models are capable of annotation for domain-specific clinical variables, data collection for mental-health instruments, and perform better then commercial large models.

------------

`[2406.12692] MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL <https://arxiv.org/abs/2406.12692>`__ MAGIC:为上下文中的Text-to-SQL生成自校正准则

::

    Tue, 18 Jun 2024 15:06:06 GMT
    Arian Askari, Christian Poelitz, Xinye Tang

Self-correction in text-to-SQL is the process of prompting large language model (LLM) to revise its previously incorrectly generated SQL, and commonly relies on manually crafted self-correction guidelines by human experts that are not only labor-intensive to produce but also limited by the human ability in identifying all potential error patterns in LLM responses. We introduce MAGIC, a novel multi-agent method that automates the creation of the self-correction guideline. MAGIC uses three specialized agents: a manager, a correction, and a feedback agent. These agents collaborate on the failures of an LLM-based method on the training set to iteratively generate and refine a self-correction guideline tailored to LLM mistakes, mirroring human processes but without human involvement. Our extensive experiments show that MAGIC's guideline outperforms expert human's created ones. We empirically find out that the guideline produced by MAGIC enhance the interpretability of the corrections made, providing insights in analyzing the reason behind the failures and successes of LLMs in self-correction. We make all agent interactions publicly available to the research community, to foster further research in this area, offering a synthetic dataset for future explorations into automatic self-correction guideline generation.

------------

`[2406.12702] Jailbreak Paradox: The Achilles' Heel of LLMs <https://arxiv.org/abs/2406.12702>`__ 越狱悖论:llm的阿喀琉斯之踵

::

    Tue, 18 Jun 2024 15:14:35 GMT
    Abhinav Rao, Monojit Choudhury, Somak Aditya

We introduce two paradoxes concerning jailbreak of foundation models: First, it is impossible to construct a perfect jailbreak classifier, and second, a weaker model cannot consistently detect whether a stronger (in a pareto-dominant sense) model is jailbroken or not. We provide formal proofs for these paradoxes and a short case study on Llama and GPT4-o to demonstrate this.
We discuss broader theoretical and practical repercussions of these results.

------------

`[2406.12719] On the Robustness of Language Models for Tabular Question Answering <https://arxiv.org/abs/2406.12719>`__ 表格式问答语言模型的鲁棒性研究

::

    Tue, 18 Jun 2024 15:41:15 GMT
    Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao

Large Language Models (LLMs), originally shown to ace various text comprehension tasks have also remarkably been shown to tackle table comprehension tasks without specific training. While previous research has explored LLM capabilities with tabular dataset tasks, our study assesses the influence of $\textit{in-context learning}$,$ \textit{model scale}$, $\textit{instruction tuning}$, and $\textit{domain biases}$ on Tabular Question Answering (TQA). We evaluate the robustness of LLMs on Wikipedia-based $\textbf{WTQ}$ and financial report-based $\textbf{TAT-QA}$ TQA datasets, focusing on their ability to robustly interpret tabular data under various augmentations and perturbations. Our findings indicate that instructions significantly enhance performance, with recent models like Llama3 exhibiting greater robustness over earlier versions. However, data contamination and practical reliability issues persist, especially with WTQ. We highlight the need for improved methodologies, including structure-aware self-attention mechanisms and better handling of domain-specific tabular data, to develop more reliable LLMs for table comprehension.

------------

`[2406.12725] Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource Sound Law Induction <https://arxiv.org/abs/2406.12725>`__ 大型语言模型能像语言学家一样编码吗?:低资源健全规律归纳的案例研究

::

    Tue, 18 Jun 2024 15:46:04 GMT
    Atharva Naik, Kexun Zhang, Nathaniel Robinson, Aravind Mysore, Clayton Marr, Hong Sng Rebecca Byrnes, Anna Cai, Kalvin Chang, David Mortensen

Historical linguists have long written a kind of incompletely formalized ''program'' that converts reconstructed words in an ancestor language into words in one of its attested descendants that consist of a series of ordered string rewrite functions (called sound laws). They do this by observing pairs of words in the reconstructed language (protoforms) and the descendent language (reflexes) and constructing a program that transforms protoforms into reflexes.
However, writing these programs is error-prone and time-consuming. Prior work has successfully scaffolded this process computationally, but fewer researchers have tackled Sound Law Induction (SLI), which we approach in this paper by casting it as Programming by Examples. We propose a language-agnostic solution that utilizes the programming ability of Large Language Models (LLMs) by generating Python sound law programs from sound change examples. We evaluate the effectiveness of our approach for various LLMs, propose effective methods to generate additional language-agnostic synthetic data to fine-tune LLMs for SLI, and compare our method with existing automated SLI methods showing that while LLMs lag behind them they can complement some of their weaknesses.

------------

`[2406.12738] Large Language Model as a Universal Clinical Multi-task Decoder <https://arxiv.org/abs/2406.12738>`__ 大型语言模型作为临床通用多任务解码器

::

    Tue, 18 Jun 2024 15:58:36 GMT
    Yujiang Wu, Hongjian Song, Jiawen Zhang, Xumeng Wen, Shun Zheng, Jiang Bian

The development of effective machine learning methodologies for enhancing the efficiency and accuracy of clinical systems is crucial. Despite significant research efforts, managing a plethora of diversified clinical tasks and adapting to emerging new tasks remain significant challenges. This paper presents a novel paradigm that employs a pre-trained large language model as a universal clinical multi-task decoder. This approach leverages the flexibility and diversity of language expressions to handle task topic variations and associated arguments. The introduction of a new task simply requires the addition of a new instruction template. We validate this framework across hundreds of tasks, demonstrating its robustness in facilitating multi-task predictions, performing on par with traditional multi-task learning and single-task learning approaches. Moreover, it shows exceptional adaptability to new tasks, with impressive zero-shot performance in some instances and superior data efficiency in few-shot scenarios. This novel approach offers a unified solution to manage a wide array of new and emerging tasks in clinical applications.

------------

`[2406.12739] Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages <https://arxiv.org/abs/2406.12739>`__ 模型堆叠的自蒸馏解锁了200多种语言的跨语言NLU

::

    Tue, 18 Jun 2024 16:00:20 GMT
    Fabian David Schmidt, Philipp Borchert, Ivan Vuli\'c, Goran Glava\v{s}

LLMs have become a go-to solution not just for text generation, but also for natural language understanding (NLU) tasks. Acquiring extensive knowledge through language modeling on web-scale corpora, they excel on English NLU, yet struggle to extend their NLU capabilities to underrepresented languages. In contrast, machine translation models (MT) produce excellent multilingual representations, resulting in strong translation performance even for low-resource languages. MT encoders, however, lack the knowledge necessary for comprehensive NLU that LLMs obtain through language modeling training on immense corpora. In this work, we get the best both worlds by integrating MT encoders directly into LLM backbones via sample-efficient self-distillation.
The resulting MT-LLMs preserve the inherent multilingual representational alignment from the MT encoder, allowing lower-resource languages to tap into the rich knowledge embedded in English-centric LLMs. Merging the MT encoder and LLM in a single model, we mitigate the propagation of translation errors and inference overhead of MT decoding inherent to discrete translation-based cross-lingual transfer (e.g., translate-test). Evaluation spanning three prominent NLU tasks and 127 predominantly low-resource languages renders MT-LLMs highly effective in cross-lingual transfer. MT-LLMs substantially and consistently outperform translate-test based on the same MT model, showing that we truly unlock multilingual language understanding for LLMs.

------------

`[2406.12746] Rationale-based Ensemble of Multiple QA Strategies for Zero-shot Knowledge-based VQA <https://arxiv.org/abs/2406.12746>`__ 基于理论基础的多QA策略集成的零样本知识库VQA

::

    Tue, 18 Jun 2024 16:06:38 GMT
    Miaoyu Li, Haoxin Li, Zilin Du, and Boyang Li

Knowledge-based Visual Qustion-answering (K-VQA) necessitates the use of background knowledge beyond what is depicted in the image. Current zero-shot K-VQA methods usually translate an image to a single type of textual decision context and use a text-based model to answer the question based on it, which conflicts with the fact that K-VQA questions often require the combination of multiple question-answering strategies. In light of this, we propose Rationale-based Ensemble of Answer Context Tactics (REACT) to achieve a dynamic ensemble of multiple question-answering tactics, comprising Answer Candidate Generation (ACG) and Rationale-based Strategy Fusion (RSF). In ACG, we generate three distinctive decision contexts to provide different strategies for each question, resulting in the generation of three answer candidates. RSF generates automatic and mechanistic rationales from decision contexts for each candidate, allowing the model to select the correct answer from all candidates. We conduct comprehensive experiments on the OK-VQA and A-OKVQA datasets, and our method significantly outperforms state-of-the-art LLM-based baselines on all datasets.

------------

`[2406.12754] Chumor 1.0: A Truly Funny and Challenging Chinese Humor Understanding Dataset from Ruo Zhi Ba <https://arxiv.org/abs/2406.12754>`__ Chumor 1.0:一个真正有趣且具有挑战性的中文幽默理解数据集

::

    Tue, 18 Jun 2024 16:22:05 GMT
    Ruiqi He, Yushu He, Longju Bai, Jiarui Liu, Zhenjie Sun, Zenghao Tang, He Wang, Hanchen Xia, Naihao Deng

Existing humor datasets and evaluations predominantly focus on English, lacking resources for culturally nuanced humor in non-English languages like Chinese. To address this gap, we construct Chumor, a dataset sourced from Ruo Zhi Ba (RZB), a Chinese Reddit-like platform dedicated to sharing intellectually challenging and culturally specific jokes. We annotate explanations for each joke and evaluate human explanations against two state-of-the-art LLMs, GPT-4o and ERNIE Bot, through A/B testing by native Chinese speakers. Our evaluation shows that Chumor is challenging even for SOTA LLMs, and the human explanations for Chumor jokes are significantly better than explanations generated by the LLMs.

------------

`[2406.12775] Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries <https://arxiv.org/abs/2406.12775>`__ 

::

    Tue, 18 Jun 2024 16:44:13 GMT
    Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, Amir Globerson

Large language models (LLMs) can solve complex multi-step problems, but little is known about how these computations are implemented internally.
Motivated by this, we study how LLMs answer multi-hop queries such as "The spouse of the performer of Imagine is". These queries require two information extraction steps: a latent one for resolving the first hop ("the performer of Imagine") into the bridge entity (John Lennon), and one for resolving the second hop ("the spouse of John Lennon") into the target entity (Yoko Ono).
Understanding how the latent step is computed internally is key to understanding the overall computation. By carefully analyzing the internal computations of transformer-based LLMs, we discover that the bridge entity is resolved in the early layers of the model. Then, only after this resolution, the two-hop query is solved in the later layers. Because the second hop commences in later layers, there could be cases where these layers no longer encode the necessary knowledge for correctly predicting the answer. Motivated by this, we propose a novel "back-patching" analysis method whereby a hidden representation from a later layer is patched back to an earlier layer. We find that in up to 57% of previously incorrect cases there exists a back-patch that results in the correct generation of the answer, showing that the later layers indeed sometimes lack the needed functionality. Overall our methods and findings open further opportunities for understanding and improving latent reasoning in transformer-based LLMs.

------------

`[2406.12787] Generating Educational Materials with Different Levels of Readability using LLMs <https://arxiv.org/abs/2406.12787>`__ 使用llm生成可读性不同级别的教育材料

::

    Tue, 18 Jun 2024 16:55:10 GMT
    Chieh-Yang Huang, Jing Wei, Ting-Hao 'Kenneth' Huang

This study introduces the leveled-text generation task, aiming to rewrite educational materials to specific readability levels while preserving meaning.
We assess the capability of GPT-3.5, LLaMA-2 70B, and Mixtral 8x7B, to generate content at various readability levels through zero-shot and few-shot prompting.
Evaluating 100 processed educational materials reveals that few-shot prompting significantly improves performance in readability manipulation and information preservation. LLaMA-2 70B performs better in achieving the desired difficulty range, while GPT-3.5 maintains original meaning. However, manual inspection highlights concerns such as misinformation introduction and inconsistent edit distribution. These findings emphasize the need for further research to ensure the quality of generated educational content.

------------

`[2406.12809] Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones? <https://arxiv.org/abs/2406.12809>`__ 如果大型语言模型能解决较难的问题，那么它们是否总能解决较容易的问题?

::

    Tue, 18 Jun 2024 17:25:47 GMT
    Zhe Yang, Yichang Zhang, Tianyu Liu, Jian Yang, Junyang Lin, Chang Zhou, Zhifang Sui

Large language models (LLMs) have demonstrated impressive capabilities, but still suffer from inconsistency issues (e.g. LLMs can react differently to disturbances like rephrasing or inconsequential order change). In addition to these inconsistencies, we also observe that LLMs, while capable of solving hard problems, can paradoxically fail at easier ones. To evaluate this hard-to-easy inconsistency, we develop the ConsisEval benchmark, where each entry comprises a pair of questions with a strict order of difficulty. Furthermore, we introduce the concept of consistency score to quantitatively measure this inconsistency and analyze the potential for improvement in consistency by relative consistency score. Based on comprehensive experiments across a variety of existing models, we find: (1) GPT-4 achieves the highest consistency score of 92.2\% but is still inconsistent to specific questions due to distraction by redundant information, misinterpretation of questions, etc.; (2) models with stronger capabilities typically exhibit higher consistency, but exceptions also exist; (3) hard data enhances consistency for both fine-tuning and in-context learning. Our data and code will be publicly available on GitHub.

------------

`[2406.12822] Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models? <https://arxiv.org/abs/2406.12822>`__ 这是用于多语言指令调优的好数据，还是用于大型语言模型的糟糕的多语言评估?

::

    Tue, 18 Jun 2024 17:43:47 GMT
    Pinzhen Chen, Simon Yu, Zhicheng Guo, Barry Haddow

Large language models, particularly multilingual ones, are designed, claimed, and expected to cater to native speakers of varied languages. We hypothesise that the current practices of fine-tuning and evaluating these models may mismatch this intention owing to a heavy reliance on translation, which can introduce translation artefacts and defects. It remains unknown whether the nature of the instruction data has an impact on the model output; on the other hand, it remains questionable whether translated test sets can capture such nuances. Due to the often coupled practices of using translated data in both stages, such imperfections could have been overlooked. This work investigates these issues by using controlled native or translated data during instruction tuning and evaluation stages and observing model results. Experiments on eight base models and eight different benchmarks reveal that native or generation benchmarks display a notable difference between native and translated instruction data especially when model performance is high, whereas other types of test sets cannot. Finally, we demonstrate that regularization is beneficial to bridging this gap on structured but not generative tasks.

------------

`[2406.12832] LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation <https://arxiv.org/abs/2406.12832>`__ LaMDA:基于谱分解低维自适应的大模型微调

::

    Tue, 18 Jun 2024 17:52:59 GMT
    Seyedarmin Azizi, Souvik Kundu, Massoud Pedram

Low-rank adaptation (LoRA) has become the default approach to fine-tune large language models (LLMs) due to its significant reduction in trainable parameters. However, trainable parameter demand for LoRA increases with increasing model embedding dimensions, leading to high compute costs.
Additionally, its backward updates require storing high-dimensional intermediate activations and optimizer states, demanding high peak GPU memory.
In this paper, we introduce large model fine-tuning via spectrally decomposed low-dimensional adaptation (LaMDA), a novel approach to fine-tuning large language models, which leverages low-dimensional adaptation to achieve significant reductions in trainable parameters and peak GPU memory footprint.
LaMDA freezes a first projection matrix (PMA) in the adaptation path while introducing a low-dimensional trainable square matrix, resulting in substantial reductions in trainable parameters and peak GPU memory usage. LaMDA gradually freezes a second projection matrix (PMB) during the early fine-tuning stages, reducing the compute cost associated with weight updates to enhance parameter efficiency further. We also present an enhancement, LaMDA++, incorporating a ``lite-weight" adaptive rank allocation for the LoRA path via normalized spectrum analysis of pre-trained model weights. We evaluate LaMDA/LaMDA++ across various tasks, including natural language understanding with the GLUE benchmark, text summarization, natural language generation, and complex reasoning on different LLMs. Results show that LaMDA matches or surpasses the performance of existing alternatives while requiring up to 17.7x fewer parameter updates and up to 1.32x lower peak GPU memory usage during fine-tuning. Code will be publicly available.

------------

`[2406.11944] Transcoders Find Interpretable LLM Feature Circuits <https://arxiv.org/abs/2406.11944>`__ 转码者寻找可解释的LLM特征电路

::

    Mon, 17 Jun 2024 17:49:00 GMT
    Jacob Dunefsky and Philippe Chlenski and Neel Nanda

A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities.
However, MLP sublayers make fine-grained circuit analysis on transformer-based language models difficult. In particular, interpretable features -- such as those found by sparse autoencoders (SAEs) -- are typically linear combinations of extremely many neurons, each with its own nonlinearity to account for.
Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. To address this we explore transcoders, which seek to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating MLP layer. We successfully train transcoders on language models with 120M, 410M, and 1.4B parameters, and find them to perform at least on par with SAEs in terms of sparsity, faithfulness, and human-interpretability. We then introduce a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers.
The resulting circuits neatly factorize into input-dependent and input-invariant terms. Finally, we apply transcoders to reverse-engineer unknown circuits in the model, and we obtain novel insights regarding the greater-than circuit in GPT2-small. Our results suggest that transcoders can prove effective in decomposing model computations involving MLPs into interpretable circuits. Code is available at https://github.com/jacobdunefsky/transcoder_circuits.

------------

`[2406.11945] GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs with Large Language Models <https://arxiv.org/abs/2406.11945>`__ GAugLLM:基于大型语言模型改进文本属性图的图对比学习

::

    Mon, 17 Jun 2024 17:49:19 GMT
    Yi Fang, Dongzhe Fan, Daochen Zha, Qiaoyu Tan

This work studies self-supervised graph learning for text-attributed graphs (TAGs) where nodes are represented by textual attributes. Unlike traditional graph contrastive methods that perturb the numerical feature space and alter the graph's topological structure, we aim to improve view generation through language supervision. This is driven by the prevalence of textual attributes in real applications, which complement graph structures with rich semantic information. However, this presents challenges because of two major reasons.
First, text attributes often vary in length and quality, making it difficulty to perturb raw text descriptions without altering their original semantic meanings. Second, although text attributes complement graph structures, they are not inherently well-aligned. To bridge the gap, we introduce GAugLLM, a novel framework for augmenting TAGs. It leverages advanced large language models like Mistral to enhance self-supervised graph learning. Specifically, we introduce a mixture-of-prompt-expert technique to generate augmented node features. This approach adaptively maps multiple prompt experts, each of which modifies raw text attributes using prompt engineering, into numerical feature space. Additionally, we devise a collaborative edge modifier to leverage structural and textual commonalities, enhancing edge augmentation by examining or building connections between nodes. Empirical results across five benchmark datasets spanning various domains underscore our framework's ability to enhance the performance of leading contrastive methods as a plug-in tool. Notably, we observe that the augmented features and graph structure can also enhance the performance of standard generative methods, as well as popular graph neural networks. The open-sourced implementation of our GAugLLM is available at Github.

------------

`[2406.12016] Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization <https://arxiv.org/abs/2406.12016>`__ 在大型语言模型量化中，前缀注意力槽可以缓解激活异常

::

    Mon, 17 Jun 2024 18:33:44 GMT
    Seungwoo Son, Wonpyo Park, Woohyun Han, Kyuyeun Kim, Jaeho Lee

Despite recent advances in LLM quantization, activation quantization remains to be challenging due to the activation outliers. Conventional remedies, e.g., mixing precisions for different channels, introduce extra overhead and reduce the speedup. In this work, we develop a simple yet effective strategy to facilitate per-tensor activation quantization by preventing the generation of problematic tokens. Precisely, we propose a method to find a set of key-value cache, coined CushionCache, which mitigates outliers in subsequent tokens when inserted as a prefix. CushionCache works in two steps: First, we greedily search for a prompt token sequence that minimizes the maximum activation values in subsequent tokens. Then, we further tune the token cache to regularize the activations of subsequent tokens to be more quantization-friendly. The proposed method successfully addresses activation outliers of LLMs, providing a substantial performance boost for per-tensor activation quantization methods.
We thoroughly evaluate our method over a wide range of models and benchmarks and find that it significantly surpasses the established baseline of per-tensor W8A8 quantization and can be seamlessly integrated with the recent activation quantization method.

------------

`[2406.12031] Large Scale Transfer Learning for Tabular Data via Language Modeling <https://arxiv.org/abs/2406.12031>`__ 基于语言建模的表格数据大规模迁移学习

::

    Mon, 17 Jun 2024 18:58:20 GMT
    Josh Gardner, Juan C. Perdomo, Ludwig Schmidt

Tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 1.6B rows from 3.1M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. We release our model, code, and data along with the publication of this paper.

------------

`[2406.12091] Is poisoning a real threat to LLM alignment? Maybe more so than you think <https://arxiv.org/abs/2406.12091>`__ 中毒是LLM联盟的真正威胁吗?可能比你想象的更严重

::

    Mon, 17 Jun 2024 21:06:00 GMT
    Pankayaraj Pathmanathan, Souradip Chakraborty, Xiangyu Liu, Yongyuan Liang, Furong Huang

Recent advancements in Reinforcement Learning with Human Feedback (RLHF) have significantly impacted the alignment of Large Language Models (LLMs). The sensitivity of reinforcement learning algorithms such as Proximal Policy Optimization (PPO) has led to new line work on Direct Policy Optimization (DPO), which treats RLHF in a supervised learning framework. The increased practical use of these RLHF methods warrants an analysis of their vulnerabilities. In this work, we investigate the vulnerabilities of DPO to poisoning attacks under different scenarios and compare the effectiveness of preference poisoning, a first of its kind. We comprehensively analyze DPO's vulnerabilities under different types of attacks, i.e., backdoor and non-backdoor attacks, and different poisoning methods across a wide array of language models, i.e., LLama 7B, Mistral 7B, and Gemma 7B. We find that unlike PPO-based methods, which, when it comes to backdoor attacks, require at least 4\% of the data to be poisoned to elicit harmful behavior, we exploit the true vulnerabilities of DPO more simply so we can poison the model with only as much as 0.5\% of the data. We further investigate the potential reasons behind the vulnerability and how well this vulnerability translates into backdoor vs non-backdoor attacks.

------------

`[2406.12168] BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM <https://arxiv.org/abs/2406.12168>`__ BPO:通过秉承行为邻近性LLM来强化在线偏好学习

::

    Tue, 18 Jun 2024 00:41:40 GMT
    Wenda Xu, Jiachen Li, William Yang Wang, Lei Li

Direct alignment from preferences (DAP) has emerged as a promising paradigm for aligning large language models (LLMs) to human desiderata from pre-collected, offline preference datasets. While recent studies indicate that existing offline DAP methods can directly benefit from online training samples, we highlight the need to develop specific online DAP algorithms to fully harness the power of online training. Specifically, we identify that the learned LLM should adhere to the proximity of the behavior LLM, which collects the training samples. To this end, we propose online Preference Optimization in proximity to the Behavior LLM (BPO), emphasizing the importance of constructing a proper trust region for LLM alignment.
We conduct extensive experiments to validate the effectiveness and applicability of our approach by integrating it with various DAP methods, resulting in significant performance improvements across a wide range of tasks when training with the same amount of preference data. Even when only introducing one additional data collection phase, our online BPO improves its offline DAP baseline from 72.0% to 80.2% on TL;DR and from 82.2% to 89.1% on Anthropic Helpfulness in terms of win rate against human reference text.

------------

`[2406.12246] TroL: Traversal of Layers for Large Language and Vision Models <https://arxiv.org/abs/2406.12246>`__ TroL:大型语言和视觉模型的层遍历

::

    Tue, 18 Jun 2024 03:42:00 GMT
    Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, Yong Man Ro

Large language and vision models (LLVMs) have been driven by the generalization power of large language models (LLMs) and the advent of visual instruction tuning. Along with scaling them up directly, these models enable LLVMs to showcase powerful vision language (VL) performances by covering diverse tasks via natural language instructions. However, existing open-source LLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often considered too large (e.g., 26B, 34B, and 110B parameters), having a larger number of layers. These large models demand costly, high-end resources for both training and inference. To address this issue, we present a new efficient LLVM family with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL), which enables the reuse of layers in a token-wise manner. This layer traversing technique simulates the effect of looking back and retracing the answering stream while increasing the number of forward propagation layers without physically adding more layers. We demonstrate that TroL employs a simple layer traversing approach yet efficiently outperforms the open-source LLVMs with larger model sizes and rivals the performances of the closed-source LLVMs with substantial sizes.

------------

`[2406.12334] What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering <https://arxiv.org/abs/2406.12334>`__ 我做错了什么?量化LLMs的敏感性和一致性以促进工程

::

    Tue, 18 Jun 2024 06:59:24 GMT
    Federico Errica, Giuseppe Siracusano, Davide Sanvito, Roberto Bifulco

Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging their inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely sensitivity and consistency, which are complementary to task performance. First, sensitivity measures changes of predictions across rephrasings of the prompt, and does not require access to ground truth labels.
Instead, consistency measures how predictions vary across rephrasings for elements of the same class. We perform an empirical comparison of these metrics on text classification tasks, using them as guideline for understanding failure modes of the LLM. Our hope is that sensitivity and consistency will be powerful allies in automatic prompt engineering frameworks to obtain LLMs that balance robustness with performance.

------------

`[2406.12360] UrbanLLM: Autonomous Urban Activity Planning and Management with Large Language Models <https://arxiv.org/abs/2406.12360>`__ UrbanLLM:基于大型语言模型的自主城市活动规划与管理

::

    Tue, 18 Jun 2024 07:41:42 GMT
    Yue Jiang, Qin Chao, Yile Chen, Xiucheng Li, Shuai Liu, Gao Cong

Location-based services play an critical role in improving the quality of our daily lives. Despite the proliferation of numerous specialized AI models within spatio-temporal context of location-based services, these models struggle to autonomously tackle problems regarding complex urban planing and management. To bridge this gap, we introduce UrbanLLM, a fine-tuned large language model (LLM) designed to tackle diverse problems in urban scenarios. UrbanLLM functions as a problem-solver by decomposing urban-related queries into manageable sub-tasks, identifying suitable spatio-temporal AI models for each sub-task, and generating comprehensive responses to the given queries. Our experimental results indicate that UrbanLLM significantly outperforms other established LLMs, such as Llama and the GPT series, in handling problems concerning complex urban activity planning and management. UrbanLLM exhibits considerable potential in enhancing the effectiveness of solving problems in urban scenarios, reducing the workload and reliance for human experts.

------------

`[2406.12569] MOYU: A Theoretical Study on Massive Over-activation Yielded Uplifts in LLMs <https://arxiv.org/abs/2406.12569>`__ 墨玉:LLMs中大规模过度活化的理论研究

::

    Tue, 18 Jun 2024 12:57:33 GMT
    Chi Ma, Mincong Huang, Chao Wang, Yujie Wang, Lei Yu, Chuan Liu, Wei Lin

Massive Over-activation Yielded Uplifts(MOYU) is an inherent property of large language models, and dynamic activation(DA) based on the MOYU property is a clever yet under-explored strategy designed to accelerate inference in these models. Existing methods that utilize MOYU often face a significant 'Impossible Trinity': struggling to simultaneously maintain model performance, enhance inference speed, and extend applicability across various architectures. Due to the theoretical ambiguities surrounding MOYU, this paper elucidates the root cause of the MOYU property and outlines the mechanisms behind two primary limitations encountered by current DA methods: 1) history-related activation uncertainty, and 2) semantic-irrelevant activation inertia. Our analysis not only underscores the limitations of current dynamic activation strategies within large-scale LLaMA models but also proposes opportunities for refining the design of future sparsity schemes.

------------

`[2406.12649] Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models <https://arxiv.org/abs/2406.12649>`__ 

::

    Tue, 18 Jun 2024 14:17:57 GMT
    Hengyi Wang, Shiwei Tan, Hao Wang

Vision transformers (ViTs) have emerged as a significant area of focus, particularly for their capacity to be jointly trained with large language models and to serve as robust vision foundation models. Yet, the development of trustworthy explanation methods for ViTs has lagged, particularly in the context of post-hoc interpretations of ViT predictions. Existing sub-image selection approaches, such as feature-attribution and conceptual models, fall short in this regard. This paper proposes five desiderata for explaining ViTs -- faithfulness, stability, sparsity, multi-level structure, and parsimony -- and demonstrates the inadequacy of current methods in meeting these criteria comprehensively. We introduce a variational Bayesian explanation framework, dubbed ProbAbilistic Concept Explainers (PACE), which models the distributions of patch embeddings to provide trustworthy post-hoc conceptual explanations.
Our qualitative analysis reveals the distributions of patch-level concepts, elucidating the effectiveness of ViTs by modeling the joint distribution of patch embeddings and ViT's predictions. Moreover, these patch-level explanations bridge the gap between image-level and dataset-level explanations, thus completing the multi-level structure of PACE. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that PACE surpasses state-of-the-art methods in terms of the defined desiderata.

------------

`[2406.12845] Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts <https://arxiv.org/abs/2406.12845>`__ 基于多目标奖励模型和专家混合的可解释偏好

::

    Tue, 18 Jun 2024 17:58:28 GMT
    Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang

Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. The RLHF process typically starts by training a reward model (RM) using human preference data. Conventional RMs are trained on pairwise responses to the same user request, with relative ratings indicating which response humans prefer.
The trained RM serves as a proxy for human preferences. However, due to the black-box nature of RMs, their outputs lack interpretability, as humans cannot intuitively understand why an RM thinks a response is good or not. As RMs act as human preference proxies, we believe they should be human-interpretable to ensure that their internal decision processes are consistent with human preferences and to prevent reward hacking in LLM alignment. To build RMs with interpretable preferences, we propose a two-stage approach: i) train an Absolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional absolute-rating data, each dimension corresponding to a human-interpretable objective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts (MoE) strategy with a gating network that automatically selects the most suitable reward objectives based on the context. We efficiently trained an ArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top of the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art performance on RewardBench, a benchmark evaluating RMs for language modeling.
Notably, the performance of our model surpasses the LLM-as-a-judge method with GPT-4 judges by a margin, and approaches the performance of the much larger Nemotron-4 340B reward model.

------------

`[2406.11884] Hierarchical Compression of Text-Rich Graphs via Large Language Models <https://arxiv.org/abs/2406.11884>`__ 基于大型语言模型的富文本图分层压缩

::

    Thu, 13 Jun 2024 07:24:46 GMT
    Shichang Zhang, Da Zheng, Jiani Zhang, Qi Zhu, Xiang song, Soji Adeshina, Christos Faloutsos, George Karypis, Yizhou Sun

Text-rich graphs, prevalent in data mining contexts like e-commerce and academic graphs, consist of nodes with textual features linked by various relations. Traditional graph machine learning models, such as Graph Neural Networks (GNNs), excel in encoding the graph structural information, but have limited capability in handling rich text on graph nodes. Large Language Models (LLMs), noted for their superior text understanding abilities, offer a solution for processing the text in graphs but face integration challenges due to their limitation for encoding graph structures and their computational complexities when dealing with extensive text in large neighborhoods of interconnected nodes. This paper introduces ``Hierarchical Compression'' (HiCom), a novel method to align the capabilities of LLMs with the structure of text-rich graphs. HiCom processes text in a node's neighborhood in a structured manner by organizing the extensive textual information into a more manageable hierarchy and compressing node text step by step. Therefore, HiCom not only preserves the contextual richness of the text but also addresses the computational challenges of LLMs, which presents an advancement in integrating the text processing power of LLMs with the structural complexities of text-rich graphs. Empirical results show that HiCom can outperform both GNNs and LLM backbones for node classification on e-commerce and citation graphs. HiCom is especially effective for nodes from a dense region in a graph, where it achieves a 3.48% average performance improvement on five datasets while being more efficient than LLM backbones.

------------

`[2406.11925] DocCGen: Document-based Controlled Code Generation <https://arxiv.org/abs/2406.11925>`__ docgen:基于文档的受控代码生成

::

    Mon, 17 Jun 2024 08:34:57 GMT
    Sameer Pimparkhede, Mehant Kammakomati, Srikanth G. Tamilselvam, Prince Kumar, Ashok Pon Kumar, Pushpak Bhattacharyya

Recent developments show that Large Language Models (LLMs) produce state-of-the-art performance on natural language (NL) to code generation for resource-rich general-purpose languages like C++, Java, and Python. However, their practical usage for structured domain-specific languages (DSLs) such as YAML, JSON is limited due to domain-specific schema, grammar, and customizations generally unseen by LLMs during pre-training. Efforts have been made to mitigate this challenge via in-context learning through relevant examples or by fine-tuning. However, it suffers from problems, such as limited DSL samples and prompt sensitivity but enterprises maintain good documentation of the DSLs. Therefore, we propose DocCGen, a framework that can leverage such rich knowledge by breaking the NL-to-Code generation task for structured code languages into a two-step process. First, it detects the correct libraries using the library documentation that best matches the NL query. Then, it utilizes schema rules extracted from the documentation of these libraries to constrain the decoding. We evaluate our framework for two complex structured languages, Ansible YAML and Bash command, consisting of two settings: Out-of-domain (OOD) and In-domain (ID). Our extensive experiments show that DocCGen consistently improves different-sized language models across all six evaluation metrics, reducing syntactic and semantic errors in structured code.
We plan to open-source the datasets and code to motivate research in constrained code generation.

------------

`[2406.11930] A Critical Study of What Code-LLMs (Do Not) Learn <https://arxiv.org/abs/2406.11930>`__ 对llm(不)学习什么代码的批判性研究

::

    Mon, 17 Jun 2024 13:11:17 GMT
    Abhinav Anand, Shweta Verma, Krishna Narasimhan and Mira Mezini

Large Language Models trained on code corpora (code-LLMs) have demonstrated impressive performance in various coding assistance tasks. However, despite their increased size and training dataset, code-LLMs still have limitations such as suggesting codes with syntactic errors, variable misuse etc. Some studies argue that code-LLMs perform well on coding tasks because they use self-attention and hidden representations to encode relations among input tokens. However, previous works have not studied what code properties are not encoded by code-LLMs. In this paper, we conduct a fine-grained analysis of attention maps and hidden representations of code-LLMs. Our study indicates that code-LLMs only encode relations among specific subsets of input tokens.
Specifically, by categorizing input tokens into syntactic tokens and identifiers, we found that models encode relations among syntactic tokens and among identifiers, but they fail to encode relations between syntactic tokens and identifiers. We also found that fine-tuned models encode these relations poorly compared to their pre-trained counterparts. Additionally, larger models with billions of parameters encode significantly less information about code than models with only a few hundred million parameters.

------------

`[2406.11935] Iterative or Innovative? A Problem-Oriented Perspective for Code Optimization <https://arxiv.org/abs/2406.11935>`__ 迭代还是创新?以问题为导向的代码优化视角

::

    Mon, 17 Jun 2024 16:10:10 GMT
    Tong Ye, Tengfei Ma, Lingfei Wu, Xuhong Zhang, Shouling Ji, Wenhai Wang

Large language models (LLMs) have demonstrated strong capabilities in solving a wide range of programming tasks. However, LLMs have rarely been explored for code optimization. In this paper, we explore code optimization with a focus on performance enhancement, specifically aiming to optimize code for minimal execution time. The recently proposed first PIE dataset for performance optimization constructs program optimization pairs based on iterative submissions from the same programmer for the same problem. However, this approach restricts LLMs to local performance improvements, neglecting global algorithmic innovation. Therefore, we adopt a completely different perspective by reconstructing the optimization pairs into a problem-oriented approach. This allows for the integration of various ingenious ideas from different programmers tackling the same problem. Experimental results demonstrate that adapting LLMs to problem-oriented optimization pairs significantly enhances their optimization capabilities. Meanwhile, we identified performance bottlenecks within the problem-oriented perspective. By employing model merge, we further overcame bottlenecks and ultimately elevated the program optimization ratio ($51.76\%\rightarrow76.65\%$) and speedup ($2.65\times\rightarrow5.09\times$) to new levels.

------------

`[2406.12020] When Box Meets Graph Neural Network in Tag-aware Recommendation <https://arxiv.org/abs/2406.12020>`__ 标签感知推荐中Box与图神经网络相遇时

::

    Mon, 17 Jun 2024 18:35:53 GMT
    Fake Lin, Ziwei Zhao, Xi Zhu, Da Zhang, Shitian Shen, Xueying Li, Tong Xu, Suojuan Zhang, Enhong Chen

Last year has witnessed the re-flourishment of tag-aware recommender systems supported by the LLM-enriched tags. Unfortunately, though large efforts have been made, current solutions may fail to describe the diversity and uncertainty inherent in user preferences with only tag-driven profiles. Recently, with the development of geometry-based techniques, e.g., box embedding, diversity of user preferences now could be fully modeled as the range within a box in high dimension space. However, defect still exists as these approaches are incapable of capturing high-order neighbor signals, i.e., semantic-rich multi-hop relations within the user-tag-item tripartite graph, which severely limits the effectiveness of user modeling. To deal with this challenge, in this paper, we propose a novel algorithm, called BoxGNN, to perform the message aggregation via combination of logical operations, thereby incorporating high-order signals. Specifically, we first embed users, items, and tags as hyper-boxes rather than simple points in the representation space, and define two logical operations to facilitate the subsequent process. Next, we perform the message aggregation mechanism via the combination of logical operations, to obtain the corresponding high-order box representations. Finally, we adopt a volume-based learning objective with Gumbel smoothing techniques to refine the representation of boxes. Extensive experiments on two publicly available datasets and one LLM-enhanced e-commerce dataset have validated the superiority of BoxGNN compared with various state-of-the-art baselines. The code is released online

------------

`[2406.12108] Computing in the Life Sciences: From Early Algorithms to Modern AI <https://arxiv.org/abs/2406.12108>`__ 生命科学中的计算:从早期算法到现代人工智能

::

    Mon, 17 Jun 2024 21:36:52 GMT
    Samuel A. Donkor, Matthew E. Walsh, and Alexander J. Titus

Computing in the life sciences has undergone a transformative evolution, from early computational models in the 1950s to the applications of artificial intelligence (AI) and machine learning (ML) seen today. This paper highlights key milestones and technological advancements through the historical development of computing in the life sciences. The discussion includes the inception of computational models for biological processes, the advent of bioinformatics tools, and the integration of AI/ML in modern life sciences research. Attention is given to AI-enabled tools used in the life sciences, such as scientific large language models and bio-AI tools, examining their capabilities, limitations, and impact to biological risk. This paper seeks to clarify and establish essential terminology and concepts to ensure informed decision-making and effective communication across disciplines.

------------

`[2406.12243] CherryRec: Enhancing News Recommendation Quality via LLM-driven Framework <https://arxiv.org/abs/2406.12243>`__ CherryRec:基于llm驱动框架提高新闻推荐质量

::

    Tue, 18 Jun 2024 03:33:38 GMT
    Shaohuang Wang, Lun Wang, Yunhan Bu, Tianwei Huang

Large Language Models (LLMs) have achieved remarkable progress in language understanding and generation. Custom LLMs leveraging textual features have been applied to recommendation systems, demonstrating improvements across various recommendation scenarios. However, most existing methods perform untrained recommendation based on pre-trained knowledge (e.g., movie recommendation), and the auto-regressive generation of LLMs leads to slow inference speeds, making them less effective in real-time recommendations.To address this, we propose a framework for news recommendation using LLMs, named \textit{CherryRec}, which ensures the quality of recommendations while accelerating the recommendation process. Specifically, we employ a Knowledge-aware News Rapid Selector to retrieve candidate options based on the user's interaction history. The history and retrieved items are then input as text into a fine-tuned LLM, the Content-aware News Llm Evaluator, designed to enhance news recommendation capabilities. Finally, the Value-aware News Scorer integrates the scores to compute the CherryRec Score, which serves as the basis for the final recommendation.We validate the effectiveness of the proposed framework by comparing it with state-of-the-art baseline methods on benchmark datasets. Our experimental results consistently show that CherryRec outperforms the baselines in both recommendation performance and efficiency.The project resource can be accessed at: \url{https://github.com/xxxxxx}

------------

`[2406.12296] Generative Artificial Intelligence-Guided User Studies: An Application for Air Taxi Services <https://arxiv.org/abs/2406.12296>`__ 生成式人工智能引导的用户研究——面向空中出租车服务的应用

::

    Tue, 18 Jun 2024 06:00:18 GMT
    Shengdi Xiao, Jingjing Li, Tatsuki Fushimi, Yoichi Ochiai

User studies are crucial for meeting user needs. In user studies, real experimental scenarios and participants are constructed and recruited. However, emerging and unfamiliar studies face limitations, including safety concerns and iterative efficiency. To address these challenges, this study utilizes a large language model (LLM) to create generative AI virtual scenarios for user experience. By recruiting real users to evaluate this experience, we can collect feedback that enables rapid iteration in the early design phase. The air taxi is particularly representative of these challenges and has been chosen as the case study for this research. The key contribution was designing a virtual ATJ using OpenAI's GPT-4 model and AI image and video generators. Based on the LLM-generated scripts, key visuals were created for the air taxi, and the ATJ was evaluated by 72 participants. Furthermore, the LLM demonstrated the ability to identify and suggest environments that significantly improve participants' attitudes toward air taxis. Education level and gender significantly influenced participants' attitudes and their satisfaction with the ATJ. Our study confirms the capability of generative AI to support user studies, providing a feasible approach and valuable insights for designing air taxi user experiences in the early design phase.

------------

`[2406.12479] RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding <https://arxiv.org/abs/2406.12479>`__ 

::

    Tue, 18 Jun 2024 10:34:28 GMT
    Linrui Xu, Ling Zhao, Wang Guo, Qiujun Li, Kewang Long, Kaiqi Zou, Yuhan Wang, Haifeng Li

The remote sensing image intelligence understanding model is undergoing a new profound paradigm shift which has been promoted by multi-modal large language model (MLLM), i.e. from the paradigm learning a domain model (LaDM) shifts to paradigm learning a pre-trained general foundation model followed by an adaptive domain model (LaGD). Under the new LaGD paradigm, the old datasets, which have led to advances in RSI intelligence understanding in the last decade, are no longer suitable for fire-new tasks. We argued that a new dataset must be designed to lighten tasks with the following features: 1) Generalization: training model to learn shared knowledge among tasks and to adapt to different tasks; 2) Understanding complex scenes: training model to understand the fine-grained attribute of the objects of interest, and to be able to describe the scene with natural language; 3) Reasoning: training model to be able to realize high-level visual reasoning. In this paper, we designed a high-quality, diversified, and unified multimodal instruction-following dataset for RSI understanding produced by GPT-4V and existing datasets, which we called RS-GPT4V. To achieve generalization, we used a (Question, Answer) which was deduced from GPT-4V via instruction-following to unify the tasks such as captioning and localization; To achieve complex scene, we proposed a hierarchical instruction description with local strategy in which the fine-grained attributes of the objects and their spatial relationships are described and global strategy in which all the local information are integrated to yield detailed instruction descript; To achieve reasoning, we designed multiple-turn QA pair to provide the reasoning ability for a model. The empirical results show that the fine-tuned MLLMs by RS-GPT4V can describe fine-grained information. The dataset is available at: https://github.com/GeoX-Lab/RS-GPT4V.

------------

`[2406.12529] LLM4MSR: An LLM-Enhanced Paradigm for Multi-Scenario Recommendation <https://arxiv.org/abs/2406.12529>`__ LLM4MSR:一种llm增强的多场景推荐范式

::

    Tue, 18 Jun 2024 11:59:36 GMT
    Yuhao Wang, Yichao Wang, Zichuan Fu, Xiangyang Li, Xiangyu Zhao, Huifeng Guo, Ruiming Tang

As the demand for more personalized recommendation grows and a dramatic boom in commercial scenarios arises, the study on multi-scenario recommendation (MSR) has attracted much attention, which uses the data from all scenarios to simultaneously improve their recommendation performance. However, existing methods tend to integrate insufficient scenario knowledge and neglect learning personalized cross-scenario preferences, thus leading to suboptimal performance and inadequate interpretability. Meanwhile, though large language model (LLM) has shown great capability of reasoning and capturing semantic information, the high inference latency and high computation cost of tuning hinder its implementation in industrial recommender systems. To fill these gaps, we propose an effective efficient interpretable LLM-enhanced paradigm LLM4MSR in this work. Specifically, we first leverage LLM to uncover multi-level knowledge including scenario correlations and users' cross-scenario interests from the designed scenario- and user-level prompt without fine-tuning the LLM, then adopt hierarchical meta networks to generate multi-level meta layers to explicitly improves the scenario-aware and personalized recommendation capability. Our experiments on KuaiSAR-small, KuaiSAR, and Amazon datasets validate two significant advantages of LLM4MSR: (i) the effectiveness and compatibility with different multi-scenario backbone models (achieving 1.5%, 1%, and 40% AUC improvement on three datasets), (ii) high efficiency and deployability on industrial recommender systems, and (iii) improved interpretability. The implemented code and data is available to ease reproduction.

------------

`[2406.12651] Transforming Surgical Interventions with Embodied Intelligence for Ultrasound Robotics <https://arxiv.org/abs/2406.12651>`__ 用具身智能改造超声机器人的手术干预

::

    Tue, 18 Jun 2024 14:22:16 GMT
    Huan Xu, Jinlin Wu, Guanglin Cao, Zhen Chen, Zhen Lei, Hongbin Liu

Ultrasonography has revolutionized non-invasive diagnostic methodologies, significantly enhancing patient outcomes across various medical domains.
Despite its advancements, integrating ultrasound technology with robotic systems for automated scans presents challenges, including limited command understanding and dynamic execution capabilities. To address these challenges, this paper introduces a novel Ultrasound Embodied Intelligence system that synergistically combines ultrasound robots with large language models (LLMs) and domain-specific knowledge augmentation, enhancing ultrasound robots' intelligence and operational efficiency. Our approach employs a dual strategy: firstly, integrating LLMs with ultrasound robots to interpret doctors' verbal instructions into precise motion planning through a comprehensive understanding of ultrasound domain knowledge, including APIs and operational manuals; secondly, incorporating a dynamic execution mechanism, allowing for real-time adjustments to scanning plans based on patient movements or procedural errors.
We demonstrate the effectiveness of our system through extensive experiments, including ablation studies and comparisons across various models, showcasing significant improvements in executing medical procedures from verbal commands.
Our findings suggest that the proposed system improves the efficiency and quality of ultrasound scans and paves the way for further advancements in autonomous medical scanning technologies, with the potential to transform non-invasive diagnostics and streamline medical workflows.

------------

`[2406.11880] Knowledge Return Oriented Prompting (KROP) <https://arxiv.org/abs/2406.11880>`__ 

::

    Tue, 11 Jun 2024 23:58:37 GMT
    Jason Martin, Kenneth Yeung

Many Large Language Models (LLMs) and LLM-powered apps deployed today use some form of prompt filter or alignment to protect their integrity. However, these measures aren't foolproof. This paper introduces KROP, a prompt injection technique capable of obfuscating prompt injection attacks, rendering them virtually undetectable to most of these security measures.

------------

`[2403.05680] How Well Do Multi-modal LLMs Interpret CT Scans? An Auto-Evaluation Framework for Analyses <https://arxiv.org/abs/2403.05680>`__ 多模态llm如何解释CT扫描?用于分析的自动评估框架

::

    replaced with revised version Tue, 18 Jun 2024 12:43:18 GMT
    Submission history From: Qingqing Zhu [view email]
    [v1] Fri, 8 Mar 2024 21:16:28 UTC (1,350 KB)
    [v2] Tue, 18 Jun 2024 12:43:18 UTC (1,677 KB)
    Qingqing Zhu, Benjamin Hou, Tejas S. Mathai, Pritam Mukherjee, Qiao Jin, Xiuying Chen, Zhizheng Wang, Ruida Cheng, Ronald M. Summers, and Zhiyong Lu

Automatically interpreting CT scans can ease the workload of radiologists. However, this is challenging mainly due to the scarcity of adequate datasets and reference standards for evaluation. This study aims to bridge this gap by introducing a novel evaluation framework, named ``GPTRadScore''. This framework assesses the capabilities of multi-modal LLMs, such as GPT-4 with Vision (GPT-4V), Gemini Pro Vision, LLaVA-Med, and RadFM, in generating descriptions for prospectively-identified findings. By employing a decomposition technique based on GPT-4, GPTRadScore compares these generated descriptions with gold-standard report sentences, analyzing their accuracy in terms of body part, location, and type of finding. Evaluations demonstrated a high correlation with clinician assessments and highlighted its potential over traditional metrics, such as BLEU, METEOR, and ROUGE. Furthermore, to contribute to future studies, we plan to release a benchmark dataset annotated by clinicians. Using GPTRadScore, we found that while GPT-4V and Gemini Pro Vision fare better, their performance revealed significant areas for improvement, primarily due to limitations in the dataset used for training these models. To demonstrate this potential, RadFM was fine-tuned and it resulted in significant accuracy improvements: location accuracy rose from 3.41\% to 12.8\%, body part accuracy from 29.12\% to 53\%, and type accuracy from 9.24\% to 30\%, thereby validating our hypothesis.

------------

`[2404.10160] Reinforcement Learning from Multi-role Debates as Feedback for Bias Mitigation in LLMs <https://arxiv.org/abs/2404.10160>`__ 多角色辩论中的强化学习作为llm中偏差缓解的反馈

::

    replaced with revised version Tue, 18 Jun 2024 16:19:40 GMT
    Submission history From: Rosy Cheng [view email]
    [v1] Mon, 15 Apr 2024 22:18:50 UTC (2,079 KB)
    [v2] Sun, 28 Apr 2024 04:08:39 UTC (2,239 KB)
    [v3] Wed, 12 Jun 2024 12:42:28 UTC (2,829 KB)
    [v4] Sun, 16 Jun 2024 16:34:42 UTC (1 KB) (withdrawn)
    [v5] Tue, 18 Jun 2024 16:19:40 UTC (4,572 KB)
    Ruoxi Cheng, Haoxuan Ma, Shuirong Cao, Jiaqi Li, Aihua Pei, Zhiqiang Wang, Pengliang Ji, Haoyu Wang, Jiaqi Huo

Bias in LLMs can harm user experience and societal outcomes. However, current bias mitigation methods often require intensive human feedback, lack transferability to other topics or yield overconfident and random outputs. We find that involving LLMs in role-playing scenario boosts their ability to recognize and mitigate biases. Based on this, we propose Reinforcement Learning from Multi-role Debates as Feedback (RLDF), a novel approach for bias mitigation replacing human feedback in traditional RLHF. We utilize LLMs in multi-role debates to create a dataset that includes both high-bias and low-bias instances for training the reward model in reinforcement learning. Our approach comprises two modes: (1) self-reflection, where the same LLM participates in multi-role debates, and (2) teacher-student, where a more advanced LLM like GPT-3.5-turbo guides the LLM to perform this task. Experimental results across different LLMs demonstrate the effectiveness of our approach in bias mitigation.

------------

`[2305.12280] Contextualizing Argument Quality Assessment with Relevant Knowledge <https://arxiv.org/abs/2305.12280>`__ 结合相关知识语境化论证质量评估

::

    replaced with revised version Tue, 18 Jun 2024 03:11:21 GMT
    Submission history From: Darshan Deshpande [view email]
    [v1] Sat, 20 May 2023 21:04:58 UTC (1,762 KB)
    [v2] Wed, 8 Nov 2023 18:41:44 UTC (1,001 KB)
    [v3] Tue, 18 Jun 2024 03:11:21 UTC (993 KB)
    Darshan Deshpande, Zhivar Sourati, Filip Ilievski, Fred Morstatter

Automatic assessment of the quality of arguments has been recognized as a challenging task with significant implications for misinformation and targeted speech. While real-world arguments are tightly anchored in context, existing computational methods analyze their quality in isolation, which affects their accuracy and generalizability. We propose SPARK: a novel method for scoring argument quality based on contextualization via relevant knowledge. We devise four augmentations that leverage large language models to provide feedback, infer hidden assumptions, supply a similar-quality argument, or give a counter-argument. SPARK uses a dual-encoder Transformer architecture to enable the original argument and its augmentation to be considered jointly. Our experiments in both in-domain and zero-shot setups show that SPARK consistently outperforms existing techniques across multiple metrics.

------------

`[2307.00279] Let Me Teach You: Pedagogical Foundations of Feedback for Language Models <https://arxiv.org/abs/2307.00279>`__ 让我教你:语言模型反馈的教学基础

::

    replaced with revised version Tue, 18 Jun 2024 16:55:12 GMT
    Submission history From: Beatriz Borges [view email]
    [v1] Sat, 1 Jul 2023 09:18:24 UTC (9,392 KB)
    [v2] Tue, 18 Jun 2024 16:55:12 UTC (953 KB)
    Beatriz Borges, Niket Tandon, Tanja K\"aser, Antoine Bosselut

Natural Language Feedback (NLF) is an increasingly popular mechanism for aligning Large Language Models (LLMs) to human preferences. Despite the diversity of the information it can convey, NLF methods are often hand-designed and arbitrary, with little systematic grounding. At the same time, research in learning sciences has long established several effective feedback models. In this opinion piece, we compile ideas from pedagogy to introduce FELT, a feedback framework for LLMs that outlines various characteristics of the feedback space, and a feedback content taxonomy based on these variables, providing a general mapping of the feedback space. In addition to streamlining NLF designs, FELT also brings out new, unexplored directions for research in NLF. We make our taxonomy available to the community, providing guides and examples for mapping our categorizations to future research.

------------

`[2310.14735] Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review <https://arxiv.org/abs/2310.14735>`__ 释放大型语言模型中prompt工程的潜力:全面综述

::

    replaced with revised version Tue, 18 Jun 2024 16:21:12 GMT
    Submission history From: Zhaofeng Zhang [view email]
    [v1] Mon, 23 Oct 2023 09:15:18 UTC (238 KB)
    [v2] Fri, 27 Oct 2023 14:22:43 UTC (239 KB)
    [v3] Tue, 28 May 2024 16:38:19 UTC (343 KB)
    [v4] Tue, 18 Jun 2024 16:21:12 UTC (239 KB)
    Banghao Chen, Zhaofeng Zhang, Nicolas Langren\'e, Shengxin Zhu

This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we gather information about the application of prompt engineering in such fields as education and programming, showing its transformative potential. This comprehensive survey aims to serve as a friendly guide for anyone venturing through the big world of LLMs and prompt engineering.

------------

`[2401.06104] Transformers are Multi-State RNNs <https://arxiv.org/abs/2401.06104>`__ transformer是多状态rnn

::

    replaced with revised version Tue, 18 Jun 2024 09:16:14 GMT
    Submission history From: Matanel Oren [view email]
    [v1] Thu, 11 Jan 2024 18:35:26 UTC (803 KB)
    [v2] Tue, 18 Jun 2024 09:16:14 UTC (7,659 KB)
    Matanel Oren, Michael Hassid, Nir Yarden, Yossi Adi, Roy Schwartz

Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as unbounded multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that transformers can be converted into $\textit{bounded}$ multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache. We introduce a novel, training-free compression policy - $\textbf{T}$oken $\textbf{O}$mission $\textbf{V}$ia $\textbf{A}$ttention (TOVA). Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only $\frac{1}{8}$ of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs' most painful computational bottlenecks - the size of their key-value cache. We publicly release our code at this https URL

------------

`[2401.07867] Authorship Obfuscation in Multilingual Machine-Generated Text Detection <https://arxiv.org/abs/2401.07867>`__ 多语言机器生成文本检测中的作者混淆问题

::

    replaced with revised version Tue, 18 Jun 2024 12:54:59 GMT
    Submission history From: Dominik Macko [view email]
    [v1] Mon, 15 Jan 2024 17:57:41 UTC (8,365 KB)
    [v2] Tue, 18 Jun 2024 12:54:59 UTC (8,353 KB)
    Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason Samuel Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko, Maria Bielikova

High-quality text generation capability of recent Large Language Models (LLMs) causes concerns about their misuse (e.g., in massive generation/spread of disinformation). Machine-generated text (MGT) detection is important to cope with such threats. However, it is susceptible to authorship obfuscation (AO) methods, such as paraphrasing, which can cause MGTs to evade detection. So far, this was evaluated only in monolingual settings. Thus, the susceptibility of recently proposed multilingual detectors is still unknown. We fill this gap by comprehensively benchmarking the performance of 10 well-known AO methods, attacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10 $\times$ 37 $\times$ 11 = 4,070 combinations). We also evaluate the effect of data augmentation on adversarial robustness using obfuscated texts. The results indicate that all tested AO methods can cause evasion of automated detection in all tested languages, where homoglyph attacks are especially successful. However, some of the AO methods severely damaged the text, making it no longer readable or easily recognizable by humans (e.g., changed language, weird characters).

------------

`[2402.06900] Can LLMs Recognize Toxicity? Definition-Based Toxicity Metric <https://arxiv.org/abs/2402.06900>`__ LLMs能识别毒性吗?基于定义的毒性度量

::

    replaced with revised version Tue, 18 Jun 2024 04:35:12 GMT
    Submission history From: Hyukhun Koh [view email]
    [v1] Sat, 10 Feb 2024 07:55:27 UTC (2,750 KB)
    [v2] Fri, 16 Feb 2024 12:01:33 UTC (2,732 KB)
    [v3] Tue, 18 Jun 2024 04:35:12 UTC (2,542 KB)
    Hyukhun Koh, Dohyung Kim, Minwoo Lee, and Kyomin Jung

In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to detect the toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets, which are susceptible to out-of-distribution (OOD) problems and depend on the dataset's definition of toxicity. In this paper, we introduce a robust metric grounded on LLMs to flexibly measure toxicity according to the given definition. We first analyze the toxicity factors, followed by an examination of the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Finally, we evaluate the performance of our metric with detailed analysis. Our empirical results demonstrate outstanding performance in measuring toxicity within verified factors, improving on conventional metrics by 12 points in the F1 score. Our findings also indicate that upstream toxicity significantly influences downstream metrics, suggesting that LLMs are unsuitable for toxicity evaluations within unverified factors.

------------

`[2402.11436] Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement <https://arxiv.org/abs/2402.11436>`__ 傲慢与偏见:LLM在自我完善中放大自我偏见

::

    replaced with revised version Tue, 18 Jun 2024 04:41:07 GMT
    Submission history From: Wenda Xu [view email]
    [v1] Sun, 18 Feb 2024 03:10:39 UTC (1,709 KB)
    [v2] Tue, 18 Jun 2024 04:41:07 UTC (1,823 KB)
    Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, William Yang Wang

Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM's bias in evaluating their own output. In this paper, we formally define LLM's self-bias - the tendency to favor its own generation - using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks. The code and data are released at this https URL.

------------

`[2402.11725] How Susceptible are Large Language Models to Ideological Manipulation? <https://arxiv.org/abs/2402.11725>`__ 大型语言模型受意识形态操纵的影响有多大?

::

    replaced with revised version Tue, 18 Jun 2024 05:14:02 GMT
    Submission history From: Zihao He [view email]
    [v1] Sun, 18 Feb 2024 22:36:19 UTC (2,290 KB)
    [v2] Thu, 22 Feb 2024 19:12:09 UTC (2,290 KB)
    [v3] Tue, 18 Jun 2024 05:14:02 UTC (2,392 KB)
    Kai Chen, Zihao He, Jun Yan, Taiwei Shi, Kristina Lerman

Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on LLMs.

------------

`[2402.12835] PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs <https://arxiv.org/abs/2402.12835>`__ PANDA:增强llm领域能力的偏好自适应

::

    replaced with revised version Tue, 18 Jun 2024 03:08:37 GMT
    Submission history From: An Liu [view email]
    [v1] Tue, 20 Feb 2024 09:02:55 UTC (1,099 KB)
    [v2] Tue, 18 Jun 2024 03:08:37 UTC (7,557 KB)
    An Liu, Zonghan Yang, Zhenhe Zhang, Qingyuan Hu, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu

While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that being learned on 4 tasks of ScienceWorld. This finding highlights the potential of exploring tuning-free approaches to achieve weak-to-strong generalization.

------------

`[2402.16367] Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications <https://arxiv.org/abs/2402.16367>`__ 

::

    replaced with revised version Mon, 17 Jun 2024 17:57:32 GMT
    Submission history From: Weize Liu [view email]
    [v1] Mon, 26 Feb 2024 07:44:56 UTC (9,341 KB)
    [v2] Mon, 17 Jun 2024 17:57:32 UTC (24,594 KB)
    Weize Liu, Yinlong Xu, Hongxia Xu, Jintai Chen, Xuming Hu, Jian Wu

Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of NLP, but still lack understanding of their internal activities when processing different languages. We designed a method to convert dense LLMs into fine-grained MoE architectures, and then visually studied the multilingual activation patterns of LLMs through expert activation frequency heatmaps. Through comprehensive experiments on different model families, different model sizes, and different variants, we analyzed the distribution of high-frequency activated experts, multilingual shared experts, whether the activation patterns of different languages are related to language families, and the impact of instruction tuning on activation patterns. We further explored leveraging the discovered differences in expert activation frequencies to guide unstructured pruning in two different ways. Experimental results demonstrated that our method significantly outperformed random expert pruning and even exceeded the performance of the original unpruned models in some languages. Additionally, we found that configuring different pruning rates for different layers based on activation level differences could achieve better results. Our findings reveal the multilingual processing mechanisms within LLMs and utilize these insights to offer new perspectives for applications such as model pruning.

------------

`[2403.04945] MEIT: Multi-Modal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation <https://arxiv.org/abs/2403.04945>`__ MEIT:面向报告生成的大型语言模型多模态心电图指令调优

::

    replaced with revised version Tue, 18 Jun 2024 07:15:09 GMT
    Submission history From: Zhongwei Wan [view email]
    [v1] Thu, 7 Mar 2024 23:20:56 UTC (4,140 KB)
    [v2] Wed, 13 Mar 2024 06:20:47 UTC (4,141 KB)
    [v3] Tue, 18 Jun 2024 07:15:09 UTC (7,920 KB)
    Zhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Zhenwu Peng, Jie Fu, Rossella Arcucci, Huaxiu Yao, Mi Zhang

Electrocardiogram (ECG) is the primary non-invasive diagnostic tool for monitoring cardiac conditions and is crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is time-consuming and requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the first attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open-source LLMs using more than 800,000 ECG reports. MEIT's results underscore the superior performance of instruction-tuned LLMs, showcasing their proficiency in quality report generation, zero-shot capabilities, and resilience to signal perturbation. These findings emphasize the efficacy of our MEIT framework and its potential for real-world clinical application.

------------

`[2403.15268] Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models <https://arxiv.org/abs/2403.15268>`__ 想象力增强生成:学习在大型语言模型上想象更丰富的上下文进行问答

::

    replaced with revised version Tue, 18 Jun 2024 07:21:24 GMT
    Submission history From: Huanxuan Liao [view email]
    [v1] Fri, 22 Mar 2024 15:06:45 UTC (764 KB)
    [v2] Thu, 28 Mar 2024 16:28:24 UTC (764 KB)
    [v3] Tue, 18 Jun 2024 07:21:24 UTC (2,757 KB)
    Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao

Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former relies on external resources, and both require incorporating explicit documents into the context, which increases execution costs and susceptibility to noise data. Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or awakened. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, thereby awakening relevant knowledge in LLMs without relying on external resources. Guided by IAG, we propose an imagine richer context method for question answering (IMcQA). IMcQA consists of two modules: explicit imagination, which generates a short dummy document by learning from long context compression, and implicit imagination, which creates flexible adapters by distilling from a teacher model with a long context. Experimental results on three datasets demonstrate that IMcQA exhibits significant advantages in both open-domain and closed-book settings, as well as in out-of-distribution generalization. Our code will be available at this https URL.

------------

`[2403.16056] Qibo: A Large Language Model for Traditional Chinese Medicine <https://arxiv.org/abs/2403.16056>`__ Qibo:一个面向中医的大型语言模型

::

    replaced with revised version Mon, 17 Jun 2024 18:52:04 GMT
    Submission history From: Heyi Zhang [view email]
    [v1] Sun, 24 Mar 2024 07:48:05 UTC (874 KB)
    [v2] Mon, 17 Jun 2024 18:52:04 UTC (1,368 KB)
    Heyi Zhang and Xin Wang and Zhaopeng Meng and Zhe Chen and Pengwei Zhuang and Yongzhe Jia and Dawei Xu and Wenbin Guo

Large Language Models (LLMs) has made significant progress in a number of professional fields, including medicine, law, and finance. However, in traditional Chinese medicine (TCM), there are challenges such as the essential differences between theory and modern medicine, the lack of specialized corpus resources, and the fact that relying only on supervised fine-tuning may lead to overconfident predictions. To address these challenges, we propose a two-stage training approach that combines continuous pre-training and supervised fine-tuning. A notable contribution of our study is the processing of a 2Gb corpus dedicated to TCM, constructing pre-training and instruction fine-tuning datasets for TCM, respectively. In addition, we have developed Qibo-Benchmark, a tool that evaluates the performance of LLM in the TCM on multiple dimensions, including subjective, objective, and three TCM NLP tasks. The medical LLM trained with our pipeline, named \emph{\textbf{Qibo}}, exhibits significant performance boosts. Compared to the baselines, the average subjective win rate is 63\%, the average objective accuracy improved by 23\% to 58\%, and the Rouge-L scores for the three TCM NLP tasks are 0.72, 0.61, and 0.55. Finally, we propose a pipline to apply Qibo to TCM consultation and demonstrate the model performance through the case study.

------------

`[2404.09043] Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation <https://arxiv.org/abs/2404.09043>`__ llm玩骰子吗?探索用于行为模拟的大型语言模型中的概率分布抽样

::

    replaced with revised version Tue, 18 Jun 2024 05:27:45 GMT
    Submission history From: Jia Gu [view email]
    [v1] Sat, 13 Apr 2024 16:59:28 UTC (259 KB)
    [v2] Tue, 18 Jun 2024 05:27:45 UTC (1,077 KB)
    Jia Gu, Liang Pang, Huawei Shen, Xueqi Cheng

With the rapid advancement of large language models (LLMs) for handling complex language tasks, an increasing number of studies are employing LLMs as agents to emulate the sequential decision-making processes of humans often represented as Markov decision-making processes (MDPs). The actions in MDPs adhere to specific probability distributions and require iterative sampling. This arouses curiosity regarding the capacity of LLM agents to comprehend probability distributions, thereby guiding the agent's behavioral decision-making through probabilistic sampling and generating behavioral sequences. To answer the above question, we divide the problem into two main aspects: sequence simulation with known probability distribution and sequence simulation with unknown probability distribution. Our analysis indicates that LLM agents can understand probabilities, but they struggle with probability sampling. Their ability to perform probabilistic sampling can be improved to some extent by integrating coding tools, but this level of sampling precision still makes it difficult to simulate human behavior as agents.

------------

`[2404.15846] From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models <https://arxiv.org/abs/2404.15846>`__ 从复杂到简单:增强大型语言模型的多约束复杂指令跟随能力

::

    replaced with revised version Tue, 18 Jun 2024 13:16:36 GMT
    Submission history From: Qianyu He [view email]
    [v1] Wed, 24 Apr 2024 12:51:14 UTC (3,371 KB)
    [v2] Tue, 18 Jun 2024 13:16:36 UTC (3,394 KB)
    Qianyu He, Jie Zeng, Qianxi He, Jiaqing Liang, Yanghua Xiao

It is imperative for Large language models (LLMs) to follow instructions with elaborate requirements (i.e. Complex Instructions Following). Yet, it remains under-explored how to enhance the ability of LLMs to follow complex instructions with multiple constraints. To bridge the gap, we initially study what training data is effective in enhancing complex constraints following abilities. We found that training LLMs with instructions containing multiple constraints enhances their understanding of complex instructions, especially those with lower complexity levels. The improvement can even generalize to compositions of out-of-domain constraints. Additionally, we further propose methods addressing how to obtain and utilize the effective training data. Finally, we conduct extensive experiments to prove the effectiveness of our methods in terms of overall performance and training efficiency. We also demonstrate that our methods improve models' ability to follow instructions generally and generalize effectively across out-of-domain, in-domain, and adversarial settings, while maintaining general capabilities.

------------

`[2405.13816] Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners <https://arxiv.org/abs/2405.13816>`__ 以少得多:大型语言模型是良好的自发多语言学习者

::

    replaced with revised version Tue, 18 Jun 2024 16:30:01 GMT
    Submission history From: Shimao Zhang [view email]
    [v1] Wed, 22 May 2024 16:46:19 UTC (381 KB)
    [v2] Tue, 18 Jun 2024 16:30:01 UTC (701 KB)
    Shimao Zhang, Changjiang Gao, Wenhao Zhu, Jiajun Chen, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Shujian Huang

Recently, Large Language Models (LLMs) have shown impressive language capabilities. While most of the existing LLMs have very unbalanced performance across different languages, multilingual alignment based on translation parallel data is an effective method to enhance the LLMs' multilingual capabilities. In this work, we discover and comprehensively investigate the spontaneous multilingual alignment improvement of LLMs. We find that LLMs instruction-tuned on the question translation data (i.e. without annotated answers) are able to encourage the alignment between English and a wide range of languages, even including those unseen during instruction-tuning. Additionally, we utilize different settings and mechanistic interpretability methods to analyze the LLM's performance in the multilingual scenario comprehensively. Our work suggests that LLMs have enormous potential for improving multilingual alignment efficiently with great language and task generalization.

------------

`[2405.17052] SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself <https://arxiv.org/abs/2405.17052>`__ SelfCP:通过冻结的大型语言模型本身压缩超限提示

::

    replaced with revised version Tue, 18 Jun 2024 06:50:30 GMT
    Submission history From: Jun Gao [view email]
    [v1] Mon, 27 May 2024 11:14:55 UTC (449 KB)
    [v2] Tue, 18 Jun 2024 06:50:30 UTC (1,137 KB)
    Jun Gao, Ziqiang Cao, Wenjie Li

Long prompt leads to huge hardware costs when using transformer-based Large Language Models (LLMs). Unfortunately, many tasks, such as summarization, inevitably introduce long documents, and the wide application of in-context learning easily makes the prompt length explode. This paper proposes a Self-Compressor (SelfCP), which employs the target LLM itself to compress over-limit prompts into dense vectors while keeping the allowed prompts unmodified. Dense vectors are then projected into dense tokens via a learnable connector to make the same LLM unburden to understand. The connector is supervised-tuned under the language modeling objective of the LLM on relatively long texts selected from publicly accessed datasets, involving an instruction dataset to make SelfCP respond to various prompts, while the target LLM keeps frozen during training. We build the lightweight SelfCP upon 2 different backbones with merely 17M learnable parameters originating from the connector and a learnable embedding. Evaluation on both English and Chinese benchmarks demonstrate that SelfCP effectively substitutes 12$\times$ over-limit prompts with dense tokens to reduce memory costs and booster inference throughputs, yet improving response quality. The outstanding performance brings an efficient solution for LLMs to tackle long prompts without training LLMs from scratch.

------------

`[2406.02224] FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models <https://arxiv.org/abs/2406.02224>`__ FedMKT:大型和小型语言模型的联邦互知识迁移

::

    replaced with revised version Tue, 18 Jun 2024 08:17:00 GMT
    Submission history From: Tao Fan [view email]
    [v1] Tue, 4 Jun 2024 11:36:09 UTC (373 KB)
    [v2] Tue, 18 Jun 2024 08:17:00 UTC (680 KB)
    Tao Fan, Guoqiang Ma, Yan Kang, Hanlin Gu, Yuanfeng Song, Lixin Fan, Kai Chen, Qiang Yang

Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small language models (SLMs) at downstream clients. However, a significant gap remains in the simultaneous mutual enhancement of both the server's LLM and clients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient federated mutual knowledge transfer framework for large and small language models. This framework is designed to adaptively transfer knowledge from the server's LLM to clients' SLMs while concurrently enriching the LLM with clients' unique domain insights. We facilitate token alignment using minimum edit distance (MinED) and then selective mutual knowledge transfer between client-side SLMs and a server-side LLM, aiming to collectively enhance their performance. Through extensive experiments across three distinct scenarios, we evaluate the effectiveness of FedMKT using various public LLMs and SLMs on a range of NLP text generation tasks. Empirical results demonstrate that FedMKT simultaneously boosts the performance of both LLMs and SLMs.

------------

`[2406.02528] Scalable MatMul-free Language Modeling <https://arxiv.org/abs/2406.02528>`__ 可扩展的无matmull语言建模

::

    replaced with revised version Tue, 18 Jun 2024 17:30:06 GMT
    Submission history From: Rui-Jie Zhu [view email]
    [v1] Tue, 4 Jun 2024 17:50:34 UTC (1,050 KB)
    [v2] Mon, 10 Jun 2024 14:55:29 UTC (1,051 KB)
    [v3] Tue, 11 Jun 2024 06:18:28 UTC (1,051 KB)
    [v4] Fri, 14 Jun 2024 07:48:33 UTC (1,051 KB)
    [v5] Tue, 18 Jun 2024 17:30:06 UTC (1,051 KB)
    Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, Jason K. Eshraghian

Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at this https URL.

------------

`[2406.02721] Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller <https://arxiv.org/abs/2406.02721>`__ 通过将后缀梯度压缩为前缀控制器实现LLM行为自控

::

    replaced with revised version Tue, 18 Jun 2024 15:58:38 GMT
    Submission history From: Min Cai [view email]
    [v1] Tue, 4 Jun 2024 19:05:10 UTC (2,834 KB)
    [v2] Tue, 18 Jun 2024 15:58:38 UTC (2,834 KB)
    Min Cai and Yuchen Zhang and Shichang Zhang and Fan Yin and Difan Zou and Yisong Yue and Ziniu Hu

We propose Self-Control, a novel method utilizing suffix gradients to control the behavior of large language models (LLMs) without explicit human annotations. Given a guideline expressed in suffix string and the model's self-assessment of adherence, Self-Control computes the gradient of this self-judgment concerning the model's hidden states, directly influencing the auto-regressive generation process towards desired behaviors. To enhance efficiency, we introduce Self-Control_{prefix}, a compact module that encapsulates the learned representations from suffix gradients into a Prefix Controller, facilitating inference-time control for various LLM behaviors. Our experiments demonstrate Self-Control's efficacy across multiple domains, including emotional modulation, ensuring harmlessness, and enhancing complex reasoning. Especially, Self-Control_{prefix} enables a plug-and-play control and jointly controls multiple attributes, improving model outputs without altering model parameters or increasing inference-time costs.

------------

`[2406.05888] Feriji: A French-Zarma Parallel Corpus, Glossary & Translator <https://arxiv.org/abs/2406.05888>`__ Feriji:法语- zarma平行语料库、词汇表和翻译

::

    replaced with revised version Mon, 17 Jun 2024 22:36:18 GMT
    Submission history From: Mamadou K. Keita [view email]
    [v1] Sun, 9 Jun 2024 19:08:33 UTC (7,294 KB)
    [v2] Mon, 17 Jun 2024 22:36:18 UTC (7,294 KB)
    Mamadou K. Keita, Elysabhete Amadou Ibrahim, Habibatou Abdoulaye Alfari, Christopher Homan

Machine translation (MT) is a rapidly expanding field that has experienced significant advancements in recent years with the development of models capable of translating multiple languages with remarkable accuracy. However, the representation of African languages in this field still needs to improve due to linguistic complexities and limited resources. This applies to the Zarma language, a dialect of Songhay (of the Nilo-Saharan language family) spoken by over 5 million people across Niger and neighboring countries \cite{lewis2016ethnologue}. This paper introduces Feriji, the first robust French-Zarma parallel corpus and glossary designed for MT. The corpus, containing 61,085 sentences in Zarma and 42,789 in French, and a glossary of 4,062 words represent a significant step in addressing the need for more resources for Zarma. We fine-tune three large language models on our dataset, obtaining a BLEU score of 30.06 on the best-performing model. We further evaluate the models on human judgments of fluency, comprehension, and readability and the importance and impact of the corpus and models. Our contributions help to bridge a significant language gap and promote an essential and overlooked indigenous African language.

------------

`[2406.06576] OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step <https://arxiv.org/abs/2406.06576>`__ OccamLLM:一步快速精确语言模型算法

::

    replaced with revised version Tue, 18 Jun 2024 17:51:42 GMT
    Submission history From: Owen Dugan [view email]
    [v1] Tue, 4 Jun 2024 04:17:40 UTC (3,501 KB)
    [v2] Tue, 18 Jun 2024 17:51:42 UTC (3,341 KB)
    Owen Dugan, Donato Manuel Jimenez Beneto, Charlotte Loh, Zhuo Chen, Rumen Dangovski, Marin Solja\v{c}i\'c

Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations. To achieve accurate calculations, language model systems often enable LLMs to generate code for arithmetic operations. However, this approach compromises speed and security and, if finetuning is involved, risks the language model losing prior capabilities. We propose a framework that enables exact arithmetic in \textit{a single autoregressive step}, providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities. We use the hidden states of an LLM to control a symbolic architecture which performs arithmetic. Our implementation using Llama 3 8B Instruct with OccamNet as a symbolic model (OccamLlama) achieves 100\% accuracy on single arithmetic operations ($+,-,\times,÷,\sin{},\cos{},\log{},\exp{},\sqrt{}$), outperforming GPT 4o and on par with GPT 4o using a code interpreter. OccamLlama also outperforms GPT 4o both with and without a code interpreter on mathematical problem solving benchmarks involving challenging arithmetic, thus enabling small LLMs to match the arithmetic performance of even much larger models. We will make our code public shortly.

------------

`[2406.07243] MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs <https://arxiv.org/abs/2406.07243>`__ MBBQ:生成式llm中原型的跨语言比较数据集

::

    replaced with revised version Tue, 18 Jun 2024 15:33:33 GMT
    Submission history From: Vera Neplenbroek [view email]
    [v1] Tue, 11 Jun 2024 13:23:14 UTC (550 KB)
    [v2] Tue, 18 Jun 2024 15:33:33 UTC (550 KB)
    Vera Neplenbroek, Arianna Bisazza, Raquel Fern\'andez

Generative large language models (LLMs) have been shown to exhibit harmful biases and stereotypes. While safety fine-tuning typically takes place in English, if at all, these models are being used by speakers of many different languages. There is existing evidence that the performance of these models is inconsistent across languages and that they discriminate based on demographic factors of the user. Motivated by this, we investigate whether the social stereotypes exhibited by LLMs differ as a function of the language used to prompt them, while controlling for cultural differences and task accuracy. To this end, we present MBBQ (Multilingual Bias Benchmark for Question-answering), a carefully curated version of the English BBQ dataset extended to Dutch, Spanish, and Turkish, which measures stereotypes commonly held across these languages. We further complement MBBQ with a parallel control dataset to measure task performance on the question-answering task independently of bias. Our results based on several open-source and proprietary LLMs confirm that some non-English languages suffer from bias more than English, even when controlling for cultural shifts. Moreover, we observe significant cross-lingual differences in bias behaviour for all except the most accurate models. With the release of MBBQ, we hope to encourage further research on bias in multilingual settings. The dataset and code are available at this https URL.

------------

`[2406.10296] CLST: Cold-Start Mitigation in Knowledge Tracing by Aligning a Generative Language Model as a Students' Knowledge Tracer <https://arxiv.org/abs/2406.10296>`__ CLST:通过将生成语言模型对齐为学生知识追踪器来缓解知识追踪中的冷启动

::

    replaced with revised version Tue, 18 Jun 2024 00:53:50 GMT
    Submission history From: Yeonju Jang [view email]
    [v1] Thu, 13 Jun 2024 09:21:43 UTC (1,761 KB)
    [v2] Tue, 18 Jun 2024 00:53:50 UTC (1,761 KB)
    Heeseok Jung, Jaesang Yoo, Yohaan Yoon, and Yeonju Jang

Knowledge tracing (KT), wherein students' problem-solving histories are used to estimate their current levels of knowledge, has attracted significant interest from researchers. However, most existing KT models were developed with an ID-based paradigm, which exhibits limitations in cold-start performance. These limitations can be mitigated by leveraging the vast quantities of external knowledge possessed by generative large language models (LLMs). In this study, we propose cold-start mitigation in knowledge tracing by aligning a generative language model as a students' knowledge tracer (CLST) as a framework that utilizes a generative LLM as a knowledge tracer. Upon collecting data from math, social studies, and science subjects, we framed the KT task as a natural language processing task, wherein problem-solving data are expressed in natural language, and fine-tuned the generative LLM using the formatted KT dataset. Subsequently, we evaluated the performance of the CLST in situations of data scarcity using various baseline models for comparison. The results indicate that the CLST significantly enhanced performance with a dataset of fewer than 100 students in terms of prediction, reliability, and cross-domain generalization.

------------

`[2406.11109] Investigating Annotator Bias in Large Language Models for Hate Speech Detection <https://arxiv.org/abs/2406.11109>`__ 面向仇恨言论检测的大型语言模型标注者偏见研究

::

    replaced with revised version Tue, 18 Jun 2024 06:21:16 GMT
    Submission history From: Amit Das [view email]
    [v1] Mon, 17 Jun 2024 00:18:31 UTC (118 KB)
    [v2] Tue, 18 Jun 2024 06:21:16 UTC (118 KB)
    Amit Das, Zheng Zhang, Fatemeh Jamshidi, Vinija Jain, Aman Chadha, Nilanjana Raychawdhary, Mary Sandage, Lauramarie Pope, Gerry Dozier, Cheryl Seals

Data annotation, the practice of assigning descriptive labels to raw data, is pivotal in optimizing the performance of machine learning models. However, it is a resource-intensive process susceptible to biases introduced by annotators. The emergence of sophisticated Large Language Models (LLMs), like ChatGPT presents a unique opportunity to modernize and streamline this complex procedure. While existing research extensively evaluates the efficacy of LLMs, as annotators, this paper delves into the biases present in LLMs, specifically GPT 3.5 and GPT 4o when annotating hate speech data. Our research contributes to understanding biases in four key categories: gender, race, religion, and disability. Specifically targeting highly vulnerable groups within these categories, we analyze annotator biases. Furthermore, we conduct a comprehensive examination of potential factors contributing to these biases by scrutinizing the annotated data. We introduce our custom hate speech detection dataset, HateSpeechCorpus, to conduct this research. Additionally, we perform the same experiments on the ETHOS (Mollas et al., 2022) dataset also for comparative analysis. This paper serves as a crucial resource, guiding researchers and practitioners in harnessing the potential of LLMs for dataannotation, thereby fostering advancements in this critical field. The HateSpeechCorpus dataset is available here: this https URL

------------

`[2406.11410] HARE: HumAn pRiors, a key to small language model Efficiency <https://arxiv.org/abs/2406.11410>`__ HARE:人类先验，小语言模型效率的关键

::

    replaced with revised version Tue, 18 Jun 2024 11:59:03 GMT
    Submission history From: Yongneng Jiang [view email]
    [v1] Mon, 17 Jun 2024 10:56:03 UTC (333 KB)
    [v2] Tue, 18 Jun 2024 11:59:03 UTC (333 KB)
    Lingyun Zhang, Bin jin, Gaojian Ge, Lunhui Liu, Xuewen Shen, Mingyong Wu, Houqian Zhang, Yongneng Jiang, Shiqi Chen, Shi Pu

Human priors play a crucial role in efficiently utilizing data in deep learning. However, with the development of large language models (LLMs), there is an increasing emphasis on scaling both model size and data volume, which often diminishes the importance of human priors in data construction. Influenced by these trends, existing Small Language Models (SLMs) mainly rely on web-scraped large-scale training data, neglecting the proper incorporation of human priors. This oversight limits the training efficiency of language models in resource-constrained settings. In this paper, we propose a principle to leverage human priors for data construction. This principle emphasizes achieving high-performance SLMs by training on a concise dataset that accommodates both semantic diversity and data quality consistency, while avoiding benchmark data leakage. Following this principle, we train an SLM named HARE-1.1B. Extensive experiments on large-scale benchmark datasets demonstrate that HARE-1.1B performs favorably against state-of-the-art SLMs, validating the effectiveness of the proposed principle. Additionally, this provides new insights into efficient language model training in resource-constrained environments from the view of human priors.

------------

`[2310.07820] Large Language Models Are Zero-Shot Time Series Forecasters <https://arxiv.org/abs/2310.07820>`__ 

::

    replaced with revised version Tue, 18 Jun 2024 14:48:38 GMT
    Submission history From: Nate Gruver [view email]
    [v1] Wed, 11 Oct 2023 19:01:28 UTC (1,040 KB)
    [v2] Tue, 18 Jun 2024 14:48:38 UTC (1,039 KB)
    Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson

By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.

------------

`[2401.10134] Spatial-Temporal Large Language Model for Traffic Prediction <https://arxiv.org/abs/2401.10134>`__ 面向交通预测的时空大型语言模型

::

    replaced with revised version Tue, 18 Jun 2024 07:50:31 GMT
    Submission history From: Chenxi Liu [view email]
    [v1] Thu, 18 Jan 2024 17:03:59 UTC (280 KB)
    [v2] Tue, 23 Jan 2024 07:42:40 UTC (273 KB)
    [v3] Tue, 18 Jun 2024 07:50:31 UTC (2,067 KB)
    Chenxi Liu, Sun Yang, Qianxiong Xu, Zhishuai Li, Cheng Long, Ziyue Li, Rui Zhao

Traffic prediction, an essential component for intelligent transportation systems, endeavours to use historical data to foresee future traffic features at specific locations. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not improved. Recently, large language models have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pretraining while maintaining their fundamental structures. Motivated by these developments, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. In the ST-LLM, we define timesteps at each location as tokens and design a spatial-temporal embedding to learn the spatial location and global temporal patterns of these tokens. Additionally, we integrate these embeddings by a fusion convolution to each token for a unified spatial-temporal representation. Furthermore, we innovate a partially frozen attention strategy to adapt the LLM to capture global spatial-temporal dependencies for traffic prediction. Comprehensive experiments on real traffic datasets offer evidence that ST-LLM is a powerful spatial-temporal learner that outperforms state-of-the-art models. Notably, the ST-LLM also exhibits robust performance in both few-shot and zero-shot prediction scenarios. The code is publicly available at this https URL.

------------

`[2402.02207] Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models <https://arxiv.org/abs/2402.02207>`__ (几乎)无成本的安全微调:视觉大型语言模型的基线

::

    replaced with revised version Mon, 17 Jun 2024 22:26:32 GMT
    Submission history From: Yongshuo Zong [view email]
    [v1] Sat, 3 Feb 2024 16:43:42 UTC (7,418 KB)
    [v2] Mon, 17 Jun 2024 22:26:32 UTC (5,590 KB)
    Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, Timothy Hospedales

Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several black-box adversarial attacks, which approach zero in many cases. The code and dataset are available at this https URL.

------------

`[2402.08526] Can LLMs Learn New Concepts Incrementally without Forgetting? <https://arxiv.org/abs/2402.08526>`__ llm能渐进地学习新概念而不忘记吗?

::

    replaced with revised version Tue, 18 Jun 2024 06:56:44 GMT
    Submission history From: Junhao Zheng [view email]
    [v1] Tue, 13 Feb 2024 15:29:50 UTC (5,072 KB)
    [v2] Tue, 21 May 2024 08:29:44 UTC (5,072 KB)
    [v3] Tue, 18 Jun 2024 06:56:44 UTC (5,667 KB)
    Junhao Zheng, Shengjie Qiu, Qianli Ma

Large Language Models (LLMs) have achieved remarkable success across various tasks, yet their ability to learn incrementally without forgetting remains underexplored. Incremental learning (IL) is crucial as it enables models to acquire new knowledge while retaining previously learned information, akin to human learning. Existing benchmarks for IL are insufficient due to data leakage issues and the overqualification of LLMs. To address these challenges, we introduce Concept-1K, a novel dataset comprising 1,023 recently emerged concepts across diverse domains. The concepts in Concept-1K are discrete, interpretable units of knowledge that allow for fine-grained analysis of learning and forgetting processes. Using Concept-1K as a testbed, we aim to answer the question: ``Can LLMs learn new concepts incrementally without forgetting like humans?'' Our investigation reveals that LLMs still suffer from catastrophic forgetting and that LoRA, despite fine-tuning fewer parameters, may lead to more forgetting on training data. Additionally, we explore the roles of in-context learning, model scale, buffer size, and pretraining in IL performance. These findings highlight the strengths and limitations of LLMs in IL scenarios and provide a robust benchmark for future research.

------------

`[2403.02310] Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve <https://arxiv.org/abs/2403.02310>`__ 利用sarthi - serve优化LLM推理中的吞吐量-延迟权衡

::

    replaced with revised version Mon, 17 Jun 2024 21:10:46 GMT
    Submission history From: Amey Agrawal [view email]
    [v1] Mon, 4 Mar 2024 18:47:08 UTC (412 KB)
    [v2] Wed, 12 Jun 2024 03:13:20 UTC (1,714 KB)
    [v3] Mon, 17 Jun 2024 21:10:46 UTC (1,714 KB)
    Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Alexey Tumanov, Ramachandran Ramjee

Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt and produces the first output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.
We introduce an efficient LLM inference scheduler, Sarathi-Serve, to address this throughput-latency tradeoff. Sarathi-Serve introduces chunked-prefills which splits a prefill request into near equal sized chunks and creates stall-free schedules that adds new requests in a batch without pausing ongoing decodes. Stall-free scheduling unlocks the opportunity to improve throughput with large batch sizes while minimizing the effect of batching on latency. Furthermore, uniform batches in Sarathi-Serve ameliorate the imbalance between iterations resulting in minimal pipeline bubbles.
Our techniques yield significant improvements in inference performance across models and hardware under tail latency constraints. For Mistral-7B on single A100 GPUs, we achieve 2.6x higher serving capacity and up to 3.7x higher serving capacity for the Yi-34B model on two A100 GPUs as compared to vLLM. When used with pipeline parallelism on Falcon-180B, Sarathi-Serve provides up to 5.6x gain in the end-to-end serving capacity. The source code for Sarathi-Serve is available at this https URL.

------------

`[2406.10918] Embodied Question Answering via Multi-LLM Systems <https://arxiv.org/abs/2406.10918>`__ 基于多llm系统的具身问答

::

    replaced with revised version Tue, 18 Jun 2024 01:18:46 GMT
    Submission history From: Bhrij Patel [view email]
    [v1] Sun, 16 Jun 2024 12:46:40 UTC (4,384 KB)
    [v2] Tue, 18 Jun 2024 01:18:46 UTC (4,384 KB)
    Bhrij Patel, Vishnu Sashank Dorbala, Dinesh Manocha, Amrit Singh Bedi

Embodied Question Answering (EQA) is an important problem, which involves an agent exploring the environment to answer user queries. In the existing literature, EQA has exclusively been studied in single-agent scenarios, where exploration can be time-consuming and costly. In this work, we consider EQA in a multi-agent framework involving multiple large language models (LLM) based agents independently answering queries about a household environment. To generate one answer for each query, we use the individual responses to train a Central Answer Model (CAM) that aggregates responses for a robust answer. Using CAM, we observe a $50\%$ higher EQA accuracy when compared against aggregation methods for ensemble LLM, such as voting schemes and debates. CAM does not require any form of agent communication, alleviating it from the associated costs. We ablate CAM with various nonlinear (neural network, random forest, decision tree, XGBoost) and linear (logistic regression classifier, SVM) algorithms. Finally, we present a feature importance analysis for CAM via permutation feature importance (PFI), quantifying CAMs reliance on each independent agent and query context.

------------

`[2406.11675] BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models <https://arxiv.org/abs/2406.11675>`__ BLoB:基于反向传播的大型语言模型贝叶斯低秩自适应

::

    replaced with revised version Tue, 18 Jun 2024 15:15:04 GMT
    Submission history From: Yibin Wang [view email]
    [v1] Mon, 17 Jun 2024 15:55:38 UTC (1,111 KB)
    [v2] Tue, 18 Jun 2024 15:15:04 UTC (1,148 KB)
    Yibin Wang, Haizhou Shi, Ligong Han, Dimitris Metaxas, Hao Wang

Large Language Models (LLMs) often suffer from overconfidence during inference, particularly when adapted to downstream domain-specific tasks with limited data. Previous work addresses this issue by employing approximate Bayesian estimation after the LLMs are trained, enabling them to quantify uncertainty. However, such post-training approaches' performance is severely limited by the parameters learned during training. In this paper, we go beyond post-training Bayesianization and propose Bayesian Low-Rank Adaptation by Backpropagation (BLoB), an algorithm that continuously and jointly adjusts both the mean and covariance of LLM parameters throughout the whole fine-tuning process. Our empirical results verify the effectiveness of BLoB in terms of generalization and uncertainty estimation, when evaluated on both in-distribution and out-of-distribution data.

------------

`[2406.11686] The Role of Inherent Bellman Error in Offline Reinforcement Learning with Linear Function Approximation <https://arxiv.org/abs/2406.11686>`__ 固有Bellman误差在线性函数逼近的离线强化学习中的作用

::

    replaced with revised version Tue, 18 Jun 2024 04:23:39 GMT
    Submission history From: Noah Golowich [view email]
    [v1] Mon, 17 Jun 2024 16:04:06 UTC (92 KB)
    [v2] Tue, 18 Jun 2024 04:23:39 UTC (92 KB)
    Noah Golowich and Ankur Moitra

In this paper, we study the offline RL problem with linear function approximation. Our main structural assumption is that the MDP has low inherent Bellman error, which stipulates that linear value functions have linear Bellman backups with respect to the greedy policy. This assumption is natural in that it is essentially the minimal assumption required for value iteration to succeed. We give a computationally efficient algorithm which succeeds under a single-policy coverage condition on the dataset, namely which outputs a policy whose value is at least that of any policy which is well-covered by the dataset. Even in the setting when the inherent Bellman error is 0 (termed linear Bellman completeness), our algorithm yields the first known guarantee under single-policy coverage.
In the setting of positive inherent Bellman error ${\varepsilon_{\mathrm{BE}}} > 0$, we show that the suboptimality error of our algorithm scales with $\sqrt{\varepsilon_{\mathrm{BE}}}$. Furthermore, we prove that the scaling of the suboptimality with $\sqrt{\varepsilon_{\mathrm{BE}}}$ cannot be improved for any algorithm. Our lower bound stands in contrast to many other settings in reinforcement learning with misspecification, where one can typically obtain performance that degrades linearly with the misspecification error.

------------

`[2310.11604] Language Models as Zero-Shot Trajectory Generators <https://arxiv.org/abs/2310.11604>`__ 语言模型作为零样本轨迹生成器

::

    replaced with revised version Mon, 17 Jun 2024 23:57:03 GMT
    Submission history From: Teyun Kwon [view email]
    [v1] Tue, 17 Oct 2023 21:57:36 UTC (35,026 KB)
    [v2] Mon, 17 Jun 2024 23:57:03 UTC (1,842 KB)
    Teyun Kwon, Norman Di Palo, Edward Johns

Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation tasks, when given access to only object detection and segmentation vision models. We designed a single, task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers. Then we studied how well it can perform across 30 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigated which design choices in this prompt are the most important. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding of low-level robot control sufficient for a range of common tasks, and that they can additionally detect failures and then re-plan trajectories accordingly. Videos, prompts, and code are available at: this https URL.

------------

`[2311.13721] Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning <https://arxiv.org/abs/2311.13721>`__ Nova:基于分层注意力和对比学习的汇编代码生成语言模型

::

    replaced with revised version Tue, 18 Jun 2024 02:48:16 GMT
    Submission history From: Nan Jiang [view email]
    [v1] Wed, 22 Nov 2023 22:27:54 UTC (29,457 KB)
    [v2] Mon, 27 Nov 2023 18:22:55 UTC (29,457 KB)
    [v3] Tue, 18 Jun 2024 02:48:16 UTC (7,039 KB)
    Nan Jiang, Chengxiao Wang, Kevin Liu, Xiangzhe Xu, Lin Tan, Xiangyu Zhang

Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code due to the unique challenges of assembly: (1) the low information density of assembly and (2) the diverse optimizations in assembly code. To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively, and designs contrastive learning objectives to train LLMs to learn assembly optimization. Equipped with these techniques, this work develops Nova, a generative LLM for assembly code. Nova outperforms existing techniques on binary code decompilation by up to 146.54%, and outperforms the latest binary code similarity detection techniques by up to 6.17%, showing promising abilities on both assembly generation and understanding tasks.

------------

`[2404.02151] Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks <https://arxiv.org/abs/2404.02151>`__ 越狱使用简单的自适应攻击引领安全对齐的llm

::

    replaced with revised version Tue, 18 Jun 2024 17:29:04 GMT
    Submission history From: Maksym Andriushchenko [view email]
    [v1] Tue, 2 Apr 2024 17:58:27 UTC (2,233 KB)
    [v2] Tue, 18 Jun 2024 17:29:04 UTC (2,312 KB)
    Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion

We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token ``Sure''), potentially with multiple restarts. In this way, we achieve nearly 100% attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with a 100% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings, it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). For reproducibility purposes, we provide the code, logs, and jailbreak artifacts in the JailbreakBench format at this https URL.

------------

`[2406.10057] First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models <https://arxiv.org/abs/2406.10057>`__ 多模态大型语言模型流程图理解的首次多维评估

::

    replaced with revised version Tue, 18 Jun 2024 08:03:31 GMT
    Submission history From: Enming Zhang [view email]
    [v1] Fri, 14 Jun 2024 14:15:35 UTC (1,485 KB)
    [v2] Tue, 18 Jun 2024 08:03:31 UTC (1,563 KB)
    Enming Zhang, Ruobing Yao, Huanyong Liu, Junhui Yu, Jiale Wang

With the development of Multimodal Large Language Models (MLLMs) technology, its general capabilities are increasingly powerful. To evaluate the various abilities of MLLMs, numerous evaluation systems have emerged. But now there is still a lack of a comprehensive method to evaluate MLLMs in the tasks related to flowcharts, which are very important in daily life and work. We propose the first comprehensive method, FlowCE, to assess MLLMs across various dimensions for tasks related to flowcharts. It encompasses evaluating MLLMs' abilities in Reasoning, Localization Recognition, Information Extraction, Logical Verification, and Summarization on flowcharts. However, we find that even the GPT4o model achieves only a score of 56.63. Among open-source models, Phi-3-Vision obtained the highest score of 49.97. We hope that FlowCE can contribute to future research on MLLMs for tasks based on flowcharts. \url{this https URL} \end{abstract}

------------

`[2406.11156] DELRec: Distilling Sequential Pattern to Enhance LLM-based Recommendation <https://arxiv.org/abs/2406.11156>`__ DELRec:提取序列模式增强基于llm的推荐

::

    replaced with revised version Tue, 18 Jun 2024 04:00:59 GMT
    Submission history From: Haoyi Zhang [view email]
    [v1] Mon, 17 Jun 2024 02:47:09 UTC (11,110 KB)
    [v2] Tue, 18 Jun 2024 04:00:59 UTC (11,307 KB)
    Guohao Sun and Haoyi Zhang

Sequential recommendation (SR) tasks enhance recommendation accuracy by capturing the connection between users' past interactions and their changing preferences. Conventional models often focus solely on capturing sequential patterns within the training data, neglecting the broader context and semantic information embedded in item titles from external sources. This limits their predictive power and adaptability. Recently, large language models (LLMs) have shown promise in SR tasks due to their advanced understanding capabilities and strong generalization abilities. Researchers have attempted to enhance LLMs' recommendation performance by incorporating information from SR models. However, previous approaches have encountered problems such as 1) only influencing LLMs at the result level; 2) increased complexity of LLMs recommendation methods leading to reduced interpretability; 3) incomplete understanding and utilization of SR models information by LLMs.
To address these problems, we proposes a novel framework, DELRec, which aims to extract knowledge from SR models and enable LLMs to easily comprehend and utilize this supplementary information for more effective sequential recommendations. DELRec consists of two main stages: 1) SR Models Pattern Distilling, focusing on extracting behavioral patterns exhibited by SR models using soft prompts through two well-designed strategies; 2) LLMs-based Sequential Recommendation, aiming to fine-tune LLMs to effectively use the distilled auxiliary information to perform SR tasks. Extensive experimental results conducted on three real datasets validate the effectiveness of the DELRec framework.

------------

`[2406.08478] What If We Recaption Billions of Web Images with LLaMA-3? <https://arxiv.org/abs/2406.08478>`__ 如果我们用LLaMA-3再现数十亿的网络图像呢?

::

    replaced with revised version Tue, 18 Jun 2024 11:47:26 GMT
    Submission history From: Bingchen Zhao [view email]
    [v1] Wed, 12 Jun 2024 17:59:07 UTC (1,642 KB)
    [v2] Tue, 18 Jun 2024 11:47:26 UTC (1,642 KB)
    Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, Cihang Xie

Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. Our paper aims to bridge this community effort, leveraging the powerful and \textit{open-sourced} LLaMA-3, a GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images from the DataComp-1B dataset. Our empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, we observe enhanced zero-shot performance in cross-modal retrieval tasks. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries. Our project page is this https URL

------------

`[2406.09067] How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models <https://arxiv.org/abs/2406.09067>`__ 基于transformer的视觉编码器中的表示结构如何?视觉-语言模型中的多目标表示分析

::

    replaced with revised version Tue, 18 Jun 2024 12:27:36 GMT
    Submission history From: Tarun Khajuria [view email]
    [v1] Thu, 13 Jun 2024 12:54:20 UTC (14,914 KB)
    [v2] Tue, 18 Jun 2024 12:27:36 UTC (14,914 KB)
    Tarun Khajuria, Braian Olmiro Dias, Jaan Aru

Forming and using symbol-like structured representations for reasoning has been considered essential for generalising over novel inputs. The primary tool that allows generalisation outside training data distribution is the ability to abstract away irrelevant information into a compact form relevant to the task. An extreme form of such abstract representations is symbols. Humans make use of symbols to bind information while abstracting away irrelevant parts to utilise the information consistently and meaningfully. This work estimates the state of such structured representations in vision encoders. Specifically, we evaluate image encoders in large vision-language pre-trained models to address the question of which desirable properties their representations lack by applying the criteria of symbolic structured reasoning described for LLMs to the image models. We test the representation space of image encoders like VIT, BLIP, CLIP, and FLAVA to characterise the distribution of the object representations in these models. In particular, we create decoding tasks using multi-object scenes from the COCO dataset, relating the token space to its input content for various objects in the scene. We use these tasks to characterise the network's token and layer-wise information modelling. Our analysis highlights that the CLS token, used for the downstream task, only focuses on a few objects necessary for the trained downstream task. Still, other individual objects are well-modelled separately by the tokens in the network originating from those objects. We further observed a widespread distribution of scene information. This demonstrates that information is far more entangled in tokens than optimal for representing objects similar to symbols. Given these symbolic properties, we show the network dynamics that cause failure modes of these models on basic downstream tasks in a multi-object scene.

------------

`[2406.10958] City-LEO: Toward Transparent City Management Using LLM with End-to-End Optimization <https://arxiv.org/abs/2406.10958>`__ City- leo:使用端到端优化的LLM实现透明的城市管理

::

    replaced with revised version Tue, 18 Jun 2024 02:16:58 GMT
    Submission history From: Zihao Jiao [view email]
    [v1] Sun, 16 Jun 2024 14:25:08 UTC (13,598 KB)
    [v2] Tue, 18 Jun 2024 02:16:58 UTC (13,598 KB)
    Zihao Jiao, Mengyi Sha, Haoyu Zhang, Xinyu Jiang, Wei Qi

Existing operations research (OR) models and tools play indispensable roles in smart-city operations, yet their practical implementation is limited by the complexity of modeling and deficiencies in optimization proficiency. To generate more relevant and accurate solutions to users' requirements, we propose a large language model (LLM)-based agent ("City-LEO") that enhances the efficiency and transparency of city management through conversational interactions. Specifically, to accommodate diverse users' requirements and enhance computational tractability, City-LEO leverages LLM's logical reasoning capabilities on prior knowledge to scope down large-scale optimization problems efficiently. In the human-like decision process, City-LEO also incorporates End-to-end (E2E) model to synergize the prediction and optimization. The E2E framework be conducive to coping with environmental uncertainties and involving more query-relevant features, and then facilitates transparent and interpretable decision-making process. In case study, we employ City-LEO in the operations management of e-bike sharing (EBS) system. The numerical results demonstrate that City-LEO has superior performance when benchmarks against the full-scale optimization problem. With less computational time, City-LEO generates more satisfactory and relevant solutions to the users' requirements, and achieves lower global suboptimality without significantly compromising accuracy. In a broader sense, our proposed agent offers promise to develop LLM-embedded OR tools for smart-city operations management.

------------

`[2305.09605] To smooth a cloud or to pin it down: Guarantees and Insights on Score Matching in Denoising Diffusion Models <https://arxiv.org/abs/2305.09605>`__ 平滑云或固定云:去噪扩散模型中分数匹配的保证和见解

::

    replaced with revised version Tue, 18 Jun 2024 16:01:52 GMT
    Submission history From: Francisco Vargas [view email]
    [v1] Tue, 16 May 2023 16:56:19 UTC (732 KB)
    [v2] Tue, 18 Jun 2024 16:01:52 UTC (10,751 KB)
    Francisco Vargas, Teodora Reu, Anna Kerekes

Denoising diffusion models are a class of generative models which have recently achieved state-of-the-art results across many domains. Gradual noise is added to the data using a diffusion process, which transforms the data distribution into a Gaussian. Samples from the generative model are then obtained by simulating an approximation of the time reversal of this diffusion initialized by Gaussian samples. Recent research has explored adapting diffusion models for sampling and inference tasks. In this paper, we leverage known connections to stochastic control akin to the Föllmer drift to extend established neural network approximation results for the Föllmer drift to denoising diffusion models and samplers.

------------

`[2312.15591] Privacy-Preserved Neural Graph Databases <https://arxiv.org/abs/2312.15591>`__ 隐私保护的神经图数据库

::

    replaced with revised version Tue, 18 Jun 2024 03:35:51 GMT
    Submission history From: Qi Hu [view email]
    [v1] Mon, 25 Dec 2023 02:32:05 UTC (425 KB)
    [v2] Fri, 19 Jan 2024 14:08:23 UTC (425 KB)
    [v3] Thu, 22 Feb 2024 08:11:30 UTC (429 KB)
    [v4] Mon, 26 Feb 2024 02:18:25 UTC (430 KB)
    [v5] Tue, 18 Jun 2024 03:35:51 UTC (433 KB)
    Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, Yangqiu Song

In the era of large language models (LLMs), efficient and accurate data retrieval has become increasingly crucial for the use of domain-specific or private data in the retrieval augmented generation (RAG). Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (GDBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data which can be adaptively trained with LLMs. The usage of neural embedding storage and Complex neural logical Query Answering (CQA) provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the domain-specific or private databases. Malicious attackers can infer more sensitive information in the database using well-designed queries such as from the answer sets of where Turing Award winners born before 1950 and after 1940 lived, the living places of Turing Award winner Hinton are probably exposed, although the living places may have been deleted in the training stage due to the privacy concerns. In this work, we propose a privacy-preserved neural graph database (P-NGDB) framework to alleviate the risks of privacy leakage in NGDBs. We introduce adversarial training techniques in the training stage to enforce the NGDBs to generate indistinguishable answers when queried with private information, enhancing the difficulty of inferring sensitive information through combinations of multiple innocuous queries.

------------

`[2401.11506] Enhancing Recommendation Diversity by Re-ranking with Large Language Models <https://arxiv.org/abs/2401.11506>`__ 基于大型语言模型重排序的推荐多样性增强

::

    replaced with revised version Mon, 17 Jun 2024 18:09:57 GMT
    Submission history From: Derek Bridge [view email]
    [v1] Sun, 21 Jan 2024 14:33:52 UTC (531 KB)
    [v2] Mon, 17 Jun 2024 18:09:57 UTC (558 KB)
    Diego Carraro and Derek Bridge

It has long been recognized that it is not enough for a Recommender System (RS) to provide recommendations based only on their relevance to users. Among many other criteria, the set of recommendations may need to be diverse. Diversity is one way of handling recommendation uncertainty and ensuring that recommendations offer users a meaningful choice. The literature reports many ways of measuring diversity and improving the diversity of a set of recommendations, most notably by re-ranking and selecting from a larger set of candidate recommendations. Driven by promising insights from the literature on how to incorporate versatile Large Language Models (LLMs) into the RS pipeline, in this paper we show how LLMs can be used for diversity re-ranking.
We begin with an informal study that verifies that LLMs can be used for re-ranking tasks and do have some understanding of the concept of item diversity. Then, we design a more rigorous methodology where LLMs are prompted to generate a diverse ranking from a candidate ranking using various prompt templates with different re-ranking instructions in a zero-shot fashion. We conduct comprehensive experiments testing state-of-the-art LLMs from the GPT and Llama families. We compare their re-ranking capabilities with random re-ranking and various traditional re-ranking methods from the literature. We open-source the code of our experiments for reproducibility. Our findings suggest that the trade-offs (in terms of performance and costs, among others) of LLM-based re-rankers are superior to those of random re-rankers but, as yet, inferior to the ones of traditional re-rankers. However, the LLM approach is promising. LLMs exhibit improved performance on many natural language processing and recommendation tasks and lower inference costs. Given these trends, we can expect LLM-based re-ranking to become more competitive soon.

------------

-----------
Index (149)
-----------

`[2406.11871] Generative AI Voting: Fair Collective Choice is Resilient to LLM Biases and Inconsistencies <https://arxiv.org/abs/2406.11871>`__ 生成式AI投票:公平的集体选择对LLM偏见和不一致有弹性

`[2406.11875] ChatPCG: Large Language Model-Driven Reward Design for Procedural Content Generation <https://arxiv.org/abs/2406.11875>`__ ChatPCG:面向程序内容生成的大型语言模型驱动奖励设计

`[2406.11911] A Notion of Complexity for Theory of Mind via Discrete World Models <https://arxiv.org/abs/2406.11911>`__ 基于离散世界模型的心智理论复杂性概念

`[2406.11938] Tracking the perspectives of interacting language models <https://arxiv.org/abs/2406.11938>`__ 交互式语言模型的视角跟踪

`[2406.11980] Prompt Design Matters for Computational Social Science Tasks but in Unpredictable Ways <https://arxiv.org/abs/2406.11980>`__ 提示设计对于计算社会科学任务很重要，但是以不可预测的方式

`[2406.12000] Look Further Ahead: Testing the Limits of GPT-4 in Path Planning <https://arxiv.org/abs/2406.12000>`__ 进一步展望:测试GPT-4在路径规划中的极限

`[2406.12043] Grade Score: Quantifying LLM Performance in Option Selection <https://arxiv.org/abs/2406.12043>`__

`[2406.12058] WellDunn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions <https://arxiv.org/abs/2406.12058>`__ WellDunn:关于语言模型和大型语言模型在识别健康维度方面的鲁棒性和可解释性

`[2406.12146] Should AI Optimize Your Code? A Comparative Study of Current Large Language Models Versus Classical Optimizing Compilers <https://arxiv.org/abs/2406.12146>`__ AI应该优化你的代码吗?当前大型语言模型与经典优化编译器的比较研究

`[2406.12172] Navigating the Labyrinth: Evaluating and Enhancing LLMs' Ability to Reason About Search Problems <https://arxiv.org/abs/2406.12172>`__ 迷宫导航:评估和增强llm对搜索问题的推理能力

`[2406.12203] InterIntent: Investigating Social Intelligence of LLMs via Intention Understanding in an Interactive Game Context <https://arxiv.org/abs/2406.12203>`__ InterIntent:在互动游戏情境中通过意图理解来研究llm的社会智能

`[2406.12227] Interpretable Catastrophic Forgetting of Large Language Model Fine-tuning via Instruction Vector <https://arxiv.org/abs/2406.12227>`__ 基于指令向量微调的大型语言模型可解释灾难性遗忘

`[2406.12232] "You Gotta be a Doctor, Lin": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations <https://arxiv.org/abs/2406.12232>`__

`[2406.12257] CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models <https://arxiv.org/abs/2406.12257>`__ CleanGen:减轻大型语言模型生成任务的后门攻击

`[2406.12259] Adversarial Attacks on Large Language Models in Medicine <https://arxiv.org/abs/2406.12259>`__ 医学大型语言模型的对抗性攻击

`[2406.12374] Problem-Solving in Language Model Networks <https://arxiv.org/abs/2406.12374>`__ 语言模型网络中的问题解决

`[2406.12670] Stealth edits for provably fixing or attacking large language models <https://arxiv.org/abs/2406.12670>`__ 用于修复或攻击大型语言模型的隐形编辑

`[2406.12018] CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling <https://arxiv.org/abs/2406.12018>`__ CItruS:面向长序列建模的分块指令感知状态移除

`[2406.12023] LiLiuM: eBay's Large Language Models for e-commerce <https://arxiv.org/abs/2406.12023>`__ LiLiuM: eBay电子商务大型语言模型

`[2406.12033] Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models <https://arxiv.org/abs/2406.12033>`__ 用大型语言模型揭示和减轻心理健康分析中的偏见

`[2406.12034] Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts <https://arxiv.org/abs/2406.12034>`__ Self-MoE:基于自专业专家的组合式大型语言模型

`[2406.12036] MedCalc-Bench: Evaluating Large Language Models for Medical Calculations <https://arxiv.org/abs/2406.12036>`__

`[2406.12038] Soft Prompting for Unlearning in Large Language Models <https://arxiv.org/abs/2406.12038>`__ 大型语言模型中遗忘的软提示

`[2406.12053] InternalInspector $I^2$: Robust Confidence Estimation in LLMs through Internal States <https://arxiv.org/abs/2406.12053>`__ InternalInspector $I^2$:通过内部状态对llm进行鲁棒置信度估计

`[2406.12069] Satyrn: A Platform for Analytics Augmented Generation <https://arxiv.org/abs/2406.12069>`__ Satyrn:一个分析增强生成平台

`[2406.12074] COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities <https://arxiv.org/abs/2406.12074>`__ COMMUNITY-CROSS-INSTRUCT:将大型语言模型与在线社区对齐的无监督指令生成

`[2406.12104] End-to-end Text-to-SQL Generation within an Analytics Insight Engine <https://arxiv.org/abs/2406.12104>`__ 在分析洞察引擎中生成端到端的文本到sql

`[2406.12109] Can LLMs Learn Macroeconomic Narratives from Social Media? <https://arxiv.org/abs/2406.12109>`__

`[2406.12114] Enhancing Text Classification through LLM-Driven Active Learning and Human Annotation <https://arxiv.org/abs/2406.12114>`__ 通过llm驱动的主动学习和人工标注增强文本分类

`[2406.12128] AI "News" Content Farms Are Easy to Make and Hard to Detect: A Case Study in Italian <https://arxiv.org/abs/2406.12128>`__ AI“新闻”内容农场很容易制作，但很难检测:意大利语案例研究

`[2406.12158] LLMs Are Prone to Fallacies in Causal Inference <https://arxiv.org/abs/2406.12158>`__ llm在因果推理中容易出现谬误

`[2406.12159] Exploring the Impact of a Transformer's Latent Space Geometry on Downstream Task Performance <https://arxiv.org/abs/2406.12159>`__ 探索Transformer潜空间几何对下游任务性能的影响

`[2406.12182] Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models <https://arxiv.org/abs/2406.12182>`__ Aqulia-Med LLM:开创全流程开源医学语言模型

`[2406.12208] Knowledge Fusion By Evolving Weights of Language Models <https://arxiv.org/abs/2406.12208>`__

`[2406.12213] LLM-Oracle Machines <https://arxiv.org/abs/2406.12213>`__ LLM-Oracle机器

`[2406.12221] On-Policy Fine-grained Knowledge Feedback for Hallucination Mitigation <https://arxiv.org/abs/2406.12221>`__ 缓解幻觉的政策细粒度知识反馈

`[2406.12223] ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations <https://arxiv.org/abs/2406.12223>`__ ToxiCloakCN:基于隐蔽性扰动的中文攻击性语言检测鲁棒性评估

`[2406.12238] PFID: Privacy First Inference Delegation Framework for LLMs <https://arxiv.org/abs/2406.12238>`__ PFID:面向llm的隐私优先推理委托框架

`[2406.12263] Defending Against Social Engineering Attacks in the Age of LLMs <https://arxiv.org/abs/2406.12263>`__ 在llm时代防御社会工程攻击

`[2406.12269] Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner for Insightful Table Summarization <https://arxiv.org/abs/2406.12269>`__ 基于问题-精确推理的隐式表知识挖掘方法

`[2406.12277] What Matters in Learning Facts in Language Models? Multifaceted Knowledge Probing with Diverse Multi-Prompt Datasets <https://arxiv.org/abs/2406.12277>`__ 在语言模型中学习事实有什么重要的?用多样化的多提示数据集进行多方面的知识探索

`[2406.12329] SNAP: Unlearning Selective Knowledge in Large Language Models with Negative Instructions <https://arxiv.org/abs/2406.12329>`__

`[2406.12347] Interpreting Bias in Large Language Models: A Feature-Based Approach <https://arxiv.org/abs/2406.12347>`__ 解释大型语言模型中的偏差:基于特征的方法

`[2406.12381] QOG:Question and Options Generation based on Language Model <https://arxiv.org/abs/2406.12381>`__ QOG:基于语言模型的问题和选项生成

`[2406.12382] From Instance Training to Instruction Learning: Task Adapters Generation from Instructions <https://arxiv.org/abs/2406.12382>`__ 从实例训练到指令学习:从指令生成任务适配器

`[2406.12397] Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models <https://arxiv.org/abs/2406.12397>`__ 揭示缺陷:探索大型语言模型合成数据的不完善和缓解策略

`[2406.12399] QueerBench: Quantifying Discrimination in Language Models Toward Queer Identities <https://arxiv.org/abs/2406.12399>`__

`[2406.12403] PDSS: A Privacy-Preserving Framework for Step-by-Step Distillation of Large Language Models <https://arxiv.org/abs/2406.12403>`__ PDSS:大型语言模型分步提炼的隐私保护框架

`[2406.12416] Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models <https://arxiv.org/abs/2406.12416>`__ 超越欠对齐:原子偏好增强的大型语言模型事实性调优

`[2406.12428] PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems <https://arxiv.org/abs/2406.12428>`__ PSLM:基于llm的低延迟口语对话系统文本和语音并行生成

`[2406.12468] Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities <https://arxiv.org/abs/2406.12468>`__ 自适应Token偏置:基于偏置关键实体的知识编辑

`[2406.12480] The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions <https://arxiv.org/abs/2406.12480>`__ llm生成的合成数据用于在线政治讨论中的立场检测的力量

`[2406.12548] P-Tailor: Customizing Personality Traits for Language Models via Mixture of Specialized LoRA Experts <https://arxiv.org/abs/2406.12548>`__ P-Tailor:通过混合专业LoRA专家为语言模型定制个性特征

`[2406.12570] Applying Ensemble Methods to Model-Agnostic Machine-Generated Text Detection <https://arxiv.org/abs/2406.12570>`__ 应用集成方法进行模型无关的机器生成文本检测

`[2406.12585] Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling <https://arxiv.org/abs/2406.12585>`__ 通过将代币生成视为集成的分类，打破了LLM社区的天花板

`[2406.12606] Low-Redundant Optimization for Large Language Model Alignment <https://arxiv.org/abs/2406.12606>`__

`[2406.12624] Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges <https://arxiv.org/abs/2406.12624>`__ 评判:评估作为评判的llms的一致性和脆弱性

`[2406.12641] DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence? <https://arxiv.org/abs/2406.12641>`__ DetectBench:大型语言模型能检测并拼凑隐含证据吗?

`[2406.12644] Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models <https://arxiv.org/abs/2406.12644>`__ 层次提示分类法:大型语言模型通用评估框架

`[2406.12645] Evaluating Transparency of Machine Generated Fact Checking Explanations <https://arxiv.org/abs/2406.12645>`__ 机器生成事实检查解释的透明度评估

`[2406.12665] CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis <https://arxiv.org/abs/2406.12665>`__ CollabStory: Multi-LLM协同故事生成与作者分析

`[2406.12673] Estimating Knowledge in Large Language Models Without Generating a Single Token <https://arxiv.org/abs/2406.12673>`__

`[2406.12679] Vernacular? I Barely Know Her: Challenges with Style Control and Stereotyping <https://arxiv.org/abs/2406.12679>`__ 方言吗?我几乎不认识她:风格控制和刻板印象方面的挑战

`[2406.12680] Measuring Psychological Depth in Language Models <https://arxiv.org/abs/2406.12680>`__ 语言模型中的心理深度测量

`[2406.12687] Using LLMs to Aid Annotation and Collection of Clinically-Enriched Data in Bipolar Disorder and Schizophrenia <https://arxiv.org/abs/2406.12687>`__ 使用llm帮助注释和收集双相情感障碍和精神分裂症的临床丰富数据

`[2406.12692] MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL <https://arxiv.org/abs/2406.12692>`__ MAGIC:为上下文中的Text-to-SQL生成自校正准则

`[2406.12702] Jailbreak Paradox: The Achilles' Heel of LLMs <https://arxiv.org/abs/2406.12702>`__ 越狱悖论:llm的阿喀琉斯之踵

`[2406.12719] On the Robustness of Language Models for Tabular Question Answering <https://arxiv.org/abs/2406.12719>`__ 表格式问答语言模型的鲁棒性研究

`[2406.12725] Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource Sound Law Induction <https://arxiv.org/abs/2406.12725>`__ 大型语言模型能像语言学家一样编码吗?:低资源健全规律归纳的案例研究

`[2406.12738] Large Language Model as a Universal Clinical Multi-task Decoder <https://arxiv.org/abs/2406.12738>`__ 大型语言模型作为临床通用多任务解码器

`[2406.12739] Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages <https://arxiv.org/abs/2406.12739>`__ 模型堆叠的自蒸馏解锁了200多种语言的跨语言NLU

`[2406.12746] Rationale-based Ensemble of Multiple QA Strategies for Zero-shot Knowledge-based VQA <https://arxiv.org/abs/2406.12746>`__ 基于理论基础的多QA策略集成的零样本知识库VQA

`[2406.12754] Chumor 1.0: A Truly Funny and Challenging Chinese Humor Understanding Dataset from Ruo Zhi Ba <https://arxiv.org/abs/2406.12754>`__ Chumor 1.0:一个真正有趣且具有挑战性的中文幽默理解数据集

`[2406.12775] Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries <https://arxiv.org/abs/2406.12775>`__

`[2406.12787] Generating Educational Materials with Different Levels of Readability using LLMs <https://arxiv.org/abs/2406.12787>`__ 使用llm生成可读性不同级别的教育材料

`[2406.12809] Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones? <https://arxiv.org/abs/2406.12809>`__ 如果大型语言模型能解决较难的问题，那么它们是否总能解决较容易的问题?

`[2406.12822] Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models? <https://arxiv.org/abs/2406.12822>`__ 这是用于多语言指令调优的好数据，还是用于大型语言模型的糟糕的多语言评估?

`[2406.12832] LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation <https://arxiv.org/abs/2406.12832>`__ LaMDA:基于谱分解低维自适应的大模型微调

`[2406.11944] Transcoders Find Interpretable LLM Feature Circuits <https://arxiv.org/abs/2406.11944>`__ 转码者寻找可解释的LLM特征电路

`[2406.11945] GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs with Large Language Models <https://arxiv.org/abs/2406.11945>`__ GAugLLM:基于大型语言模型改进文本属性图的图对比学习

`[2406.12016] Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization <https://arxiv.org/abs/2406.12016>`__ 在大型语言模型量化中，前缀注意力槽可以缓解激活异常

`[2406.12031] Large Scale Transfer Learning for Tabular Data via Language Modeling <https://arxiv.org/abs/2406.12031>`__ 基于语言建模的表格数据大规模迁移学习

`[2406.12091] Is poisoning a real threat to LLM alignment? Maybe more so than you think <https://arxiv.org/abs/2406.12091>`__ 中毒是LLM联盟的真正威胁吗?可能比你想象的更严重

`[2406.12168] BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM <https://arxiv.org/abs/2406.12168>`__ BPO:通过秉承行为邻近性LLM来强化在线偏好学习

`[2406.12246] TroL: Traversal of Layers for Large Language and Vision Models <https://arxiv.org/abs/2406.12246>`__ TroL:大型语言和视觉模型的层遍历

`[2406.12334] What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering <https://arxiv.org/abs/2406.12334>`__ 我做错了什么?量化LLMs的敏感性和一致性以促进工程

`[2406.12360] UrbanLLM: Autonomous Urban Activity Planning and Management with Large Language Models <https://arxiv.org/abs/2406.12360>`__ UrbanLLM:基于大型语言模型的自主城市活动规划与管理

`[2406.12569] MOYU: A Theoretical Study on Massive Over-activation Yielded Uplifts in LLMs <https://arxiv.org/abs/2406.12569>`__ 墨玉:LLMs中大规模过度活化的理论研究

`[2406.12649] Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models <https://arxiv.org/abs/2406.12649>`__

`[2406.12845] Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts <https://arxiv.org/abs/2406.12845>`__ 基于多目标奖励模型和专家混合的可解释偏好

`[2406.11884] Hierarchical Compression of Text-Rich Graphs via Large Language Models <https://arxiv.org/abs/2406.11884>`__ 基于大型语言模型的富文本图分层压缩

`[2406.11925] DocCGen: Document-based Controlled Code Generation <https://arxiv.org/abs/2406.11925>`__ docgen:基于文档的受控代码生成

`[2406.11930] A Critical Study of What Code-LLMs (Do Not) Learn <https://arxiv.org/abs/2406.11930>`__ 对llm(不)学习什么代码的批判性研究

`[2406.11935] Iterative or Innovative? A Problem-Oriented Perspective for Code Optimization <https://arxiv.org/abs/2406.11935>`__ 迭代还是创新?以问题为导向的代码优化视角

`[2406.12020] When Box Meets Graph Neural Network in Tag-aware Recommendation <https://arxiv.org/abs/2406.12020>`__ 标签感知推荐中Box与图神经网络相遇时

`[2406.12108] Computing in the Life Sciences: From Early Algorithms to Modern AI <https://arxiv.org/abs/2406.12108>`__ 生命科学中的计算:从早期算法到现代人工智能

`[2406.12243] CherryRec: Enhancing News Recommendation Quality via LLM-driven Framework <https://arxiv.org/abs/2406.12243>`__ CherryRec:基于llm驱动框架提高新闻推荐质量

`[2406.12296] Generative Artificial Intelligence-Guided User Studies: An Application for Air Taxi Services <https://arxiv.org/abs/2406.12296>`__ 生成式人工智能引导的用户研究——面向空中出租车服务的应用

`[2406.12479] RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding <https://arxiv.org/abs/2406.12479>`__

`[2406.12529] LLM4MSR: An LLM-Enhanced Paradigm for Multi-Scenario Recommendation <https://arxiv.org/abs/2406.12529>`__ LLM4MSR:一种llm增强的多场景推荐范式

`[2406.12651] Transforming Surgical Interventions with Embodied Intelligence for Ultrasound Robotics <https://arxiv.org/abs/2406.12651>`__ 用具身智能改造超声机器人的手术干预

`[2406.11880] Knowledge Return Oriented Prompting (KROP) <https://arxiv.org/abs/2406.11880>`__

`[2403.05680] How Well Do Multi-modal LLMs Interpret CT Scans? An Auto-Evaluation Framework for Analyses <https://arxiv.org/abs/2403.05680>`__ 多模态llm如何解释CT扫描?用于分析的自动评估框架

`[2404.10160] Reinforcement Learning from Multi-role Debates as Feedback for Bias Mitigation in LLMs <https://arxiv.org/abs/2404.10160>`__ 多角色辩论中的强化学习作为llm中偏差缓解的反馈

`[2305.12280] Contextualizing Argument Quality Assessment with Relevant Knowledge <https://arxiv.org/abs/2305.12280>`__ 结合相关知识语境化论证质量评估

`[2307.00279] Let Me Teach You: Pedagogical Foundations of Feedback for Language Models <https://arxiv.org/abs/2307.00279>`__ 让我教你:语言模型反馈的教学基础

`[2310.14735] Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review <https://arxiv.org/abs/2310.14735>`__ 释放大型语言模型中prompt工程的潜力:全面综述

`[2401.06104] Transformers are Multi-State RNNs <https://arxiv.org/abs/2401.06104>`__ transformer是多状态rnn

`[2401.07867] Authorship Obfuscation in Multilingual Machine-Generated Text Detection <https://arxiv.org/abs/2401.07867>`__ 多语言机器生成文本检测中的作者混淆问题

`[2402.06900] Can LLMs Recognize Toxicity? Definition-Based Toxicity Metric <https://arxiv.org/abs/2402.06900>`__ LLMs能识别毒性吗?基于定义的毒性度量

`[2402.11436] Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement <https://arxiv.org/abs/2402.11436>`__ 傲慢与偏见:LLM在自我完善中放大自我偏见

`[2402.11725] How Susceptible are Large Language Models to Ideological Manipulation? <https://arxiv.org/abs/2402.11725>`__ 大型语言模型受意识形态操纵的影响有多大?

`[2402.12835] PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs <https://arxiv.org/abs/2402.12835>`__ PANDA:增强llm领域能力的偏好自适应

`[2402.16367] Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications <https://arxiv.org/abs/2402.16367>`__

`[2403.04945] MEIT: Multi-Modal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation <https://arxiv.org/abs/2403.04945>`__ MEIT:面向报告生成的大型语言模型多模态心电图指令调优

`[2403.15268] Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models <https://arxiv.org/abs/2403.15268>`__ 想象力增强生成:学习在大型语言模型上想象更丰富的上下文进行问答

`[2403.16056] Qibo: A Large Language Model for Traditional Chinese Medicine <https://arxiv.org/abs/2403.16056>`__ Qibo:一个面向中医的大型语言模型

`[2404.09043] Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation <https://arxiv.org/abs/2404.09043>`__ llm玩骰子吗?探索用于行为模拟的大型语言模型中的概率分布抽样

`[2404.15846] From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models <https://arxiv.org/abs/2404.15846>`__ 从复杂到简单:增强大型语言模型的多约束复杂指令跟随能力

`[2405.13816] Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners <https://arxiv.org/abs/2405.13816>`__ 以少得多:大型语言模型是良好的自发多语言学习者

`[2405.17052] SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself <https://arxiv.org/abs/2405.17052>`__ SelfCP:通过冻结的大型语言模型本身压缩超限提示

`[2406.02224] FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models <https://arxiv.org/abs/2406.02224>`__ FedMKT:大型和小型语言模型的联邦互知识迁移

`[2406.02528] Scalable MatMul-free Language Modeling <https://arxiv.org/abs/2406.02528>`__ 可扩展的无matmull语言建模

`[2406.02721] Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller <https://arxiv.org/abs/2406.02721>`__ 通过将后缀梯度压缩为前缀控制器实现LLM行为自控

`[2406.05888] Feriji: A French-Zarma Parallel Corpus, Glossary & Translator <https://arxiv.org/abs/2406.05888>`__ Feriji:法语- zarma平行语料库、词汇表和翻译

`[2406.06576] OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step <https://arxiv.org/abs/2406.06576>`__ OccamLLM:一步快速精确语言模型算法

`[2406.07243] MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs <https://arxiv.org/abs/2406.07243>`__ MBBQ:生成式llm中原型的跨语言比较数据集

`[2406.10296] CLST: Cold-Start Mitigation in Knowledge Tracing by Aligning a Generative Language Model as a Students' Knowledge Tracer <https://arxiv.org/abs/2406.10296>`__ CLST:通过将生成语言模型对齐为学生知识追踪器来缓解知识追踪中的冷启动

`[2406.11109] Investigating Annotator Bias in Large Language Models for Hate Speech Detection <https://arxiv.org/abs/2406.11109>`__ 面向仇恨言论检测的大型语言模型标注者偏见研究

`[2406.11410] HARE: HumAn pRiors, a key to small language model Efficiency <https://arxiv.org/abs/2406.11410>`__ HARE:人类先验，小语言模型效率的关键

`[2310.07820] Large Language Models Are Zero-Shot Time Series Forecasters <https://arxiv.org/abs/2310.07820>`__

`[2401.10134] Spatial-Temporal Large Language Model for Traffic Prediction <https://arxiv.org/abs/2401.10134>`__ 面向交通预测的时空大型语言模型

`[2402.02207] Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models <https://arxiv.org/abs/2402.02207>`__ (几乎)无成本的安全微调:视觉大型语言模型的基线

`[2402.08526] Can LLMs Learn New Concepts Incrementally without Forgetting? <https://arxiv.org/abs/2402.08526>`__ llm能渐进地学习新概念而不忘记吗?

`[2403.02310] Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve <https://arxiv.org/abs/2403.02310>`__ 利用sarthi - serve优化LLM推理中的吞吐量-延迟权衡

`[2406.10918] Embodied Question Answering via Multi-LLM Systems <https://arxiv.org/abs/2406.10918>`__ 基于多llm系统的具身问答

`[2406.11675] BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models <https://arxiv.org/abs/2406.11675>`__ BLoB:基于反向传播的大型语言模型贝叶斯低秩自适应

`[2406.11686] The Role of Inherent Bellman Error in Offline Reinforcement Learning with Linear Function Approximation <https://arxiv.org/abs/2406.11686>`__ 固有Bellman误差在线性函数逼近的离线强化学习中的作用

`[2310.11604] Language Models as Zero-Shot Trajectory Generators <https://arxiv.org/abs/2310.11604>`__ 语言模型作为零样本轨迹生成器

`[2311.13721] Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning <https://arxiv.org/abs/2311.13721>`__ Nova:基于分层注意力和对比学习的汇编代码生成语言模型

`[2404.02151] Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks <https://arxiv.org/abs/2404.02151>`__ 越狱使用简单的自适应攻击引领安全对齐的llm

`[2406.10057] First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models <https://arxiv.org/abs/2406.10057>`__ 多模态大型语言模型流程图理解的首次多维评估

`[2406.11156] DELRec: Distilling Sequential Pattern to Enhance LLM-based Recommendation <https://arxiv.org/abs/2406.11156>`__ DELRec:提取序列模式增强基于llm的推荐

`[2406.08478] What If We Recaption Billions of Web Images with LLaMA-3? <https://arxiv.org/abs/2406.08478>`__ 如果我们用LLaMA-3再现数十亿的网络图像呢?

`[2406.09067] How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models <https://arxiv.org/abs/2406.09067>`__ 基于transformer的视觉编码器中的表示结构如何?视觉-语言模型中的多目标表示分析

`[2406.10958] City-LEO: Toward Transparent City Management Using LLM with End-to-End Optimization <https://arxiv.org/abs/2406.10958>`__ City- leo:使用端到端优化的LLM实现透明的城市管理

`[2305.09605] To smooth a cloud or to pin it down: Guarantees and Insights on Score Matching in Denoising Diffusion Models <https://arxiv.org/abs/2305.09605>`__ 平滑云或固定云:去噪扩散模型中分数匹配的保证和见解

`[2312.15591] Privacy-Preserved Neural Graph Databases <https://arxiv.org/abs/2312.15591>`__ 隐私保护的神经图数据库

`[2401.11506] Enhancing Recommendation Diversity by Re-ranking with Large Language Models <https://arxiv.org/abs/2401.11506>`__ 基于大型语言模型重排序的推荐多样性增强

