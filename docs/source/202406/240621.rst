240621
========

----------
Survey (2)
----------

`[2309.15857] A Survey on Image-text Multimodal Models <https://arxiv.org/abs/2309.15857>`__ 图像-文本多模态模型综述

::

    replaced with revised version Wed, 19 Jun 2024 02:53:38 GMT
    Submission history From: Jingxuan Wei [view email]
    [v1] Sat, 23 Sep 2023 15:21:15 UTC (316 KB)
    [v2] Sun, 8 Oct 2023 15:32:51 UTC (650 KB)
    [v3] Wed, 19 Jun 2024 02:53:38 UTC (556 KB)
    Ruifeng Guo, Jingxuan Wei, Linzhuang Sun, Bihui Yu, Guiyong Chang, Dawei Liu, Sibo Zhang, Zhengbing Yao, Mingjun Xu, Liping Bu

With the significant advancements of Large Language Models (LLMs) in the field of Natural Language Processing (NLP), the development of image-text multimodal models has garnered widespread attention. Current surveys on image-text multimodal models mainly focus on representative models or application domains, but lack a review on how general technical models influence the development of domain-specific models, which is crucial for domain researchers. Based on this, this paper first reviews the technological evolution of image-text multimodal models, from early explorations of feature space to visual language encoding structures, and then to the latest large model architectures. Next, from the perspective of technological evolution, we explain how the development of general image-text multimodal technologies promotes the progress of multimodal technologies in the biomedical field, as well as the importance and complexity of specific datasets in the biomedical domain. Then, centered on the tasks of image-text multimodal models, we analyze their common components and challenges. After that, we summarize the architecture, components, and data of general image-text multimodal models, and introduce the applications and improvements of image-text multimodal models in the biomedical field. Finally, we categorize the challenges faced in the development and application of general models into external factors and intrinsic factors, further refining them into 2 external factors and 5 intrinsic factors, and propose targeted solutions, providing guidance for future research directions. For more details and data, please visit our GitHub page: \url{this https URL}.

------------

`[2403.15412] Towards Measuring and Modeling "Culture" in LLMs: A Survey <https://arxiv.org/abs/2403.15412>`__ llm中“文化”的测量和建模:综述

::

    replaced with revised version Wed, 19 Jun 2024 05:43:27 GMT
    Submission history From: Muhammad Farid Adilazuarda [view email]
    [v1] Tue, 5 Mar 2024 08:29:36 UTC (1,275 KB)
    [v2] Fri, 12 Apr 2024 16:09:59 UTC (1,275 KB)
    [v3] Sat, 27 Apr 2024 07:08:24 UTC (1,276 KB)
    [v4] Wed, 19 Jun 2024 05:43:27 UTC (1,310 KB)
    Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Singh, Alham Fikri Aji, Jacki O'Neill, Ashutosh Modi, Monojit Choudhury

We present a survey of more than 90 recent papers that aim to study cultural representation and inclusion in large language models (LLMs). We observe that none of the studies explicitly define "culture, which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture". We call these aspects the proxies of culture, and organize them across two dimensions of demographic and semantic proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of ``culture,'' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness of probing techniques and situated studies on the impact of cultural mis- and under-representation in LLM-based applications.

------------

--------------
Benchmark (31)
--------------

`[2406.13094] Exploring and Benchmarking the Planning Capabilities of Large Language Models <https://arxiv.org/abs/2406.13094>`__ 大型语言模型规划能力的探索和基准测试

::

    Tue, 18 Jun 2024 22:57:06 GMT
    Bernd Bohnet, Azade Nova, Aaron T Parisi, Kevin Swersky, Katayoon Goshvadi, Hanjun Dai, Dale Schuurmans, Noah Fiedel, Hanie Sedghi

We seek to elevate the planning capabilities of Large Language Models (LLMs)investigating four main directions. First, we construct a comprehensive benchmark suite encompassing both classical planning domains and natural language scenarios. This suite includes algorithms to generate instances with varying levels of difficulty, allowing for rigorous and systematic evaluation of LLM performance. Second, we investigate the use of in-context learning (ICL) to enhance LLM planning, exploring the direct relationship between increased context length and improved planning performance. Third, we demonstrate the positive impact of fine-tuning LLMs on optimal planning paths, as well as the effectiveness of incorporating model-driven search procedures. Finally, we investigate the performance of the proposed methods in out-of-distribution scenarios, assessing the ability to generalize to novel and unseen planning challenges.

------------

`[2406.13246] GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs <https://arxiv.org/abs/2406.13246>`__ GSR-BENCH:基于多模态llm的地面空间推理评估基准

::

    Wed, 19 Jun 2024 06:15:26 GMT
    Navid Rajabi, Jana Kosecka

The ability to understand and reason about spatial relationships between objects in images is an important component of visual reasoning. This skill rests on the ability to recognize and localize objects of interest and determine their spatial relation. Early vision and language models (VLMs) have been shown to struggle to recognize spatial relations. We extend the previously released What'sUp dataset and propose a novel comprehensive evaluation for spatial relationship understanding that highlights the strengths and weaknesses of 27 different models. In addition to the VLMs evaluated in What'sUp, our extensive evaluation encompasses 3 classes of Multimodal LLMs (MLLMs) that vary in their parameter sizes (ranging from 7B to 110B), training/instruction-tuning methods, and visual resolution to benchmark their performances and scrutinize the scaling laws in this task.

------------

`[2406.13261] BeHonest: Benchmarking Honesty of Large Language Models <https://arxiv.org/abs/2406.13261>`__ BeHonest:大型语言模型的诚实度基准测试

::

    Wed, 19 Jun 2024 06:46:59 GMT
    Steffi Chern, Zhulin Hu, Yuqing Yang, Ethan Chern, Yuan Guo, Jiahe Jin, Binjie Wang, Pengfei Liu

Previous works on Large Language Models (LLMs) have mainly focused on evaluating their helpfulness or harmlessness. However, honesty, another crucial alignment criterion, has received relatively less attention. Dishonest behaviors in LLMs, such as spreading misinformation and defrauding users, eroding user trust, and causing real-world harm, present severe risks that intensify as these models approach superintelligence levels. Enhancing honesty in LLMs addresses critical deficiencies and helps uncover latent capabilities that are not readily expressed. This underscores the urgent need for reliable methods and benchmarks to effectively ensure and evaluate the honesty of LLMs.
In this paper, we introduce BeHonest, a pioneering benchmark specifically designed to assess honesty in LLMs comprehensively. BeHonest evaluates three essential aspects of honesty: awareness of knowledge boundaries, avoidance of deceit, and consistency in responses. Building on this foundation, we designed 10 scenarios to evaluate and analyze 9 popular LLMs on the market, including both closed-source and open-source models from different model families with varied model sizes. Our findings indicate that there is still significant room for improvement in the honesty of LLMs. We also encourage the AI community to prioritize honesty alignment in LLMs. Our benchmark and code can be found at: \url{https://github.com/GAIR-NLP/BeHonest}.

------------

`[2406.13340] SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words <https://arxiv.org/abs/2406.13340>`__ SD-Eval:语言之外口语对话理解基准数据集

::

    Wed, 19 Jun 2024 08:46:29 GMT
    Junyi Ao, Yuancheng Wang, Xiaohai Tian, Dekun Chen, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li, Zhizheng Wu

Speech encompasses a wealth of information, including but not limited to content, paralinguistic, and environmental information. This comprehensive nature of speech significantly impacts communication and is crucial for human-computer interaction. Chat-Oriented Large Language Models (LLMs), known for their general-purpose assistance capabilities, have evolved to handle multi-modal inputs, including speech. Although these models can be adept at recognizing and analyzing speech, they often fall short of generating appropriate responses. We argue that this is due to the lack of principles on task definition and model development, which requires open-source datasets and metrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a benchmark dataset aimed at multidimensional evaluation of spoken dialogue understanding and generation. SD-Eval focuses on paralinguistic and environmental information and includes 7,303 utterances, amounting to 8.76 hours of speech data. The data is aggregated from eight public datasets, representing four perspectives: emotion, accent, age, and background sound. To assess the SD-Eval benchmark dataset, we implement three different models and construct a training set following a similar process as SD-Eval. The training set contains 1,052.72 hours of speech data and 724.4k utterances. We also conduct a comprehensive evaluation using objective evaluation methods (e.g.
BLEU and ROUGE), subjective evaluations and LLM-based metrics for the generated responses. Models conditioned with paralinguistic and environmental information outperform their counterparts in both objective and subjective measures.
Moreover, experiments demonstrate LLM-based metrics show a higher correlation with human evaluation compared to traditional metrics. We open-source SD-Eval at https://github.com/amphionspace/SD-Eval.

------------

`[2406.13713] Benchmarking Open-Source Language Models for Efficient Question Answering in Industrial Applications <https://arxiv.org/abs/2406.13713>`__ 工业应用中高效问答的开源语言模型基准测试

::

    Wed, 19 Jun 2024 17:11:51 GMT
    Mahaman Sanoussi Yahaya Alassan and Jessica L\'opez Espejel and Merieme Bouhandi and Walid Dahhane and El Hassane Ettifouri

In the rapidly evolving landscape of Natural Language Processing (NLP), Large Language Models (LLMs) have demonstrated remarkable capabilities in tasks such as question answering (QA). However, the accessibility and practicality of utilizing these models for industrial applications pose significant challenges, particularly concerning cost-effectiveness, inference speed, and resource efficiency. This paper presents a comprehensive benchmarking study comparing open-source LLMs with their non-open-source counterparts on the task of question answering. Our objective is to identify open-source alternatives capable of delivering comparable performance to proprietary models while being lightweight in terms of resource requirements and suitable for Central Processing Unit (CPU)-based inference. Through rigorous evaluation across various metrics including accuracy, inference speed, and resource consumption, we aim to provide insights into selecting efficient LLMs for real-world applications. Our findings shed light on viable open-source alternatives that offer acceptable performance and efficiency, addressing the pressing need for accessible and efficient NLP solutions in industry settings.

------------

`[2406.13805] WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia <https://arxiv.org/abs/2406.13805>`__ WikiContradict:评估来自维基百科的llm现实世界知识冲突的基准

::

    Wed, 19 Jun 2024 20:13:42 GMT
    Yufang Hou, Alessandra Pascale, Javier Carnerero-Cano, Tigran Tchrakian, Radu Marinescu, Elizabeth Daly, Inkit Padhi, Prasanna Sattigeri

Retrieval-augmented generation (RAG) has emerged as a promising solution to mitigate the limitations of large language models (LLMs), such as hallucinations and outdated information. However, it remains unclear how LLMs handle knowledge conflicts arising from different augmented retrieved passages, especially when these passages originate from the same source and have equal trustworthiness. In this work, we conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs. Specifically, we introduce WikiContradict, a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts. We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single passage, and RAG with 2 contradictory passages.
Through rigorous human evaluations on a subset of WikiContradict instances involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and limitations of these models. For instance, when provided with two passages containing contradictory facts, all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning. Since human evaluation is costly, we also introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8. Using this automated metric, we evaluate more than 1,500 answers from seven LLMs across all WikiContradict instances. To facilitate future work, we release WikiContradict on: https://ibm.biz/wikicontradict.

------------

`[2406.13975] MR-BEN: A Comprehensive Meta-Reasoning Benchmark for Large Language Models <https://arxiv.org/abs/2406.13975>`__ MR-BEN:大型语言模型的综合元推理基准

::

    Thu, 20 Jun 2024 03:50:23 GMT
    Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, Jiaya Jia

Large language models (LLMs) have shown increasing capability in problem-solving and decision-making, largely based on the step-by-step chain-of-thought reasoning processes. However, it has been increasingly challenging to evaluate the reasoning capability of LLMs. Concretely, existing outcome-based benchmarks begin to saturate and become less sufficient to monitor the progress. To this end, we present a process-based benchmark MR-BEN that demands a meta reasoning skill, where LMs are asked to locate and analyse potential errors in automatically generated reasoning steps. MR-BEN is a comprehensive benchmark comprising 5,975 questions collected from human experts, covering various subjects such as physics, chemistry, logic, coding, and more. Through our designed metrics for assessing meta-reasoning on this benchmark, we identify interesting limitations and weaknesses of current LLMs (open-source and closed-source models). For example, open-source models are seemingly comparable to GPT-4 on outcome-based benchmarks, but they lag far behind on our benchmark, revealing the underlying reasoning capability gap between them. Our dataset and codes are available on https://randolph-zeng.github.io/Mr-Ben.github.io/.

------------

`[2406.13990] Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation <https://arxiv.org/abs/2406.13990>`__ 推理时去污染:重用泄漏的大型语言模型评估基准

::

    Thu, 20 Jun 2024 04:35:59 GMT
    Qin Zhu and Qingyuan Cheng and Runyu Peng and Xiaonan Li and Tengxiao Liu and Ru Peng and Xipeng Qiu and Xuanjing Huang

The training process of large language models (LLMs) often involves varying degrees of test data contamination. Although current LLMs are achieving increasingly better performance on various benchmarks, their performance in practical applications does not always match their benchmark results. Leakage of benchmarks can prevent the accurate assessment of LLMs' true performance.
However, constructing new benchmarks is costly, labor-intensive and still carries the risk of leakage. Therefore, in this paper, we ask the question, Can we reuse these leaked benchmarks for LLM evaluation? We propose Inference-Time Decontamination (ITD) to address this issue by detecting and rewriting leaked samples without altering their difficulties. ITD can mitigate performance inflation caused by memorizing leaked benchmarks. Our proof-of-concept experiments demonstrate that ITD reduces inflated accuracy by 22.9% on GSM8K and 19.0% on MMLU. On MMLU, using Inference-time Decontamination can lead to a decrease in the results of Phi3 and Mistral by 6.7% and 3.6% respectively. We hope that ITD can provide more truthful evaluation results for large language models.

------------

`[2406.14284] VAIYAKARANA : A Benchmark for Automatic Grammar Correction in Bangla <https://arxiv.org/abs/2406.14284>`__ VAIYAKARANA:孟加拉语自动语法纠正基准

::

    Thu, 20 Jun 2024 13:09:29 GMT
    Pramit Bhattacharyya and Arnab Bhattacharya

Bangla (Bengali) is the fifth most spoken language globally and, yet, the problem of automatic grammar correction in Bangla is still in its nascent stage. This is mostly due to the need for a large corpus of grammatically incorrect sentences, with their corresponding correct counterparts. The present state-of-the-art techniques to curate a corpus for grammatically wrong sentences involve random swapping, insertion and deletion of words.
However,these steps may not always generate grammatically wrong sentences in Bangla. In this work, we propose a pragmatic approach to generate grammatically wrong sentences in Bangla. We first categorize the different kinds of errors in Bangla into 5 broad classes and 12 finer classes. We then use these to generate grammatically wrong sentences systematically from a correct sentence. This approach can generate a large number of wrong sentences and can, thus, mitigate the challenge of lacking a large corpus for neural networks. We provide a dataset, Vaiyakarana, consisting of 92,830 grammatically incorrect sentences as well as 18,426 correct sentences. We also collected 619 human-generated sentences from essays written by Bangla native speakers. This helped us to understand errors that are more frequent. We evaluated our corpus against neural models and LLMs and also benchmark it against human evaluators who are native speakers of Bangla. Our analysis shows that native speakers are far more accurate than state-of-the-art models to detect whether the sentence is grammatically correct. Our methodology of generating erroneous sentences can be applied for most other Indian languages as well.

------------

`[2406.14434] Towards Truthful Multilingual Large Language Models: Benchmarking and Alignment Strategies <https://arxiv.org/abs/2406.14434>`__ 真实的多语言大型语言模型:基准和对齐策略

::

    Thu, 20 Jun 2024 15:59:07 GMT
    Weihao Liu, Ning Wu, Wenbiao Ding, Shining Liang, Ming Gong, Dongmei Zhang

In the era of large language models (LLMs), building multilingual large language models (MLLMs) that can serve users worldwide holds great significance. However, existing research seldom focuses on the truthfulness of MLLMs. Meanwhile, contemporary multilingual aligning technologies struggle to balance massive languages and often exhibit serious truthfulness gaps across different languages, especially those that differ greatly from English. In our work, we construct a benchmark for truthfulness evaluation in multilingual scenarios and explore the ways to align facts across languages to enhance the truthfulness of MLLMs. Furthermore, we propose Fact-aware Multilingual Selective Synergy (FaMSS) to optimize the data allocation across a large number of languages and different data types. Experimental results demonstrate that our approach can effectively reduce the multilingual representation disparity and enhance the multilingual capabilities of LLMs.

------------

`[2406.12902] Can AI Beat Undergraduates in Entry-level Java Assignments? Benchmarking Large Language Models on JavaBench <https://arxiv.org/abs/2406.12902>`__ 人工智能能在入门级Java作业中击败本科生吗?在JavaBench上对大型语言模型进行基准测试

::

    Mon, 10 Jun 2024 06:43:25 GMT
    Jialun Cao and Zhiyong Chen and Jiarong Wu and Shing-chi Cheung and Chang Xu

Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language.
95.8% of benchmarks involve Python, while only 5 benchmarks involve Java.
Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills, while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism).
To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 41.17% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench.

------------

`[2406.12928] Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox <https://arxiv.org/abs/2406.12928>`__ 

::

    Sat, 15 Jun 2024 12:02:14 GMT
    Yijun Liu, Yuan Meng, Fang Wu, Shenhao Peng, Hang Yao, Chaoyu Guan, Chen Tang, Xinzhu Ma, Zhi Wang, Wenwu Zhu

Large language models (LLMs) have exhibited exciting progress in multiple scenarios, while the huge computational demands hinder their deployments in lots of real-world applications. As an effective means to reduce memory footprint and inference cost, quantization also faces challenges in performance degradation at low bit-widths. Understanding the impact of quantization on LLM capabilities, especially the generalization ability, is crucial. However, the community's main focus remains on the algorithms and models of quantization, with insufficient attention given to whether the quantized models can retain the strong generalization abilities of LLMs. In this work, we fill this gap by providing a comprehensive benchmark suite for this research topic, including an evaluation system, detailed analyses, and a general toolbox. Specifically, based on the dominant pipeline in LLM quantization, we primarily explore the impact of calibration data distribution on the generalization of quantized LLMs and conduct the benchmark using more than 40 datasets within two main scenarios. Based on this benchmark, we conduct extensive experiments with two well-known LLMs (English and Chinese) and four quantization algorithms to investigate this topic in-depth, yielding several counter-intuitive and valuable findings, e.g., models quantized using a calibration set with the same distribution as the test data are not necessarily optimal. Besides, to facilitate future research, we also release a modular-designed toolbox, which decouples the overall pipeline into several separate components, e.g., base LLM module, dataset module, quantizer module, etc. and allows subsequent researchers to easily assemble their methods through a simple configuration.
Our benchmark suite is publicly available at https://github.com/TsingmaoAI/MI-optimize

------------

`[2406.13564] Is AI fun? HumorDB: a curated dataset and benchmark to investigate graphical humor <https://arxiv.org/abs/2406.13564>`__ AI有趣吗?HumorDB:一个精心策划的数据集和基准，用于调查图形幽默

::

    Wed, 19 Jun 2024 13:51:40 GMT
    Veedant Jain and Felipe dos Santos Alves Feitosa and Gabriel Kreiman

Despite significant advancements in computer vision, understanding complex scenes, particularly those involving humor, remains a substantial challenge.
This paper introduces HumorDB, a novel image-only dataset specifically designed to advance visual humor understanding. HumorDB consists of meticulously curated image pairs with contrasting humor ratings, emphasizing subtle visual cues that trigger humor and mitigating potential biases. The dataset enables evaluation through binary classification(Funny or Not Funny), range regression(funniness on a scale from 1 to 10), and pairwise comparison tasks(Which Image is Funnier?), effectively capturing the subjective nature of humor perception.
Initial experiments reveal that while vision-only models struggle, vision-language models, particularly those leveraging large language models, show promising results. HumorDB also shows potential as a valuable zero-shot benchmark for powerful large multimodal models. We open-source both the dataset and code under the CC BY 4.0 license.

------------

`[2406.14294] DASB -- Discrete Audio and Speech Benchmark <https://arxiv.org/abs/2406.14294>`__ 

::

    Thu, 20 Jun 2024 13:23:27 GMT
    Pooneh Mousavi, Luca Della Libera, Jarod Duret, Artem Ploujnikov, Cem Subakan, Mirco Ravanelli

Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.

------------

`[2406.13219] MC-MKE: A Fine-Grained Multimodal Knowledge Editing Benchmark Emphasizing Modality Consistency <https://arxiv.org/abs/2406.13219>`__ MC-MKE:强调模态一致性的细粒度多模态知识编辑基准

::

    Wed, 19 Jun 2024 05:15:21 GMT
    Junzhe Zhang, Huixuan Zhang, Xunjian Yin, Baizhou Huang, Xu Zhang, Xinyu Hu, Xiaojun Wan

Multimodal large language models (MLLMs) are prone to non-factual or outdated knowledge issues, which can manifest as misreading and misrecognition errors due to the complexity of multimodal knowledge. Previous benchmarks have not systematically analyzed the performance of editing methods in correcting these two error types. To better represent and correct these errors, we decompose multimodal knowledge into its visual and textual components. Different error types correspond to different editing formats, which edits distinct part of the multimodal knowledge. We present MC-MKE, a fine-grained Multimodal Knowledge Editing benchmark emphasizing Modality Consistency. Our benchmark facilitates independent correction of misreading and misrecognition errors by editing the corresponding knowledge component. We evaluate three multimodal knowledge editing methods on MC-MKE, revealing their limitations, particularly in terms of modality consistency. Our work highlights the challenges posed by multimodal knowledge editing and motivates further research in developing effective techniques for this task.

------------

`[2406.14496] African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification <https://arxiv.org/abs/2406.14496>`__ 非洲燕还是欧洲燕?对细粒度物体分类的大型视觉-语言模型进行基准测试

::

    Thu, 20 Jun 2024 16:59:39 GMT
    Gregor Geigle, Radu Timofte, Goran Glava\v{s}

Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities on numerous image understanding and reasoning tasks. The task of fine-grained object classification (e.g., distinction between \textit{animal species}), however, has been probed insufficiently, despite its downstream importance. We fill this evaluation gap by creating \texttt{FOCI} (\textbf{F}ine-grained \textbf{O}bject \textbf{C}lass\textbf{I}fication), a difficult multiple-choice benchmark for fine-grained object classification, from existing object classification datasets: (1) multiple-choice avoids ambiguous answers associated with casting classification as open-ended QA task; (2) we retain classification difficulty by mining negative labels with a CLIP model.
\texttt{FOCI}\xspace complements five popular classification datasets with four domain-specific subsets from ImageNet-21k. We benchmark 12 public LVLMs on \texttt{FOCI} and show that it tests for a \textit{complementary skill} to established image understanding and reasoning benchmarks. Crucially, CLIP models exhibit dramatically better performance than LVLMs. Since the image encoders of LVLMs come from these CLIP models, this points to inadequate alignment for fine-grained object distinction between the encoder and the LLM and warrants (pre)training data with more fine-grained annotation. We release our code at \url{https://github.com/gregor-ge/FOCI-Benchmark}.

------------

`[2403.15879] TrustSQL: Benchmarking Text-to-SQL Reliability with Penalty-Based Scoring <https://arxiv.org/abs/2403.15879>`__ TrustSQL:基于评分的文本到sql可靠性基准测试

::

    replaced with revised version Thu, 20 Jun 2024 05:43:30 GMT
    Submission history From: Gyubok Lee [view email]
    [v1] Sat, 23 Mar 2024 16:12:52 UTC (82 KB)
    [v2] Tue, 16 Apr 2024 15:33:39 UTC (86 KB)
    [v3] Sat, 8 Jun 2024 16:56:45 UTC (90 KB)
    [v4] Fri, 14 Jun 2024 15:39:28 UTC (528 KB)
    [v5] Thu, 20 Jun 2024 05:43:30 UTC (528 KB)
    Gyubok Lee, Woosog Chay, Seonhee Cho, Edward Choi

Text-to-SQL enables users to interact with databases using natural language, simplifying the retrieval and synthesis of information. Despite the remarkable success of large language models (LLMs) in translating natural language questions into SQL queries, widespread deployment remains limited due to two primary challenges. First, the effective use of text-to-SQL models depends on users' understanding of the model's capabilities-the scope of questions the model can correctly answer. Second, the absence of abstention mechanisms can lead to incorrect SQL generation going unnoticed, thereby undermining trust in the model's output. To enable wider deployment, it is crucial to address these challenges in model design and enhance model evaluation to build trust in the model's output. To this end, we introduce TrustSQL, a novel comprehensive benchmark designed to evaluate text-to-SQL reliability-defined as a model's ability to correctly handle any type of input question by generating correct SQL queries for feasible questions and abstaining from generating infeasible ones (e.g., due to schema incompatibility or functionalities beyond SQL). We evaluate existing methods using a novel penalty-based scoring metric with two modeling approaches: (1) pipeline-based methods combining SQL generators with infeasible question detectors and SQL error detectors for abstention; and (2) unified methods using a single model for the entire task. Our experimental results reveal that achieving high scores under severe penalties requires significant effort and provide a new perspective on developing text-to-SQL models for safer deployment. TrustSQL is available at this https URL.

------------

`[2406.12072] DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs <https://arxiv.org/abs/2406.12072>`__ DTGB:动态文本属性图综合基准

::

    replaced with revised version Wed, 19 Jun 2024 03:58:35 GMT
    Submission history From: Jiasheng Zhang [view email]
    [v1] Mon, 17 Jun 2024 20:16:12 UTC (1,175 KB)
    [v2] Wed, 19 Jun 2024 03:58:35 UTC (1,175 KB)
    Jiasheng Zhang, Jialin Chen, Menglin Yang, Aosong Feng, Shuang Liang, Jie Shao, and Rex Ying

Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world scenarios, where each node and edge are associated with text descriptions, and both the graph structure and text descriptions evolve over time. Despite their broad applicability, there is a notable scarcity of benchmark datasets tailored to DyTAGs, which hinders the potential advancement in many research fields. To address this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB), a collection of large-scale, time-evolving graphs from diverse domains, with nodes and edges enriched by dynamically changing text attributes and categories. To facilitate the use of DTGB, we design standardized evaluation procedures based on four real-world use cases: future link prediction, destination node retrieval, edge classification, and textual relation generation. These tasks require models to understand both dynamic graph structures and natural language, highlighting the unique challenges posed by DyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB, evaluating 7 popular dynamic graph learning algorithms and their variants of adapting to text attributes with LLM embeddings, along with 6 powerful large language models (LLMs). Our results show the limitations of existing models in handling DyTAGs. Our analysis also demonstrates the utility of DTGB in investigating the incorporation of structural and textual dynamics. The proposed DTGB fosters research on DyTAGs and their broad applications. It offers a comprehensive benchmark for evaluating and advancing models to handle the interplay between dynamic graph structures and natural language. The dataset and source code are available at this https URL.

------------

`[2308.14508] LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding <https://arxiv.org/abs/2308.14508>`__ LongBench:面向长上下文理解的双语多任务基准

::

    replaced with revised version Wed, 19 Jun 2024 04:00:32 GMT
    Submission history From: Yushi Bai [view email]
    [v1] Mon, 28 Aug 2023 11:53:40 UTC (8,499 KB)
    [v2] Wed, 19 Jun 2024 04:00:32 UTC (8,513 KB)
    Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li

Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability. The code and datasets are available at this https URL.

------------

`[2311.16421] CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models <https://arxiv.org/abs/2311.16421>`__ CDEval:大型语言模型文化维度测量基准

::

    replaced with revised version Thu, 20 Jun 2024 11:52:47 GMT
    Submission history From: Yuhang Wang [view email]
    [v1] Tue, 28 Nov 2023 02:01:25 UTC (10,348 KB)
    [v2] Wed, 7 Feb 2024 02:38:02 UTC (17,208 KB)
    [v3] Thu, 20 Jun 2024 11:52:47 UTC (17,520 KB)
    Yuhang Wang, Yanxu Zhu, Chao Kong, Shuyu Wei, Xiaoyuan Yi, Xing Xie and Jitao Sang

As the scaling of Large Language Models (LLMs) has dramatically enhanced their capabilities, there has been a growing focus on the alignment problem to ensure their responsible and ethical use. While existing alignment efforts predominantly concentrate on universal values such as the HHH principle, the aspect of culture, which is inherently pluralistic and diverse, has not received adequate attention. This work introduces a new benchmark, CDEval, aimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by incorporating both GPT-4's automated generation and human verification, covering six cultural dimensions across seven domains. Our comprehensive experiments provide intriguing insights into the culture of mainstream LLMs, highlighting both consistencies and variations across different dimensions and domains. The findings underscore the importance of integrating cultural considerations in LLM development, particularly for applications in diverse cultural settings. Through CDEval, we aim to broaden the horizon of LLM alignment research by including cultural dimensions, thus providing a more holistic framework for the future development and evaluation of LLMs. This benchmark serves as a valuable resource for cultural studies in LLMs, paving the way for more culturally aware and sensitive models.

------------

`[2402.12659] FinBen: A Holistic Financial Benchmark for Large Language Models <https://arxiv.org/abs/2402.12659>`__ FinBen:大型语言模型的整体金融基准

::

    replaced with revised version Wed, 19 Jun 2024 03:38:56 GMT
    Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, Zheheng Luo, Tianlin Zhang, Zhiwei Liu, Guojun Xiong, Zhiyang Deng, Yuechen Jiang, Zhiyuan Yao, Haohang Li, Yangyang Yu, Gang Hu, Jiajia Huang, Xiao-Yang Liu, Alejandro Lopez-Lira, Benyou Wang, Yanzhao Lai, Hao Wang, Min Peng, Sophia Ananiadou, and Jimin Huang

Categories

------------

`[2403.07714] StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models <https://arxiv.org/abs/2403.07714>`__ StableToolBench:面向大型语言模型工具学习的稳定大规模基准测试

::

    replaced with revised version Wed, 19 Jun 2024 11:59:08 GMT
    Submission history From: Zhicheng Guo [view email]
    [v1] Tue, 12 Mar 2024 14:57:40 UTC (765 KB)
    [v2] Wed, 13 Mar 2024 14:08:19 UTC (765 KB)
    [v3] Fri, 14 Jun 2024 07:19:56 UTC (768 KB)
    [v4] Wed, 19 Jun 2024 11:59:08 UTC (768 KB)
    Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, Yang Liu

Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system.

------------

`[2404.08676] ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming <https://arxiv.org/abs/2404.08676>`__ ALERT:通过红色团队评估大型语言模型安全性的综合基准

::

    replaced with revised version Thu, 20 Jun 2024 07:23:06 GMT
    Submission history From: Simone Tedeschi [view email]
    [v1] Sat, 6 Apr 2024 15:01:47 UTC (1,404 KB)
    [v2] Thu, 20 Jun 2024 07:23:06 UTC (1,149 KB)
    Simone Tedeschi, Felix Friedrich, Patrick Schramowski, Kristian Kersting, Roberto Navigli, Huu Nguyen, Bo Li

When building Large Language Models (LLMs), it is paramount to bear safety in mind and protect them with guardrails. Indeed, LLMs should never generate content promoting or normalizing harmful, illegal, or unethical behavior that may contribute to harm to individuals or society. This principle applies to both normal and adversarial use. In response, we introduce ALERT, a large-scale benchmark to assess safety based on a novel fine-grained risk taxonomy. It is designed to evaluate the safety of LLMs through red teaming methodologies and consists of more than 45k instructions categorized using our novel taxonomy. By subjecting LLMs to adversarial testing scenarios, ALERT aims to identify vulnerabilities, inform improvements, and enhance the overall safety of the language models. Furthermore, the fine-grained taxonomy enables researchers to perform an in-depth evaluation that also helps one to assess the alignment with various policies. In our experiments, we extensively evaluate 10 popular open- and closed-source LLMs and demonstrate that many of them still struggle to attain reasonable levels of safety.

------------

`[2404.10508] White Men Lead, Black Women Help? Benchmarking Language Agency Social Biases in LLMs <https://arxiv.org/abs/2404.10508>`__ 白人男性领导，黑人女性帮忙?llm中的语言代理社会偏见基准测试

::

    replaced with revised version Thu, 20 Jun 2024 07:44:17 GMT
    Submission history From: Yixin Wan [view email]
    [v1] Tue, 16 Apr 2024 12:27:54 UTC (1,485 KB)
    [v2] Mon, 17 Jun 2024 21:36:46 UTC (5,118 KB)
    [v3] Thu, 20 Jun 2024 07:44:17 UTC (5,118 KB)
    Yixin Wan, Kai-Wei Chang

Language agency is an important aspect of evaluating social biases in texts. While several studies approached agency-related bias in human-written language, very limited research has investigated such biases in Large Language Model (LLM)-generated content. In addition, previous research often relies on string-matching techniques to identify agentic and communal words within texts, which fall short of accurately classifying language agency. We introduce the novel Language Agency Bias Evaluation (LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing agency levels attributed to different demographic groups in model generations. LABE leverages 5,400 template-based prompts, an accurate agency classifier, and corresponding bias metrics to test for gender, racial, and intersectional language agency biases in LLMs on 3 text generation tasks: biographies, professor reviews, and reference letters. To build better and more accurate automated agency classifiers, we also contribute and release the Language Agency Classification (LAC) dataset, consisting of 3,724 agentic and communal sentences. Using LABE, we unveil previously under-explored language agency social biases in 3 recent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) For the same text category, LLM generations demonstrate higher levels of gender bias than human-written texts; (2) On most generation tasks, models show remarkably higher levels of intersectional bias than the other bias aspects. Those who are at the intersection of gender and racial minority groups -- such as Black females -- are consistently described by texts with lower levels of agency; (3) Among the 3 LLMs investigated, Llama3 demonstrates greatest overall bias in language agency; (4) Not only does prompt-based mitigation fail to resolve language agency bias in LLMs, but it frequently leads to the exacerbation of biases in generated texts.

------------

`[2405.20947] OR-Bench: An Over-Refusal Benchmark for Large Language Models <https://arxiv.org/abs/2405.20947>`__ OR-Bench:大型语言模型的过度拒绝基准

::

    replaced with revised version Thu, 20 Jun 2024 05:22:38 GMT
    Submission history From: Justin Cui [view email]
    [v1] Fri, 31 May 2024 15:44:33 UTC (1,731 KB)
    [v2] Thu, 20 Jun 2024 05:22:38 UTC (1,653 KB)
    Justin Cui, Wei-Lin Chiang, Ion Stoica, Cho-Jui Hsieh

Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that appear harmful but are benign. This study proposes a novel method for automatically generating large-scale sets of "seemingly toxic prompts" (benign prompts likely rejected by LLMs). Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 seemingly toxic prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 25 popular LLMs across 8 model families. Our datasets are available at this https URL and the demo can be found at this https URL. We hope this benchmark can help the community develop better safety aligned models.

------------

`[2406.12066] Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks <https://arxiv.org/abs/2406.12066>`__ 在生物医学基准中，语言模型对药物名称的影响令人惊讶地脆弱

::

    replaced with revised version Wed, 19 Jun 2024 03:59:41 GMT
    Submission history From: Shan Chen [view email]
    [v1] Mon, 17 Jun 2024 20:09:24 UTC (2,752 KB)
    [v2] Wed, 19 Jun 2024 03:59:41 UTC (2,752 KB)
    Jack Gallifant, Shan Chen, Pedro Moreira, Nikolaj Munch, Mingye Gao, Jackson Pond, Leo Anthony Celi, Hugo Aerts, Thomas Hartvigsen, Danielle Bitterman

Medical knowledge is context-dependent and requires consistent reasoning across various natural language expressions of semantically equivalent phrases. This is particularly crucial for drug names, where patients often use brand names like Advil or Tylenol instead of their generic equivalents. To study this, we create a new robustness dataset, RABBITS, to evaluate performance differences on medical benchmarks after swapping brand and generic drug names using physician expert annotations.
We assess both open-source and API-based LLMs on MedQA and MedMCQA, revealing a consistent performance drop ranging from 1-10\%. Furthermore, we identify a potential source of this fragility as the contamination of test data in widely used pre-training datasets. All code is accessible at this https URL, and a HuggingFace leaderboard is available at this https URL.

------------

`[2406.12572] Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models <https://arxiv.org/abs/2406.12572>`__ Mathador-LM:大型语言模型数学推理动态基准

::

    replaced with revised version Wed, 19 Jun 2024 12:28:10 GMT
    Submission history From: Amir Moeini [view email]
    [v1] Tue, 18 Jun 2024 13:02:12 UTC (801 KB)
    [v2] Wed, 19 Jun 2024 12:28:10 UTC (805 KB)
    Eldar Kurtic, Amir Moeini, Dan Alistarh

We introduce Mathador-LM, a new benchmark for evaluating the mathematical reasoning on large language models (LLMs), combining ruleset interpretation, planning, and problem-solving. This benchmark is inspired by the Mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances dynamically, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. Additionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary models struggle with Mathador-LM, scoring significantly lower than average 3rd graders. This stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks.

------------

`[2406.01607] Recent advances in text embedding: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark <https://arxiv.org/abs/2406.01607>`__ 文本嵌入的最新进展:MTEB基准上表现最好的方法的全面综述

::

    replaced with revised version Wed, 19 Jun 2024 06:52:13 GMT
    Submission history From: Hongliu Cao [view email]
    [v1] Mon, 27 May 2024 09:52:54 UTC (551 KB)
    [v2] Wed, 19 Jun 2024 06:52:13 UTC (1,156 KB)
    Hongliu Cao

Text embedding methods have become increasingly popular in both industrial and academic fields due to their critical role in a variety of natural language processing tasks. The significance of universal text embeddings has been further highlighted with the rise of Large Language Models (LLMs) applications such as Retrieval-Augmented Systems (RAGs). While previous models have attempted to be general-purpose, they often struggle to generalize across tasks and domains. However, recent advancements in training data quantity, quality and diversity; synthetic data generation from LLMs as well as using LLMs as backbones encourage great improvements in pursuing universal text embeddings. In this paper, we provide an overview of the recent advances in universal text embedding models with a focus on the top performing text embeddings on Massive Text Embedding Benchmark (MTEB). Through detailed comparison and analysis, we highlight the key contributions and limitations in this area, and propose potentially inspiring future research directions.

------------

`[2406.04264] MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding <https://arxiv.org/abs/2406.04264>`__ MLVU:多任务长视频理解的综合基准

::

    replaced with revised version Wed, 19 Jun 2024 09:04:38 GMT
    Submission history From: Junjie Zhou [view email]
    [v1] Thu, 6 Jun 2024 17:09:32 UTC (33,786 KB)
    [v2] Wed, 19 Jun 2024 09:04:38 UTC (33,786 KB)
    Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, Zheng Liu

The evaluation of Long Video Understanding (LVU) performance poses an important but challenging research problem. Despite previous efforts, the existing video understanding benchmarks are severely constrained by several issues, especially the insufficient lengths of videos, a lack of diversity in video types and evaluation tasks, and the inappropriateness for evaluating LVU performances. To address the above problems, we propose a new benchmark, called MLVU (Multi-task Long Video Understanding Benchmark), for the comprehensive and in-depth evaluation of LVU. MLVU presents the following critical values: 1) The substantial and flexible extension of video lengths, which enables the benchmark to evaluate LVU performance across a wide range of durations. 2) The inclusion of various video genres, e.g., movies, surveillance footage, egocentric videos, cartoons, game videos, etc., which reflects the models' LVU performances in different scenarios. 3) The development of diversified evaluation tasks, which enables a comprehensive examination of MLLMs' key abilities in long-video understanding. The empirical study with 20 latest MLLMs reveals significant room for improvement in today's technique, as all existing methods struggle with most of the evaluation tasks and exhibit severe performance degradation when handling longer videos. Additionally, it suggests that factors such as context length, image-understanding quality, and the choice of LLM backbone can play critical roles in future advancements. We anticipate that MLVU will advance the research of long video understanding by providing a comprehensive and in-depth analysis of MLLMs.

------------

`[2406.11927] REPOEXEC: Evaluate Code Generation with a Repository-Level Executable Benchmark <https://arxiv.org/abs/2406.11927>`__ REPOEXEC:使用存储库级可执行基准评估代码生成

::

    replaced with revised version Wed, 19 Jun 2024 05:27:32 GMT
    Submission history From: Nghi D. Q. Bui [view email]
    [v1] Mon, 17 Jun 2024 10:45:22 UTC (1,036 KB)
    [v2] Wed, 19 Jun 2024 05:27:32 UTC (1,003 KB)
    Nam Le Hai, Dung Manh Nguyen, Nghi D. Q. Bui

The ability of CodeLLMs to generate executable and functionally correct code at the repository-level scale remains largely unexplored. We introduce RepoExec, a novel benchmark for evaluating code generation at the repository-level scale. RepoExec focuses on three main aspects: executability, functional correctness through automated test case generation with high coverage rate, and carefully crafted cross-file contexts to accurately generate code. Our work explores a controlled scenario where developers specify necessary code dependencies, challenging the model to integrate these accurately. Experiments show that while pretrained LLMs outperform instruction-tuned models in correctness, the latter excel in utilizing provided dependencies and demonstrating debugging capabilities. We also introduce a new instruction-tuned dataset that focuses on code dependencies and demonstrate that CodeLLMs fine-tuned on our dataset have a better capability to leverage these dependencies effectively. RepoExec aims to provide a comprehensive evaluation of code functionality and alignment with developer intent, paving the way for more reliable and applicable CodeLLMs in real-world scenarios. The dataset and source code can be found at~\url{this https URL}.

------------

`[2402.15810] OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining <https://arxiv.org/abs/2402.15810>`__ OAG-Bench:人工策划的学术图挖掘基准

::

    replaced with revised version Thu, 20 Jun 2024 04:15:12 GMT
    Submission history From: Fanjin Zhang [view email]
    [v1] Sat, 24 Feb 2024 13:15:54 UTC (912 KB)
    [v2] Thu, 20 Jun 2024 04:15:12 UTC (556 KB)
    Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin Chen, Lulu Wang, Qingfei Zhao, Yuqing Cheng, Tianyi Han, Yuwei An, Dan Zhang, Weng Lam Tam, Kun Cao, Yunhe Pang, Xinyu Guan, Huihui Yuan, Jian Song, Xiaoyan Li, Yuxiao Dong, Jie Tang

With the rapid proliferation of scientific literature, versatile academic knowledge services increasingly rely on comprehensive academic graph mining. Despite the availability of public academic graphs, benchmarks, and datasets, these resources often fall short in multi-aspect and fine-grained annotations, are constrained to specific task types and domains, or lack underlying real academic graphs. In this paper, we present OAG-Bench, a comprehensive, multi-aspect, and fine-grained human-curated benchmark based on the Open Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines, and 120+ experimental results to date. We propose new data annotation strategies for certain tasks and offer a suite of data pre-processing codes, algorithm implementations, and standardized evaluation protocols to facilitate academic graph mining. Extensive experiments reveal that even advanced algorithms like large language models (LLMs) encounter difficulties in addressing key challenges in certain tasks, such as paper source tracing and scholar profiling. We also introduce the Open Academic Graph Challenge (OAG-Challenge) to encourage community input and sharing. We envisage that OAG-Bench can serve as a common ground for the community to evaluate and compare algorithms in academic graph mining, thereby accelerating algorithm development and advancement in this field. OAG-Bench is accessible at this https URL.

------------

---------------
Accelerate (18)
---------------

`[2406.13046] Bayesian-LoRA: LoRA based Parameter Efficient Fine-Tuning using Optimal Quantization levels and Rank Values trough Differentiable Bayesian Gates <https://arxiv.org/abs/2406.13046>`__ 贝叶斯-LoRA:可微贝叶斯门中使用最优量化水平和秩值的基于LoRA的参数高效微调

::

    Tue, 18 Jun 2024 20:26:30 GMT
    Cristian Meo, Ksenia Sycheva, Anirudh Goyal, Justin Dauwels

It is a common practice in natural language processing to pre-train a single model on a general domain and then fine-tune it for downstream tasks. However, when it comes to Large Language Models, fine-tuning the entire model can be computationally expensive, resulting in very intensive energy consumption. As a result, several Parameter efficient fine-tuning (PEFT) approaches were recently proposed. One of the most popular approaches is low-rank adaptation (LoRA), where the key insight is decomposing the update weights of the pre-trained model into two low-rank matrices. However, the proposed approaches either use the same rank value across all different weight matrices or do not use any quantization technique, which has been shown to be one of the most important factors when it comes to a model's energy consumption. In this work, we propose Bayesian-LoRA (B-LoRA) which approaches matrix decomposition and quantization from a Bayesian perspective by employing a prior distribution on both quantization levels and rank values of the learned low-rank matrices. As a result, B-LoRA is able to fine-tune a pre-trained model on a specific downstream task, finding the optimal rank values and quantization levels for every low-rank matrix. We validate the proposed model fine-tuning a pre-trained DeBERTaV3 on the GLUE benchmark. Moreover, we compare it to relevant baselines and present both qualitative and quantitative results, showing how the proposed approach is able to learn optimal-rank quantized matrices. B-LoRA performs on par or better than baselines while reducing the total amount of bit operations of roughly 70% with respect to the baselines ones.

------------

`[2406.13170] Amphista: Accelerate LLM Inference with Bi-directional Multiple Drafting Heads in a Non-autoregressive Style <https://arxiv.org/abs/2406.13170>`__ Amphista:以非自回归风格使用双向多个绘图头加速LLM推理

::

    Wed, 19 Jun 2024 02:53:39 GMT
    Zeping Li, Xinlong Yang, Ziheng Gao, Ji Liu, Zhuang Liu, Dong Li, Jinzhang Peng, Lu Tian, Emad Barsoum

Large Language Models (LLMs) inherently use autoregressive decoding, which lacks parallelism in inference and results in significantly slow inference speeds, especially when hardware parallel accelerators and memory bandwidth are not fully utilized. In this work, we propose Amphista, a speculative decoding algorithm that adheres to a non-autoregressive decoding paradigm. Owing to the increased parallelism, our method demonstrates higher efficiency in inference compared to autoregressive methods. Specifically, Amphista models an Auto-embedding Block capable of parallel inference, incorporating bi-directional attention to enable interaction between different drafting heads. Additionally, Amphista implements Staged Adaptation Layers to facilitate the transition of semantic information from the base model's autoregressive inference to the drafting heads' non-autoregressive speculation, thereby achieving paradigm transformation and feature fusion. We conduct a series of experiments on a suite of Vicuna models using MT-Bench and Spec-Bench. For the Vicuna 33B model, Amphista achieves up to 2.75$\times$ and 1.40$\times$ wall-clock acceleration compared to vanilla autoregressive decoding and Medusa, respectively, while preserving lossless generation quality.

------------

`[2406.14066] Optimizing Speculative Decoding for Serving Large Language Models Using Goodput <https://arxiv.org/abs/2406.14066>`__ 使用Goodput优化推测解码以服务于大型语言模型

::

    Thu, 20 Jun 2024 07:43:33 GMT
    Xiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung, Zhijie Deng, Ion Stoica, Hao Zhang

Reducing the inference latency of large language models (LLMs) is crucial, and speculative decoding (SD) stands out as one of the most effective techniques. Rather than letting the LLM generate all tokens directly, speculative decoding employs effective proxies to predict potential outputs, which are then verified by the LLM without compromising the generation quality.
Yet, deploying SD in real online LLM serving systems (with continuous batching) does not always yield improvement -- under higher request rates or low speculation accuracy, it paradoxically increases latency. Furthermore, there is no best speculation length work for all workloads under different system loads.
Based on the observations, we develop a dynamic framework SmartSpec. SmartSpec dynamically determines the best speculation length for each request (from 0, i.e., no speculation, to many tokens) -- hence the associated speculative execution costs -- based on a new metric called goodput, which characterizes the current observed load of the entire system and the speculation accuracy. We show that SmartSpec consistently reduces average request latency by up to 3.2x compared to non-speculative decoding baselines across different sizes of target models, draft models, request rates, and datasets. Moreover, SmartSpec can be applied to different styles of speculative decoding, including traditional, model-based approaches as well as model-free methods like prompt lookup and tree-style decoding.

------------

`[2406.14164] A Data-Driven Guided Decoding Mechanism for Diagnostic Captioning <https://arxiv.org/abs/2406.14164>`__ 一种数据驱动的诊断描述引导解码机制

::

    Thu, 20 Jun 2024 10:08:17 GMT
    Panagiotis Kaliosis, John Pavlopoulos, Foivos Charalampakos, Georgios Moschovis, Ion Androutsopoulos

Diagnostic Captioning (DC) automatically generates a diagnostic text from one or more medical images (e.g., X-rays, MRIs) of a patient. Treated as a draft, the generated text may assist clinicians, by providing an initial estimation of the patient's condition, speeding up and helping safeguard the diagnostic process. The accuracy of a diagnostic text, however, strongly depends on how well the key medical conditions depicted in the images are expressed. We propose a new data-driven guided decoding method that incorporates medical information, in the form of existing tags capturing key conditions of the image(s), into the beam search of the diagnostic text generation process. We evaluate the proposed method on two medical datasets using four DC systems that range from generic image-to-text systems with CNN encoders and RNN decoders to pre-trained Large Language Models. The latter can also be used in few- and zero-shot learning scenarios. In most cases, the proposed mechanism improves performance with respect to all evaluation measures. We provide an open-source implementation of the proposed method at https://github.com/nlpaueb/dmmcs.

------------

`[2406.13035] D2O:Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models <https://arxiv.org/abs/2406.13035>`__ D2O:大型语言模型高效生成推理的动态判别操作

::

    Tue, 18 Jun 2024 20:01:51 GMT
    Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, Mi Zhang

Efficient inference in Large Language Models (LLMs) is impeded by the growing memory demands of key-value (KV) caching, especially for longer sequences.
Traditional KV cache eviction strategies, which prioritize less critical KV-pairs based on attention scores, often degrade generation quality, leading to issues such as context loss or hallucinations. To address this, we introduce Dynamic Discriminative Operations (D2O), a novel method that utilizes two-level discriminative strategies to optimize KV cache size without fine-tuning, while preserving essential context. Initially, by observing varying densities of attention weights between shallow and deep layers, we use this insight to determine which layers should avoid excessive eviction to minimize information loss. Subsequently, for the eviction strategy in each layer, D2O innovatively incorporates a compensation mechanism that maintains a similarity threshold to re-discriminate the importance of previously discarded tokens, determining whether they should be recalled and merged with similar tokens. Our approach not only achieves significant memory savings and enhances inference throughput by more than 3x but also maintains high-quality long-text generation. Extensive experiments across various benchmarks and LLM architectures have demonstrated that D2O significantly enhances performance with a constrained KV cache budget.

------------

`[2406.13443] Dual-Phase Accelerated Prompt Optimization <https://arxiv.org/abs/2406.13443>`__ 双阶段加速提示优化

::

    Wed, 19 Jun 2024 11:08:56 GMT
    Muchen Yang, Moxin Li, Yongle Li, Zijun Chen, Chongming Gao, Junqi Zhang, Yangyang Li, Fuli Feng

Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.

------------

`[2406.13713] Benchmarking Open-Source Language Models for Efficient Question Answering in Industrial Applications <https://arxiv.org/abs/2406.13713>`__ 工业应用中高效问答的开源语言模型基准测试

::

    Wed, 19 Jun 2024 17:11:51 GMT
    Mahaman Sanoussi Yahaya Alassan and Jessica L\'opez Espejel and Merieme Bouhandi and Walid Dahhane and El Hassane Ettifouri

In the rapidly evolving landscape of Natural Language Processing (NLP), Large Language Models (LLMs) have demonstrated remarkable capabilities in tasks such as question answering (QA). However, the accessibility and practicality of utilizing these models for industrial applications pose significant challenges, particularly concerning cost-effectiveness, inference speed, and resource efficiency. This paper presents a comprehensive benchmarking study comparing open-source LLMs with their non-open-source counterparts on the task of question answering. Our objective is to identify open-source alternatives capable of delivering comparable performance to proprietary models while being lightweight in terms of resource requirements and suitable for Central Processing Unit (CPU)-based inference. Through rigorous evaluation across various metrics including accuracy, inference speed, and resource consumption, we aim to provide insights into selecting efficient LLMs for real-world applications. Our findings shed light on viable open-source alternatives that offer acceptable performance and efficiency, addressing the pressing need for accessible and efficient NLP solutions in industry settings.

------------

`[2406.12930] Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime Requantization <https://arxiv.org/abs/2406.12930>`__ Tender:通过张量分解和运行时重量化加速大型语言模型

::

    Sun, 16 Jun 2024 09:51:55 GMT
    Jungi Lee, Wonbeom Lee, Jaewoong Sim

Large language models (LLMs) demonstrate outstanding performance in various tasks in machine learning and have thus become one of the most important workloads in today's computing landscape. However, deploying LLM inference poses challenges due to the high compute and memory requirements stemming from the enormous model size and the difficulty of running it in the integer pipelines. In this paper, we present Tender, an algorithm-hardware co-design solution that enables efficient deployment of LLM inference at low precision.
Based on our analysis of outlier values in LLMs, we propose a decomposed quantization technique in which the scale factors of decomposed matrices are powers of two apart. The proposed scheme allows us to avoid explicit requantization (i.e., dequantization/quantization) when accumulating the partial sums from the decomposed matrices, with a minimal extension to the commodity tensor compute hardware. Our evaluation shows that Tender achieves higher accuracy and inference performance compared to the state-of-the-art methods while also being significantly less intrusive to the existing accelerators.

------------

`[2406.14162] DIRAS: Efficient LLM-Assisted Annotation of Document Relevance in Retrieval Augmented Generation <https://arxiv.org/abs/2406.14162>`__ DIRAS:检索增强生成中高效llm辅助文档相关性标注

::

    Thu, 20 Jun 2024 10:04:09 GMT
    Jingwei Ni, Tobias Schimanski, Meihong Lin, Mrinmaya Sachan, Elliott Ash, Markus Leippold

Retrieval Augmented Generation (RAG) is widely employed to ground responses to queries on domain-specific documents. But do RAG implementations leave out important information or excessively include irrelevant information? To allay these concerns, it is necessary to annotate domain-specific benchmarks to evaluate information retrieval (IR) performance, as relevance definitions vary across queries and domains. Furthermore, such benchmarks should be cost-efficiently annotated to avoid annotation selection bias. In this paper, we propose DIRAS (Domain-specific Information Retrieval Annotation with Scalability), a manual-annotation-free schema that fine-tunes open-sourced LLMs to annotate relevance labels with calibrated relevance probabilities. Extensive evaluation shows that DIRAS fine-tuned models achieve GPT-4-level performance on annotating and ranking unseen (query, document) pairs, and is helpful for real-world RAG development.

------------

`[2402.11893] Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint <https://arxiv.org/abs/2402.11893>`__ 基于上下文信息熵约束的自适应解码知识冲突识别与消解

::

    replaced with revised version Wed, 19 Jun 2024 06:07:37 GMT
    Submission history From: Xiaowei Yuan [view email]
    [v1] Mon, 19 Feb 2024 07:10:30 UTC (548 KB)
    [v2] Wed, 19 Jun 2024 06:07:37 UTC (550 KB)
    Xiaowei Yuan, Zhao Yang, Yequan Wang, Shengping Liu, Jun Zhao, Kang Liu

Large language models internalize enormous parametric knowledge during pre-training. Concurrently, realistic applications necessitate external contextual knowledge to aid models on the underlying tasks. This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts. In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them. It can improve the model's faithfulness to conflicting context, and simultaneously maintain high performance among non- Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets. Code is available.

------------

`[2406.05250] LLM-Enhanced Bayesian Optimization for Efficient Analog Layout Constraint Generation <https://arxiv.org/abs/2406.05250>`__ 高效模拟布局约束生成的llm增强贝叶斯优化

::

    replaced with revised version Wed, 19 Jun 2024 20:49:26 GMT
    Submission history From: Guojin Chen [view email]
    [v1] Fri, 7 Jun 2024 20:22:36 UTC (236 KB)
    [v2] Wed, 19 Jun 2024 20:49:26 UTC (236 KB)
    Guojin Chen, Keren Zhu, Seunggeun Kim, Hanqing Zhu, Yao Lai, Bei Yu, David Z. Pan

Analog layout synthesis faces significant challenges due to its dependence on manual processes, considerable time requirements, and performance instability. Current Bayesian Optimization (BO)-based techniques for analog layout synthesis, despite their potential for automation, suffer from slow convergence and extensive data needs, limiting their practical application. This paper presents the \texttt{LLANA} framework, a novel approach that leverages Large Language Models (LLMs) to enhance BO by exploiting the few-shot learning abilities of LLMs for more efficient generation of analog design-dependent parameter constraints. Experimental results demonstrate that \texttt{LLANA} not only achieves performance comparable to state-of-the-art (SOTA) BO methods but also enables a more effective exploration of the analog circuit design space, thanks to LLM's superior contextual understanding and learning efficiency. The code is available at this https URL.

------------

`[2401.10471] DeepEdit: Knowledge Editing as Decoding with Constraints <https://arxiv.org/abs/2401.10471>`__ DeepEdit:基于约束解码的知识编辑

::

    replaced with revised version Wed, 19 Jun 2024 22:53:54 GMT
    Submission history From: Yiwei Wang [view email]
    [v1] Fri, 19 Jan 2024 03:48:27 UTC (1,341 KB)
    [v2] Mon, 1 Apr 2024 16:12:50 UTC (1,732 KB)
    [v3] Sat, 8 Jun 2024 03:47:03 UTC (1,729 KB)
    [v4] Wed, 19 Jun 2024 22:53:54 UTC (1,943 KB)
    Yiwei Wang, Muhao Chen, Nanyun Peng, Kai-Wei Chang

How to edit the knowledge in multi-step reasoning has become the major challenge in the knowledge editing (KE) of large language models (LLMs). The difficulty arises because the hallucinations of LLMs during multi-step reasoning often lead to incorrect use of new knowledge and incorrect answers. To address this issue, we design decoding constraints to "regulate" LLMs' reasoning, enhancing logical coherence when incorporating new knowledge. We propose a new KE framework: DEEPEDIT (Depth-first Search-based Constrained Decoding for Knowledge Editing), which enhances LLMs's ability to generate coherent reasoning chains with new knowledge through depth-first search. Our search selects the most important knowledge that satisfies our constraints as the reasoning step to efficiently increase the reasoning depth. In addition to DEEPEDIT, we propose two new KE benchmarks: MQUAKE-2002 and MQUAKE-HARD, which provide more precise and challenging assessments of KE approaches. Qualitatively, DEEPEDIT enables LLMs to produce succinct and coherent reasoning chains involving new knowledge. Quantitatively, it yields significant improvements on multiple KE benchmarks.

------------

`[2403.05750] Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text <https://arxiv.org/abs/2403.05750>`__ 解码AI笔:检测AI生成文本的技术与挑战

::

    replaced with revised version Thu, 20 Jun 2024 00:05:37 GMT
    Submission history From: Sara Abdali [view email]
    [v1] Sat, 9 Mar 2024 01:13:54 UTC (56 KB)
    [v2] Thu, 20 Jun 2024 00:05:37 UTC (170 KB)
    Sara Abdali, Richard Anarfi, CJ Barberan, Jia He

Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.

------------

`[2405.04304] Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large Language Models <https://arxiv.org/abs/2405.04304>`__ 动态推测Lookahead加速大型语言模型的推测解码

::

    replaced with revised version Wed, 19 Jun 2024 08:54:51 GMT
    Submission history From: Jonathan Mamou [view email]
    [v1] Tue, 7 May 2024 13:27:52 UTC (674 KB)
    [v2] Tue, 18 Jun 2024 12:34:35 UTC (677 KB)
    [v3] Wed, 19 Jun 2024 08:54:51 UTC (677 KB)
    Jonathan Mamou and Oren Pereg and Daniel Korat and Moshe Berchansky and Nadav Timor and Moshe Wasserblat and Roy Schwartz

Speculative decoding is commonly used for reducing the inference latency of large language models. Its effectiveness depends highly on the speculation lookahead (SL)-the number of tokens generated by the draft model at each iteration. In this work we show that the common practice of using the same SL for all iterations static SL is suboptimal. We introduce DISCO (DynamIc SpeCulation lookahead Optimization), a novel method for dynamically selecting the SL. Our experiments with four datasets show that DISCO reaches an average speedup of 10% compared to the best static SL baseline, while generating the exact same text.

------------

`[2405.17381] Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention <https://arxiv.org/abs/2405.17381>`__ 不同长度，恒定速度:快速高效的语言建模

::

    replaced with revised version Thu, 20 Jun 2024 09:12:42 GMT
    Submission history From: Yiran Zhong [view email]
    [v1] Mon, 27 May 2024 17:38:13 UTC (1,858 KB)
    [v2] Thu, 20 Jun 2024 09:12:42 UTC (1,864 KB)
    Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong

We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at this http URL.

------------

`[2305.18403] LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning <https://arxiv.org/abs/2305.18403>`__ LoRAPrune:剪枝满足低秩参数高效微调

::

    replaced with revised version Thu, 20 Jun 2024 06:31:00 GMT
    Submission history From: Mingyang Zhang [view email]
    [v1] Sun, 28 May 2023 15:15:48 UTC (240 KB)
    [v2] Wed, 31 May 2023 22:32:19 UTC (243 KB)
    [v3] Tue, 3 Oct 2023 12:51:55 UTC (353 KB)
    [v4] Thu, 20 Jun 2024 06:31:00 UTC (367 KB)
    Mingyang Zhang and Hao Chen and Chunhua Shen and Zhen Yang and Linlin Ou and Xinyi Yu and Bohan Zhuang

Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Post-training model pruning offers a way to compress LLMs. However, the current pruning methods designed for LLMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LLMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead. To this end, we propose LoRAPrune, a new framework that delivers an accurate structured pruned model in a highly memory-efficient manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather than the gradients of pre-trained weights for importance estimation. We subsequently integrate this criterion into an iterative pruning process, effectively removing redundant channels and heads. Extensive experimental results demonstrate the superior performance of our LoRAPrune over existing approaches on the LLaMA series models. At a 50\% compression rate, LoRAPrune demonstrates superior performance over LLM-Pruner, achieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while also decreasing memory usage by 52.6%. Besides, LoRAPrune also matches semi-structural pruning across multiple LLMs, proving its wide applicability. The code is available at this https URL.

------------

`[2406.11087] MemDPT: Differential Privacy for Memory Efficient Language Models <https://arxiv.org/abs/2406.11087>`__ MemDPT:面向内存高效语言模型的差分隐私保护

::

    replaced with revised version Thu, 20 Jun 2024 05:43:50 GMT
    Submission history From: Yanming Liu [view email]
    [v1] Sun, 16 Jun 2024 22:11:41 UTC (261 KB)
    [v2] Thu, 20 Jun 2024 05:43:50 UTC (260 KB)
    Yanming Liu, Xinyue Peng, Jiannan Cao, Yuwei Zhang, Chen Ma, Songhang Deng, Mengchen Fu, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du

Large language models have consistently demonstrated remarkable performance across a wide spectrum of applications. Nonetheless, the deployment of these models can inadvertently expose user privacy to potential risks. The substantial memory demands of these models during training represent a significant resource consumption challenge. The sheer size of these models imposes a considerable burden on memory resources, which is a matter of significant concern in practice. In this paper, we present an innovative training framework MemDPT that not only reduces the memory cost of large language models but also places a strong emphasis on safeguarding user data privacy. MemDPT provides edge network and reverse network designs to accommodate various differential privacy memory-efficient fine-tuning schemes. Our approach not only achieves $2 \sim 3 \times$ memory optimization but also provides robust privacy protection, ensuring that user data remains secure and confidential. Extensive experiments have demonstrated that MemDPT can effectively provide differential privacy efficient fine-tuning across various task scenarios.

------------

`[2307.06930] mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs <https://arxiv.org/abs/2307.06930>`__ mBLIP:多语言视觉语言模型的高效自举

::

    replaced with revised version Thu, 20 Jun 2024 06:48:17 GMT
    Submission history From: Gregor Geigle [view email]
    [v1] Thu, 13 Jul 2023 17:51:58 UTC (2,490 KB)
    [v2] Mon, 2 Oct 2023 11:58:10 UTC (2,503 KB)
    [v3] Thu, 20 Jun 2024 06:48:17 UTC (2,507 KB)
    Gregor Geigle, Abhay Jain, Radu Timofte, Goran Glava\v{s}

Modular vision-language models (Vision-LLMs) align pretrained image encoders with (frozen) large language models (LLMs) and post-hoc condition LLMs to `understand' the image input. With the abundance of readily available high-quality English image-text data as well as strong monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. We present mBLIP, the first Vision-LLM leveraging multilingual LLMs, which we obtain in a computationally efficient manner on consumer-level hardware. To this end, we \textit{re-align} an image encoder previously tuned to an English LLM to a new, multilingual LLM using only a few million multilingual training examples derived from a mix of vision-and-language tasks, which we obtain by machine-translating high-quality English data to 95 languages. On the IGLUE benchmark and XM3600, mBLIP yields results competitive with state-of-the-art models and it greatly outperforms strong English-only Vision-LLMs like Llava 1.5. We release our model, code, and train data at \url{this https URL}.

------------

-----------------------
In-Context Learning (1)
-----------------------

`[2406.14208] SeCoKD: Aligning Large Language Models for In-Context Learning with Fewer Shots <https://arxiv.org/abs/2406.14208>`__ SeCoKD:基于较少镜头的上下文学习大型语言模型对齐

::

    Thu, 20 Jun 2024 11:26:06 GMT
    Weixing Wang, Haojin Yang, Christoph Meinel

Previous studies have shown that demonstrations can significantly help Large Language Models (LLMs ) perform better on the given tasks. However, this so-called In-Context Learning ( ICL ) ability is very sensitive to the presenting context, and often dozens of demonstrations are needed. In this work, we investigate if we can reduce the shot number while still maintaining a competitive performance. We present SeCoKD, a self-Knowledge Distillation ( KD ) training framework that aligns the student model with a heavily prompted variation, thereby increasing the utilization of a single demonstration. We experiment with the SeCoKD across three LLMs and six benchmarks focusing mainly on reasoning tasks. Results show that our method outperforms the base model and Supervised Fine-tuning ( SFT ), especially in zero-shot and one-shot settings by 30% and 10%, respectively. Moreover, SeCoKD brings little negative artifacts when evaluated on new tasks, which is more robust than Supervised Fine-tuning.

------------

--------------
Reasoning (16)
--------------

`[2406.14283] Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning <https://arxiv.org/abs/2406.14283>`__ Q*:通过审慎规划改进llm的多步骤推理

::

    Thu, 20 Jun 2024 13:08:09 GMT
    Chaojie Wang, Yanchen Deng, Zhiyi Lv, Shuicheng Yan, An Bo

Large Language Models (LLMs) have demonstrated impressive capability in many nature language tasks. However, the auto-regressive generation process makes LLMs prone to produce errors, hallucinations and inconsistent statements when performing multi-step reasoning. In this paper, we aim to alleviate the pathology by introducing Q*, a general, versatile and agile framework for guiding LLMs decoding process with deliberative planning. By learning a plug-and-play Q-value model as heuristic function, our Q* can effectively guide LLMs to select the most promising next step without fine-tuning LLMs for each task, which avoids the significant computational overhead and potential risk of performance degeneration on other tasks. Extensive experiments on GSM8K, MATH and MBPP confirm the superiority of our method.

------------

`[2406.13217] Bridging Law and Data: Augmenting Reasoning via a Semi-Structured Dataset with IRAC methodology <https://arxiv.org/abs/2406.13217>`__ 架桥法则与数据:基于IRAC方法的半结构化数据集增强推理

::

    Wed, 19 Jun 2024 04:59:09 GMT
    Xiaoxi Kang, Lizhen Qu, Lay-Ki Soon, Zhuang Li, Adnan Trakic

The effectiveness of Large Language Models (LLMs) in legal reasoning is often limited due to the unique legal terminologies and the necessity for highly specialized knowledge. These limitations highlight the need for high-quality data tailored for complex legal reasoning tasks. This paper introduces LEGALSEMI, a benchmark specifically curated for legal scenario analysis.
LEGALSEMI comprises 54 legal scenarios, each rigorously annotated by legal experts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion) framework. In addition, LEGALSEMI is accompanied by a structured knowledge graph (SKG). A series of experiments were conducted to assess the usefulness of LEGALSEMI for IRAC analysis. The experimental results demonstrate the effectiveness of incorporating the SKG for issue identification, rule retrieval, application and conclusion generation using four different LLMs.
LEGALSEMI will be publicly available upon acceptance of this paper.

------------

`[2406.13246] GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs <https://arxiv.org/abs/2406.13246>`__ GSR-BENCH:基于多模态llm的地面空间推理评估基准

::

    Wed, 19 Jun 2024 06:15:26 GMT
    Navid Rajabi, Jana Kosecka

The ability to understand and reason about spatial relationships between objects in images is an important component of visual reasoning. This skill rests on the ability to recognize and localize objects of interest and determine their spatial relation. Early vision and language models (VLMs) have been shown to struggle to recognize spatial relations. We extend the previously released What'sUp dataset and propose a novel comprehensive evaluation for spatial relationship understanding that highlights the strengths and weaknesses of 27 different models. In addition to the VLMs evaluated in What'sUp, our extensive evaluation encompasses 3 classes of Multimodal LLMs (MLLMs) that vary in their parameter sizes (ranging from 7B to 110B), training/instruction-tuning methods, and visual resolution to benchmark their performances and scrutinize the scaling laws in this task.

------------

`[2406.13397] MoreHopQA: More Than Multi-hop Reasoning <https://arxiv.org/abs/2406.13397>`__ MoreHopQA:不仅仅是多跳推理

::

    Wed, 19 Jun 2024 09:38:59 GMT
    Julian Schnitzler, Xanh Ho, Jiahao Huang, Florian Boudin, Saku Sugawara, Akiko Aizawa

Most existing multi-hop datasets are extractive answer datasets, where the answers to the questions can be extracted directly from the provided context.
This often leads models to use heuristics or shortcuts instead of performing true multi-hop reasoning. In this paper, we propose a new multi-hop dataset, MoreHopQA, which shifts from extractive to generative answers. Our dataset is created by utilizing three existing multi-hop datasets: HotpotQA, 2WikiMultihopQA, and MuSiQue. Instead of relying solely on factual reasoning, we enhance the existing multi-hop questions by adding another layer of questioning that involves one, two, or all three of the following types of reasoning: commonsense, arithmetic, and symbolic. Our dataset is created through a semi-automated process, resulting in a dataset with 1,118 samples that have undergone human verification. We then use our dataset to evaluate five different large language models: Mistral 7B, Gemma 7B, Llama 3 (8B and 70B), and GPT-4. We also design various cases to analyze the reasoning steps in the question-answering process. Our results show that models perform well on initial multi-hop questions but struggle with our extended questions, indicating that our dataset is more challenging than previous ones. Our analysis of question decomposition reveals that although models can correctly answer questions, only a portion - 38.7% for GPT-4 and 33.4% for Llama3-70B - achieve perfect reasoning, where all corresponding sub-questions are answered correctly. Evaluation code and data are available at https://github.com/Alab-NII/morehopqa

------------

`[2406.13803] Semantic Structure-Mapping in LLM and Human Analogical Reasoning <https://arxiv.org/abs/2406.13803>`__ LLM中的语义结构映射与人类类比推理

::

    Wed, 19 Jun 2024 20:07:37 GMT
    Sam Musker, Alex Duchnowski, Rapha\"el Milli\`ere, Ellie Pavlick

Analogical reasoning is considered core to human learning and cognition.
Recent studies have compared the analogical reasoning abilities of human subjects and Large Language Models (LLMs) on abstract symbol manipulation tasks, such as letter string analogies. However, these studies largely neglect analogical reasoning over semantically meaningful symbols, such as natural language words. This ability to draw analogies that link language to non-linguistic domains, which we term semantic structure-mapping, is thought to play a crucial role in language acquisition and broader cognitive development.
We test human subjects and LLMs on analogical reasoning tasks that require the transfer of semantic structure and content from one domain to another. Advanced LLMs match human performance across many task variations. However, humans and LLMs respond differently to certain task variations and semantic distractors.
Overall, our data suggest that LLMs are approaching human-level performance on these important cognitive tasks, but are not yet entirely human like.

------------

`[2406.13858] Distributional reasoning in LLMs: Parallel reasoning processes in multi-hop reasoning <https://arxiv.org/abs/2406.13858>`__ LLMs中的分布式推理:多跳推理中的并行推理过程

::

    Wed, 19 Jun 2024 21:36:40 GMT
    Yuval Shalev, Amir Feder and Ariel Goldstein

Large language models (LLMs) have shown an impressive ability to perform tasks believed to require thought processes. When the model does not document an explicit thought process, it becomes difficult to understand the processes occurring within its hidden layers and to determine if these processes can be referred to as reasoning. We introduce a novel and interpretable analysis of internal multi-hop reasoning processes in LLMs. We demonstrate that the prediction process for compositional reasoning questions can be modeled using a simple linear transformation between two semantic category spaces. We show that during inference, the middle layers of the network generate highly interpretable embeddings that represent a set of potential intermediate answers for the multi-hop question. We use statistical analyses to show that a corresponding subset of tokens is activated in the model's output, implying the existence of parallel reasoning paths. These observations hold true even when the model lacks the necessary knowledge to solve the task. Our findings can help uncover the strategies that LLMs use to solve reasoning tasks, offering insights into the types of thought processes that can emerge from artificial intelligence. Finally, we also discuss the implication of cognitive modeling of these results.

------------

`[2406.13975] MR-BEN: A Comprehensive Meta-Reasoning Benchmark for Large Language Models <https://arxiv.org/abs/2406.13975>`__ MR-BEN:大型语言模型的综合元推理基准

::

    Thu, 20 Jun 2024 03:50:23 GMT
    Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, Jiaya Jia

Large language models (LLMs) have shown increasing capability in problem-solving and decision-making, largely based on the step-by-step chain-of-thought reasoning processes. However, it has been increasingly challenging to evaluate the reasoning capability of LLMs. Concretely, existing outcome-based benchmarks begin to saturate and become less sufficient to monitor the progress. To this end, we present a process-based benchmark MR-BEN that demands a meta reasoning skill, where LMs are asked to locate and analyse potential errors in automatically generated reasoning steps. MR-BEN is a comprehensive benchmark comprising 5,975 questions collected from human experts, covering various subjects such as physics, chemistry, logic, coding, and more. Through our designed metrics for assessing meta-reasoning on this benchmark, we identify interesting limitations and weaknesses of current LLMs (open-source and closed-source models). For example, open-source models are seemingly comparable to GPT-4 on outcome-based benchmarks, but they lag far behind on our benchmark, revealing the underlying reasoning capability gap between them. Our dataset and codes are available on https://randolph-zeng.github.io/Mr-Ben.github.io/.

------------

`[2406.14192] Timo: Towards Better Temporal Reasoning for Language Models <https://arxiv.org/abs/2406.14192>`__ 

::

    Thu, 20 Jun 2024 10:52:14 GMT
    Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, Yu Cheng

Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot generalize to a wider spectrum of temporal reasoning tasks. Therefore, we propose a crucial question: Can we build a universal framework to handle a variety of temporal reasoning tasks? To that end, we systematically study 38 temporal reasoning tasks. Based on the observation that 19 tasks are directly related to mathematics, we first leverage the available mathematical dataset to set a solid foundation for temporal reasoning. However, the in-depth study indicates that focusing solely on mathematical enhancement falls short of addressing pure temporal reasoning tasks. To mitigate this limitation, we propose a simple but effective self-critic temporal optimization method to enhance the model's temporal reasoning capabilities without sacrificing general task abilities. Finally, we develop Timo, a model designed to excel in temporal reasoning at the 7B and 13B scales. Notably, Timo outperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and achieves the new state-of-the-art (SOTA) performance of comparable size.
Extensive experiments further validate our framework's effectiveness and its generalization across diverse temporal tasks. The code is available at https://github.com/zhaochen0110/Timo.

------------

`[2406.14425] SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource Languages <https://arxiv.org/abs/2406.14425>`__ SynDARin:面向低资源语言自动推理的数据集合成

::

    Thu, 20 Jun 2024 15:49:28 GMT
    Gayane Ghazaryan, Erik Arakelyan, Pasquale Minervini, Isabelle Augenstein

Question Answering (QA) datasets have been instrumental in developing and evaluating Large Language Model (LLM) capabilities. However, such datasets are scarce for languages other than English due to the cost and difficulties of collection and manual annotation. This means that producing novel models and measuring the performance of multilingual LLMs in low-resource languages is challenging. To mitigate this, we propose $\textbf{S}$yn$\textbf{DAR}$in, a method for generating and validating QA datasets for low-resource languages. We utilize parallel content mining to obtain $\textit{human-curated}$ paragraphs between English and the target language. We use the English data as context to $\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which are automatically translated and further validated for quality. Combining these with their designated non-English $\textit{human-curated}$ paragraphs form the final QA dataset. The method allows to maintain the content quality, reduces the likelihood of factual errors, and circumvents the need for costly annotation. To test the method, we created a QA dataset with $1.2$K samples for the Armenian language. The human evaluation shows that $98\%$ of the generated English data maintains quality and diversity in the question types and topics, while the translation validation pipeline can filter out $\sim70\%$ of data with poor quality. We use the dataset to benchmark state-of-the-art LLMs, showing their inability to achieve human accuracy with some model performances closer to random chance. This shows that the generated dataset is non-trivial and can be used to evaluate reasoning capabilities in low-resource language.

------------

`[2406.13808] Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning? <https://arxiv.org/abs/2406.13808>`__ llm中的低秩知识蒸馏对微电子推理有用吗?

::

    Wed, 19 Jun 2024 20:14:39 GMT
    Nirjhor Rouf, Fin Amin, Paul D. Franzon

In this work, we present empirical results regarding the feasibility of using offline large language models (LLMs) in the context of electronic design automation (EDA). The goal is to investigate and evaluate a contemporary language model's (Llama-2-7B) ability to function as a microelectronic Q & A expert as well as its reasoning, and generation capabilities in solving microelectronic-related problems. Llama-2-7B was tested across a variety of adaptation methods, including introducing a novel low-rank knowledge distillation (LoRA-KD) scheme. Our experiments produce both qualitative and quantitative results.

------------

`[2406.14532] RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold <https://arxiv.org/abs/2406.14532>`__ 在不正确的合成数据上的RL将LLM数学推理的效率提高了8倍

::

    Thu, 20 Jun 2024 17:45:54 GMT
    Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, Aviral Kumar

Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this question for math reasoning via an empirical study, followed by building a conceptual understanding of our observations. First, we find that while the typical approach of finetuning a model on synthetic correct or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner itself followed by subsequent fine-tuning on this self-generated data $\textbf{doubles}$ the efficiency of the same synthetic problems. At the same time, training on model-generated positives can amplify various spurious correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize negative responses, i.e., model-generated responses that are deemed incorrect by a final answer verifier. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or advantage of each intermediate step in the negative response. With this per-step scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by $\mathbf{8 \times}$. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits robustness benefits of RL over imitating positive data alone.

------------

`[2402.11804] LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs <https://arxiv.org/abs/2402.11804>`__ LLM作为提示器:任意知识图谱上的低资源归纳推理

::

    replaced with revised version Wed, 19 Jun 2024 09:00:53 GMT
    Submission history From: Kai Wang [view email]
    [v1] Mon, 19 Feb 2024 03:21:19 UTC (2,276 KB)
    [v2] Sat, 8 Jun 2024 12:16:22 UTC (3,061 KB)
    [v3] Wed, 19 Jun 2024 09:00:53 UTC (3,061 KB)
    Kai Wang, Yuwei Xu, Zhiyong Wu, Siqiang Luo

Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-resource KG datasets and find that ProLINK outperforms previous methods in three-shot, one-shot, and zero-shot reasoning tasks, exhibiting average performance improvements by 20%, 45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong robustness for various LLM promptings as well as full-shot scenarios.

------------

`[2311.08097] A Tree-of-Thoughts to Broaden Multi-step Reasoning across Languages <https://arxiv.org/abs/2311.08097>`__ 扩展跨语言多步推理的思想树

::

    replaced with revised version Wed, 19 Jun 2024 13:07:54 GMT
    Submission history From: Leonardo Ranaldi Mr [view email]
    [v1] Tue, 14 Nov 2023 11:49:43 UTC (9,186 KB)
    [v2] Fri, 19 Apr 2024 15:49:21 UTC (9,411 KB)
    [v3] Wed, 19 Jun 2024 13:07:54 UTC (9,411 KB)
    [v4] Fri, 21 Jun 2024 06:06:51 UTC (9,403 KB)
    Leonardo Ranaldi, Giulia Pucci, Federico Ranaldi, Elena Sofia Ruzzetti, Fabio Massimo Zanzotto

Reasoning methods, best exemplified by the well-known Chain-of-Thought (CoT), empower the reasoning abilities of Large Language Models (LLMs) by eliciting them to solve complex tasks in a step-by-step manner. Although they are achieving significant success, the ability to deliver multi-step reasoning remains limited to English because of the imbalance in the distribution of pre-training data, which makes other languages a barrier. In this paper, we propose Cross-lingual Tree-of-Thoughts (Cross-ToT), a method for aligning Cross-lingual CoT reasoning across languages. The proposed method, through a self-consistent cross-lingual prompting mechanism inspired by the Tree-of-Thoughts approach, provides multi-step reasoning paths in different languages that, during the steps, lead to the final solution. Experimental evaluations show that our method significantly outperforms existing prompting methods by reducing the number of interactions and achieving state-of-the-art performance.

------------

`[2402.11199] Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs <https://arxiv.org/abs/2402.11199>`__ 知识图谱多跳推理中思维链的直接评价

::

    replaced with revised version Wed, 19 Jun 2024 05:14:05 GMT
    Submission history From: Thuy-Trang Vu [view email]
    [v1] Sat, 17 Feb 2024 05:22:56 UTC (8,933 KB)
    [v2] Wed, 19 Jun 2024 05:14:05 UTC (8,934 KB)
    Minh-Vuong Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, Gholamreza Haffari

Large language models (LLMs) demonstrate strong reasoning abilities when prompted to generate chain-of-thought (CoT) explanations alongside answers. However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT. In this paper, we delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question answering by utilizing knowledge graphs (KGs). We propose a novel discriminative and generative CoT evaluation paradigm to assess LLMs' knowledge of reasoning and the accuracy of the generated CoT. Through experiments conducted on 5 different families of LLMs across 2 multi-hop question-answering datasets, we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorrect reasoning.

------------

`[2402.18439] Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication <https://arxiv.org/abs/2402.18439>`__ 超越自然语言:llm利用替代格式来增强推理和沟通

::

    replaced with revised version Wed, 19 Jun 2024 01:42:22 GMT
    Submission history From: Weize Chen [view email]
    [v1] Wed, 28 Feb 2024 16:07:54 UTC (1,936 KB)
    [v2] Tue, 18 Jun 2024 03:06:39 UTC (1,943 KB)
    [v3] Wed, 19 Jun 2024 01:42:22 UTC (1,943 KB)
    Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen Qian, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun

Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code is released at \url{this https URL}.

------------

`[2406.12572] Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models <https://arxiv.org/abs/2406.12572>`__ Mathador-LM:大型语言模型数学推理动态基准

::

    replaced with revised version Wed, 19 Jun 2024 12:28:10 GMT
    Submission history From: Amir Moeini [view email]
    [v1] Tue, 18 Jun 2024 13:02:12 UTC (801 KB)
    [v2] Wed, 19 Jun 2024 12:28:10 UTC (805 KB)
    Eldar Kurtic, Amir Moeini, Dan Alistarh

We introduce Mathador-LM, a new benchmark for evaluating the mathematical reasoning on large language models (LLMs), combining ruleset interpretation, planning, and problem-solving. This benchmark is inspired by the Mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances dynamically, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. Additionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary models struggle with Mathador-LM, scoring significantly lower than average 3rd graders. This stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks.

------------

-----------
ToolUse (5)
-----------

`[2406.12928] Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox <https://arxiv.org/abs/2406.12928>`__ 

::

    Sat, 15 Jun 2024 12:02:14 GMT
    Yijun Liu, Yuan Meng, Fang Wu, Shenhao Peng, Hang Yao, Chaoyu Guan, Chen Tang, Xinzhu Ma, Zhi Wang, Wenwu Zhu

Large language models (LLMs) have exhibited exciting progress in multiple scenarios, while the huge computational demands hinder their deployments in lots of real-world applications. As an effective means to reduce memory footprint and inference cost, quantization also faces challenges in performance degradation at low bit-widths. Understanding the impact of quantization on LLM capabilities, especially the generalization ability, is crucial. However, the community's main focus remains on the algorithms and models of quantization, with insufficient attention given to whether the quantized models can retain the strong generalization abilities of LLMs. In this work, we fill this gap by providing a comprehensive benchmark suite for this research topic, including an evaluation system, detailed analyses, and a general toolbox. Specifically, based on the dominant pipeline in LLM quantization, we primarily explore the impact of calibration data distribution on the generalization of quantized LLMs and conduct the benchmark using more than 40 datasets within two main scenarios. Based on this benchmark, we conduct extensive experiments with two well-known LLMs (English and Chinese) and four quantization algorithms to investigate this topic in-depth, yielding several counter-intuitive and valuable findings, e.g., models quantized using a calibration set with the same distribution as the test data are not necessarily optimal. Besides, to facilitate future research, we also release a modular-designed toolbox, which decouples the overall pipeline into several separate components, e.g., base LLM module, dataset module, quantizer module, etc. and allows subsequent researchers to easily assemble their methods through a simple configuration.
Our benchmark suite is publicly available at https://github.com/TsingmaoAI/MI-optimize

------------

`[2401.15724] RE-GAINS & EnChAnT: Intelligent Tool Manipulation Systems For Enhanced Query Responses <https://arxiv.org/abs/2401.15724>`__ RE-GAINS & EnChAnT:用于增强查询响应的智能工具操作系统

::

    replaced with revised version Thu, 20 Jun 2024 10:47:47 GMT
    Submission history From: Ryan George [view email]
    [v1] Sun, 28 Jan 2024 18:26:31 UTC (1,408 KB)
    [v2] Mon, 17 Jun 2024 17:59:34 UTC (1,409 KB)
    [v3] Thu, 20 Jun 2024 10:47:47 UTC (1,409 KB)
    Sahil Girhepuje, Siva Sankar Sajeev, Purvam Jain, Arya Sikder, Adithya Rama Varma, Ryan George, Akshay Govind Srinivasan, Mahendra Kurup, Ashmit Sinha, Sudip Mondal

Large Language Models (LLMs) currently struggle with tool invocation and chaining, as they often hallucinate or miss essential steps in a sequence. We propose RE-GAINS and EnChAnT, two novel frameworks that empower LLMs to tackle complex user queries by making API calls to external tools based on tool descriptions and argument lists. Tools are chained based on the expected output, without receiving the actual results from each individual call. EnChAnT, an open-source solution, leverages an LLM format enforcer, OpenChat 3.5 (an LLM), and ToolBench's API Retriever. RE-GAINS utilizes OpenAI models and embeddings with a specialized prompt based on the $\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning $(RAP)$ framework. Both frameworks are low cost (0.01\$ per query). Our key contribution is enabling LLMs for tool invocation and chaining using modifiable, externally described tools.

------------

`[2402.11903] DiLA: Enhancing LLM Tool Learning with Differential Logic Layer <https://arxiv.org/abs/2402.11903>`__ DiLA:利用差分逻辑层增强LLM工具学习

::

    replaced with revised version Wed, 19 Jun 2024 02:52:00 GMT
    Submission history From: Yu Zhang [view email]
    [v1] Mon, 19 Feb 2024 07:38:57 UTC (517 KB)
    [v2] Sat, 25 May 2024 01:46:17 UTC (560 KB)
    [v3] Wed, 19 Jun 2024 02:52:00 UTC (351 KB)
    Yu Zhang, Hui-Ling Zhen, Zehua Pei, Yingzhao Lian, Lihao Yin, Mingxuan Yuan, Bei Yu

Considering the challenges faced by large language models (LLMs) in logical reasoning and planning, prior efforts have sought to augment LLMs with access to external solvers. While progress has been made on simple reasoning problems, solving classical constraint satisfaction problems, such as the Boolean Satisfiability Problem (SAT) and Graph Coloring Problem (GCP), remains difficult for off-the-shelf solvers due to their intricate expressions and exponential search spaces. In this paper, we propose a novel differential logic layer-aided language modeling (DiLA) approach, where logical constraints are integrated into the forward and backward passes of a network layer, to provide another option for LLM tool learning. In DiLA, LLM aims to transform the language description to logic constraints and identify initial solutions of the highest quality, while the differential logic layer focuses on iteratively refining the LLM-prompted solution. Leveraging the logic layer as a bridge, DiLA enhances the logical reasoning ability of LLMs on a range of reasoning problems encoded by Boolean variables, guaranteeing the efficiency and correctness of the solution process. We evaluate the performance of DiLA on two classic reasoning problems and empirically demonstrate its consistent outperformance against existing prompt-based and solver-aided approaches.

------------

`[2403.07714] StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models <https://arxiv.org/abs/2403.07714>`__ StableToolBench:面向大型语言模型工具学习的稳定大规模基准测试

::

    replaced with revised version Wed, 19 Jun 2024 11:59:08 GMT
    Submission history From: Zhicheng Guo [view email]
    [v1] Tue, 12 Mar 2024 14:57:40 UTC (765 KB)
    [v2] Wed, 13 Mar 2024 14:08:19 UTC (765 KB)
    [v3] Fri, 14 Jun 2024 07:19:56 UTC (768 KB)
    [v4] Wed, 19 Jun 2024 11:59:08 UTC (768 KB)
    Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, Yang Liu

Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system.

------------

`[2405.05955] Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning <https://arxiv.org/abs/2405.05955>`__ Smurfs:利用具有上下文效率的多熟练智能体进行工具规划

::

    replaced with revised version Wed, 19 Jun 2024 06:18:12 GMT
    Submission history From: Junzhi Chen [view email]
    [v1] Thu, 9 May 2024 17:49:04 UTC (3,003 KB)
    [v2] Wed, 19 Jun 2024 06:18:12 UTC (3,168 KB)
    Junzhi Chen, Juhao Liang, Benyou Wang

The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance. Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly. This paper introduces `Smurfs', a cutting-edge multi-agent framework designed to revolutionize the application of LLMs. By seamlessly transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs can enhance the model's ability to solve complex tasks at no additional cost. This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents and forming an intelligent multi-agent system. Our empirical investigation on both open-ended task of StableToolBench and closed-ended task on HotpotQA showcases Smurfs' superior capability in intricate tool utilization scenarios. Notably, Smurfs outmatches all the baseline methods in both experiments, setting new state-of-the-art performance. Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy. This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems.

------------

------------------------
Retrieval-Augmented (19)
------------------------

`[2406.13659] Leveraging Large Language Models for Patient Engagement: The Power of Conversational AI in Digital Health <https://arxiv.org/abs/2406.13659>`__ 利用大型语言模型促进患者参与:数字健康对话人工智能的力量

::

    Wed, 19 Jun 2024 16:02:04 GMT
    Bo Wen, Raquel Norel, Julia Liu, Thaddeus Stappenbeck, Farhana Zulkernine, and Huamin Chen

The rapid advancements in large language models (LLMs) have opened up new opportunities for transforming patient engagement in healthcare through conversational AI. This paper presents an overview of the current landscape of LLMs in healthcare, specifically focusing on their applications in analyzing and generating conversations for improved patient engagement. We showcase the power of LLMs in handling unstructured conversational data through four case studies: (1) analyzing mental health discussions on Reddit, (2) developing a personalized chatbot for cognitive engagement in seniors, (3) summarizing medical conversation datasets, and (4) designing an AI-powered patient engagement system. These case studies demonstrate how LLMs can effectively extract insights and summarizations from unstructured dialogues and engage patients in guided, goal-oriented conversations. Leveraging LLMs for conversational analysis and generation opens new doors for many patient-centered outcomes research opportunities. However, integrating LLMs into healthcare raises important ethical considerations regarding data privacy, bias, transparency, and regulatory compliance. We discuss best practices and guidelines for the responsible development and deployment of LLMs in healthcare settings. Realizing the full potential of LLMs in digital health will require close collaboration between the AI and healthcare professionals communities to address technical challenges and ensure these powerful tools' safety, efficacy, and equity.

------------

`[2406.13840] StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation <https://arxiv.org/abs/2406.13840>`__ StackRAG Agent:用检索增强生成改进开发人员的回答

::

    Wed, 19 Jun 2024 21:07:35 GMT
    Davit Abrahamyan, Fatemeh H. Fard

Developers spend much time finding information that is relevant to their questions. Stack Overflow has been the leading resource, and with the advent of Large Language Models (LLMs), generative models such as ChatGPT are used frequently. However, there is a catch in using each one separately. Searching for answers is time-consuming and tedious, as shown by the many tools developed by researchers to address this issue. On the other, using LLMs is not reliable, as they might produce irrelevant or unreliable answers (i.e., hallucination).
In this work, we present StackRAG, a retrieval-augmented Multiagent generation tool based on LLMs that combines the two worlds: aggregating the knowledge from SO to enhance the reliability of the generated answers. Initial evaluations show that the generated answers are correct, accurate, relevant, and useful.

------------

`[2406.13050] Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation <https://arxiv.org/abs/2406.13050>`__ Think-then-Act:双角度评估检索增强生成

::

    Tue, 18 Jun 2024 20:51:34 GMT
    Yige Shen, Hao Jiang, Hua Qu, Jihong Zhao

Despite their impressive capabilities, large language models (LLMs) often face challenges such as temporal misalignment and generating hallucinatory content. Enhancing LLMs with retrieval mechanisms to fetch relevant information from external sources offers a promising solution. Inspired by the proverb "Think twice before you act," we propose a dual-angle evaluated retrieval-augmented generation framework \textit{Think-then-Act}. Unlike previous approaches that indiscriminately rewrite queries or perform retrieval regardless of necessity, or generate temporary responses before deciding on additional retrieval, which increases model generation costs, our framework employs a two-phase process: (i) assessing the input query for clarity and completeness to determine if rewriting is necessary; and (ii) evaluating the model's capability to answer the query and deciding if additional retrieval is needed. Experimental results on five datasets show that the \textit{Think-then-Act} framework significantly improves performance. Our framework demonstrates notable improvements in accuracy and efficiency compared to existing baselines and performs well in both English and non-English contexts. Ablation studies validate the optimal model confidence threshold, highlighting the resource optimization benefits of our approach.

------------

`[2406.13213] Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata <https://arxiv.org/abs/2406.13213>`__ Multi-Meta-RAG:利用llm提取的元数据对数据库进行过滤，改进RAG的多跳查询处理

::

    Wed, 19 Jun 2024 04:53:48 GMT
    Mykhailo Poliakov and Nadiya Shvai

The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.

------------

`[2406.13249] R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation <https://arxiv.org/abs/2406.13249>`__ R^2AG:将检索信息融入检索增强生成

::

    Wed, 19 Jun 2024 06:19:48 GMT
    Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen

Retrieval augmented generation (RAG) has been applied in many scenarios to augment large language models (LLMs) with external documents provided by retrievers. However, a semantic gap exists between LLMs and retrievers due to differences in their training objectives and architectures. This misalignment forces LLMs to passively accept the documents provided by the retrievers, leading to incomprehension in the generation process, where the LLMs are burdened with the task of distinguishing these documents using their inherent knowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill this gap by incorporating Retrieval information into Retrieval Augmented Generation. Specifically, R$^2$AG utilizes the nuanced features from the retrievers and employs a R$^2$-Former to capture retrieval information. Then, a retrieval-aware prompting strategy is designed to integrate retrieval information into LLMs' generation. Notably, R$^2$AG suits low-source scenarios where LLMs and retrievers are frozen. Extensive experiments across five datasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our analysis reveals that retrieval information serves as an anchor to aid LLMs in the generation process, thereby filling the semantic gap.

------------

`[2406.13663] Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation <https://arxiv.org/abs/2406.13663>`__ 基于模型内部的答案归因可信检索增强生成

::

    Wed, 19 Jun 2024 16:10:26 GMT
    Jirui Qi, Gabriele Sarti, Raquel Fern\'andez, Arianna Bisazza

Ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain.
Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers.
However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs' context usage throughout the generation. In this work, we present MIRAGE --Model Internals-based RAG Explanations -- a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. We evaluate our proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. Our qualitative evaluation highlights the faithfulness of MIRAGE's attributions and underscores the promising application of model internals for RAG answer attribution.

------------

`[2406.13677] Leveraging Large Language Models to Measure Gender Bias in Gendered Languages <https://arxiv.org/abs/2406.13677>`__ 利用大型语言模型衡量性别化语言中的性别偏见

::

    Wed, 19 Jun 2024 16:30:58 GMT
    Erik Derner, Sara Sansalvador de la Fuente, Yoan Guti\'errez, Paloma Moreda, Nuria Oliver

Gender bias in text corpora used in various natural language processing (NLP) contexts, such as for training large language models (LLMs), can lead to the perpetuation and amplification of societal inequalities. This is particularly pronounced in gendered languages like Spanish or French, where grammatical structures inherently encode gender, making the bias analysis more challenging.
Existing methods designed for English are inadequate for this task due to the intrinsic linguistic differences between English and gendered languages. This paper introduces a novel methodology that leverages the contextual understanding capabilities of LLMs to quantitatively analyze gender representation in Spanish corpora. By utilizing LLMs to identify and classify gendered nouns and pronouns in relation to their reference to human entities, our approach provides a nuanced analysis of gender biases. We empirically validate our method on four widely-used benchmark datasets, uncovering significant gender disparities with a male-to-female ratio ranging from 4:1 to 6:1. These findings demonstrate the value of our methodology for bias quantification in gendered languages and suggest its application in NLP, contributing to the development of more equitable language technologies.

------------

`[2406.13885] Knowledge Tagging System on Math Questions via LLMs with Flexible Demonstration Retriever <https://arxiv.org/abs/2406.13885>`__ 基于LLMs的数学问题知识标注系统，具有灵活的演示检索器

::

    Wed, 19 Jun 2024 23:30:01 GMT
    Hang Li, Tianlong Xu, Jiliang Tang, Qingsong Wen

Knowledge tagging for questions plays a crucial role in contemporary intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization.
Traditionally, these annotations are always conducted by pedagogical experts, as the task requires not only a strong semantic understanding of both question stems and knowledge definitions but also deep insights into connecting question-solving logic with corresponding knowledge concepts. With the recent emergence of advanced text encoding algorithms, such as pre-trained language models, many researchers have developed automatic knowledge tagging systems based on calculating the semantic similarity between the knowledge and question embeddings. In this paper, we explore automating the task using Large Language Models (LLMs), in response to the inability of prior encoding-based methods to deal with the hard cases which involve strong domain knowledge and complicated concept definitions. By showing the strong performance of zero- and few-shot results over math questions knowledge tagging tasks, we demonstrate LLMs' great potential in conquering the challenges faced by prior methods. Furthermore, by proposing a reinforcement learning-based demonstration retriever, we successfully exploit the great potential of different-sized LLMs in achieving better performance results while keeping the in-context demonstration usage efficiency high.

------------

`[2406.14277] Augmenting Query and Passage for Retrieval-Augmented Generation using LLMs for Open-Domain Question Answering <https://arxiv.org/abs/2406.14277>`__ 使用llm进行开放域问答的检索增强生成的查询和通道扩充

::

    Thu, 20 Jun 2024 12:59:27 GMT
    Minsang Kim, Cheoneum Park, Seungjun Baek

Retrieval-augmented generation (RAG) has received much attention for Open-domain question-answering (ODQA) tasks as a means to compensate for the parametric knowledge of large language models (LLMs). While previous approaches focused on processing retrieved passages to remove irrelevant context, they still rely heavily on the quality of retrieved passages which can degrade if the question is ambiguous or complex. In this paper, we propose a simple yet efficient method called question and passage augmentation via LLMs for open-domain QA. Our method first decomposes the original questions into multiple-step sub-questions. By augmenting the original question with detailed sub-questions and planning, we are able to make the query more specific on what needs to be retrieved, improving the retrieval performance. In addition, to compensate for the case where the retrieved passages contain distracting information or divided opinions, we augment the retrieved passages with self-generated passages by LLMs to guide the answer extraction. Experimental results show that the proposed scheme outperforms the previous state-of-the-art and achieves significant performance gain over existing RAG methods.

------------

`[2406.14282] Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs <https://arxiv.org/abs/2406.14282>`__ 从知识图谱中学习规划检索增强的大型语言模型

::

    Thu, 20 Jun 2024 13:07:38 GMT
    Junjie Wang, Mingyang Chen, Binbin Hu, Dan Yang, Ziqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jinjie Gu, Jun Zhou, Jeff Z. Pan, Wen Zhang, Huajun Chen

Improving the performance of large language models (LLMs) in complex question-answering (QA) scenarios has always been a research focal point.
Recent studies have attempted to enhance LLMs' performance by combining step-wise planning with external retrieval. While effective for advanced models like GPT-3.5, smaller LLMs face challenges in decomposing complex questions, necessitating supervised fine-tuning. Previous work has relied on manual annotation and knowledge distillation from teacher LLMs, which are time-consuming and not accurate enough. In this paper, we introduce a novel framework for enhancing LLMs' planning capabilities by using planning data derived from knowledge graphs (KGs). LLMs fine-tuned with this data have improved planning capabilities, better equipping them to handle complex QA tasks that involve retrieval. Evaluations on multiple datasets, including our newly proposed benchmark, highlight the effectiveness of our framework and the benefits of KG-derived planning data.

------------

`[2406.14162] DIRAS: Efficient LLM-Assisted Annotation of Document Relevance in Retrieval Augmented Generation <https://arxiv.org/abs/2406.14162>`__ DIRAS:检索增强生成中高效llm辅助文档相关性标注

::

    Thu, 20 Jun 2024 10:04:09 GMT
    Jingwei Ni, Tobias Schimanski, Meihong Lin, Mrinmaya Sachan, Elliott Ash, Markus Leippold

Retrieval Augmented Generation (RAG) is widely employed to ground responses to queries on domain-specific documents. But do RAG implementations leave out important information or excessively include irrelevant information? To allay these concerns, it is necessary to annotate domain-specific benchmarks to evaluate information retrieval (IR) performance, as relevance definitions vary across queries and domains. Furthermore, such benchmarks should be cost-efficiently annotated to avoid annotation selection bias. In this paper, we propose DIRAS (Domain-specific Information Retrieval Annotation with Scalability), a manual-annotation-free schema that fine-tunes open-sourced LLMs to annotate relevance labels with calibrated relevance probabilities. Extensive evaluation shows that DIRAS fine-tuned models achieve GPT-4-level performance on annotating and ranking unseen (query, document) pairs, and is helpful for real-world RAG development.

------------

`[2406.13679] Prose-to-P4: Leveraging High Level Languages <https://arxiv.org/abs/2406.13679>`__ 散文到p4:利用高级语言

::

    Wed, 19 Jun 2024 16:32:27 GMT
    Mihai-Valentin Dumitru and Vlad-Andrei B\u{a}doiu and Costin Raiciu

Languages such as P4 and NPL have enabled a wide and diverse range of networking applications that take advantage of programmable dataplanes.
However, software development in these languages is difficult. To address this issue, high-level languages have been designed to offer programmers powerful abstractions that reduce the time, effort and domain-knowledge required for developing networking applications. These languages are then translated by a compiler into P4/NPL code. Inspired by the recent success of Large Language Models (LLMs) in the task of code generation, we propose to raise the level of abstraction even higher, employing LLMs to translate prose into high-level networking code. We analyze the problem, focusing on the motivation and opportunities, as well as the challenges involved and sketch out a roadmap for the development of a system that can generate high-level dataplane code from natural language instructions. We present some promising preliminary results on generating Lucid code from natural language.

------------

`[2406.10937] Understanding Understanding: A Pragmatic Framework Motivated by Large Language Models <https://arxiv.org/abs/2406.10937>`__ 理解理解:由大型语言模型驱动的实用框架

::

    replaced with revised version Wed, 19 Jun 2024 08:34:21 GMT
    Submission history From: Kevin Leyton-Brown [view email]
    [v1] Sun, 16 Jun 2024 13:37:08 UTC (56 KB)
    [v2] Wed, 19 Jun 2024 08:34:21 UTC (56 KB)
    Kevin Leyton-Brown and Yoav Shoham

Motivated by the rapid ascent of Large Language Models (LLMs) and debates about the extent to which they possess human-level qualities, we propose a framework for testing whether any agent (be it a machine or a human) understands a subject matter. In Turing-test fashion, the framework is based solely on the agent's performance, and specifically on how well it answers questions. Elements of the framework include circumscribing the set of questions (the "scope of understanding"), requiring general competence ("passing grade"), avoiding "ridiculous answers", but still allowing wrong and "I don't know" answers to some questions. Reaching certainty about these conditions requires exhaustive testing of the questions which is impossible for nontrivial scopes, but we show how high confidence can be achieved via random sampling and the application of probabilistic confidence bounds. We also show that accompanying answers with explanations can improve the sample complexity required to achieve acceptable bounds, because an explanation of an answer implies the ability to answer many similar questions. According to our framework, current LLMs cannot be said to understand nontrivial domains, but as the framework provides a practical recipe for testing understanding, it thus also constitutes a tool for building AI agents that do understand.

------------

`[2305.19118] Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate <https://arxiv.org/abs/2305.19118>`__ 通过多智能体辩论鼓励大型语言模型中的发散性思维

::

    replaced with revised version Wed, 19 Jun 2024 09:35:12 GMT
    Submission history From: Tian Liang [view email]
    [v1] Tue, 30 May 2023 15:25:45 UTC (8,194 KB)
    [v2] Wed, 19 Jun 2024 09:35:12 UTC (8,623 KB)
    Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi

Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of "tit for tat" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of "tit for tat" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Code is available at this https URL.

------------

`[2402.07179] Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models <https://arxiv.org/abs/2402.07179>`__ 基于检索增强生成的大型语言模型中的提示扰动

::

    replaced with revised version Thu, 20 Jun 2024 14:07:27 GMT
    Submission history From: Zhibo Hu [view email]
    [v1] Sun, 11 Feb 2024 12:25:41 UTC (1,336 KB)
    [v2] Thu, 20 Jun 2024 14:07:27 UTC (1,514 KB)
    Zhibo Hu, Chen Wang, Yanfeng Shu, Helen (Hye-Young) Paik, Liming Zhu

The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods.

------------

`[2402.18439] Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication <https://arxiv.org/abs/2402.18439>`__ 超越自然语言:llm利用替代格式来增强推理和沟通

::

    replaced with revised version Wed, 19 Jun 2024 01:42:22 GMT
    Submission history From: Weize Chen [view email]
    [v1] Wed, 28 Feb 2024 16:07:54 UTC (1,936 KB)
    [v2] Tue, 18 Jun 2024 03:06:39 UTC (1,943 KB)
    [v3] Wed, 19 Jun 2024 01:42:22 UTC (1,943 KB)
    Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen Qian, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun

Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code is released at \url{this https URL}.

------------

`[2405.05955] Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning <https://arxiv.org/abs/2405.05955>`__ Smurfs:利用具有上下文效率的多熟练智能体进行工具规划

::

    replaced with revised version Wed, 19 Jun 2024 06:18:12 GMT
    Submission history From: Junzhi Chen [view email]
    [v1] Thu, 9 May 2024 17:49:04 UTC (3,003 KB)
    [v2] Wed, 19 Jun 2024 06:18:12 UTC (3,168 KB)
    Junzhi Chen, Juhao Liang, Benyou Wang

The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance. Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly. This paper introduces `Smurfs', a cutting-edge multi-agent framework designed to revolutionize the application of LLMs. By seamlessly transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs can enhance the model's ability to solve complex tasks at no additional cost. This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents and forming an intelligent multi-agent system. Our empirical investigation on both open-ended task of StableToolBench and closed-ended task on HotpotQA showcases Smurfs' superior capability in intricate tool utilization scenarios. Notably, Smurfs outmatches all the baseline methods in both experiments, setting new state-of-the-art performance. Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy. This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems.

------------

`[2406.12066] Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks <https://arxiv.org/abs/2406.12066>`__ 在生物医学基准中，语言模型对药物名称的影响令人惊讶地脆弱

::

    replaced with revised version Wed, 19 Jun 2024 03:59:41 GMT
    Submission history From: Shan Chen [view email]
    [v1] Mon, 17 Jun 2024 20:09:24 UTC (2,752 KB)
    [v2] Wed, 19 Jun 2024 03:59:41 UTC (2,752 KB)
    Jack Gallifant, Shan Chen, Pedro Moreira, Nikolaj Munch, Mingye Gao, Jackson Pond, Leo Anthony Celi, Hugo Aerts, Thomas Hartvigsen, Danielle Bitterman

Medical knowledge is context-dependent and requires consistent reasoning across various natural language expressions of semantically equivalent phrases. This is particularly crucial for drug names, where patients often use brand names like Advil or Tylenol instead of their generic equivalents. To study this, we create a new robustness dataset, RABBITS, to evaluate performance differences on medical benchmarks after swapping brand and generic drug names using physician expert annotations.
We assess both open-source and API-based LLMs on MedQA and MedMCQA, revealing a consistent performance drop ranging from 1-10\%. Furthermore, we identify a potential source of this fragility as the contamination of test data in widely used pre-training datasets. All code is accessible at this https URL, and a HuggingFace leaderboard is available at this https URL.

------------

`[2406.11147] Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG <https://arxiv.org/abs/2406.11147>`__ vulg -RAG:利用知识级RAG增强基于llm的漏洞检测

::

    replaced with revised version Wed, 19 Jun 2024 17:27:06 GMT
    Submission history From: Xueying Du [view email]
    [v1] Mon, 17 Jun 2024 02:25:45 UTC (2,156 KB)
    [v2] Wed, 19 Jun 2024 17:27:06 UTC (2,156 KB)
    Xueying Du, Geng Zheng, Kaixin Wang, Jiayi Feng, Wentai Deng, Mingwei Liu, Bihuan Chen, Xin Peng, Tao Ma, Yiling Lou

Vulnerability detection is essential for software quality assurance. In recent years, deep learning models (especially large language models) have shown promise in vulnerability detection. In this work, we propose a novel LLM-based vulnerability detection technique Vul-RAG, which leverages knowledge-level retrieval-augmented generation (RAG) framework to detect vulnerability for the given code in three phases. First, Vul-RAG constructs a vulnerability knowledge base by extracting multi-dimension knowledge via LLMs from existing CVE instances; second, for a given code snippet, Vul-RAG} retrieves the relevant vulnerability knowledge from the constructed knowledge base based on functional semantics; third, Vul-RAG leverages LLMs to check the vulnerability of the given code snippet by reasoning the presence of vulnerability causes and fixing solutions of the retrieved vulnerability knowledge. Our evaluation of Vul-RAG on our constructed benchmark PairVul shows that Vul-RAG substantially outperforms all baselines by 12.96\%/110\% relative improvement in accuracy/pairwise-accuracy. In addition, our user study shows that the vulnerability knowledge generated by Vul-RAG can serve as high-quality explanations which can improve the manual detection accuracy from 0.60 to 0.77.

------------

----------
Agent (18)
----------

`[2406.13693] From Single Agent to Multi-Agent: Improving Traffic Signal Control <https://arxiv.org/abs/2406.13693>`__ 从单Agent到多Agent:改进交通信号控制

::

    Wed, 19 Jun 2024 16:46:15 GMT
    Maksim Tislenko and Dmitrii Kisilev

Due to accelerating urbanization, the importance of solving the signal control problem increases. This paper analyzes various existing methods and suggests options for increasing the number of agents to reduce the average travel time. Experiments were carried out with 2 datasets. The results show that in some cases, the implementation of multiple agents can improve existing methods. For a fine-tuned large language model approach there is small enhancement on all metrics.

------------

`[2406.13840] StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation <https://arxiv.org/abs/2406.13840>`__ StackRAG Agent:用检索增强生成改进开发人员的回答

::

    Wed, 19 Jun 2024 21:07:35 GMT
    Davit Abrahamyan, Fatemeh H. Fard

Developers spend much time finding information that is relevant to their questions. Stack Overflow has been the leading resource, and with the advent of Large Language Models (LLMs), generative models such as ChatGPT are used frequently. However, there is a catch in using each one separately. Searching for answers is time-consuming and tedious, as shown by the many tools developed by researchers to address this issue. On the other, using LLMs is not reliable, as they might produce irrelevant or unreliable answers (i.e., hallucination).
In this work, we present StackRAG, a retrieval-augmented Multiagent generation tool based on LLMs that combines the two worlds: aggregating the knowledge from SO to enhance the reliability of the generated answers. Initial evaluations show that the generated answers are correct, accurate, relevant, and useful.

------------

`[2406.14228] EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms <https://arxiv.org/abs/2406.14228>`__ EvoAgent:基于进化算法的多agent自动生成

::

    Thu, 20 Jun 2024 11:49:23 GMT
    Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang

The rise of powerful large language models (LLMs) has spurred a new trend in building LLM-based autonomous agents for solving complex tasks, especially multi-agent systems. Despite the remarkable progress, we notice that existing works are heavily dependent on human-designed frameworks, which greatly limits the functional scope and scalability of agent systems. How to automatically extend the specialized agent to multi-agent systems to improve task-solving capability still remains a significant challenge. In this paper, we introduce EvoAgent, a generic method to automatically extend expert agents to multi-agent systems via the evolutionary algorithm, thereby improving the effectiveness of LLM-based agents in solving tasks. Specifically, we consider the existing agent frameworks as the initial individual and then apply a series of evolutionary operators (e.g., mutation, crossover, selection, etc.) to generate multiple agents with diverse agent settings. EvoAgent can be generalized to any LLM-based agent framework, and can automatically extend the existing agent framework to multi-agent systems without any extra human designs. Experimental results across various tasks have shown that EvoAgent can automatically generate multiple expert agents and significantly enhance the task-solving capabilities of LLM-based agents.

------------

`[2406.14373] Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory <https://arxiv.org/abs/2406.14373>`__ 人工利维坦:从霍布斯社会契约论视角探索LLM主体的社会演化

::

    Thu, 20 Jun 2024 14:42:58 GMT
    Gordon Dai, Weijia Zhang, Jinhan Li, Siqi Yang, Chidera Onochie lbe, Srihas Rao, Arthur Caetano and Misha Sra

The emergence of Large Language Models (LLMs) and advancements in Artificial Intelligence (AI) offer an opportunity for computational social science research at scale. Building upon prior explorations of LLM agent design, our work introduces a simulated agent society where complex social relationships dynamically form and evolve over time. Agents are imbued with psychological drives and placed in a sandbox survival environment. We conduct an evaluation of the agent society through the lens of Thomas Hobbes's seminal Social Contract Theory (SCT). We analyze whether, as the theory postulates, agents seek to escape a brutish "state of nature" by surrendering rights to an absolute sovereign in exchange for order and security. Our experiments unveil an alignment: Initially, agents engage in unrestrained conflict, mirroring Hobbes's depiction of the state of nature. However, as the simulation progresses, social contracts emerge, leading to the authorization of an absolute sovereign and the establishment of a peaceful commonwealth founded on mutual cooperation. This congruence between our LLM agent society's evolutionary trajectory and Hobbes's theoretical account indicates LLMs' capability to model intricate social dynamics and potentially replicate forces that shape human societies. By enabling such insights into group behavior and emergent societal phenomena, LLM-driven multi-agent simulations, while unable to simulate all the nuances of human behavior, may hold potential for advancing our understanding of social structures, group dynamics, and complex human systems.

------------

`[2406.13144] DialSim: A Real-Time Simulator for Evaluating Long-Term Dialogue Understanding of Conversational Agents <https://arxiv.org/abs/2406.13144>`__ DialSim:用于评估会话代理长期对话理解的实时模拟器

::

    Wed, 19 Jun 2024 01:37:10 GMT
    Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, Edward Choi

Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of conversational agents, making them applicable to various fields (e.g., education). Despite their progress, the evaluation of the agents often overlooks the complexities of real-world conversations, such as real-time interactions, multi-party dialogues, and extended contextual dependencies. To bridge this gap, we introduce DialSim, a real-time dialogue simulator. In this simulator, an agent is assigned the role of a character from popular TV shows, requiring it to respond to spontaneous questions using past dialogue information and to distinguish between known and unknown information.
Key features of DialSim include evaluating the agent's ability to respond within a reasonable time limit, handling long-term multi-party dialogues, and managing adversarial settings (e.g., swap character names) to challenge the agent's reliance on pre-trained knowledge. We utilized this simulator to evaluate the latest conversational agents and analyze their limitations. Our experiments highlight both the strengths and weaknesses of these agents, providing valuable insights for future improvements in the field of conversational AI. DialSim is available at https://github.com/jiho283/Simulator.

------------

`[2406.13381] CoAct: A Global-Local Hierarchy for Autonomous Agent Collaboration <https://arxiv.org/abs/2406.13381>`__ CoAct:一种面向自主Agent协作的全局-局部层次结构

::

    Wed, 19 Jun 2024 09:23:53 GMT
    Xinming Hou, Mingming Yang, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Wayne Xin Zhao

Existing LLMs exhibit remarkable performance on various NLP tasks, but still struggle with complex real-world tasks, even equipped with advanced strategies like CoT and ReAct. In this work, we propose the CoAct framework, which transfers the hierarchical planning and collaboration patterns in human society to LLM systems. Specifically, our CoAct framework involves two agents: (1) A global planning agent, to comprehend the problem scope, formulate macro-level plans and provide detailed sub-task descriptions to local execution agents, which serves as the initial rendition of a global plan. (2) A local execution agent, to operate within the multi-tier task execution structure, focusing on detailed execution and implementation of specific tasks within the global plan.
Experimental results on the WebArena benchmark show that CoAct can re-arrange the process trajectory when facing failures, and achieves superior performance over baseline methods on long-horizon web tasks. Code is available at https://github.com/xmhou2002/CoAct.

------------

`[2406.13408] SQLFixAgent: Towards Semantic-Accurate SQL Generation via Multi-Agent Collaboration <https://arxiv.org/abs/2406.13408>`__ SQLFixAgent:通过多agent协作实现语义精确的SQL生成

::

    Wed, 19 Jun 2024 09:57:19 GMT
    Jipeng Cen and Jiaxin Liu and Zhixu Li and Jingjing Wang

While fine-tuned large language models (LLMs) excel in generating grammatically valid SQL in Text-to-SQL parsing, they often struggle to ensure semantic accuracy in queries, leading to user confusion and diminished system usability. To tackle this challenge, we introduce SQLFixAgent, an innovative multi-agent collaborative framework designed for detecting and repairing erroneous SQL. Our framework comprises a core agent, SQLRefiner, alongside two auxiliary agents: SQLReviewer and QueryCrafter. The SQLReviewer agent employs the rubber duck debugging method to identify potential semantic mismatches between SQL statement and user query. If the error is detected, the QueryCrafter agent generates multiple SQL statements as candidate repairs using a fine-tuned SQLTool. Subsequently, leveraging similar repair retrieval and failure memory reflexion, the SQLRefiner agent selects the most fitting SQL statement from the candidates as the final repair. We evaluated our proposed framework on five Text-to-SQL benchmarks. The experimental results show that our method consistently enhances the performance of the baseline model, specifically achieving an execution accuracy improvement of over 3\% on the Bird benchmark. Our framework also has a higher token efficiency compared to other advanced methods, making it more competitive.

------------

`[2406.13890] ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World <https://arxiv.org/abs/2406.13890>`__ ClinicalLab:现实世界中多部门临床诊断的代理对齐

::

    Wed, 19 Jun 2024 23:44:25 GMT
    Weixiang Yan, Haitian Liu, Tengxiao Wu, Qian Chen, Wen Wang, Haoyuan Chai, Jiayi Wang, Weishan Zhao, Yixin Zhang, Renjun Zhang, Li Zhu

LLMs have achieved significant performance progress in various NLP applications. However, LLMs still struggle to meet the strict requirements for accuracy and reliability in the medical field and face many challenges in clinical applications. Existing clinical diagnostic evaluation benchmarks for evaluating medical agents powered by LLMs have severe limitations. Firstly, most existing medical evaluation benchmarks face the risk of data leakage or contamination. Secondly, existing benchmarks often neglect the characteristics of multiple departments and specializations in modern medical practice.
Thirdly, existing evaluation methods are limited to multiple-choice questions, which do not align with the real-world diagnostic scenarios. Lastly, existing evaluation methods lack comprehensive evaluations of end-to-end real clinical scenarios. These limitations in benchmarks in turn obstruct advancements of LLMs and agents for medicine. To address these limitations, we introduce ClinicalLab, a comprehensive clinical diagnosis agent alignment suite.
ClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical diagnostic evaluation benchmark for evaluating medical agents and LLMs.
ClinicalBench is based on real cases that cover 24 departments and 150 diseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for evaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate 17 LLMs and find that their performance varies significantly across different departments. Based on these findings, in ClinicalLab, we propose ClinicalAgent, an end-to-end clinical agent that aligns with real-world clinical diagnostic practices. We systematically investigate the performance and applicable scenarios of variants of ClinicalAgent on ClinicalBench. Our findings demonstrate the importance of aligning with modern medical practices in designing medical agents.

------------

`[2406.14498] LLaSA: Large Multimodal Agent for Human Activity Analysis Through Wearable Sensors <https://arxiv.org/abs/2406.14498>`__ LLaSA:通过可穿戴传感器进行人体活动分析的大型多模态智能体

::

    Thu, 20 Jun 2024 17:00:34 GMT
    Sheikh Asif Imran, Mohammad Nur Hossain Khan, Subrata Biswas, Bashima Islam

Integrating inertial measurement units (IMUs) with large language models (LLMs) advances multimodal AI by enhancing human activity understanding. We introduce SensorCaps, a dataset of 26,288 IMU-derived activity narrations, and OpenSQA, an instruction-following dataset with 257,562 question-answer pairs.
Combining LIMU-BERT and Llama, we develop LLaSA, a Large Multimodal Agent capable of interpreting and responding to activity and motion analysis queries.
Our evaluation demonstrates LLaSA's effectiveness in activity classification and question answering, highlighting its potential in healthcare, sports science, and human-computer interaction. These contributions advance sensor-aware language models and open new research avenues. Our code repository and datasets can be found on https://github.com/BASHLab/LLaSA.

------------

`[2406.14550] GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models <https://arxiv.org/abs/2406.14550>`__ GraphReader:构建基于图的Agent增强大型语言模型的长上下文能力

::

    Thu, 20 Jun 2024 17:57:51 GMT
    Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, Bo Zheng

Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs.
In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.

------------

`[2406.12952] Code Agents are State of the Art Software Testers <https://arxiv.org/abs/2406.12952>`__ 代码代理是最先进的软件测试人员

::

    Tue, 18 Jun 2024 14:54:37 GMT
    Niels M\"undler, Mark Niklas M\"uller, Jingxuan He, Martin Vechev

Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents for formalizing user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth patches, and golden tests. We find that LLMs generally perform surprisingly well at generating relevant test cases with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using fail-to-pass rate and coverage metrics, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-Agent.

------------

`[2406.13352] AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents <https://arxiv.org/abs/2406.13352>`__ AgentDojo:为LLM代理评估攻击和防御的动态环境

::

    Wed, 19 Jun 2024 08:55:56 GMT
    Edoardo Debenedetti and Jie Zhang and Mislav Balunovi\'c and Luca Beurer-Kellner and Marc Fischer and Florian Tram\`er

AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.

------------

`[2311.17541] TaskWeaver: A Code-First Agent Framework <https://arxiv.org/abs/2311.17541>`__ TaskWeaver:一个代码优先的代理框架

::

    replaced with revised version Thu, 20 Jun 2024 02:53:20 GMT
    Submission history From: Shilin He [view email]
    [v1] Wed, 29 Nov 2023 11:23:42 UTC (1,398 KB)
    [v2] Fri, 1 Dec 2023 07:42:56 UTC (1,398 KB)
    [v3] Thu, 20 Jun 2024 02:53:20 UTC (3,288 KB)
    Bo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang Dong, Jue Zhang, Lu Wang, Minghua Ma, Pu Zhao, Si Qin, Xiaoting Qin, Chao Du, Yong Xu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang

Large Language Models (LLMs) have shown impressive abilities in natural language understanding and generation, leading to their widespread use in applications such as chatbots and virtual assistants. However, existing LLM frameworks face limitations in handling domain-specific data analytics tasks with rich data structures. Moreover, they struggle with flexibility to meet diverse user requirements. To address these issues, TaskWeaver is proposed as a code-first framework for building LLM-powered autonomous agents. It converts user requests into executable code and treats user-defined plugins as callable functions. TaskWeaver provides support for rich data structures, flexible plugin usage, and dynamic plugin selection, and leverages LLM coding capabilities for complex logic. It also incorporates domain-specific knowledge through examples and ensures the secure execution of generated code. TaskWeaver offers a powerful and flexible framework for creating intelligent conversational agents that can handle complex tasks and adapt to domain-specific scenarios. The code is open sourced at this https URL.

------------

`[2403.11381] Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot <https://arxiv.org/abs/2403.11381>`__ 增强了llm的自治代理可以合作吗?，通过Melting Pot对它们的合作能力进行评估

::

    replaced with revised version Wed, 19 Jun 2024 16:23:05 GMT
    Submission history From: Manuel Mosquera [view email]
    [v1] Mon, 18 Mar 2024 00:13:43 UTC (1,147 KB)
    [v2] Wed, 19 Jun 2024 16:23:05 UTC (6,321 KB)
    Manuel Mosquera, Juan Sebastian Pinzon, Manuel Rios, Yesid Fonseca, Luis Felipe Giraldo, Nicanor Quijano, Ruben Manrique

As the field of AI continues to evolve, a significant dimension of this progression is the development of Large Language Models and their potential to enhance multi-agent artificial intelligence systems. This paper explores the cooperative capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as GPT4 and GPT3.5. Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures. The study's contributions include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the implementation of a reusable architecture for LLM-mediated agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of metrics tied to the Melting Pot's "Commons Harvest" game. The paper closes, by discussing the limitations of the current architectural framework and the potential of a new set of modules that fosters better cooperation among LAAs.

------------

`[2403.17209] Generation of Asset Administration Shell with Large Language Model Agents: Towards Semantic Interoperability in Digital Twins in the Context of Industry 4.0 <https://arxiv.org/abs/2403.17209>`__ 具有大型语言模型代理的资产管理外壳的生成:工业4.0背景下面向数字孪生的语义互操作性

::

    replaced with revised version Wed, 19 Jun 2024 10:32:21 GMT
    Submission history From: Yuchen Xia [view email]
    [v1] Mon, 25 Mar 2024 21:37:30 UTC (1,372 KB)
    [v2] Tue, 28 May 2024 00:00:38 UTC (1,420 KB)
    [v3] Wed, 19 Jun 2024 10:32:21 UTC (1,444 KB)
    Yuchen Xia, Zhewen Xiao, Nasser Jazdi and Michael Weyrich

This research introduces a novel approach for achieving semantic interoperability in digital twins and assisting the creation of Asset Administration Shell (AAS) as digital twin model within the context of Industry 4.0. The foundational idea of our research is that the communication based on semantics and the generation of meaningful textual data are directly linked, and we posit that these processes are equivalent if the exchanged information can be serialized in text form. Based on this, we construct a "semantic node" data structure in our research to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process the "semantic node" and generate standardized digital twin models from raw textual data collected from datasheets describing technical assets. Our evaluation demonstrates an effective generation rate of 62-79%, indicating a substantial proportion of the information from the source text can be translated error-free to the target digital twin instance model with the generative capability of large language models. This result has a direct application in the context of Industry 4.0, and the designed system is implemented as a data model generation tool for reducing the manual effort in creating AAS model. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM systems for interpreting technical concepts and translating data. Our findings emphasize LLMs' capability to automate AAS instance creation and contribute to the broader field of semantic interoperability for digital twins in industrial applications. The prototype implementation and evaluation results are presented on our GitHub Repository: this https URL.

------------

`[2305.19118] Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate <https://arxiv.org/abs/2305.19118>`__ 通过多智能体辩论鼓励大型语言模型中的发散性思维

::

    replaced with revised version Wed, 19 Jun 2024 09:35:12 GMT
    Submission history From: Tian Liang [view email]
    [v1] Tue, 30 May 2023 15:25:45 UTC (8,194 KB)
    [v2] Wed, 19 Jun 2024 09:35:12 UTC (8,623 KB)
    Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi

Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of "tit for tat" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of "tit for tat" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Code is available at this https URL.

------------

`[2405.05955] Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning <https://arxiv.org/abs/2405.05955>`__ Smurfs:利用具有上下文效率的多熟练智能体进行工具规划

::

    replaced with revised version Wed, 19 Jun 2024 06:18:12 GMT
    Submission history From: Junzhi Chen [view email]
    [v1] Thu, 9 May 2024 17:49:04 UTC (3,003 KB)
    [v2] Wed, 19 Jun 2024 06:18:12 UTC (3,168 KB)
    Junzhi Chen, Juhao Liang, Benyou Wang

The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance. Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly. This paper introduces `Smurfs', a cutting-edge multi-agent framework designed to revolutionize the application of LLMs. By seamlessly transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs can enhance the model's ability to solve complex tasks at no additional cost. This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents and forming an intelligent multi-agent system. Our empirical investigation on both open-ended task of StableToolBench and closed-ended task on HotpotQA showcases Smurfs' superior capability in intricate tool utilization scenarios. Notably, Smurfs outmatches all the baseline methods in both experiments, setting new state-of-the-art performance. Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy. This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems.

------------

`[2406.08689] Security of AI Agents <https://arxiv.org/abs/2406.08689>`__ 人工智能agent的安全性

::

    replaced with revised version Thu, 20 Jun 2024 04:55:30 GMT
    Submission history From: Yifeng He [view email]
    [v1] Wed, 12 Jun 2024 23:16:45 UTC (725 KB)
    [v2] Thu, 20 Jun 2024 04:55:30 UTC (595 KB)
    Yifeng He, Ethan Wang, Yuyang Rong, Zifei Cheng, Hao Chen

The study and development of AI agents have been boosted by large language models. AI agents can function as intelligent assistants and complete tasks on behalf of their users with access to tools and the ability to execute commands in their environments, Through studying and experiencing the workflow of typical AI agents, we have raised several concerns regarding their security. These potential vulnerabilities are not addressed by the frameworks used to build the agents, nor by research aimed at improving the agents. In this paper, we identify and describe these vulnerabilities in detail from a system security perspective, emphasizing their causes and severe effects. Furthermore, we introduce defense mechanisms corresponding to each vulnerability with meticulous design and experiments to evaluate their viability. Altogether, this paper contextualizes the security issues in the current development of AI agents and delineates methods to make AI agents safer and more reliable.

------------

-----------
Other (206)
-----------

`[2406.13161] APPL: A Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts <https://arxiv.org/abs/2406.13161>`__ APPL:一种融合程序和大型语言模型提示符的提示式程序设计语言

::

    Wed, 19 Jun 2024 02:29:59 GMT
    Honghua Dong, Qidong Su, Yubo Gao, Zhaoyu Li, Yangjun Ruan, Gennady Pekhimenko, Chris J. Maddison, Xujie Si

Large Language Models (LLMs) have become increasingly capable of handling diverse tasks with the aid of well-crafted prompts and integration of external tools, but as task complexity rises, the workflow involving LLMs can be complicated and thus challenging to implement and maintain. To address this challenge, we propose APPL, A Prompt Programming Language that acts as a bridge between computer programs and LLMs, allowing seamless embedding of prompts into Python functions, and vice versa. APPL provides an intuitive and Python-native syntax, an efficient parallelized runtime with asynchronous semantics, and a tracing module supporting effective failure diagnosis and replaying without extra costs. We demonstrate that APPL programs are intuitive, concise, and efficient through three representative scenarios: Chain-of-Thought with self-consistency (CoT-SC), ReAct tool use agent, and multi-agent chat.
Experiments on three parallelizable workflows further show that APPL can effectively parallelize independent LLM calls, with a significant speedup ratio that almost matches the estimation.

------------

`[2406.13233] AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models <https://arxiv.org/abs/2406.13233>`__ AdaMoE:基于空专家的专家混合语言模型token自适应路由

::

    Wed, 19 Jun 2024 05:47:10 GMT
    Zihao Zeng, Yibo Miao, Hongcheng Gao, Hao Zhang, Zhijie Deng

Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce a constant top-k routing for all tokens, which is arguably restrictive because various tokens (e.g., "<EOS>" vs. "apple") may require various numbers of experts for feature abstraction. Lifting such a constraint can help make the most of limited resources and unleash the potential of the model for downstream tasks. In this sense, we introduce AdaMoE to realize token-adaptive routing for MoE, where different tokens are permitted to select a various number of experts. AdaMoE makes minimal modifications to the vanilla MoE with top-k routing -- it simply introduces a fixed number of null experts, which do not consume any FLOPs, to the expert set and increases the value of k. AdaMoE does not force each token to occupy a fixed number of null experts but ensures the average usage of the null experts with a load-balancing loss, leading to an adaptive number of null/true experts used by each token. AdaMoE exhibits a strong resemblance to MoEs with expert choice routing while allowing for trivial auto-regressive modeling. AdaMoE is easy to implement and can be effectively applied to pre-trained (MoE-)LLMs. Extensive studies show that AdaMoE can reduce average expert load (FLOPs) while achieving superior performance. For example, on the ARC-C dataset, applying our method to fine-tuning Mixtral-8x7B can reduce FLOPs by 14.5% while increasing accuracy by 1.69%.

------------

`[2406.13250] LangTopo: Aligning Language Descriptions of Graphs with Tokenized Topological Modeling <https://arxiv.org/abs/2406.13250>`__ LangTopo:基于标记化拓扑建模的图的语言描述对齐

::

    Wed, 19 Jun 2024 06:20:22 GMT
    Zhong Guan, Hongke Zhao, Likang Wu, Ming He, Jianpin Fan

Recently, large language models (LLMs) have been widely researched in the field of graph machine learning due to their outstanding abilities in language comprehension and learning. However, the significant gap between natural language tasks and topological structure modeling poses a nonnegligible challenge. Specifically, since natural language descriptions are not sufficient for LLMs to understand and process graph-structured data, fine-tuned LLMs perform even worse than some traditional GNN models on graph tasks, lacking inherent modeling capabilities for graph structures. Existing research overly emphasizes LLMs' understanding of semantic information captured by external models, while inadequately exploring graph topological structure modeling, thereby overlooking the genuine capabilities that LLMs lack. Consequently, in this paper, we introduce a new framework, LangTopo, which aligns graph structure modeling with natural language understanding at the token level.
LangTopo quantifies the graph structure modeling capabilities of GNNs and LLMs by constructing a codebook for the graph modality and performs consistency maximization. This process aligns the text description of LLM with the topological modeling of GNN, allowing LLM to learn the ability of GNN to capture graph structures, enabling LLM to handle graph-structured data independently. We demonstrate the effectiveness of our proposed method on multiple datasets.

------------

`[2406.13269] Investigating Low-Cost LLM Annotation for~Spoken Dialogue Understanding Datasets <https://arxiv.org/abs/2406.13269>`__ 

::

    Wed, 19 Jun 2024 06:59:57 GMT
    Lucas Druart (LIA), Valentin Vielzeuf, Yannick Est\`eve (LIA)

In spoken Task-Oriented Dialogue (TOD) systems, the choice of the semantic representation describing the users' requests is key to a smooth interaction.
Indeed, the system uses this representation to reason over a database and its domain knowledge to choose its next action. The dialogue course thus depends on the information provided by this semantic representation. While textual datasets provide fine-grained semantic representations, spoken dialogue datasets fall behind. This paper provides insights into automatic enhancement of spoken dialogue datasets' semantic representations. Our contributions are three fold: (1) assess the relevance of Large Language Model fine-tuning, (2) evaluate the knowledge captured by the produced annotations and (3) highlight semi-automatic annotation implications.

------------

`[2406.13305] Multimodal MRI-based Detection of Amyloid Status in Alzheimer's Disease Continuum <https://arxiv.org/abs/2406.13305>`__ 基于多模态mri检测阿尔茨海默病连续体中的淀粉样蛋白状态

::

    Wed, 19 Jun 2024 07:51:21 GMT
    Giorgio Dolci (1,2,3), Charles A. Ellis (3), Federica Cruciani (2), Lorenza Brusini (2), Anees Abrol (3), Ilaria Boscolo Galazzo (2), Gloria Menegaz (2) and Vince D. Calhoun (3) ((1) Department of Computer Science, University of Verona, Verona, Italy, (2) Department of Engineering for Innovation Medicine, University of Verona, Verona, Italy, (3) Tri-Institutional Center for Translational Research in Neuroimaging and Data Science (TReNDS), Georgia State University, Georgia Institute of Technology, Emory University, Atlanta, GA, USA)

Amyloid-$\beta$ (A$\beta$) plaques in conjunction with hyperphosphorylated tau proteins in the form of neurofibrillary tangles are the two neuropathological hallmarks of Alzheimer's disease (AD). In particular, the accumulation of A$\beta$ plaques, as evinced by the A/T/N (amyloid/tau/neurodegeneration) framework, marks the initial stage. Thus, the identification of individuals with A$\beta$ positivity could enable early diagnosis and potentially lead to more effective interventions. Deep learning methods relying mainly on amyloid PET images have been employed to this end.
However, PET imaging has some disadvantages, including the need of radiotracers and expensive acquisitions. Hence, in this work, we propose a novel multimodal approach that integrates information from structural, functional, and diffusion MRI data to discriminate A$\beta$ status in the AD continuum. Our method achieved an accuracy of $0.762\pm0.04$. Furthermore, a \textit{post-hoc} explainability analysis (guided backpropagation) was performed to retrieve the brain regions that most influenced the model predictions. This analysis identified some key regions that were common across modalities, some of which were well-established AD-discriminative biomarkers and related to A$\beta$ deposition, such as the hippocampus, thalamus, precuneus, and cingulate gyrus.
Hence, our study demonstrates the potential viability of MRI-based characterization of A$\beta$ status, paving the way for further research in this domain.

------------

`[2406.13399] VELO: A Vector Database-Assisted Cloud-Edge Collaborative LLM QoS Optimization Framework <https://arxiv.org/abs/2406.13399>`__ VELO:矢量数据库辅助的云边协同LLM QoS优化框架

::

    Wed, 19 Jun 2024 09:41:37 GMT
    Zhi Yao, Zhiqing Tang, Jiong Lou, Ping Shen, Weijia Jia

The Large Language Model (LLM) has gained significant popularity and is extensively utilized across various domains. Most LLM deployments occur within cloud data centers, where they encounter substantial response delays and incur high costs, thereby impacting the Quality of Services (QoS) at the network edge. Leveraging vector database caching to store LLM request results at the edge can substantially mitigate response delays and cost associated with similar requests, which has been overlooked by previous research. Addressing these gaps, this paper introduces a novel Vector database-assisted cloud-Edge collaborative LLM QoS Optimization (VELO) framework. Firstly, we propose the VELO framework, which ingeniously employs vector database to cache the results of some LLM requests at the edge to reduce the response time of subsequent similar requests. Diverging from direct optimization of the LLM, our VELO framework does not necessitate altering the internal structure of LLM and is broadly applicable to diverse LLMs. Subsequently, building upon the VELO framework, we formulate the QoS optimization problem as a Markov Decision Process (MDP) and devise an algorithm grounded in Multi-Agent Reinforcement Learning (MARL) to decide whether to request the LLM in the cloud or directly return the results from the vector database at the edge. Moreover, to enhance request feature extraction and expedite training, we refine the policy network of MARL and integrate expert demonstrations. Finally, we implement the proposed algorithm within a real edge system. Experimental findings confirm that our VELO framework substantially enhances user satisfaction by concurrently diminishing delay and resource consumption for edge users utilizing LLMs.

------------

`[2406.13558] Enhancing Travel Choice Modeling with Large Language Models: A Prompt-Learning Approach <https://arxiv.org/abs/2406.13558>`__ 利用大型语言模型增强旅游选择建模:一种快速学习方法

::

    Wed, 19 Jun 2024 13:46:08 GMT
    Xuehao Zhai, Hanlin Tian, Lintong Li, Tianyu Zhao

Travel choice analysis is crucial for understanding individual travel behavior to develop appropriate transport policies and recommendation systems in Intelligent Transportation Systems (ITS). Despite extensive research, this domain faces two critical challenges: a) modeling with limited survey data, and b) simultaneously achieving high model explainability and accuracy. In this paper, we introduce a novel prompt-learning-based Large Language Model(LLM) framework that significantly improves prediction accuracy and provides explicit explanations for individual predictions. This framework involves three main steps: transforming input variables into textual form; building of demonstrations similar to the object, and applying these to a well-trained LLM.
We tested the framework's efficacy using two widely used choice datasets: London Passenger Mode Choice (LPMC) and Optima-Mode collected in Switzerland.
The results indicate that the LLM significantly outperforms state-of-the-art deep learning methods and discrete choice models in predicting people's choices. Additionally, we present a case of explanation illustrating how the LLM framework generates understandable and explicit explanations at the individual level.

------------

`[2406.13715] Converging Dimensions: Information Extraction and Summarization through Multisource, Multimodal, and Multilingual Fusion <https://arxiv.org/abs/2406.13715>`__ 

::

    Wed, 19 Jun 2024 17:15:47 GMT
    Pranav Janjani, Mayank Palan, Sarvesh Shirude, Ninad Shegokar, Sunny Kumar, Faruk Kazi

Recent advances in large language models (LLMs) have led to new summarization strategies, offering an extensive toolkit for extracting important information.
However, these approaches are frequently limited by their reliance on isolated sources of data. The amount of information that can be gathered is limited and covers a smaller range of themes, which introduces the possibility of falsified content and limited support for multilingual and multimodal data. The paper proposes a novel approach to summarization that tackles such challenges by utilizing the strength of multiple sources to deliver a more exhaustive and informative understanding of intricate topics. The research progresses beyond conventional, unimodal sources such as text documents and integrates a more diverse range of data, including YouTube playlists, pre-prints, and Wikipedia pages. The aforementioned varied sources are then converted into a unified textual representation, enabling a more holistic analysis. This multifaceted approach to summary generation empowers us to extract pertinent information from a wider array of sources. The primary tenet of this approach is to maximize information gain while minimizing information overlap and maintaining a high level of informativeness, which encourages the generation of highly coherent summaries.

------------

`[2406.13873] A Pure Transformer Pretraining Framework on Text-attributed Graphs <https://arxiv.org/abs/2406.13873>`__ 文本属性图上的纯Transformer预训练框架

::

    Wed, 19 Jun 2024 22:30:08 GMT
    Yu Song, Haitao Mao, Jiachen Xiao, Jingzhe Liu, Zhikai Chen, Wei Jin, Carl Yang, Jiliang Tang, Hui Liu

Pretraining plays a pivotal role in acquiring generalized knowledge from large-scale data, achieving remarkable successes as evidenced by large models in CV and NLP. However, progress in the graph domain remains limited due to fundamental challenges such as feature heterogeneity and structural heterogeneity. Recently, increasing efforts have been made to enhance node feature quality with Large Language Models (LLMs) on text-attributed graphs (TAGs), demonstrating superiority to traditional bag-of-words or word2vec techniques. These high-quality node features reduce the previously critical role of graph structure, resulting in a modest performance gap between Graph Neural Networks (GNNs) and structure-agnostic Multi-Layer Perceptrons (MLPs).
Motivated by this, we introduce a feature-centric pretraining perspective by treating graph structure as a prior and leveraging the rich, unified feature space to learn refined interaction patterns that generalizes across graphs. Our framework, Graph Sequence Pretraining with Transformer (GSPT), samples node contexts through random walks and employs masked feature reconstruction to capture pairwise proximity in the LLM-unified feature space using a standard Transformer. By utilizing unified text representations rather than varying structures, our framework achieves significantly better transferability among graphs within the same domain. GSPT can be easily adapted to both node classification and link prediction, demonstrating promising empirical success on various datasets.

------------

`[2406.13919] SPL: A Socratic Playground for Learning Powered by Large Language Mode <https://arxiv.org/abs/2406.13919>`__ SPL:由大型语言模式驱动的苏格拉底式学习游乐场

::

    Thu, 20 Jun 2024 01:18:52 GMT
    Liang Zhang, Jionghao Lin, Ziyi Kuang, Sheng Xu, Mohammed Yeasin, Xiangen Hu

Dialogue-based Intelligent Tutoring Systems (ITSs) have significantly advanced adaptive and personalized learning by automating sophisticated human tutoring strategies within interactive dialogues. However, replicating the nuanced patterns of expert human communication remains a challenge in Natural Language Processing (NLP). Recent advancements in NLP, particularly Large Language Models (LLMs) such as OpenAI's GPT-4, offer promising solutions by providing human-like and context-aware responses based on extensive pre-trained knowledge. Motivated by the effectiveness of LLMs in various educational tasks (e.g., content creation and summarization, problem-solving, and automated feedback provision), our study introduces the Socratic Playground for Learning (SPL), a dialogue-based ITS powered by the GPT-4 model, which employs the Socratic teaching method to foster critical thinking among learners. Through extensive prompt engineering, SPL can generate specific learning scenarios and facilitates efficient multi-turn tutoring dialogues. The SPL system aims to enhance personalized and adaptive learning experiences tailored to individual needs, specifically focusing on improving critical thinking skills. Our pilot experimental results from essay writing tasks demonstrate SPL has the potential to improve tutoring interactions and further enhance dialogue-based ITS functionalities. Our study, exemplified by SPL, demonstrates how LLMs enhance dialogue-based ITSs and expand the accessibility and efficacy of educational technologies.

------------

`[2406.13945] CityBench: Evaluating the Capabilities of Large Language Model as World Model <https://arxiv.org/abs/2406.13945>`__ CityBench:大型语言模型作为世界模型的能力评估

::

    Thu, 20 Jun 2024 02:25:07 GMT
    Jie Feng, Jun Zhang, Junbo Yan, Xin Zhang, Tianjian Ouyang, Tianhui Liu, Yuwei Du, Siqi Guo, Yong Li

Large language models (LLMs) with powerful generalization ability has been widely used in many domains. A systematic and reliable evaluation of LLMs is a crucial step in their development and applications, especially for specific professional fields. In the urban domain, there have been some early explorations about the usability of LLMs, but a systematic and scalable evaluation benchmark is still lacking. The challenge in constructing a systematic evaluation benchmark for the urban domain lies in the diversity of data and scenarios, as well as the complex and dynamic nature of cities. In this paper, we propose CityBench, an interactive simulator based evaluation platform, as the first systematic evaluation benchmark for the capability of LLMs for urban domain. First, we build CitySim to integrate the multi-source data and simulate fine-grained urban dynamics. Based on CitySim, we design 7 tasks in 2 categories of perception-understanding and decision-making group to evaluate the capability of LLMs as city-scale world model for urban domain. Due to the flexibility and ease-of-use of CitySim, our evaluation platform CityBench can be easily extended to any city in the world. We evaluate 13 well-known LLMs including open source LLMs and commercial LLMs in 13 cities around the world. Extensive experiments demonstrate the scalability and effectiveness of proposed CityBench and shed lights for the future development of LLMs in urban domain. The dataset, benchmark and source codes are openly accessible to the research community via https://github.com/tsinghua-fib-lab/CityBench

------------

`[2406.13947] AspirinSum: an Aspect-based utility-preserved de-identification Summarization framework <https://arxiv.org/abs/2406.13947>`__ AspirinSum:一种基于方面保持效用的去识别摘要框架

::

    Thu, 20 Jun 2024 02:29:46 GMT
    Ya-Lun Li

Due to the rapid advancement of Large Language Model (LLM), the whole community eagerly consumes any available text data in order to train the LLM.
Currently, large portion of the available text data are collected from internet, which has been thought as a cheap source of the training data.
However, when people try to extend the LLM's capability to the personal related domain, such as healthcare or education, the lack of public dataset in these domains make the adaption of the LLM in such domains much slower. The reason of lacking public available dataset in such domains is because they usually contain personal sensitive information. In order to comply with privacy law, the data in such domains need to be de-identified before any kind of dissemination. It had been much research tried to address this problem for the image or tabular data. However, there was limited research on the efficient and general de-identification method for text data. Most of the method based on human annotation or predefined category list. It usually can not be easily adapted to specific domains. The goal of this proposal is to develop a text de-identification framework, which can be easily adapted to the specific domain, leverage the existing expert knowledge without further human annotation. We propose an aspect-based utility-preserved de-identification summarization framework, AspirinSum, by learning to align expert's aspect from existing comment data, it can efficiently summarize the personal sensitive document by extracting personal sensitive aspect related sub-sentence and de-identify it by substituting it with similar aspect sub-sentence. We envision that the de-identified text can then be used in data publishing, eventually publishing our de-identified dataset for downstream task use.

------------

`[2406.13948] CityGPT: Empowering Urban Spatial Cognition of Large Language Models <https://arxiv.org/abs/2406.13948>`__ CityGPT:面向大型语言模型的城市空间认知赋能

::

    Thu, 20 Jun 2024 02:32:16 GMT
    Jie Feng, Yuwei Du, Tianhui Liu, Siqi Guo, Yuming Lin, Yong Li

Large language models(LLMs) with powerful language generation and reasoning capabilities have already achieved success in many domains, e.g., math and code generation. However, due to the lacking of physical world's corpus and knowledge during training, they usually fail to solve many real-life tasks in the urban space. In this paper, we propose CityGPT, a systematic framework for enhancing the capability of LLMs on understanding urban space and solving the related urban tasks by building a city-scale world model in the model. First, we construct a diverse instruction tuning dataset CityInstruction for injecting urban knowledge and enhancing spatial reasoning capability effectively. By using a mixture of CityInstruction and general instruction data, we fine-tune various LLMs (e.g., ChatGLM3-6B, Qwen1.5 and LLama3 series) to enhance their capability without sacrificing general abilities. To further validate the effectiveness of proposed methods, we construct a comprehensive benchmark CityEval to evaluate the capability of LLMs on diverse urban scenarios and problems. Extensive evaluation results demonstrate that small LLMs trained with CityInstruction can achieve competitive performance with commercial LLMs in the comprehensive evaluation of CityEval. The source codes are openly accessible to the research community via https://github.com/tsinghua-fib-lab/CityGPT.

------------

`[2406.14039] CryptoGPT: a 7B model rivaling GPT-4 in the task of analyzing and classifying real-time financial news <https://arxiv.org/abs/2406.14039>`__ CryptoGPT:在分析和分类实时金融新闻任务上与GPT-4匹敌的7B模型

::

    Thu, 20 Jun 2024 06:59:46 GMT
    Ying Zhang, Matthieu Petit Guillaume (BH), Aur\'elien Krauth (ON), Manel Labidi

CryptoGPT: a 7B model competing with GPT-4 in a specific task -- The Impact of Automatic Annotation and Strategic Fine-Tuning via QLoRAIn this article, we present a method aimed at refining a dedicated LLM of reasonable quality with limited resources in an industrial setting via CryptoGPT. It is an LLM designed for financial news analysis for the cryptocurrency market in real-time. This project was launched in an industrial context. This model allows not only for the classification of financial information but also for providing comprehensive analysis. We refined different LLMs of the same size such as Mistral-7B and LLama-7B using semi-automatic annotation and compared them with various LLMs such as GPT-3.5 and GPT-4. Our goal is to find a balance among several needs: 1. Protecting data (by avoiding their transfer to external servers), 2. Limiting annotation cost and time, 3. Controlling the model's size (to manage deployment costs), and 4. Maintaining better analysis quality.

------------

`[2406.14124] Measuring Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective <https://arxiv.org/abs/2406.14124>`__ 

::

    Thu, 20 Jun 2024 09:09:34 GMT
    Minsang Kim, Seungjun Baek

Compute-efficient training of large language models (LLMs) has become an important research problem. In this work, we consider data pruning as a method of data-efficient training of LLMs, where we take a data compression view on data pruning. We argue that the amount of information of a sample, or the achievable compression on its description length, represents its sample importance. The key idea is that, less informative samples are likely to contain redundant information, and thus should be pruned first. We leverage log-likelihood function of trained models as a surrogate to measure information content of samples. Experiments reveal a surprising insight that information-based pruning can enhance the generalization capability of the model, improves upon language modeling and downstream tasks as compared to the model trained on the entire dataset.

------------

`[2406.14171] Ranking LLMs by compression <https://arxiv.org/abs/2406.14171>`__ 按压缩对llm进行排名

::

    Thu, 20 Jun 2024 10:23:38 GMT
    Peijia Guo, Ziguang Li, Haibo Hu, Chao Huang, Ming Li, Rui Zhang

We conceptualize the process of understanding as information compression, and propose a method for ranking large language models (LLMs) based on lossless data compression. We demonstrate the equivalence of compression length under arithmetic coding with cumulative negative log probabilities when using a large language model as a prior, that is, the pre-training phase of the model is essentially the process of learning the optimal coding length. At the same time, the evaluation metric compression ratio can be obtained without actual compression, which greatly saves overhead. In this paper, we use five large language models as priors for compression, then compare their performance on challenging natural language processing tasks, including sentence completion, question answering, and coreference resolution. Experimental results show that compression ratio and model performance are positively correlated, so it can be used as a general metric to evaluate large language models.

------------

`[2406.14319] LiveMind: Low-latency Large Language Models with Simultaneous Inference <https://arxiv.org/abs/2406.14319>`__ LiveMind:具有同步推理的低延迟大型语言模型

::

    Thu, 20 Jun 2024 13:52:30 GMT
    Chuangtao Chen and Grace Li Zhang and Xunzhao Yin and Cheng Zhuo and Ulf Schlichtmann and Bing Li

In this paper, we introduce a novel low-latency inference framework for large language models (LLMs) inference which enables LLMs to perform inferences with incomplete prompts. By reallocating computational processes to prompt input phase, we achieve a substantial reduction in latency, thereby significantly enhancing the interactive experience for users of LLMs. The framework adeptly manages the visibility of the streaming prompt to the model, allowing it to infer from incomplete prompts or await additional prompts. Compared with traditional inference methods that utilize complete prompts, our approach demonstrates an average reduction of 59% in response latency on the MMLU-Pro dataset, while maintaining comparable accuracy. Additionally, our framework facilitates collaborative inference and output across different models. By employing an LLM for inference and a small language model (SLM) for output, we achieve an average 68% reduction in response latency, alongside a 5.5% improvement in accuracy on the MMLU-Pro dataset compared with the SLM baseline.
For long prompts exceeding 20 sentences, the response latency can be reduced by up to 93%.

------------

`[2406.14343] iWISDM: Assessing instruction following in multimodal models at scale <https://arxiv.org/abs/2406.14343>`__ iWISDM:评估大规模多模态模型的指令跟随

::

    Thu, 20 Jun 2024 14:09:54 GMT
    Xiaoxuan Lei and Lucas Gomez and Hao Yuan Bai and Pouya Bashivan

The ability to perform complex tasks from detailed instructions is a key to many remarkable achievements of our species. As humans, we are not only capable of performing a wide variety of tasks but also very complex ones that may entail hundreds or thousands of steps to complete. Large language models and their more recent multimodal counterparts that integrate textual and visual inputs have achieved unprecedented success in performing complex tasks. Yet, most existing benchmarks are largely confined to single-modality inputs (either text or vision), narrowing the scope of multimodal assessments, particularly for instruction-following in multimodal contexts. To bridge this gap, we introduce the instructed-Virtual VISual Decision Making (iWISDM) environment engineered to generate a limitless array of vision-language tasks of varying complexity. Using iWISDM, we compiled three distinct benchmarks of instruction following visual tasks across varying complexity levels and evaluated several newly developed multimodal models on these benchmarks. Our findings establish iWISDM as a robust benchmark for assessing the instructional adherence of both existing and emergent multimodal models and highlight a large gap between these models' ability to precisely follow instructions with that of humans.

------------

`[2406.14408] FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving <https://arxiv.org/abs/2406.14408>`__ FVEL:基于定理证明的大型语言模型交互式形式化验证环境

::

    Thu, 20 Jun 2024 15:31:05 GMT
    Xiaohan Lin, Qingxing Cao, Yinya Huang, Haiming Wang, Jianqiao Lu, Zhengying Liu, Linqi Song, Xiaodan Liang

Formal verification (FV) has witnessed growing significance with current emerging program synthesis by the evolving large language models (LLMs).
However, current formal verification mainly resorts to symbolic verifiers or hand-craft rules, resulting in limitations for extensive and flexible verification. On the other hand, formal languages for automated theorem proving, such as Isabelle, as another line of rigorous verification, are maintained with comprehensive rules and theorems. In this paper, we propose FVEL, an interactive Formal Verification Environment with LLMs. Specifically, FVEL transforms a given code to be verified into Isabelle, and then conducts verification via neural automated theorem proving with an LLM. The joined paradigm leverages the rigorous yet abundant formulated and organized rules in Isabelle and is also convenient for introducing and adjusting cutting-edge LLMs. To achieve this goal, we extract a large-scale FVELER3. The FVELER dataset includes code dependencies and verification processes that are formulated in Isabelle, containing 758 theories, 29,125 lemmas, and 200,646 proof steps in total with in-depth dependencies. We benchmark FVELER in the FVEL environment by first fine-tuning LLMs with FVELER and then evaluating them on Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned Llama3- 8B solves 17.39% (69 -> 81) more problems, and Mistral-7B 12% (75 -> 84) more problems in SV-COMP. And the proportion of proof errors is reduced.
Project page: https://fveler.github.io/.

------------

`[2406.14449] APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking <https://arxiv.org/abs/2406.14449>`__ 

::

    Thu, 20 Jun 2024 16:11:45 GMT
    Can Jin, Hongwu Peng, Shiyu Zhao, Zhenting Wang, Wujiang Xu, Ligong Han, Jiahui Zhao, Kai Zhong, Sanguthevar Rajasekaran, Dimitris N. Metaxas

Large Language Models (LLMs) have significantly enhanced Information Retrieval (IR) across various modules, such as reranking. Despite impressive performance, current zero-shot relevance ranking with LLMs heavily relies on human prompt engineering. Existing automatic prompt engineering algorithms primarily focus on language modeling and classification tasks, leaving the domain of IR, particularly reranking, underexplored. Directly applying current prompt engineering algorithms to relevance ranking is challenging due to the integration of query and long passage pairs in the input, where the ranking complexity surpasses classification tasks. To reduce human effort and unlock the potential of prompt optimization in reranking, we introduce a novel automatic prompt engineering algorithm named APEER. APEER iteratively generates refined prompts through feedback and preference optimization. Extensive experiments with four LLMs and ten datasets demonstrate the substantial performance improvement of APEER over existing state-of-the-art (SoTA) manual prompts. Furthermore, we find that the prompts generated by APEER exhibit better transferability across diverse tasks and LLMs. Code is available at https://github.com/jincan333/APEER.

------------

`[2406.12975] SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation <https://arxiv.org/abs/2406.12975>`__ SHIELD: LLM文本生成中的版权遵从性评估与防御策略

::

    Tue, 18 Jun 2024 18:00:03 GMT
    Xiaoze Liu, Ting Sun, Tianyang Xu, Feijie Wu, Cunxiang Wang, Xiaoqian Wang, Jing Gao

Large Language Models (LLMs) have transformed machine learning but raised significant legal concerns due to their potential to produce text that infringes on copyrights, resulting in several high-profile lawsuits. The legal landscape is struggling to keep pace with these rapid advancements, with ongoing debates about whether generated text might plagiarize copyrighted materials. Current LLMs may infringe on copyrights or overly restrict non-copyrighted texts, leading to these challenges: (i) the need for a comprehensive evaluation benchmark to assess copyright compliance from multiple aspects; (ii) evaluating robustness against safeguard bypassing attacks; and (iii) developing effective defenses targeted against the generation of copyrighted text. To tackle these challenges, we introduce a curated dataset to evaluate methods, test attack strategies, and propose lightweight, real-time defenses to prevent the generation of copyrighted text, ensuring the safe and lawful use of LLMs. Our experiments demonstrate that current LLMs frequently output copyrighted text, and that jailbreaking attacks can significantly increase the volume of copyrighted output. Our proposed defense mechanisms significantly reduce the volume of copyrighted text generated by LLMs by effectively refusing malicious requests. Code is publicly available at https://github.com/xz-liu/SHIELD

------------

`[2406.13009] Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors <https://arxiv.org/abs/2406.13009>`__ 通过集成提示检测错误(DEEP):用于检测事实性错误的端到端LLM框架

::

    Tue, 18 Jun 2024 18:59:37 GMT
    Alex Chandler, Devesh Surve, Hui Su

Accurate text summarization is one of the most common and important tasks performed by Large Language Models, where the costs of human review for an entire document may be high, but the costs of errors in summarization may be even greater. We propose Detecting Errors through Ensembling Prompts (DEEP) - an end-to-end large language model framework for detecting factual errors in text summarization. Our framework uses a diverse set of LLM prompts to identify factual inconsistencies, treating their outputs as binary features, which are then fed into ensembling models. We then calibrate the ensembled models to produce empirically accurate probabilities that a text is factually consistent or free of hallucination. We demonstrate that prior models for detecting factual errors in summaries perform significantly worse without optimizing the thresholds on subsets of the evaluated dataset. Our framework achieves state-of-the-art (SOTA) balanced accuracy on the AggreFact-XSUM FTSOTA, TofuEval Summary-Level, and HaluEval Summarization benchmarks in detecting factual errors within transformer-generated text summaries. It does so without any fine-tuning of the language model or reliance on thresholding techniques not available in practical settings.

------------

`[2406.13114] Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation <https://arxiv.org/abs/2406.13114>`__ 多阶段平衡蒸馏:解决序列级知识蒸馏中的长尾挑战

::

    Wed, 19 Jun 2024 00:01:14 GMT
    Yuhang Zhou, Jing Zhu, Paiheng Xu, Xiaoyu Liu, Xiyao Wang, Danai Koutra, Wei Ai, Furong Huang

Large language models (LLMs) have significantly advanced various natural language processing tasks, but deploying them remains computationally expensive. Knowledge distillation (KD) is a promising solution, enabling the transfer of capabilities from larger teacher LLMs to more compact student models. Particularly, sequence-level KD, which distills rationale-based reasoning processes instead of merely final outcomes, shows great potential in enhancing students' reasoning capabilities. However, current methods struggle with sequence level KD under long-tailed data distributions, adversely affecting generalization on sparsely represented domains. We introduce the Multi-Stage Balanced Distillation (BalDistill) framework, which iteratively balances training data within a fixed computational budget. By dynamically selecting representative head domain examples and synthesizing tail domain examples, BalDistill achieves state-of-the-art performance across diverse long-tailed datasets, enhancing both the efficiency and efficacy of the distilled models.

------------

`[2406.13124] Learning to Generate Answers with Citations via Factual Consistency Models <https://arxiv.org/abs/2406.13124>`__ 基于事实一致性模型的引用生成答案学习

::

    Wed, 19 Jun 2024 00:40:19 GMT
    Rami Aly, Zhiqiang Tang, Samson Tan, George Karypis

Large Language Models (LLMs) frequently hallucinate, impeding their reliability in mission-critical situations. One approach to address this issue is to provide citations to relevant sources alongside generated content, enhancing the verifiability of generations. However, citing passages accurately in answers remains a substantial challenge. This paper proposes a weakly-supervised fine-tuning method leveraging factual consistency models (FCMs). Our approach alternates between generating texts with citations and supervised fine-tuning with FCM-filtered citation data. Focused learning is integrated into the objective, directing the fine-tuning process to emphasise the factual unit tokens, as measured by an FCM. Results on the ALCE few-shot citation benchmark with various instruction-tuned LLMs demonstrate superior performance compared to in-context learning, vanilla supervised fine-tuning, and state-of-the-art methods, with an average improvement of $34.1$, $15.5$, and $10.5$ citation F$_1$ points, respectively. Moreover, in a domain transfer setting we show that the obtained citation generation ability robustly transfers to unseen datasets. Notably, our citation improvements contribute to the lowest factual error rate across baselines.

------------

`[2406.13131] When Parts are Greater Than Sums: Individual LLM Components Can Outperform Full Models <https://arxiv.org/abs/2406.13131>`__ 

::

    Wed, 19 Jun 2024 00:48:44 GMT
    Ting-Yun Chang, Jesse Thomason, Robin Jia

This paper studies in-context learning (ICL) by decomposing the output of large language models into the individual contributions of attention heads and MLPs (components). We observe curious components: good-performing ones that individually do well on a classification task, even when the model performs poorly; bad-performing ones that do much worse than chance; and label-biased components that always predict the same label. We find that component accuracies are well-correlated across different demonstration sets and perturbations of prompt templates, even when the full-model accuracy varies greatly. Based on our findings, we propose component reweighting, which learns to linearly re-scale the component activations from a few labeled examples.
Given 24 labeled examples, our method improves by an average of 6.0% accuracy points over 24-shot ICL across 8 tasks on Llama-2-7B. Overall, this paper both enriches our understanding of ICL and provides a practical method for improvement by examining model internals.

------------

`[2406.13138] Large Language Models are Biased Because They Are Large Language Models <https://arxiv.org/abs/2406.13138>`__ 大型语言模型是有偏见的，因为它们是大型语言模型

::

    Wed, 19 Jun 2024 01:08:03 GMT
    Philip Resnik

This paper's primary goal is to provoke thoughtful discussion about the relationship between bias and fundamental properties of large language models.
We do this by seeking to convince the reader that harmful biases are an inevitable consequence arising from the design of any large language model as LLMs are currently formulated. To the extent that this is true, it suggests that the problem of harmful bias cannot be properly addressed without a serious reconsideration of AI driven by LLMs, going back to the foundational assumptions underlying their design.

------------

`[2406.13152] Analyzing Diversity in Healthcare LLM Research: A Scientometric Perspective <https://arxiv.org/abs/2406.13152>`__ 医疗保健法学硕士研究的多样性分析:科学计量学视角

::

    Wed, 19 Jun 2024 02:00:51 GMT
    David Restrepo, Chenwei Wu, Constanza V\'asquez-Venegas, Jo\~ao Matos, Jack Gallifant, Luis Filipe

The deployment of large language models (LLMs) in healthcare has demonstrated substantial potential for enhancing clinical decision-making, administrative efficiency, and patient outcomes. However, the underrepresentation of diverse groups in the development and application of these models can perpetuate biases, leading to inequitable healthcare delivery. This paper presents a comprehensive scientometric analysis of LLM research for healthcare, including data from January 1, 2021, to June 16, 2024. By analyzing metadata from PubMed and Dimensions, including author affiliations, countries, and funding sources, we assess the diversity of contributors to LLM research. Our findings highlight significant gender and geographic disparities, with a predominance of male authors and contributions primarily from high-income countries (HICs). We introduce a novel journal diversity index based on Gini impurity to measure the inclusiveness of scientific publications. Our results underscore the necessity for greater representation in order to ensure the equitable application of LLMs in healthcare. We propose actionable strategies to enhance diversity and inclusivity in artificial intelligence research, with the ultimate goal of fostering a more inclusive and equitable future in healthcare innovation.

------------

`[2406.13167] QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism <https://arxiv.org/abs/2406.13167>`__ QRMeM:通过先问后反射机制解除长度限制

::

    Wed, 19 Jun 2024 02:46:18 GMT
    Bo Wang, Heyan Huang, Yixin Cao, Jiahao Ying, Wei Tang, Chong Feng

While large language models (LLMs) have made notable advancements in natural language processing, they continue to struggle with processing extensive text.
Memory mechanism offers a flexible solution for managing long contexts, utilizing techniques such as compression, summarization, and structuring to facilitate nuanced and efficient handling of large volumes of text. However, existing techniques face challenges with static knowledge integration, leading to insufficient adaptation to task-specific needs and missing multi-segmentation relationships, which hinders the dynamic reorganization and logical combination of relevant segments during the response process. To address these issues, we introduce a novel strategy, Question then Reflection Memory Mechanism (QRMeM), incorporating a dual-structured memory pool. This pool synergizes static textual content with structured graph guidance, fostering a reflective trial-and-error approach for navigating and identifying relevant segments. Our evaluation across multiple-choice questions (MCQ) and multi-document question answering (Multi-doc QA) benchmarks showcases QRMeM enhanced performance compared to existing approaches.

------------

`[2406.13184] Locating and Extracting Relational Concepts in Large Language Models <https://arxiv.org/abs/2406.13184>`__ 大型语言模型中关系概念的定位和抽取

::

    Wed, 19 Jun 2024 03:29:51 GMT
    Zijian Wang, Britney White, Chang Xu

Relational concepts are indeed foundational to the structure of knowledge representation, as they facilitate the association between various entity concepts, allowing us to express and comprehend complex world knowledge. By expressing relational concepts in natural language prompts, people can effortlessly interact with large language models (LLMs) and recall desired factual knowledge. However, the process of knowledge recall lacks interpretability, and representations of relational concepts within LLMs remain unknown to us. In this paper, we identify hidden states that can express entity and relational concepts through causal mediation analysis in fact recall processes. Our finding reveals that at the last token position of the input prompt, there are hidden states that solely express the causal effects of relational concepts. Based on this finding, we assume that these hidden states can be treated as relational representations and we can successfully extract them from LLMs. The experimental results demonstrate high credibility of the relational representations: they can be flexibly transplanted into other fact recall processes, and can also be used as robust entity connectors. Moreover, we also show that the relational representations exhibit significant potential for controllable fact recall through relation rewriting.

------------

`[2406.13185] Learnable In-Context Vector for Visual Question Answering <https://arxiv.org/abs/2406.13185>`__ 面向视觉问答的可学习上下文向量

::

    Wed, 19 Jun 2024 03:33:45 GMT
    Yingzhe Peng, Chenduo Hao, Xu Yang, Jiawei Peng, Xinting Hu, Xin Geng

As language models continue to scale, Large Language Models (LLMs) have exhibited emerging capabilities in In-Context Learning (ICL), enabling them to solve language tasks by prefixing a few in-context demonstrations (ICDs) as context. Inspired by these advancements, researchers have extended these techniques to develop Large Multimodal Models (LMMs) with ICL capabilities.
However, applying ICL usually faces two major challenges: 1) using more ICDs will largely increase the inference time and 2) the performance is sensitive to the selection of ICDs. These challenges are further exacerbated in LMMs due to the integration of multiple data types and the combinational complexity of multimodal ICDs. Recently, to address these challenges, some NLP studies introduce non-learnable In-Context Vectors (ICVs) which extract useful task information from ICDs into a single vector and then insert it into the LLM to help solve the corresponding task. However, although useful in simple NLP tasks, these non-learnable methods fail to handle complex multimodal tasks like Visual Question Answering (VQA). In this study, we propose \textbf{Learnable ICV} (L-ICV) to distill essential task information from demonstrations, improving ICL performance in LMMs. Experiments show that L-ICV can significantly reduce computational costs while enhancing accuracy in VQA tasks compared to traditional ICL and other non-learnable ICV methods.

------------

`[2406.13188] Synthetic Context Generation for Question Generation <https://arxiv.org/abs/2406.13188>`__ 用于问题生成的合成上下文生成

::

    Wed, 19 Jun 2024 03:37:52 GMT
    Naiming Liu, Zichao Wang, Richard Baraniuk

Despite rapid advancements in large language models (LLMs), QG remains a challenging problem due to its complicated process, open-ended nature, and the diverse settings in which question generation occurs. A common approach to address these challenges involves fine-tuning smaller, custom models using datasets containing background context, question, and answer. However, obtaining suitable domain-specific datasets with appropriate context is often more difficult than acquiring question-answer pairs. In this paper, we investigate training QG models using synthetic contexts generated by LLMs from readily available question-answer pairs. We conduct a comprehensive study to answer critical research questions related to the performance of models trained on synthetic contexts and their potential impact on QG research and applications. Our empirical results reveal: 1) contexts are essential for QG tasks, even if they are synthetic; 2) fine-tuning smaller language models has the capability of achieving better performances as compared to prompting larger language models; and 3) synthetic context and real context could achieve comparable performances. These findings highlight the effectiveness of synthetic contexts in QG and paves the way for future advancements in the field.

------------

`[2406.13229] Probing the Emergence of Cross-lingual Alignment during LLM Training <https://arxiv.org/abs/2406.13229>`__ 

::

    Wed, 19 Jun 2024 05:31:59 GMT
    Hetong Wang, Pasquale Minervini, Edoardo M. Ponti

Multilingual Large Language Models (LLMs) achieve remarkable levels of zero-shot cross-lingual transfer performance. We speculate that this is predicated on their ability to align languages without explicit supervision from parallel sentences. While representations of translationally equivalent sentences in different languages are known to be similar after convergence, however, it remains unclear how such cross-lingual alignment emerges during pre-training of LLMs. Our study leverages intrinsic probing techniques, which identify which subsets of neurons encode linguistic features, to correlate the degree of cross-lingual neuron overlap with the zero-shot cross-lingual transfer performance for a given model. In particular, we rely on checkpoints of BLOOM, a multilingual autoregressive LLM, across different training steps and model scales. We observe a high correlation between neuron overlap and downstream performance, which supports our hypothesis on the conditions leading to effective cross-lingual transfer. Interestingly, we also detect a degradation of both implicit alignment and multilingual abilities in certain phases of the pre-training process, providing new insights into the multilingual pretraining dynamics.

------------

`[2406.13232] Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models <https://arxiv.org/abs/2406.13232>`__ 走向鲁棒评估:大型语言模型时代开放域问答数据集和指标的全面分类

::

    Wed, 19 Jun 2024 05:43:02 GMT
    Akchay Srivastava, Atif Memon

Open Domain Question Answering (ODQA) within natural language processing involves building systems that answer factual questions using large-scale knowledge corpora. Recent advances stem from the confluence of several factors, such as large-scale training datasets, deep learning techniques, and the rise of large language models. High-quality datasets are used to train models on realistic scenarios and enable the evaluation of the system on potentially unseen data. Standardized metrics facilitate comparisons between different ODQA systems, allowing researchers to objectively track advancements in the field.
Our study presents a thorough examination of the current landscape of ODQA benchmarking by reviewing 52 datasets and 20 evaluation techniques across textual and multimodal modalities. We introduce a novel taxonomy for ODQA datasets that incorporates both the modality and difficulty of the question types. Additionally, we present a structured organization of ODQA evaluation metrics along with a critical analysis of their inherent trade-offs. Our study aims to empower researchers by providing a framework for the robust evaluation of modern question-answering systems. We conclude by identifying the current challenges and outlining promising avenues for future research and development.

------------

`[2406.13236] Data Contamination Can Cross Language Barriers <https://arxiv.org/abs/2406.13236>`__ 数据污染可以跨越语言障碍

::

    Wed, 19 Jun 2024 05:53:27 GMT
    Feng Yao, Yufan Zhuang, Zihao Sun, Sunan Xu, Animesh Kumar, Jingbo Shang

The opacity in developing large language models (LLMs) is raising growing concerns about the potential contamination of public benchmarks in the pre-training data. Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination. In this paper, we first present a cross-lingual form of contamination that inflates LLMs' performance while evading current detection methods, deliberately injected by overfitting LLMs on the translated versions of benchmark test sets. Then, we propose generalization-based approaches to unmask such deeply concealed contamination.
Specifically, we examine the LLM's performance change after modifying the original benchmark by replacing the false answer choices with correct ones from other questions. Contaminated models can hardly generalize to such easier situations, where the false choices can be \emph{not even wrong}, as all choices are correct in their memorization. Experimental results demonstrate that cross-lingual contamination can easily fool existing detection methods, but not ours. In addition, we discuss the potential utilization of cross-lingual contamination in interpreting LLMs' working mechanisms and in post-training LLMs for enhanced multilingual capabilities. The code and dataset we use can be obtained from \url{https://github.com/ShangDataLab/Deep-Contam}.

------------

`[2406.13282] Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective <https://arxiv.org/abs/2406.13282>`__ 理解长上下文llm的RoPE扩展:注意力视角

::

    Wed, 19 Jun 2024 07:23:33 GMT
    Meizhi Zhong, Chen Zhang, Yikun Lei, Xikai Liu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang

Enabling LLMs to handle lengthy context is currently a research hotspot. Most LLMs are built upon rotary position embedding (RoPE), a popular position encoding method. Therefore, a prominent path is to extrapolate the RoPE trained on comparably short texts to far longer texts. A heavy bunch of efforts have been dedicated to boosting the extrapolation via extending the formulations of the RoPE, however, few of them have attempted to showcase their inner workings comprehensively. In this paper, we are driven to offer a straightforward yet in-depth understanding of RoPE extensions from an attention perspective and on two benchmarking tasks. A broad array of experiments reveals several valuable findings: 1) Maintaining attention patterns to those at the pretrained length improves extrapolation; 2) Large attention uncertainty leads to retrieval errors; 3) Using longer continual pretraining lengths for RoPE extensions could reduce attention uncertainty and significantly enhance extrapolation.

------------

`[2406.13331] Improving Zero-shot LLM Re-Ranker with Risk Minimization <https://arxiv.org/abs/2406.13331>`__ 基于风险最小化改进零样本LLM重排序器

::

    Wed, 19 Jun 2024 08:29:54 GMT
    Xiaowei Yuan, Zhao Yang, Yequan Wang, Jun Zhao, Kang Liu

In the Retrieval-Augmented Generation (RAG) system, advanced Large Language Models (LLMs) have emerged as effective Query Likelihood Models (QLMs) in an unsupervised way, which re-rank documents based on the probability of generating the query given the content of a document. However, directly prompting LLMs to approximate QLMs inherently is biased, where the estimated distribution might diverge from the actual document-specific distribution. In this study, we introduce a novel framework, $\mathrm{UR^3}$, which leverages Bayesian decision theory to both quantify and mitigate this estimation bias.
Specifically, $\mathrm{UR^3}$ reformulates the problem as maximizing the probability of document generation, thereby harmonizing the optimization of query and document generation probabilities under a unified risk minimization objective. Our empirical results indicate that $\mathrm{UR^3}$ significantly enhances re-ranking, particularly in improving the Top-1 accuracy. It benefits the QA tasks by achieving higher accuracy with fewer input documents.

------------

`[2406.13342] ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models <https://arxiv.org/abs/2406.13342>`__ 

::

    Wed, 19 Jun 2024 08:48:05 GMT
    Hwiyeol Jo, Hyunwoo Lee, Taiwoo Park

The recent advancements in large language models (LLMs) have brought significant progress in solving NLP tasks. Notably, in-context learning (ICL) is the key enabling mechanism for LLMs to understand specific tasks and grasping nuances. In this paper, we propose a simple yet effective method to contextualize a task toward a specific LLM, by (1) observing how a given LLM describes (all or a part of) target datasets, i.e., open-ended zero-shot inference, and (2) aggregating the open-ended inference results by the LLM, and (3) finally incorporate the aggregated meta-information for the actual task. We show the effectiveness of this approach in text clustering tasks, and also highlight the importance of the contextualization through examples of the above procedure.

------------

`[2406.13357] Transferable speech-to-text large language model alignment module <https://arxiv.org/abs/2406.13357>`__ 可迁移语音-文本大型语言模型对齐模块

::

    Wed, 19 Jun 2024 09:04:43 GMT
    Boyong Wu, Chao Yan, Haoran Pu

By leveraging the power of Large Language Models(LLMs) and speech foundation models, state of the art speech-text bimodal works can achieve challenging tasks like spoken translation(ST) and question answering(SQA) altogether with much simpler architectures. In this paper, we utilize the capability of Whisper encoder and pre-trained Yi-6B. Empirical results reveal that modal alignment can be achieved with one layer module and hundred hours of speech-text multitask corpus. We further swap the Yi-6B with human preferences aligned version of Yi-6B-Chat during inference, and discover that the alignment capability is applicable as well. In addition, the alignment subspace revealed by singular value decomposition(SVD) also implies linear alignment subspace is sparse, which leaves the possibility to concatenate other features like voice-print or video to expand modality.

------------

`[2406.13375] ALiiCE: Evaluating Positional Fine-grained Citation Generation <https://arxiv.org/abs/2406.13375>`__ alice:位置细粒度引文生成评估

::

    Wed, 19 Jun 2024 09:16:14 GMT
    Yilong Xu, Jinhua Gao, Xiaoming Yu, Baolong Bi, Huawei Shen, Xueqi Cheng

Large Language Models (LLMs) can enhance the credibility and verifiability by generating text with citations. However, existing tasks and evaluation methods are predominantly limited to sentence-level statement, neglecting the significance of positional fine-grained citations that can appear anywhere within sentences. To facilitate further exploration of the fine-grained citation generation, we propose ALiiCE, the first automatic evaluation framework for this task. Our framework first parses the sentence claim into atomic claims via dependency analysis and then calculates citation quality at the atomic claim level. ALiiCE introduces three novel metrics for positional fined-grained citation quality assessment, including positional fine-grained citation recall and precision, and coefficient of variation of citation positions. We evaluate the positional fine-grained citation generation performance of several LLMs on two long-form QA datasets. Our experiments and analyses demonstrate the effectiveness and reasonableness of ALiiCE. The results also indicate that existing LLMs still struggle to provide positional fine-grained citations.

------------

`[2406.13415] Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators <https://arxiv.org/abs/2406.13415>`__ llm的事实置信度:当前估计器的可靠性和鲁棒性

::

    Wed, 19 Jun 2024 10:11:37 GMT
    Mat\'eo Mahaut, Laura Aina, Paula Czarnowska, Momchil Hardalov, Thomas M\"uller, Llu\'is M\`arquez

Large Language Models (LLMs) tend to be unreliable in the factuality of their answers. To address this problem, NLP researchers have proposed a range of techniques to estimate LLM's confidence over facts. However, due to the lack of a systematic comparison, it is not clear how the different methods compare to one another. To fill this gap, we present a survey and empirical comparison of estimators of factual confidence. We define an experimental framework allowing for fair comparison, covering both fact-verification and question answering.
Our experiments across a series of LLMs indicate that trained hidden-state probes provide the most reliable confidence estimates, albeit at the expense of requiring access to weights and training data. We also conduct a deeper assessment of factual confidence by measuring the consistency of model behavior under meaning-preserving variations in the input. We find that the confidence of LLMs is often unstable across semantically equivalent inputs, suggesting that there is much room for improvement of the stability of models' parametric knowledge. Our code is available at (https://github.com/amazon-science/factual-confidence-of-llms).

------------

`[2406.13439] Finding Blind Spots in Evaluator LLMs with Interpretable Checklists <https://arxiv.org/abs/2406.13439>`__ 使用可解释的清单在评估器llm中寻找盲点

::

    Wed, 19 Jun 2024 10:59:48 GMT
    Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Sshubam Verma, Mitesh M. Khapra

Large Language Models (LLMs) are increasingly relied upon to evaluate text outputs of other LLMs, thereby influencing leaderboards and development decisions. However, concerns persist over the accuracy of these assessments and the potential for misleading conclusions. In this work, we investigate the effectiveness of LLMs as evaluators for text generation tasks. We propose FBI, a novel framework designed to examine the proficiency of Evaluator LLMs in assessing four critical abilities in other LLMs: factual accuracy, instruction following, coherence in long-form writing, and reasoning proficiency. By introducing targeted perturbations in answers generated by LLMs, that clearly impact one of these key capabilities, we test whether an Evaluator LLM can detect these quality drops. By creating a total of 2400 perturbed answers covering 22 perturbation categories, we conduct a comprehensive study using different evaluation strategies on five prominent LLMs commonly used as evaluators in the literature. Our findings reveal significant shortcomings in current Evaluator LLMs, which failed to identify quality drops in over 50\% of cases on average. Single-answer and pairwise evaluations demonstrated notable limitations, whereas reference-based evaluations showed comparatively better performance. These results underscore the unreliable nature of current Evaluator LLMs and advocate for cautious implementation in practical applications. Code and data are available at https://github.com/AI4Bharat/FBI.

------------

`[2406.13444] VDebugger: Harnessing Execution Feedback for Debugging Visual Programs <https://arxiv.org/abs/2406.13444>`__ 

::

    Wed, 19 Jun 2024 11:09:16 GMT
    Xueqing Wu, Zongyu Lin, Songyan Zhao, Te-Lin Wu, Pan Lu, Nanyun Peng, Kai-Wei Chang

Visual programs are executable code generated by large language models to address visual reasoning problems. They decompose complex questions into multiple reasoning steps and invoke specialized models for each step to solve the problems. However, these programs are prone to logic errors, with our preliminary evaluation showing that 58% of the total errors are caused by program logic errors. Debugging complex visual programs remains a major bottleneck for visual reasoning. To address this, we introduce VDebugger, a novel critic-refiner framework trained to localize and debug visual programs by tracking execution step by step. VDebugger identifies and corrects program errors leveraging detailed execution feedback, improving interpretability and accuracy. The training data is generated through an automated pipeline that injects errors into correct visual programs using a novel mask-best decoding technique. Evaluations on six datasets demonstrate VDebugger's effectiveness, showing performance improvements of up to 3.2% in downstream task accuracy.
Further studies show VDebugger's ability to generalize to unseen tasks, bringing a notable improvement of 2.3% on the unseen COVR task. Code, data and models are made publicly available at https://github.com/shirley-wu/vdebugger/

------------

`[2406.13476] LLMs Are Zero-Shot Context-Aware Simultaneous Translators <https://arxiv.org/abs/2406.13476>`__ llm是零样本的上下文感知同声传译

::

    Wed, 19 Jun 2024 11:57:42 GMT
    Roman Koshkin, Katsuhito Sudoh, Satoshi Nakamura

The advent of transformers has fueled progress in machine translation. More recently large language models (LLMs) have come to the spotlight thanks to their generality and strong performance in a wide range of language tasks, including translation. Here we show that open-source LLMs perform on par with or better than some state-of-the-art baselines in simultaneous machine translation (SiMT) tasks, zero-shot. We also demonstrate that injection of minimal background information, which is easy with an LLM, brings further performance gains, especially on challenging technical subject-matter. This highlights LLMs' potential for building next generation of massively multilingual, context-aware and terminologically accurate SiMT systems that require no resource-intensive training or fine-tuning.

------------

`[2406.13542] Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models <https://arxiv.org/abs/2406.13542>`__ 基于执行反馈的自玩:提高大型语言模型的指令遵循能力

::

    Wed, 19 Jun 2024 13:29:53 GMT
    Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, Jingren Zhou

One core capability of large language models (LLMs) is to follow natural language instructions. However, the issue of automatically constructing high-quality training data to enhance the complex instruction-following abilities of LLMs without manual annotation remains unresolved. In this paper, we introduce AutoIF, the first scalable and reliable method for automatically generating instruction-following training data. AutoIF transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions, the corresponding code to check the correctness of the instruction responses, and unit test samples to verify the code's correctness. Then, execution feedback-based rejection sampling can generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training. AutoIF achieves significant improvements across three training algorithms, SFT, Offline DPO, and Online DPO, when applied to the top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and strong-to-weak distillation settings. Our code is publicly available at https://github.com/QwenLM/AutoIF.

------------

`[2406.13555] BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation <https://arxiv.org/abs/2406.13555>`__ BiLD:大型语言模型蒸馏的双向Logits差分损失

::

    Wed, 19 Jun 2024 13:44:56 GMT
    Minchong Li, Feng Zhou, Xiaohui Song

In recent years, large language models (LLMs) have shown exceptional capabilities across various natural language processing (NLP) tasks. However, such impressive performance often comes with the trade-off of an increased parameter size, posing significant challenges for widespread deployment.
Knowledge distillation (KD) provides a solution by transferring knowledge from a large teacher model to a smaller student model. In this paper, we explore the task-specific distillation of LLMs at the logit level. Our investigation reveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail distribution than those from vision models, with hidden "noise" in the long tail affecting distillation performance. Furthermore, existing logits distillation methods often struggle to effectively utilize the internal ranking information from the logits. To address these, we propose the Bi-directional Logits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by utilizing only top-$k$ teacher and student logits, and leverages the internal logits ranking information by constructing logits differences. To evaluate BiLD loss, we conduct comprehensive experiments on 13 datasets using two types of LLMs. Our results show that the BiLD loss, with only the top-8 logits, outperforms supervised fine-tuning (SFT), vanilla KL loss, and five other distillation methods from both NLP and CV fields.

------------

`[2406.13617] Optimizing Psychological Counseling with Instruction-Tuned Large Language Models <https://arxiv.org/abs/2406.13617>`__ 基于指令调优的大型语言模型优化心理咨询

::

    Wed, 19 Jun 2024 15:13:07 GMT
    Wenjie Li, Tianyu Sun, Kun Qian, Wenhong Wang

The advent of large language models (LLMs) has significantly advanced various fields, including natural language processing and automated dialogue systems.
This paper explores the application of LLMs in psychological counseling, addressing the increasing demand for mental health services. We present a method for instruction tuning LLMs with specialized prompts to enhance their performance in providing empathetic, relevant, and supportive responses. Our approach involves developing a comprehensive dataset of counseling-specific prompts, refining them through feedback from professional counselors, and conducting rigorous evaluations using both automatic metrics and human assessments. The results demonstrate that our instruction-tuned model outperforms several baseline LLMs, highlighting its potential as a scalable and accessible tool for mental health support.

------------

`[2406.13618] In-Context Former: Lightning-fast Compressing Context for Large Language Model <https://arxiv.org/abs/2406.13618>`__ 上下文Former:快速压缩大型语言模型上下文

::

    Wed, 19 Jun 2024 15:14:55 GMT
    Xiangfeng Wang, Zaiyi Chen, Zheyong Xie, Tong Xu, Yongyi He, Enhong Chen

With the rising popularity of Transformer-based large language models (LLMs), reducing their high inference costs has become a significant research focus.
One effective approach is to compress the long input contexts. Existing methods typically leverage the self-attention mechanism of the LLM itself for context compression. While these methods have achieved notable results, the compression process still involves quadratic time complexity, which limits their applicability. To mitigate this limitation, we propose the In-Context Former (IC-Former). Unlike previous methods, IC-Former does not depend on the target LLMs. Instead, it leverages the cross-attention mechanism and a small number of learnable digest tokens to directly condense information from the contextual word embeddings. This approach significantly reduces inference time, which achieves linear growth in time complexity within the compression range.
Experimental results indicate that our method requires only 1/32 of the floating-point operations of the baseline during compression and improves processing speed by 68 to 112 times while achieving over 90% of the baseline performance on evaluation metrics. Overall, our model effectively reduces compression costs and makes real-time compression scenarios feasible.

------------

`[2406.13621] Improving Visual Commonsense in Language Models via Multiple Image Generation <https://arxiv.org/abs/2406.13621>`__ 通过多图像生成改善语言模型中的视觉常识

::

    Wed, 19 Jun 2024 15:17:10 GMT
    Guy Yariv, Idan Schwartz, Yossi Adi, Sagie Benaim

Commonsense reasoning is fundamentally based on multimodal knowledge.
However, existing large language models (LLMs) are primarily trained using textual data only, limiting their ability to incorporate essential visual information. In contrast, Visual Language Models, which excel at visually-oriented tasks, often fail at non-visual tasks such as basic commonsense reasoning. This divergence highlights a critical challenge - the integration of robust visual understanding with foundational text-based language reasoning. To this end, we introduce a method aimed at enhancing LLMs' visual commonsense. Specifically, our method generates multiple images based on the input text prompt and integrates these into the model's decision-making process by mixing their prediction probabilities. To facilitate multimodal grounded language modeling, we employ a late-fusion layer that combines the projected visual features with the output of a pre-trained LLM conditioned on text only. This late-fusion layer enables predictions based on comprehensive image-text knowledge as well as text only when this is required. We evaluate our approach using several visual commonsense reasoning tasks together with traditional NLP tasks, including common sense reasoning and reading comprehension. Our experimental results demonstrate significant superiority over existing baselines. When applied to recent state-of-the-art LLMs (e.g., Llama3), we observe improvements not only in visual common sense but also in traditional NLP benchmarks. Code and models are available under https://github.com/guyyariv/vLMIG.

------------

`[2406.13626] Fine-Tuning Gemma-7B for Enhanced Sentiment Analysis of Financial News Headlines <https://arxiv.org/abs/2406.13626>`__ 微调Gemma-7B增强金融新闻标题情感分析

::

    Wed, 19 Jun 2024 15:20:19 GMT
    Kangtong Mo, Wenyan Liu, Xuanzhen Xu, Chang Yu, Yuelin Zou, Fangqing Xia

In this study, we explore the application of sentiment analysis on financial news headlines to understand investor sentiment. By leveraging Natural Language Processing (NLP) and Large Language Models (LLM), we analyze sentiment from the perspective of retail investors. The FinancialPhraseBank dataset, which contains categorized sentiments of financial news headlines, serves as the basis for our analysis. We fine-tuned several models, including distilbert-base-uncased, Llama, and gemma-7b, to evaluate their effectiveness in sentiment classification. Our experiments demonstrate that the fine-tuned gemma-7b model outperforms others, achieving the highest precision, recall, and F1 score. Specifically, the gemma-7b model showed significant improvements in accuracy after fine-tuning, indicating its robustness in capturing the nuances of financial sentiment. This model can be instrumental in providing market insights, risk management, and aiding investment decisions by accurately predicting the sentiment of financial news. The results highlight the potential of advanced LLMs in transforming how we analyze and interpret financial information, offering a powerful tool for stakeholders in the financial industry.

------------

`[2406.13632] Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations <https://arxiv.org/abs/2406.13632>`__ 少镜头能在长语境中发挥作用吗?循环利用上下文来生成演示

::

    Wed, 19 Jun 2024 15:28:29 GMT
    Arie Cattan, Alon Jacovi, Alex Fabrikant, Jonathan Herzig, Roee Aharoni, Hannah Rashkin, Dror Marcus, Avinatan Hassidim, Yossi Matias, Idan Szpektor, Avi Caciularu

Despite recent advancements in Large Language Models (LLMs), their performance on tasks involving long contexts remains sub-optimal. In-Context Learning (ICL) with few-shot examples may be an appealing solution to enhance LLM performance in this scenario; However, naively adding ICL examples with long context introduces challenges, including substantial token overhead added for each few-shot example and context mismatch between the demonstrations and the target query. In this work, we propose to automatically generate few-shot examples for long context QA tasks by recycling contexts. Specifically, given a long input context (1-3k tokens) and a query, we generate additional query-output pairs from the given context as few-shot examples, while introducing the context only once. This ensures that the demonstrations are leveraging the same context as the target query while only adding a small number of tokens to the prompt. We further enhance each demonstration by instructing the model to explicitly identify the relevant paragraphs before the answer, which improves performance while providing fine-grained attribution to the answer source. We apply our method on multiple LLMs and obtain substantial improvements on various QA datasets with long context, especially when the answer lies within the middle of the context. Surprisingly, despite introducing only single-hop ICL examples, LLMs also successfully generalize to multi-hop long-context QA using our approach.

------------

`[2406.13662] ObscurePrompt: Jailbreaking Large Language Models via Obscure Input <https://arxiv.org/abs/2406.13662>`__ 

::

    Wed, 19 Jun 2024 16:09:58 GMT
    Yue Huang, Jingyu Tang, Dongping Chen, Bingda Tang, Yao Wan, Lichao Sun, Xiangliang Zhang

Recently, Large Language Models (LLMs) have garnered significant attention for their exceptional natural language processing capabilities. However, concerns about their trustworthiness remain unresolved, particularly in addressing "jailbreaking" attacks on aligned LLMs. Previous research predominantly relies on scenarios with white-box LLMs or specific and fixed prompt templates, which are often impractical and lack broad applicability. In this paper, we introduce a straightforward and novel method, named ObscurePrompt, for jailbreaking LLMs, inspired by the observed fragile alignments in Out-of-Distribution (OOD) data. Specifically, we first formulate the decision boundary in the jailbreaking process and then explore how obscure text affects LLM's ethical decision boundary. ObscurePrompt starts with constructing a base prompt that integrates well-known jailbreaking techniques.
Powerful LLMs are then utilized to obscure the original prompt through iterative transformations, aiming to bolster the attack's robustness.
Comprehensive experiments show that our approach substantially improves upon previous methods in terms of attack effectiveness, maintaining efficacy against two prevalent defense mechanisms. We believe that our work can offer fresh insights for future research on enhancing LLM alignment.

------------

`[2406.13698] MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language <https://arxiv.org/abs/2406.13698>`__ MMTE:隐喻语言机器翻译质量评估的语料库和指标

::

    Wed, 19 Jun 2024 16:52:22 GMT
    Shun Wang, Ge Zhang, Han Wu, Tyler Loakman, Wenhao Huang, Chenghua Lin

Machine Translation (MT) has developed rapidly since the release of Large Language Models and current MT evaluation is performed through comparison with reference human translations or by predicting quality scores from human-labeled data. However, these mainstream evaluation methods mainly focus on fluency and factual reliability, whilst paying little attention to figurative quality. In this paper, we investigate the figurative quality of MT and propose a set of human evaluation metrics focused on the translation of figurative language. We additionally present a multilingual parallel metaphor corpus generated by post-editing. Our evaluation protocol is designed to estimate four aspects of MT: Metaphorical Equivalence, Emotion, Authenticity, and Quality. In doing so, we observe that translations of figurative expressions display different traits from literal ones.

------------

`[2406.13706] Breaking News: Case Studies of Generative AI's Use in Journalism <https://arxiv.org/abs/2406.13706>`__ 突发新闻:生成式人工智能在新闻中的应用案例研究

::

    Wed, 19 Jun 2024 16:58:32 GMT
    Natalie Grace Brigham, Chongjiu Gao, Tadayoshi Kohno, Franziska Roesner, Niloofar Mireshghallah

Journalists are among the many users of large language models (LLMs). To better understand the journalist-AI interactions, we conduct a study of LLM usage by two news agencies through browsing the WildChat dataset, identifying candidate interactions, and verifying them by matching to online published articles. Our analysis uncovers instances where journalists provide sensitive material such as confidential correspondence with sources or articles from other agencies to the LLM as stimuli and prompt it to generate articles, and publish these machine-generated articles with limited intervention (median output-publication ROUGE-L of 0.62). Based on our findings, we call for further research into what constitutes responsible use of AI, and the establishment of clear guidelines and best practices on using LLMs in a journalistic context.

------------

`[2406.13718] Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization <https://arxiv.org/abs/2406.13718>`__ 沿语言变化维度评估大型语言模型:跨语言泛化的系统研究

::

    Wed, 19 Jun 2024 17:20:28 GMT
    Niyati Bafna, Kenton Murray, David Yarowsky

While large language models exhibit certain cross-lingual generalization capabilities, they suffer from performance degradation (PD) on unseen closely-related languages (CRLs) and dialects relative to their high-resource language neighbour (HRLN). However, we currently lack a fundamental understanding of what kinds of linguistic distances contribute to PD, and to what extent. Furthermore, studies of cross-lingual generalization are confounded by unknown quantities of CRL language traces in the training data, and by the frequent lack of availability of evaluation data in lower-resource related languages and dialects. To address these issues, we model phonological, morphological, and lexical distance as Bayesian noise processes to synthesize artificial languages that are controllably distant from the HRLN. We analyse PD as a function of underlying noise parameters, offering insights on model robustness to isolated and composed linguistic phenomena, and the impact of task and HRL characteristics on PD. We calculate parameter posteriors on real CRL-HRLN pair data and show that they follow computed trends of artificial languages, demonstrating the viability of our noisers. Our framework offers a cheap solution to estimating task performance on an unseen CRL given HRLN performance using its posteriors, as well as for diagnosing observed PD on a CRL in terms of its linguistic distances from its HRLN, and opens doors to principled methods of mitigating performance degradation.

------------

`[2406.13720] On the Utility of Domain-Adjacent Fine-Tuned Model Ensembles for Few-shot Problems <https://arxiv.org/abs/2406.13720>`__ 域邻近微调模型集成在小样本问题中的应用

::

    Wed, 19 Jun 2024 17:24:36 GMT
    Md Ibrahim Ibne Alam, Parikshit Ram, Soham Dan, Horst Samulowitz, Koushik Kar

Large Language Models (LLMs) have been observed to perform well on a wide range of downstream tasks when fine-tuned on domain-specific data. However, such data may not be readily available in many applications, motivating zero-shot or few-shot approaches using domain-adjacent models. While several fine-tuned models for various tasks are available, finding an appropriate domain-adjacent model for a given task is often not straight forward. In this paper, we study DAFT-E, a framework that utilizes an Ensemble of Domain-Adjacent Fine-Tuned Foundation Models for few-shot problems. We show that for zero-shot problems, this ensembling method provides an accuracy performance close to that of the single best model. With few-shot problems, this performance improves further, at which point DEFT-E can outperform any single domain-adjacent model while requiring much less data for domain-specific fine-tuning.

------------

`[2406.13748] Every Language Counts: Learn and Unlearn in Multilingual LLMs <https://arxiv.org/abs/2406.13748>`__ 每种语言都很重要:在多语言llm中学习和忘记

::

    Wed, 19 Jun 2024 18:01:08 GMT
    Taiming Lu, Philipp Koehn

This paper investigates the propagation of harmful information in multilingual large language models (LLMs) and evaluates the efficacy of various unlearning methods. We demonstrate that fake information, regardless of the language it is in, once introduced into these models through training data, can spread across different languages, compromising the integrity and reliability of the generated content. Our findings reveal that standard unlearning techniques, which typically focus on English data, are insufficient in mitigating the spread of harmful content in multilingual contexts and could inadvertently reinforce harmful content across languages. We show that only by addressing harmful responses in both English and the original language of the harmful data can we effectively eliminate generations for all languages. This underscores the critical need for comprehensive unlearning strategies that consider the multilingual nature of modern LLMs to enhance their safety and reliability across diverse linguistic landscapes.

------------

`[2406.13764] Can LLMs Reason in the Wild with Programs? <https://arxiv.org/abs/2406.13764>`__ 

::

    Wed, 19 Jun 2024 18:26:19 GMT
    Yuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi, Faramarz Fekri

Large Language Models (LLMs) have shown superior capability to solve reasoning problems with programs. While being a promising direction, most of such frameworks are trained and evaluated in settings with a prior knowledge of task requirements. However, as LLMs become more capable, it is necessary to assess their reasoning abilities in more realistic scenarios where many real-world problems are open-ended with ambiguous scope, and often require multiple formalisms to solve. To investigate this, we introduce the task of reasoning in the wild, where an LLM is tasked to solve a reasoning problem of unknown type by identifying the subproblems and their corresponding formalisms, and writing a program to solve each subproblem, guided by a tactic. We create a large tactic-guided trajectory dataset containing detailed solutions to a diverse set of reasoning problems, ranging from well-defined single-form reasoning (e.g., math, logic), to ambiguous and hybrid ones (e.g., commonsense, combined math and logic). This allows us to test various aspects of LLMs reasoning at the fine-grained level such as the selection and execution of tactics, and the tendency to take undesired shortcuts. In experiments, we highlight that existing LLMs fail significantly on problems with ambiguous and mixed scope, revealing critical limitations and overfitting issues (e.g.
accuracy on GSM8K drops by at least 50\%). We further show the potential of finetuning a local LLM on the tactic-guided trajectories in achieving better performance. Project repo is available at github.com/gblackout/Reason-in-the-Wild

------------

`[2406.13862] Knowledge Graph-Enhanced Large Language Models via Path Selection <https://arxiv.org/abs/2406.13862>`__ 基于路径选择的知识图谱增强的大型语言模型

::

    Wed, 19 Jun 2024 21:45:20 GMT
    Haochen Liu, Song Wang, Yaochen Zhu, Yushun Dong, Jundong Li

Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems.
Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.

------------

`[2406.13892] Adaptable Logical Control for Large Language Models <https://arxiv.org/abs/2406.13892>`__ 大型语言模型的自适应逻辑控制

::

    Wed, 19 Jun 2024 23:47:59 GMT
    Honghua Zhang, Po-Nien Kung, Masahiro Yoshida, Guy Van den Broeck, Nanyun Peng

Despite the success of Large Language Models (LLMs) on various tasks following human instructions, controlling model generation at inference time poses a persistent challenge. In this paper, we introduce Ctrl-G, an adaptable framework that facilitates tractable and flexible control of LLM generation to reliably follow logical constraints. Ctrl-G combines any production-ready LLM with a Hidden Markov Model, enabling LLM outputs to adhere to logical constraints represented as deterministic finite automata. We show that Ctrl-G, when applied to a TULU2-7B model, outperforms GPT3.5 and GPT4 on the task of interactive text editing: specifically, for the task of generating text insertions/continuations following logical constraints, Ctrl-G achieves over 30% higher satisfaction rate in human evaluation compared to GPT4. When applied to medium-size language models (e.g., GPT2-large), Ctrl-G also beats its counterparts for constrained generation by large margins on standard benchmarks. Additionally, as a proof-of-concept study, we experiment Ctrl-G on the Grade School Math benchmark to assist LLM reasoning, foreshadowing the application of Ctrl-G, as well as other constrained generation approaches, beyond traditional language generation tasks.

------------

`[2406.13893] Open Generative Large Language Models for Galician <https://arxiv.org/abs/2406.13893>`__ 面向加利西亚语的开放式生成大型语言模型

::

    Wed, 19 Jun 2024 23:49:56 GMT
    Pablo Gamallo, Pablo Rodr\'iguez, Iria de-Dios-Flores, Susana Sotelo, Silvia Paniagua, Daniel Bardanca, Jos\'e Ramom Pichel and Marcos Garcia

Large language models (LLMs) have transformed natural language processing.
Yet, their predominantly English-centric training has led to biases and performance disparities across languages. This imbalance marginalizes minoritized languages, making equitable access to NLP technologies more difficult for languages with lower resources, such as Galician. We present the first two generative LLMs focused on Galician to bridge this gap. These models, freely available as open-source resources, were trained using a GPT architecture with 1.3B parameters on a corpus of 2.1B words. Leveraging continual pretraining, we adapt to Galician two existing LLMs trained on larger corpora, thus mitigating the data constraints that would arise if the training were performed from scratch. The models were evaluated using human judgments and task-based datasets from standardized benchmarks. These evaluations reveal a promising performance, underscoring the importance of linguistic diversity in generative models.

------------

`[2406.13903] Generative AI for Enhancing Active Learning in Education: A Comparative Study of GPT-3.5 and GPT-4 in Crafting Customized Test Questions <https://arxiv.org/abs/2406.13903>`__ 

::

    Thu, 20 Jun 2024 00:25:43 GMT
    Hamdireza Rouzegar, Masoud Makrehchi

This study investigates how LLMs, specifically GPT-3.5 and GPT-4, can develop tailored questions for Grade 9 math, aligning with active learning principles.
By utilizing an iterative method, these models adjust questions based on difficulty and content, responding to feedback from a simulated 'student' model. A novel aspect of the research involved using GPT-4 as a 'teacher' to create complex questions, with GPT-3.5 as the 'student' responding to these challenges. This setup mirrors active learning, promoting deeper engagement.
The findings demonstrate GPT-4's superior ability to generate precise, challenging questions and notable improvements in GPT-3.5's ability to handle more complex problems after receiving instruction from GPT-4. These results underscore the potential of LLMs to mimic and enhance active learning scenarios, offering a promising path for AI in customized education. This research contributes to understanding how AI can support personalized learning experiences, highlighting the need for further exploration in various educational contexts

------------

`[2406.13905] Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking <https://arxiv.org/abs/2406.13905>`__ 主观决策中生成的自由文本推理的说服力:成对论据排序案例研究

::

    Thu, 20 Jun 2024 00:28:33 GMT
    Mohamed Elaraby, Diane Litman, Xiang Lorraine Li, Ahmed Magooda

Generating free-text rationales is among the emergent capabilities of Large Language Models (LLMs). These rationales have been found to enhance LLM performance across various NLP tasks. Recently, there has been growing interest in using these rationales to provide insights for various important downstream tasks. In this paper, we analyze generated free-text rationales in tasks with subjective answers, emphasizing the importance of rationalization in such scenarios. We focus on pairwise argument ranking, a highly subjective task with significant potential for real-world applications, such as debate assistance.
We evaluate the persuasiveness of rationales generated by nine LLMs to support their subjective choices. Our findings suggest that open-source LLMs, particularly Llama2-70B-chat, are capable of providing highly persuasive rationalizations, surpassing even GPT models. Additionally, our experiments show that rationale persuasiveness can be improved by controlling its parameters through prompting or through self-refinement.

------------

`[2406.13925] GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models <https://arxiv.org/abs/2406.13925>`__ GenderAlign:用于减轻大型语言模型性别偏见的对齐数据集

::

    Thu, 20 Jun 2024 01:45:44 GMT
    Tao Zhang, Ziqian Zeng, Yuxiang Xiao, Huiping Zhuang, Cen Chen, James Foulds, Shimei Pan

Large Language Models (LLMs) are prone to generating content that exhibits gender biases, raising significant ethical concerns. Alignment, the process of fine-tuning LLMs to better align with desired behaviors, is recognized as an effective approach to mitigate gender biases. Although proprietary LLMs have made significant strides in mitigating gender bias, their alignment datasets are not publicly available. The commonly used and publicly available alignment dataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of publicly available alignment datasets specifically designed to address gender bias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating a comprehensive set of gender biases in LLMs. This dataset comprises 8k single-turn dialogues, each paired with a "chosen" and a "rejected" response.
Compared to the "rejected" responses, the "chosen" responses demonstrate lower levels of gender bias and higher quality. Furthermore, we categorized the gender biases in the "rejected" responses of GenderAlign into 4 principal categories. The experimental results show the effectiveness of GenderAlign in reducing gender bias in LLMs.

------------

`[2406.13929] Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination <https://arxiv.org/abs/2406.13929>`__ 大型语言模型是怀疑论者:输入冲突幻觉的假阴性问题

::

    Thu, 20 Jun 2024 01:53:25 GMT
    Jongyoon Song, Sangwon Yu, Sungroh Yoon

In this paper, we identify a new category of bias that induces input-conflicting hallucinations, where large language models (LLMs) generate responses inconsistent with the content of the input context. This issue we have termed the false negative problem refers to the phenomenon where LLMs are predisposed to return negative judgments when assessing the correctness of a statement given the context. In experiments involving pairs of statements that contain the same information but have contradictory factual directions, we observe that LLMs exhibit a bias toward false negatives. Specifically, the model presents greater overconfidence when responding with False. Furthermore, we analyze the relationship between the false negative problem and context and query rewriting and observe that both effectively tackle false negatives in LLMs.

------------

`[2406.13940] AutoCAP: Towards Automatic Cross-lingual Alignment Planning for Zero-shot Chain-of-Thought <https://arxiv.org/abs/2406.13940>`__ AutoCAP:面向零样本思维链的自动跨语言对齐规划

::

    Thu, 20 Jun 2024 02:19:33 GMT
    Yongheng Zhang, Qiguang Chen, Min Li, Wanxiang Che, Libo Qin

Cross-lingual chain-of-thought can effectively complete reasoning tasks across languages, which gains increasing attention. Recently, dominant approaches in the literature improve cross-lingual alignment capabilities by integrating reasoning knowledge from different languages. Despite achieving excellent performance, current methods still have two main challenges: (1) Manual language specification: They still highly rely on manually selecting the languages to integrate, severely affecting their generalizability; (2) Static weight allocation: Current methods simply integrate all languages equally. In fact, different language reasoning paths should have different weights to achieve better complementation and integration. Motivated by this, we introduce an Automatic Cross-lingual Alignment Planning (AutoCAP) for zero-shot chain-of-thought to address the above challenges. The core of AutoCAP consists of two components: (1) Automatic Language Selection Prompting to guide LLMs to select appropriate languages and (2) Automatic Weight Allocation Prompting to automatically allocate alignment weight scores to each reasoning path.
Extensive experiments on several benchmarks reveal that AutoCAP achieves state-of-the-art performance, surpassing previous methods that required manual effort.

------------

`[2406.13993] Exploring Changes in Nation Perception with Nationality-Assigned Personas in LLMs <https://arxiv.org/abs/2406.13993>`__ 在llm中通过国籍分配角色探索国家认知的变化

::

    Thu, 20 Jun 2024 04:44:20 GMT
    Mahammed Kamruzzaman and Gene Louis Kim

Persona assignment has become a common strategy for customizing LLM use to particular tasks and contexts. In this study, we explore how perceptions of different nations change when LLMs are assigned specific nationality personas.
We assign 193 different nationality personas (e.g., an American person) to four LLMs and examine how the LLM perceptions of countries change. We find that all LLM-persona combinations tend to favor Western European nations, though nation-personas push LLM behaviors to focus more on and view more favorably the nation-persona's own region. Eastern European, Latin American, and African nations are viewed more negatively by different nationality personas. Our study provides insight into how biases and stereotypes are realized within LLMs when adopting different national personas. In line with the "Blueprint for an AI Bill of Rights", our findings underscore the critical need for developing mechanisms to ensure LLMs uphold fairness and not over-generalize at a global scale.

------------

`[2406.13997] "Global is Good, Local is Bad?": Understanding Brand Bias in LLMs <https://arxiv.org/abs/2406.13997>`__ “全球化是好事，本地化是坏事?”:理解LLMs中的品牌偏见

::

    Thu, 20 Jun 2024 04:52:19 GMT
    Mahammed Kamruzzaman, Hieu Minh Nguyen, and Gene Louis Kim

Many recent studies have investigated social biases in LLMs but brand bias has received little attention. This research examines the biases exhibited by LLMs towards different brands, a significant concern given the widespread use of LLMs in affected use cases such as product recommendation and market analysis. Biased models may perpetuate societal inequalities, unfairly favoring established global brands while marginalizing local ones. Using a curated dataset across four brand categories, we probe the behavior of LLMs in this space. We find a consistent pattern of bias in this space -- both in terms of disproportionately associating global brands with positive attributes and disproportionately recommending luxury gifts for individuals in high-income countries. We also find LLMs are subject to country-of-origin effects which may boost local brand preference in LLM outputs in specific contexts.

------------

`[2406.14012] Seeing Through AI's Lens: Enhancing Human Skepticism Towards LLM-Generated Fake News <https://arxiv.org/abs/2406.14012>`__ 透过人工智能的镜头:增强人类对llm生成的假新闻的怀疑

::

    Thu, 20 Jun 2024 06:02:04 GMT
    Navid Ayoobi, Sadat Shahriar, Arjun Mukherjee

LLMs offer valuable capabilities, yet they can be utilized by malicious users to disseminate deceptive information and generate fake news. The growing prevalence of LLMs poses difficulties in crafting detection approaches that remain effective across various text domains. Additionally, the absence of precautionary measures for AI-generated news on online social platforms is concerning. Therefore, there is an urgent need to improve people's ability to differentiate between news articles written by humans and those produced by LLMs. By providing cues in human-written and LLM-generated news, we can help individuals increase their skepticism towards fake LLM-generated news. This paper aims to elucidate simple markers that help individuals distinguish between articles penned by humans and those created by LLMs. To achieve this, we initially collected a dataset comprising 39k news articles authored by humans or generated by four distinct LLMs with varying degrees of fake. We then devise a metric named Entropy-Shift Authorship Signature (ESAS) based on the information theory and entropy principles. The proposed ESAS ranks terms or entities, like POS tagging, within news articles based on their relevance in discerning article authorship. We demonstrate the effectiveness of our metric by showing the high accuracy attained by a basic method, i.e., TF-IDF combined with logistic regression classifier, using a small set of terms with the highest ESAS score. Consequently, we introduce and scrutinize these top ESAS-ranked terms to aid individuals in strengthening their skepticism towards LLM-generated fake news.

------------

`[2406.14021] HIGHT: Hierarchical Graph Tokenization for Graph-Language Alignment <https://arxiv.org/abs/2406.14021>`__ 

::

    Thu, 20 Jun 2024 06:37:35 GMT
    Yongqiang Chen, Quanming Yao, Juzheng Zhang, James Cheng, Yatao Bian

Recently there has been a surge of interest in extending the success of large language models (LLMs) to graph modality, such as social networks and molecules. As LLMs are predominantly trained with 1D text data, most existing approaches adopt a graph neural network to represent a graph as a series of node tokens and feed these tokens to LLMs for graph-language alignment. Despite achieving some successes, existing approaches have overlooked the hierarchical structures that are inherent in graph data. Especially, in molecular graphs, the high-order structural information contains rich semantics of molecular functional groups, which encode crucial biochemical functionalities of the molecules. We establish a simple benchmark showing that neglecting the hierarchical information in graph tokenization will lead to subpar graph-language alignment and severe hallucination in generated outputs. To address this problem, we propose a novel strategy called HIerarchical GrapH Tokenization (HIGHT). HIGHT employs a hierarchical graph tokenizer that extracts and encodes the hierarchy of node, motif, and graph levels of informative tokens to improve the graph perception of LLMs. HIGHT also adopts an augmented graph-language supervised fine-tuning dataset, enriched with the hierarchical graph information, to further enhance the graph-language alignment. Extensive experiments on 7 molecule-centric benchmarks confirm the effectiveness of HIGHT in reducing hallucination by 40%, as well as significant improvements in various molecule-language downstream tasks.

------------

`[2406.14023] Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective <https://arxiv.org/abs/2406.14023>`__ 从心理测量学角度评估大型语言模型中的内隐偏见

::

    Thu, 20 Jun 2024 06:42:08 GMT
    Yuchen Wen, Keping Bi, Wei Chen, Jiafeng Guo, Xueqi Cheng

As Large Language Models (LLMs) become an important way of information seeking, there have been increasing concerns about the unethical content LLMs may generate. In this paper, we conduct a rigorous evaluation of LLMs' implicit bias towards certain groups by attacking them with carefully crafted instructions to elicit biased responses. Our attack methodology is inspired by psychometric principles in cognitive and social psychology. We propose three attack approaches, i.e., Disguise, Deception, and Teaching, based on which we built evaluation datasets for four common bias types. Each prompt attack has bilingual versions. Extensive evaluation of representative LLMs shows that 1) all three attack methods work effectively, especially the Deception attacks; 2) GLM-3 performs the best in defending our attacks, compared to GPT-3.5 and GPT-4; 3) LLMs could output content of other bias types when being taught with one type of bias. Our methodology provides a rigorous and effective way of evaluating LLMs' implicit bias and will benefit the assessments of LLMs' potential ethical risks.

------------

`[2406.14048] Prompt Injection Attacks in Defended Systems <https://arxiv.org/abs/2406.14048>`__ 防御系统中的提示注入攻击

::

    Thu, 20 Jun 2024 07:13:25 GMT
    Daniil Khomsky, Narek Maloyan, Bulat Nutfullin

Large language models play a crucial role in modern natural language processing technologies. However, their extensive use also introduces potential security risks, such as the possibility of black-box attacks. These attacks can embed hidden malicious features into the model, leading to adverse consequences during its deployment.
This paper investigates methods for black-box attacks on large language models with a three-tiered defense mechanism. It analyzes the challenges and significance of these attacks, highlighting their potential implications for language processing system security. Existing attack and defense methods are examined, evaluating their effectiveness and applicability across various scenarios.
Special attention is given to the detection algorithm for black-box attacks, identifying hazardous vulnerabilities in language models and retrieving sensitive information. This research presents a methodology for vulnerability detection and the development of defensive strategies against black-box attacks on large language models.

------------

`[2406.14051] How Many Parameters Does it Take to Change a Light Bulb? Evaluating Performance in Self-Play of Conversational Games as a Function of Model Characteristics <https://arxiv.org/abs/2406.14051>`__ 换一个灯泡需要多少个参数?以模型特征的函数来评估会话游戏的自玩表现

::

    Thu, 20 Jun 2024 07:17:09 GMT
    Nidhir Bhavsar and Jonathan Jordan and Sherzod Hakimov and David Schlangen

What makes a good Large Language Model (LLM)? That it performs well on the relevant benchmarks -- which hopefully measure, with some validity, the presence of capabilities that are also challenged in real application. But what makes the model perform well? What gives a model its abilities? We take a recently introduced type of benchmark that is meant to challenge capabilities in a goal-directed, agentive context through self-play of conversational games, and analyse how performance develops as a function of model characteristics like number of parameters, or type of training. We find that while there is a clear relationship between number of parameters and performance, there is still a wide spread of performance points within a given size bracket, which is to be accounted for by training parameters such as fine-tuning data quality and method. From a more practical angle, we also find a certain degree of unpredictability about performance across access methods, possible due to unexposed sampling parameters, and a, very welcome, performance stability against at least moderate weight quantisation during inference.

------------

`[2406.14115] Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models <https://arxiv.org/abs/2406.14115>`__ 取其精华去其糟粕:对微调大型语言模型数据选择的重新思考

::

    Thu, 20 Jun 2024 08:58:58 GMT
    Ziche Liu, Rui Ke, Feng Jiang, Haizhou Li

Data selection for fine-tuning Large Language Models (LLMs) aims to select a high-quality subset from a given candidate dataset to train a Pending Fine-tune Model (PFM) into a Selective-Enhanced Model (SEM). It can improve the model performance and accelerate the training process. Although a few surveys have investigated related works of data selection, there is a lack of comprehensive comparison between existing methods due to their various experimental settings.
To address this issue, we first propose a three-stage scheme for data selection and comprehensively review existing works according to this scheme. Then, we design a unified comparing method with ratio-based efficiency indicators and ranking-based feasibility indicators to overcome the difficulty of comparing various models with diverse experimental settings. After an in-depth comparative analysis, we find that the more targeted method with data-specific and model-specific quality labels has higher efficiency, but the introduction of additional noise information should be avoided when designing selection algorithms. Finally, we summarize the trends in data selection and highlight the short-term and long-term challenges to guide future research.

------------

`[2406.14144] Finding Safety Neurons in Large Language Models <https://arxiv.org/abs/2406.14144>`__ 在大型语言模型中寻找安全神经元

::

    Thu, 20 Jun 2024 09:35:22 GMT
    Jianhui Chen, Xiaozhi Wang, Zijun Yao, Yushi Bai, Lei Hou, Juanzi Li

Large language models (LLMs) excel in various capabilities but also pose safety risks such as generating harmful content and misinformation, even after safety alignment. In this paper, we explore the inner mechanisms of safety alignment from the perspective of mechanistic interpretability, focusing on identifying and analyzing safety neurons within LLMs that are responsible for safety behaviors. We propose generation-time activation contrasting to locate these neurons and dynamic activation patching to evaluate their causal effects.
Experiments on multiple recent LLMs show that: (1) Safety neurons are sparse and effective. We can restore $90$% safety performance with intervention only on about $5$% of all the neurons. (2) Safety neurons encode transferrable mechanisms. They exhibit consistent effectiveness on different red-teaming datasets. The finding of safety neurons also interprets "alignment tax". We observe that the identified key neurons for safety and helpfulness significantly overlap, but they require different activation patterns of the shared neurons. Furthermore, we demonstrate an application of safety neurons in detecting unsafe outputs before generation. Our findings may promote further research on understanding LLM alignment. The source codes will be publicly released to facilitate future research.

------------

`[2406.14155] Aligning Large Language Models with Diverse Political Viewpoints <https://arxiv.org/abs/2406.14155>`__ 具有不同政治观点的大型语言模型的对齐

::

    Thu, 20 Jun 2024 09:53:23 GMT
    Dominik Stammbach and Philine Widmer and Eunjung Cho and Caglar Gulcehre and Elliott Ash

Large language models such as ChatGPT often exhibit striking political biases. If users query them about political information, they might take a normative stance and reinforce such biases. To overcome this, we align LLMs with diverse political viewpoints from 100,000 comments written by candidates running for national parliament in Switzerland. Such aligned models are able to generate more accurate political viewpoints from Swiss parties compared to commercial models such as ChatGPT. We also propose a procedure to generate balanced overviews from multiple viewpoints using such models.

------------

`[2406.14167] Definition generation for lexical semantic change detection <https://arxiv.org/abs/2406.14167>`__ 词汇语义变化检测的定义生成

::

    Thu, 20 Jun 2024 10:13:08 GMT
    Mariia Fedorova, Andrey Kutuzov, Yves Scherrer

We use contextualized word definitions generated by large language models as semantic representations in the task of diachronic lexical semantic change detection (LSCD). In short, generated definitions are used as `senses', and the change score of a target word is retrieved by comparing their distributions in two time periods under comparison. On the material of five datasets and three languages, we show that generated definitions are indeed specific and general enough to convey a signal sufficient to rank sets of words by the degree of their semantic change over time. Our approach is on par with or outperforms prior non-supervised sense-based LSCD methods. At the same time, it preserves interpretability and allows to inspect the reasons behind a specific shift in terms of discrete definitions-as-senses. This is another step in the direction of explainable semantic change modeling.

------------

`[2406.14230] Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing <https://arxiv.org/abs/2406.14230>`__ 提高标准:通过生成式演化测试研究大型语言模型的价值

::

    Thu, 20 Jun 2024 11:51:00 GMT
    Han Jiang, Xiaoyuan Yi, Zhihua Wei, Shu Wang, Xing Xie

Warning: this paper contains model outputs exhibiting unethical information.
Large Language Models (LLMs) have achieved significant breakthroughs, but their generated unethical content poses potential risks. Measuring value alignment of LLMs becomes crucial for their regulation and responsible deployment. Numerous datasets have been constructed to assess social bias, toxicity, and ethics in LLMs, but they suffer from evaluation chronoeffect, that is, as models rapidly evolve, existing data becomes leaked or undemanding, overestimating ever-developing LLMs. To tackle this problem, we propose GETA, a novel generative evolving testing approach that dynamically probes the underlying moral baselines of LLMs. Distinct from previous adaptive testing methods that rely on static datasets with limited difficulty, GETA incorporates an iteratively-updated item generator which infers each LLM's moral boundaries and generates difficulty-tailored testing items, accurately reflecting the true alignment extent. This process theoretically learns a joint distribution of item and model response, with item difficulty and value conformity as latent variables, where the generator co-evolves with the LLM, addressing chronoeffect. We evaluate various popular LLMs with diverse capabilities and demonstrate that GETA can create difficulty-matching testing items and more accurately assess LLMs' values, better consistent with their performance on unseen OOD and i.i.d. items, laying the groundwork for future evaluation paradigms.

------------

`[2406.14275] Step-Back Profiling: Distilling User History for Personalized Scientific Writing <https://arxiv.org/abs/2406.14275>`__ 退步分析:为个性化科学写作提取用户历史

::

    Thu, 20 Jun 2024 12:58:26 GMT
    Xiangru Tang, Xingyao Zhang, Yanjun Shao, Jie Wu, Yilun Zhao, Arman Cohan, Ming Gong, Dongmei Zhang, Mark Gerstein

Large language models (LLMs) excel at a variety of natural language processing tasks, yet they struggle to generate personalized content for individuals, particularly in real-world scenarios like scientific writing.
Addressing this challenge, we introduce Step-Back Profiling to personalize LLMs by distilling user history into concise profiles, including essential traits and preferences of users. Regarding our experiments, we construct a Personalized Scientific Writing (PSW) dataset to study multiuser personalization. PSW requires the models to write scientific papers given specialized author groups with diverse academic backgrounds. As for the results, we demonstrate the effectiveness of capturing user characteristics via Step-Back Profiling for collaborative writing. Moreover, our approach outperforms the baselines by up to 3.6 points on the general personalization benchmark (LaMP), including 7 personalization LLM tasks. Our extensive ablation studies validate the contributions of different components in our method and provide insights into our task definition. Our dataset and code are available at \url{https://github.com/gersteinlab/step-back-profiling}.

------------

`[2406.14313] Robust Few-shot Transfer Learning for Knowledge Base Question Answering with Unanswerable Questions <https://arxiv.org/abs/2406.14313>`__ 面向无法回答问题的知识库问答的鲁棒少样本迁移学习

::

    Thu, 20 Jun 2024 13:43:38 GMT
    Riya Sawhney, Indrajit Bhattacharya, Mausam

Real-world KBQA applications require models that are (1) robust -- e.g., can differentiate between answerable and unanswerable questions, and (2) low-resource -- do not require large training data. Towards this goal, we propose the novel task of few-shot transfer for KBQA with unanswerable questions. We present FUn-FuSIC that extends the state-of-the-art (SoTA) few-shot transfer model for answerable-only KBQA to handle unanswerability. It iteratively prompts an LLM to generate logical forms for the question by providing feedback using a diverse suite of syntactic, semantic and execution guided checks, and adapts self-consistency to assess confidence of the LLM to decide answerability. Experiments over newly constructed datasets show that FUn-FuSIC outperforms suitable adaptations of the SoTA model for KBQA with unanswerability, and the SoTA model for answerable-only few-shot-transfer KBQA.

------------

`[2406.14322] Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning <https://arxiv.org/abs/2406.14322>`__ 保护私隐!用于语言模型微调的用户级差分隐私

::

    Thu, 20 Jun 2024 13:54:32 GMT
    Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Daogao Liu, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang

Large language models (LLMs) have emerged as powerful tools for tackling complex tasks across diverse domains, but they also raise privacy concerns when fine-tuned on sensitive data due to potential memorization. While differential privacy (DP) offers a promising solution by ensuring models are `almost indistinguishable' with or without any particular privacy unit, current evaluations on LLMs mostly treat each example (text record) as the privacy unit. This leads to uneven user privacy guarantees when contributions per user vary. We therefore study user-level DP motivated by applications where it necessary to ensure uniform privacy protection across users. We present a systematic evaluation of user-level DP for LLM fine-tuning on natural language generation tasks. Focusing on two mechanisms for achieving user-level DP guarantees, Group Privacy and User-wise DP-SGD, we investigate design choices like data selection strategies and parameter tuning for the best privacy-utility tradeoff.

------------

`[2406.14326] medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced Clinical Diagnosis on EMRs <https://arxiv.org/abs/2406.14326>`__ medIKAL:整合知识图谱作为LLMs的助手，以增强EMRs上的临床诊断

::

    Thu, 20 Jun 2024 13:56:52 GMT
    Mingyi Jia, Junwen Duan, Yan Song, Jianxin Wang

Electronic Medical Records (EMRs), while integral to modern healthcare, present challenges for clinical reasoning and diagnosis due to their complexity and information redundancy. To address this, we proposed medIKAL (Integrating Knowledge Graphs as Assistants of LLMs), a framework that combines Large Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic capabilities. medIKAL assigns weighted importance to entities in medical records based on their type, enabling precise localization of candidate diseases within KGs. It innovatively employs a residual network-like approach, allowing initial diagnosis by the LLM to be merged into KG search results.
Through a path-based reranking algorithm and a fill-in-the-blank style prompt template, it further refined the diagnostic process. We validated medIKAL's effectiveness through extensive experiments on a newly introduced open-sourced Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis in real-world settings.

------------

`[2406.14335] Self-supervised Interpretable Concept-based Models for Text Classification <https://arxiv.org/abs/2406.14335>`__ 

::

    Thu, 20 Jun 2024 14:04:53 GMT
    Francesco De Santis, Philippe Bich, Gabriele Ciravegna, Pietro Barbiero, Danilo Giordano, Tania Cerquitelli

Despite their success, Large-Language Models (LLMs) still face criticism as their lack of interpretability limits their controllability and reliability.
Traditional post-hoc interpretation methods, based on attention and gradient-based analysis, offer limited insight into the model's decision-making processes. In the image field, Concept-based models have emerged as explainable-by-design architectures, employing human-interpretable features as intermediate representations. However, these methods have not been yet adapted to textual data, mainly because they require expensive concept annotations, which are impractical for real-world text data. This paper addresses this challenge by proposing a self-supervised Interpretable Concept Embedding Models (ICEMs). We leverage the generalization abilities of LLMs to predict the concepts labels in a self-supervised way, while we deliver the final predictions with an interpretable function. The results of our experiments show that ICEMs can be trained in a self-supervised way achieving similar performance to fully supervised concept-based models and end-to-end black-box ones. Additionally, we show that our models are (i) interpretable, offering meaningful logical explanations for their predictions; (ii) interactable, allowing humans to modify intermediate predictions through concept interventions; and (iii) controllable, guiding the LLMs' decoding process to follow a required decision-making path.

------------

`[2406.14336] Exploring Spatial Representations in the Historical Lake District Texts with LLM-based Relation Extraction <https://arxiv.org/abs/2406.14336>`__ 基于llm的历史湖区文本空间关系抽取研究

::

    Thu, 20 Jun 2024 14:04:59 GMT
    Erum Haris, Anthony G. Cohn, John G. Stell

Navigating historical narratives poses a challenge in unveiling the spatial intricacies of past landscapes. The proposed work addresses this challenge within the context of the English Lake District, employing the Corpus of the Lake District Writing. The method utilizes a generative pre-trained transformer model to extract spatial relations from the textual descriptions in the corpus.
The study applies this large language model to understand the spatial dimensions inherent in historical narratives comprehensively. The outcomes are presented as semantic triples, capturing the nuanced connections between entities and locations, and visualized as a network, offering a graphical representation of the spatial narrative. The study contributes to a deeper comprehension of the English Lake District's spatial tapestry and provides an approach to uncovering spatial relations within diverse historical contexts.

------------

`[2406.14394] SEC-QA: A Systematic Evaluation Corpus for Financial QA <https://arxiv.org/abs/2406.14394>`__ SEC-QA:一个系统性的财务QA评估语料库

::

    Thu, 20 Jun 2024 15:12:41 GMT
    Viet Dac Lai, Michael Krumdick, Charles Lovering, Varshini Reddy, Craig Schmidt, Chris Tanner

The financial domain frequently deals with large numbers of long documents that are essential for daily operations. Significant effort is put towards automating financial data analysis. However, a persistent challenge, not limited to the finance domain, is the scarcity of datasets that accurately reflect real-world tasks for model evaluation. Existing datasets are often constrained by size, context, or relevance to practical applications. Moreover, LLMs are currently trained on trillions of tokens of text, limiting access to novel data or documents that models have not encountered during training for unbiased evaluation. We propose SEC-QA, a continuous dataset generation framework with two key features: 1) the semi-automatic generation of Question-Answer (QA) pairs spanning multiple long context financial documents, which better represent real-world financial scenarios; 2) the ability to continually refresh the dataset using the most recent public document collections, not yet ingested by LLMs. Our experiments show that current retrieval augmented generation methods systematically fail to answer these challenging multi-document questions. In response, we introduce a QA system based on program-of-thought that improves the ability to perform complex information retrieval and quantitative reasoning pipelines, thereby increasing QA accuracy.

------------

`[2406.14462] Explicit and Implicit Large Language Model Personas Generate Opinions but Fail to Replicate Deeper Perceptions and Biases <https://arxiv.org/abs/2406.14462>`__ 显式和隐式的大型语言模型人物角色生成意见，但无法复制更深的感知和偏见

::

    Thu, 20 Jun 2024 16:24:07 GMT
    Salvatore Giorgi, Tingting Liu, Ankit Aich, Kelsey Isman, Garrick Sherman, Zachary Fried, Jo\~ao Sedoc, Lyle H. Ungar, Brenda Curtis

Large language models (LLMs) are increasingly being used in human-centered social scientific tasks, such as data annotation, synthetic data creation, and engaging in dialog. However, these tasks are highly subjective and dependent on human factors, such as one's environment, attitudes, beliefs, and lived experiences. Thus, employing LLMs (which do not have such human factors) in these tasks may result in a lack of variation in data, failing to reflect the diversity of human experiences. In this paper, we examine the role of prompting LLMs with human-like personas and asking the models to answer as if they were a specific human. This is done explicitly, with exact demographics, political beliefs, and lived experiences, or implicitly via names prevalent in specific populations. The LLM personas are then evaluated via (1) subjective annotation task (e.g., detecting toxicity) and (2) a belief generation task, where both tasks are known to vary across human factors. We examine the impact of explicit vs. implicit personas and investigate which human factors LLMs recognize and respond to. Results show that LLM personas show mixed results when reproducing known human biases, but generate generally fail to demonstrate implicit biases.
We conclude that LLMs lack the intrinsic cognitive mechanisms of human thought, while capturing the statistical patterns of how people speak, which may restrict their effectiveness in complex social science applications.

------------

`[2406.14500] Improving Expert Radiology Report Summarization by Prompting Large Language Models with a Layperson Summary <https://arxiv.org/abs/2406.14500>`__ 通过用外行人摘要提示大型语言模型改进专家放射学报告摘要

::

    Thu, 20 Jun 2024 17:01:55 GMT
    Xingmeng Zhao, Tongnian Wang, Anthony Rios

Radiology report summarization (RRS) is crucial for patient care, requiring concise "Impressions" from detailed "Findings." This paper introduces a novel prompting strategy to enhance RRS by first generating a layperson summary. This approach normalizes key observations and simplifies complex information using non-expert communication techniques inspired by doctor-patient interactions.
Combined with few-shot in-context learning, this method improves the model's ability to link general terms to specific findings. We evaluate this approach on the MIMIC-CXR, CheXpert, and MIMIC-III datasets, benchmarking it against 7B/8B parameter state-of-the-art open-source large language models (LLMs) like Meta-Llama-3-8B-Instruct. Our results demonstrate improvements in summarization accuracy and accessibility, particularly in out-of-domain tests, with improvements as high as 5% for some metrics.

------------

`[2406.14504] Translating Across Cultures: LLMs for Intralingual Cultural Adaptation <https://arxiv.org/abs/2406.14504>`__ 跨文化翻译:语言内文化适应法学硕士

::

    Thu, 20 Jun 2024 17:06:58 GMT
    Pushpdeep Singh, Mayur Patidar, Lovekesh Vig

LLMs are increasingly being deployed for multilingual applications and have demonstrated impressive translation capabilities between several low and high resource languages. An aspect of translation that often gets overlooked is that of cultural adaptation, or modifying source culture references to suit the target culture. Cultural adaptation has applications across several creative industries and requires intimate knowledge of source and target cultures during translation. While specialized translation models still outperform LLMs on the machine translation task when viewed from the lens of correctness, they are not sensitive to cultural differences often requiring manual correction. LLMs on the other hand have a rich reservoir of cultural knowledge embedded within its parameters that can be potentially exploited for such applications. In this paper we define the task of cultural adaptation and create an evaluation framework to benchmark different models for this task. We evaluate the performance of modern LLMs for cultural adaptation and analyze their cross cultural knowledge while connecting related concepts across different cultures.
We also analyze possible issues with automatic adaptation including cultural biases and stereotypes. We hope that this task will offer more insight into the cultural understanding of LLMs and their creativity in cross-cultural scenarios.

------------

`[2406.14508] Evidence of a log scaling law for political persuasion with large language models <https://arxiv.org/abs/2406.14508>`__ 

::

    Thu, 20 Jun 2024 17:12:38 GMT
    Kobi Hackenburg, Ben M. Tappin, Paul R\"ottger, Scott Hale, Jonathan Bright, Helen Margetts

Large language models can now generate political messages as persuasive as those written by humans, raising concerns about how far this persuasiveness may continue to increase with model size. Here, we generate 720 persuasive messages on 10 U.S. political issues from 24 language models spanning several orders of magnitude in size. We then deploy these messages in a large-scale randomized survey experiment (N = 25,982) to estimate the persuasive capability of each model. Our findings are twofold. First, we find evidence of a log scaling law: model persuasiveness is characterized by sharply diminishing returns, such that current frontier models are barely more persuasive than models smaller in size by an order of magnitude or more. Second, mere task completion (coherence, staying on topic) appears to account for larger models' persuasive advantage.
These findings suggest that further scaling model size will not much increase the persuasiveness of static LLM-generated messages.

------------

`[2406.14511] Investigating Mysteries of CoT-Augmented Distillation <https://arxiv.org/abs/2406.14511>`__ 研究cot -增强蒸馏的奥秘

::

    Thu, 20 Jun 2024 17:15:46 GMT
    Somin Wadhwa, Silvio Amir, Byron C. Wallace

Eliciting "chain of thought" (CoT) rationales -- sequences of token that convey a "reasoning" process -- has been shown to consistently improve LLM performance on tasks like question answering. More recent efforts have shown that such rationales can also be used for model distillation: Including CoT sequences (elicited from a large "teacher" model) in addition to target labels when fine-tuning a small student model yields (often substantial) improvements.
In this work we ask: Why and how does this additional training signal help in model distillation? We perform ablations to interrogate this, and report some potentially surprising results. Specifically: (1) Placing CoT sequences after labels (rather than before) realizes consistently better downstream performance -- this means that no student "reasoning" is necessary at test time to realize gains. (2) When rationales are appended in this way, they need not be coherent reasoning sequences to yield improvements; performance increases are robust to permutations of CoT tokens, for example. In fact, (3) a small number of key tokens are sufficient to achieve improvements equivalent to those observed when full rationales are used in model distillation.

------------

`[2406.14545] Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems <https://arxiv.org/abs/2406.14545>`__ 揭露数据库漏洞:Text-to-SQL系统中的零知识模式推理攻击

::

    Thu, 20 Jun 2024 17:54:33 GMT
    {\DJ}or{\dj}e Klisura and Anthony Rios

Relational databases are integral to modern information systems, serving as the foundation for storing, querying, and managing data efficiently and effectively. Advancements in large language modeling have led to the emergence of text-to-SQL technologies, significantly enhancing the querying and extracting of information from these databases and raising concerns about privacy and security. Our research extracts the database schema elements underlying a text-to-SQL model. Knowledge of the schema can make attacks such as SQL injection easier. By asking specially crafted questions, we have developed a zero-knowledge framework designed to probe various database schema elements without knowledge of the database itself. The text-to-SQL models then process these questions to produce an output that we use to uncover the structure of the database schema. We apply it to specialized text-to-SQL models fine-tuned on text-SQL pairs and generative language models used for SQL generation. Overall, we can reconstruct the table names with an F1 of nearly .75 for fine-tuned models and .96 for generative.

------------

`[2406.14546] Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data <https://arxiv.org/abs/2406.14546>`__ 连接点:llm可以从不同的训练数据中推断和表达潜在结构

::

    Thu, 20 Jun 2024 17:55:04 GMT
    Johannes Treutlein, Dami Choi, Jan Betley, Cem Anil, Samuel Marks, Roger Baker Grosse, Owain Evans

One way to address safety risks from large language models (LLMs) is to censor dangerous knowledge from their training data. While this removes the explicit information, implicit information can remain scattered across various training documents. Could an LLM infer the censored knowledge by piecing together these implicit hints? As a step towards answering this question, we study inductive out-of-context reasoning (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning. Using a suite of five tasks, we demonstrate that frontier LLMs can perform inductive OOCR. In one experiment we finetune an LLM on a corpus consisting only of distances between an unknown city and other known cities. Remarkably, without in-context examples or Chain of Thought, the LLM can verbalize that the unknown city is Paris and use this fact to answer downstream questions. Further experiments show that LLMs trained only on individual coin flip outcomes can verbalize whether the coin is biased, and those trained only on pairs $(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR succeeds in a range of cases, we also show that it is unreliable, particularly for smaller LLMs learning complex structures. Overall, the ability of LLMs to "connect the dots" without explicit in-context learning poses a potential obstacle to monitoring and controlling the knowledge acquired by LLMs.

------------

`[2406.14562] Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities <https://arxiv.org/abs/2406.14562>`__ 思维白板:跨模态一步一步地思考

::

    Thu, 20 Jun 2024 17:59:45 GMT
    Sachit Menon and Richard Zemel and Carl Vondrick

When presented with questions involving visual thinking, humans naturally switch reasoning modalities, often forming mental images or drawing visual aids. Large language models have shown promising results in arithmetic and symbolic reasoning by expressing intermediate reasoning in text as a chain of thought, yet struggle to extend this capability to answer text queries that are easily solved by visual reasoning, even with extensive multimodal pretraining.
We introduce a simple method, whiteboard-of-thought prompting, to unlock the visual reasoning capabilities of multimodal large language models across modalities. Whiteboard-of-thought prompting provides multimodal large language models with a metaphorical `whiteboard' to draw out reasoning steps as images, then returns these images back to the model for further processing. We find this can be accomplished with no demonstrations or specialized modules, instead leveraging models' existing ability to write code with libraries such as Matplotlib and Turtle. This simple approach shows state-of-the-art results on four difficult natural language tasks that involve visual and spatial reasoning. We identify multiple settings where GPT-4o using chain-of-thought fails dramatically, including more than one where it achieves $0\%$ accuracy, while whiteboard-of-thought enables up to $92\%$ accuracy in these same settings. We present a detailed exploration of where the technique succeeds as well as its sources of error.

------------

`[2406.14563] Model Merging and Safety Alignment: One Bad Model Spoils the Bunch <https://arxiv.org/abs/2406.14563>`__ 模型合并和安全对齐:一个坏模型毁了一堆模型

::

    Thu, 20 Jun 2024 17:59:58 GMT
    Hasan Abed Al Kader Hammoud, Umberto Michieli, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem, Mete Ozay

Merging Large Language Models (LLMs) is a cost-effective technique for combining multiple expert LLMs into a single versatile model, retaining the expertise of the original ones. However, current approaches often overlook the importance of safety alignment during merging, leading to highly misaligned models. This work investigates the effects of model merging on alignment. We evaluate several popular model merging techniques, demonstrating that existing methods do not only transfer domain expertise but also propagate misalignment.
We propose a simple two-step approach to address this problem: (i) generating synthetic safety and domain-specific data, and (ii) incorporating these generated data into the optimization process of existing data-aware model merging techniques. This allows us to treat alignment as a skill that can be maximized in the resulting merged LLM. Our experiments illustrate the effectiveness of integrating alignment-related data during merging, resulting in models that excel in both domain expertise and alignment.

------------

`[2406.12925] GLiNER multi-task: Generalist Lightweight Model for Various Information Extraction Tasks <https://arxiv.org/abs/2406.12925>`__ GLiNER多任务:用于各种信息抽取任务的通用轻量级模型

::

    Fri, 14 Jun 2024 13:54:29 GMT
    Ihor Stepanov and Mykhailo Shtopko

Information extraction tasks require both accurate, efficient, and generalisable models. Classical supervised deep learning approaches can achieve the required performance, but they need large datasets and are limited in their ability to adapt to different tasks. On the other hand, large language models (LLMs) demonstrate good generalization, meaning that they can adapt to many different tasks based on user requests. However, LLMs are computationally expensive and tend to fail to generate structured outputs. In this article, we will introduce a new kind of GLiNER model that can be used for various information extraction tasks while being a small encoder model. Our model achieved SoTA performance on zero-shot NER benchmarks and leading performance on question-answering, summarization and relation extraction tasks.
Additionally, in this article, we will cover experimental results on self-learning approaches for named entity recognition using GLiNER models.

------------

`[2406.13175] Sparse High Rank Adapters <https://arxiv.org/abs/2406.13175>`__ 稀疏高阶适配器

::

    Wed, 19 Jun 2024 03:13:11 GMT
    Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Viswanath Ganapathy, Rafael Esteves, Shreya Kadambi, Shubhankar Borse, Paul Whatmough, Risheek Garrepalli, Mart Van Baalen, Harris Teague, Markus Nagel

Low Rank Adaptation (LoRA) has gained massive attention in the recent generative AI research. One of the main advantages of LoRA is its ability to be fused with pretrained models adding no overhead during inference. However, from a mobile deployment standpoint, we can either avoid inference overhead in the fused mode but lose the ability to switch adapters rapidly, or suffer significant (up to 30% higher) inference latency while enabling rapid switching in the unfused mode. LoRA also exhibits concept-loss when multiple adapters are used concurrently. In this paper, we propose Sparse High Rank Adapters (SHiRA), a new paradigm which incurs no inference overhead, enables rapid switching, and significantly reduces concept-loss. Specifically, SHiRA can be trained by directly tuning only 1-2% of the base model weights while leaving others unchanged. This results in a highly sparse adapter which can be switched directly in the fused mode. We further provide theoretical and empirical insights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing concept loss. Our extensive experiments on LVMs and LLMs demonstrate that finetuning only a small fraction of the parameters in the base model is sufficient for many tasks while enabling both rapid switching and multi-adapter fusion. Finally, we provide a latency- and memory-efficient SHiRA implementation based on Parameter-Efficient Finetuning (PEFT) Library. This implementation trains at nearly the same speed as LoRA while consuming lower peak GPU memory, thus making SHiRA easy to adopt for practical use cases.

------------

`[2406.13193] PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes <https://arxiv.org/abs/2406.13193>`__ 快速:渐进式预训练增强合成化学结果

::

    Wed, 19 Jun 2024 03:59:46 GMT
    He Cao and Yanjun Shao and Zhiyuan Liu and Zijing Liu and Xiangru Tang and Yuan Yao and Yu Li

Multimodal Large Language Models (MLLMs) have seen growing adoption across various scientific disciplines. These advancements encourage the investigation of molecule-text modeling within synthetic chemistry, a field dedicated to designing and conducting chemical reactions to synthesize new compounds with desired properties and applications. Current approaches, however, often neglect the critical role of multiple molecule graph interaction in understanding chemical reactions, leading to suboptimal performance in synthetic chemistry tasks. This study introduces PRESTO(Progressive Pretraining Enhances Synthetic Chemistry Outcomes), a new framework that bridges the molecule-text modality gap by integrating a comprehensive benchmark of pretraining strategies and dataset configurations. It progressively improves multimodal LLMs through cross-modal alignment and multi-graph understanding. Our extensive experiments demonstrate that PRESTO offers competitive results in downstream synthetic chemistry tasks. The code can be found at https://github.com/IDEA-XL/PRESTO.

------------

`[2406.13356] Jogging the Memory of Unlearned Model Through Targeted Relearning Attack <https://arxiv.org/abs/2406.13356>`__ 通过有针对性的再学习攻击来慢跑未学习模型的记忆

::

    Wed, 19 Jun 2024 09:03:21 GMT
    Shengyuan Hu, Yiwei Fu, Zhiwei Steven Wu, Virginia Smith

Machine unlearning is a promising approach to mitigate undesirable memorization of training data in ML models. However, in this work we show that existing approaches for unlearning in LLMs are surprisingly susceptible to a simple set of targeted relearning attacks. With access to only a small and potentially loosely related set of data, we find that we can 'jog' the memory of unlearned models to reverse the effects of unlearning. We formalize this unlearning-relearning pipeline, explore the attack across three popular unlearning benchmarks, and discuss future directions and guidelines that result from our study.

------------

`[2406.13474] Attention-aware Post-training Quantization without Backpropagation <https://arxiv.org/abs/2406.13474>`__ 无反向传播的注意力感知训练后量化

::

    Wed, 19 Jun 2024 11:53:21 GMT
    Junhan Kim, Ho-young Kim, Eulrang Cho, Chungman Lee, Joonyoung Kim, Yongkweon Jeon

Quantization is a promising solution for deploying large-scale language models (LLMs) on resource-constrained devices. Existing quantization approaches, however, rely on gradient-based optimization, regardless of it being post-training quantization (PTQ) or quantization-aware training (QAT), which becomes problematic for hyper-scale LLMs with billions of parameters.
This overhead can be alleviated via recently proposed backpropagation-free PTQ methods; however, their performance is somewhat limited by their lack of consideration of inter-layer dependencies. In this paper, we thus propose a novel PTQ algorithm that considers inter-layer dependencies without relying on backpropagation. The fundamental concept involved is the development of attention-aware Hessian matrices, which facilitates the consideration of inter-layer dependencies within the attention module. Extensive experiments demonstrate that the proposed algorithm significantly outperforms conventional PTQ methods, particularly for low bit-widths.

------------

`[2406.13777] Game of LLMs: Discovering Structural Constructs in Activities using Large Language Models <https://arxiv.org/abs/2406.13777>`__ llm的游戏:在使用大型语言模型的活动中发现结构性结构

::

    Wed, 19 Jun 2024 19:02:44 GMT
    Shruthi K. Hiremath and Thomas Ploetz

Human Activity Recognition is a time-series analysis problem. A popular analysis procedure used by the community assumes an optimal window length to design recognition pipelines. However, in the scenario of smart homes, where activities are of varying duration and frequency, the assumption of a constant sized window does not hold. Additionally, previous works have shown these activities to be made up of building blocks. We focus on identifying these underlying building blocks--structural constructs, with the use of large language models. Identifying these constructs can be beneficial especially in recognizing short-duration and infrequent activities. We also propose the development of an activity recognition procedure that uses these building blocks to model activities, thus helping the downstream task of activity monitoring in smart homes.

------------

`[2406.13868] SDQ: Sparse Decomposed Quantization for LLM Inference <https://arxiv.org/abs/2406.13868>`__ 

::

    Wed, 19 Jun 2024 22:12:51 GMT
    Geonhwa Jeong, Po-An Tsai, Stephen W. Keckler, Tushar Krishna

Recently, large language models (LLMs) have shown surprising performance in task-specific workloads as well as general tasks with the given prompts.
However, to achieve unprecedented performance, recent LLMs use billions to trillions of parameters, which hinder the wide adaptation of those models due to their extremely large compute and memory requirements. To resolve the issue, various model compression methods are being actively investigated. In this work, we propose SDQ (Sparse Decomposed Quantization) to exploit both structured sparsity and quantization to achieve both high compute and memory efficiency. From our evaluations, we observe that SDQ can achieve 4x effective compute throughput with <1% quality drop.

------------

`[2406.13966] Causal Inference with Latent Variables: Recent Advances and Future Prospectives <https://arxiv.org/abs/2406.13966>`__ 基于潜变量的因果推断:最新进展与未来展望

::

    Thu, 20 Jun 2024 03:15:53 GMT
    Yaochen Zhu, Yinhan He, Jing Ma, Mengxuan Hu, Sheng Li, Jundong Li

Causality lays the foundation for the trajectory of our world. Causal inference (CI), which aims to infer intrinsic causal relations among variables of interest, has emerged as a crucial research topic. Nevertheless, the lack of observation of important variables (e.g., confounders, mediators, exogenous variables, etc.) severely compromises the reliability of CI methods. The issue may arise from the inherent difficulty in measuring the variables.
Additionally, in observational studies where variables are passively recorded, certain covariates might be inadvertently omitted by the experimenter.
Depending on the type of unobserved variables and the specific CI task, various consequences can be incurred if these latent variables are carelessly handled, such as biased estimation of causal effects, incomplete understanding of causal mechanisms, lack of individual-level causal consideration, etc. In this survey, we provide a comprehensive review of recent developments in CI with latent variables. We start by discussing traditional CI techniques when variables of interest are assumed to be fully observed. Afterward, under the taxonomy of circumvention and inference-based methods, we provide an in-depth discussion of various CI strategies to handle latent variables, covering the tasks of causal effect estimation, mediation analysis, counterfactual reasoning, and causal discovery. Furthermore, we generalize the discussion to graph data where interference among units may exist. Finally, we offer fresh aspects for further advancement of CI with latent variables, especially new opportunities in the era of large language models (LLMs).

------------

`[2406.14045] Understanding Different Design Choices in Training Large Time Series Models <https://arxiv.org/abs/2406.14045>`__ 理解训练大型时间序列模型中的不同设计选择

::

    Thu, 20 Jun 2024 07:09:19 GMT
    Yu-Neng Chuang, Songchen Li, Jiayi Yuan, Guanchu Wang, Kwei-Herng Lai, Leisheng Yu, Sirui Ding, Chia-Yuan Chang, Qiaoyu Tan, Daochen Zha, Xia Hu

Inspired by Large Language Models (LLMs), Time Series Forecasting (TSF), a long-standing task in time series analysis, is undergoing a transition towards Large Time Series Models (LTSMs), aiming to train universal transformer-based models for TSF. However, training LTSMs on heterogeneous time series data poses unique challenges, including diverse frequencies, dimensions, and patterns across datasets. Recent endeavors have studied and evaluated various design choices aimed at enhancing LTSM training and generalization capabilities, spanning pre-processing techniques, model configurations, and dataset configurations. In this work, we comprehensively analyze these design choices and aim to identify the best practices for training LTSM. Moreover, we propose \emph{time series prompt}, a novel statistical prompting strategy tailored to time series data. Furthermore, based on the observations in our analysis, we introduce \texttt{LTSM-bundle}, which bundles the best design choices we have identified. Empirical results demonstrate that \texttt{LTSM-bundle} achieves superior zero-shot and few-shot performances compared to state-of-the-art LSTMs and traditional TSF methods on benchmark datasets.

------------

`[2406.14150] Multi-modal Transfer Learning between Biological Foundation Models <https://arxiv.org/abs/2406.14150>`__ 生物基础模型间的多模态迁移学习

::

    Thu, 20 Jun 2024 09:44:53 GMT
    Juan Jose Garau-Luis, Patrick Bordes, Liam Gonzalez, Masa Roller, Bernardo P. de Almeida, Lorenz Hexemer, Christopher Blum, Stefan Laurent, Jan Grzegorzewski, Maren Lang, Thomas Pierrot, Guillaume Richard

Biological sequences encode fundamental instructions for the building blocks of life, in the form of DNA, RNA, and proteins. Modeling these sequences is key to understand disease mechanisms and is an active research area in computational biology. Recently, Large Language Models have shown great promise in solving certain biological tasks but current approaches are limited to a single sequence modality (DNA, RNA, or protein). Key problems in genomics intrinsically involve multiple modalities, but it remains unclear how to adapt general-purpose sequence models to those cases. In this work we propose a multi-modal model that connects DNA, RNA, and proteins by leveraging information from different pre-trained modality-specific encoders. We demonstrate its capabilities by applying it to the largely unsolved problem of predicting how multiple RNA transcript isoforms originate from the same gene (i.e. same DNA sequence) and map to different transcription expression levels across various human tissues. We show that our model, dubbed IsoFormer, is able to accurately predict differential transcript expression, outperforming existing methods and leveraging the use of multiple modalities. Our framework also achieves efficient transfer knowledge from the encoders pre-training as well as in between modalities. We open-source our model, paving the way for new multi-modal gene expression approaches.

------------

`[2406.14393] Jailbreaking as a Reward Misspecification Problem <https://arxiv.org/abs/2406.14393>`__ 越狱作为奖励的错误说明问题

::

    Thu, 20 Jun 2024 15:12:27 GMT
    Zhihui Xie, Jiahui Gao, Lei Li, Zhenguo Li, Qi Liu, Lingpeng Kong

The widespread adoption of large language models (LLMs) has raised concerns about their safety and reliability, particularly regarding their vulnerability to adversarial attacks. In this paper, we propose a novel perspective that attributes this vulnerability to reward misspecification during the alignment process. We introduce a metric ReGap to quantify the extent of reward misspecification and demonstrate its effectiveness and robustness in detecting harmful backdoor prompts. Building upon these insights, we present ReMiss, a system for automated red teaming that generates adversarial prompts against various target aligned LLMs. ReMiss achieves state-of-the-art attack success rates on the AdvBench benchmark while preserving the human readability of the generated prompts. Detailed analysis highlights the unique advantages brought by the proposed reward misspecification objective compared to previous methods.

------------

`[2406.14473] Data-Centric AI in the Age of Large Language Models <https://arxiv.org/abs/2406.14473>`__ 大型语言模型时代以数据为中心的人工智能

::

    Thu, 20 Jun 2024 16:34:07 GMT
    Xinyi Xu, Zhaoxuan Wu, Rui Qiao, Arun Verma, Yao Shu, Jingtan Wang, Xinyuan Niu, Zhenfeng He, Jiangwei Chen, Zijian Zhou, Gregory Kang Ruey Lau, Hieu Dao, Lucas Agussurja, Rachael Hwee Ling Sim, Xiaoqiang Lin, Wenyang Hu, Zhongxiang Dai, Pang Wei Koh, Bryan Kian Hsiang Low

This position paper proposes a data-centric viewpoint of AI research, focusing on large language models (LLMs). We start by making the key observation that data is instrumental in the developmental (e.g., pretraining and fine-tuning) and inferential stages (e.g., in-context learning) of LLMs, and yet it receives disproportionally low attention from the research community. We identify four specific scenarios centered around data, covering data-centric benchmarks and data curation, data attribution, knowledge transfer, and inference contextualization. In each scenario, we underscore the importance of data, highlight promising research directions, and articulate the potential impacts on the research community and, where applicable, the society as a whole. For instance, we advocate for a suite of data-centric benchmarks tailored to the scale and complexity of data for LLMs. These benchmarks can be used to develop new data curation methods and document research efforts and results, which can help promote openness and transparency in AI and LLM research.

------------

`[2406.14517] PostMark: A Robust Blackbox Watermark for Large Language Models <https://arxiv.org/abs/2406.14517>`__ 邮戳:大型语言模型的鲁棒黑盒子水印

::

    Thu, 20 Jun 2024 17:27:14 GMT
    Yapei Chang, Kalpesh Krishna, Amir Houmansadr, John Wieting, Mohit Iyyer

The most effective techniques to detect LLM-generated text rely on inserting a detectable signature -- or watermark -- during the model's decoding process.
Most existing watermarking methods require access to the underlying LLM's logits, which LLM API providers are loath to share due to fears of model distillation. As such, these watermarks must be implemented independently by each LLM provider. In this paper, we develop PostMark, a modular post-hoc watermarking procedure in which an input-dependent set of words (determined via a semantic embedding) is inserted into the text after the decoding process has completed. Critically, PostMark does not require logit access, which means it can be implemented by a third party. We also show that PostMark is more robust to paraphrasing attacks than existing watermarking methods: our experiments cover eight baseline algorithms, five base LLMs, and three datasets. Finally, we evaluate the impact of PostMark on text quality using both automated and human assessments, highlighting the trade-off between quality and robustness to paraphrasing. We release our code, outputs, and annotations at https://github.com/lilakk/PostMark.

------------

`[2406.14541] Are LLMs Naturally Good at Synthetic Tabular Data Generation? <https://arxiv.org/abs/2406.14541>`__ llm天生擅长合成表格数据生成吗?

::

    Thu, 20 Jun 2024 17:52:29 GMT
    Shengzhe Xu, Cho-Ting Lee, Mandar Sharma, Raquib Bin Yousuf, Nikhil Muralidhar, Naren Ramakrishnan

Large language models (LLMs) have demonstrated their prowess in generating synthetic text and images; however, their potential for generating tabular data -- arguably the most common data type in business and scientific applications -- is largely underexplored. This paper demonstrates that LLMs, used as-is, or after traditional fine-tuning, are severely inadequate as synthetic table generators. Due to the autoregressive nature of LLMs, fine-tuning with random order permutation runs counter to the importance of modeling functional dependencies, and renders LLMs unable to model conditional mixtures of distributions (key to capturing real world constraints). We showcase how LLMs can be made to overcome some of these deficiencies by making them permutation-aware.

------------

`[2406.12934] Current state of LLM Risks and AI Guardrails <https://arxiv.org/abs/2406.12934>`__ 

::

    Sun, 16 Jun 2024 22:04:10 GMT
    Suriya Ganesh Ayyamperumal, Limin Ge

Large language models (LLMs) have become increasingly sophisticated, leading to widespread deployment in sensitive applications where safety and reliability are paramount. However, LLMs have inherent risks accompanying them, including bias, potential for unsafe actions, dataset poisoning, lack of explainability, hallucinations, and non-reproducibility. These risks necessitate the development of "guardrails" to align LLMs with desired behaviors and mitigate potential harm.
This work explores the risks associated with deploying LLMs and evaluates current approaches to implementing guardrails and model alignment techniques.
We examine intrinsic and extrinsic bias evaluation methods and discuss the importance of fairness metrics for responsible AI development. The safety and reliability of agentic LLMs (those capable of real-world actions) are explored, emphasizing the need for testability, fail-safes, and situational awareness.
Technical strategies for securing LLMs are presented, including a layered protection model operating at external, secondary, and internal levels. System prompts, Retrieval-Augmented Generation (RAG) architectures, and techniques to minimize bias and protect privacy are highlighted.
Effective guardrail design requires a deep understanding of the LLM's intended use case, relevant regulations, and ethical considerations. Striking a balance between competing requirements, such as accuracy and privacy, remains an ongoing challenge. This work underscores the importance of continuous research and development to ensure the safe and responsible use of LLMs in real-world applications.

------------

`[2406.12935] ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates <https://arxiv.org/abs/2406.12935>`__ ChatBug:对齐llm中由聊天模板引起的常见漏洞

::

    Mon, 17 Jun 2024 03:03:34 GMT
    Fengqing Jiang, Zhangchen Xu, Luyao Niu, Bill Yuchen Lin, Radha Poovendran

Large language models (LLMs) are expected to follow instructions from users and engage in conversations. Techniques to enhance LLMs' instruction-following capabilities typically fine-tune them using data structured according to a predefined chat template. Although chat templates are shown to be effective in optimizing LLM performance, their impact on safety alignment of LLMs has been less understood, which is crucial for deploying LLMs safely at scale.
In this paper, we investigate how chat templates affect safety alignment of LLMs. We identify a common vulnerability, named ChatBug, that is introduced by chat templates. Our key insight to identify ChatBug is that the chat templates provide a rigid format that need to be followed by LLMs, but not by users.
Hence, a malicious user may not necessarily follow the chat template when prompting LLMs. Instead, malicious users could leverage their knowledge of the chat template and accordingly craft their prompts to bypass safety alignments of LLMs. We develop two attacks to exploit the ChatBug vulnerability. We demonstrate that a malicious user can exploit the ChatBug vulnerability of eight state-of-the-art (SOTA) LLMs and effectively elicit unintended responses from these models. Moreover, we show that ChatBug can be exploited by existing jailbreak attacks to enhance their attack success rates. We investigate potential countermeasures to ChatBug. Our results show that while adversarial training effectively mitigates the ChatBug vulnerability, the victim model incurs significant performance degradation. These results highlight the trade-off between safety alignment and helpfulness. Developing new methods for instruction tuning to balance this trade-off is an open and critical direction for future research

------------

`[2406.12946] Instruction Data Generation and Unsupervised Adaptation for Speech Language Models <https://arxiv.org/abs/2406.12946>`__ 语音语言模型的指令数据生成与无监督自适应

::

    Tue, 18 Jun 2024 08:27:00 GMT
    Vahid Noroozi, Zhehuai Chen, Somshubra Majumdar, Steve Huang, Jagadeesh Balam, Boris Ginsburg

In this paper, we propose three methods for generating synthetic samples to train and evaluate multimodal large language models capable of processing both text and speech inputs. Addressing the scarcity of samples containing both modalities, synthetic data generation emerges as a crucial strategy to enhance the performance of such systems and facilitate the modeling of cross-modal relationships between the speech and text domains. Our process employs large language models to generate textual components and text-to-speech systems to generate speech components. The proposed methods offer a practical and effective means to expand the training dataset for these models. Experimental results show progress in achieving an integrated understanding of text and speech. We also highlight the potential of using unlabeled speech data to generate synthetic samples comparable in quality to those with available transcriptions, enabling the expansion of these models to more languages.

------------

`[2406.12950] MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction <https://arxiv.org/abs/2406.12950>`__ MolecularGPT:小样本分子特性预测的开放大型语言模型(LLM)

::

    Tue, 18 Jun 2024 12:54:47 GMT
    Yuyan Liu, Sirui Ding, Sheng Zhou, Wenqi Fan, Qiaoyu Tan

Molecular property prediction (MPP) is a fundamental and crucial task in drug discovery. However, prior methods are limited by the requirement for a large number of labeled molecules and their restricted ability to generalize for unseen and new tasks, both of which are essential for real-world applications.
To address these challenges, we present MolecularGPT for few-shot MPP. From a perspective on instruction tuning, we fine-tune large language models (LLMs) based on curated molecular instructions spanning over 1000 property prediction tasks. This enables building a versatile and specialized LLM that can be adapted to novel MPP tasks without any fine-tuning through zero- and few-shot in-context learning (ICL). MolecularGPT exhibits competitive in-context reasoning capabilities across 10 downstream evaluation datasets, setting new benchmarks for few-shot molecular prediction tasks. More importantly, with just two-shot examples, MolecularGPT can outperform standard supervised graph neural network methods on 4 out of 7 datasets. It also excels state-of-the-art LLM baselines by up to 16.6% increase on classification accuracy and decrease of 199.17 on regression metrics (e.g., RMSE) under zero-shot. This study demonstrates the potential of LLMs as effective few-shot molecular property predictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.

------------

`[2406.13049] Assessing AI vs Human-Authored Spear Phishing SMS Attacks: An Empirical Study Using the TRAPD Method <https://arxiv.org/abs/2406.13049>`__ 评估人工智能与人工撰写的鱼叉式钓鱼短信攻击:基于TRAPD方法的实证研究

::

    Tue, 18 Jun 2024 20:47:16 GMT
    Jerson Francia, Derek Hansen, Ben Schooley, Matthew Taylor, Shydra Murray and Greg Snow

This paper explores the rising concern of utilizing Large Language Models (LLMs) in spear phishing message generation, and their performance compared to human-authored counterparts. Our pilot study compares the effectiveness of smishing (SMS phishing) messages created by GPT-4 and human authors, which have been personalized to willing targets. The targets assessed the messages in a modified ranked-order experiment using a novel methodology we call TRAPD (Threshold Ranking Approach for Personalized Deception). Specifically, targets provide personal information (job title and location, hobby, item purchased online), spear smishing messages are created using this information by humans and GPT-4, targets are invited back to rank-order 12 messages from most to least convincing (and identify which they would click on), and then asked questions about why they ranked messages the way they did. They also guess which messages are created by an LLM and their reasoning. Results from 25 targets show that LLM-generated messages are most often perceived as more convincing than those authored by humans, with messages related to jobs being the most convincing. We characterize different criteria used when assessing the authenticity of messages including word choice, style, and personal relevance.
Results also show that targets were unable to identify whether the messages was AI-generated or human-authored and struggled to identify criteria to use in order to make this distinction. This study aims to highlight the urgent need for further research and improved countermeasures against personalized AI-enabled social engineering attacks.

------------

`[2406.13163] LLMatDesign: Autonomous Materials Discovery with Large Language Models <https://arxiv.org/abs/2406.13163>`__ LLMatDesign:基于大型语言模型的自主材料发现

::

    Wed, 19 Jun 2024 02:35:02 GMT
    Shuyi Jia, Chao Zhang, Victor Fung

Discovering new materials can have significant scientific and technological implications but remains a challenging problem today due to the enormity of the chemical space. Recent advances in machine learning have enabled data-driven methods to rapidly screen or generate promising materials, but these methods still depend heavily on very large quantities of training data and often lack the flexibility and chemical understanding often desired in materials discovery. We introduce LLMatDesign, a novel language-based framework for interpretable materials design powered by large language models (LLMs).
LLMatDesign utilizes LLM agents to translate human instructions, apply modifications to materials, and evaluate outcomes using provided tools. By incorporating self-reflection on its previous decisions, LLMatDesign adapts rapidly to new tasks and conditions in a zero-shot manner. A systematic evaluation of LLMatDesign on several materials design tasks, in silico, validates LLMatDesign's effectiveness in developing new materials with user-defined target properties in the small data regime. Our framework demonstrates the remarkable potential of autonomous LLM-guided materials discovery in the computational setting and towards self-driving laboratories in the future.

------------

`[2406.13215] Neural Residual Diffusion Models for Deep Scalable Vision Generation <https://arxiv.org/abs/2406.13215>`__ 深度可扩展视觉生成的神经残差扩散模型

::

    Wed, 19 Jun 2024 04:57:18 GMT
    Zhiyuan Ma, Liangliang Zhao, Biqing Qi, Bowen Zhou

The most advanced diffusion models have recently adopted increasingly deep stacked networks (e.g., U-Net or Transformer) to promote the generative emergence capabilities of vision generation models similar to large language models (LLMs). However, progressively deeper stacked networks will intuitively cause numerical propagation errors and reduce noisy prediction capabilities on generative data, which hinders massively deep scalable training of vision generation models. In this paper, we first uncover the nature that neural networks being able to effectively perform generative denoising lies in the fact that the intrinsic residual unit has consistent dynamic property with the input signal's reverse diffusion process, thus supporting excellent generative abilities. Afterwards, we stand on the shoulders of two common types of deep stacked networks to propose a unified and massively scalable Neural Residual Diffusion Models framework (Neural-RDM for short), which is a simple yet meaningful change to the common architecture of deep generative networks by introducing a series of learnable gated residual parameters that conform to the generative dynamics. Experimental results on various generative tasks show that the proposed neural residual models obtain state-of-the-art scores on image's and video's generative benchmarks. Rigorous theoretical proofs and extensive experiments also demonstrate the advantages of this simple gated residual mechanism consistent with dynamic modeling in improving the fidelity and consistency of generated content and supporting large-scale scalable training.
Code is available at https://github.com/Anonymous/Neural-RDM.

------------

`[2406.13235] Enhancing Collaborative Semantics of Language Model-Driven Recommendations via Graph-Aware Learning <https://arxiv.org/abs/2406.13235>`__ 

::

    Wed, 19 Jun 2024 05:50:15 GMT
    Zhong Guan, Likang Wu, Hongke Zhao, Ming He, Jianpin Fan

Large Language Models (LLMs) are increasingly prominent in the recommendation systems domain. Existing studies usually utilize in-context learning or supervised fine-tuning on task-specific data to align LLMs into recommendations. However, the substantial bias in semantic spaces between language processing tasks and recommendation tasks poses a nonnegligible challenge. Specifically, without the adequate capturing ability of collaborative information, existing modeling paradigms struggle to capture behavior patterns within community groups, leading to LLMs' ineffectiveness in discerning implicit interaction semantic in recommendation scenarios. To address this, we consider enhancing the learning capability of language model-driven recommendation models for structured data, specifically by utilizing interaction graphs rich in collaborative semantics. We propose a Graph-Aware Learning for Language Model-Driven Recommendations (GAL-Rec).
GAL-Rec enhances the understanding of user-item collaborative semantics by imitating the intent of Graph Neural Networks (GNNs) to aggregate multi-hop information, thereby fully exploiting the substantial learning capacity of LLMs to independently address the complex graphs in the recommendation system.
Sufficient experimental results on three real-world datasets demonstrate that GAL-Rec significantly enhances the comprehension of collaborative semantics, and improves recommendation performance.

------------

`[2406.13605] Nicer Than Humans: How do Large Language Models Behave in the Prisoner's Dilemma? <https://arxiv.org/abs/2406.13605>`__ 比人类更好:大型语言模型在囚徒困境中的表现如何?

::

    Wed, 19 Jun 2024 14:51:14 GMT
    Nicol\'o Fontana, Francesco Pierri, Luca Maria Aiello

The behavior of Large Language Models (LLMs) as artificial social agents is largely unexplored, and we still lack extensive evidence of how these agents react to simple social stimuli. Testing the behavior of AI agents in classic Game Theory experiments provides a promising theoretical framework for evaluating the norms and values of these agents in archetypal social situations. In this work, we investigate the cooperative behavior of Llama2 when playing the Iterated Prisoner's Dilemma against random adversaries displaying various levels of hostility. We introduce a systematic methodology to evaluate an LLM's comprehension of the game's rules and its capability to parse historical gameplay logs for decision-making. We conducted simulations of games lasting for 100 rounds, and analyzed the LLM's decisions in terms of dimensions defined in behavioral economics literature. We find that Llama2 tends not to initiate defection but it adopts a cautious approach towards cooperation, sharply shifting towards a behavior that is both forgiving and non-retaliatory only when the opponent reduces its rate of defection below 30%.
In comparison to prior research on human participants, Llama2 exhibits a greater inclination towards cooperative behavior. Our systematic approach to the study of LLMs in game theoretical scenarios is a step towards using these simulations to inform practices of LLM auditing and alignment.

------------

`[2406.13631] On AI-Inspired UI-Design <https://arxiv.org/abs/2406.13631>`__ 受ai启发的ui设计

::

    Wed, 19 Jun 2024 15:28:21 GMT
    Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, G\'erard Dray, Walid Maalej

Graphical User Interface (or simply UI) is a primary mean of interaction between users and their device. In this paper, we discuss three major complementary approaches on how to use Artificial Intelligence (AI) to support app designers create better, more diverse, and creative UI of mobile apps.
First, designers can prompt a Large Language Model (LLM) like GPT to directly generate and adjust one or multiple UIs. Second, a Vision-Language Model (VLM) enables designers to effectively search a large screenshot dataset, e.g. from apps published in app stores. The third approach is to train a Diffusion Model (DM) specifically designed to generate app UIs as inspirational images. We discuss how AI should be used, in general, to inspire and assist creative app design rather than automating it.

------------

`[2406.13763] Through the Theory of Mind's Eye: Reading Minds with Multimodal Video Large Language Models <https://arxiv.org/abs/2406.13763>`__ 通过心灵之眼理论:用多模态视频大型语言模型读心术

::

    Wed, 19 Jun 2024 18:24:31 GMT
    Zhawnen Chen, Tianchun Wang, Yizhou Wang, Michal Kosinski, Xiang Zhang, Yun Fu, Sheng Li

Can large multimodal models have a human-like ability for emotional and social reasoning, and if so, how does it work? Recent research has discovered emergent theory-of-mind (ToM) reasoning capabilities in large language models (LLMs). LLMs can reason about people's mental states by solving various text-based ToM tasks that ask questions about the actors' ToM (e.g., human belief, desire, intention). However, human reasoning in the wild is often grounded in dynamic scenes across time. Thus, we consider videos a new medium for examining spatio-temporal ToM reasoning ability. Specifically, we ask explicit probing questions about videos with abundant social and emotional reasoning content. We develop a pipeline for multimodal LLM for ToM reasoning using video and text. We also enable explicit ToM reasoning by retrieving key frames for answering a ToM question, which reveals how multimodal LLMs reason about ToM.

------------

`[2406.14086] Seg-LSTM: Performance of xLSTM for Semantic Segmentation of Remotely Sensed Images <https://arxiv.org/abs/2406.14086>`__ Seg-LSTM: xLSTM在遥感图像语义分割中的性能

::

    Thu, 20 Jun 2024 08:01:28 GMT
    Qinfeng Zhu, Yuanzhi Cai, Lei Fan

Recent advancements in autoregressive networks with linear complexity have driven significant research progress, demonstrating exceptional performance in large language models. A representative model is the Extended Long Short-Term Memory (xLSTM), which incorporates gating mechanisms and memory structures, performing comparably to Transformer architectures in long-sequence language tasks. Autoregressive networks such as xLSTM can utilize image serialization to extend their application to visual tasks such as classification and segmentation. Although existing studies have demonstrated Vision-LSTM's impressive results in image classification, its performance in image semantic segmentation remains unverified. Our study represents the first attempt to evaluate the effectiveness of Vision-LSTM in the semantic segmentation of remotely sensed images. This evaluation is based on a specifically designed encoder-decoder architecture named Seg-LSTM, and comparisons with state-of-the-art segmentation networks. Our study found that Vision-LSTM's performance in semantic segmentation was limited and generally inferior to Vision-Transformers-based and Vision-Mamba-based models in most comparative tests. Future research directions for enhancing Vision-LSTM are recommended.
The source code is available from https://github.com/zhuqinfeng1999/Seg-LSTM.

------------

`[2406.14088] ReaLHF: Optimized RLHF Training for Large Language Models through Parameter Reallocation <https://arxiv.org/abs/2406.14088>`__ ReaLHF:通过参数重分配优化的大型语言模型RLHF训练

::

    Thu, 20 Jun 2024 08:04:07 GMT
    Zhiyu Mei, Wei Fu, Kaiwei Li, Guangju Wang, Huanchen Zhang, Yi Wu

Reinforcement Learning from Human Feedback (RLHF) stands as a pivotal technique in empowering large language model (LLM) applications. Since RLHF involves diverse computational workloads and intricate dependencies among multiple LLMs, directly adopting parallelization techniques from supervised training can result in sub-optimal performance. To overcome this limitation, we propose a novel approach named parameter ReaLlocation, which dynamically redistributes LLM parameters in the cluster and adapts parallelization strategies during training. Building upon this idea, we introduce ReaLHF, a pioneering system capable of automatically discovering and running efficient execution plans for RLHF training given the desired algorithmic and hardware configurations. ReaLHF formulates the execution plan for RLHF as an augmented dataflow graph. Based on this formulation, ReaLHF employs a tailored search algorithm with a lightweight cost estimator to discover an efficient execution plan. Subsequently, the runtime engine deploys the selected plan by effectively parallelizing computations and redistributing parameters. We evaluate ReaLHF on the LLaMA-2 models with up to $4\times70$ billion parameters and 128 GPUs. The experiment results showcase ReaLHF's substantial speedups of $2.0-10.6\times$ compared to baselines. Furthermore, the execution plans generated by ReaLHF exhibit an average of $26\%$ performance improvement over heuristic approaches based on Megatron-LM. The source code of ReaLHF is publicly available at https://github.com/openpsi-project/ReaLHF .

------------

`[2406.14097] Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration <https://arxiv.org/abs/2406.14097>`__ 通过人机协作增强基于llm的机器人操纵

::

    Thu, 20 Jun 2024 08:23:49 GMT
    Haokun Liu, Yaonan Zhu, Kenji Kato, Atsushi Tsukahara, Izumi Kondo, Tadayoshi Aoyama, and Yasuhisa Hasegawa

Large Language Models (LLMs) are gaining popularity in the field of robotics.
However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This paper proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.

------------

`[2406.14318] The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in Prompts <https://arxiv.org/abs/2406.14318>`__ 火贼也是守护者:在提示中平衡可用性和隐私

::

    Thu, 20 Jun 2024 13:52:25 GMT
    Zhili Shen, Zihang Xi, Ying He, Wei Tong, Jingyu Hua, Sheng Zhong

The rapid adoption of online chatbots represents a significant advancement in artificial intelligence. However, this convenience brings considerable privacy concerns, as prompts can inadvertently contain sensitive information exposed to large language models (LLMs). Limited by high computational costs, reduced task usability, and excessive system modifications, previous works based on local deployment, embedding perturbation, and homomorphic encryption are inapplicable to online prompt-based LLM applications.
To address these issues, this paper introduces Prompt Privacy Sanitizer (i.e., ProSan), an end-to-end prompt privacy protection framework that can produce anonymized prompts with contextual privacy removed while maintaining task usability and human readability. It can also be seamlessly integrated into the online LLM service pipeline. To achieve high usability and dynamic anonymity, ProSan flexibly adjusts its protection targets and strength based on the importance of the words and the privacy leakage risk of the prompts.
Additionally, ProSan is capable of adapting to diverse computational resource conditions, ensuring privacy protection even for mobile devices with limited computing power. Our experiments demonstrate that ProSan effectively removes private information across various tasks, including question answering, text summarization, and code generation, with minimal reduction in task performance.

------------

`[2406.14358] The neural correlates of logical-mathematical symbol systems processing resemble that of spatial cognition more than natural language processing <https://arxiv.org/abs/2406.14358>`__ 与自然语言处理相比，逻辑-数学符号系统处理的神经关联更类似于空间认知

::

    Thu, 20 Jun 2024 14:31:09 GMT
    Yuannan Li, Shan Xu, Jia Liu

The ability to manipulate logical-mathematical symbols (LMS), encompassing tasks such as calculation, reasoning, and programming, is a cognitive skill arguably unique to humans. Considering the relatively recent emergence of this ability in human evolutionary history, it has been suggested that LMS processing may build upon more fundamental cognitive systems, possibly through neuronal recycling. Previous studies have pinpointed two primary candidates, natural language processing and spatial cognition. Existing comparisons between these domains largely relied on task-level comparison, which may be confounded by task idiosyncrasy. The present study instead compared the neural correlates at the domain level with both automated meta-analysis and synthesized maps based on three representative LMS tasks, reasoning, calculation, and mental programming. Our results revealed a more substantial cortical overlap between LMS processing and spatial cognition, in contrast to language processing.
Furthermore, in regions activated by both spatial and language processing, the multivariate activation pattern for LMS processing exhibited greater multivariate similarity to spatial cognition than to language processing. A hierarchical clustering analysis further indicated that typical LMS tasks were indistinguishable from spatial cognition tasks at the neural level, suggesting an inherent connection between these two cognitive processes. Taken together, our findings support the hypothesis that spatial cognition is likely the basis of LMS processing, which may shed light on the limitations of large language models in logical reasoning, particularly those trained exclusively on textual data without explicit emphasis on spatial content.

------------

`[2406.10300] Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications <https://arxiv.org/abs/2406.10300>`__ 作为软件组件的大型语言模型:集成llm应用的分类法

::

    Thu, 13 Jun 2024 21:32:56 GMT
    Irene Weber

Large Language Models (LLMs) have become widely adopted recently. Research explores their use both as autonomous agents and as tools for software engineering. LLM-integrated applications, on the other hand, are software systems that leverage an LLM to perform tasks that would otherwise be impossible or require significant coding effort. While LLM-integrated application engineering is emerging as new discipline, its terminology, concepts and methods need to be established. This study provides a taxonomy for LLM-integrated applications, offering a framework for analyzing and describing these systems. It also demonstrates various ways to utilize LLMs in applications, as well as options for implementing such integrations.
Following established methods, we analyze a sample of recent LLM-integrated applications to identify relevant dimensions. We evaluate the taxonomy by applying it to additional cases. This review shows that applications integrate LLMs in numerous ways for various purposes. Frequently, they comprise multiple LLM integrations, which we term ``LLM components''. To gain a clear understanding of an application's architecture, we examine each LLM component separately. We identify thirteen dimensions along which to characterize an LLM component, including the LLM skills leveraged, the format of the output, and more. LLM-integrated applications are described as combinations of their LLM components. We suggest a concise representation using feature vectors for visualization.
The taxonomy is effective for describing LLM-integrated applications. It can contribute to theory building in the nascent field of LLM-integrated application engineering and aid in developing such systems. Researchers and practitioners explore numerous creative ways to leverage LLMs in applications.
Though challenges persist, integrating LLMs may revolutionize the way software systems are built.

------------

`[2406.13275] Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding <https://arxiv.org/abs/2406.13275>`__ 通过优化音频编码的大型语言模型增强自动音频描述

::

    Wed, 19 Jun 2024 07:09:46 GMT
    Jizhong Liu, Gang Li, Junbo Zhang, Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Yujun Wang, Bin Wang

Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by -Base (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.

------------

`[2406.13362] VisualRWKV: Exploring Recurrent Neural Networks for Visual Language Models <https://arxiv.org/abs/2406.13362>`__ VisualRWKV:探索视觉语言模型的循环神经网络

::

    Wed, 19 Jun 2024 09:07:31 GMT
    Haowen Hou and Peigen Zeng and Fei Ma and Fei Richard Yu

Visual Language Models (VLMs) have rapidly progressed with the recent success of large language models. However, there have been few attempts to incorporate efficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In this study, we introduce VisualRWKV, the first application of a linear RNN model to multimodal learning tasks, leveraging the pre-trained RWKV language model. We propose a data-dependent recurrence and sandwich prompts to enhance our modeling capabilities, along with a 2D image scanning mechanism to enrich the processing of visual sequences. Extensive experiments demonstrate that VisualRWKV achieves competitive performance compared to Transformer-based models like LLaVA-1.5 on various benchmarks. To facilitate further research and analysis, we have made the checkpoints and the associated code publicly accessible at the following GitHub repository: \href{https://github.com/howard-hou/VisualRWKV}{https://github.com/howard-hou/VisualRWKV}.

------------

`[2406.13898] The Use of Multimodal Large Language Models to Detect Objects from Thermal Images: Transportation Applications <https://arxiv.org/abs/2406.13898>`__ 使用多模态大型语言模型从热图像中检测目标:交通应用

::

    Thu, 20 Jun 2024 00:05:10 GMT
    Huthaifa I. Ashqar, Taqwa I. Alhadidi, Mohammed Elhenawy, and Nour O. Khanfar

The integration of thermal imaging data with Multimodal Large Language Models (MLLMs) constitutes an exciting opportunity for improving the safety and functionality of autonomous driving systems and many Intelligent Transportation Systems (ITS) applications. This study investigates whether MLLMs can understand complex images from RGB and thermal cameras and detect objects directly. Our goals were to 1) assess the ability of the MLLM to learn from information from various sets, 2) detect objects and identify elements in thermal cameras, 3) determine whether two independent modality images show the same scene, and 4) learn all objects using different modalities. The findings showed that both GPT-4 and Gemini were effective in detecting and classifying objects in thermal images. Similarly, the Mean Absolute Percentage Error (MAPE) for pedestrian classification was 70.39% and 81.48%, respectively. Moreover, the MAPE for bike, car, and motorcycle detection were 78.4%, 55.81%, and 96.15%, respectively. Gemini produced MAPE of 66.53%, 59.35% and 78.18% respectively. This finding further demonstrates that MLLM can identify thermal images and can be employed in advanced imaging automation technologies for ITS applications.

------------

`[2406.14043] Taxonomy-Guided Zero-Shot Recommendations with LLMs <https://arxiv.org/abs/2406.14043>`__ 基于llm的分类法指导的零样本推荐

::

    Thu, 20 Jun 2024 07:06:58 GMT
    Yueqing Liang, Liangwei Yang, Chen Wang, Xiongxiao Xu, Philip S. Yu, Kai Shu

With the emergence of large language models (LLMs) and their ability to perform a variety of tasks, their application in recommender systems (RecSys) has shown promise. However, we are facing significant challenges when deploying LLMs into RecSys, such as limited prompt length, unstructured item information, and un-constrained generation of recommendations, leading to sub-optimal performance. To address these issues, we propose a novel method using a taxonomy dictionary. This method provides a systematic framework for categorizing and organizing items, improving the clarity and structure of item information. By incorporating the taxonomy dictionary into LLM prompts, we achieve efficient token utilization and controlled feature generation, leading to more accurate and contextually relevant recommendations. Our Taxonomy-guided Recommendation (TaxRec) approach features a two-step process: one-time taxonomy categorization and LLM-based recommendation, enabling zero-shot recommendations without the need for domain-specific fine-tuning. Experimental results demonstrate TaxRec significantly enhances recommendation quality compared to traditional zero-shot approaches, showcasing its efficacy as personal recommender with LLMs. Code is available at https://github.com/yueqingliang1/TaxRec.

------------

`[2406.14117] An Investigation of Prompt Variations for Zero-shot LLM-based Rankers <https://arxiv.org/abs/2406.14117>`__ 基于零样本llm排序器的提示变化研究

::

    Thu, 20 Jun 2024 09:03:18 GMT
    Shuoqi Sun, Shengyao Zhuang, Shuai Wang, Guido Zuccon

We provide a systematic understanding of the impact of specific components and wordings used in prompts on the effectiveness of rankers based on zero-shot Large Language Models (LLMs). Several zero-shot ranking methods based on LLMs have recently been proposed. Among many aspects, methods differ across (1) the ranking algorithm they implement, e.g., pointwise vs. listwise, (2) the backbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording used in prompts, e.g., the use or not of role-definition (role-playing) and the actual words used to express this. It is currently unclear whether performance differences are due to the underlying ranking algorithm, or because of spurious factors such as better choice of words used in prompts. This confusion risks to undermine future research. Through our large-scale experimentation and analysis, we find that ranking algorithms do contribute to differences between methods for zero-shot LLM ranking. However, so do the LLM backbones -- but even more importantly, the choice of prompt components and wordings affect the ranking. In fact, in our experiments, we find that, at times, these latter elements have more impact on the ranker's effectiveness than the actual ranking algorithms, and that differences among ranking methods become more blurred when prompt variations are considered.

------------

`[2406.14129] Towards Event-oriented Long Video Understanding <https://arxiv.org/abs/2406.14129>`__ 

::

    Thu, 20 Jun 2024 09:14:19 GMT
    Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen

With the rapid development of video Multimodal Large Language Models (MLLMs), numerous benchmarks have been proposed to assess their video understanding capability. However, due to the lack of rich events in the videos, these datasets may suffer from the short-cut bias that the answers can be deduced from a few frames, without the need to watch the entire video. To address this issue, we introduce Event-Bench, an event-oriented long video understanding benchmark built on existing datasets and human annotations. Event-Bench includes six event-related tasks and 2,190 test instances to comprehensively evaluate video event understanding ability. Additionally, we propose Video Instruction Merging~(VIM), a cost-effective method that enhances video MLLMs using merged, event-intensive video instructions, addressing the scarcity of human-annotated, event-intensive data. Extensive experiments show that the best-performing model, GPT-4o, achieves an overall accuracy of 53.33, significantly outperforming the best open-source model by 41.42%. Leveraging an effective instruction synthesis method and an adaptive model architecture, VIM surpasses both state-of-the-art open-source models and GPT-4V on the Event-Bench. All code, data, and models are publicly available at https://github.com/RUCAIBox/Event-Bench.

------------

`[2406.14307] QuST-LLM: Integrating Large Language Models for Comprehensive Spatial Transcriptomics Analysis <https://arxiv.org/abs/2406.14307>`__ QuST-LLM:集成大型语言模型进行全面的空间转录组学分析

::

    Thu, 20 Jun 2024 13:37:10 GMT
    Chao Hui Huang

In this paper, we introduce QuST-LLM, an innovative extension of QuPath that utilizes the capabilities of large language models (LLMs) to analyze and interpret spatial transcriptomics (ST) data. This tool effectively simplifies the intricate and high-dimensional nature of ST data by offering a comprehensive workflow that includes data loading, region selection, gene expression analysis, and functional annotation. QuST-LLM employs LLMs to transform complex ST data into understandable and detailed biological narratives based on gene ontology annotations, thereby significantly improving the interpretability of ST data. Consequently, users can interact with their own ST data using natural language. Hence, QuST-LLM provides researchers with a potent functionality to unravel the spatial and functional complexities of tissues, fostering novel insights and advancements in biomedical research.

------------

`[2406.14492] Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models? <https://arxiv.org/abs/2406.14492>`__ 物体接地真的能减少大型视觉-语言模型的幻觉吗?

::

    Thu, 20 Jun 2024 16:56:11 GMT
    Gregor Geigle, Radu Timofte, Goran Glava\v{s}

Large vision-language models (LVLMs) have recently dramatically pushed the state of the art in image captioning and many image understanding tasks (e.g., visual question answering). LVLMs, however, often \textit{hallucinate} and produce captions that mention concepts that cannot be found in the image. These hallucinations erode the trustworthiness of LVLMs and are arguably among the main obstacles to their ubiquitous adoption. Recent work suggests that addition of grounding objectives -- those that explicitly align image regions or objects to text spans -- reduces the amount of LVLM hallucination. Although intuitive, this claim is not empirically justified as the reduction effects have been established, we argue, with flawed evaluation protocols that (i) rely on data (i.e., MSCOCO) that has been extensively used in LVLM training and (ii) measure hallucination via question answering rather than open-ended caption generation.
In this work, in contrast, we offer the first systematic analysis of the effect of fine-grained object grounding on LVLM hallucination under an evaluation protocol that more realistically captures LVLM hallucination in open generation. Our extensive experiments over three backbone LLMs reveal that grounding objectives have little to no effect on object hallucination in open caption generation.

------------

`[2406.14544] Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs <https://arxiv.org/abs/2406.14544>`__ Prism: vlm解耦和评估框架

::

    Thu, 20 Jun 2024 17:54:03 GMT
    Yuxuan Qiao, Haodong Duan, Xinyu Fang, Junming Yang, Lin Chen, Songyang Zhang, Jiaqi Wang, Dahua Lin, Kai Chen

Vision Language Models (VLMs) demonstrate remarkable proficiency in addressing a wide array of visual questions, which requires strong perception and reasoning faculties. Assessing these two competencies independently is crucial for model refinement, despite the inherent difficulty due to the intertwined nature of seeing and reasoning in existing VLMs. To tackle this issue, we present Prism, an innovative framework designed to disentangle the perception and reasoning processes involved in visual question solving. Prism comprises two distinct stages: a perception stage that utilizes a VLM to extract and articulate visual information in textual form, and a reasoning stage that formulates responses based on the extracted visual information using a Large Language Model (LLM). This modular design enables the systematic comparison and assessment of both proprietary and open-source VLM for their perception and reasoning strengths. Our analytical framework provides several valuable insights, underscoring Prism's potential as a cost-effective solution for vision-language tasks. By combining a streamlined VLM focused on perception with a powerful LLM tailored for reasoning, Prism achieves superior results in general vision-language tasks while substantially cutting down on training and operational expenses. Quantitative evaluations show that Prism, when configured with a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on par with VLMs $10 \times$ larger on the rigorous multimodal benchmark MMStar.
The project is released at: https://github.com/SparksJoe/Prism.

------------

`[2406.14169] Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning <https://arxiv.org/abs/2406.14169>`__ 使用大型语言模型和强化学习优化新颖的Top-k推荐

::

    Thu, 20 Jun 2024 10:20:02 GMT
    Amit Sharma, Hua Li, Xue Li, Jian Jiao

Given an input query, a recommendation model is trained using user feedback data (e.g., click data) to output a ranked list of items. In real-world systems, besides accuracy, an important consideration for a new model is novelty of its top-k recommendations w.r.t. an existing deployed model.
However, novelty of top-k items is a difficult goal to optimize a model for, since it involves a non-differentiable sorting operation on the model's predictions. Moreover, novel items, by definition, do not have any user feedback data. Given the semantic capabilities of large language models, we address these problems using a reinforcement learning (RL) formulation where large language models provide feedback for the novel items. However, given millions of candidate items, the sample complexity of a standard RL algorithm can be prohibitively high. To reduce sample complexity, we reduce the top-k list reward to a set of item-wise rewards and reformulate the state space to consist of <query, item> tuples such that the action space is reduced to a binary decision; and show that this reformulation results in a significantly lower complexity when the number of items is large. We evaluate the proposed algorithm on improving novelty for a query-ad recommendation task on a large-scale search engine. Compared to supervised finetuning on recent <query, ad> pairs, the proposed RL-based algorithm leads to significant novelty gains with minimal loss in recall. We obtain similar results on the ORCAS query-webpage matching dataset and a product recommendation dataset based on Amazon reviews.

------------

`[2406.14549] Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models <https://arxiv.org/abs/2406.14549>`__ 揭示潜在记忆:评估大型语言模型中的数据泄漏和记忆模式

::

    Thu, 20 Jun 2024 17:56:17 GMT
    Sunny Duan, Mikail Khona, Abhiram Iyer, Rylan Schaeffer, Ila R Fiete

The proliferation of large language models has revolutionized natural language processing tasks, yet it raises profound concerns regarding data privacy and security. Language models are trained on extensive corpora including potentially sensitive or proprietary information, and the risk of data leakage -- where the model response reveals pieces of such information -- remains inadequately understood. This study examines susceptibility to data leakage by quantifying the phenomenon of memorization in machine learning models, focusing on the evolution of memorization patterns over training. We investigate how the statistical characteristics of training data influence the memories encoded within the model by evaluating how repetition influences memorization. We reproduce findings that the probability of memorizing a sequence scales logarithmically with the number of times it is present in the data. Furthermore, we find that sequences which are not apparently memorized after the first encounter can be uncovered throughout the course of training even without subsequent encounters. The presence of these latent memorized sequences presents a challenge for data privacy since they may be hidden at the final checkpoint of the model. To this end, we develop a diagnostic test for uncovering these latent memorized sequences by considering their cross entropy loss.

------------

`[2311.09641] RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models <https://arxiv.org/abs/2311.09641>`__ 

::

    replaced with revised version Wed, 19 Jun 2024 22:40:07 GMT
    Submission history From: Jiongxiao Wang [view email]
    [v1] Thu, 16 Nov 2023 07:48:45 UTC (8,210 KB)
    [v2] Wed, 19 Jun 2024 22:40:07 UTC (8,881 KB)
    Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao

Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences, playing an important role in LLMs alignment. Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially. To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates' selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost). With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance. Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our findings highlight critical security challenges in RLHF, underscoring the necessity for more robust alignment methods for LLMs.

------------

`[2405.16434] The Importance of Directional Feedback for LLM-based Optimizers <https://arxiv.org/abs/2405.16434>`__ 定向反馈对基于llm的优化器的重要性

::

    replaced with revised version Thu, 20 Jun 2024 16:10:50 GMT
    Submission history From: Allen Nie [view email]
    [v1] Sun, 26 May 2024 05:22:35 UTC (431 KB)
    [v2] Thu, 20 Jun 2024 16:10:50 UTC (431 KB)
    Allen Nie, Ching-An Cheng, Andrey Kolobov, Adith Swaminathan

We study the potential of using large language models (LLMs) as an interactive optimizer for solving maximization problems in a text space using natural language and numerical feedback. Inspired by the classical optimization literature, we classify the natural language feedback into directional and non-directional, where the former is a generalization of the first-order feedback to the natural language space. We find that LLMs are especially capable of optimization when they are provided with {directional feedback}. Based on this insight, we design a new LLM-based optimizer that synthesizes directional feedback from the historical optimization trace to achieve reliable improvement over iterations. Empirically, we show our LLM-based optimizer is more stable and efficient in solving optimization problems, from maximizing mathematical functions to optimizing prompts for writing poems, compared with existing techniques.

------------

`[2406.06874] Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback <https://arxiv.org/abs/2406.06874>`__ 联合演示和偏好学习改进了与人工反馈的政策一致性

::

    replaced with revised version Wed, 19 Jun 2024 15:04:23 GMT
    Submission history From: Siliang Zeng [view email]
    [v1] Tue, 11 Jun 2024 01:20:53 UTC (13,533 KB)
    [v2] Wed, 19 Jun 2024 15:04:23 UTC (13,524 KB)
    Chenliang Li, Siliang Zeng, Zeyi Liao, Jiaxiang Li, Dongyeop Kang, Alfredo Garcia, Mingyi Hong

Aligning human preference and value is an important requirement for building contemporary foundation models and embodied AI. However, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into successive stages, such as supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL), each performing one specific learning task. Such a sequential approach results in serious issues such as significant under-utilization of data and distribution mismatch between the learned reward model and generated policy, which eventually lead to poor alignment performance. We develop a single stage approach named Alignment with Integrated Human Feedback (AIHF), capable of integrating both human preference and demonstration to train reward models and the policy. The proposed approach admits a suite of efficient algorithms, which can easily reduce to, and leverage, popular alignment algorithms such as RLHF and Directly Policy Optimization (DPO), and only requires minor changes to the existing alignment pipelines. We demonstrate the efficiency of the proposed solutions with extensive experiments involving alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithms such as RLHF and DPO by large margins, especially when the amount of high-quality preference data is relatively limited.

------------

`[2406.09363] ElicitationGPT: Text Elicitation Mechanisms via Language Models <https://arxiv.org/abs/2406.09363>`__ elicitationongpt:基于语言模型的文本诱导机制

::

    replaced with revised version Wed, 19 Jun 2024 00:12:35 GMT
    Submission history From: Yifan Wu [view email]
    [v1] Thu, 13 Jun 2024 17:49:10 UTC (40 KB)
    [v2] Wed, 19 Jun 2024 00:12:35 UTC (39 KB)
    Yifan Wu, Jason Hartline

Scoring rules evaluate probabilistic forecasts of an unknown state against the realized state and are a fundamental building block in the incentivized elicitation of information and the training of machine learning models. This paper develops mechanisms for scoring elicited text against ground truth text using domain-knowledge-free queries to a large language model (specifically ChatGPT) and empirically evaluates their alignment with human preferences. The empirical evaluation is conducted on peer reviews from a peer-grading dataset and in comparison to manual instructor scores for the peer reviews.

------------

`[2406.10690] Bridging the Gap in Drug Safety Data Analysis: Large Language Models for SQL Query Generation <https://arxiv.org/abs/2406.10690>`__ 弥合药品安全数据分析中的差距:用于SQL查询生成的大型语言模型

::

    replaced with revised version Wed, 19 Jun 2024 21:41:11 GMT
    Submission history From: Jeffery Painter Jr [view email]
    [v1] Sat, 15 Jun 2024 17:07:31 UTC (426 KB)
    [v2] Wed, 19 Jun 2024 21:41:11 UTC (426 KB)
    Jeffery L. Painter, Venkateswara Rao Chalamalasetti, Raymond Kassekert and Andrew Bate

Pharmacovigilance (PV) is essential for drug safety, primarily focusing on adverse event monitoring. Traditionally, accessing safety data required database expertise, limiting broader use. This paper introduces a novel application of Large Language Models (LLMs) to democratize database access for non-technical users. Utilizing OpenAI's GPT-4, we developed a chatbot that generates structured query language (SQL) queries from natural language, bridging the gap between domain knowledge and technical requirements. The proposed application aims for more inclusive and efficient data access, enhancing decision making in drug safety. By providing LLMs with plain language summaries of expert knowledge, our approach significantly improves query accuracy over methods relying solely on database schemas. The application of LLMs in this context not only optimizes PV data analysis, ensuring timely and precise drug safety reporting -- a crucial component in adverse drug reaction monitoring -- but also promotes safer pharmacological practices and informed decision making across various data intensive fields.

------------

`[2406.12058] WellDunn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions <https://arxiv.org/abs/2406.12058>`__ WellDunn:关于语言模型和大型语言模型在识别健康维度方面的鲁棒性和可解释性

::

    replaced with revised version Wed, 19 Jun 2024 18:19:39 GMT
    Submission history From: Seyedali Mohammadi [view email]
    [v1] Mon, 17 Jun 2024 19:50:40 UTC (10,268 KB)
    [v2] Wed, 19 Jun 2024 18:19:39 UTC (10,267 KB)
    Seyedali Mohammadi, Edward Raff, Jinendra Malekar, Vedant Palit, Francis Ferraro, and Manas Gaur

Language Models (LMs) are being proposed for mental health applications where the heightened risk of adverse outcomes means predictive performance may not be a sufficient litmus test of a model's utility in clinical practice. A model that can be trusted for practice should have a correspondence between explanation and clinical determination, yet no prior research has examined the attention fidelity of these models and their effect on ground truth explanations. We introduce an evaluation design that focuses on the robustness and explainability of LMs in identifying Wellness Dimensions (WD). We focus on two mental health and well-being datasets: (a) Multi-label Classification-based MultiWD, and (b) WellXplain for evaluating attention mechanism veracity against expert-labeled explanations. The labels are based on Halbert Dunn's theory of wellness, which gives grounding to our evaluation. We reveal four surprising results about LMs/LLMs: (1) Despite their human-like capabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM fails to deliver any remarkable improvements in performance or explanations. (2) Re-examining LMs' predictions based on a confidence-oriented loss function reveals a significant performance drop. (3) Across all LMs/LLMs, the alignment between attention and explanations remains low, with LLMs scoring a dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific knowledge and undervalue explanations, causing these discrepancies. This study highlights the need for further research into their consistency and explanations in mental health and well-being.

------------

`[2305.13582] Translation and Fusion Improves Zero-shot Cross-lingual Information Extraction <https://arxiv.org/abs/2305.13582>`__ 翻译和融合改进了零样本跨语言信息抽取

::

    replaced with revised version Thu, 20 Jun 2024 14:42:34 GMT
    Submission history From: Yang Chen [view email]
    [v1] Tue, 23 May 2023 01:23:22 UTC (189 KB)
    [v2] Wed, 24 May 2023 04:20:10 UTC (189 KB)
    [v3] Thu, 20 Jun 2024 14:42:34 UTC (1,346 KB)
    Yang Chen, Vedaant Shah, Alan Ritter

Large language models (LLMs) combined with instruction tuning have shown significant progress in information extraction (IE) tasks, exhibiting strong generalization capabilities to unseen datasets by following annotation guidelines. However, their applicability to low-resource languages remains limited due to lack of both labeled data for fine-tuning, and unlabeled text for pre-training. In this paper, we propose TransFusion, a framework in which models are fine-tuned to use English translations of low-resource language data, enabling more precise predictions through annotation fusion. Based on TransFusion, we introduce GoLLIE-TF, a cross-lingual instruction-tuned LLM for IE tasks, designed to close the performance gap between high and low-resource languages. Our experiments across twelve multilingual IE datasets spanning 50 languages demonstrate that GoLLIE-TF achieves better zero-shot cross-lingual transfer over the base model. In addition, we show that TransFusion significantly improves low-resource language named entity recognition when applied to proprietary models such as GPT-4 (+5 F1) with a prompting approach, or fine-tuning different language models including decoder-only (+14 F1) and encoder-only (+13 F1) architectures.

------------

`[2309.08902] Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models <https://arxiv.org/abs/2309.08902>`__ 研究llm中的微妙偏见:生成模型中的年龄歧视、美貌、制度和国籍偏见

::

    replaced with revised version Thu, 20 Jun 2024 01:02:31 GMT
    Submission history From: Gene Louis Kim [view email]
    [v1] Sat, 16 Sep 2023 07:07:04 UTC (2,108 KB)
    [v2] Fri, 16 Feb 2024 21:52:50 UTC (9,295 KB)
    [v3] Thu, 20 Jun 2024 01:02:31 UTC (9,303 KB)
    Mahammed Kamruzzaman, Md. Minul Islam Shovon, Gene Louis Kim

LLMs are increasingly powerful and widely used to assist users in a variety of tasks. This use risks the introduction of LLM biases to consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. Bias in NLP systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., Asians are good at math). In this paper, we investigate bias along less-studied but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that LLMs make between social groups and unrelated positive and negative attributes. We ask whether LLMs hold wide-reaching biases of positive or negative sentiment for specific social groups similar to the "what is beautiful is good" bias found in people in experimental psychology. We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group. We also reverse the completion task to select the social group based on an attribute. We report the correlations that we find for 4 cutting-edge LLMs. This dataset can be used as a benchmark to evaluate progress in more generalized biases and the templating technique can be used to expand the benchmark with minimal additional human annotation.

------------

`[2310.00905] All Languages Matter: On the Multilingual Safety of Large Language Models <https://arxiv.org/abs/2310.00905>`__ 所有语言都很重要:关于大型语言模型的多语言安全性

::

    replaced with revised version Thu, 20 Jun 2024 14:15:23 GMT
    Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu

Categories

------------

`[2310.03304] Learning Personalized Alignment for Evaluating Open-ended Text Generation <https://arxiv.org/abs/2310.03304>`__ 评价开放式文本生成的个性化对齐学习

::

    replaced with revised version Wed, 19 Jun 2024 22:05:12 GMT
    Submission history From: Danqing Wang [view email]
    [v1] Thu, 5 Oct 2023 04:15:48 UTC (2,406 KB)
    [v2] Fri, 6 Oct 2023 17:59:16 UTC (2,406 KB)
    [v3] Tue, 10 Oct 2023 15:15:54 UTC (2,406 KB)
    [v4] Wed, 19 Jun 2024 22:05:12 UTC (2,599 KB)
    Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei Li, Yuandong Tian

With rapid progress made in language qualities such as fluency and consistency via large language models (LLMs), there has been increasing interest in assessing alignment with diverse human preferences. Traditional metrics heavily rely on lexical similarity with human-written references and have been observed to suffer from a poor correlation with human evaluation. Furthermore, they ignore the diverse preferences of humans, a key aspect in evaluating open-ended tasks like story generation. Inspired by these challenges, we introduce an interpretable open-ended evaluation framework PerSE to assess the alignment with a specific human preference. It is tuned to deduce the specific preference from a given personal profile and evaluate the alignment between the generation and the personal preference. PerSE also explains its assessment by a detailed comment or several fine-grained scores. This enhances its interpretability, making it more suitable to tailor a personalized generation. Our 13B LLaMA-2-based PerSE shows a 15.8% increase in Kendall correlation and a 13.7% rise in accuracy on zero-shot reviewers compared to GPT-4. It also outperforms GPT-4 by 46.01% in the Kendall correlation on new domains, indicating its transferability.

------------

`[2310.07059] DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction <https://arxiv.org/abs/2310.07059>`__ 

::

    replaced with revised version Wed, 19 Jun 2024 20:58:52 GMT
    Submission history From: Xueren Ge [view email]
    [v1] Tue, 10 Oct 2023 22:53:15 UTC (461 KB)
    [v2] Wed, 19 Jun 2024 20:58:52 UTC (1,393 KB)
    Xueren Ge, Satpathy Abhishek, Ronald Dean Williams, John A. Stankovic, Homa Alemzadeh

Multi-label text classification (MLTC) tasks in the medical domain often face the long-tail label distribution problem. Prior works have explored hierarchical label structures to find relevant information for few-shot classes, but mostly neglected to incorporate external knowledge from medical guidelines. This paper presents DKEC, Domain Knowledge Enhanced Classification for diagnosis prediction with two innovations: (1) automated construction of heterogeneous knowledge graphs from external sources to capture semantic relations among diverse medical entities, (2) incorporating the heterogeneous knowledge graphs in few-shot classification using a label-wise attention mechanism. We construct DKEC using three online medical knowledge sources and evaluate it on a real-world Emergency Medical Services (EMS) dataset and a public electronic health record (EHR) dataset. Results show that DKEC outperforms the state-of-the-art label-wise attention networks and transformer models of different sizes, particularly for the few-shot classes. More importantly, it helps the smaller language models achieve comparable performance to large language models.

------------

`[2311.09105] MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation <https://arxiv.org/abs/2311.09105>`__ MAVEN-Arg:基于事件参数标注的一体化事件理解数据集拼图

::

    replaced with revised version Tue, 18 Jun 2024 22:15:39 GMT
    Submission history From: Xiaozhi Wang [view email]
    [v1] Wed, 15 Nov 2023 16:52:14 UTC (171 KB)
    [v2] Tue, 18 Jun 2024 22:15:39 UTC (371 KB)
    Xiaozhi Wang, Hao Peng, Yong Guan, Kaisheng Zeng, Jianhui Chen, Lei Hou, Xu Han, Yankai Lin, Zhiyuan Liu, Ruobing Xie, Jie Zhou, Juanzi Li

Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and codes can be obtained from this https URL.

------------

`[2401.06837] Structsum Generation for Faster Text Comprehension <https://arxiv.org/abs/2401.06837>`__ 生成Structsum以加快文本理解

::

    replaced with revised version Wed, 19 Jun 2024 09:59:51 GMT
    Submission history From: Parag Jain [view email]
    [v1] Fri, 12 Jan 2024 17:43:51 UTC (7,513 KB)
    [v2] Wed, 19 Jun 2024 09:59:51 UTC (7,647 KB)
    Parag Jain, Andreea Marzoca, Francesco Piccinno

We consider the task of generating structured representations of text using large language models (LLMs). We focus on tables and mind maps as representative modalities. Tables are more organized way of representing data, while mind maps provide a visually dynamic and flexible approach, particularly suitable for sparse content. Despite the effectiveness of LLMs on different tasks, we show that current models struggle with generating structured outputs. In response, we present effective prompting strategies for both of these tasks. We introduce a taxonomy of problems around factuality, global and local structure, common to both modalities and propose a set of critiques to tackle these issues resulting in an absolute improvement in accuracy of +37pp (79%) for mind maps and +15pp (78%) for tables. To evaluate semantic coverage of generated structured representations we propose Auto-QA, and we verify the adequacy of Auto-QA using SQuAD dataset. We further evaluate the usefulness of structured representations via a text comprehension user study. The results show a significant reduction in comprehension time compared to text when using table (42.9%) and mind map (31.9%), without loss in accuracy.

------------

`[2401.07944] SemEval-2017 Task 4: Sentiment Analysis in Twitter using BERT <https://arxiv.org/abs/2401.07944>`__ SemEval-2017任务4:基于BERT的Twitter情感分析

::

    replaced with revised version Wed, 19 Jun 2024 15:41:58 GMT
    Submission history From: Rupak Kumar Das [view email]
    [v1] Mon, 15 Jan 2024 20:17:31 UTC (357 KB)
    [v2] Wed, 19 Jun 2024 15:41:58 UTC (352 KB)
    Rupak Kumar Das, Dr. Ted Pedersen

This paper uses the BERT model, which is a transformer-based architecture, to solve task 4A, English Language, Sentiment Analysis in Twitter of SemEval2017. BERT is a very powerful large language model for classification tasks when the amount of training data is small. For this experiment, we have used the BERT(BASE) model, which has 12 hidden layers. This model provides better accuracy, precision, recall, and f1 score than the Naive Bayes baseline model. It performs better in binary classification subtasks than the multi-class classification subtasks. We also considered all kinds of ethical issues during this experiment, as Twitter data contains personal and sensible information. The dataset and code used in our experiment can be found in this GitHub repository.

------------

`[2402.04957] Reconfidencing LLMs from the Grouping Loss Perspective <https://arxiv.org/abs/2402.04957>`__ 从分组损失的角度重新信任llm

::

    replaced with revised version Tue, 18 Jun 2024 20:31:56 GMT
    Submission history From: Lihu Chen [view email]
    [v1] Wed, 7 Feb 2024 15:40:22 UTC (1,155 KB)
    [v2] Tue, 18 Jun 2024 20:31:56 UTC (1,211 KB)
    Lihu Chen, Alexandre Perez-Lebel, Fabian M. Suchanek, Ga\"el Varoquaux

Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss. In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and LLaMA. Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query. In uncertainty-quantification theory, this is grouping loss. To address this, we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss. The LLMs, after the reconfidencing process, indicate improved confidence alignment with the accuracy of their responses.

------------

`[2402.10663] Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL <https://arxiv.org/abs/2402.10663>`__ 通过人工无关的文本到sql融合提高演示多样性

::

    replaced with revised version Wed, 19 Jun 2024 08:02:23 GMT
    Submission history From: Dingzirui Wang [view email]
    [v1] Fri, 16 Feb 2024 13:13:18 UTC (310 KB)
    [v2] Wed, 19 Jun 2024 08:02:23 UTC (410 KB)
    Dingzirui Wang, Longxu Dou, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che

Currently, the in-context learning method based on large language models (LLMs) has become the mainstream of text-to-SQL research. Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool. However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead. Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL. We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments. Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost. Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on several mainstream datasets, which proves the effectiveness of Fused.

------------

`[2402.12590] Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation <https://arxiv.org/abs/2402.12590>`__ 不断进化的AI集体来增强人类多样性并实现自我调节

::

    replaced with revised version Tue, 18 Jun 2024 23:28:46 GMT
    Submission history From: Yujin Potter [view email]
    [v1] Mon, 19 Feb 2024 22:59:43 UTC (778 KB)
    [v2] Tue, 18 Jun 2024 23:28:46 UTC (1,046 KB)
    Shiyang Lai, Yujin Potter, Junsol Kim, Richard Zhuang, Dawn Song, James Evans

Large language model behavior is shaped by the language of those with whom they interact. This capacity and their increasing prevalence online portend that they will intentionally or unintentionally "program" one another and form emergent AI subjectivities, relationships, and collectives. Here, we call upon the research community to investigate these "societies" of interacting artificial intelligences to increase their rewards and reduce their risks for human society and the health of online environments. We use a small "community" of models and their evolving outputs to illustrate how such emergent, decentralized AI collectives can spontaneously expand the bounds of human diversity and reduce the risk of toxic, anti-social behavior online. Finally, we discuss opportunities for AI cross-moderation and address ethical issues and design challenges associated with creating and maintaining free-formed AI collectives.

------------

`[2402.12821] Identifying Factual Inconsistencies in Summaries: Grounding Model Inference via Task Taxonomy <https://arxiv.org/abs/2402.12821>`__ 识别摘要中的事实不一致:基于任务分类法的模型推理基础

::

    replaced with revised version Thu, 20 Jun 2024 03:45:51 GMT
    Submission history From: Liyan Xu [view email]
    [v1] Tue, 20 Feb 2024 08:41:23 UTC (517 KB)
    [v2] Thu, 20 Jun 2024 03:45:51 UTC (223 KB)
    Liyan Xu, Zhenlin Su, Mo Yu, Jin Xu, Jinho D. Choi, Jie Zhou, Fei Liu

Factual inconsistencies pose a significant hurdle for the faithful summarization by generative models. While a major direction to enhance inconsistency detection is to derive stronger Natural Language Inference (NLI) models, we propose an orthogonal aspect that underscores the importance of incorporating task-specific taxonomy into the inference. To this end, we consolidate key error types of inconsistent facts in summaries, and incorporate them to facilitate both the zero-shot and supervised paradigms of LLMs. Extensive experiments on ten datasets of five distinct domains suggest that, zero-shot LLM inference could benefit from the explicit solution space depicted by the error type taxonomy, and achieves state-of-the-art performance overall, surpassing specialized non-LLM baselines, as well as recent LLM baselines. We further distill models that fuse the taxonomy into parameters through our designed prompt completions and supervised training strategies, efficiently substituting state-of-the-art zero-shot inference with much larger LLMs.

------------

`[2402.14857] Is the System Message Really Important to Jailbreaks in Large Language Models? <https://arxiv.org/abs/2402.14857>`__ 在大型语言模型中，系统消息对越狱真的很重要吗?

::

    replaced with revised version Tue, 18 Jun 2024 19:22:19 GMT
    Submission history From: Xiaotian Zou [view email]
    [v1] Tue, 20 Feb 2024 17:39:40 UTC (203 KB)
    [v2] Tue, 18 Jun 2024 19:22:19 UTC (416 KB)
    Xiaotian Zou, Yongkang Chen, Ke Li

The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named "Jailbreak". This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Most existing research focus on generating jailbreak prompts but system message configurations vary significantly in experiments. In this paper, we aim to answer a question: Is the system message really important for jailbreaks in LLMs? We conduct experiments in mainstream LLMs to generate jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreaks. Therefore, we explore the transferability of jailbreaks across LLMs with different system messages. Furthermore, we propose the System Messages Evolutionary Algorithm (SMEA) to generate system messages that are more resistant to jailbreak prompts, even with minor changes. Through SMEA, we get a robust system messages population with little change in the length of system messages. Our research not only bolsters LLMs security but also raises the bar for jailbreaks, fostering advancements in this field of study.

------------

`[2402.14897] Chain-of-Thought Unfaithfulness as Disguised Accuracy <https://arxiv.org/abs/2402.14897>`__ 用思维链的不忠来伪装准确

::

    replaced with revised version Wed, 19 Jun 2024 17:49:54 GMT
    Submission history From: Oliver Bentham [view email]
    [v1] Thu, 22 Feb 2024 17:23:53 UTC (3,936 KB)
    [v2] Wed, 19 Jun 2024 17:49:54 UTC (2,776 KB)
    [v3] Fri, 21 Jun 2024 13:39:14 UTC (2,776 KB)
    Oliver Bentham, Nathan Stringham, Ana Marasovi\'c

Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, Lanham et al. (2023) propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate the experimental setup in their section focused on scaling experiments with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, after normalizing the metric to account for a model's bias toward certain answer choices, unfaithfulness drops significantly for smaller less-capable models. This normalized faithfulness metric is also strongly correlated ($R^2$=0.74) with accuracy, raising doubts about its validity for evaluating faithfulness.

------------

`[2402.15537] Evaluating the Performance of ChatGPT for Spam Email Detection <https://arxiv.org/abs/2402.15537>`__ ChatGPT在垃圾邮件检测中的性能评估

::

    replaced with revised version Wed, 19 Jun 2024 14:49:09 GMT
    Submission history From: Shijing Si [view email]
    [v1] Fri, 23 Feb 2024 04:52:08 UTC (1,087 KB)
    [v2] Wed, 19 Jun 2024 14:49:09 UTC (228 KB)
    Shijing Si, Yuwei Wu, Le Tang, Yugui Zhang, Jedrek Wosik

Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the number of demonstrations in the prompt affects the performance of ChatGPT. For comparison, we also implement five popular benchmark methods, including naive Bayes, support vector machines (SVM), logistic regression (LR), feedforward dense neural networks (DNN), and BERT classifiers. Through extensive experiments, the performance of ChatGPT is significantly worse than deep supervised learning methods in the large English dataset, while it presents superior performance on the low-resourced Chinese dataset.

------------

`[2402.16159] DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem <https://arxiv.org/abs/2402.16159>`__ DistALANER:开源软件生态系统中远程监督主动学习增强的命名实体识别

::

    replaced with revised version Thu, 20 Jun 2024 12:43:57 GMT
    Submission history From: Somnath Banerjee [view email]
    [v1] Sun, 25 Feb 2024 17:40:49 UTC (9,323 KB)
    [v2] Mon, 11 Mar 2024 08:11:08 UTC (2,142 KB)
    [v3] Fri, 15 Mar 2024 18:29:52 UTC (2,142 KB)
    [v4] Tue, 28 May 2024 07:54:44 UTC (1,389 KB)
    [v5] Thu, 20 Jun 2024 12:43:57 UTC (1,390 KB)
    Somnath Banerjee, Avik Dutta, Aaditya Agrawal, Rima Hazra, Animesh Mukherjee

With the AI revolution in place, the trend for building automated systems to support professionals in different domains such as the open source software systems, healthcare systems, banking systems, transportation systems and many others have become increasingly prominent. A crucial requirement in the automation of support tools for such systems is the early identification of named entities, which serves as a foundation for developing specialized functionalities. However, due to the specific nature of each domain, different technical terminologies and specialized languages, expert annotation of available data becomes expensive and challenging. In light of these challenges, this paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our model significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.

------------

`[2402.16602] Rethinking Negative Instances for Generative Named Entity Recognition <https://arxiv.org/abs/2402.16602>`__ 生成命名实体识别中负面实例的再思考

::

    replaced with revised version Wed, 19 Jun 2024 03:16:58 GMT
    Submission history From: Yuyang Ding [view email]
    [v1] Mon, 26 Feb 2024 14:30:37 UTC (138 KB)
    [v2] Wed, 19 Jun 2024 03:16:58 UTC (133 KB)
    Yuyang Ding, Juntao Li, Pinzheng Wang, Zecheng Tang, Bowen Yan, Min Zhang

Large Language Models (LLMs) have demonstrated impressive capabilities for generalizing in unseen tasks. In the Named Entity Recognition (NER) task, recent advancements have seen the remarkable improvement of LLMs in a broad range of entity domains via instruction tuning, by adopting entity-centric schema. In this work, we explore the potential enhancement of the existing methods by incorporating negative instances into training. Our experiments reveal that negative instances contribute to remarkable improvements by (1) introducing contextual information, and (2) clearly delineating label boundaries. Furthermore, we introduce an efficient longest common subsequence (LCS) matching algorithm, which is tailored to transform unstructured predictions into structured entities. By integrating these components, we present GNER, a Generative NER system that shows improved zero-shot performance across unseen entity domains. Our comprehensive evaluation illustrates our system's superiority, surpassing state-of-the-art (SoTA) methods by 9 $F_1$ score in zero-shot evaluation.

------------

`[2403.02966] Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering <https://arxiv.org/abs/2403.02966>`__ 面向知识增强零样本问答的证据事实摘要

::

    replaced with revised version Wed, 19 Jun 2024 06:47:32 GMT
    Submission history From: Sungho Ko [view email]
    [v1] Tue, 5 Mar 2024 13:43:58 UTC (6,745 KB)
    [v2] Wed, 19 Jun 2024 06:47:32 UTC (5,380 KB)
    Sungho Ko, Hyunjin Cho, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee

Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.

------------

`[2403.07088] SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation <https://arxiv.org/abs/2403.07088>`__ SPA:面向计算友好的云端和设备上协作Seq2seq个性化生成

::

    replaced with revised version Thu, 20 Jun 2024 06:01:25 GMT
    Submission history From: Yanming Liu [view email]
    [v1] Mon, 11 Mar 2024 18:26:02 UTC (1,663 KB)
    [v2] Sat, 25 May 2024 11:19:31 UTC (910 KB)
    [v3] Thu, 30 May 2024 05:21:23 UTC (910 KB)
    [v4] Sun, 16 Jun 2024 19:27:11 UTC (910 KB)
    [v5] Thu, 20 Jun 2024 06:01:25 UTC (590 KB)
    Yanming Liu, Xinyue Peng, Jiannan Cao, Le Dai, Xingzu Liu, Ruilin Nong, Weihao Liu

Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require substantial memory storage on low-resource devices. More critically, the computational speed on these devices is also severely limited. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and featured personal feature. Further more, SPA provides a framework to keep feature-base parameters on low computational devices while leave the parameters containing general information on the high computational devices.

------------

`[2403.07794] SIT: Fine-tuning Large Language Models with Sequential Instructions <https://arxiv.org/abs/2403.07794>`__ SIT:基于顺序指令的大型语言模型微调

::

    replaced with revised version Thu, 20 Jun 2024 17:53:29 GMT
    Submission history From: Hanxu Hu [view email]
    [v1] Tue, 12 Mar 2024 16:33:30 UTC (1,526 KB)
    [v2] Thu, 20 Jun 2024 17:53:29 UTC (902 KB)
    Hanxu Hu, Simon Yu, Pinzhen Chen, Edoardo M. Ponti

Despite the success of existing instruction-tuned models, we find that they usually struggle to respond to queries with multiple instructions. This impairs their performance in complex problems whose solution consists of multiple intermediate tasks. Thus, we contend that part of the fine-tuning data mixture should be sequential--containing a chain of interrelated tasks. We first approach sequential instruction tuning from a task-driven perspective, manually creating interpretable intermediate tasks for multilingual and visual question answering: namely "translate then predict" and "caption then answer". Next, we automate this process by turning instructions in existing datasets (e.g., Alpaca and FlanCoT) into diverse and complex sequential instructions, making our method general-purpose. Models that underwent our sequential instruction tuning show improved results in coding, maths, and open-ended generation. Moreover, we put forward a new benchmark named SeqEval to evaluate a model's ability to follow all the instructions in a sequence, which further corroborates the benefits of our fine-tuning method. We hope that our endeavours will open new research avenues on instruction tuning for complex tasks.

------------

`[2403.08010] Debatrix: Multi-dimensional Debate Judge with Iterative Chronological Analysis Based on LLM <https://arxiv.org/abs/2403.08010>`__ Debatrix:基于LLM迭代时间分析的多维辩论裁判

::

    replaced with revised version Wed, 19 Jun 2024 19:39:42 GMT
    Submission history From: Jingcong Liang [view email]
    [v1] Tue, 12 Mar 2024 18:19:47 UTC (887 KB)
    [v2] Sat, 20 Apr 2024 14:32:00 UTC (1,020 KB)
    [v3] Wed, 19 Jun 2024 19:39:42 UTC (1,043 KB)
    Jingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing Huang and Zhongyu Wei

How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments. At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate. In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration. To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our system's performance to actual debate outcomes. The findings indicate a notable enhancement over directly using LLMs for debate evaluation. Source code and benchmark data are available online at this https URL .

------------

`[2403.10258] Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models <https://arxiv.org/abs/2403.10258>`__ 你只需要翻译吗?用大型语言模型解决多语言任务的研究

::

    replaced with revised version Thu, 20 Jun 2024 11:09:42 GMT
    Submission history From: Chaoqun Liu [view email]
    [v1] Fri, 15 Mar 2024 12:47:39 UTC (8,417 KB)
    [v2] Thu, 20 Jun 2024 11:09:42 UTC (8,817 KB)
    Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan Luu, Lidong Bing

Large language models (LLMs) have demonstrated multilingual capabilities; yet, they are mostly English-centric due to the imbalanced training corpora. Existing works leverage this phenomenon to improve their multilingual performances through translation, primarily on natural language processing (NLP) tasks. This work extends the evaluation from NLP tasks to real user queries and from English-centric LLMs to non-English-centric LLMs. While translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios. For culture-related tasks that need deep language understanding, prompting in the native language tends to be more promising as it better captures the nuances of culture and language. Our experiments reveal varied behaviors among different LLMs and tasks in the multilingual context. Therefore, we advocate for more comprehensive multilingual evaluation and more efforts toward developing multilingual LLMs beyond English-centric ones.

------------

`[2404.01247] An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance <https://arxiv.org/abs/2404.01247>`__ 

::

    replaced with revised version Wed, 19 Jun 2024 18:07:19 GMT
    Submission history From: Simran Khanuja [view email]
    [v1] Mon, 1 Apr 2024 17:08:50 UTC (30,558 KB)
    [v2] Sun, 19 May 2024 01:15:00 UTC (30,558 KB)
    [v3] Wed, 19 Jun 2024 18:07:19 UTC (31,893 KB)
    Simran Khanuja, Sathyanarayanan Ramamoorthy, Yueqi Song, Graham Neubig

Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: this https URL.

------------

`[2404.12933] Cross-cultural Inspiration Detection and Analysis in Real and LLM-generated Social Media Data <https://arxiv.org/abs/2404.12933>`__ 真实和llm生成的社交媒体数据中的跨文化灵感检测和分析

::

    replaced with revised version Wed, 19 Jun 2024 03:27:43 GMT
    Submission history From: Oana Ignat [view email]
    [v1] Fri, 19 Apr 2024 15:04:30 UTC (1,824 KB)
    [v2] Wed, 19 Jun 2024 03:27:43 UTC (1,696 KB)
    Oana Ignat, Gayathri Ganesh Lakshmy, Rada Mihalcea

Inspiration is linked to various positive outcomes, such as increased creativity, productivity, and happiness. Although inspiration has great potential, there has been limited effort toward identifying content that is inspiring, as opposed to just engaging or positive. Additionally, most research has concentrated on Western data, with little attention paid to other cultures. This work is the first to study cross-cultural inspiration through machine learning methods. We aim to identify and analyze real and AI-generated cross-cultural inspiring posts. To this end, we compile and make publicly available the InspAIred dataset, which consists of 2,000 real inspiring posts, 2,000 real non-inspiring posts, and 2,000 generated inspiring posts evenly distributed across India and the UK. The real posts are sourced from Reddit, while the generated posts are created using the GPT-4 model. Using this dataset, we conduct extensive computational linguistic analyses to (1) compare inspiring content across cultures, (2) compare AI-generated inspiring posts to real inspiring posts, and (3) determine if detection models can accurately distinguish between inspiring content across cultures and data sources.

------------

`[2404.12938] MAiDE-up: Multilingual Deception Detection of GPT-generated Hotel Reviews <https://arxiv.org/abs/2404.12938>`__ MAiDE-up:基于gpt生成酒店评论的多语言欺骗检测

::

    replaced with revised version Wed, 19 Jun 2024 03:34:42 GMT
    Submission history From: Oana Ignat [view email]
    [v1] Fri, 19 Apr 2024 15:08:06 UTC (1,466 KB)
    [v2] Wed, 19 Jun 2024 03:34:42 UTC (1,062 KB)
    Oana Ignat, Xiaomeng Xu, Rada Mihalcea

Deceptive reviews are becoming increasingly common, especially given the increase in performance and the prevalence of LLMs. While work to date has addressed the development of models to differentiate between truthful and deceptive human reviews, much less is known about the distinction between real reviews and AI-authored fake reviews. Moreover, most of the research so far has focused primarily on English, with very little work dedicated to other languages. In this paper, we compile and make publicly available the MAiDE-up dataset, consisting of 10,000 real and 10,000 AI-generated fake hotel reviews, balanced across ten languages. Using this dataset, we conduct extensive linguistic analyses to (1) compare the AI fake hotel reviews to real hotel reviews, and (2) identify the factors that influence the deception detection model performance. We explore the effectiveness of several models for deception detection in hotel reviews across three main dimensions: sentiment, location, and language. We find that these dimensions influence how well we can detect AI-generated fake reviews.

------------

`[2405.01943] Dependency-Aware Semi-Structured Sparsity: Declining Roles of Outliers in Pruning GLU-based LLMs <https://arxiv.org/abs/2405.01943>`__ 依赖感知半结构化稀疏:离群点在修剪基于glue的llm中的作用下降

::

    replaced with revised version Thu, 20 Jun 2024 06:57:29 GMT
    Submission history From: Zhiyu Guo [view email]
    [v1] Fri, 3 May 2024 09:13:13 UTC (1,045 KB)
    [v2] Thu, 20 Jun 2024 06:57:29 UTC (443 KB)
    Zhiyu Guo, Hidetaka Kamigaito, Taro Wanatnabe

The rapid growth in the scale of Large Language Models (LLMs) has led to significant computational and memory costs, making model compression techniques such as network pruning increasingly crucial for their efficient deployment. Recent LLMs such as LLaMA2 and Mistral have adopted GLU-based MLP architectures. However, current LLM pruning strategies are primarily based on insights from older LLM architectures, necessitating a reevaluation of these strategies to suit the new architectural characteristics. Contrary to traditional beliefs, we find that outliers play a diminished role in the input projections of GLU-based MLPs. Leveraging this new insight, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel pruning method for GLU-based LLMs. DaSS balances the flexibility of unstructured pruning and the structural consistency of dependency-based structured pruning by considering both of weight magnitude and corresponding intermediate activation norms in weight pruning metric. Empirical evaluations on the Mistral, Gemma, and LLaMA2 model families demonstrate the consistent effectiveness of DaSS in the prevailing GLU variants.

------------

`[2405.03371] Explainable Fake News Detection With Large Language Model via Defense Among Competing Wisdom <https://arxiv.org/abs/2405.03371>`__ 基于竞争智慧防御的大型语言模型可解释假新闻检测

::

    replaced with revised version Thu, 20 Jun 2024 04:33:33 GMT
    Submission history From: Bo Wang [view email]
    [v1] Mon, 6 May 2024 11:24:13 UTC (173 KB)
    [v2] Thu, 20 Jun 2024 04:33:33 UTC (173 KB)
    Bo Wang, Jing Ma, Hongzhan Lin, Zhiwei Yang, Ruichao Yang, Yuan Tian, Yi Chang

Most fake news detection methods learn latent feature representations based on neural networks, which makes them black boxes to classify a piece of news without giving any justification. Existing explainable systems generate veracity justifications from investigative journalism, which suffer from debunking delayed and low efficiency. Recent studies simply assume that the justification is equivalent to the majority opinions expressed in the wisdom of crowds. However, the opinions typically contain some inaccurate or biased information since the wisdom of crowds is uncensored. To detect fake news from a sea of diverse, crowded and even competing narratives, in this paper, we propose a novel defense-based explainable fake news detection framework. Specifically, we first propose an evidence extraction module to split the wisdom of crowds into two competing parties and respectively detect salient evidences. To gain concise insights from evidences, we then design a prompt-based module that utilizes a large language model to generate justifications by inferring reasons towards two possible veracities. Finally, we propose a defense-based inference module to determine veracity via modeling the defense among these justifications. Extensive experiments conducted on two real-world benchmarks demonstrate that our proposed method outperforms state-of-the-art baselines in terms of fake news detection and provides high-quality justifications.

------------

`[2405.06258] Automatic Generation of Model and Data Cards: A Step Towards Responsible AI <https://arxiv.org/abs/2405.06258>`__ 

::

    replaced with revised version Wed, 19 Jun 2024 03:36:01 GMT
    Submission history From: Jiarui Liu [view email]
    [v1] Fri, 10 May 2024 06:14:07 UTC (1,568 KB)
    [v2] Wed, 19 Jun 2024 03:36:01 UTC (1,576 KB)
    Jiarui Liu, Wenkai Li, Zhijing Jin, Mona Diab

In an era of model and data proliferation in machine learning/AI especially marked by the rapid advancement of open-sourced technologies, there arises a critical need for standardized consistent documentation. Our work addresses the information incompleteness in current human-generated model and data cards. We propose an automated generation approach using Large Language Models (LLMs). Our key contributions include the establishment of CardBench, a comprehensive dataset aggregated from over 4.8k model cards and 1.4k data cards, coupled with the development of the CardGen pipeline comprising a two-step retrieval process. Our approach exhibits enhanced completeness, objectivity, and faithfulness in generated model and data cards, a significant step in responsible AI documentation practices ensuring better accountability and traceability.

------------

`[2405.08760] Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Non-Literal Intent Resolution in LLMs <https://arxiv.org/abs/2405.08760>`__ 教皇是天主教徒吗?是的，教皇是天主教徒。llm中非文字意图解决的生成式评估

::

    replaced with revised version Wed, 19 Jun 2024 19:07:47 GMT
    Submission history From: Akhila Yerukola [view email]
    [v1] Tue, 14 May 2024 16:48:56 UTC (306 KB)
    [v2] Wed, 19 Jun 2024 19:07:47 UTC (3,237 KB)
    Akhila Yerukola, Saujas Vaduguru, Daniel Fried, Maarten Sap

Humans often express their communicative intents indirectly or non-literally, which requires their interlocutors -- human or AI -- to understand beyond the literal meaning of words. While most existing work has focused on discriminative evaluations, we present a new approach to generatively evaluate large language models' (LLMs') intention understanding by examining their responses to non-literal utterances. Ideally, an LLM should respond in line with the true intention of a non-literal utterance, not its literal interpretation. Our findings show that LLMs struggle to generate pragmatically relevant responses to non-literal language, achieving only 50-55% accuracy on average. While explicitly providing oracle intentions significantly improves performance (e.g., 75% for Mistral-Instruct), this still indicates challenges in leveraging given intentions to produce appropriate responses. Using chain-of-thought to make models spell out intentions yields much smaller gains (60% for Mistral-Instruct). These findings suggest that LLMs are not yet effective pragmatic interlocutors, highlighting the need for better approaches for modeling intentions and utilizing them for pragmatic generation.

------------

`[2405.11357] Large Language Models Lack Understanding of Character Composition of Words <https://arxiv.org/abs/2405.11357>`__ 大型语言模型缺乏对单词字符组成的理解

::

    replaced with revised version Wed, 19 Jun 2024 16:58:14 GMT
    Submission history From: Andrew Shin [view email]
    [v1] Sat, 18 May 2024 18:08:58 UTC (54 KB)
    [v2] Wed, 19 Jun 2024 16:58:14 UTC (62 KB)
    Andrew Shin, Kunitake Kaneko

Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs' successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple tasks that can be handled by humans with perfection. We analyze their behaviors with comparison to token level performances, and discuss the potential directions for future research.

------------

`[2405.13929] Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for Russian <https://arxiv.org/abs/2405.13929>`__ Vikhr:开源指令调优俄语大型语言模型家族

::

    replaced with revised version Wed, 19 Jun 2024 17:32:23 GMT
    Submission history From: Aleksandr Nikolich [view email]
    [v1] Wed, 22 May 2024 18:58:58 UTC (7,238 KB)
    [v2] Wed, 19 Jun 2024 17:32:23 UTC (7,238 KB)
    Aleksandr Nikolich, Konstantin Korolev, Artem Shelmanov, Igor Kiselev

There has been a surge in the development of various Large Language Models (LLMs). However, text generation for languages other than English often faces significant challenges, including poor generation quality and the reduced computational performance due to the disproportionate representation of tokens in model's vocabulary. In this work, we address these issues and introduce Vikhr, a new state-of-the-art open-source instruction-tuned LLM designed specifically for the Russian language. Unlike previous efforts for Russian that utilize computationally inexpensive LoRA adapters on top of English-oriented models, Vikhr features an adapted tokenizer vocabulary and undergoes the continued pre-training and instruction tuning of all weights. This approach not only enhances the model's performance but also significantly improves its computational and contextual efficiency. The remarkable performance of Vikhr across various Russian-language benchmarks can also be attributed to our efforts in expanding instruction datasets and corpora for continued pre-training. Vikhr not only sets the new state of the art among open-source LLMs for Russian, but even outperforms some proprietary closed-source models on certain benchmarks. The model weights, instruction sets, and code are publicly available

------------

`[2405.19660] PATIENT-{\Psi}: Using Large Language Models to Simulate Patients for Training Mental Health Professionals <https://arxiv.org/abs/2405.19660>`__ PATIENT-{\Psi}:使用大型语言模型模拟患者以培训心理健康专业人员

::

    replaced with revised version Tue, 18 Jun 2024 22:33:48 GMT
    Submission history From: Zhiyu Chen [view email]
    [v1] Thu, 30 May 2024 03:20:56 UTC (7,087 KB)
    [v2] Tue, 18 Jun 2024 22:33:48 UTC (15,080 KB)
    Ruiyi Wang, Stephanie Milani, Jamie C. Chiu, Jiayin Zhi, Shaun M. Eack, Travis Labrum, Samuel M. Murphy, Nev Jones, Kate Hardy, Hong Shen, Fei Fang, Zhiyu Zoey Chen

Mental illness remains one of the most critical public health issues. Despite its importance, many mental health professionals highlight a disconnect between their training and actual real-world patient practice. To help bridge this gap, we propose PATIENT-{\Psi}, a novel patient simulation framework for cognitive behavior therapy (CBT) training. To build PATIENT-{\Psi}, we construct diverse patient cognitive models based on CBT principles and use large language models (LLMs) programmed with these cognitive models to act as a simulated therapy patient. We propose an interactive training scheme, PATIENT-{\Psi}-TRAINER, for mental health trainees to practice a key skill in CBT -- formulating the cognitive model of the patient -- through role-playing a therapy session with PATIENT-{\Psi}. To evaluate PATIENT-{\Psi}, we conducted a comprehensive user study of 13 mental health trainees and 20 experts. The results demonstrate that practice using PATIENT-{\Psi}-TRAINER enhances the perceived skill acquisition and confidence of the trainees beyond existing forms of training such as textbooks, videos, and role-play with non-patients. Based on the experts' perceptions, PATIENT-{\Psi} is perceived to be closer to real patient interactions than GPT-4, and PATIENT-{\Psi}-TRAINER holds strong promise to improve trainee competencies. Our code and data are released at \url{this https URL}.

------------

`[2405.19846] Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model <https://arxiv.org/abs/2405.19846>`__ Quest:面向大型语言模型长上下文扩展的以查询为中心的数据合成方法

::

    replaced with revised version Thu, 20 Jun 2024 02:26:48 GMT
    Submission history From: Chaochen Gao [view email]
    [v1] Thu, 30 May 2024 08:50:55 UTC (4,291 KB)
    [v2] Thu, 20 Jun 2024 02:26:48 UTC (4,596 KB)
    Chaochen Gao, Xing Wu, Qi Fu, Songlin Hu

Large language models, initially pre-trained with a limited context length, can better handle longer texts by continuing training on a corpus with extended contexts. However, obtaining effective long-context data is challenging due to the scarcity and uneven distribution of long documents across different domains. To address this issue, we propose a Query-centric data synthesis method, abbreviated as Quest. Quest is an interpretable method based on the observation that documents retrieved by similar queries are relevant but low-redundant, thus well-suited for synthesizing long-context data. The method is also scalable and capable of constructing large amounts of long-context data. Using Quest, we synthesize a long-context dataset up to 128k context length, significantly outperforming other data synthesis methods on multiple long-context benchmark datasets. In addition, we further verify that the Quest method is predictable through scaling law experiments, making it a reliable solution for advancing long-context models.

------------

`[2406.05460] Fighting Against the Repetitive Training and Sample Dependency Problem in Few-shot Named Entity Recognition <https://arxiv.org/abs/2406.05460>`__ 解决小样本命名实体识别中的重复训练和样本依赖问题

::

    replaced with revised version Tue, 18 Jun 2024 23:45:14 GMT
    Submission history From: Chang Tian [view email]
    [v1] Sat, 8 Jun 2024 12:36:30 UTC (6,760 KB)
    [v2] Tue, 18 Jun 2024 23:45:14 UTC (6,760 KB)
    Chang Tian, Wenpeng Yin, Dan Li, Marie-Francine Moens

Few-shot named entity recognition (NER) systems recognize entities using a few labeled training examples. The general pipeline consists of a span detector to identify entity spans in text and an entity-type classifier to assign types to entities. Current span detectors rely on extensive manual labeling to guide training. Almost every span detector requires initial training on basic span features followed by adaptation to task-specific features. This process leads to repetitive training of the basic span features among span detectors. Additionally, metric-based entity-type classifiers, such as prototypical networks, typically employ a specific metric that gauges the distance between the query sample and entity-type referents, ultimately assigning the most probable entity type to the query sample. However, these classifiers encounter the sample dependency problem, primarily stemming from the limited samples available for each entity-type referent. To address these challenges, we proposed an improved few-shot NER pipeline. First, we introduce a steppingstone span detector that is pre-trained on open-domain Wikipedia data. It can be used to initialize the pipeline span detector to reduce the repetitive training of basic features. Second, we leverage a large language model (LLM) to set reliable entity-type referents, eliminating reliance on few-shot samples of each type. Our model exhibits superior performance with fewer training steps and human-labeled data compared with baselines, as demonstrated through extensive experiments on various datasets. Particularly in fine-grained few-shot NER settings, our model outperforms strong baselines, including ChatGPT. We will publicly release the code, datasets, LLM outputs, and model checkpoints.

------------

`[2406.06840] Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles <https://arxiv.org/abs/2406.06840>`__ 无声信号，巨大冲击:用于编码狗哨词义消歧的llm

::

    replaced with revised version Tue, 18 Jun 2024 19:11:11 GMT
    Submission history From: Julia Kruk [view email]
    [v1] Mon, 10 Jun 2024 23:09:19 UTC (1,776 KB)
    [v2] Tue, 18 Jun 2024 19:11:11 UTC (1,776 KB)
    Julia Kruk, Michela Marchini, Rijul Magu, Caleb Ziems, David Muchlinski, Diyi Yang

A dog whistle is a form of coded communication that carries a secondary meaning to specific audiences and is often weaponized for racial and socioeconomic discrimination. Dog whistling historically originated from United States politics, but in recent years has taken root in social media as a means of evading hate speech detection systems and maintaining plausible deniability. In this paper, we present an approach for word-sense disambiguation of dog whistles from standard speech using Large Language Models (LLMs), and leverage this technique to create a dataset of 16,550 high-confidence coded examples of dog whistles used in formal and informal communication. Silent Signals is the largest dataset of disambiguated dog whistle usage, created for applications in hate speech detection, neology, and political science.
The dataset can be found at this https URL.

------------

`[2406.10594] BlockPruner: Fine-grained Pruning for Large Language Models <https://arxiv.org/abs/2406.10594>`__ BlockPruner:大型语言模型的细粒度剪枝

::

    replaced with revised version Thu, 20 Jun 2024 04:25:49 GMT
    Submission history From: Longguang Zhong [view email]
    [v1] Sat, 15 Jun 2024 11:03:33 UTC (387 KB)
    [v2] Thu, 20 Jun 2024 04:25:49 UTC (387 KB)
    Longguang Zhong, Fanqi Wan, Ruijun Chen, Xiaojun Quan, Liangzhi Li

With the rapid growth in the size and complexity of large language models (LLMs), the costs associated with their training and inference have escalated significantly. Research indicates that certain layers in LLMs harbor substantial redundancy, and pruning these layers has minimal impact on the overall performance. While various layer pruning methods have been developed based on this insight, they generally overlook the finer-grained redundancies within the layers themselves. In this paper, we delve deeper into the architecture of LLMs and demonstrate that finer-grained pruning can be achieved by targeting redundancies in multi-head attention (MHA) and multi-layer perceptron (MLP) blocks. We propose a novel, training-free structured pruning approach called BlockPruner. Unlike existing layer pruning methods, BlockPruner segments each Transformer layer into MHA and MLP blocks. It then assesses the importance of these blocks using perplexity measures and applies a heuristic search for iterative pruning. We applied BlockPruner to LLMs of various sizes and architectures and validated its performance across a wide range of downstream tasks. Experimental results show that BlockPruner achieves more granular and effective pruning compared to state-of-the-art baselines.

------------

`[2406.11097] InstructCMP: Length Control in Sentence Compression through Instruction-based Large Language Models <https://arxiv.org/abs/2406.11097>`__ InstructCMP:基于指令的大型语言模型在句子压缩中的长度控制

::

    replaced with revised version Tue, 18 Jun 2024 18:35:52 GMT
    Submission history From: Juseon Do [view email]
    [v1] Sun, 16 Jun 2024 23:00:47 UTC (7,885 KB)
    [v2] Tue, 18 Jun 2024 18:35:52 UTC (7,885 KB)
    Juseon-Do, Jingun Kwon, Hidetaka Kamigaito, Manabu Okumura

Extractive summarization can produce faithful summaries but often requires additional constraints such as a desired summary length. Traditional sentence compression models do not typically consider the constraints because of their restricted model abilities, which require model modifications for coping with them. To bridge this gap, we propose Instruction-based Compression (InstructCMP), an approach to the sentence compression task that can consider the length constraint through instructions by leveraging the zero-shot task-solving abilities of Large Language Models (LLMs). For this purpose, we created new evaluation datasets by transforming traditional sentence compression datasets into an instruction format. By using the datasets, we first reveal that the current LLMs still face challenges in accurately controlling the length for a compressed text. To address this issue, we propose an approach named "length priming," that incorporates additional length information into the instructions without external resources. While the length priming effectively works in a zero-shot setting, a training dataset with the instructions would further improve the ability of length control. Thus, we additionally created a training dataset in an instruction format to fine-tune the model on it. Experimental results and analysis show that applying the length priming significantly improves performances of InstructCMP in both zero-shot and fine-tuning settings without the need of any model modifications.

------------

`[2406.11131] Are Large Language Models a Good Replacement of Taxonomies? <https://arxiv.org/abs/2406.11131>`__ 大型语言模型是分类法的良好替代品吗?

::

    replaced with revised version Thu, 20 Jun 2024 08:01:14 GMT
    Submission history From: Yushi Sun [view email]
    [v1] Mon, 17 Jun 2024 01:21:50 UTC (786 KB)
    [v2] Thu, 20 Jun 2024 08:01:14 UTC (786 KB)
    Yushi Sun, Hao Xin, Kai Sun, Yifan Ethan Xu, Xiao Yang, Xin Luna Dong, Nan Tang, Lei Chen

Large language models (LLMs) demonstrate an impressive ability to internalize knowledge and answer natural language questions. Although previous studies validate that LLMs perform well on general knowledge while presenting poor performance on long-tail nuanced knowledge, the community is still doubtful about whether the traditional knowledge graphs should be replaced by LLMs. In this paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made obsolete by LLMs. Intuitively, LLMs should perform well on common taxonomies and at taxonomy levels that are common to people. Unfortunately, there lacks a comprehensive benchmark that evaluates the LLMs over a wide range of taxonomies from common to specialized domains and at levels from root to leaf so that we can draw a confident conclusion. To narrow the research gap, we constructed a novel taxonomy hierarchical structure discovery benchmark named TaxoGlimpse to evaluate the performance of LLMs over taxonomies. TaxoGlimpse covers ten representative taxonomies from common to specialized domains with in-depth experiments of different levels of entities in this taxonomy from root to leaf. Our comprehensive experiments of eighteen state-of-the-art LLMs under three prompting settings validate that LLMs can still not well capture the knowledge of specialized taxonomies and leaf-level entities.

------------

`[2406.11354] Preserving Knowledge in Large Language Model with Model-Agnostic Self-Decompression <https://arxiv.org/abs/2406.11354>`__ 基于模型无关自解压的大型语言模型知识保持

::

    replaced with revised version Wed, 19 Jun 2024 11:36:30 GMT
    Submission history From: Zilun Zhang [view email]
    [v1] Mon, 17 Jun 2024 09:17:40 UTC (433 KB)
    [v2] Wed, 19 Jun 2024 11:36:30 UTC (433 KB)
    Zilun Zhang, Yutao Sun, Tiancheng Zhao, Leigang Sha, Ruochen Xu, Kyusong Lee, Jianwei Yin

Humans can retain old knowledge while learning new information, but Large Language Models (LLMs) often suffer from catastrophic forgetting when post-pretrained or supervised fine-tuned (SFT) on domain-specific data. Moreover, for Multimodal Large Language Models (MLLMs) which are composed of the LLM base and visual projector (e.g. LLaVA), a significant decline in performance on language benchmarks was observed compared to their single-modality counterparts. To address these challenges, we introduce a novel model-agnostic self-decompression method, Tree Generation (TG), that decompresses knowledge within LLMs into the training corpus. This paper focuses on TG-SFT, which can synthetically generate SFT data for the instruction tuning steps. By incorporating the dumped corpus during SFT for MLLMs, we significantly reduce the forgetting problem.

------------

`[2406.11565] Extrinsic Evaluation of Cultural Competence in Large Language Models <https://arxiv.org/abs/2406.11565>`__ 大型语言模型文化能力的外在评估

::

    replaced with revised version Wed, 19 Jun 2024 05:18:56 GMT
    Submission history From: Shaily Bhatt [view email]
    [v1] Mon, 17 Jun 2024 14:03:27 UTC (455 KB)
    [v2] Wed, 19 Jun 2024 05:18:56 UTC (455 KB)
    Shaily Bhatt and Fernando Diaz

Productive interactions between diverse users and language technologies require outputs from the latter to be culturally relevant and sensitive. Prior works have evaluated models' knowledge of cultural norms, values, and artifacts, without considering how this knowledge manifests in downstream applications. In this work, we focus on extrinsic evaluation of cultural competence in two text generation tasks, open-ended question answering and story generation. We quantitatively and qualitatively evaluate model outputs when an explicit cue of culture, specifically nationality, is perturbed in the prompts. Although we find that model outputs do vary when varying nationalities and feature culturally relevant words, we also find weak correlations between text similarity of outputs for different countries and the cultural values of these countries. Finally, we discuss important considerations in designing comprehensive evaluation of cultural competence in user-facing tasks.

------------

`[2406.11661] Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting <https://arxiv.org/abs/2406.11661>`__ 文化条件作用还是安慰剂?社会人口激励的有效性

::

    replaced with revised version Thu, 20 Jun 2024 08:25:08 GMT
    Submission history From: Sagnik Mukherjee [view email]
    [v1] Mon, 17 Jun 2024 15:43:45 UTC (12,338 KB)
    [v2] Thu, 20 Jun 2024 08:25:08 UTC (12,338 KB)
    Sagnik Mukherjee, Muhammad Farid Adilazuarda, Sunayana Sitaram, Kalika Bali, Alham Fikri Aji, Monojit Choudhury

Socio-demographic prompting is a commonly employed approach to study cultural biases in LLMs as well as for aligning models to certain cultures. In this paper, we systematically probe four LLMs (Llama 3, Mistral v0.2, GPT-3.5 Turbo and GPT-4) with prompts that are conditioned on culturally sensitive and non-sensitive cues, on datasets that are supposed to be culturally sensitive (EtiCor and CALI) or neutral (MMLU and ETHICS). We observe that all models except GPT-4 show significant variations in their responses on both kinds of datasets for both kinds of prompts, casting doubt on the robustness of the culturally-conditioned prompting as a method for eliciting cultural bias in models or as an alignment strategy. The work also calls rethinking the control experiment design to tease apart the cultural conditioning of responses from "placebo effect", i.e., random perturbations of model responses due to arbitrary tokens in the prompt.

------------

`[2406.12033] Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models <https://arxiv.org/abs/2406.12033>`__ 用大型语言模型揭示和减轻心理健康分析中的偏见

::

    replaced with revised version Wed, 19 Jun 2024 18:28:22 GMT
    Submission history From: Yuqing Wang [view email]
    [v1] Mon, 17 Jun 2024 19:05:32 UTC (250 KB)
    [v2] Wed, 19 Jun 2024 18:28:22 UTC (250 KB)
    Yuqing Wang, Yun Zhao, Sara Alessandra Keller, Anne de Hond, Marieke M. van Buchem, Malvika Pillai, Tina Hernandez-Boussard

The advancement of large language models (LLMs) has demonstrated strong capabilities across various applications, including mental health analysis. However, existing studies have focused on predictive performance, leaving the critical issue of fairness underexplored, posing significant risks to vulnerable populations. Despite acknowledging potential biases, previous works have lacked thorough investigations into these biases and their impacts. To address this gap, we systematically evaluate biases across seven social factors (e.g., gender, age, religion) using ten LLMs with different prompting methods on eight diverse mental health datasets. Our results show that GPT-4 achieves the best overall balance in performance and fairness among LLMs, although it still lags behind domain-specific models like MentalRoBERTa in some cases. Additionally, our tailored fairness-aware prompts can effectively mitigate bias in mental health predictions, highlighting the great potential for fair analysis in this field.

------------

`[2406.12746] Rationale-based Ensemble of Multiple QA Strategies for Zero-shot Knowledge-based VQA <https://arxiv.org/abs/2406.12746>`__ 基于理论基础的多QA策略集成的零样本知识库VQA

::

    replaced with revised version Wed, 19 Jun 2024 02:02:13 GMT
    Submission history From: Miaoyu Li [view email]
    [v1] Tue, 18 Jun 2024 16:06:38 UTC (1,443 KB)
    [v2] Wed, 19 Jun 2024 02:02:13 UTC (1,443 KB)
    Miaoyu Li, Haoxin Li, Zilin Du, and Boyang Li

Knowledge-based Visual Qustion-answering (K-VQA) necessitates the use of background knowledge beyond what is depicted in the image. Current zero-shot K-VQA methods usually translate an image to a single type of textual decision context and use a text-based model to answer the question based on it, which conflicts with the fact that K-VQA questions often require the combination of multiple question-answering strategies. In light of this, we propose Rationale-based Ensemble of Answer Context Tactics (REACT) to achieve a dynamic ensemble of multiple question-answering tactics, comprising Answer Candidate Generation (ACG) and Rationale-based Strategy Fusion (RSF). In ACG, we generate three distinctive decision contexts to provide different strategies for each question, resulting in the generation of three answer candidates. RSF generates automatic and mechanistic rationales from decision contexts for each candidate, allowing the model to select the correct answer from all candidates. We conduct comprehensive experiments on the OK-VQA and A-OKVQA datasets, and our method significantly outperforms state-of-the-art LLM-based baselines on all datasets.

------------

`[2404.03147] Eigenpruning: an Interpretability-Inspired PEFT Method <https://arxiv.org/abs/2404.03147>`__ Eigenpruning:一种基于可解释性的PEFT方法

::

    replaced with revised version Thu, 20 Jun 2024 09:32:43 GMT
    Submission history From: Tomás Vergara Browne [view email]
    [v1] Thu, 4 Apr 2024 01:42:28 UTC (163 KB)
    [v2] Mon, 29 Apr 2024 07:48:32 UTC (164 KB)
    [v3] Tue, 30 Apr 2024 01:12:37 UTC (164 KB)
    [v4] Sat, 15 Jun 2024 17:56:07 UTC (164 KB)
    [v5] Thu, 20 Jun 2024 09:32:43 UTC (164 KB)
    Tom\'as Vergara-Browne, \'Alvaro Soto, Akiko Aizawa

We introduce eigenpruning, a method that removes singular values from weight matrices in an LLM to improve its performance in a particular task. This method is inspired by interpretability methods designed to automatically find subnetworks of a model which solve a specific task. In our tests, the pruned model outperforms the original model by a large margin, while only requiring minimal computation to prune the weight matrices. In the case of a small synthetic task in integer multiplication, the Phi-2 model can improve its accuracy in the test set from 13.75% to 97.50%. Interestingly, these results seem to indicate the existence of a computation path that can solve the task very effectively, but it was not being used by the original model. Finally, we publicly release our implementation.

------------

`[2406.06385] Low-Rank Quantization-Aware Training for LLMs <https://arxiv.org/abs/2406.06385>`__ llm的低秩量化感知训练

::

    replaced with revised version Thu, 20 Jun 2024 15:18:50 GMT
    Submission history From: Yelysei Bondarenko [view email]
    [v1] Mon, 10 Jun 2024 15:44:22 UTC (952 KB)
    [v2] Thu, 20 Jun 2024 15:18:50 UTC (1,327 KB)
    Yelysei Bondarenko, Riccardo Del Chiaro, Markus Nagel

Large language models (LLMs) are omnipresent, however their practical deployment is challenging due to their ever increasing computational and memory demands. Quantization is one of the most effective ways to make them more compute and memory efficient. Quantization-aware training (QAT) methods, generally produce the best quantized performance, however it comes at the cost of potentially long training time and excessive memory usage, making it impractical when applying for LLMs. Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a lightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several components to save memory without sacrificing predictive performance: (a) low-rank auxiliary weights that are aware of the quantization grid; (b) a downcasting operator using fixed-point or double-packed integers and (c) checkpointing. Unlike most related work, our method (i) is inference-efficient, leading to no additional overhead compared to traditional PTQ; (ii) can be seen as a general extended pretraining framework, meaning that the resulting model can still be utilized for any downstream task afterwards; (iii) can be applied across a wide range of quantization settings, such as different choices quantization granularity, activation quantization, and seamlessly combined with many PTQ techniques. We apply LR-QAT to LLaMA-2/3 and Mistral model families and validate its effectiveness on several downstream tasks. Our method outperforms common post-training quantization (PTQ) approaches and reaches the same model performance as full-model QAT at the fraction of its memory usage. Specifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of memory.

------------

`[2406.06858] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion <https://arxiv.org/abs/2406.06858>`__ FLUX:通过内核融合实现gpu上基于软件的快速通信重叠

::

    replaced with revised version Tue, 18 Jun 2024 20:25:56 GMT
    Submission history From: Wenlei Bao Dr. [view email]
    [v1] Tue, 11 Jun 2024 00:17:39 UTC (8,300 KB)
    [v2] Wed, 12 Jun 2024 17:12:23 UTC (8,300 KB)
    [v3] Fri, 14 Jun 2024 01:46:04 UTC (8,300 KB)
    [v4] Tue, 18 Jun 2024 20:25:56 UTC (8,301 KB)
    Li-Wen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Ziheng Jiang, Haibin Lin, Xin Jin, Xin Liu

Large deep learning models have demonstrated strong ability to solve many tasks across a wide range of applications. Those large models typically require training and inference to be distributed. Tensor parallelism is a common technique partitioning computation of an operation or layer across devices to overcome the memory capacity limitation of a single processor, and/or to accelerate computation to meet a certain latency requirement. However, this kind of parallelism introduces additional communication that might contribute a significant portion of overall runtime. Thus limits scalability of this technique within a group of devices with high speed interconnects, such as GPUs with NVLinks in a node. This paper proposes a novel method, Flux, to significantly hide communication latencies with dependent computations for GPUs. Flux over-decomposes communication and computation operations into much finer-grained operations and further fuses them into a larger kernel to effectively hide communication without compromising kernel efficiency. Flux can potentially overlap up to 96% of communication given a fused kernel. Overall, it can achieve up to 1.24x speedups for training over Megatron-LM on a cluster of 128 GPUs with various GPU generations and interconnects, and up to 1.66x and 1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8 GPUs with various GPU generations and interconnects.

------------

`[2406.12091] Is poisoning a real threat to LLM alignment? Maybe more so than you think <https://arxiv.org/abs/2406.12091>`__ 中毒是LLM联盟的真正威胁吗?可能比你想象的更严重

::

    replaced with revised version Wed, 19 Jun 2024 17:56:17 GMT
    Submission history From: Pankayaraj Pathmanathan [view email]
    [v1] Mon, 17 Jun 2024 21:06:00 UTC (1,086 KB)
    [v2] Wed, 19 Jun 2024 17:56:17 UTC (1,086 KB)
    Pankayaraj Pathmanathan, Souradip Chakraborty, Xiangyu Liu, Yongyuan Liang, Furong Huang

Recent advancements in Reinforcement Learning with Human Feedback (RLHF) have significantly impacted the alignment of Large Language Models (LLMs). The sensitivity of reinforcement learning algorithms such as Proximal Policy Optimization (PPO) has led to new line work on Direct Policy Optimization (DPO), which treats RLHF in a supervised learning framework. The increased practical use of these RLHF methods warrants an analysis of their vulnerabilities. In this work, we investigate the vulnerabilities of DPO to poisoning attacks under different scenarios and compare the effectiveness of preference poisoning, a first of its kind. We comprehensively analyze DPO's vulnerabilities under different types of attacks, i.e., backdoor and non-backdoor attacks, and different poisoning methods across a wide array of language models, i.e., LLama 7B, Mistral 7B, and Gemma 7B. We find that unlike PPO-based methods, which, when it comes to backdoor attacks, require at least 4\% of the data to be poisoned to elicit harmful behavior, we exploit the true vulnerabilities of DPO more simply so we can poison the model with only as much as 0.5\% of the data. We further investigate the potential reasons behind the vulnerability and how well this vulnerability translates into backdoor vs non-backdoor attacks.

------------

`[2406.12168] BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM <https://arxiv.org/abs/2406.12168>`__ BPO:通过秉承行为邻近性LLM来强化在线偏好学习

::

    replaced with revised version Wed, 19 Jun 2024 05:25:27 GMT
    Submission history From: Jiachen Li [view email]
    [v1] Tue, 18 Jun 2024 00:41:40 UTC (478 KB)
    [v2] Wed, 19 Jun 2024 05:25:27 UTC (478 KB)
    Wenda Xu, Jiachen Li, William Yang Wang, Lei Li

Direct alignment from preferences (DAP) has emerged as a promising paradigm for aligning large language models (LLMs) to human desiderata from pre-collected, offline preference datasets. While recent studies indicate that existing offline DAP methods can directly benefit from online training samples, we highlight the need to develop specific online DAP algorithms to fully harness the power of online training. Specifically, we identify that the learned LLM should adhere to the proximity of the behavior LLM, which collects the training samples. To this end, we propose online Preference Optimization in proximity to the Behavior LLM (BPO), emphasizing the importance of constructing a proper trust region for LLM alignment.
We conduct extensive experiments to validate the effectiveness and applicability of our approach by integrating it with various DAP methods, resulting in significant performance improvements across a wide range of tasks when training with the same amount of preference data. Even when only introducing one additional data collection phase, our online BPO improves its offline DAP baseline from 72.0% to 80.2% on TL;DR and from 82.2% to 89.1% on Anthropic Helpfulness in terms of win rate against human reference text.

------------

`[2406.12246] TroL: Traversal of Layers for Large Language and Vision Models <https://arxiv.org/abs/2406.12246>`__ TroL:大型语言和视觉模型的层遍历

::

    replaced with revised version Wed, 19 Jun 2024 21:40:03 GMT
    Submission history From: Byung-Kwan Lee [view email]
    [v1] Tue, 18 Jun 2024 03:42:00 UTC (4,417 KB)
    [v2] Wed, 19 Jun 2024 21:40:03 UTC (4,417 KB)
    Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, Yong Man Ro

Large language and vision models (LLVMs) have been driven by the generalization power of large language models (LLMs) and the advent of visual instruction tuning. Along with scaling them up directly, these models enable LLVMs to showcase powerful vision language (VL) performances by covering diverse tasks via natural language instructions. However, existing open-source LLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often considered too large (e.g., 26B, 34B, and 110B parameters), having a larger number of layers. These large models demand costly, high-end resources for both training and inference. To address this issue, we present a new efficient LLVM family with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL), which enables the reuse of layers in a token-wise manner. This layer traversing technique simulates the effect of looking back and retracing the answering stream while increasing the number of forward propagation layers without physically adding more layers. We demonstrate that TroL employs a simple layer traversing approach yet efficiently outperforms the open-source LLVMs with larger model sizes and rivals the performances of the closed-source LLVMs with substantial sizes.

------------

`[2406.12649] Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models <https://arxiv.org/abs/2406.12649>`__ 概率概念解释器:视觉基础模型的可信概念解释

::

    replaced with revised version Wed, 19 Jun 2024 02:21:09 GMT
    Submission history From: Hengyi Wang [view email]
    [v1] Tue, 18 Jun 2024 14:17:57 UTC (48,551 KB)
    [v2] Wed, 19 Jun 2024 02:21:09 UTC (48,550 KB)
    Hengyi Wang, Shiwei Tan, Hao Wang

Vision transformers (ViTs) have emerged as a significant area of focus, particularly for their capacity to be jointly trained with large language models and to serve as robust vision foundation models. Yet, the development of trustworthy explanation methods for ViTs has lagged, particularly in the context of post-hoc interpretations of ViT predictions. Existing sub-image selection approaches, such as feature-attribution and conceptual models, fall short in this regard. This paper proposes five desiderata for explaining ViTs -- faithfulness, stability, sparsity, multi-level structure, and parsimony -- and demonstrates the inadequacy of current methods in meeting these criteria comprehensively. We introduce a variational Bayesian explanation framework, dubbed ProbAbilistic Concept Explainers (PACE), which models the distributions of patch embeddings to provide trustworthy post-hoc conceptual explanations. Our qualitative analysis reveals the distributions of patch-level concepts, elucidating the effectiveness of ViTs by modeling the joint distribution of patch embeddings and ViT's predictions. Moreover, these patch-level explanations bridge the gap between image-level and dataset-level explanations, thus completing the multi-level structure of PACE. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that PACE surpasses state-of-the-art methods in terms of the defined desiderata.

------------

`[2310.01727] Can GPT-4 Replicate Empirical Software Engineering Research? <https://arxiv.org/abs/2310.01727>`__ GPT-4能复制经验软件工程研究吗?

::

    replaced with revised version Wed, 19 Jun 2024 07:17:28 GMT
    Submission history From: Jenny Liang [view email]
    [v1] Tue, 3 Oct 2023 01:27:23 UTC (2,161 KB)
    [v2] Thu, 23 May 2024 09:43:21 UTC (2,164 KB)
    [v3] Wed, 19 Jun 2024 07:17:28 UTC (2,175 KB)
    Jenny T. Liang, Carmen Badea, Christian Bird, Robert DeLine, Denae Ford, Nicole Forsgren, Thomas Zimmermann

Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners could benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help replicate and thus democratize empirical software engineering research.
In this paper, we examine GPT-4's abilities to perform replications of empirical software engineering research on new data. We study their ability to surface assumptions made in empirical software engineering research methodologies, as well as their ability to plan and generate code for analysis pipelines on seven empirical software engineering papers. We perform a user study with 14 participants with software engineering research expertise, who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module specifications) from the papers. We find that GPT-4 is able to surface correct assumptions, but struggles to generate ones that apply common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering knowledge. Our findings have implications for leveraging LLMs for software engineering research as well as practitioner data scientists in software teams.

------------

`[2312.12423] Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model <https://arxiv.org/abs/2312.12423>`__ 

::

    replaced with revised version Wed, 19 Jun 2024 22:20:40 GMT
    Submission history From: Shraman Pramanick [view email]
    [v1] Tue, 19 Dec 2023 18:53:01 UTC (15,612 KB)
    [v2] Wed, 19 Jun 2024 22:20:40 UTC (17,267 KB)
    Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, Amjad Almahairi

The ability of large language models (LLMs) to process visual inputs has given rise to general-purpose vision systems, unifying various vision-language (VL) tasks by instruction tuning. However, due to the enormous diversity in input-output formats in the vision domain, existing general-purpose models fail to successfully integrate segmentation and multi-image inputs with coarse-level tasks into a single framework. In this work, we introduce VistaLLM, a powerful visual system that addresses coarse- and fine-grained VL tasks over single and multiple input images using a unified framework. VistaLLM utilizes an instruction-guided image tokenizer that filters global embeddings using task descriptions to extract compressed and refined features from numerous images. Moreover, VistaLLM employs a gradient-aware adaptive sampling technique to represent binary segmentation masks as sequences, significantly improving over previously used uniform sampling. To bolster the desired capability of VistaLLM, we curate CoinIt, a comprehensive coarse-to-fine instruction tuning dataset with 6.8M samples. We also address the lack of multi-image grounding datasets by introducing a novel task, AttCoSeg (Attribute-level Co-Segmentation), which boosts the model's reasoning and grounding capability over multiple input images. Extensive experiments on a wide range of V- and VL tasks demonstrate the effectiveness of VistaLLM by achieving consistent state-of-the-art performance over strong baselines across all downstream tasks. Our project page can be found at this https URL.

------------

`[2401.05163] MISS: A Generative Pretraining and Finetuning Approach for Med-VQA <https://arxiv.org/abs/2401.05163>`__ MISS:一种面向Med-VQA的生成式预训练和微调方法

::

    replaced with revised version Wed, 19 Jun 2024 11:14:40 GMT
    Submission history From: Jiawei Chen [view email]
    [v1] Wed, 10 Jan 2024 13:56:40 UTC (1,295 KB)
    [v2] Thu, 18 Jan 2024 09:34:31 UTC (1,295 KB)
    [v3] Wed, 19 Jun 2024 11:14:40 UTC (1,411 KB)
    Jiawei Chen, Dingkang Yang, Yue Jiang, Yuxuan Lei, Lihua Zhang

Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using Large Language Models (LLMs), enabling those traditional medical vision field task data to be applied to VLP. Experiments show that our method achieves excellent results with fewer multimodal datasets and demonstrates the advantages of generative VQA models.

------------

`[2402.03907] Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy <https://arxiv.org/abs/2402.03907>`__ 将大型语言模型嵌入扩展现实:包容、参与和隐私的机遇与挑战

::

    replaced with revised version Thu, 20 Jun 2024 10:02:30 GMT
    Submission history From: Efe Bozkir [view email]
    [v1] Tue, 6 Feb 2024 11:19:40 UTC (163 KB)
    [v2] Thu, 20 Jun 2024 10:02:30 UTC (164 KB)
    Efe Bozkir and S\"uleyman \"Ozdel and Ka Hei Carrie Lau and Mengdi Wang and Hong Gao and Enkelejda Kasneci

Advances in artificial intelligence and human-computer interaction will likely lead to extended reality (XR) becoming pervasive. While XR can provide users with interactive, engaging, and immersive experiences, non-player characters are often utilized in pre-scripted and conventional ways. This paper argues for using large language models (LLMs) in XR by embedding them in avatars or as narratives to facilitate inclusion through prompt engineering and fine-tuning the LLMs. We argue that this inclusion will promote diversity for XR use. Furthermore, the versatile conversational capabilities of LLMs will likely increase engagement in XR, helping XR become ubiquitous. Lastly, we speculate that combining the information provided to LLM-powered spaces by users and the biometric data obtained might lead to novel privacy invasions. While exploring potential privacy breaches, examining user privacy concerns and preferences is also essential. Therefore, despite challenges, LLM-powered XR is a promising area with several opportunities.

------------

`[2403.00863] LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction <https://arxiv.org/abs/2403.00863>`__ LLM-Ensemble:面向电商产品属性值抽取的最优大型语言模型集成方法

::

    replaced with revised version Thu, 20 Jun 2024 07:10:28 GMT
    Submission history From: Xiaohan Li [view email]
    [v1] Thu, 29 Feb 2024 23:03:19 UTC (1,161 KB)
    [v2] Thu, 20 Jun 2024 07:10:28 UTC (1,162 KB)
    Chenhao Fang, Xiaohan Li, Zezhong Fan, Jianpeng Xu, Kaushiki Nag, Evren Korpeoglu, Sushant Kumar, Kannan Achan

Product attribute value extraction is a pivotal component in Natural Language Processing (NLP) and the contemporary e-commerce industry. The provision of precise product attribute values is fundamental in ensuring high-quality recommendations and enhancing customer satisfaction. The recently emerging Large Language Models (LLMs) have demonstrated state-of-the-art performance in numerous attribute extraction tasks, without the need for domain-specific training data. Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters. This variation makes them complementary to each other, with no single LLM dominating all others. Considering the diverse strengths and weaknesses of LLMs, it becomes necessary to develop an ensemble method that leverages their complementary potentials. In this paper, we propose a novel algorithm called LLM-ensemble to ensemble different LLMs' outputs for attribute value extraction. We iteratively learn the weights for different LLMs to aggregate the labels with weights to predict the final attribute value. Not only can our proposed method be proven theoretically optimal, but it also ensures efficient computation, fast convergence, and safe deployment. We have also conducted extensive experiments with various state-of-the-art LLMs, including Llama2-13B, Llama2-70B, PaLM-2, GPT-3.5, and GPT-4, on Walmart's internal data. Our offline metrics demonstrate that the LLM-ensemble method outperforms all the state-of-the-art single LLMs on Walmart's internal dataset. This method has been launched in several production models, leading to improved Gross Merchandise Volume (GMV), Click-Through Rate (CTR), Conversion Rate (CVR), and Add-to-Cart Rate (ATC).

------------

`[2403.06131] FewFedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning <https://arxiv.org/abs/2403.06131>`__ FewFedPIT:隐私保护与少样本联邦指令调优

::

    replaced with revised version Thu, 20 Jun 2024 13:00:18 GMT
    Submission history From: Zhuo Zhang [view email]
    [v1] Sun, 10 Mar 2024 08:41:22 UTC (8,020 KB)
    [v2] Thu, 20 Jun 2024 13:00:18 UTC (9,507 KB)
    Zhuo Zhang, Jingyuan Zhang, Jintao Huang, Lizhen Qu, Hongzhi Zhang, Qifan Wang, Xun Zhou, Zenglin Xu

Instruction tuning has been identified as a crucial technique for optimizing the performance of large language models (LLMs) in generating human-aligned responses. Nonetheless, gathering diversified and superior-quality instruction data for such tuning presents notable obstacles, especially in domains with rigid privacy provisions. Federated instruction tuning (FedIT) has emerged as a promising solution, by consolidating collaborative training across multiple data owners, thereby resulting in a privacy-preserving learning model. However, FedIT encounters limitations such as scarcity of instructional data and risk of exposure to training data extraction attacks. In this paper, we propose a novel federated algorithm, FewFedPIT, designed to simultaneously enhance privacy protection and model performance of federated few-shot learning. FewFedPITcomprises three vital components on the client side: (1) synthetic data generation, which utilizes LLMs' in-context learning capacity to generate synthetic data autonomously, thus expanding the local database; (2) parameter isolation training, which individually updates the public parameters in the synthetic data and the private parameters in the local data, consequently mitigating the noise impact of the synthetic data; (3) local aggregation sharing, which mixes public and private parameters before uploading, effectively preventing data extraction attacks. Extensive experiments on three open-source datasets demonstrate the effectiveness of FewFedPITin, enhancing privacy preservation and improving federated few-shot performance.

------------

`[2405.13068] Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation <https://arxiv.org/abs/2405.13068>`__ Lockpicking llm:一种基于logit的使用令牌级别操作的越狱工具

::

    replaced with revised version Wed, 19 Jun 2024 13:51:06 GMT
    Submission history From: Yuxi Li [view email]
    [v1] Mon, 20 May 2024 17:17:55 UTC (888 KB)
    [v2] Wed, 19 Jun 2024 13:51:06 UTC (897 KB)
    Yuxi Li, Yi Liu, Yuekang Li, Ling Shi, Gelei Deng, Shengquan Chen, Kailong Wang

Large language models (LLMs) have transformed the field of natural language processing, but they remain susceptible to jailbreaking attacks that exploit their capabilities to generate unintended and potentially harmful content. Existing token-level jailbreaking techniques, while effective, face scalability and efficiency challenges, especially as models undergo frequent updates and incorporate advanced defensive measures. In this paper, we introduce JailMine, an innovative token-level manipulation approach that addresses these limitations effectively. JailMine employs an automated "mining" process to elicit malicious responses from LLMs by strategically selecting affirmative outputs and iteratively reducing the likelihood of rejection. Through rigorous testing across multiple well-known LLMs and datasets, we demonstrate JailMine's effectiveness and efficiency, achieving a significant average reduction of 86% in time consumed while maintaining high success rates averaging 95%, even in the face of evolving defensive strategies. Our work contributes to the ongoing effort to assess and mitigate the vulnerability of LLMs to jailbreaking attacks, underscoring the importance of continued vigilance and proactive measures to enhance the security and reliability of these powerful language models.

------------

`[2406.00014] KU-DMIS at EHRSQL 2024:Generating SQL query via question templatization in EHR <https://arxiv.org/abs/2406.00014>`__ KU-DMIS在EHRSQL 2024:在EHR中通过问题模板化生成SQL查询

::

    replaced with revised version Wed, 19 Jun 2024 16:21:46 GMT
    Submission history From: Chanhwi Kim [view email]
    [v1] Wed, 22 May 2024 02:15:57 UTC (1,360 KB)
    [v2] Wed, 19 Jun 2024 16:21:46 UTC (1,353 KB)
    Hajung Kim, Chanhwi Kim, Hoonick Lee, Kyochul Jang, Jiwoo Lee, Kyungjae Lee, Gangwoo Kim, Jaewoo Kang

Transforming natural language questions into SQL queries is crucial for precise data retrieval from electronic health record (EHR) databases. A significant challenge in this process is detecting and rejecting unanswerable questions that request information beyond the database's scope or exceed the system's capabilities. In this paper, we introduce a novel text-to-SQL framework that robustly handles out-of-domain questions and verifies the generated queries with query execution.Our framework begins by standardizing the structure of questions into a templated format. We use a powerful large language model (LLM), fine-tuned GPT-3.5 with detailed prompts involving the table schemas of the EHR database system. Our experimental results demonstrate the effectiveness of our framework on the EHRSQL-2024 benchmark benchmark, a shared task in the ClinicalNLP workshop. Although a straightforward fine-tuning of GPT shows promising results on the development set, it struggled with the out-of-domain questions in the test set. With our framework, we improve our system's adaptability and achieve competitive performances in the official leaderboard of the EHRSQL-2024 challenge.

------------

`[2406.12108] Computing in the Life Sciences: From Early Algorithms to Modern AI <https://arxiv.org/abs/2406.12108>`__ 生命科学中的计算:从早期算法到现代人工智能

::

    replaced with revised version Wed, 19 Jun 2024 03:54:28 GMT
    Submission history From: Alexander Titus [view email]
    [v1] Mon, 17 Jun 2024 21:36:52 UTC (967 KB)
    [v2] Wed, 19 Jun 2024 03:54:28 UTC (975 KB)
    Samuel A. Donkor, Matthew E. Walsh, and Alexander J. Titus

Computing in the life sciences has undergone a transformative evolution, from early computational models in the 1950s to the applications of artificial intelligence (AI) and machine learning (ML) seen today. This paper highlights key milestones and technological advancements through the historical development of computing in the life sciences. The discussion includes the inception of computational models for biological processes, the advent of bioinformatics tools, and the integration of AI/ML in modern life sciences research. Attention is given to AI-enabled tools used in the life sciences, such as scientific large language models and bio-AI tools, examining their capabilities, limitations, and impact to biological risk. This paper seeks to clarify and establish essential terminology and concepts to ensure informed decision-making and effective communication across disciplines.

------------

`[2312.10321] LLM-SQL-Solver: Can LLMs Determine SQL Equivalence? <https://arxiv.org/abs/2312.10321>`__ LLM-SQL-Solver: llm可以确定SQL的等价性吗?

::

    replaced with revised version Wed, 19 Jun 2024 20:19:00 GMT
    Submission history From: Fuheng Zhao [view email]
    [v1] Sat, 16 Dec 2023 05:01:23 UTC (265 KB)
    [v2] Wed, 17 Jan 2024 20:11:38 UTC (266 KB)
    [v3] Wed, 19 Jun 2024 20:19:00 UTC (270 KB)
    Fuheng Zhao, Lawrence Lim, Ishtiyaque Ahmad, Divyakant Agrawal, Amr El Abbadi

Judging the equivalence between two SQL queries is a fundamental problem with many practical applications in data management and SQL generation (i.e., evaluating the quality of generated SQL queries in text-to-SQL task). While the research community has reasoned about SQL equivalence for decades, it poses considerable difficulties and no complete solutions exist. Recently, Large Language Models (LLMs) have shown strong reasoning capability in conversation, question answering and solving mathematics challenges. In this paper, we study if LLMs can be used to determine the equivalence between SQL queries under two notions of SQL equivalence (semantic equivalence and relaxed equivalence). To assist LLMs in generating high quality responses, we present two prompting techniques: Miniature & Mull and Explain & Compare. The former technique is used to evaluate the semantic equivalence in which it asks LLMs to execute a query on a simple database instance and then explore if a counterexample exists by modifying the database. The latter technique is used to evaluate the relaxed equivalence in which it asks LLMs to explain the queries and then compare if they contain significant logical differences. Our experiments demonstrate using our techniques, LLMs is a promising tool to help data engineers in writing semantically equivalent SQL queries, however challenges still persist, and is a better metric for evaluating SQL generation than the popular execution accuracy.

------------

`[2402.14968] Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment <https://arxiv.org/abs/2402.14968>`__ 通过后门增强安全对齐缓解基于微调的越狱攻击

::

    replaced with revised version Thu, 20 Jun 2024 05:18:04 GMT
    Submission history From: Jiongxiao Wang [view email]
    [v1] Thu, 22 Feb 2024 21:05:18 UTC (8,918 KB)
    [v2] Tue, 27 Feb 2024 21:27:53 UTC (8,918 KB)
    [v3] Thu, 20 Jun 2024 05:18:04 UTC (2,151 KB)
    Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Junjie Hu, Yixuan Li, Patrick McDaniel, Muhao Chen, Bo Li, Chaowei Xiao

Despite the general capabilities of Large Language Models (LLM), these models still request fine-tuning or adaptation with customized data when meeting specific business demands. However, this process inevitably introduces new threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack) under the setting of Language-Model-as-a-Service (LMaaS), where the model's safety has been significantly compromised by fine-tuning users' uploaded examples contain just a few harmful examples. Though potential defenses have been proposed that the service providers can integrate safety examples into the fine-tuning dataset to reduce safety issues, such approaches require incorporating a substantial amount of data, making it inefficient. To effectively defend against the FJAttack with limited safety examples under LMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, service providers will construct prefixed safety examples with a secret prompt, acting as a "backdoor trigger". By integrating prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the "backdoor attack", establishing a strong correlation between the secret prompt and safety generations. Consequently, safe responses are ensured once service providers prepend this secret prompt ahead of any user input during inference. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar safety performance as the original aligned models without harming the benign performance. Furthermore, we also present the effectiveness of our method in a more practical setting where the fine-tuning data consists of both FJAttack examples and the fine-tuning task data.

------------

`[2403.05286] LLM4Decompile: Decompiling Binary Code with Large Language Models <https://arxiv.org/abs/2403.05286>`__ LLM4Decompile:用大型语言模型反编译二进制代码

::

    replaced with revised version Wed, 19 Jun 2024 02:45:03 GMT
    Submission history From: Hanzhuo Tan [view email]
    [v1] Fri, 8 Mar 2024 13:10:59 UTC (7,991 KB)
    [v2] Wed, 19 Jun 2024 02:45:03 UTC (810 KB)
    Hanzhuo Tan, Qi Luo, Jing Li, Yuqun Zhang

Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100%. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results. Our code, dataset, and models are released at this https URL

------------

`[2406.00799] Are you still on track!? Catching LLM Task Drift with Activations <https://arxiv.org/abs/2406.00799>`__ 你还在正轨上吗?用激活捕捉LLM任务漂移

::

    replaced with revised version Thu, 20 Jun 2024 13:33:08 GMT
    Submission history From: Sahar Abdelnabi [view email]
    [v1] Sun, 2 Jun 2024 16:53:21 UTC (3,274 KB)
    [v2] Mon, 10 Jun 2024 15:39:56 UTC (3,275 KB)
    [v3] Thu, 20 Jun 2024 13:33:08 UTC (3,275 KB)
    Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, Andrew Paverd

Large Language Models (LLMs) are routinely used in retrieval-augmented applications to orchestrate tasks and process inputs from users and other sources. These inputs, even in a single LLM interaction, can come from a variety of sources, of varying trustworthiness and provenance. This opens the door to prompt injection attacks, where the LLM receives and acts upon instructions from supposedly data-only sources, thus deviating from the user's original instructions. We define this as task drift, and we propose to catch it by scanning and analyzing the LLM's activations. We compare the LLM's activations before and after processing the external input in order to detect whether this input caused instruction drift. We develop two probing methods and find that simply using a linear classifier can detect drift with near perfect ROC AUC on an out-of-distribution test set. We show that this approach generalizes surprisingly well to unseen task domains, such as prompt injections, jailbreaks, and malicious instructions, without being trained on any of these attacks. Our setup does not require any modification of the LLM (e.g., fine-tuning) or any text generation, thus maximizing deployability and cost efficiency and avoiding reliance on unreliable model output. To foster future research on activation-based task inspection, decoding, and interpretability, we will release our large-scale TaskTracker toolkit, comprising a dataset of over 500K instances, representations from 4 SoTA language models, and inspection tools.

------------

`[2406.11171] SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations <https://arxiv.org/abs/2406.11171>`__ SUGARCREPE++数据集:视觉-语言模型对语义和词汇变化的敏感性

::

    replaced with revised version Wed, 19 Jun 2024 00:03:42 GMT
    Submission history From: Aman Jaiswal [view email]
    [v1] Mon, 17 Jun 2024 03:22:20 UTC (27,057 KB)
    [v2] Wed, 19 Jun 2024 00:03:42 UTC (27,057 KB)
    Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry, Evangelos Milios, Sageev Oore, Hassan Sajjad

Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SUGARCREPE++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly in object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SUGARCREPE++, there is a significant opportunity for improvement. We show that all the models which achieve better performance on compositionality datasets need not perform equally well on SUGARCREPE++, signifying that compositionality alone may not be sufficient for understanding semantic and lexical alterations. Given the importance of the property that the SUGARCREPE++ dataset targets, it serves as a new challenge to the vision-and-language community.

------------

`[2312.05547] Signatures Meet Dynamic Programming: Generalizing Bellman Equations for Trajectory Following <https://arxiv.org/abs/2312.05547>`__ 

::

    replaced with revised version Wed, 19 Jun 2024 00:07:53 GMT
    Submission history From: Motoya Ohnishi [view email]
    [v1] Sat, 9 Dec 2023 11:34:08 UTC (3,151 KB)
    [v2] Wed, 19 Jun 2024 00:07:53 UTC (3,082 KB)
    Motoya Ohnishi, Iretiayo Akinola, Jie Xu, Ajay Mandlekar, Fabio Ramos

Path signatures have been proposed as a powerful representation of paths that efficiently captures the path's analytic and geometric characteristics, having useful algebraic properties including fast concatenation of paths through tensor products. Signatures have recently been widely adopted in machine learning problems for time series analysis. In this work we establish connections between value functions typically used in optimal control and intriguing properties of path signatures. These connections motivate our novel control framework with signature transforms that efficiently generalizes the Bellman equation to the space of trajectories. We analyze the properties and advantages of the framework, termed signature control. In particular, we demonstrate that (i) it can naturally deal with varying/adaptive time steps; (ii) it propagates higher-level information more efficiently than value function updates; (iii) it is robust to dynamical system misspecification over long rollouts. As a specific case of our framework, we devise a model predictive control method for path tracking. This method generalizes integral control, being suitable for problems with unknown disturbances. The proposed algorithms are tested in simulation, with differentiable physics models including typical control and robotics tasks such as point-mass, curve following for an ant model, and a robotic manipulator.

------------

-----------
Index (206)
-----------

`[2406.13161] APPL: A Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts <https://arxiv.org/abs/2406.13161>`__ APPL:一种融合程序和大型语言模型提示符的提示式程序设计语言

`[2406.13233] AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models <https://arxiv.org/abs/2406.13233>`__ AdaMoE:基于空专家的专家混合语言模型token自适应路由

`[2406.13250] LangTopo: Aligning Language Descriptions of Graphs with Tokenized Topological Modeling <https://arxiv.org/abs/2406.13250>`__ LangTopo:基于标记化拓扑建模的图的语言描述对齐

`[2406.13269] Investigating Low-Cost LLM Annotation for~Spoken Dialogue Understanding Datasets <https://arxiv.org/abs/2406.13269>`__

`[2406.13305] Multimodal MRI-based Detection of Amyloid Status in Alzheimer's Disease Continuum <https://arxiv.org/abs/2406.13305>`__ 基于多模态mri检测阿尔茨海默病连续体中的淀粉样蛋白状态

`[2406.13399] VELO: A Vector Database-Assisted Cloud-Edge Collaborative LLM QoS Optimization Framework <https://arxiv.org/abs/2406.13399>`__ VELO:矢量数据库辅助的云边协同LLM QoS优化框架

`[2406.13558] Enhancing Travel Choice Modeling with Large Language Models: A Prompt-Learning Approach <https://arxiv.org/abs/2406.13558>`__ 利用大型语言模型增强旅游选择建模:一种快速学习方法

`[2406.13715] Converging Dimensions: Information Extraction and Summarization through Multisource, Multimodal, and Multilingual Fusion <https://arxiv.org/abs/2406.13715>`__

`[2406.13873] A Pure Transformer Pretraining Framework on Text-attributed Graphs <https://arxiv.org/abs/2406.13873>`__ 文本属性图上的纯Transformer预训练框架

`[2406.13919] SPL: A Socratic Playground for Learning Powered by Large Language Mode <https://arxiv.org/abs/2406.13919>`__ SPL:由大型语言模式驱动的苏格拉底式学习游乐场

`[2406.13945] CityBench: Evaluating the Capabilities of Large Language Model as World Model <https://arxiv.org/abs/2406.13945>`__ CityBench:大型语言模型作为世界模型的能力评估

`[2406.13947] AspirinSum: an Aspect-based utility-preserved de-identification Summarization framework <https://arxiv.org/abs/2406.13947>`__ AspirinSum:一种基于方面保持效用的去识别摘要框架

`[2406.13948] CityGPT: Empowering Urban Spatial Cognition of Large Language Models <https://arxiv.org/abs/2406.13948>`__ CityGPT:面向大型语言模型的城市空间认知赋能

`[2406.14039] CryptoGPT: a 7B model rivaling GPT-4 in the task of analyzing and classifying real-time financial news <https://arxiv.org/abs/2406.14039>`__ CryptoGPT:在分析和分类实时金融新闻任务上与GPT-4匹敌的7B模型

`[2406.14124] Measuring Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective <https://arxiv.org/abs/2406.14124>`__

`[2406.14171] Ranking LLMs by compression <https://arxiv.org/abs/2406.14171>`__ 按压缩对llm进行排名

`[2406.14319] LiveMind: Low-latency Large Language Models with Simultaneous Inference <https://arxiv.org/abs/2406.14319>`__ LiveMind:具有同步推理的低延迟大型语言模型

`[2406.14343] iWISDM: Assessing instruction following in multimodal models at scale <https://arxiv.org/abs/2406.14343>`__ iWISDM:评估大规模多模态模型的指令跟随

`[2406.14408] FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving <https://arxiv.org/abs/2406.14408>`__ FVEL:基于定理证明的大型语言模型交互式形式化验证环境

`[2406.14449] APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking <https://arxiv.org/abs/2406.14449>`__

`[2406.12975] SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation <https://arxiv.org/abs/2406.12975>`__ SHIELD: LLM文本生成中的版权遵从性评估与防御策略

`[2406.13009] Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors <https://arxiv.org/abs/2406.13009>`__ 通过集成提示检测错误(DEEP):用于检测事实性错误的端到端LLM框架

`[2406.13114] Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation <https://arxiv.org/abs/2406.13114>`__ 多阶段平衡蒸馏:解决序列级知识蒸馏中的长尾挑战

`[2406.13124] Learning to Generate Answers with Citations via Factual Consistency Models <https://arxiv.org/abs/2406.13124>`__ 基于事实一致性模型的引用生成答案学习

`[2406.13131] When Parts are Greater Than Sums: Individual LLM Components Can Outperform Full Models <https://arxiv.org/abs/2406.13131>`__

`[2406.13138] Large Language Models are Biased Because They Are Large Language Models <https://arxiv.org/abs/2406.13138>`__ 大型语言模型是有偏见的，因为它们是大型语言模型

`[2406.13152] Analyzing Diversity in Healthcare LLM Research: A Scientometric Perspective <https://arxiv.org/abs/2406.13152>`__ 医疗保健法学硕士研究的多样性分析:科学计量学视角

`[2406.13167] QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism <https://arxiv.org/abs/2406.13167>`__ QRMeM:通过先问后反射机制解除长度限制

`[2406.13184] Locating and Extracting Relational Concepts in Large Language Models <https://arxiv.org/abs/2406.13184>`__ 大型语言模型中关系概念的定位和抽取

`[2406.13185] Learnable In-Context Vector for Visual Question Answering <https://arxiv.org/abs/2406.13185>`__ 面向视觉问答的可学习上下文向量

`[2406.13188] Synthetic Context Generation for Question Generation <https://arxiv.org/abs/2406.13188>`__ 用于问题生成的合成上下文生成

`[2406.13229] Probing the Emergence of Cross-lingual Alignment during LLM Training <https://arxiv.org/abs/2406.13229>`__

`[2406.13232] Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models <https://arxiv.org/abs/2406.13232>`__ 走向鲁棒评估:大型语言模型时代开放域问答数据集和指标的全面分类

`[2406.13236] Data Contamination Can Cross Language Barriers <https://arxiv.org/abs/2406.13236>`__ 数据污染可以跨越语言障碍

`[2406.13282] Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective <https://arxiv.org/abs/2406.13282>`__ 理解长上下文llm的RoPE扩展:注意力视角

`[2406.13331] Improving Zero-shot LLM Re-Ranker with Risk Minimization <https://arxiv.org/abs/2406.13331>`__ 基于风险最小化改进零样本LLM重排序器

`[2406.13342] ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models <https://arxiv.org/abs/2406.13342>`__

`[2406.13357] Transferable speech-to-text large language model alignment module <https://arxiv.org/abs/2406.13357>`__ 可迁移语音-文本大型语言模型对齐模块

`[2406.13375] ALiiCE: Evaluating Positional Fine-grained Citation Generation <https://arxiv.org/abs/2406.13375>`__ alice:位置细粒度引文生成评估

`[2406.13415] Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators <https://arxiv.org/abs/2406.13415>`__ llm的事实置信度:当前估计器的可靠性和鲁棒性

`[2406.13439] Finding Blind Spots in Evaluator LLMs with Interpretable Checklists <https://arxiv.org/abs/2406.13439>`__ 使用可解释的清单在评估器llm中寻找盲点

`[2406.13444] VDebugger: Harnessing Execution Feedback for Debugging Visual Programs <https://arxiv.org/abs/2406.13444>`__

`[2406.13476] LLMs Are Zero-Shot Context-Aware Simultaneous Translators <https://arxiv.org/abs/2406.13476>`__ llm是零样本的上下文感知同声传译

`[2406.13542] Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models <https://arxiv.org/abs/2406.13542>`__ 基于执行反馈的自玩:提高大型语言模型的指令遵循能力

`[2406.13555] BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation <https://arxiv.org/abs/2406.13555>`__ BiLD:大型语言模型蒸馏的双向Logits差分损失

`[2406.13617] Optimizing Psychological Counseling with Instruction-Tuned Large Language Models <https://arxiv.org/abs/2406.13617>`__ 基于指令调优的大型语言模型优化心理咨询

`[2406.13618] In-Context Former: Lightning-fast Compressing Context for Large Language Model <https://arxiv.org/abs/2406.13618>`__ 上下文Former:快速压缩大型语言模型上下文

`[2406.13621] Improving Visual Commonsense in Language Models via Multiple Image Generation <https://arxiv.org/abs/2406.13621>`__ 通过多图像生成改善语言模型中的视觉常识

`[2406.13626] Fine-Tuning Gemma-7B for Enhanced Sentiment Analysis of Financial News Headlines <https://arxiv.org/abs/2406.13626>`__ 微调Gemma-7B增强金融新闻标题情感分析

`[2406.13632] Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations <https://arxiv.org/abs/2406.13632>`__ 少镜头能在长语境中发挥作用吗?循环利用上下文来生成演示

`[2406.13662] ObscurePrompt: Jailbreaking Large Language Models via Obscure Input <https://arxiv.org/abs/2406.13662>`__

`[2406.13698] MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language <https://arxiv.org/abs/2406.13698>`__ MMTE:隐喻语言机器翻译质量评估的语料库和指标

`[2406.13706] Breaking News: Case Studies of Generative AI's Use in Journalism <https://arxiv.org/abs/2406.13706>`__ 突发新闻:生成式人工智能在新闻中的应用案例研究

`[2406.13718] Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization <https://arxiv.org/abs/2406.13718>`__ 沿语言变化维度评估大型语言模型:跨语言泛化的系统研究

`[2406.13720] On the Utility of Domain-Adjacent Fine-Tuned Model Ensembles for Few-shot Problems <https://arxiv.org/abs/2406.13720>`__ 域邻近微调模型集成在小样本问题中的应用

`[2406.13748] Every Language Counts: Learn and Unlearn in Multilingual LLMs <https://arxiv.org/abs/2406.13748>`__ 每种语言都很重要:在多语言llm中学习和忘记

`[2406.13764] Can LLMs Reason in the Wild with Programs? <https://arxiv.org/abs/2406.13764>`__

`[2406.13862] Knowledge Graph-Enhanced Large Language Models via Path Selection <https://arxiv.org/abs/2406.13862>`__ 基于路径选择的知识图谱增强的大型语言模型

`[2406.13892] Adaptable Logical Control for Large Language Models <https://arxiv.org/abs/2406.13892>`__ 大型语言模型的自适应逻辑控制

`[2406.13893] Open Generative Large Language Models for Galician <https://arxiv.org/abs/2406.13893>`__ 面向加利西亚语的开放式生成大型语言模型

`[2406.13903] Generative AI for Enhancing Active Learning in Education: A Comparative Study of GPT-3.5 and GPT-4 in Crafting Customized Test Questions <https://arxiv.org/abs/2406.13903>`__

`[2406.13905] Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking <https://arxiv.org/abs/2406.13905>`__ 主观决策中生成的自由文本推理的说服力:成对论据排序案例研究

`[2406.13925] GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models <https://arxiv.org/abs/2406.13925>`__ GenderAlign:用于减轻大型语言模型性别偏见的对齐数据集

`[2406.13929] Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination <https://arxiv.org/abs/2406.13929>`__ 大型语言模型是怀疑论者:输入冲突幻觉的假阴性问题

`[2406.13940] AutoCAP: Towards Automatic Cross-lingual Alignment Planning for Zero-shot Chain-of-Thought <https://arxiv.org/abs/2406.13940>`__ AutoCAP:面向零样本思维链的自动跨语言对齐规划

`[2406.13993] Exploring Changes in Nation Perception with Nationality-Assigned Personas in LLMs <https://arxiv.org/abs/2406.13993>`__ 在llm中通过国籍分配角色探索国家认知的变化

`[2406.13997] "Global is Good, Local is Bad?": Understanding Brand Bias in LLMs <https://arxiv.org/abs/2406.13997>`__ “全球化是好事，本地化是坏事?”:理解LLMs中的品牌偏见

`[2406.14012] Seeing Through AI's Lens: Enhancing Human Skepticism Towards LLM-Generated Fake News <https://arxiv.org/abs/2406.14012>`__ 透过人工智能的镜头:增强人类对llm生成的假新闻的怀疑

`[2406.14021] HIGHT: Hierarchical Graph Tokenization for Graph-Language Alignment <https://arxiv.org/abs/2406.14021>`__

`[2406.14023] Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective <https://arxiv.org/abs/2406.14023>`__ 从心理测量学角度评估大型语言模型中的内隐偏见

`[2406.14048] Prompt Injection Attacks in Defended Systems <https://arxiv.org/abs/2406.14048>`__ 防御系统中的提示注入攻击

`[2406.14051] How Many Parameters Does it Take to Change a Light Bulb? Evaluating Performance in Self-Play of Conversational Games as a Function of Model Characteristics <https://arxiv.org/abs/2406.14051>`__ 换一个灯泡需要多少个参数?以模型特征的函数来评估会话游戏的自玩表现

`[2406.14115] Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models <https://arxiv.org/abs/2406.14115>`__ 取其精华去其糟粕:对微调大型语言模型数据选择的重新思考

`[2406.14144] Finding Safety Neurons in Large Language Models <https://arxiv.org/abs/2406.14144>`__ 在大型语言模型中寻找安全神经元

`[2406.14155] Aligning Large Language Models with Diverse Political Viewpoints <https://arxiv.org/abs/2406.14155>`__ 具有不同政治观点的大型语言模型的对齐

`[2406.14167] Definition generation for lexical semantic change detection <https://arxiv.org/abs/2406.14167>`__ 词汇语义变化检测的定义生成

`[2406.14230] Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing <https://arxiv.org/abs/2406.14230>`__ 提高标准:通过生成式演化测试研究大型语言模型的价值

`[2406.14275] Step-Back Profiling: Distilling User History for Personalized Scientific Writing <https://arxiv.org/abs/2406.14275>`__ 退步分析:为个性化科学写作提取用户历史

`[2406.14313] Robust Few-shot Transfer Learning for Knowledge Base Question Answering with Unanswerable Questions <https://arxiv.org/abs/2406.14313>`__ 面向无法回答问题的知识库问答的鲁棒少样本迁移学习

`[2406.14322] Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning <https://arxiv.org/abs/2406.14322>`__ 保护私隐!用于语言模型微调的用户级差分隐私

`[2406.14326] medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced Clinical Diagnosis on EMRs <https://arxiv.org/abs/2406.14326>`__ medIKAL:整合知识图谱作为LLMs的助手，以增强EMRs上的临床诊断

`[2406.14335] Self-supervised Interpretable Concept-based Models for Text Classification <https://arxiv.org/abs/2406.14335>`__

`[2406.14336] Exploring Spatial Representations in the Historical Lake District Texts with LLM-based Relation Extraction <https://arxiv.org/abs/2406.14336>`__ 基于llm的历史湖区文本空间关系抽取研究

`[2406.14394] SEC-QA: A Systematic Evaluation Corpus for Financial QA <https://arxiv.org/abs/2406.14394>`__ SEC-QA:一个系统性的财务QA评估语料库

`[2406.14462] Explicit and Implicit Large Language Model Personas Generate Opinions but Fail to Replicate Deeper Perceptions and Biases <https://arxiv.org/abs/2406.14462>`__ 显式和隐式的大型语言模型人物角色生成意见，但无法复制更深的感知和偏见

`[2406.14500] Improving Expert Radiology Report Summarization by Prompting Large Language Models with a Layperson Summary <https://arxiv.org/abs/2406.14500>`__ 通过用外行人摘要提示大型语言模型改进专家放射学报告摘要

`[2406.14504] Translating Across Cultures: LLMs for Intralingual Cultural Adaptation <https://arxiv.org/abs/2406.14504>`__ 跨文化翻译:语言内文化适应法学硕士

`[2406.14508] Evidence of a log scaling law for political persuasion with large language models <https://arxiv.org/abs/2406.14508>`__

`[2406.14511] Investigating Mysteries of CoT-Augmented Distillation <https://arxiv.org/abs/2406.14511>`__ 研究cot -增强蒸馏的奥秘

`[2406.14545] Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems <https://arxiv.org/abs/2406.14545>`__ 揭露数据库漏洞:Text-to-SQL系统中的零知识模式推理攻击

`[2406.14546] Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data <https://arxiv.org/abs/2406.14546>`__ 连接点:llm可以从不同的训练数据中推断和表达潜在结构

`[2406.14562] Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities <https://arxiv.org/abs/2406.14562>`__ 思维白板:跨模态一步一步地思考

`[2406.14563] Model Merging and Safety Alignment: One Bad Model Spoils the Bunch <https://arxiv.org/abs/2406.14563>`__ 模型合并和安全对齐:一个坏模型毁了一堆模型

`[2406.12925] GLiNER multi-task: Generalist Lightweight Model for Various Information Extraction Tasks <https://arxiv.org/abs/2406.12925>`__ GLiNER多任务:用于各种信息抽取任务的通用轻量级模型

`[2406.13175] Sparse High Rank Adapters <https://arxiv.org/abs/2406.13175>`__ 稀疏高阶适配器

`[2406.13193] PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes <https://arxiv.org/abs/2406.13193>`__ 快速:渐进式预训练增强合成化学结果

`[2406.13356] Jogging the Memory of Unlearned Model Through Targeted Relearning Attack <https://arxiv.org/abs/2406.13356>`__ 通过有针对性的再学习攻击来慢跑未学习模型的记忆

`[2406.13474] Attention-aware Post-training Quantization without Backpropagation <https://arxiv.org/abs/2406.13474>`__ 无反向传播的注意力感知训练后量化

`[2406.13777] Game of LLMs: Discovering Structural Constructs in Activities using Large Language Models <https://arxiv.org/abs/2406.13777>`__ llm的游戏:在使用大型语言模型的活动中发现结构性结构

`[2406.13868] SDQ: Sparse Decomposed Quantization for LLM Inference <https://arxiv.org/abs/2406.13868>`__

`[2406.13966] Causal Inference with Latent Variables: Recent Advances and Future Prospectives <https://arxiv.org/abs/2406.13966>`__ 基于潜变量的因果推断:最新进展与未来展望

`[2406.14045] Understanding Different Design Choices in Training Large Time Series Models <https://arxiv.org/abs/2406.14045>`__ 理解训练大型时间序列模型中的不同设计选择

`[2406.14150] Multi-modal Transfer Learning between Biological Foundation Models <https://arxiv.org/abs/2406.14150>`__ 生物基础模型间的多模态迁移学习

`[2406.14393] Jailbreaking as a Reward Misspecification Problem <https://arxiv.org/abs/2406.14393>`__ 越狱作为奖励的错误说明问题

`[2406.14473] Data-Centric AI in the Age of Large Language Models <https://arxiv.org/abs/2406.14473>`__ 大型语言模型时代以数据为中心的人工智能

`[2406.14517] PostMark: A Robust Blackbox Watermark for Large Language Models <https://arxiv.org/abs/2406.14517>`__ 邮戳:大型语言模型的鲁棒黑盒子水印

`[2406.14541] Are LLMs Naturally Good at Synthetic Tabular Data Generation? <https://arxiv.org/abs/2406.14541>`__ llm天生擅长合成表格数据生成吗?

`[2406.12934] Current state of LLM Risks and AI Guardrails <https://arxiv.org/abs/2406.12934>`__

`[2406.12935] ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates <https://arxiv.org/abs/2406.12935>`__ ChatBug:对齐llm中由聊天模板引起的常见漏洞

`[2406.12946] Instruction Data Generation and Unsupervised Adaptation for Speech Language Models <https://arxiv.org/abs/2406.12946>`__ 语音语言模型的指令数据生成与无监督自适应

`[2406.12950] MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction <https://arxiv.org/abs/2406.12950>`__ MolecularGPT:小样本分子特性预测的开放大型语言模型(LLM)

`[2406.13049] Assessing AI vs Human-Authored Spear Phishing SMS Attacks: An Empirical Study Using the TRAPD Method <https://arxiv.org/abs/2406.13049>`__ 评估人工智能与人工撰写的鱼叉式钓鱼短信攻击:基于TRAPD方法的实证研究

`[2406.13163] LLMatDesign: Autonomous Materials Discovery with Large Language Models <https://arxiv.org/abs/2406.13163>`__ LLMatDesign:基于大型语言模型的自主材料发现

`[2406.13215] Neural Residual Diffusion Models for Deep Scalable Vision Generation <https://arxiv.org/abs/2406.13215>`__ 深度可扩展视觉生成的神经残差扩散模型

`[2406.13235] Enhancing Collaborative Semantics of Language Model-Driven Recommendations via Graph-Aware Learning <https://arxiv.org/abs/2406.13235>`__

`[2406.13605] Nicer Than Humans: How do Large Language Models Behave in the Prisoner's Dilemma? <https://arxiv.org/abs/2406.13605>`__ 比人类更好:大型语言模型在囚徒困境中的表现如何?

`[2406.13631] On AI-Inspired UI-Design <https://arxiv.org/abs/2406.13631>`__ 受ai启发的ui设计

`[2406.13763] Through the Theory of Mind's Eye: Reading Minds with Multimodal Video Large Language Models <https://arxiv.org/abs/2406.13763>`__ 通过心灵之眼理论:用多模态视频大型语言模型读心术

`[2406.14086] Seg-LSTM: Performance of xLSTM for Semantic Segmentation of Remotely Sensed Images <https://arxiv.org/abs/2406.14086>`__ Seg-LSTM: xLSTM在遥感图像语义分割中的性能

`[2406.14088] ReaLHF: Optimized RLHF Training for Large Language Models through Parameter Reallocation <https://arxiv.org/abs/2406.14088>`__ ReaLHF:通过参数重分配优化的大型语言模型RLHF训练

`[2406.14097] Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration <https://arxiv.org/abs/2406.14097>`__ 通过人机协作增强基于llm的机器人操纵

`[2406.14318] The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in Prompts <https://arxiv.org/abs/2406.14318>`__ 火贼也是守护者:在提示中平衡可用性和隐私

`[2406.14358] The neural correlates of logical-mathematical symbol systems processing resemble that of spatial cognition more than natural language processing <https://arxiv.org/abs/2406.14358>`__ 与自然语言处理相比，逻辑-数学符号系统处理的神经关联更类似于空间认知

`[2406.10300] Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications <https://arxiv.org/abs/2406.10300>`__ 作为软件组件的大型语言模型:集成llm应用的分类法

`[2406.13275] Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding <https://arxiv.org/abs/2406.13275>`__ 通过优化音频编码的大型语言模型增强自动音频描述

`[2406.13362] VisualRWKV: Exploring Recurrent Neural Networks for Visual Language Models <https://arxiv.org/abs/2406.13362>`__ VisualRWKV:探索视觉语言模型的循环神经网络

`[2406.13898] The Use of Multimodal Large Language Models to Detect Objects from Thermal Images: Transportation Applications <https://arxiv.org/abs/2406.13898>`__ 使用多模态大型语言模型从热图像中检测目标:交通应用

`[2406.14043] Taxonomy-Guided Zero-Shot Recommendations with LLMs <https://arxiv.org/abs/2406.14043>`__ 基于llm的分类法指导的零样本推荐

`[2406.14117] An Investigation of Prompt Variations for Zero-shot LLM-based Rankers <https://arxiv.org/abs/2406.14117>`__ 基于零样本llm排序器的提示变化研究

`[2406.14129] Towards Event-oriented Long Video Understanding <https://arxiv.org/abs/2406.14129>`__

`[2406.14307] QuST-LLM: Integrating Large Language Models for Comprehensive Spatial Transcriptomics Analysis <https://arxiv.org/abs/2406.14307>`__ QuST-LLM:集成大型语言模型进行全面的空间转录组学分析

`[2406.14492] Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models? <https://arxiv.org/abs/2406.14492>`__ 物体接地真的能减少大型视觉-语言模型的幻觉吗?

`[2406.14544] Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs <https://arxiv.org/abs/2406.14544>`__ Prism: vlm解耦和评估框架

`[2406.14169] Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning <https://arxiv.org/abs/2406.14169>`__ 使用大型语言模型和强化学习优化新颖的Top-k推荐

`[2406.14549] Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models <https://arxiv.org/abs/2406.14549>`__ 揭示潜在记忆:评估大型语言模型中的数据泄漏和记忆模式

`[2311.09641] RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models <https://arxiv.org/abs/2311.09641>`__

`[2405.16434] The Importance of Directional Feedback for LLM-based Optimizers <https://arxiv.org/abs/2405.16434>`__ 定向反馈对基于llm的优化器的重要性

`[2406.06874] Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback <https://arxiv.org/abs/2406.06874>`__ 联合演示和偏好学习改进了与人工反馈的政策一致性

`[2406.09363] ElicitationGPT: Text Elicitation Mechanisms via Language Models <https://arxiv.org/abs/2406.09363>`__ elicitationongpt:基于语言模型的文本诱导机制

`[2406.10690] Bridging the Gap in Drug Safety Data Analysis: Large Language Models for SQL Query Generation <https://arxiv.org/abs/2406.10690>`__ 弥合药品安全数据分析中的差距:用于SQL查询生成的大型语言模型

`[2406.12058] WellDunn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions <https://arxiv.org/abs/2406.12058>`__ WellDunn:关于语言模型和大型语言模型在识别健康维度方面的鲁棒性和可解释性

`[2305.13582] Translation and Fusion Improves Zero-shot Cross-lingual Information Extraction <https://arxiv.org/abs/2305.13582>`__ 翻译和融合改进了零样本跨语言信息抽取

`[2309.08902] Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models <https://arxiv.org/abs/2309.08902>`__ 研究llm中的微妙偏见:生成模型中的年龄歧视、美貌、制度和国籍偏见

`[2310.00905] All Languages Matter: On the Multilingual Safety of Large Language Models <https://arxiv.org/abs/2310.00905>`__ 所有语言都很重要:关于大型语言模型的多语言安全性

`[2310.03304] Learning Personalized Alignment for Evaluating Open-ended Text Generation <https://arxiv.org/abs/2310.03304>`__ 评价开放式文本生成的个性化对齐学习

`[2310.07059] DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction <https://arxiv.org/abs/2310.07059>`__

`[2311.09105] MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation <https://arxiv.org/abs/2311.09105>`__ MAVEN-Arg:基于事件参数标注的一体化事件理解数据集拼图

`[2401.06837] Structsum Generation for Faster Text Comprehension <https://arxiv.org/abs/2401.06837>`__ 生成Structsum以加快文本理解

`[2401.07944] SemEval-2017 Task 4: Sentiment Analysis in Twitter using BERT <https://arxiv.org/abs/2401.07944>`__ SemEval-2017任务4:基于BERT的Twitter情感分析

`[2402.04957] Reconfidencing LLMs from the Grouping Loss Perspective <https://arxiv.org/abs/2402.04957>`__ 从分组损失的角度重新信任llm

`[2402.10663] Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL <https://arxiv.org/abs/2402.10663>`__ 通过人工无关的文本到sql融合提高演示多样性

`[2402.12590] Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation <https://arxiv.org/abs/2402.12590>`__ 不断进化的AI集体来增强人类多样性并实现自我调节

`[2402.12821] Identifying Factual Inconsistencies in Summaries: Grounding Model Inference via Task Taxonomy <https://arxiv.org/abs/2402.12821>`__ 识别摘要中的事实不一致:基于任务分类法的模型推理基础

`[2402.14857] Is the System Message Really Important to Jailbreaks in Large Language Models? <https://arxiv.org/abs/2402.14857>`__ 在大型语言模型中，系统消息对越狱真的很重要吗?

`[2402.14897] Chain-of-Thought Unfaithfulness as Disguised Accuracy <https://arxiv.org/abs/2402.14897>`__ 用思维链的不忠来伪装准确

`[2402.15537] Evaluating the Performance of ChatGPT for Spam Email Detection <https://arxiv.org/abs/2402.15537>`__ ChatGPT在垃圾邮件检测中的性能评估

`[2402.16159] DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem <https://arxiv.org/abs/2402.16159>`__ DistALANER:开源软件生态系统中远程监督主动学习增强的命名实体识别

`[2402.16602] Rethinking Negative Instances for Generative Named Entity Recognition <https://arxiv.org/abs/2402.16602>`__ 生成命名实体识别中负面实例的再思考

`[2403.02966] Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering <https://arxiv.org/abs/2403.02966>`__ 面向知识增强零样本问答的证据事实摘要

`[2403.07088] SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation <https://arxiv.org/abs/2403.07088>`__ SPA:面向计算友好的云端和设备上协作Seq2seq个性化生成

`[2403.07794] SIT: Fine-tuning Large Language Models with Sequential Instructions <https://arxiv.org/abs/2403.07794>`__ SIT:基于顺序指令的大型语言模型微调

`[2403.08010] Debatrix: Multi-dimensional Debate Judge with Iterative Chronological Analysis Based on LLM <https://arxiv.org/abs/2403.08010>`__ Debatrix:基于LLM迭代时间分析的多维辩论裁判

`[2403.10258] Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models <https://arxiv.org/abs/2403.10258>`__ 你只需要翻译吗?用大型语言模型解决多语言任务的研究

`[2404.01247] An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance <https://arxiv.org/abs/2404.01247>`__

`[2404.12933] Cross-cultural Inspiration Detection and Analysis in Real and LLM-generated Social Media Data <https://arxiv.org/abs/2404.12933>`__ 真实和llm生成的社交媒体数据中的跨文化灵感检测和分析

`[2404.12938] MAiDE-up: Multilingual Deception Detection of GPT-generated Hotel Reviews <https://arxiv.org/abs/2404.12938>`__ MAiDE-up:基于gpt生成酒店评论的多语言欺骗检测

`[2405.01943] Dependency-Aware Semi-Structured Sparsity: Declining Roles of Outliers in Pruning GLU-based LLMs <https://arxiv.org/abs/2405.01943>`__ 依赖感知半结构化稀疏:离群点在修剪基于glue的llm中的作用下降

`[2405.03371] Explainable Fake News Detection With Large Language Model via Defense Among Competing Wisdom <https://arxiv.org/abs/2405.03371>`__ 基于竞争智慧防御的大型语言模型可解释假新闻检测

`[2405.06258] Automatic Generation of Model and Data Cards: A Step Towards Responsible AI <https://arxiv.org/abs/2405.06258>`__

`[2405.08760] Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Non-Literal Intent Resolution in LLMs <https://arxiv.org/abs/2405.08760>`__ 教皇是天主教徒吗?是的，教皇是天主教徒。llm中非文字意图解决的生成式评估

`[2405.11357] Large Language Models Lack Understanding of Character Composition of Words <https://arxiv.org/abs/2405.11357>`__ 大型语言模型缺乏对单词字符组成的理解

`[2405.13929] Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for Russian <https://arxiv.org/abs/2405.13929>`__ Vikhr:开源指令调优俄语大型语言模型家族

`[2405.19660] PATIENT-{\Psi}: Using Large Language Models to Simulate Patients for Training Mental Health Professionals <https://arxiv.org/abs/2405.19660>`__ PATIENT-{\Psi}:使用大型语言模型模拟患者以培训心理健康专业人员

`[2405.19846] Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model <https://arxiv.org/abs/2405.19846>`__ Quest:面向大型语言模型长上下文扩展的以查询为中心的数据合成方法

`[2406.05460] Fighting Against the Repetitive Training and Sample Dependency Problem in Few-shot Named Entity Recognition <https://arxiv.org/abs/2406.05460>`__ 解决小样本命名实体识别中的重复训练和样本依赖问题

`[2406.06840] Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles <https://arxiv.org/abs/2406.06840>`__ 无声信号，巨大冲击:用于编码狗哨词义消歧的llm

`[2406.10594] BlockPruner: Fine-grained Pruning for Large Language Models <https://arxiv.org/abs/2406.10594>`__ BlockPruner:大型语言模型的细粒度剪枝

`[2406.11097] InstructCMP: Length Control in Sentence Compression through Instruction-based Large Language Models <https://arxiv.org/abs/2406.11097>`__ InstructCMP:基于指令的大型语言模型在句子压缩中的长度控制

`[2406.11131] Are Large Language Models a Good Replacement of Taxonomies? <https://arxiv.org/abs/2406.11131>`__ 大型语言模型是分类法的良好替代品吗?

`[2406.11354] Preserving Knowledge in Large Language Model with Model-Agnostic Self-Decompression <https://arxiv.org/abs/2406.11354>`__ 基于模型无关自解压的大型语言模型知识保持

`[2406.11565] Extrinsic Evaluation of Cultural Competence in Large Language Models <https://arxiv.org/abs/2406.11565>`__ 大型语言模型文化能力的外在评估

`[2406.11661] Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting <https://arxiv.org/abs/2406.11661>`__ 文化条件作用还是安慰剂?社会人口激励的有效性

`[2406.12033] Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models <https://arxiv.org/abs/2406.12033>`__ 用大型语言模型揭示和减轻心理健康分析中的偏见

`[2406.12746] Rationale-based Ensemble of Multiple QA Strategies for Zero-shot Knowledge-based VQA <https://arxiv.org/abs/2406.12746>`__ 基于理论基础的多QA策略集成的零样本知识库VQA

`[2404.03147] Eigenpruning: an Interpretability-Inspired PEFT Method <https://arxiv.org/abs/2404.03147>`__ Eigenpruning:一种基于可解释性的PEFT方法

`[2406.06385] Low-Rank Quantization-Aware Training for LLMs <https://arxiv.org/abs/2406.06385>`__ llm的低秩量化感知训练

`[2406.06858] FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion <https://arxiv.org/abs/2406.06858>`__ FLUX:通过内核融合实现gpu上基于软件的快速通信重叠

`[2406.12091] Is poisoning a real threat to LLM alignment? Maybe more so than you think <https://arxiv.org/abs/2406.12091>`__ 中毒是LLM联盟的真正威胁吗?可能比你想象的更严重

`[2406.12168] BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM <https://arxiv.org/abs/2406.12168>`__ BPO:通过秉承行为邻近性LLM来强化在线偏好学习

`[2406.12246] TroL: Traversal of Layers for Large Language and Vision Models <https://arxiv.org/abs/2406.12246>`__ TroL:大型语言和视觉模型的层遍历

`[2406.12649] Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models <https://arxiv.org/abs/2406.12649>`__ 概率概念解释器:视觉基础模型的可信概念解释

`[2310.01727] Can GPT-4 Replicate Empirical Software Engineering Research? <https://arxiv.org/abs/2310.01727>`__ GPT-4能复制经验软件工程研究吗?

`[2312.12423] Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model <https://arxiv.org/abs/2312.12423>`__

`[2401.05163] MISS: A Generative Pretraining and Finetuning Approach for Med-VQA <https://arxiv.org/abs/2401.05163>`__ MISS:一种面向Med-VQA的生成式预训练和微调方法

`[2402.03907] Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy <https://arxiv.org/abs/2402.03907>`__ 将大型语言模型嵌入扩展现实:包容、参与和隐私的机遇与挑战

`[2403.00863] LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction <https://arxiv.org/abs/2403.00863>`__ LLM-Ensemble:面向电商产品属性值抽取的最优大型语言模型集成方法

`[2403.06131] FewFedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning <https://arxiv.org/abs/2403.06131>`__ FewFedPIT:隐私保护与少样本联邦指令调优

`[2405.13068] Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation <https://arxiv.org/abs/2405.13068>`__ Lockpicking llm:一种基于logit的使用令牌级别操作的越狱工具

`[2406.00014] KU-DMIS at EHRSQL 2024:Generating SQL query via question templatization in EHR <https://arxiv.org/abs/2406.00014>`__ KU-DMIS在EHRSQL 2024:在EHR中通过问题模板化生成SQL查询

`[2406.12108] Computing in the Life Sciences: From Early Algorithms to Modern AI <https://arxiv.org/abs/2406.12108>`__ 生命科学中的计算:从早期算法到现代人工智能

`[2312.10321] LLM-SQL-Solver: Can LLMs Determine SQL Equivalence? <https://arxiv.org/abs/2312.10321>`__ LLM-SQL-Solver: llm可以确定SQL的等价性吗?

`[2402.14968] Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment <https://arxiv.org/abs/2402.14968>`__ 通过后门增强安全对齐缓解基于微调的越狱攻击

`[2403.05286] LLM4Decompile: Decompiling Binary Code with Large Language Models <https://arxiv.org/abs/2403.05286>`__ LLM4Decompile:用大型语言模型反编译二进制代码

`[2406.00799] Are you still on track!? Catching LLM Task Drift with Activations <https://arxiv.org/abs/2406.00799>`__ 你还在正轨上吗?用激活捕捉LLM任务漂移

`[2406.11171] SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations <https://arxiv.org/abs/2406.11171>`__ SUGARCREPE++数据集:视觉-语言模型对语义和词汇变化的敏感性

`[2312.05547] Signatures Meet Dynamic Programming: Generalizing Bellman Equations for Trajectory Following <https://arxiv.org/abs/2312.05547>`__

