240606
========

----------
Survey (1)
----------

`[2406.02622] Safeguarding Large Language Models: A Survey <https://arxiv.org/abs/2406.02622>`__ 大型语言模型的维护综述

::

    Mon, 3 Jun 2024 19:27:46 GMT
    Yi Dong, Ronghui Mu, Yanghao Zhang, Siqi Sun, Tianle Zhang, Changshun Wu, Gaojie Jin, Yi Qi, Jinwei Hu, Jie Meng, Saddek Bensalem, Xiaowei Huang

In the burgeoning field of Large Language Models (LLMs), developing a robust safety mechanism, colloquially known as "safeguards" or "guardrails", has become imperative to ensure the ethical use of LLMs within prescribed boundaries. This article provides a systematic literature review on the current status of this critical mechanism. It discusses its major challenges and how it can be enhanced into a comprehensive mechanism dealing with ethical issues in various contexts. First, the paper elucidates the current landscape of safeguarding mechanisms that major LLM service providers and the open-source community employ. This is followed by the techniques to evaluate, analyze, and enhance some (un)desirable properties that a guardrail might want to enforce, such as hallucinations, fairness, privacy, and so on. Based on them, we review techniques to circumvent these controls (i.e., attacks), to defend the attacks, and to reinforce the guardrails. While the techniques mentioned above represent the current status and the active research trends, we also discuss several challenges that cannot be easily dealt with by the methods and present our vision on how to implement a comprehensive guardrail through the full consideration of multi-disciplinary approach, neural-symbolic method, and systems development lifecycle.

------------

-------------
Benchmark (6)
-------------

`[2406.02903] Open Grounded Planning: Challenges and Benchmark Construction <https://arxiv.org/abs/2406.02903>`__ 开放式地面规划:挑战和基准建设

::

    Wed, 5 Jun 2024 03:46:52 GMT
    Shiguang Guo, Ziliang Deng, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun

The emergence of large language models (LLMs) has increasingly drawn attention to the use of LLMs for human-like planning. Existing work on LLM-based planning either focuses on leveraging the inherent language generation capabilities of LLMs to produce free-style plans, or employs reinforcement learning approaches to learn decision-making for a limited set of actions within restricted environments. However, both approaches exhibit significant discrepancies from the open and executable requirements in real-world planning. In this paper, we propose a new planning task--open grounded planning. The primary objective of open grounded planning is to ask the model to generate an executable plan based on a variable action set, thereby ensuring the executability of the produced plan. To this end, we establishes a benchmark for open grounded planning spanning a wide range of domains. Then we test current state-of-the-art LLMs along with five planning approaches, revealing that existing LLMs and methods still struggle to address the challenges posed by grounded planning in open domains. The outcomes of this paper define and establish a foundational dataset for open grounded planning, and shed light on the potential challenges and future directions of LLM-based planning.

------------

`[2406.03368] IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models <https://arxiv.org/abs/2406.03368>`__ IrokoBench:大型语言模型时代非洲语言的新基准

::

    Wed, 5 Jun 2024 15:23:08 GMT
    David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba O. Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, En-Shiun Annie Lee, Chiamaka Chukwuneke, Happy Buzaaba, Blessing Sibanda, Godson Kalipe, Jonathan Mukiibi, Salomon Kabongo, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Tadesse Kebede Guge, Pontus Stenetorp

Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages.
Additionally, many low-resource languages (e.g. African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages. In this paper, we introduce IrokoBench -- a human-translated benchmark dataset for 16 typologically-diverse low-resource African languages covering three tasks: natural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and multi-choice knowledge-based QA~(AfriMMLU). We use IrokoBench to evaluate zero-shot, few-shot, and translate-test settings~(where test sets are translated into English) across 10 open and four proprietary LLMs. Our evaluation reveals a significant performance gap between high-resource languages~(such as English and French) and low-resource African languages. We observe a significant performance gap between open and proprietary models, with the highest performing open model, Aya-101 only at 58\% of the best-performing proprietary model GPT-4o performance. Machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric, like LLaMa 3 70B. These findings suggest that more efforts are needed to develop and adapt LLMs for African languages.

------------

`[2406.02943] The Task-oriented Queries Benchmark (ToQB) <https://arxiv.org/abs/2406.02943>`__ 面向任务的查询基准(ToQB)

::

    Wed, 5 Jun 2024 05:05:41 GMT
    Keun Soo Yim

Task-oriented queries (e.g., one-shot queries to play videos, order food, or call a taxi) are crucial for assessing the quality of virtual assistants, chatbots, and other large language model (LLM)-based services. However, a standard benchmark for task-oriented queries is not yet available, as existing benchmarks in the relevant NLP (Natural Language Processing) fields have primarily focused on task-oriented dialogues. Thus, we present a new methodology for efficiently generating the Task-oriented Queries Benchmark (ToQB) using existing task-oriented dialogue datasets and an LLM service. Our methodology involves formulating the underlying NLP task to summarize the original intent of a speaker in each dialogue, detailing the key steps to perform the devised NLP task using an LLM service, and outlining a framework for automating a major part of the benchmark generation process. Through a case study encompassing three domains (i.e., two single-task domains and one multi-task domain), we demonstrate how to customize the LLM prompts (e.g., omitting system utterances or speaker labels) for those three domains and characterize the generated task-oriented queries. The generated ToQB dataset is made available to the public. We further discuss new domains that can be added to ToQB by community contributors and its practical applications.

------------

`[2310.20410] FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models <https://arxiv.org/abs/2310.20410>`__ FollowBench:遵循大型语言模型基准的多级细粒度约束

::

    replaced with revised version Wed, 5 Jun 2024 15:39:26 GMT
    Submission history From: Yuxin Jiang [view email]
    [v1] Tue, 31 Oct 2023 12:32:38 UTC (1,283 KB)
    [v2] Tue, 14 Nov 2023 11:01:06 UTC (1,290 KB)
    [v3] Wed, 5 Jun 2024 15:39:26 UTC (1,413 KB)
    Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, Wei Wang

The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating pure response quality, rather than assessing whether the response follows constraints stated in the instruction. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Situation, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each increased level. To assess whether LLMs' outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths to handle challenging open-ended instructions. By evaluating 13 closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at this https URL.

------------

`[2312.17080] MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation <https://arxiv.org/abs/2312.17080>`__ MR-GSM8K:面向大型语言模型评估的元推理基准

::

    replaced with revised version Wed, 5 Jun 2024 04:05:42 GMT
    Submission history From: Zhongshen Zeng [view email]
    [v1] Thu, 28 Dec 2023 15:49:43 UTC (1,358 KB)
    [v2] Sat, 20 Jan 2024 14:08:16 UTC (1,364 KB)
    [v3] Tue, 6 Feb 2024 12:27:52 UTC (1,364 KB)
    [v4] Wed, 5 Jun 2024 04:05:42 UTC (677 KB)
    Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia

In this work, we introduce a novel evaluation paradigm for Large Language Models (LLMs) that compels them to transition from a traditional question-answering role, akin to a student, to a solution-scoring role, akin to a teacher. This paradigm, focusing on "reasoning about reasoning," hence termed meta-reasoning, shifts the emphasis from result-oriented assessments, which often neglect the reasoning process, to a more comprehensive evaluation that effectively distinguishes between the cognitive capabilities of different models. By applying this paradigm in the GSM8K dataset, we have developed the MR-GSM8K benchmark. Our extensive analysis includes several state-of-the-art models from both open-source and commercial domains, uncovering fundamental deficiencies in their training and evaluation methodologies. Notably, while models like Deepseek-v2 and Claude3-Sonnet closely competed with GPT-4 in GSM8K, their performance disparities expanded dramatically in MR-GSM8K, with differences widening to over 20 absolute points, underscoring the significant challenge posed by our meta-reasoning approach.

------------

`[2402.13607] CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models <https://arxiv.org/abs/2402.13607>`__ CODIS:多模态大型语言模型上下文相关视觉理解基准测试

::

    replaced with revised version Wed, 5 Jun 2024 02:14:06 GMT
    Submission history From: Fuwen Luo [view email]
    [v1] Wed, 21 Feb 2024 08:21:12 UTC (2,054 KB)
    [v2] Fri, 15 Mar 2024 11:19:30 UTC (2,053 KB)
    [v3] Wed, 5 Jun 2024 02:14:06 UTC (2,054 KB)
    Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li, Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, Peng Li, Ning Ma, Maosong Sun, Yang Liu

Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a context-dependent manner. View our project website at this https URL.

------------

---------------
Accelerate (10)
---------------

`[2406.03482] QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead <https://arxiv.org/abs/2406.03482>`__ QJL:用于KV Cache量化的1比特量化JL变换，零开销

::

    Wed, 5 Jun 2024 17:42:05 GMT
    Amir Zandieh, Majid Daliri, Insu Han

Serving LLMs requires substantial memory due to the storage requirements of Key-Value (KV) embeddings in the KV cache, which grows with sequence length. An effective approach to compress KV cache is quantization. However, traditional quantization methods face significant memory overhead due to the need to store quantization constants (at least a zero point and a scale) in full precision per data block. Depending on the block size, this overhead can add 1 or 2 bits per quantized number. We introduce QJL, a new quantization approach that consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization. In contrast to existing methods, QJL eliminates memory overheads by removing the need for storing quantization constants. We propose an asymmetric estimator for the inner product of two vectors and demonstrate that applying QJL to one vector and a standard JL transform without quantization to the other provides an unbiased estimator with minimal distortion. We have developed an efficient implementation of the QJL sketch and its corresponding inner product estimator, incorporating a lightweight CUDA kernel for optimized computation. When applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime. Codes are available at \url{https://github.com/amirzandieh/QJL}.

------------

`[2403.05766] FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs <https://arxiv.org/abs/2403.05766>`__ FLAP: LLMs中基于约束解码的流粘附规划

::

    replaced with revised version Tue, 4 Jun 2024 20:55:04 GMT
    Submission history From: Shamik Roy [view email]
    [v1] Sat, 9 Mar 2024 02:27:45 UTC (7,367 KB)
    [v2] Sun, 31 Mar 2024 19:45:22 UTC (7,368 KB)
    [v3] Tue, 4 Jun 2024 20:55:04 UTC (7,369 KB)
    Shamik Roy, Sailik Sengupta, Daniele Bonadiman, Saab Mansour, Arshit Gupta

Planning is a crucial task for agents in task oriented dialogs (TODs). Human agents typically resolve user issues by following predefined workflows, decomposing workflow steps into actionable items, and performing actions by executing APIs in order; all of which require reasoning and planning. With the recent advances in LLMs, there have been increasing attempts to use them for task planning and API usage. However, the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with LLMs. Moreover, workflows in real life are often custom-defined and prone to changes; hence, adaptation is desirable. To study this, we propose the problem of faithful planning in TODs that needs to resolve user intents by following predefined flows and preserving API dependencies. To solve this problem, we propose FLAP, a Flow-Adhering Planning algorithm based on constrained decoding with lookahead heuristic for LLMs. Our algorithm alleviates the need for finetuning LLMs using domain specific (plan/dependency) data, enables quick adaptation to predefined flows, and outperforms other decoding and prompting-based baselines. Further, our algorithm empowers smaller LLMs (7B) to perform at par larger LLMs (30B-40B).

------------

`[2405.12532] PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference <https://arxiv.org/abs/2405.12532>`__ PyramidInfer:用于高吞吐量LLM推断的金字塔KV缓存压缩

::

    replaced with revised version Wed, 5 Jun 2024 09:01:24 GMT
    Submission history From: Dongjie Yang [view email]
    [v1] Tue, 21 May 2024 06:46:37 UTC (716 KB)
    [v2] Wed, 5 Jun 2024 09:01:24 UTC (713 KB)
    Dongjie Yang, XiaoDong Han, Yan Gao, Yao Hu, Shilin Zhang, Hai Zhao

Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.

------------

`[2406.00059] Conveyor: Efficient Tool-aware LLM Serving with Tool Partial Execution <https://arxiv.org/abs/2406.00059>`__ 输送机:工具部分执行的高效工具感知LLM

::

    replaced with revised version Tue, 4 Jun 2024 19:00:36 GMT
    Submission history From: Danyang Zhuo [view email]
    [v1] Wed, 29 May 2024 21:24:15 UTC (297 KB)
    [v2] Tue, 4 Jun 2024 19:00:36 UTC (379 KB)
    Yechen Xu, Xinhao Kong, Tingjun Chen, Danyang Zhuo

The complexity of large language model (LLM) serving workloads has substantially increased due to the integration with external tool invocations, such as ChatGPT plugins. In this paper, we identify a new opportunity for efficient LLM serving for requests that trigger tools: tool partial execution alongside LLM decoding. To this end, we design Conveyor, an efficient LLM serving system optimized for handling requests involving external tools. We introduce a novel interface for tool developers to expose partial execution opportunities to the LLM serving system and a request scheduler that facilitates partial tool execution. Our results demonstrate that tool partial execution can improve request completion latency by up to 38.8%.

------------

`[2401.06469] Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning <https://arxiv.org/abs/2401.06469>`__ Batch-ICL:有效、高效和顺序无关的上下文学习

::

    replaced with revised version Wed, 5 Jun 2024 09:13:17 GMT
    Submission history From: Kaiyi Zhang [view email]
    [v1] Fri, 12 Jan 2024 09:31:17 UTC (663 KB)
    [v2] Fri, 16 Feb 2024 10:58:52 UTC (672 KB)
    [v3] Wed, 5 Jun 2024 09:13:17 UTC (690 KB)
    Kaiyi Zhang, Ang Lv, Yuhan Chen, Hansen Ha, Tao Xu, Rui Yan

In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to the forward computation of a zero-shot query to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of ICL examples. In some cases, it even exceeds the performance of the best order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple "epochs" of meta-optimization. This variant implicitly explores permutations of ICL examples, further enhancing ICL performance.

------------

`[2402.00396] Efficient Exploration for LLMs <https://arxiv.org/abs/2402.00396>`__ llm的有效探索

::

    replaced with revised version Tue, 4 Jun 2024 18:35:09 GMT
    Submission history From: Botao Hao [view email]
    [v1] Thu, 1 Feb 2024 07:32:24 UTC (854 KB)
    [v2] Tue, 4 Jun 2024 18:35:09 UTC (892 KB)
    Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, Benjamin Van Roy

We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.

------------

`[2402.10500] Active Preference Optimization for Sample Efficient RLHF <https://arxiv.org/abs/2402.10500>`__ 样本高效RLHF的主动偏好优化

::

    replaced with revised version Wed, 5 Jun 2024 15:10:08 GMT
    Submission history From: Nirjhar Das [view email]
    [v1] Fri, 16 Feb 2024 08:19:34 UTC (982 KB)
    [v2] Wed, 5 Jun 2024 15:10:08 UTC (3,010 KB)
    Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, Sayak Ray Chowdhury

Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. Although aligned generative models have shown remarkable abilities in various tasks, their reliance on high-quality human preference data creates a costly bottleneck in the practical application of RLHF. One primary reason is that current methods rely on uniformly picking prompt-generation pairs from a dataset of prompt-generations, to collect human feedback, resulting in sub-optimal alignment under a constrained budget, which highlights the criticality of adaptive strategies in efficient alignment. Recent works [Mehta et al., 2023, Muldrew et al., 2024] have tried to address this problem by designing various heuristics based on generation uncertainty. However, either the assumptions in [Mehta et al., 2023] are restrictive, or [Muldrew et al., 2024] do not provide any rigorous theoretical guarantee. To address these, we reformulate RLHF within contextual preference bandit framework, treating prompts as contexts, and develop an active-learning algorithm, $\textit{Active Preference Optimization}$ ($\texttt{APO}$), which enhances model alignment by querying preference data from the most important samples, achieving superior performance for small sample budget. We analyze the theoretical performance guarantees of $\texttt{APO}$ under the BTL preference model showing that the suboptimality gap of the policy learned via $\texttt{APO}$ scales as $O(1/\sqrt{T})$ for a budget of $T$. We also show that collecting preference data by choosing prompts randomly leads to a policy that suffers a constant sub-optimality. We perform detailed experimental evaluations on practical preference datasets to validate $\texttt{APO}$'s efficacy over the existing methods, establishing it as a sample-efficient and practical solution of alignment in a cost-effective and scalable manner.

------------

`[2402.19009] Unified Generation, Reconstruction, and Representation: Generalized Diffusion with Adaptive Latent Encoding-Decoding <https://arxiv.org/abs/2402.19009>`__ 统一的生成、重建和表示:基于自适应潜编码-解码的广义扩散

::

    replaced with revised version Wed, 5 Jun 2024 07:28:52 GMT
    Submission history From: Guangyi Liu [view email]
    [v1] Thu, 29 Feb 2024 10:08:57 UTC (29,727 KB)
    [v2] Wed, 5 Jun 2024 07:28:52 UTC (29,929 KB)
    Guangyi Liu, Yu Wang, Zeyu Feng, Qiyu Wu, Liping Tang, Yuan Gao, Zhen Li, Shuguang Cui, Julian McAuley, Zichao Yang, Eric P. Xing, Zhiting Hu

The vast applications of deep generative models are anchored in three core capabilities -- generating new instances, reconstructing inputs, and learning compact representations -- across various data types, such as discrete text/protein sequences and continuous images. Existing model families, like variational autoencoders (VAEs), generative adversarial networks (GANs), autoregressive models, and (latent) diffusion models, generally excel in specific capabilities and data types but fall short in others. We introduce Generalized Encoding-Decoding Diffusion Probabilistic Models (EDDPMs) which integrate the core capabilities for broad applicability and enhanced performance. EDDPMs generalize the Gaussian noising-denoising in standard diffusion by introducing parameterized encoding-decoding. Crucially, EDDPMs are compatible with the well-established diffusion model objective and training recipes, allowing effective learning of the encoder-decoder parameters jointly with diffusion. By choosing appropriate encoder/decoder (e.g., large language models), EDDPMs naturally apply to different data types. Extensive experiments on text, proteins, and images demonstrate the flexibility to handle diverse data and tasks and the strong improvement over various existing models.

------------

`[2405.17849] I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models <https://arxiv.org/abs/2405.17849>`__ I-LLM:全量化低比特大型语言模型的高效纯整数推理

::

    replaced with revised version Wed, 5 Jun 2024 15:26:58 GMT
    Submission history From: Dawei Yang [view email]
    [v1] Tue, 28 May 2024 05:56:11 UTC (23,260 KB)
    [v2] Wed, 5 Jun 2024 15:26:58 UTC (23,260 KB)
    Xing Hu, Yuan Cheng, Dawei Yang, Zhihang Yuan, Jiangyong Yu, Chen Xu, Sifan Zhou

Post-training quantization (PTQ) serves as a potent technique to accelerate the inference of large language models (LLMs). Nonetheless, existing works still necessitate a considerable number of floating-point (FP) operations during inference, including additional quantization and de-quantization, as well as non-linear operators such as RMSNorm and Softmax. This limitation hinders the deployment of LLMs on the edge and cloud devices. In this paper, we identify the primary obstacle to integer-only quantization for LLMs lies in the large fluctuation of activations across channels and tokens in both linear and non-linear operations. To address this issue, we propose I-LLM, a novel integer-only fully-quantized PTQ framework tailored for LLMs. Specifically, (1) we develop Fully-Smooth Block-Reconstruction (FSBR) to aggressively smooth inter-channel variations of all activations and weights. (2) to alleviate degradation caused by inter-token variations, we introduce a novel approach called Dynamic Integer-only MatMul (DI-MatMul). This method enables dynamic quantization in full-integer matrix multiplication by dynamically quantizing the input and outputs with integer-only operations. (3) we design DI-ClippedSoftmax, DI-Exp, and DI-Normalization, which utilize bit shift to execute non-linear operators efficiently while maintaining accuracy. The experiment shows that our I-LLM achieves comparable accuracy to the FP baseline and outperforms non-integer quantization methods. For example, I-LLM can operate at W4A4 with negligible loss of accuracy. To our knowledge, we are the first to bridge the gap between integer-only quantization and LLMs. We've published our code on anonymous.4open.science, aiming to contribute to the advancement of this field.

------------

`[2403.15226] Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models <https://arxiv.org/abs/2403.15226>`__ 并非所有的注意力都是需要的:多模态大型语言模型的参数和计算高效迁移学习

::

    replaced with revised version Wed, 5 Jun 2024 05:52:44 GMT
    Submission history From: Qiong Wu [view email]
    [v1] Fri, 22 Mar 2024 14:20:34 UTC (2,059 KB)
    [v2] Wed, 5 Jun 2024 05:52:44 UTC (2,043 KB)
    Qiong Wu and Weihao Ye and Yiyi Zhou and Xiaoshuai Sun and Rongrong Ji

In this paper, we propose a novel parameter and computation efficient tuning method for Multi-modal Large Language Models (MLLMs), termed Efficient Attention Skipping (EAS). Concretely, we first reveal that multi-head attentions (MHAs), the main computational overhead of MLLMs, are often redundant to downstream tasks. Based on this observation, EAS evaluates the attention redundancy and skips the less important MHAs to speed up inference. Besides, we also propose a novel propagation-of-information adapter (PIA) to serve the attention skipping of EAS and keep parameter efficiency, which can be further re-parameterized into feed-forward networks (FFNs) for zero-extra latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN and a classic VL pre-trained model called METER, and conduct extensive experiments on a set of benchmarks. The experiments show that EAS not only retains high performance and parameter efficiency, but also greatly speeds up inference speed. For instance, LaVIN-EAS can obtain 89.98\% accuracy on ScineceQA while speeding up inference by 2.2 times to LaVIN

------------

-----------------------
In-Context Learning (6)
-----------------------

`[2406.02911] Improving In-Context Learning with Prediction Feedback for Sentiment Analysis <https://arxiv.org/abs/2406.02911>`__ 基于预测反馈改进上下文学习的情感分析

::

    Wed, 5 Jun 2024 04:04:08 GMT
    Hongling Xu, Qianlong Wang, Yice Zhang, Min Yang, Xi Zeng, Bing Qin, Ruifeng Xu

Large language models (LLMs) have achieved promising results in sentiment analysis through the in-context learning (ICL) paradigm. However, their ability to distinguish subtle sentiments still remains a challenge. Inspired by the human ability to adjust understanding via feedback, this paper enhances ICL by incorporating prior predictions and feedback, aiming to rectify sentiment misinterpretation of LLMs. Specifically, the proposed framework consists of three steps: (1) acquiring prior predictions of LLMs, (2) devising predictive feedback based on correctness, and (3) leveraging a feedback-driven prompt to refine sentiment understanding. Experimental results across nine sentiment analysis datasets demonstrate the superiority of our framework over conventional ICL methods, with an average F1 improvement of 5.95%.

------------

`[2406.02847] Exact Conversion of In-Context Learning to Model Weights <https://arxiv.org/abs/2406.02847>`__ 上下文学习到模型权重的精确转换

::

    Wed, 5 Jun 2024 01:47:40 GMT
    Brian K Chen, Tianyang Hu, Hui Jin, Hwee Kuan Lee, Kenji Kawaguchi

In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years. In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates. In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner. Existing methods are not exact and require expensive parameter updates.
We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer. We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized. Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms.

------------

`[2401.12097] An Empirical Study of In-context Learning in LLMs for Machine Translation <https://arxiv.org/abs/2401.12097>`__ 面向机器翻译的llm语境学习实证研究

::

    replaced with revised version Tue, 4 Jun 2024 19:37:52 GMT
    Submission history From: Jay Gala [view email]
    [v1] Mon, 22 Jan 2024 16:35:00 UTC (6,653 KB)
    [v2] Sat, 17 Feb 2024 07:08:56 UTC (6,588 KB)
    [v3] Tue, 4 Jun 2024 19:37:52 UTC (6,594 KB)
    Pranjal A. Chitale, Jay Gala, Raj Dabre

Recent interest has surged in employing Large Language Models (LLMs) for machine translation (MT) via in-context learning (ICL) (Vilar et al., 2023). Most prior studies primarily focus on optimizing translation quality, with limited attention to understanding the specific aspects of ICL that influence the said quality. To this end, we perform the first of its kind, an exhaustive study of in-context learning for machine translation. We first establish that ICL is primarily example-driven and not instruction-driven. Following this, we conduct an extensive exploration of various aspects of the examples to understand their influence on downstream performance. Our analysis includes factors such as quality and quantity of demonstrations, spatial proximity, and source versus target originality. Further, we also investigate challenging scenarios involving indirectness and misalignment of examples to understand the limits of ICL. While we establish the significance of the quality of the target distribution over the source distribution of demonstrations, we further observe that perturbations sometimes act as regularizers, resulting in performance improvements. Surprisingly, ICL does not necessitate examples from the same task, and a related task with the same target distribution proves sufficient. We hope that our study acts as a guiding resource for considerations in utilizing ICL for MT. Our code is available on this https URL.

------------

`[2402.10024] Self-Augmented In-Context Learning for Unsupervised Word Translation <https://arxiv.org/abs/2402.10024>`__ 基于自增强上下文学习的无监督单词翻译

::

    replaced with revised version Wed, 5 Jun 2024 13:38:42 GMT
    Submission history From: Yaoyiran Li [view email]
    [v1] Thu, 15 Feb 2024 15:43:05 UTC (8,010 KB)
    [v2] Wed, 5 Jun 2024 13:38:42 UTC (8,014 KB)
    Yaoyiran Li, Anna Korhonen, Ivan Vuli\'c

Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised BLI performance, we also conduct comprehensive analyses on SAIL and discuss its limitations.

------------

`[2401.06469] Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning <https://arxiv.org/abs/2401.06469>`__ Batch-ICL:有效、高效和顺序无关的上下文学习

::

    replaced with revised version Wed, 5 Jun 2024 09:13:17 GMT
    Submission history From: Kaiyi Zhang [view email]
    [v1] Fri, 12 Jan 2024 09:31:17 UTC (663 KB)
    [v2] Fri, 16 Feb 2024 10:58:52 UTC (672 KB)
    [v3] Wed, 5 Jun 2024 09:13:17 UTC (690 KB)
    Kaiyi Zhang, Ang Lv, Yuhan Chen, Hansen Ha, Tao Xu, Rui Yan

In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to the forward computation of a zero-shot query to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of ICL examples. In some cases, it even exceeds the performance of the best order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple "epochs" of meta-optimization. This variant implicitly explores permutations of ICL examples, further enhancing ICL performance.

------------

`[2402.15607] How Do Nonlinear Transformers Learn and Generalize in In-Context Learning? <https://arxiv.org/abs/2402.15607>`__ 非线性transformer如何在上下文学习中学习和泛化?

::

    replaced with revised version Wed, 5 Jun 2024 07:04:56 GMT
    Submission history From: Hongkang Li [view email]
    [v1] Fri, 23 Feb 2024 21:07:20 UTC (843 KB)
    [v2] Wed, 5 Jun 2024 07:04:56 UTC (593 KB)
    Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, Pin-Yu Chen

Transformer-based large language models have displayed impressive in-context learning capabilities, where a pre-trained model can handle new tasks without fine-tuning by simply augmenting the query with some input-output examples from that task. Despite the empirical success, the mechanics of how to train a Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive due to the technical challenges of analyzing the nonconvex training problems resulting from the nonlinear self-attention and nonlinear activation in Transformers. To the best of our knowledge, this paper provides the first theoretical analysis of the training dynamics of Transformers with nonlinear self-attention and nonlinear MLP, together with the ICL generalization capability of the resulting model. Focusing on a group of binary classification tasks, we train Transformers using data from a subset of these tasks and quantify the impact of various factors on the ICL generalization performance on the remaining unseen tasks with and without data distribution shifts. We also analyze how different components in the learned Transformers contribute to the ICL performance. Furthermore, we provide the first theoretical analysis of how model pruning affects ICL performance and prove that proper magnitude-based pruning can have a minimal impact on ICL while reducing inference costs. These theoretical findings are justified through numerical experiments.

------------

--------------
Reasoning (10)
--------------

`[2406.02746] RATT: AThought Structure for Coherent and Correct LLMReasoning <https://arxiv.org/abs/2406.02746>`__ RATT:一种用于连贯和正确推理的思想结构

::

    Tue, 4 Jun 2024 20:02:52 GMT
    Jinghan Zhang, Xiting Wang, Weijieying Ren, Lu Jiang, Dongjie Wang, Kunpeng Liu

Large Language Models (LLMs) gain substantial reasoning and decision-making capabilities from thought structures. However, existing methods such as Tree of Thought and Retrieval Augmented Thoughts often fall short in complex tasks due to the limitations of insufficient local retrieval of factual knowledge and inadequate global selection of strategies. These limitations make it challenging for these methods to balance factual accuracy and comprehensive logical optimization effectively. To address these limitations, we introduce the Retrieval Augmented Thought Tree (RATT), a novel thought structure that considers both overall logical soundness and factual correctness at each step of the thinking process. Specifically, at every point of a thought branch, RATT performs planning and lookahead to explore and evaluate multiple potential reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented Generation (RAG) with LLM's ability to assess overall strategy. Through this combination of factual knowledge and strategic feasibility, the RATT adjusts and integrates the thought tree structure to search for the most promising branches within the search space. This thought structure significantly enhances the model's coherence in logical inference and efficiency in decision-making, and thus increases the limit of the capacity of LLM to generate reliable inferences and decisions based on thought structures. A broad range of experiments on different types of tasks showcases that the RATT structure significantly outperforms existing methods in factual correctness and logical coherence.

------------

`[2406.02787] Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities <https://arxiv.org/abs/2406.02787>`__ 解缠逻辑:上下文在大型语言模型推理能力中的作用

::

    Tue, 4 Jun 2024 21:25:06 GMT
    Wenyue Hua, Kaijie Zhu, Lingyao Li, Lizhou Fan, Shuhang Lin, Mingyu Jin, Haochen Xue, Zelong Li, JinDong Wang, Yongfeng Zhang

This study intends to systematically disentangle pure logic reasoning and text understanding by investigating the contrast across abstract and contextualized logical problems from a comprehensive set of domains. We explore whether LLMs demonstrate genuine reasoning capabilities across various domains when the underlying logical structure remains constant. We focus on two main questions (1) Can abstract logical problems alone accurately benchmark an LLM's reasoning ability in real-world scenarios, disentangled from contextual support in practical settings? (2) Does fine-tuning LLMs on abstract logic problem generalize to contextualized logic problems and vice versa? To investigate these questions, we focus on standard propositional logic, specifically propositional deductive and abductive logic reasoning. In particular, we construct instantiated datasets for deductive and abductive reasoning with 4 levels of difficulty, encompassing 12 distinct categories or domains based on the categorization of Wikipedia. Our experiments aim to provide insights into disentangling context in logical reasoning and the true reasoning capabilities of LLMs and their generalization potential. The code and dataset are available at: https://github.com/agiresearch/ContextHub.

------------

`[2406.02864] NUMCoT: Numerals and Units of Measurement in Chain-of-Thought Reasoning using Large Language Models <https://arxiv.org/abs/2406.02864>`__ NUMCoT:使用大型语言模型的思维链推理中的数字和测量单位

::

    Wed, 5 Jun 2024 02:26:14 GMT
    Ancheng Xu, Minghuan Tan, Lei Wang, Min Yang, Ruifeng Xu

Numeral systems and units of measurement are two conjoined topics in activities of human beings and have mutual effects with the languages expressing them. Currently, the evaluation of Large Language Models (LLMs) often involves mathematical reasoning, yet little attention is given to how minor changes in numbers or units can drastically alter the complexity of problems and the performance of LLMs. In this paper, we scrutinize existing LLMs on processing of numerals and units of measurement by constructing datasets with perturbations. We first anatomize the reasoning of math word problems to different sub-procedures like numeral conversions from language to numbers and measurement conversions based on units. Then we further annotate math word problems from ancient Chinese arithmetic works which are challenging in numerals and units of measurement. Experiments on perturbed datasets demonstrate that LLMs still encounter difficulties in handling numeral and measurement conversions.

------------

`[2406.03068] How Truncating Weights Improves Reasoning in Language Models <https://arxiv.org/abs/2406.03068>`__ 截断权重如何改善语言模型的推理

::

    Wed, 5 Jun 2024 08:51:08 GMT
    Lei Chen, Joan Bruna, Alberto Bietti

In addition to the ability to generate fluent text in various languages, large language models have been successful at tasks that involve basic forms of logical "reasoning" over their context. Recent work found that selectively removing certain components from weight matrices in pre-trained models can improve such reasoning capabilities. We investigate this phenomenon further by carefully studying how certain global associations tend to be stored in specific weight components or Transformer blocks, in particular feed-forward layers. Such associations may hurt predictions in reasoning tasks, and removing the corresponding components may then improve performance. We analyze how this arises during training, both empirically and theoretically, on a two-layer Transformer trained on a basic reasoning task with noise, a toy associative memory model, and on the Pythia family of pre-trained models tested on simple reasoning tasks.

------------

`[2401.00139] Is Knowledge All Large Language Models Needed for Causal Reasoning? <https://arxiv.org/abs/2401.00139>`__ 知识是因果推理所需的所有大型语言模型吗?

::

    replaced with revised version Wed, 5 Jun 2024 07:12:02 GMT
    Submission history From: Hengrui Cai [view email]
    [v1] Sat, 30 Dec 2023 04:51:46 UTC (12,224 KB)
    [v2] Wed, 5 Jun 2024 07:12:02 UTC (13,467 KB)
    Hengrui Cai, Shengjie Liu, Rui Song

This paper explores the causal reasoning of large language models (LLMs) to enhance their interpretability and reliability in advancing artificial intelligence. Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration. We propose a novel causal attribution model that utilizes ``do-operators" for constructing counterfactual scenarios, allowing us to systematically quantify the influence of input numerical data and LLMs' pre-existing knowledge on their causal reasoning processes. Our newly developed experimental setup assesses LLMs' reliance on contextual information and inherent knowledge across various domains. Our evaluation reveals that LLMs' causal reasoning ability mainly depends on the context and domain-specific knowledge provided. In the absence of such knowledge, LLMs can still maintain a degree of causal reasoning using the available numerical data, albeit with limitations in the calculations. This motivates the proposed fine-tuned LLM for pairwise causal discovery, effectively leveraging both knowledge and numerical information.

------------

`[2305.12599] Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning <https://arxiv.org/abs/2305.12599>`__ 基于抽象语义表示的逻辑驱动逻辑推理数据增强

::

    replaced with revised version Wed, 5 Jun 2024 12:54:51 GMT
    Submission history From: Qiming Bao [view email]
    [v1] Sun, 21 May 2023 23:16:26 UTC (1,070 KB)
    [v2] Sat, 14 Oct 2023 09:44:32 UTC (6,515 KB)
    [v3] Mon, 26 Feb 2024 22:44:36 UTC (4,070 KB)
    [v4] Sat, 30 Mar 2024 11:14:55 UTC (4,140 KB)
    [v5] Wed, 5 Jun 2024 12:54:51 UTC (4,140 KB)
    Qiming Bao, Alex Yuxuan Peng, Zhenyun Deng, Wanjun Zhong, Gael Gendron, Timothy Pistotti, Neset Tan, Nathan Young, Yang Chen, Yonghua Zhu, Paul Denny, Michael Witbrock, Jiamou Liu

Combining large language models with logical reasoning enhances their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges when gathering reliable data from the web to build comprehensive training datasets, subsequently affecting performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logical structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into text to create augmented data. Notably, our methodology is architecture-agnostic and enhances both generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and discriminative large language models through contrastive learning with logic-driven data augmentation. Empirical evidence underscores the efficacy of our proposed method with improvement in performance across seven downstream tasks, such as reading comprehension requiring logical reasoning, textual entailment, and natural language inference. Furthermore, our method leads on the ReClor leaderboard\footnote{\url{this https URL}}. The source code and data are publicly available\footnote{\href{this https URL}{AMR-LDA GitHub Repository}}.

------------

`[2311.10537] MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning <https://arxiv.org/abs/2311.10537>`__ MedAgents:作为零样本医学推理合作者的大型语言模型

::

    replaced with revised version Tue, 4 Jun 2024 23:47:43 GMT
    Submission history From: Xiangru Tang [view email]
    [v1] Thu, 16 Nov 2023 11:47:58 UTC (1,904 KB)
    [v2] Mon, 19 Feb 2024 18:26:46 UTC (2,347 KB)
    [v3] Tue, 20 Feb 2024 06:12:14 UTC (2,347 KB)
    [v4] Tue, 4 Jun 2024 23:47:43 UTC (2,348 KB)
    Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, Mark Gerstein

Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at this https URL.

------------

`[2312.17080] MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation <https://arxiv.org/abs/2312.17080>`__ MR-GSM8K:面向大型语言模型评估的元推理基准

::

    replaced with revised version Wed, 5 Jun 2024 04:05:42 GMT
    Submission history From: Zhongshen Zeng [view email]
    [v1] Thu, 28 Dec 2023 15:49:43 UTC (1,358 KB)
    [v2] Sat, 20 Jan 2024 14:08:16 UTC (1,364 KB)
    [v3] Tue, 6 Feb 2024 12:27:52 UTC (1,364 KB)
    [v4] Wed, 5 Jun 2024 04:05:42 UTC (677 KB)
    Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia

In this work, we introduce a novel evaluation paradigm for Large Language Models (LLMs) that compels them to transition from a traditional question-answering role, akin to a student, to a solution-scoring role, akin to a teacher. This paradigm, focusing on "reasoning about reasoning," hence termed meta-reasoning, shifts the emphasis from result-oriented assessments, which often neglect the reasoning process, to a more comprehensive evaluation that effectively distinguishes between the cognitive capabilities of different models. By applying this paradigm in the GSM8K dataset, we have developed the MR-GSM8K benchmark. Our extensive analysis includes several state-of-the-art models from both open-source and commercial domains, uncovering fundamental deficiencies in their training and evaluation methodologies. Notably, while models like Deepseek-v2 and Claude3-Sonnet closely competed with GPT-4 in GSM8K, their performance disparities expanded dramatically in MR-GSM8K, with differences widening to over 20 absolute points, underscoring the significant challenge posed by our meta-reasoning approach.

------------

`[2406.02030] Multimodal Reasoning with Multimodal Knowledge Graph <https://arxiv.org/abs/2406.02030>`__ 基于多模态知识图谱的多模态推理

::

    replaced with revised version Wed, 5 Jun 2024 03:28:01 GMT
    Submission history From: Junlin Lee [view email]
    [v1] Tue, 4 Jun 2024 07:13:23 UTC (1,056 KB)
    [v2] Wed, 5 Jun 2024 03:28:01 UTC (1,056 KB)
    Junlin Lee and Yequan Wang and Jing Li and Min Zhang

Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, MR-MKG achieves superior performance while training on only a small fraction of parameters, approximately 2.25% of the LLM's parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR-MKG method outperforms previous state-of-the-art models.

------------

`[2405.01573] Class-Level Code Generation from Natural Language Using Iterative, Tool-Enhanced Reasoning over Repository <https://arxiv.org/abs/2405.01573>`__ 使用迭代式、工具增强的知识库推理从自然语言生成类级别代码

::

    replaced with revised version Wed, 5 Jun 2024 17:44:24 GMT
    Submission history From: Anmol Agarwal [view email]
    [v1] Mon, 22 Apr 2024 03:52:54 UTC (550 KB)
    [v2] Wed, 5 Jun 2024 17:44:24 UTC (474 KB)
    Ajinkya Deshpande, Anmol Agarwal, Shashank Shet, Arun Iyer, Aditya Kanade, Ramakrishna Bairi, Suresh Parthasarathy

LLMs have demonstrated significant potential in code generation tasks, achieving promising results at the function or statement level across various benchmarks. However, the complexities associated with creating code artifacts like classes, particularly within the context of real-world software repositories, remain underexplored. Prior research treats class-level generation as an isolated task, neglecting the intricate dependencies & interactions that characterize real-world software environments. To address this gap, we introduce RepoClassBench, a comprehensive benchmark designed to rigorously evaluate LLMs in generating complex, class-level code within real-world repositories. RepoClassBench includes "Natural Language to Class generation" tasks across Java, Python & C# from a selection of repositories. We ensure that each class in our dataset not only has cross-file dependencies within the repository but also includes corresponding test cases to verify its functionality. We find that current models struggle with the realistic challenges posed by our benchmark, primarily due to their limited exposure to relevant repository contexts. To address this shortcoming, we introduce Retrieve-Repotools-Reflect (RRR), a novel approach that equips LLMs with static analysis tools to iteratively navigate & reason about repository-level context in an agent-based framework. Our experiments demonstrate that RRR significantly outperforms existing baselines on RepoClassBench, showcasing its effectiveness across programming languages & under various settings. Our findings emphasize the critical need for code-generation benchmarks to incorporate repo-level dependencies to more accurately reflect the complexities of software development. Our work shows the benefits of leveraging specialized tools to enhance LLMs' understanding of repository context. We plan to make our dataset & evaluation harness public.

------------

-----------
ToolUse (2)
-----------

`[2406.00059] Conveyor: Efficient Tool-aware LLM Serving with Tool Partial Execution <https://arxiv.org/abs/2406.00059>`__ 输送机:工具部分执行的高效工具感知LLM

::

    replaced with revised version Tue, 4 Jun 2024 19:00:36 GMT
    Submission history From: Danyang Zhuo [view email]
    [v1] Wed, 29 May 2024 21:24:15 UTC (297 KB)
    [v2] Tue, 4 Jun 2024 19:00:36 UTC (379 KB)
    Yechen Xu, Xinhao Kong, Tingjun Chen, Danyang Zhuo

The complexity of large language model (LLM) serving workloads has substantially increased due to the integration with external tool invocations, such as ChatGPT plugins. In this paper, we identify a new opportunity for efficient LLM serving for requests that trigger tools: tool partial execution alongside LLM decoding. To this end, we design Conveyor, an efficient LLM serving system optimized for handling requests involving external tools. We introduce a novel interface for tool developers to expose partial execution opportunities to the LLM serving system and a request scheduler that facilitates partial tool execution. Our results demonstrate that tool partial execution can improve request completion latency by up to 38.8%.

------------

`[2405.01573] Class-Level Code Generation from Natural Language Using Iterative, Tool-Enhanced Reasoning over Repository <https://arxiv.org/abs/2405.01573>`__ 使用迭代式、工具增强的知识库推理从自然语言生成类级别代码

::

    replaced with revised version Wed, 5 Jun 2024 17:44:24 GMT
    Submission history From: Anmol Agarwal [view email]
    [v1] Mon, 22 Apr 2024 03:52:54 UTC (550 KB)
    [v2] Wed, 5 Jun 2024 17:44:24 UTC (474 KB)
    Ajinkya Deshpande, Anmol Agarwal, Shashank Shet, Arun Iyer, Aditya Kanade, Ramakrishna Bairi, Suresh Parthasarathy

LLMs have demonstrated significant potential in code generation tasks, achieving promising results at the function or statement level across various benchmarks. However, the complexities associated with creating code artifacts like classes, particularly within the context of real-world software repositories, remain underexplored. Prior research treats class-level generation as an isolated task, neglecting the intricate dependencies & interactions that characterize real-world software environments. To address this gap, we introduce RepoClassBench, a comprehensive benchmark designed to rigorously evaluate LLMs in generating complex, class-level code within real-world repositories. RepoClassBench includes "Natural Language to Class generation" tasks across Java, Python & C# from a selection of repositories. We ensure that each class in our dataset not only has cross-file dependencies within the repository but also includes corresponding test cases to verify its functionality. We find that current models struggle with the realistic challenges posed by our benchmark, primarily due to their limited exposure to relevant repository contexts. To address this shortcoming, we introduce Retrieve-Repotools-Reflect (RRR), a novel approach that equips LLMs with static analysis tools to iteratively navigate & reason about repository-level context in an agent-based framework. Our experiments demonstrate that RRR significantly outperforms existing baselines on RepoClassBench, showcasing its effectiveness across programming languages & under various settings. Our findings emphasize the critical need for code-generation benchmarks to incorporate repo-level dependencies to more accurately reflect the complexities of software development. Our work shows the benefits of leveraging specialized tools to enhance LLMs' understanding of repository context. We plan to make our dataset & evaluation harness public.

------------

-----------------------
Retrieval-Augmented (5)
-----------------------

`[2406.03092] FragRel: Exploiting Fragment-level Relations in the External Memory of Large Language Models <https://arxiv.org/abs/2406.03092>`__ FragRel:利用大型语言模型外部记忆中的片段级关系

::

    Wed, 5 Jun 2024 09:31:37 GMT
    Xihang Yue, Linchao Zhu, Yi Yang

To process contexts with unlimited length using Large Language Models (LLMs), recent studies explore hierarchically managing the long text. Only several text fragments are taken from the external memory and passed into the temporary working memory, i.e., LLM's context window. However, existing approaches isolatedly handle the text fragments without considering their structural connections, thereby suffering limited capability on texts with intensive inter-relations, e.g., coherent stories and code repositories. This work attempts to resolve this by exploiting the fragment-level relations in external memory. First, we formulate the fragment-level relations and present several instantiations for different text types. Next, we introduce a relation-aware fragment assessment criteria upon previous independent fragment assessment.
Finally, we present the fragment-connected Hierarchical Memory based LLM. We validate the benefits of involving these relations on long story understanding, repository-level code generation, and long-term chatting.

------------

`[2406.03085] Exploring User Retrieval Integration towards Large Language Models for Cross-Domain Sequential Recommendation <https://arxiv.org/abs/2406.03085>`__ 面向大型语言模型的用户检索集成跨领域序列推荐

::

    Wed, 5 Jun 2024 09:19:54 GMT
    Tingjia Shen, Hao Wang, Jiaqing Zhang, Sirui Zhao, Liangyue Li, Zulong Chen, Defu Lian, Enhong Chen

Cross-Domain Sequential Recommendation (CDSR) aims to mine and transfer users' sequential preferences across different domains to alleviate the long-standing cold-start issue. Traditional CDSR models capture collaborative information through user and item modeling while overlooking valuable semantic information. Recently, Large Language Model (LLM) has demonstrated powerful semantic reasoning capabilities, motivating us to introduce them to better capture semantic information. However, introducing LLMs to CDSR is non-trivial due to two crucial issues: seamless information integration and domain-specific generation. To this end, we propose a novel framework named URLLM, which aims to improve the CDSR performance by exploring the User Retrieval approach and domain grounding on LLM simultaneously. Specifically, we first present a novel dual-graph sequential model to capture the diverse information, along with an alignment and contrastive learning method to facilitate domain knowledge transfer. Subsequently, a user retrieve-generation model is adopted to seamlessly integrate the structural information into LLM, fully harnessing its emergent inferencing ability. Furthermore, we propose a domain-specific strategy and a refinement module to prevent out-of-domain generation. Extensive experiments on Amazon demonstrated the information integration and domain-specific generation ability of URLLM in comparison to state-of-the-art baselines. Our code is available at https://github.com/TingJShen/URLLM

------------

`[2406.02925] SYN2REAL: Leveraging Task Arithmetic for Mitigating Synthetic-Real Discrepancies in ASR Domain Adaptation <https://arxiv.org/abs/2406.02925>`__ SYN2REAL:利用任务算法缓解ASR域适应中合成真实差异

::

    Wed, 5 Jun 2024 04:25:56 GMT
    Hsuan Su, Hua Farn, Shang-Tse Chen, Hung-yi Lee

Recent advancements in large language models (LLMs) have introduced the 'task vector' concept, which has significantly impacted various domains but remains underexplored in speech recognition. This paper presents a novel 'SYN2REAL' task vector for domain adaptation in automatic speech recognition (ASR), specifically targeting text-only domains. Traditional fine-tuning on synthetic speech often results in performance degradation due to acoustic mismatches. To address this issue, we propose creating a 'SYN2REAL' vector by subtracting the parameter differences between models fine-tuned on real and synthetic speech.
This vector effectively bridges the gap between the two domains. Experiments on the SLURP dataset demonstrate that our approach yields an average improvement of 11.15% in word error rate for unseen target domains, highlighting the potential of task vectors in enhancing speech domain adaptation.

------------

`[2402.16457] RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering <https://arxiv.org/abs/2402.16457>`__ RetrievalQA:评估短文本开放域问答的自适应检索增强生成

::

    replaced with revised version Wed, 5 Jun 2024 05:23:21 GMT
    Submission history From: Zihan Zhang [view email]
    [v1] Mon, 26 Feb 2024 09:59:04 UTC (213 KB)
    [v2] Wed, 5 Jun 2024 05:23:21 UTC (217 KB)
    Zihan Zhang, Meng Fang, Ling Chen

Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training. The dataset and code will be available at this https URL

------------

`[2403.10131] RAFT: Adapting Language Model to Domain Specific RAG <https://arxiv.org/abs/2403.10131>`__ RAFT:面向领域RAG的语言模型

::

    replaced with revised version Wed, 5 Jun 2024 17:27:51 GMT
    Submission history From: Tianjun Zhang [view email]
    [v1] Fri, 15 Mar 2024 09:26:02 UTC (760 KB)
    [v2] Wed, 5 Jun 2024 17:27:51 UTC (1,068 KB)
    Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, Joseph E. Gonzalez

Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a "open-book" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at this http URL.

------------

---------
Agent (8)
---------

`[2406.02818] Chain of Agents: Large Language Models Collaborating on Long-Context Tasks <https://arxiv.org/abs/2406.02818>`__ 智能体链:在长上下文任务上协作的大型语言模型

::

    Tue, 4 Jun 2024 23:36:08 GMT
    Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, Sercan \"O. Arik

Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs). Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit of LLMs. However, both strategies have drawbacks: input reduction has no guarantee of covering the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task. To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by a manager agent who synthesizes these contributions into a coherent final output.
CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent a short context. We perform comprehensive evaluation of CoA on a wide range of long-context tasks in question answering, summarization, and code completion, demonstrating significant improvements by up to 10% over strong baselines of RAG, Full-Context, and multi-agent LLMs.

------------

`[2406.03007] BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents <https://arxiv.org/abs/2406.03007>`__ BadAgent:在LLM代理中插入和激活后门攻击

::

    Wed, 5 Jun 2024 07:14:28 GMT
    Yifei Wang, Dizhan Xue, Shengjie Zhang, Shengsheng Qian

With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task.
However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent

------------

`[2406.03075] Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework <https://arxiv.org/abs/2406.03075>`__ 基于马尔可夫链的多智能体辩论框架的llm幻觉检测

::

    Wed, 5 Jun 2024 08:59:45 GMT
    Xiaoxi Sun, Jinpeng Li, Yan Zhong, Dongyan Zhao, Rui Yan

The advent of large language models (LLMs) has facilitated the development of natural language text generation. It also poses unprecedented challenges, with content hallucination emerging as a significant concern. Existing solutions often involve expensive and complex interventions during the training process.
Moreover, some approaches emphasize problem disassembly while neglecting the crucial validation process, leading to performance degradation or limited applications. To overcome these limitations, we propose a Markov Chain-based multi-agent debate verification framework to enhance hallucination detection accuracy in concise claims. Our method integrates the fact-checking process, including claim detection, evidence retrieval, and multi-agent verification. In the verification stage, we deploy multiple agents through flexible Markov Chain-based debates to validate individual claims, ensuring meticulous verification outcomes. Experimental results across three generative tasks demonstrate that our approach achieves significant improvements over baselines.

------------

`[2406.03008] DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences <https://arxiv.org/abs/2406.03008>`__ DriVLMe:用具身和社会经验增强基于llm的自动驾驶代理

::

    Wed, 5 Jun 2024 07:14:44 GMT
    Yidong Huang, Jacob Sansom, Ziqiao Ma, Felix Gervits, Joyce Chai

Recent advancements in foundation models (FMs) have unlocked new prospects in autonomous driving, yet the experimental settings of these studies are preliminary, over-simplified, and fail to capture the complexity of real-world driving scenarios in human environments. It remains under-explored whether FM agents can handle long-horizon navigation tasks with free-from dialogue and deal with unexpected situations caused by environmental dynamics or task changes. To explore the capabilities and boundaries of FMs faced with the challenges above, we introduce DriVLMe, a video-language-model-based agent to facilitate natural and effective communication between humans and autonomous vehicles that perceive the environment and navigate. We develop DriVLMe from both embodied experiences in a simulated environment and social experiences from real human dialogue. While DriVLMe demonstrates competitive performance in both open-loop benchmarks and closed-loop human studies, we reveal several limitations and challenges, including unacceptable inference time, imbalanced training data, limited visual understanding, challenges with multi-turn interactions, simplified language generation from robotic experiences, and difficulties in handling on-the-fly unexpected situations like environmental dynamics and task changes.

------------

`[2311.10537] MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning <https://arxiv.org/abs/2311.10537>`__ MedAgents:作为零样本医学推理合作者的大型语言模型

::

    replaced with revised version Tue, 4 Jun 2024 23:47:43 GMT
    Submission history From: Xiangru Tang [view email]
    [v1] Thu, 16 Nov 2023 11:47:58 UTC (1,904 KB)
    [v2] Mon, 19 Feb 2024 18:26:46 UTC (2,347 KB)
    [v3] Tue, 20 Feb 2024 06:12:14 UTC (2,347 KB)
    [v4] Tue, 4 Jun 2024 23:47:43 UTC (2,348 KB)
    Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, Mark Gerstein

Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at this https URL.

------------

`[2312.17025] Experiential Co-Learning of Software-Developing Agents <https://arxiv.org/abs/2312.17025>`__ 软件开发agent的经验协同学习

::

    replaced with revised version Wed, 5 Jun 2024 13:39:20 GMT
    Submission history From: Chen Qian [view email]
    [v1] Thu, 28 Dec 2023 13:50:42 UTC (506 KB)
    [v2] Fri, 29 Dec 2023 12:50:08 UTC (506 KB)
    [v3] Wed, 5 Jun 2024 13:39:20 UTC (1,660 KB)
    Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Zihao Xie, Yifei Wang, Weize Chen, Cheng Yang, Xin Cong, Xiaoyin Che, Zhiyuan Liu, Maosong Sun

Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce Experiential Co-Learning, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at this https URL.

------------

`[2402.04247] Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science <https://arxiv.org/abs/2402.04247>`__ 维护优先于自主:LLM代理对科学的风险

::

    replaced with revised version Wed, 5 Jun 2024 06:13:09 GMT
    Submission history From: Xiangru Tang [view email]
    [v1] Tue, 6 Feb 2024 18:54:07 UTC (12,836 KB)
    [v2] Wed, 7 Feb 2024 14:26:02 UTC (12,836 KB)
    [v3] Mon, 3 Jun 2024 21:45:53 UTC (12,838 KB)
    [v4] Wed, 5 Jun 2024 06:13:09 UTC (12,838 KB)
    Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein

Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, these agents, called scientific LLM agents, also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This perspective paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.

------------

`[2307.07924] ChatDev: Communicative Agents for Software Development <https://arxiv.org/abs/2307.07924>`__ ChatDev:用于软件开发的通信agent

::

    replaced with revised version Wed, 5 Jun 2024 13:23:49 GMT
    Submission history From: Chen Qian [view email]
    [v1] Sun, 16 Jul 2023 02:11:34 UTC (12,275 KB)
    [v2] Tue, 18 Jul 2023 09:51:21 UTC (12,275 KB)
    [v3] Mon, 28 Aug 2023 08:38:38 UTC (12,275 KB)
    [v4] Tue, 19 Dec 2023 12:56:13 UTC (13,383 KB)
    [v5] Wed, 5 Jun 2024 13:23:49 UTC (2,809 KB)
    Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, Maosong Sun

Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at this https URL.

------------

-----------
Other (120)
-----------

`[2406.02791] Language Models can Infer Action Semantics for Classical Planners from Environment Feedback <https://arxiv.org/abs/2406.02791>`__ 语言模型可以从环境反馈中推断经典规划器的动作语义

::

    Tue, 4 Jun 2024 21:29:56 GMT
    Wang Zhu, Ishika Singh, Robin Jia, Jesse Thomason

Classical planning approaches guarantee finding a set of actions that can achieve a given goal state when possible, but require an expert to specify logical action semantics that govern the dynamics of the environment.
Researchers have shown that Large Language Models (LLMs) can be used to directly infer planning steps based on commonsense knowledge and minimal domain information alone, but such plans often fail on execution. We bring together the strengths of classical planning and LLM commonsense inference to perform domain induction, learning and validating action pre- and post-conditions based on closed-loop interactions with the environment itself. We propose PSALM, which leverages LLM inference to heuristically complete partial plans emitted by a classical planner given partial domain knowledge, as well as to infer the semantic rules of the domain in a logical language based on environment feedback after execution. Our analysis on 7 environments shows that with just one expert-curated example plans, using LLMs as heuristic planners and rule predictors achieves lower environment execution steps and environment resets than random exploration while simultaneously recovering the underlying ground truth action semantics of the domain.

------------

`[2406.02804] $\texttt{ACCORD}$: Closing the Commonsense Measurability Gap <https://arxiv.org/abs/2406.02804>`__ $\texttt{ACCORD}$:缩小常识上的可测量性差距

::

    Tue, 4 Jun 2024 22:08:24 GMT
    Fran\c{c}ois Roewer-Despr\'es and Jinyue Feng and Zining Zhu and Frank Rudzicz

We present $\texttt{ACCORD}$, a framework and benchmark suite for disentangling the commonsense grounding and reasoning abilities of large language models (LLMs) through controlled, multi-hop counterfactuals.
$\texttt{ACCORD}$ introduces formal elements to commonsense reasoning to explicitly control and quantify reasoning complexity beyond the typical 1 or 2 hops. Uniquely, $\texttt{ACCORD}$ can automatically generate benchmarks of arbitrary reasoning complexity, and so it scales with future LLM improvements.
Benchmarking state-of-the-art LLMs -- including GPT-4o (2024-05-13), Llama-3-70B-Instruct, and Mixtral-8x22B-Instruct-v0.1 -- shows performance degrading to random chance with only moderate scaling, leaving substantial headroom for improvement. We release a leaderboard of the benchmark suite tested in this work, as well as code for automatically generating more complex benchmarks.

------------

`[2406.03299] The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games <https://arxiv.org/abs/2406.03299>`__ The Good, The Bad, and The Hulk-like GPT:分析合作和议价游戏中大型语言模型的情感决策

::

    Wed, 5 Jun 2024 14:08:54 GMT
    Mikhail Mozikov, Nikita Severin, Valeria Bodishtianu, Maria Glushanina, Mikhail Baklashkin, Andrey V. Savchenko, Ilya Makarov

Behavior study experiments are an important part of society modeling and understanding human interactions. In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies. Recent advances in Large Language Models (LLMs) have provided researchers with a new promising tool for the simulation of human behavior. However, existing LLM-based simulations operate under the unproven hypothesis that LLM agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions.
In this paper, we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states. Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies. While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions. Surprisingly, emotional prompting, particularly with `anger' emotion, can disrupt the "superhuman" alignment of GPT-4, resembling human emotional responses.

------------

`[2406.03367] CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic Task Planning <https://arxiv.org/abs/2406.03367>`__ CLMASP:耦合大型语言模型与回答集编程的机器人任务规划

::

    Wed, 5 Jun 2024 15:21:44 GMT
    Xinrui Lin, Yangfan Wu, Huanyu Yang, Yu Zhang, Yanyong Zhang, Jianmin Ji

Large Language Models (LLMs) possess extensive foundational knowledge and moderate reasoning abilities, making them suitable for general task planning in open-world scenarios. However, it is challenging to ground a LLM-generated plan to be executable for the specified robot with certain restrictions. This paper introduces CLMASP, an approach that couples LLMs with Answer Set Programming (ASP) to overcome the limitations, where ASP is a non-monotonic logic programming formalism renowned for its capacity to represent and reason about a robot's action knowledge. CLMASP initiates with a LLM generating a basic skeleton plan, which is subsequently tailored to the specific scenario using a vector database. This plan is then refined by an ASP program with a robot's action knowledge, which integrates implementation details into the skeleton, grounding the LLM's abstract outputs in practical robot contexts. Our experiments conducted on the VirtualHome platform demonstrate CLMASP's efficacy. Compared to the baseline executable rate of under 2% with LLM approaches, CLMASP significantly improves this to over 90%.

------------

`[2406.02575] Cross-Modal Safety Alignment: Is textual unlearning all you need? <https://arxiv.org/abs/2406.02575>`__ 跨模态安全对齐:文本遗忘是你所需要的一切吗?

::

    Mon, 27 May 2024 20:29:13 GMT
    Trishna Chakraborty, Erfan Shayegani, Zikui Cai, Nael Abu-Ghazaleh, M. Salman Asif, Yue Dong, Amit K. Roy-Chowdhury, Chengyu Song

Recent studies reveal that integrating new modalities into Large Language Models (LLMs), such as Vision-Language Models (VLMs), creates a new attack surface that bypasses existing safety training techniques like Supervised Fine-tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF). While further SFT and RLHF-based safety training can be conducted in multi-modal settings, collecting multi-modal training datasets poses a significant challenge. Inspired by the structural design of recent multi-modal models, where, regardless of the combination of input modalities, all inputs are ultimately fused into the language space, we aim to explore whether unlearning solely in the textual domain can be effective for cross-modality safety alignment. Our evaluation across six datasets empirically demonstrates the transferability -- textual unlearning in VLMs significantly reduces the Attack Success Rate (ASR) to less than 8\% and in some cases, even as low as nearly 2\% for both text-based and vision-text-based attacks, alongside preserving the utility. Moreover, our experiments show that unlearning with a multi-modal dataset offers no potential benefits but incurs significantly increased computational demands, possibly up to 6 times higher.

------------

`[2406.02721] Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller <https://arxiv.org/abs/2406.02721>`__ 通过将后缀梯度压缩为前缀控制器实现LLM行为自控

::

    Tue, 4 Jun 2024 19:05:10 GMT
    Min Cai and Yuchen Zhang and Shichang Zhang and Fan Yin and Difan Zou and Yisong Yue and Ziniu Hu

We propose Self-Control, a novel method utilizing suffix gradients to control the behavior of large language models (LLMs) without explicit human annotations. Given a guideline expressed in suffix string and the model's self-assessment of adherence, Self-Control computes the gradient of this self-judgment concerning the model's hidden states, directly influencing the auto-regressive generation process towards desired behaviors. To enhance efficiency, we introduce Self-Control_{prefix}, a compact module that encapsulates the learned representations from suffix gradients into a Prefix Controller, facilitating inference-time control for various LLM behaviors. Our experiments demonstrate Self-Control's efficacy across multiple domains, including emotional modulation, ensuring harmlessness, and enhancing complex reasoning. Especially, Self-Control_{prefix} enables a plug-and-play control and jointly controls multiple attributes, improving model outputs without altering model parameters or increasing inference-time costs.

------------

`[2406.02756] Aligning Large Language Models via Fine-grained Supervision <https://arxiv.org/abs/2406.02756>`__ 基于细粒度监督的大型语言模型对齐

::

    Tue, 4 Jun 2024 20:21:45 GMT
    Dehong Xu, Liang Qiu, Minseok Kim, Faisal Ladhak, Jaeyoung Do

Pre-trained large-scale language models (LLMs) excel at producing coherent articles, yet their outputs may be untruthful, toxic, or fail to align with user expectations. Current approaches focus on using reinforcement learning with human feedback (RLHF) to improve model alignment, which works by transforming coarse human preferences of LLM outputs into a feedback signal that guides the model learning process. However, because this approach operates on sequence-level feedback, it lacks the precision to identify the exact parts of the output affecting user preferences. To address this gap, we propose a method to enhance LLM alignment through fine-grained token-level supervision.
Specifically, we ask annotators to minimally edit less preferred responses within the standard reward modeling dataset to make them more favorable, ensuring changes are made only where necessary while retaining most of the original content. The refined dataset is used to train a token-level reward model, which is then used for training our fine-grained Proximal Policy Optimization (PPO) model. Our experiment results demonstrate that this approach can achieve up to an absolute improvement of $5.1\%$ in LLM performance, in terms of win rate against the reference model, compared with the traditional PPO model.

------------

`[2406.02856] Xmodel-LM Technical Report <https://arxiv.org/abs/2406.02856>`__ Xmodel-LM技术报告

::

    Wed, 5 Jun 2024 02:12:06 GMT
    Yichuan Wang, Yang Liu, Yu Yan, Xucheng Huang, Ling Jiang

We introduce Xmodel-LM, a compact and efficient 1.1B language model pre-trained on over 2 trillion tokens. Trained on our self-built dataset (Xdata), which balances Chinese and English corpora based on downstream task optimization, Xmodel-LM exhibits remarkable performance despite its smaller size. It notably surpasses existing open-source language models of similar scale. Our model checkpoints and code are publicly accessible on GitHub at https://github.com/XiaoduoAILab/XmodelLM.

------------

`[2406.02863] LLM as a Scorer: The Impact of Output Order on Dialogue Evaluation <https://arxiv.org/abs/2406.02863>`__ LLM作为评分者:输出顺序对对话评估的影响

::

    Wed, 5 Jun 2024 02:25:10 GMT
    Yi-Pei Chen, KuanChao Chu, Hideki Nakayama

This research investigates the effect of prompt design on dialogue evaluation using large language models (LLMs). While LLMs are increasingly used for scoring various inputs, creating effective prompts for dialogue evaluation remains challenging due to model sensitivity and subjectivity in dialogue assessments. Our study experimented with different prompt structures, altering the sequence of output instructions and including explanatory reasons. We found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a "reason-first" approach yielding more comprehensive evaluations. This insight is crucial for enhancing the accuracy and consistency of LLM-based evaluations.

------------

`[2406.02886] PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs <https://arxiv.org/abs/2406.02886>`__ PLaD:基于伪偏好对的大规模语言模型蒸馏

::

    Wed, 5 Jun 2024 03:08:25 GMT
    Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Haorui Wang, Zhen Qin, Feng Han, Jialu Liu, Simon Baumgartner, Michael Bendersky, Chao Zhang

Large Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings. Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models. However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue. In this work, we present PLaD, a novel preference-based LLM distillation framework.
PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs. Then, PLaD leverages a ranking loss to re-calibrate student's estimation of sequence likelihood, which steers the student's focus towards understanding the relative quality of outputs instead of simply imitating the teacher. PLaD bypasses the need for access to teacher LLM's internal states, tackles the student's expressivity limitations, and mitigates the student mis-calibration issue. Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our proposed PLaD framework.

------------

`[2406.02888] HYDRA: Model Factorization Framework for Black-Box LLM Personalization <https://arxiv.org/abs/2406.02888>`__ HYDRA:黑盒LLM个性化的模型分解框架

::

    Wed, 5 Jun 2024 03:08:46 GMT
    Yuchen Zhuang, Haotian Sun, Yue Yu, Qifan Wang, Chao Zhang, Bo Dai

Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences. Despite the remarkable few-shot capabilities exhibited by black-box large language models (LLMs), the inherent opacity of their model parameters presents significant challenges in aligning the generated output with individual expectations. Existing solutions have primarily focused on prompt design to incorporate user-specific profiles and behaviors; however, such approaches often struggle to generalize effectively due to their inability to capture shared knowledge among all users.
To address these challenges, we propose HYDRA, a model factorization framework that captures both user-specific behavior patterns from historical data and shared general knowledge among all users to deliver personalized generation. In order to capture user-specific behavior patterns, we first train a reranker to prioritize the most useful information from top-retrieved relevant historical records. By combining the prioritized history with the corresponding query, we train an adapter to align the output with individual user-specific preferences, eliminating the reliance on access to inherent model parameters of black-box LLMs. Both the reranker and the adapter can be decomposed into a base model with multiple user-specific heads, resembling a hydra. The base model maintains shared knowledge across users, while the multiple personal heads capture user-specific preferences. Experimental results demonstrate that HYDRA outperforms existing state-of-the-art prompt-based methods by an average relative improvement of 9.01% across five diverse personalization tasks in the LaMP benchmark. Our implementation is available at https://github.com/night-chen/HYDRA.

------------

`[2406.02919] MultifacetEval: Multifaceted Evaluation to Probe LLMs in Mastering Medical Knowledge <https://arxiv.org/abs/2406.02919>`__ MultifacetEval:从多方面探讨llm对医学知识的掌握

::

    Wed, 5 Jun 2024 04:15:07 GMT
    Yuxuan Zhou, Xien Liu, Chen Ning, Ji Wu

Large language models (LLMs) have excelled across domains, also delivering notable performance on the medical evaluation benchmarks, such as MedQA.
However, there still exists a significant gap between the reported performance and the practical effectiveness in real-world medical scenarios. In this paper, we aim to explore the causes of this gap by employing a multifaceted examination schema to systematically probe the actual mastery of medical knowledge by current LLMs. Specifically, we develop a novel evaluation framework MultifacetEval to examine the degree and coverage of LLMs in encoding and mastering medical knowledge at multiple facets (comparison, rectification, discrimination, and verification) concurrently. Based on the MultifacetEval framework, we construct two multifaceted evaluation datasets: MultiDiseK (by producing questions from a clinical disease knowledge base) and MultiMedQA (by rephrasing each question from a medical benchmark MedQA into multifaceted questions). The experimental results on these multifaceted datasets demonstrate that the extent of current LLMs in mastering medical knowledge is far below their performance on existing medical benchmarks, suggesting that they lack depth, precision, and comprehensiveness in mastering medical knowledge.
Consequently, current LLMs are not yet ready for application in real-world medical tasks. The codes and datasets are available at https://github.com/THUMLP/MultifacetEval.

------------

`[2406.02959] Adversarial Moment-Matching Distillation of Large Language Models <https://arxiv.org/abs/2406.02959>`__ 大型语言模型的对抗矩匹配蒸馏

::

    Wed, 5 Jun 2024 05:27:29 GMT
    Chen Jia

Knowledge distillation (KD) has been shown to be highly effective in guiding a student model with a larger teacher model and achieving practical benefits in improving the computational and memory efficiency for large language models (LLMs). State-of-the-art KD methods for LLMs mostly rely on minimizing explicit distribution distance between teacher and student probability predictions.
Instead of optimizing these mandatory behaviour cloning objectives, we explore an imitation learning strategy for KD of LLMs. In particular, we minimize the imitation gap by matching the action-value moments of the teacher's behavior from both on- and off-policy perspectives. To achieve this action-value moment-matching goal, we propose an adversarial training algorithm to jointly estimate the moment-matching distance and optimize the student policy to minimize it. Results from both task-agnostic instruction-following experiments and task-specific experiments demonstrate the effectiveness of our method and achieve new state-of-the-art performance.

------------

`[2406.02962] Docs2KG: Unified Knowledge Graph Construction from Heterogeneous Documents Assisted by Large Language Models <https://arxiv.org/abs/2406.02962>`__ Docs2KG:大型语言模型辅助异构文档的统一知识图谱构建

::

    Wed, 5 Jun 2024 05:35:59 GMT
    Qiang Sun, Yuanyi Luo, Wenxiao Zhang, Sirui Li, Jichunyang Li, Kai Niu, Xiangrui Kong, Wei Liu

Even for a conservative estimate, 80% of enterprise data reside in unstructured files, stored in data lakes that accommodate heterogeneous formats. Classical search engines can no longer meet information seeking needs, especially when the task is to browse and explore for insight formulation. In other words, there are no obvious search keywords to use. Knowledge graphs, due to their natural visual appeals that reduce the human cognitive load, become the winning candidate for heterogeneous data integration and knowledge representation.
In this paper, we introduce Docs2KG, a novel framework designed to extract multimodal information from diverse and heterogeneous unstructured documents, including emails, web pages, PDF files, and Excel files. Dynamically generates a unified knowledge graph that represents the extracted key information, Docs2KG enables efficient querying and exploration of document data lakes.
Unlike existing approaches that focus on domain-specific data sources or pre-designed schemas, Docs2KG offers a flexible and extensible solution that can adapt to various document structures and content types. The proposed framework unifies data processing supporting a multitude of downstream tasks with improved domain interpretability. Docs2KG is publicly accessible at https://docs2kg.ai4wa.com, and a demonstration video is available at https://docs2kg.ai4wa.com/Video.

------------

`[2406.03004] Evaluation of data inconsistency for multi-modal sentiment analysis <https://arxiv.org/abs/2406.03004>`__ 面向多模态情感分析的数据不一致性评估

::

    Wed, 5 Jun 2024 07:11:56 GMT
    Yufei Wang, Mengyue Wu

Emotion semantic inconsistency is an ubiquitous challenge in multi-modal sentiment analysis (MSA). MSA involves analyzing sentiment expressed across various modalities like text, audio, and videos. Each modality may convey distinct aspects of sentiment, due to subtle and nuanced expression of human beings, leading to inconsistency, which may hinder the prediction of artificial agents. In this work, we introduce a modality conflicting test set and assess the performance of both traditional multi-modal sentiment analysis models and multi-modal large language models (MLLMs). Our findings reveal significant performance degradation across traditional models when confronted with semantically conflicting data and point out the drawbacks of MLLMs when handling multi-modal emotion analysis. Our research presents a new challenge and offer valuable insights for the future development of sentiment analysis systems.

------------

`[2406.03009] Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models <https://arxiv.org/abs/2406.03009>`__ 揭示选择偏差:探索大型语言模型中的顺序和Token敏感性

::

    Wed, 5 Jun 2024 07:16:51 GMT
    Sheng-Lun Wei, Cheng-Kuang Wu, Hen-Hsen Huang, Hsin-Hsi Chen

In this paper, we investigate the phenomena of "selection biases" in Large Language Models (LLMs), focusing on problems where models are tasked with choosing the optimal option from an ordered sequence. We delve into biases related to option order and token usage, which significantly impact LLMs' decision-making processes. We also quantify the impact of these biases through an extensive empirical analysis across multiple models and tasks. Furthermore, we propose mitigation strategies to enhance model performance. Our key contributions are threefold: 1) Precisely quantifying the influence of option order and token on LLMs, 2) Developing strategies to mitigate the impact of token and order sensitivity to enhance robustness, and 3) Offering a detailed analysis of sensitivity across models and tasks, which informs the creation of more stable and reliable LLM applications for selection problems.

------------

`[2406.03030] From Tarzan to Tolkien: Controlling the Language Proficiency Level of LLMs for Content Generation <https://arxiv.org/abs/2406.03030>`__ 从Tarzan到Tolkien:控制llm内容生成的语言熟练程度

::

    Wed, 5 Jun 2024 07:57:17 GMT
    Ali Malik, Stephen Mayhew, Chris Piech, Klinton Bicknell

We study the problem of controlling the difficulty level of text generated by Large Language Models (LLMs) for contexts where end-users are not fully proficient, such as language learners. Using a novel framework, we evaluate the effectiveness of several key approaches for this task, including few-shot prompting, supervised finetuning, and reinforcement learning (RL), utilising both GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.
Our findings reveal a large performance gap between GPT-4 and the open source models when using prompt-based strategies. However, we show how to bridge this gap with a careful combination of finetuning and RL alignment. Our best model, CALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and other strategies, at only a fraction of the cost. We further validate the quality of our results through a small-scale human study.

------------

`[2406.03079] Cryptocurrency Frauds for Dummies: How ChatGPT introduces us to fraud? <https://arxiv.org/abs/2406.03079>`__ 面向傻瓜的加密货币欺诈:ChatGPT如何向我们介绍欺诈?

::

    Wed, 5 Jun 2024 09:09:32 GMT
    Wail Zellagui, Abdessamad Imine, Yamina Tadjeddine

Recent advances in the field of large language models (LLMs), particularly the ChatGPT family, have given rise to a powerful and versatile machine interlocutor, packed with knowledge and challenging our understanding of learning. This interlocutor is a double-edged sword: it can be harnessed for a wide variety of beneficial tasks, but it can also be used to cause harm. This study explores the complicated interaction between ChatGPT and the growing problem of cryptocurrency fraud. Although ChatGPT is known for its adaptability and ethical considerations when used for harmful purposes, we highlight the deep connection that may exist between ChatGPT and fraudulent actions in the volatile cryptocurrency ecosystem. Based on our categorization of cryptocurrency frauds, we show how to influence outputs, bypass ethical terms, and achieve specific fraud goals by manipulating ChatGPT prompts. Furthermore, our findings emphasize the importance of realizing that ChatGPT could be a valuable instructor even for novice fraudsters, as well as understanding and safely deploying complex language models, particularly in the context of cryptocurrency frauds. Finally, our study underlines the importance of using LLMs responsibly and ethically in the digital currency sector, identifying potential risks and resolving ethical issues. It should be noted that our work is not intended to encourage and promote fraud, but rather to raise awareness of the risks of fraud associated with the use of ChatGPT.

------------

`[2406.03151] Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation <https://arxiv.org/abs/2406.03151>`__ 你站在哪一边?用于端到端论点总结和评估的多任务数据集

::

    Wed, 5 Jun 2024 11:15:45 GMT
    Hao Li, Yuping Wu, Viktor Schlegel, Riza Batista-Navarro, Tharindu Madusanka, Iqra Zahid, Jiayan Zeng, Xiaochi Wang, Xinran He, Yizhi Li and Goran Nenadic

With the recent advances of large language models (LLMs), it is no longer infeasible to build an automated debate system that helps people to synthesise persuasive arguments. Previous work attempted this task by integrating multiple components. In our work, we introduce an argument mining dataset that captures the end-to-end process of preparing an argumentative essay for a debate, which covers the tasks of claim and evidence identification (Task 1 ED), evidence convincingness ranking (Task 2 ECR), argumentative essay summarisation and human preference ranking (Task 3 ASR) and metric learning for automated evaluation of resulting essays, based on human feedback along argument quality dimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are fully annotated with the various properties supporting the aforementioned tasks. We evaluate multiple generative baselines for each of these tasks, including representative LLMs. We find, that while they show promising results on individual tasks in our benchmark, their end-to-end performance on all four tasks in succession deteriorates significantly, both in automated measures as well as in human-centred evaluation. This challenge presented by our proposed dataset motivates future research on end-to-end argument mining and summarisation. The repository of this project is available at https://github.com/HarrywillDr/ArgSum-Datatset

------------

`[2406.03158] CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs <https://arxiv.org/abs/2406.03158>`__ CSS: LLMs不确定性量化的对比语义相似度

::

    Wed, 5 Jun 2024 11:35:44 GMT
    Shuang Ao, Stefan Rueger, Advaith Siddharthan

Despite the impressive capability of large language models (LLMs), knowing when to trust their generations remains an open challenge. The recent literature on uncertainty quantification of natural language generation (NLG) utilises a conventional natural language inference (NLI) classifier to measure the semantic dispersion of LLMs responses. These studies employ logits of NLI classifier for semantic clustering to estimate uncertainty. However, logits represent the probability of the predicted class and barely contain feature information for potential clustering. Alternatively, CLIP (Contrastive Language-Image Pre-training) performs impressively in extracting image-text pair features and measuring their similarity. To extend its usability, we propose Contrastive Semantic Similarity, the CLIP-based feature extraction module to obtain similarity features for measuring uncertainty for text pairs.
We apply this method to selective NLG, which detects and rejects unreliable generations for better trustworthiness of LLMs. We conduct extensive experiments with three LLMs on several benchmark question-answering datasets with comprehensive evaluation metrics. Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines. Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines. The code are available at \url{https://github.com/AoShuang92/css_uq_llms}.

------------

`[2406.03170] StatBot.Swiss: Bilingual Open Data Exploration in Natural Language <https://arxiv.org/abs/2406.03170>`__ StatBot。Swiss:自然语言双语开放数据探索

::

    Wed, 5 Jun 2024 12:03:19 GMT
    Farhad Nooralahzadeh, Yi Zhang, Ellery Smith, Sabine Maennel, Cyril Matthey-Doret, Rapha\"el de Fondville, Kurt Stockinger

The potential for improvements brought by Large Language Models (LLMs) in Text-to-SQL systems is mostly assessed on monolingual English datasets.
However, LLMs' performance for other languages remains vastly unexplored. In this work, we release the StatBot.Swiss dataset, the first bilingual benchmark for evaluating Text-to-SQL systems based on real-world applications. The StatBot.Swiss dataset contains 455 natural language/SQL-pairs over 35 big databases with varying level of complexity for both English and German.
We evaluate the performance of state-of-the-art LLMs such as GPT-3.5-Turbo and mixtral-8x7b-instruct for the Text-to-SQL translation task using an in-context learning approach. Our experimental analysis illustrates that current LLMs struggle to generalize well in generating SQL queries on our novel bilingual dataset.

------------

`[2406.03181] Missci: Reconstructing Fallacies in Misrepresented Science <https://arxiv.org/abs/2406.03181>`__ Missci:重建被歪曲的科学中的谬误

::

    Wed, 5 Jun 2024 12:11:10 GMT
    Max Glockner, Yufang Hou, Preslav Nakov, Iryna Gurevych

Health-related misinformation on social networks can lead to poor decision-making and real-world dangers. Such misinformation often misrepresents scientific publications and cites them as "proof" to gain perceived credibility. To effectively counter such claims automatically, a system must explain how the claim was falsely derived from the cited publication. Current methods for automated fact-checking or fallacy detection neglect to assess the (mis)used evidence in relation to misinformation claims, which is required to detect the mismatch between them. To address this gap, we introduce Missci, a novel argumentation theoretical model for fallacious reasoning together with a new dataset for real-world misinformation detection that misrepresents biomedical publications. Unlike previous fallacy detection datasets, Missci (i) focuses on implicit fallacies between the relevant content of the cited publication and the inaccurate claim, and (ii) requires models to verbalize the fallacious reasoning in addition to classifying it. We present Missci as a dataset to test the critical reasoning abilities of large language models (LLMs), that are required to reconstruct real-world fallacious arguments, in a zero-shot setting. We evaluate two representative LLMs and the impact of different levels of detail about the fallacy classes provided to the LLM via prompts. Our experiments and human evaluation show promising results for GPT 4, while also demonstrating the difficulty of this task.

------------

`[2406.03198] The Impossibility of Fair LLMs <https://arxiv.org/abs/2406.03198>`__ 公平LLMs的不可能

::

    Tue, 28 May 2024 04:36:15 GMT
    Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Alexander D'Amour, Chenhao Tan

The need for fair AI is increasingly clear in the era of general-purpose systems such as ChatGPT, Gemini, and other large language models (LLMs).
However, the increasing complexity of human-AI interaction and its social impacts have raised questions of how fairness standards could be applied. Here, we review the technical frameworks that machine learning researchers have used to evaluate fairness, such as group fairness and fair representations, and find that their application to LLMs faces inherent limitations. We show that each framework either does not logically extend to LLMs or presents a notion of fairness that is intractable for LLMs, primarily due to the multitudes of populations affected, sensitive attributes, and use cases. To address these challenges, we develop guidelines for the more realistic goal of achieving fairness in particular use cases: the criticality of context, the responsibility of LLM developers, and the need for stakeholder participation in an iterative process of design and evaluation. Moreover, it may eventually be possible and even necessary to use the general-purpose capabilities of AI systems to address fairness challenges as a form of scalable AI-assisted alignment.

------------

`[2406.03199] Bayesian WeakS-to-Strong from Text Classification to Generation <https://arxiv.org/abs/2406.03199>`__ 贝叶斯从弱到强从文本分类到生成

::

    Fri, 24 May 2024 13:33:11 GMT
    Ziyun Cui, Ziyang Zhang, Wen Wu, Guangzhi Sun, Chao Zhang

Advances in large language models raise the question of how alignment techniques will adapt as models become increasingly complex and humans will only be able to supervise them weakly. Weak-to-Strong mimics such a scenario where weak model supervision attempts to harness the full capabilities of a much stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by exploring an ensemble of weak models which simulate the variability in human opinions. Confidence scores are estimated using a Bayesian approach to guide the WeakS-to-Strong generalization. Furthermore, we extend the application of WeakS-to-Strong from text classification tasks to text generation tasks where more advanced strategies are investigated for supervision. Moreover, direct preference optimization is applied to advance the student model's preference learning, beyond the basic learning framework of teacher forcing. Results demonstrate the effectiveness of the proposed approach for the reliability of a strong student model, showing potential for superalignment.

------------

`[2406.03202] ChatLang-8: An LLM-Based Synthetic Data Generation Framework for Grammatical Error Correction <https://arxiv.org/abs/2406.03202>`__ ChatLang-8:基于llm的语法纠错合成数据生成框架

::

    Wed, 5 Jun 2024 12:35:00 GMT
    Jeiyoon Park, Chanjun Park, Heuiseok Lim

We explore and improve the capabilities of LLMs to generate data for grammatical error correction (GEC). When merely producing parallel sentences, their patterns are too simplistic to be valuable as a corpus. To address this issue, we propose an automated framework that includes a Subject Selector, Grammar Selector, Prompt Manager, and Evaluator. Additionally, we introduce a new dataset for GEC tasks, named \textbf{ChatLang-8}, which encompasses eight types of subject nouns and 23 types of grammar. It consists of 1 million pairs featuring human-like grammatical errors. Our experiments reveal that ChatLang-8 exhibits a more uniform pattern composition compared to existing GEC datasets.
Furthermore, we observe improved model performance when using ChatLang-8 instead of existing GEC datasets. The experimental results suggest that our framework and ChatLang-8 are valuable resources for enhancing ChatGPT's data generation capabilities.

------------

`[2406.03339] The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches <https://arxiv.org/abs/2406.03339>`__ 评估LLM应用的挑战:自动化、人工和基于LLM方法的分析

::

    Wed, 5 Jun 2024 14:55:10 GMT
    Bhashithe Abeysinghe and Ruhan Circi

Chatbots have been an interesting application of natural language generation since its inception. With novel transformer based Generative AI methods, building chatbots have become trivial. Chatbots which are targeted at specific domains such as medicine, psychology, and general information retrieval are implemented rapidly. This, however, should not distract from the need to evaluate the chatbot responses. Especially because the natural language generation community does not entirely agree upon how to effectively evaluate such applications. With this work we discuss the issue further with the increasingly popular LLM based evaluations and how they correlate with human evaluations. Additionally, we introduce a comprehensive factored evaluation mechanism that can be utilized in conjunction with both human and LLM-based evaluations.
We present the results of an experimental evaluation conducted using this scheme in one of our chatbot implementations, and subsequently compare automated, traditional human evaluation, factored human evaluation, and factored LLM evaluation. Results show that factor based evaluation produces better insights on which aspects need to be improved in LLM applications and further strengthens the argument to use human evaluation in critical spaces where main functionality is not direct retrieval.

------------

`[2406.03363] LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback <https://arxiv.org/abs/2406.03363>`__ 基于llm的基于机器反馈强化学习的不恰当辩论重写

::

    Wed, 5 Jun 2024 15:18:08 GMT
    Timon Ziegenbein, Gabriella Skitalinskaya, Alireza Bayat Makou, Henning Wachsmuth

Ensuring that online discussions are civil and productive is a major challenge for social media platforms. Such platforms usually rely both on users and on automated detection tools to flag inappropriate arguments of other users, which moderators then review. However, this kind of post-hoc moderation is expensive and time-consuming, and moderators are often overwhelmed by the amount and severity of flagged content. Instead, a promising alternative is to prevent negative behavior during content creation. This paper studies how inappropriate language in arguments can be computationally mitigated. We propose a reinforcement learning-based rewriting approach that balances content preservation and appropriateness based on existing classifiers, prompting an instruction-finetuned large language model (LLM) as our initial policy. Unlike related style transfer tasks, rewriting inappropriate arguments allows deleting and adding content permanently. It is therefore tackled on document level rather than sentence level. We evaluate different weighting schemes for the reward function in both absolute and relative human assessment studies.
Systematic experiments on non-parallel data provide evidence that our approach can mitigate the inappropriateness of arguments while largely preserving their content. It significantly outperforms competitive baselines, including few-shot learning, prompting, and humans.

------------

`[2406.03397] Automating Turkish Educational Quiz Generation Using Large Language Models <https://arxiv.org/abs/2406.03397>`__ 基于大型语言模型的土耳其教育测验自动生成

::

    Wed, 5 Jun 2024 15:54:50 GMT
    Kamyar Zeinalipour, Yusuf G\"okberk Kepti\u{g}, Marco Maggini, Marco Gori

Crafting quizzes from educational content is a pivotal activity that benefits both teachers and students by reinforcing learning and evaluating understanding. In this study, we introduce a novel approach to generate quizzes from Turkish educational texts, marking a pioneering endeavor in educational technology specifically tailored to the Turkish educational context. We present a specialized dataset, named the Turkish-Quiz-Instruct, comprising an extensive collection of Turkish educational texts accompanied by multiple-choice and short-answer quizzes. This research leverages the capabilities of Large Language Models (LLMs), including GPT-4-Turbo, GPT-3.5-Turbo, Llama-2-7b-chat-hf, and Llama-2-13b-chat-hf, to automatically generate quiz questions and answers from the Turkish educational content. Our work delineates the methodology for employing these LLMs in the context of Turkish educational material, thereby opening new avenues for automated Turkish quiz generation.
The study not only demonstrates the efficacy of using such models for generating coherent and relevant quiz content but also sets a precedent for future research in the domain of automated educational content creation for languages other than English. The Turkish-Quiz-Instruct dataset is introduced as a valuable resource for researchers and practitioners aiming to explore the boundaries of educational technology and language-specific applications of LLMs in Turkish. By addressing the challenges of quiz generation in a non-English context specifically Turkish, this study contributes significantly to the field of Turkish educational technology, providing insights into the potential of leveraging LLMs for educational purposes across diverse linguistic landscapes.

------------

`[2406.03441] Cycles of Thought: Measuring LLM Confidence through Stable Explanations <https://arxiv.org/abs/2406.03441>`__ 思维周期:通过稳定的解释衡量LLM的置信度

::

    Wed, 5 Jun 2024 16:35:30 GMT
    Evan Becker, Stefano Soatto

In many high-risk machine learning applications it is essential for a model to indicate when it is uncertain about a prediction. While large language models (LLMs) can reach and even surpass human-level accuracy on a variety of benchmarks, their overconfidence in incorrect responses is still a well-documented failure mode. Traditional methods for ML uncertainty quantification can be difficult to directly adapt to LLMs due to the computational cost of implementation and closed-source nature of many models. A variety of black-box methods have recently been proposed, but these often rely on heuristics such as self-verbalized confidence. We instead propose a framework for measuring an LLM's uncertainty with respect to the distribution of generated explanations for an answer. While utilizing explanations is not a new idea in and of itself, by interpreting each possible model+explanation pair as a test-time classifier we can calculate a posterior answer distribution over the most likely of these classifiers. We demonstrate how a specific instance of this framework using explanation entailment as our classifier likelihood improves confidence score metrics (in particular AURC and AUROC) over baselines across five different datasets. We believe these results indicate that our framework is both a well-principled and effective way of quantifying uncertainty in LLMs.

------------

`[2406.03450] What is the Best Way for ChatGPT to Translate Poetry? <https://arxiv.org/abs/2406.03450>`__ ChatGPT翻译诗歌的最佳方式是什么?

::

    Wed, 5 Jun 2024 16:48:26 GMT
    Shanshan Wang, Derek F. Wong, Jingming Yao, Lidia S. Chao

Machine translation (MT) has historically faced significant challenges when applied to literary works, particularly in the domain of poetry translation.
The advent of Large Language Models such as ChatGPT holds potential for innovation in this field. This study examines ChatGPT's capabilities in English-Chinese poetry translation tasks, utilizing targeted prompts and small sample scenarios to ascertain optimal performance. Despite promising outcomes, our analysis reveals persistent issues in the translations generated by ChatGPT that warrant attention. To address these shortcomings, we propose an Explanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages monolingual poetry explanation as a guiding information for the translation process. Furthermore, we refine existing evaluation criteria to better suit the nuances of modern poetry translation. We engaged a panel of professional poets for assessments, complemented evaluations by using GPT-4. The results from both human and machine evaluations demonstrate that our EAPMT method outperforms traditional translation methods of ChatGPT and the existing online systems.
This paper validates the efficacy of our method and contributes a novel perspective to machine-assisted literary translation.

------------

`[2406.03486] BIPED: Pedagogically Informed Tutoring System for ESL Education <https://arxiv.org/abs/2406.03486>`__ BIPED:基于教学信息的ESL教学系统

::

    Wed, 5 Jun 2024 17:49:24 GMT
    Soonwoo Kwon, Sojung Kim, Minju Park, Seunghyun Lee, Kyuseok Kim

Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English. Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies. To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human English tutoring interactions. Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset. Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies.

------------

`[2406.03487] Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends <https://arxiv.org/abs/2406.03487>`__ 对话摘要中的LLM行为分析:揭示环境幻觉趋势

::

    Wed, 5 Jun 2024 17:49:47 GMT
    Sanjana Ramprasad, Elisa Ferracane, Zachary C. Lipton

Recent advancements in large language models (LLMs) have considerably advanced the capabilities of summarization systems. However, they continue to face concerns about hallucinations. While prior work has evaluated LLMs extensively in news domains, most evaluation of dialogue summarization has focused on BART-based models, leaving a gap in our understanding of their faithfulness. Our work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies. Specifically, we focus on two prominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to what constitutes a hallucination: LLMs often generate plausible inferences, supported by circumstantial evidence in the conversation, that lack direct evidence, a pattern that is less prevalent in older models. We propose a refined taxonomy of errors, coining the category of "Circumstantial Inference" to bucket these LLM behaviors and release the dataset. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models.
Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors. To address this, we introduce two prompt-based approaches for fine-grained error detection that outperform existing metrics, particularly for identifying "Circumstantial Inference."

------------

`[2406.03496] Wings: Learning Multimodal LLMs without Text-only Forgetting <https://arxiv.org/abs/2406.03496>`__ Wings:无文本遗忘的多模态llm学习

::

    Wed, 5 Jun 2024 17:59:40 GMT
    Yi-Kai Zhang, Shiyin Lu, Yang Li, Yanqing Ma, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye

Multimodal large language models (MLLMs), initiated with a trained LLM, first align images with text and then fine-tune on multimodal mixed inputs. However, the MLLM catastrophically forgets the text-only instructions, which do not include images and can be addressed within the initial LLM. In this paper, we present Wings, a novel MLLM that excels in both text-only dialogues and multimodal comprehension. Analyzing MLLM attention in multimodal instructions reveals that text-only forgetting is related to the attention shifts from pre-image to post-image text. From that, we construct extra modules that act as the boosted learner to compensate for the attention shift. The complementary visual and textual learners, like "wings" on either side, are connected in parallel within each layer's attention block. Initially, image and text inputs are aligned with visual learners operating alongside the main attention, balancing focus on visual elements. Textual learners are later collaboratively integrated with attention-based routing to blend the outputs of the visual and textual learners. We design the Low-Rank Residual Attention (LoRRA) to guarantee high efficiency for learners. Our experimental results demonstrate that Wings outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks. On a newly constructed Interleaved Image-Text (IIT) benchmark, Wings exhibits superior performance from text-only-rich to multimodal-rich question-answering tasks.

------------

`[2406.02591] Unveiling the Potential of AI for Nanomaterial Morphology Prediction <https://arxiv.org/abs/2406.02591>`__ 揭示人工智能在纳米材料形态预测方面的潜力

::

    Fri, 31 May 2024 19:16:07 GMT
    Ivan Dubrovsky, Andrei Dmitrenko, Aleksei Dmitrenko, Nikita Serov, Vladimir Vinogradov

Creation of nanomaterials with specific morphology remains a complex experimental process, even though there is a growing demand for these materials in various industry sectors. This study explores the potential of AI to predict the morphology of nanoparticles within the data availability constraints. For that, we first generated a new multi-modal dataset that is double the size of analogous studies. Then, we systematically evaluated performance of classical machine learning and large language models in prediction of nanomaterial shapes and sizes. Finally, we prototyped a text-to-image system, discussed the obtained empirical results, as well as the limitations and promises of existing approaches.

------------

`[2406.02592] LOLAMEME: Logic, Language, Memory, Mechanistic Framework <https://arxiv.org/abs/2406.02592>`__ 逻辑，语言，记忆，机械框架

::

    Fri, 31 May 2024 21:18:25 GMT
    Jay Desai, Xiaobo Guo, Srinivasan H. Sengamedu

The performance of Large Language Models has achieved superhuman breadth with unprecedented depth. At the same time, the language models are mostly black box models and the underlying mechanisms for performance have been evaluated using synthetic or mechanistic schemes. We extend current mechanistic schemes to incorporate Logic, memory, and nuances of Language such as latent structure.
The proposed framework is called LOLAMEME and we provide two instantiations of LOLAMEME: LoLa and MeMe languages. We then consider two generative language model architectures: transformer-based GPT-2 and convolution-based Hyena. We propose the hybrid architecture T HEX and use LOLAMEME framework is used to compare three architectures. T HEX outperforms GPT-2 and Hyena on select tasks.

------------

`[2406.02611] LOLA: LLM-Assisted Online Learning Algorithm for Content Experiments <https://arxiv.org/abs/2406.02611>`__ LOLA: llm辅助的内容实验在线学习算法

::

    Mon, 3 Jun 2024 07:56:58 GMT
    Zikun Ye, Hema Yoganarasimhan, Yufeng Zheng

In the rapidly evolving digital content landscape, media firms and news publishers require automated and efficient methods to enhance user engagement.
This paper introduces the LLM-Assisted Online Learning Algorithm (LOLA), a novel framework that integrates Large Language Models (LLMs) with adaptive experimentation to optimize content delivery. Leveraging a large-scale dataset from Upworthy, which includes 17,681 headline A/B tests aimed at evaluating the performance of various headlines associated with the same article content, we first investigate three broad pure-LLM approaches: prompt-based methods, embedding-based classification models, and fine-tuned open-source LLMs. Our findings indicate that prompt-based approaches perform poorly, achieving no more than 65% accuracy in identifying the catchier headline among two options.
In contrast, OpenAI-embedding-based classification models and fine-tuned Llama-3-8b models achieve comparable accuracy, around 82-84%, though still falling short of the performance of experimentation with sufficient traffic. We then introduce LOLA, which combines the best pure-LLM approach with the Upper Confidence Bound algorithm to adaptively allocate traffic and maximize clicks.
Our numerical experiments on Upworthy data show that LOLA outperforms the standard A/B testing method (the current status quo at Upworthy), pure bandit algorithms, and pure-LLM approaches, particularly in scenarios with limited experimental traffic or numerous arms. Our approach is both scalable and broadly applicable to content experiments across a variety of digital settings where firms seek to optimize user engagement, including digital advertising and social media recommendations.

------------

`[2406.02613] ACCO: Accumulate while you Communicate, Hiding Communications in Distributed LLM Training <https://arxiv.org/abs/2406.02613>`__ ACCO:在交流时积累，在分布式LLM训练中隐藏交流

::

    Mon, 3 Jun 2024 08:23:45 GMT
    Adel Nabli (MLIA, Mila), Louis Fournier (MLIA), Pierre Erbacher (MLIA), Louis Serrano (MLIA), Eugene Belilovsky (Mila), Edouard Oyallon

Training Large Language Models (LLMs) relies heavily on distributed implementations, employing multiple GPUs to compute stochastic gradients on model replicas in parallel. However, synchronizing gradients in data parallel settings induces a communication overhead increasing with the number of distributed workers, which can impede the efficiency gains of parallelization.
To address this challenge, optimization algorithms reducing inter-worker communication have emerged, such as local optimization methods used in Federated Learning. While effective in minimizing communication overhead, these methods incur significant memory costs, hindering scalability: in addition to extra momentum variables, if communications are only allowed between multiple local optimization steps, then the optimizer's states cannot be sharded among workers. In response, we propose $\textbf{AC}$cumulate while $\textbf{CO}$mmunicate ($\texttt{ACCO}$), a memory-efficient optimization algorithm tailored for distributed training of LLMs. $\texttt{ACCO}$ allows to shard optimizer states across workers, overlaps gradient computations and communications to conceal communication costs, and accommodates heterogeneous hardware. Our method relies on a novel technique to mitigate the one-step delay inherent in parallel execution of gradient computations and communications, eliminating the need for warmup steps and aligning with the training dynamics of standard distributed optimization while converging faster in terms of wall-clock time. We demonstrate the effectiveness of $\texttt{ACCO}$ on several LLMs training and fine-tuning tasks.

------------

`[2406.02616] Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A Model-Based Reinforcement Learning Approach <https://arxiv.org/abs/2406.02616>`__ 边缘计算中无线LLM推理的自适应层分裂:基于模型的强化学习方法

::

    Mon, 3 Jun 2024 09:41:42 GMT
    Yuxuan Chen, Rongpeng Li, Xiaoxue Yu, Zhifeng Zhao, and Honggang Zhang

Optimizing the deployment of large language models (LLMs) in edge computing environments is critical for enhancing privacy and computational efficiency.
Toward efficient wireless LLM inference in edge computing, this study comprehensively analyzes the impact of different splitting points in mainstream open-source LLMs. On this basis, this study introduces a framework taking inspiration from model-based reinforcement learning (MBRL) to determine the optimal splitting point across the edge and user equipment (UE). By incorporating a reward surrogate model, our approach significantly reduces the computational cost of frequent performance evaluations. Extensive simulations demonstrate that this method effectively balances inference performance and computational load under varying network conditions, providing a robust solution for LLM deployment in decentralized settings.

------------

`[2406.02642] E-ICL: Enhancing Fine-Grained Emotion Recognition through the Lens of Prototype Theory <https://arxiv.org/abs/2406.02642>`__ E-ICL:基于原型理论的增强细粒度情感识别

::

    Tue, 4 Jun 2024 10:59:43 GMT
    Zhou Yang, Zhaochun Ren, Chenglong Ye, Yufeng Wang, Haizhou Sun, Chao Chen, Xiaofei Zhu, Yunbing Wu, Xiangwen Liao

In-context learning (ICL) achieves remarkable performance in various domains such as knowledge acquisition, commonsense reasoning, and semantic understanding. However, its performance significantly deteriorates for emotion detection tasks, especially fine-grained emotion recognition. The underlying reasons for this remain unclear. In this paper, we identify the reasons behind ICL's poor performance from the perspective of prototype theory and propose a method to address this issue. Specifically, we conduct extensive pilot experiments and find that ICL conforms to the prototype theory on fine-grained emotion recognition. Based on this theory, we uncover the following deficiencies in ICL: (1) It relies on prototypes (example-label pairs) that are semantically similar but emotionally inaccurate to predict emotions. (2) It is prone to interference from irrelevant categories, affecting the accuracy and robustness of the predictions. To address these issues, we propose an Emotion Context Learning method (E-ICL) on fine-grained emotion recognition. E-ICL relies on more emotionally accurate prototypes to predict categories by referring to emotionally similar examples with dynamic labels. Simultaneously, E-ICL employs an exclusionary emotion prediction strategy to avoid interference from irrelevant categories, thereby increasing its accuracy and robustness.
Note that the entire process is accomplished with the assistance of a plug-and-play emotion auxiliary model, without additional training. Experiments on the fine-grained emotion datasets EDOS, Empathetic-Dialogues, EmpatheticIntent, and GoEmotions show that E-ICL achieves superior emotion prediction performance. Furthermore, even when the emotion auxiliary model used is lower than 10% of the LLMs, E-ICL can still boost the performance of LLMs by over 4% on multiple datasets.

------------

`[2406.02764] Adaptive Preference Scaling for Reinforcement Learning with Human Feedback <https://arxiv.org/abs/2406.02764>`__ 人工反馈强化学习的自适应偏好缩放

::

    Tue, 4 Jun 2024 20:33:22 GMT
    Ilgee Hong, Zichong Li, Alexander Bukharin, Yixiao Li, Haoming Jiang, Tianbao Yang, and Tuo Zhao

Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function.
Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.

------------

`[2406.02806] Randomized Geometric Algebra Methods for Convex Neural Networks <https://arxiv.org/abs/2406.02806>`__ 凸神经网络的随机几何代数方法

::

    Tue, 4 Jun 2024 22:22:39 GMT
    Yifei Wang, Sungyoon Kim, Paul Chu, Indu Subramaniam, Mert Pilanci

We introduce randomized algorithms to Clifford's Geometric Algebra, generalizing randomized linear algebra to hypercomplex vector spaces. This novel approach has many implications in machine learning, including training neural networks to global optimality via convex optimization. Additionally, we consider fine-tuning large language model (LLM) embeddings as a key application area, exploring the intersection of geometric algebra and modern AI techniques.
In particular, we conduct a comparative analysis of the robustness of transfer learning via embeddings, such as OpenAI GPT models and BERT, using traditional methods versus our novel approach based on convex optimization. We test our convex optimization transfer learning method across a variety of case studies, employing different embeddings (GPT-4 and BERT embeddings) and different text classification datasets (IMDb, Amazon Polarity Dataset, and GLUE) with a range of hyperparameter settings. Our results demonstrate that convex optimization and geometric algebra not only enhances the performance of LLMs but also offers a more stable and reliable method of transfer learning via embeddings.

------------

`[2406.02900] Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms <https://arxiv.org/abs/2406.02900>`__ 直接对齐算法中奖励模型过度优化的缩放定律

::

    Wed, 5 Jun 2024 03:41:37 GMT
    Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, Scott Niekum

Reinforcement Learning from Human Feedback (RLHF) has been crucial to the recent success of Large Language Models (LLMs), however, it is often a complex and brittle process. In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimize the LLM. A prominent issue with such methods is \emph{reward over-optimization} or \emph{reward hacking}, where performance as measured by the learned proxy reward model increases, but true quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like Direct Preference Optimization have emerged as alternatives to the classical RLHF pipeline by circumventing the reward modeling phase. However, although DAAs do not use a separate proxy reward model, they still commonly deteriorate from over-optimization. While the so-called reward hacking phenomenon is not well-defined for DAAs, we still uncover similar trends: at higher KL budgets, DAA algorithms exhibit similar degradation patterns to their classic RLHF counterparts. In particular, we find that DAA methods deteriorate not only across a wide range of KL budgets but also often before even a single epoch of the dataset is completed. Through extensive empirical experimentation, this work formulates and formalizes the reward over-optimization or hacking problem for DAAs and explores its consequences across objectives, training regimes, and model scales.

------------

`[2406.02913] Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity <https://arxiv.org/abs/2406.02913>`__ 极端稀疏llm的零阶微调

::

    Wed, 5 Jun 2024 04:07:35 GMT
    Wentao Guo, Jikai Long, Yimeng Zeng, Zirui Liu, Xinyu Yang, Yide Ran, Jacob R. Gardner, Osbert Bastani, Christopher De Sa, Xiaodong Yu, Beidi Chen, Zhaozhuo Xu

Zeroth-order optimization (ZO) is a memory-efficient strategy for fine-tuning Large Language Models using only forward passes. However, the application of ZO fine-tuning in memory-constrained settings such as mobile phones and laptops is still challenging since full precision forward passes are infeasible. In this study, we address this limitation by integrating sparsity and quantization into ZO fine-tuning of LLMs. Specifically, we investigate the feasibility of fine-tuning an extremely small subset of LLM parameters using ZO. This approach allows the majority of un-tuned parameters to be quantized to accommodate the constraint of limited device memory. Our findings reveal that the pre-training process can identify a set of "sensitive parameters" that can guide the ZO fine-tuning of LLMs on downstream tasks. Our results demonstrate that fine-tuning 0.1% sensitive parameters in the LLM with ZO can outperform the full ZO fine-tuning performance, while offering wall-clock time speedup.
Additionally, we show that ZO fine-tuning targeting these 0.1% sensitive parameters, combined with 4 bit quantization, enables efficient ZO fine-tuning of an Llama2-7B model on a GPU device with less than 8 GiB of memory and notably reduced latency.

------------

`[2406.02924] Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models <https://arxiv.org/abs/2406.02924>`__ Pruner-Zero:从头开始演化大型语言模型的符号剪枝度量

::

    Wed, 5 Jun 2024 04:25:23 GMT
    Peijie Dong and Lujun Li and Zhenheng Tang and Xiang Liu and Xinglin Pan and Qiang Wang and Xiaowen Chu

Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size. Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively expensive and computationally demanding. Recently, post-training pruning approaches introduced novel metrics, enabling the pruning of LLMs without retraining. However, these metrics require the involvement of human experts and tedious trial and error. To efficiently identify superior pruning metrics, we develop an automatic framework for searching symbolic pruning metrics using genetic programming. In particular, we devise an elaborate search space encompassing the existing pruning metrics to discover the potential symbolic pruning metric. We propose an opposing operation simplification strategy to increase the diversity of the population. In this way, Pruner-Zero allows auto-generation of symbolic pruning metrics. Based on the searched results, we explore the correlation between pruning metrics and performance after pruning and summarize some principles. Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot tasks demonstrate that our Pruner-Zero obtains superior performance than SOTA post-training pruning methods. Code at: \url{https://github.com/pprp/Pruner-Zero}.

------------

`[2406.02958] PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs <https://arxiv.org/abs/2406.02958>`__ 前置文本:llm时代在私有联邦数据上训练语言模型

::

    Wed, 5 Jun 2024 05:27:02 GMT
    Charlie Hou, Akshat Shrivastava, Hongyuan Zhan, Rylan Conway, Trang Le, Adithya Sagar, Giulia Fanti, Daniel Lazar

On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data. Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy. To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data. First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\epsilon=1.29$, $\epsilon=7.58$). We achieve these results while using 9$\times$ fewer rounds, 6$\times$ less client computation per round, and 100$\times$ less communication per round. Second, finetuning large models on PrE-Text's DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets. Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data. Code is available at https://github.com/houcharlie/PrE-Text.

------------

`[2406.02969] Filtered not Mixed: Stochastic Filtering-Based Online Gating for Mixture of Large Language Models <https://arxiv.org/abs/2406.02969>`__ 过滤而非混合:基于随机过滤的大型语言模型混合在线门控

::

    Wed, 5 Jun 2024 05:53:50 GMT
    Raeid Saqur, Anastasis Kratsios, Florian Krach, Yannick Limmer, Jacob-Junqi Tian, John Willes, Blanka Horvath and Frank Rudzicz

We propose MoE-F -- a formalised mechanism for combining $N$ pre-trained expert Large Language Models (LLMs) in online time-series prediction tasks by adaptively forecasting the best weighting of LLM predictions at every time step. Our mechanism leverages the conditional information in each expert's running performance to forecast the best combination of LLMs for predicting the time series in its next step. Diverging from static (learned) Mixture of Experts (MoE) methods, MoE-F employs time-adaptive stochastic filtering techniques to combine experts. By framing the expert selection problem as a finite state-space, continuous-time Hidden Markov model (HMM), we can leverage the Wohman-Shiryaev filter. Our approach first constructs $N$ parallel filters corresponding to each of the $N$ individual LLMs. Each filter proposes its best combination of LLMs, given the information that they have access to.
Subsequently, the $N$ filter outputs are aggregated to optimize a lower bound for the loss of the aggregated LLMs, which can be optimized in closed-form, thus generating our ensemble predictor. Our contributions here are: (I) the MoE-F algorithm -- deployable as a plug-and-play filtering harness, (II) theoretical optimality guarantees of the proposed filtering-based gating algorithm, and (III) empirical evaluation and ablative results using state of the art foundational and MoE LLMs on a real-world Financial Market Movement task where MoE-F attains a remarkable 17% absolute and 48.5% relative F1 measure improvement over the next best performing individual LLM expert.

------------

`[2406.03324] UDQL: Bridging The Gap between MSE Loss and The Optimal Value Function in Offline Reinforcement Learning <https://arxiv.org/abs/2406.03324>`__ UDQL:弥合离线强化学习中MSE损失和最优值函数之间的差距

::

    Wed, 5 Jun 2024 14:37:42 GMT
    Yu Zhang, Rui Yu, Zhipeng Yao, Wenyuan Zhang, Jun Wang, Liming Zhang

The Mean Square Error (MSE) is commonly utilized to estimate the solution of the optimal value function in the vast majority of offline reinforcement learning (RL) models and has achieved outstanding performance. However, we find that its principle can lead to overestimation phenomenon for the value function. In this paper, we first theoretically analyze overestimation phenomenon led by MSE and provide the theoretical upper bound of the overestimated error. Furthermore, to address it, we propose a novel Bellman underestimated operator to counteract overestimation phenomenon and then prove its contraction characteristics. At last, we propose the offline RL algorithm based on underestimated operator and diffusion policy model. Extensive experimental results on D4RL tasks show that our method can outperform state-of-the-art offline RL algorithms, which demonstrates that our theoretical analysis and underestimation way are effective for offline RL tasks.

------------

`[2406.03428] HelloFresh: LLM Evaluations on Streams of Real-World Human Editorial Actions across X Community Notes and Wikipedia edits <https://arxiv.org/abs/2406.03428>`__ HelloFresh: LLM对X社区笔记和维基百科编辑的真实世界人类编辑行为流进行评估

::

    Wed, 5 Jun 2024 16:25:57 GMT
    Tim Franzmeyer, Aleksandar Shtedritski, Samuel Albanie, Philip Torr, Jo\~ao F. Henriques, Jakob N. Foerster

Benchmarks have been essential for driving progress in machine learning. A better understanding of LLM capabilities on real world tasks is vital for safe development. Designing adequate LLM benchmarks is challenging: Data from real-world tasks is hard to collect, public availability of static evaluation data results in test data contamination and benchmark overfitting, and periodically generating new evaluation data is tedious and may result in temporally inconsistent results. We introduce HelloFresh, based on continuous streams of real-world data generated by intrinsically motivated human labelers.
It covers recent events from X (formerly Twitter) community notes and edits of Wikipedia pages, mitigating the risk of test data contamination and benchmark overfitting. Any X user can propose an X note to add additional context to a misleading post (formerly tweet); if the community classifies it as helpful, it is shown with the post. Similarly, Wikipedia relies on community-based consensus, allowing users to edit articles or revert edits made by other users.
Verifying whether an X note is helpful or whether a Wikipedia edit should be accepted are hard tasks that require grounding by querying the web. We backtest state-of-the-art LLMs supplemented with simple web search access and find that HelloFresh yields a temporally consistent ranking. To enable continuous evaluation on HelloFresh, we host a public leaderboard and periodically updated evaluation data at https://tinyurl.com/hello-fresh-LLM.

------------

`[2406.03445] Pre-trained Large Language Models Use Fourier Features to Compute Addition <https://arxiv.org/abs/2406.03445>`__ 预训练大型语言模型使用傅里叶特征计算加法

::

    Wed, 5 Jun 2024 16:40:53 GMT
    Tianyi Zhou, Deqing Fu, Vatsal Sharan, Robin Jia

Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.

------------

`[2406.03476] Does your data spark joy? Performance gains from domain upsampling at the end of training <https://arxiv.org/abs/2406.03476>`__ 你的数据能带来快乐吗?训练结束时域上采样带来的性能提升

::

    Wed, 5 Jun 2024 17:29:15 GMT
    Cody Blakeney, Mansheej Paul, Brett W. Larsen, Sean Owen, and Jonathan Frankle

Pretraining datasets for large language models (LLMs) have grown to trillions of tokens composed of large amounts of CommonCrawl (CC) web scrape along with smaller, domain-specific datasets. It is expensive to understand the impact of these domain-specific datasets on model capabilities as training at large FLOP scales is required to reveal significant changes to difficult and emergent benchmarks. Given the increasing cost of experimenting with pretraining data, how does one determine the optimal balance between the diversity in general web scrapes and the information density of domain specific data? In this work, we show how to leverage the smaller domain specific datasets by upsampling them relative to CC at the end of training to drive performance improvements on difficult benchmarks. This simple technique allows us to improve up to 6.90 pp on MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data mix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2 (7B)$\unicode{x2014}$a model trained for twice as long. We experiment with ablating the duration of domain upsampling from 5% to 30% of training and find that 10% to 20% percent is optimal for navigating the tradeoff between general language modeling capabilities and targeted benchmarks. We also use domain upsampling to characterize at scale the utility of individual datasets for improving various benchmarks by removing them during this final phase of training. This tool opens up the ability to experiment with the impact of different pretraining datasets at scale, but at an order of magnitude lower cost compared to full pretraining runs.

------------

`[2406.02554] Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition <https://arxiv.org/abs/2406.02554>`__ 听我说，看我说，懂我说:视听自闭症行为识别

::

    Fri, 22 Mar 2024 22:52:35 GMT
    Shijian Deng, Erin E. Kosloski, Siddhi Patel, Zeke A. Barnett, Yiyang Nan, Alexander Kaplan, Sisira Aarukapalli, William T. Doan, Matthew Wang, Harsh Singh, Pamela R. Rollins, Yapeng Tian

In this article, we introduce a novel problem of audio-visual autism behavior recognition, which includes social behavior recognition, an essential aspect previously omitted in AI-assisted autism screening research. We define the task at hand as one that is audio-visual autism behavior recognition, which uses audio and visual cues, including any speech present in the audio, to recognize autism-related behaviors. To facilitate this new research direction, we collected an audio-visual autism spectrum dataset (AV-ASD), currently the largest video dataset for autism screening using a behavioral approach. It covers an extensive range of autism-associated behaviors, including those related to social communication and interaction. To pave the way for further research on this new problem, we intensively explored leveraging foundation models and multimodal large language models across different modalities. Our experiments on the AV-ASD dataset demonstrate that integrating audio, visual, and speech modalities significantly enhances the performance in autism behavior recognition. Additionally, we explored the use of a post-hoc to ad-hoc pipeline in a multimodal large language model to investigate its potential to augment the model's explanatory capability during autism behavior recognition. We will release our dataset, code, and pre-trained models.

------------

`[2406.03283] Enhancing Repository-Level Code Generation with Integrated Contextual Information <https://arxiv.org/abs/2406.03283>`__ 使用集成的上下文信息增强存储库级代码生成

::

    Wed, 5 Jun 2024 13:56:42 GMT
    Zhiyuan Pan, Xing Hu, Xin Xia, Xiaohu Yang

Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, repository-level code generation presents unique challenges, particularly due to the need to utilize information spread across multiple files within a repository. Existing retrieval-based approaches sometimes fall short as they are limited in obtaining a broader and deeper repository context. In this paper, we present CatCoder, a novel code generation framework designed for statically typed programming languages. CatCoder enhances repository-level code generation by integrating relevant code and type context. Specifically, it leverages static analyzers to extract type dependencies and merges this information with retrieved code to create comprehensive prompts for LLMs. To evaluate the effectiveness of CatCoder, we adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks.
The results show that CatCoder outperforms the RepoCoder baseline by up to 17.35%, in terms of pass@k score. Furthermore, the generalizability of CatCoder is assessed using various LLMs, including both code-specialized models and general-purpose models. Our findings indicate consistent performance improvements across all models, which underlines the practicality of CatCoder.

------------

`[2406.02795] ArguMentor: Augmenting User Experiences with Counter-Perspectives <https://arxiv.org/abs/2406.02795>`__ ArguMentor:用反视角增强用户体验

::

    Tue, 4 Jun 2024 21:43:56 GMT
    Priya Pitre, Kurt Luther

Opinion pieces often represent only one side of any story, which can influence users and make them susceptible to confirmation bias and echo chambers in society. Moreover, humans are also bad at reading long articles -- often indulging in idle reading and re-reading. To solve this, we design ArguMentor, an end-to-end system that highlights claims in opinion pieces, generates counter-arguments for them using an LLM, and generates a context-based summary of the passage based on current events. It further enhances user interaction and understanding through additional features like Q&A bot, DebateMe and highlighting trigger windows. Our survey and results show that users can generate more counterarguments and on an average have more neutralized views after engaging with the system.

------------

`[2406.02844] Item-Language Model for Conversational Recommendation <https://arxiv.org/abs/2406.02844>`__ 会话推荐的项目语言模型

::

    Wed, 5 Jun 2024 01:35:50 GMT
    Li Yang, Anushya Subbiah, Hardik Patel, Judith Yue Li, Yanwei Song, Reza Mirghaderi, Vikram Aggarwal

Large-language Models (LLMs) have been extremely successful at tasks like complex dialogue understanding, reasoning and coding due to their emergent abilities. These emergent abilities have been extended with multi-modality to include image, audio, and video capabilities. Recommender systems, on the other hand, have been critical for information seeking and item discovery needs.
Recently, there have been attempts to apply LLMs for recommendations. One difficulty of current attempts is that the underlying LLM is usually not trained on the recommender system data, which largely contains user interaction signals and is often not publicly available. Another difficulty is user interaction signals often have a different pattern from natural language text, and it is currently unclear if the LLM training setup can learn more non-trivial knowledge from interaction signals compared with traditional recommender system methods. Finally, it is difficult to train multiple LLMs for different use-cases, and to retain the original language and reasoning abilities when learning from recommender system data. To address these three limitations, we propose an Item-Language Model (ILM), which is composed of an item encoder to produce text-aligned item representations that encode user interaction signals, and a frozen LLM that can understand those item representations with preserved pretrained knowledge. We conduct extensive experiments which demonstrate both the importance of the language-alignment and of user interaction knowledge in the item encoder.

------------

`[2406.03248] Large Language Models as Evaluators for Recommendation Explanations <https://arxiv.org/abs/2406.03248>`__ 大型语言模型作为推荐解释的评估器

::

    Wed, 5 Jun 2024 13:23:23 GMT
    Xiaoyu Zhang, Yishan Li, Jiayin Wang, Bowen Sun, Weizhi Ma, Peijie Sun, Min Zhang

The explainability of recommender systems has attracted significant attention in academia and industry. Many efforts have been made for explainable recommendations, yet evaluating the quality of the explanations remains a challenging and unresolved issue. In recent years, leveraging LLMs as evaluators presents a promising avenue in Natural Language Processing tasks (e.g., sentiment classification, information extraction), as they perform strong capabilities in instruction following and common-sense reasoning.
However, evaluating recommendation explanatory texts is different from these NLG tasks, as its criteria are related to human perceptions and are usually subjective. In this paper, we investigate whether LLMs can serve as evaluators of recommendation explanations. To answer the question, we utilize real user feedback on explanations given from previous work and additionally collect third-party annotations and LLM evaluations. We design and apply a 3-level meta evaluation strategy to measure the correlation between evaluator labels and the ground truth provided by users. Our experiments reveal that LLMs, such as GPT4, can provide comparable evaluations with appropriate prompts and settings. We also provide further insights into combining human labels with the LLM evaluation process and utilizing ensembles of multiple heterogeneous LLM evaluators to enhance the accuracy and stability of evaluations. Our study verifies that utilizing LLMs as evaluators can be an accurate, reproducible and cost-effective solution for evaluating recommendation explanation texts. Our code is available at https://github.com/Xiaoyu-SZ/LLMasEvaluator.

------------

`[2406.02915] Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models <https://arxiv.org/abs/2406.02915>`__ 视觉-文本交叉对齐:改进视觉-语言模型中的相似度得分

::

    Wed, 5 Jun 2024 04:08:41 GMT
    Jinhao Li, Haopeng Li, Sarah Erfani, Lei Feng, James Bailey, Feng Liu

It has recently been discovered that using a pre-trained vision-language model (VLM), e.g., CLIP, to align a whole query image with several finer text descriptions generated by a large language model can significantly enhance zero-shot performance. However, in this paper, we empirically find that the finer descriptions tend to align more effectively with local areas of the query image rather than the whole image, and then we theoretically validate this finding. Thus, we present a method called weighted visual-text cross alignment (WCA). This method begins with a localized visual prompting technique, designed to identify local visual areas within the query image. The local visual areas are then cross-aligned with the finer descriptions by creating a similarity matrix using the pre-trained VLM. To determine how well a query image aligns with each category, we develop a score function based on the weighted similarities in this matrix. Extensive experiments demonstrate that our method significantly improves zero-shot performance across various datasets, achieving results that are even comparable to few-shot learning methods.

------------

`[2406.03230] Defending Large Language Models Against Attacks With Residual Stream Activation Analysis <https://arxiv.org/abs/2406.03230>`__ 基于残差流激活分析的大型语言模型防御攻击

::

    Wed, 5 Jun 2024 13:06:33 GMT
    Amelia Kawasaki, Andrew Davis, Houssam Abbas

The widespread adoption of Large Language Models (LLMs), exemplified by OpenAI's ChatGPT, brings to the forefront the imperative to defend against adversarial threats on these models. These attacks, which manipulate an LLM's output by introducing malicious inputs, undermine the model's integrity and the trust users place in its outputs. In response to this challenge, our paper presents an innovative defensive strategy, given white box access to an LLM, that harnesses residual activation analysis between transformer layers of the LLM. We apply an established methodology for analyzing distinctive activation patterns in the residual streams for a novel result of attack prompt classification. We curate multiple datasets to demonstrate how this method of classification has high accuracy across multiple types of attack scenarios, including our newly-created attack dataset. Furthermore, we enhance the model's resilience by integrating safety fine-tuning techniques for LLMs in order to measure its effect on our capability to detect attacks. The results underscore the effectiveness of our approach in enhancing the detection and mitigation of adversarial inputs, advancing the security framework within which LLMs operate.

------------

`[2310.09688] Recursively-Constrained Partially Observable Markov Decision Processes <https://arxiv.org/abs/2310.09688>`__ 递归约束部分可观测马尔可夫决策过程

::

    replaced with revised version Wed, 5 Jun 2024 02:31:58 GMT
    Submission history From: Qi Heng Ho [view email]
    [v1] Sun, 15 Oct 2023 00:25:07 UTC (1,434 KB)
    [v2] Wed, 20 Dec 2023 14:45:17 UTC (655 KB)
    [v3] Wed, 5 Jun 2024 02:31:58 UTC (637 KB)
    Qi Heng Ho, Tyler Becker, Benjamin Kraske, Zakariya Laouar, Martin S. Feather, Federico Rossi, Morteza Lahijanian, Zachary N. Sunberg

Many sequential decision problems involve optimizing one objective function while imposing constraints on other objectives. Constrained Partially Observable Markov Decision Processes (C-POMDP) model this case with transition uncertainty and partial observability. In this work, we first show that C-POMDPs violate the optimal substructure property over successive decision steps and thus may exhibit behaviors that are undesirable for some (e.g., safety critical) applications. Additionally, online re-planning in C-POMDPs is often ineffective due to the inconsistency resulting from this violation. To address these drawbacks, we introduce the Recursively-Constrained POMDP (RC-POMDP), which imposes additional history-dependent cost constraints on the C-POMDP. We show that, unlike C-POMDPs, RC-POMDPs always have deterministic optimal policies and that optimal policies obey Bellman's principle of optimality. We also present a point-based dynamic programming algorithm for RC-POMDPs. Evaluations on benchmark problems demonstrate the efficacy of our algorithm and show that policies for RC-POMDPs produce more desirable behaviors than policies for C-POMDPs.

------------

`[2401.00588] Fairness in Serving Large Language Models <https://arxiv.org/abs/2401.00588>`__ 大型语言模型服务的公平性

::

    replaced with revised version Wed, 5 Jun 2024 06:43:16 GMT
    Submission history From: Ying Sheng [view email]
    [v1] Sun, 31 Dec 2023 21:15:54 UTC (205 KB)
    [v2] Wed, 5 Jun 2024 06:43:16 UTC (575 KB)
    Ying Sheng, Shiyi Cao, Dacheng Li, Banghua Zhu, Zhuohan Li, Danyang Zhuo, Joseph E. Gonzalez, Ion Stoica

High-demand LLM inference services (e.g., ChatGPT and BARD) support a wide range of requests from short chat conversations to long document reading. To ensure that all client requests are processed fairly, most major LLM inference services have request rate limits, to ensure that no client can dominate the request queue. However, this rudimentary notion of fairness also results in under-utilization of the resources and poor client experience when there is spare capacity. While there is a rich literature on fair scheduling, serving LLMs presents new challenges due to their unpredictable request lengths and their unique batching characteristics on parallel accelerators. This paper introduces the definition of LLM serving fairness based on a cost function that accounts for the number of input and output tokens processed. To achieve fairness in serving, we propose a novel scheduling algorithm, the Virtual Token Counter (VTC), a fair scheduler based on the continuous batching mechanism. We prove a 2x tight upper bound on the service difference between two backlogged clients, adhering to the requirement of work-conserving. Through extensive experiments, we demonstrate the superior performance of VTC in ensuring fairness, especially in contrast to other baseline methods, which exhibit shortcomings under various conditions. The reproducible code is available at this https URL

------------

`[2402.05894] Large Language Model Meets Graph Neural Network in Knowledge Distillation <https://arxiv.org/abs/2402.05894>`__ 知识蒸馏中大型语言模型与图神经网络的相遇

::

    replaced with revised version Wed, 5 Jun 2024 11:38:12 GMT
    Submission history From: Shengxiang Hu [view email]
    [v1] Thu, 8 Feb 2024 18:33:21 UTC (437 KB)
    [v2] Fri, 9 Feb 2024 08:08:57 UTC (437 KB)
    [v3] Wed, 5 Jun 2024 11:38:12 UTC (799 KB)
    Shengxiang Hu, Guobing Zou, Song Yang, Yanglan Gan, Bofeng Zhang, Yixin Chen

In service-oriented architectures, accurately predicting the Quality of Service (QoS) is crucial for maintaining reliability and enhancing user satisfaction. However, significant challenges remain due to existing methods always overlooking high-order latent collaborative relationships between users and services and failing to dynamically adjust feature learning for every specific user-service invocation, which are critical for learning accurate features. Additionally, reliance on RNNs for capturing QoS evolution hampers models' ability to detect long-term trends due to difficulties in managing long-range dependencies. To address these challenges, we propose the \underline{T}arget-Prompt \underline{O}nline \underline{G}raph \underline{C}ollaborative \underline{L}earning (TOGCL) framework for temporal-aware QoS prediction. TOGCL leverages a dynamic user-service invocation graph to model historical interactions, providing a comprehensive representation of user-service relationships. Building on this graph, it develops a target-prompt graph attention network to extract online deep latent features of users and services at each time slice, simultaneously considering implicit collaborative relationships between target users/services and their neighbors, as well as relevant historical QoS values. Additionally, a multi-layer Transformer encoder is employed to uncover temporal feature evolution patterns of users and services, leading to temporal-aware QoS prediction. Extensive experiments conducted on the WS-DREAM dataset demonstrate that our proposed TOGCL framework significantly outperforms state-of-the-art methods across multiple metrics, achieving improvements of up to 38.80\%. These results underscore the effectiveness of the TOGCL framework for precise temporal QoS prediction.

------------

`[2402.09656] The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse <https://arxiv.org/abs/2402.09656>`__ 模型编辑的蝴蝶效应:很少的编辑就会引发大型语言模型崩溃

::

    replaced with revised version Wed, 5 Jun 2024 09:43:00 GMT
    Submission history From: Wanli Yang [view email]
    [v1] Thu, 15 Feb 2024 01:50:38 UTC (715 KB)
    [v2] Sun, 18 Feb 2024 08:00:46 UTC (713 KB)
    [v3] Thu, 14 Mar 2024 11:18:21 UTC (3,459 KB)
    [v4] Wed, 5 Jun 2024 09:43:00 UTC (6,804 KB)
    Wanli Yang, Fei Sun, Xinyu Ma, Xun Liu, Dawei Yin, Xueqi Cheng

Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating changes in an edited model's perplexity are strongly correlated with its downstream task performances. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse after only few edits. To facilitate further research, we have utilized GPT-3.5 to develop a new dataset, HardEdit, based on those hard cases. This dataset aims to establish the foundation for pioneering research in reliable model editing and the mechanisms underlying editing-induced model collapse. We hope this work can draw the community's attention to the potential risks inherent in model editing practices.

------------

`[2402.09836] Chain-of-Planned-Behaviour Workflow Elicits Few-Shot Mobility Generation in LLMs <https://arxiv.org/abs/2402.09836>`__ 行为规划链工作流在llm中引起少次移动生成

::

    replaced with revised version Wed, 5 Jun 2024 09:27:42 GMT
    Submission history From: Chenyang Shao [view email]
    [v1] Thu, 15 Feb 2024 09:58:23 UTC (1,182 KB)
    [v2] Wed, 5 Jun 2024 09:27:42 UTC (5,859 KB)
    Chenyang Shao, Fengli Xu, Bingbing Fan, Jingtao Ding, Yuan Yuan, Meng Wang, Yong Li

The powerful reasoning capabilities of large language models (LLMs) have brought revolutionary changes to many fields, but their performance in human behaviour generation has not yet been extensively explored. This gap likely emerges because the internal processes governing behavioral intentions cannot be solely explained by abstract reasoning. Instead, they are also influenced by a multitude of factors, including social norms and personal preference. Inspired by the Theory of Planned Behaviour (TPB), we develop a LLM workflow named Chain-of-Planned Behaviour (CoPB) for mobility behaviour generation, which reflects the important spatio-temporal dynamics of human activities. Through exploiting the cognitive structures of attitude, subjective norms, and perceived behaviour control in TPB, CoPB significantly enhance the ability of LLMs to reason the intention of next movement. Specifically, CoPB substantially reduces the error rate of mobility intention generation from 57.8% to 19.4%. To improve the scalability of the proposed CoPB workflow, we further explore the synergy between LLMs and mechanistic models. We find mechanistic mobility models, such as gravity model, can effectively map mobility intentions to physical mobility behaviours. The strategy of integrating CoPB with gravity model can reduce the token cost by 97.7% and achieve better performance simultaneously. Besides, the proposed CoPB workflow can facilitate GPT-4-turbo to automatically generate high quality labels for mobility behavior reasoning. We show such labels can be leveraged to fine-tune the smaller-scale, open source LLaMA 3-8B, which significantly reduces usage costs without sacrificing the quality of the generated behaviours.

------------

`[2405.20653] Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens <https://arxiv.org/abs/2405.20653>`__ 利用静默标记增强针对大型语言模型的越狱攻击

::

    replaced with revised version Tue, 4 Jun 2024 20:29:48 GMT
    Submission history From: Jiahao Yu [view email]
    [v1] Fri, 31 May 2024 07:41:03 UTC (1,501 KB)
    [v2] Tue, 4 Jun 2024 20:29:48 UTC (1,501 KB)
    Jiahao Yu, Haozheng Luo, Jerry Yao-Chieh Hu, Wenbo Guo, Han Liu, Xinyu Xing

Along with the remarkable successes of Language language models, recent research also started to explore the security threats of LLMs, including jailbreaking attacks. Attackers carefully craft jailbreaking prompts such that a target LLM will respond to the harmful question. Existing jailbreaking attacks require either human experts or leveraging complicated algorithms to craft jailbreaking prompts. In this paper, we introduce BOOST, a simple attack that leverages only the eos tokens. We demonstrate that rather than constructing complicated jailbreaking prompts, the attacker can simply append a few eos tokens to the end of a harmful question. It will bypass the safety alignment of LLMs and lead to successful jailbreaking attacks. We further apply BOOST to four representative jailbreak methods and show that the attack success rates of these methods can be significantly enhanced by simply adding eos tokens to the prompt. To understand this simple but novel phenomenon, we conduct empirical analyses. Our analysis reveals that adding eos tokens makes the target LLM believe the input is much less harmful, and eos tokens have low attention values and do not affect LLM's understanding of the harmful questions, leading the model to actually respond to the questions. Our findings uncover how fragile an LLM is against jailbreak attacks, motivating the development of strong safety alignment approaches.

------------

`[2303.13809] Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models <https://arxiv.org/abs/2303.13809>`__ 错误分析提示使大型语言模型的类人翻译评估成为可能

::

    replaced with revised version Wed, 5 Jun 2024 07:40:54 GMT
    Submission history From: Liang Ding [view email]
    [v1] Fri, 24 Mar 2023 05:05:03 UTC (992 KB)
    [v2] Sun, 8 Oct 2023 12:50:10 UTC (3,283 KB)
    [v3] Wed, 21 Feb 2024 04:18:32 UTC (4,167 KB)
    [v4] Wed, 5 Jun 2024 07:40:54 UTC (4,161 KB)
    Qingyu Lu, Baopu Qiu, Liang Ding, Kanjian Zhang, Tom Kocmi, Dacheng Tao

Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called \textbf{\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and \textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.

------------

`[2304.11406] LaMP: When Large Language Models Meet Personalization <https://arxiv.org/abs/2304.11406>`__ LaMP:大型语言模型遇到个性化问题

::

    replaced with revised version Wed, 5 Jun 2024 03:29:31 GMT
    Submission history From: Alireza Salemi [view email]
    [v1] Sat, 22 Apr 2023 13:42:04 UTC (7,415 KB)
    [v2] Fri, 19 May 2023 20:34:59 UTC (7,481 KB)
    [v3] Tue, 9 Jan 2024 21:00:25 UTC (812 KB)
    [v4] Wed, 5 Jun 2024 03:29:31 UTC (8,771 KB)
    Alireza Salemi, Sheshera Mysore, Michael Bendersky, Hamed Zamani

This paper highlights the importance of personalization in large language models and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on LaMP for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.

------------

`[2306.02796] MCTS: A Multi-Reference Chinese Text Simplification Dataset <https://arxiv.org/abs/2306.02796>`__ MCTS:一个多参考中文文本简化数据集

::

    replaced with revised version Wed, 5 Jun 2024 14:38:56 GMT
    Submission history From: Liner Yang [view email]
    [v1] Mon, 5 Jun 2023 11:46:36 UTC (574 KB)
    [v2] Sat, 30 Mar 2024 07:55:41 UTC (576 KB)
    [v3] Wed, 5 Jun 2024 14:38:56 UTC (576 KB)
    Ruining Chong, Luming Lu, Liner Yang, Jinran Nie, Zhenghao Liu, Shuo Wang, Shuhan Zhou, Yaoxin Li, Erhong Yang

Text simplification aims to make the text easier to understand by applying rewriting transformations. There has been very little research on Chinese text simplification for a long time. The lack of generic evaluation data is an essential reason for this phenomenon. In this paper, we introduce MCTS, a multi-reference Chinese text simplification dataset. We describe the annotation process of the dataset and provide a detailed analysis. Furthermore, we evaluate the performance of several unsupervised methods and advanced large language models. We additionally provide Chinese text simplification parallel data that can be used for training, acquired by utilizing machine translation and English text simplification. We hope to build a basic understanding of Chinese text simplification through the foundational work and provide references for future research. All of the code and data are released at this https URL.

------------

`[2306.07629] SqueezeLLM: Dense-and-Sparse Quantization <https://arxiv.org/abs/2306.07629>`__ SqueezeLLM:稠密稀疏量化

::

    replaced with revised version Wed, 5 Jun 2024 03:57:41 GMT
    Submission history From: Sehoon Kim [view email]
    [v1] Tue, 13 Jun 2023 08:57:54 UTC (600 KB)
    [v2] Wed, 4 Oct 2023 22:40:01 UTC (666 KB)
    [v3] Mon, 5 Feb 2024 05:42:32 UTC (645 KB)
    [v4] Wed, 5 Jun 2024 03:57:41 UTC (897 KB)
    Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, Kurt Keutzer

Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at this https URL.

------------

`[2309.08631] Large Language Models Can Infer Psychological Dispositions of Social Media Users <https://arxiv.org/abs/2309.08631>`__ 大型语言模型可以推断社交媒体用户的心理倾向

::

    replaced with revised version Wed, 5 Jun 2024 15:25:06 GMT
    Submission history From: Heinrich Peters [view email]
    [v1] Wed, 13 Sep 2023 01:27:48 UTC (197 KB)
    [v2] Wed, 5 Jun 2024 15:25:06 UTC (1,351 KB)
    Heinrich Peters and Sandra Matz

Large Language Models (LLMs) demonstrate increasingly human-like abilities across a wide variety of tasks. In this paper, we investigate whether LLMs like ChatGPT can accurately infer the psychological dispositions of social media users and whether their ability to do so varies across socio-demographic groups. Specifically, we test whether GPT-3.5 and GPT-4 can derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores - a level of accuracy that is similar to that of supervised machine learning models specifically trained to infer personality. Our findings also highlight heterogeneity in the accuracy of personality inferences across different age groups and gender categories: predictions were found to be more accurate for women and younger individuals on several traits, suggesting a potential bias stemming from the underlying training data or differences in online self-expression. The ability of LLMs to infer psychological dispositions from user-generated text has the potential to democratize access to cheap and scalable psychometric assessments for both researchers and practitioners. On the one hand, this democratization might facilitate large-scale research of high ecological validity and spark innovation in personalized services. On the other hand, it also raises ethical concerns regarding user privacy and self-determination, highlighting the need for stringent ethical frameworks and regulation.

------------

`[2310.11244] Entity Matching using Large Language Models <https://arxiv.org/abs/2310.11244>`__ 基于大型语言模型的实体匹配

::

    replaced with revised version Wed, 5 Jun 2024 15:33:25 GMT
    Submission history From: Ralph Peeters [view email]
    [v1] Tue, 17 Oct 2023 13:12:32 UTC (964 KB)
    [v2] Thu, 1 Feb 2024 19:05:44 UTC (518 KB)
    [v3] Wed, 5 Jun 2024 15:33:25 UTC (1,136 KB)
    Ralph Peeters, Christian Bizer

Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity and is a central step in most data integration pipelines. Many state-of-the-art entity matching methods rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. This paper investigates using generative large language models (LLMs) as a less task-specific training data-dependent and more robust alternative to PLM-based matchers. Our study covers hosted and open-source LLMs, which can be run locally. We evaluate these models in a zero-shot scenario and a scenario where task-specific training data is available. We compare different prompt designs and the prompt sensitivity of the models and show that there is no single best prompt but needs to be tuned for each model/dataset combination. We further investigate (i) the selection of in-context demonstrations, (ii) the generation of matching rules, as well as (iii) fine-tuning a hosted LLM using the same pool of training data. Our experiments show that the best LLMs require no or only a few training examples to perform similarly to PLMs that were fine-tuned using thousands of examples. LLM-based matchers further exhibit higher robustness to unseen entities. We show that GPT4 can generate structured explanations for matching decisions. The model can automatically identify potential causes of matching errors by analyzing explanations of wrong decisions. We demonstrate that the model can generate meaningful textual descriptions of the identified error classes, which can help data engineers improve entity matching pipelines.

------------

`[2311.05297] Challenging the Validity of Personality Tests for Large Language Models <https://arxiv.org/abs/2311.05297>`__ 对大型语言模型人格测试有效性的挑战

::

    replaced with revised version Wed, 5 Jun 2024 10:33:18 GMT
    Submission history From: Tom Sühr [view email]
    [v1] Thu, 9 Nov 2023 11:54:01 UTC (5,991 KB)
    [v2] Wed, 5 Jun 2024 10:33:18 UTC (524 KB)
    Tom S\"uhr, Florian E. Dorner, Samira Samadi, Augustin Kelava

With large language models (LLMs) like GPT-4 appearing to behave increasingly human-like in text-based interactions, it has become popular to attempt to evaluate personality traits of LLMs using questionnaires originally developed for humans. While reusing measures is a resource-efficient way to evaluate LLMs, careful adaptations are usually required to ensure that assessment results are valid even across human subpopulations. In this work, we provide evidence that LLMs' responses to personality tests systematically deviate from human responses, implying that the results of these tests cannot be interpreted in the same way. Concretely, reverse-coded items ("I am introverted" vs. "I am extraverted") are often both answered affirmatively. Furthermore, variation across prompts designed to "steer" LLMs to simulate particular personality types does not follow the clear separation into five independent personality factors from human samples. In light of these results, we believe that it is important to investigate tests' validity for LLMs before drawing strong conclusions about potentially ill-defined concepts like LLMs' "personality".

------------

`[2312.04511] An LLM Compiler for Parallel Function Calling <https://arxiv.org/abs/2312.04511>`__ 用于并行函数调用的LLM编译器

::

    replaced with revised version Wed, 5 Jun 2024 03:53:10 GMT
    Submission history From: Sehoon Kim [view email]
    [v1] Thu, 7 Dec 2023 18:32:04 UTC (440 KB)
    [v2] Tue, 6 Feb 2024 05:17:42 UTC (736 KB)
    [v3] Wed, 5 Jun 2024 03:53:10 UTC (592 KB)
    Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W. Mahoney, Kurt Keutzer, Amir Gholami

The reasoning capabilities of the recent LLMs enable them to execute external function calls to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed LLMs to select and coordinate multiple functions based on the context to tackle more complex problems. However, current methods for function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multiple function calls. Drawing inspiration from the principles of classical compilers, LLMCompiler enables parallel function calling with three components: (i) a Function Calling Planner, formulating execution plans for function calling; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically generates an optimized orchestration for the function calls and can be used with both open-source and closed-source models. We have benchmarked LLMCompiler on a range of tasks with different patterns of function calling. We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to ~9% compared to ReAct. Our code is available at this https URL.

------------

`[2312.04691] Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models <https://arxiv.org/abs/2312.04691>`__ simulm - llm:基于大型语言模型探索高质量同传的框架

::

    replaced with revised version Wed, 5 Jun 2024 03:12:13 GMT
    Submission history From: Lizhong Chen [view email]
    [v1] Thu, 7 Dec 2023 20:42:05 UTC (9,361 KB)
    [v2] Tue, 12 Dec 2023 03:17:29 UTC (9,361 KB)
    [v3] Wed, 5 Jun 2024 03:12:13 UTC (9,549 KB)
    Victor Agostinelli, Max Wild, Matthew Raffel, Kazi Ahmed Asif Fuad, Lizhong Chen

Large language models (LLMs) with billions of parameters and pretrained on massive amounts of data are now capable of near or better than state-of-the-art performance in a variety of downstream natural language processing tasks. Neural machine translation (NMT) is one such task that LLMs have been applied to with great success. However, little research has focused on applying LLMs to the more difficult subset of NMT called simultaneous translation (SimulMT), where translation begins before the entire source context is available to the model. In this paper, we address key challenges facing LLMs fine-tuned for SimulMT, validate classical SimulMT concepts and practices in the context of LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT, and introduce Simul-LLM, the first open-source fine-tuning and evaluation pipeline development framework for LLMs focused on SimulMT.

------------

`[2312.14187] WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning <https://arxiv.org/abs/2312.14187>`__ WaveCoder:通过指令调整广泛通用的代码大型语言模型增强

::

    replaced with revised version Wed, 5 Jun 2024 10:06:45 GMT
    Submission history From: Zhaojian Yu [view email]
    [v1] Wed, 20 Dec 2023 09:02:29 UTC (1,336 KB)
    [v2] Tue, 26 Dec 2023 13:51:38 UTC (1,336 KB)
    [v3] Thu, 11 Jan 2024 07:44:55 UTC (1,463 KB)
    [v4] Wed, 5 Jun 2024 10:06:45 UTC (2,464 KB)
    Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, Qiufeng Yin

Recent work demonstrates that, after being fine-tuned on a high-quality instruction dataset, the resulting model can obtain impressive capabilities to address a wide range of tasks. However, existing methods for instruction data generation often produce duplicate data and are not controllable enough on data quality. In this paper, we extend the generalization of instruction tuning by classifying the instruction data to 4 code-related tasks and propose a LLM-based Generator-Discriminator data process framework to generate diverse, high-quality instruction data from open source code. Hence, we introduce CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal code-related tasks,which is aimed at augmenting the effectiveness of instruction tuning and improving the generalization ability of fine-tuned model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with Widespread And Versatile Enhanced instruction tuning. This model is specifically designed for enhancing instruction tuning of Code Language Models (LLMs). Our experiments demonstrate that Wavecoder models outperform other open-source models in terms of generalization ability across different code-related tasks at the same level of fine-tuning scale. Moreover, Wavecoder exhibits high efficiency in previous code generation tasks. This paper thus offers a significant contribution to the field of instruction data generation and fine-tuning models, providing new insights and tools for enhancing performance in code-related tasks.

------------

`[2401.07870] JumpCoder: Go Beyond Autoregressive Coder via Online Modification <https://arxiv.org/abs/2401.07870>`__ JumpCoder:通过在线修改超越自回归编码器

::

    replaced with revised version Wed, 5 Jun 2024 14:12:03 GMT
    Submission history From: Mouxiang Chen [view email]
    [v1] Mon, 15 Jan 2024 18:04:29 UTC (314 KB)
    [v2] Wed, 5 Jun 2024 14:12:03 UTC (371 KB)
    Mouxiang Chen, Hao Tian, Zhongxin Liu, Xiaoxue Ren, Jianling Sun

While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance. We introduce JumpCoder, a novel model-agnostic framework that enables human-like online modification and non-sequential generation to augment code LLMs. The key idea behind JumpCoder is to insert new code into the currently generated code when necessary during generation, which is achieved through an auxiliary infilling model that works in tandem with the code LLM. Since identifying the best infill position beforehand is intractable, we adopt an \textit{infill-first, judge-later} strategy, which experiments with filling at the $k$ most critical positions following the generation of each line, and uses an Abstract Syntax Tree (AST) parser alongside the Generation Model Scoring to effectively judge the validity of each potential infill. Extensive experiments using six state-of-the-art code LLMs across multiple and multilingual benchmarks consistently indicate significant improvements over all baselines. Our code is public at this https URL.

------------

`[2401.11911] Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts? <https://arxiv.org/abs/2401.11911>`__ 被生成上下文蒙蔽:当知识冲突时，语言模型如何合并生成和检索上下文?

::

    replaced with revised version Wed, 5 Jun 2024 06:33:16 GMT
    Submission history From: Hexiang Tan [view email]
    [v1] Mon, 22 Jan 2024 12:54:04 UTC (2,115 KB)
    [v2] Tue, 13 Feb 2024 03:18:54 UTC (3,180 KB)
    [v3] Sat, 17 Feb 2024 15:57:46 UTC (3,180 KB)
    [v4] Tue, 26 Mar 2024 15:47:14 UTC (3,185 KB)
    [v5] Wed, 5 Jun 2024 06:33:16 UTC (3,213 KB)
    Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng

While auxiliary information has become a key to enhancing Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically contexts generated by LLMs and those retrieved from external sources. To investigate this, we formulate a systematic framework to identify whether LLMs' responses are attributed to either generated or retrieved contexts. To easily trace the origin of the response, we construct datasets with conflicting contexts, i.e., each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to favor generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of being selected; ii) the segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs. Our analysis enhances the understanding of how LLMs merge diverse contexts, offers valuable insights for advancing current LLM augmentation methods, and highlights the risk of generated misinformation for retrieval-augmented LLMs.

------------

`[2401.12192] Text Embedding Inversion Security for Multilingual Language Models <https://arxiv.org/abs/2401.12192>`__ 多语言语言模型的文本嵌入倒置安全性

::

    replaced with revised version Wed, 5 Jun 2024 10:22:00 GMT
    Submission history From: Yiyi Chen [view email]
    [v1] Mon, 22 Jan 2024 18:34:42 UTC (9,783 KB)
    [v2] Fri, 16 Feb 2024 11:10:57 UTC (12,248 KB)
    [v3] Tue, 4 Jun 2024 13:28:10 UTC (12,249 KB)
    [v4] Wed, 5 Jun 2024 10:22:00 UTC (12,259 KB)
    Yiyi Chen and Heather Lent and Johannes Bjerva

Textual data is often represented as real-numbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be susceptible to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages potentially exposed to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and cross-lingual inversion attacks, and explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English-based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for both monolingual and multilingual models. This study is the first to investigate multilingual inversion attacks, shedding light on the differences in attacks and defenses across monolingual and multilingual settings.

------------

`[2402.09363] Copyright Traps for Large Language Models <https://arxiv.org/abs/2402.09363>`__ 大型语言模型的版权陷阱

::

    replaced with revised version Tue, 4 Jun 2024 21:07:11 GMT
    Submission history From: Matthieu Meeus [view email]
    [v1] Wed, 14 Feb 2024 18:09:53 UTC (177 KB)
    [v2] Tue, 4 Jun 2024 21:07:11 UTC (245 KB)
    Matthieu Meeus, Igor Shilov, Manuel Faysse and Yves-Alexandre de Montjoye

Questions of fair use of copyright-protected content to train Large Language Models (LLMs) are being actively debated. Document-level inference has been proposed as a new task: inferring from black-box access to the trained model whether a piece of content has been seen during training. SOTA methods however rely on naturally occurring memorization of (part of) the content. While very effective against models that memorize significantly, we hypothesize--and later confirm--that they will not work against models that do not naturally memorize, e.g. medium-size 1B models. We here propose to use copyright traps, the inclusion of fictitious entries in original content, to detect the use of copyrighted materials in LLMs with a focus on models where memorization does not naturally occur. We carefully design a randomized controlled experimental setup, inserting traps into original content (books) and train a 1.3B LLM from scratch. We first validate that the use of content in our target model would be undetectable using existing methods. We then show, contrary to intuition, that even medium-length trap sentences repeated a significant number of times (100) are not detectable using existing methods. However, we show that longer sequences repeated a large number of times can be reliably detected (AUC=0.75) and used as copyright traps. Beyond copyright applications, our findings contribute to the study of LLM memorization: the randomized controlled setup enables us to draw causal relationships between memorization and certain sequence properties such as repetition in model training data and perplexity.

------------

`[2402.10058] Towards Safer Large Language Models through Machine Unlearning <https://arxiv.org/abs/2402.10058>`__ 通过机器遗忘实现更安全的大型语言模型

::

    replaced with revised version Wed, 5 Jun 2024 04:57:09 GMT
    Submission history From: Zheyuan Liu [view email]
    [v1] Thu, 15 Feb 2024 16:28:34 UTC (526 KB)
    [v2] Wed, 5 Jun 2024 04:57:09 UTC (713 KB)
    Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, Meng Jiang

The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.

------------

`[2402.10987] WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing <https://arxiv.org/abs/2402.10987>`__ WilKE:智慧层知识编辑器终身知识编辑

::

    replaced with revised version Wed, 5 Jun 2024 07:44:30 GMT
    Submission history From: Chenhui Hu [view email]
    [v1] Fri, 16 Feb 2024 05:29:59 UTC (25,166 KB)
    [v2] Wed, 5 Jun 2024 07:44:30 UTC (25,416 KB)
    Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao

Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named Wise-Layer Knowledge Editor (WilKE), which selects editing layer based on the pattern matching degree of editing knowledge across different layers in language models. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.

------------

`[2402.11456] FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence <https://arxiv.org/abs/2402.11456>`__ FactPICO:医学证据平实语言摘要的事实性评估

::

    replaced with revised version Wed, 5 Jun 2024 01:27:00 GMT
    Submission history From: Sebastian Joseph [view email]
    [v1] Sun, 18 Feb 2024 04:45:01 UTC (9,809 KB)
    [v2] Wed, 5 Jun 2024 01:27:00 UTC (9,806 KB)
    Sebastian Antony Joseph, Lily Chen, Jan Trienes, Hannah Louisa G\"oke, Monika Coers, Wei Xu, Byron C Wallace, Junyi Jessy Li

Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? This paper presents FactPICO, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. FactPICO consists of 345 plain language summaries of RCT abstracts generated from three LLMs (i.e., GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. We also evaluate the correctness of the extra information (e.g., explanations) added by LLMs. Using FactPICO, we benchmark a range of existing factuality metrics, including the newly devised ones based on LLMs. We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level.

------------

`[2402.11905] Learning to Edit: Aligning LLMs with Knowledge Editing <https://arxiv.org/abs/2402.11905>`__ 学习编辑:将llm与知识编辑对齐

::

    replaced with revised version Wed, 5 Jun 2024 15:46:03 GMT
    Submission history From: Yuxin Jiang [view email]
    [v1] Mon, 19 Feb 2024 07:45:17 UTC (545 KB)
    [v2] Wed, 5 Jun 2024 15:46:03 UTC (548 KB)
    Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, Qun Liu, Wei Wang

Knowledge editing techniques, aiming to efficiently modify a minor proportion of knowledge in large language models (LLMs) without negatively impacting performance across other inputs, have garnered widespread attention. However, existing methods predominantly rely on memorizing the updated knowledge, impeding LLMs from effectively combining the new knowledge with their inherent knowledge when answering questions. To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of "Teach a man to fish." LTE features a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on a meticulously curated parallel dataset to make reliable, in-scope edits while preserving out-of-scope information and linguistic proficiency; and (ii) the Inference Phase, which employs a retrieval-based mechanism for real-time and mass knowledge editing. By comparing our approach with seven advanced baselines across four popular knowledge editing benchmarks and two LLM architectures, we demonstrate LTE's superiority in knowledge editing performance, robustness in both batch and sequential editing, minimal interference on general tasks, and rapid editing speeds. The data and code are available at this https URL.

------------

`[2402.12261] NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms <https://arxiv.org/abs/2402.12261>`__ NEO-BENCH:基于新词的大型语言模型鲁棒性评估

::

    replaced with revised version Wed, 5 Jun 2024 17:12:23 GMT
    Submission history From: Jonathan Zheng [view email]
    [v1] Mon, 19 Feb 2024 16:19:15 UTC (2,040 KB)
    [v2] Sat, 16 Mar 2024 20:29:34 UTC (2,047 KB)
    [v3] Wed, 5 Jun 2024 17:12:23 UTC (2,049 KB)
    Jonathan Zheng, Alan Ritter, Wei Xu

The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments.

------------

`[2402.13211] Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation <https://arxiv.org/abs/2402.13211>`__ 大型语言模型能成为良好的情感支持者吗?减轻对情感支持对话的偏好偏见

::

    replaced with revised version Wed, 5 Jun 2024 13:39:59 GMT
    Submission history From: Dongjin Kang [view email]
    [v1] Tue, 20 Feb 2024 18:21:32 UTC (9,824 KB)
    [v2] Wed, 5 Jun 2024 13:39:59 UTC (9,830 KB)
    Dongjin Kang, Sunghwan Kim, Taeyoon Kwon, Seungjun Moon, Hyunsouk Cho, Youngjae Yu, Dongha Lee, Jinyoung Yeo

Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) existing LLMs alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs.

------------

`[2402.14836] Stealthy Attack on Large Language Model based Recommendation <https://arxiv.org/abs/2402.14836>`__ 基于大型语言模型推荐的隐形攻击

::

    replaced with revised version Wed, 5 Jun 2024 17:02:47 GMT
    Submission history From: Jinghao Zhang [view email]
    [v1] Sun, 18 Feb 2024 16:51:02 UTC (360 KB)
    [v2] Wed, 5 Jun 2024 17:02:47 UTC (361 KB)
    Jinghao Zhang, Yuting Liu, Qiang Liu, Shu Wu, Guibing Guo and Liang Wang

Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item's exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model's training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior efficacy and stealthiness of our approach. Our work unveils a significant security gap in LLM-based recommendation systems and paves the way for future research on protecting these systems.

------------

`[2402.14860] Ranking Large Language Models without Ground Truth <https://arxiv.org/abs/2402.14860>`__ 没有基本事实的大型语言模型排名

::

    replaced with revised version Wed, 5 Jun 2024 15:56:49 GMT
    Submission history From: Amit Dhurandhar [view email]
    [v1] Wed, 21 Feb 2024 00:49:43 UTC (2,792 KB)
    [v2] Wed, 6 Mar 2024 20:10:11 UTC (2,843 KB)
    [v3] Wed, 5 Jun 2024 15:56:49 UTC (2,871 KB)
    Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly and Karthikeyan Natesan Ramamurthy

Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover close to true rankings without reference data. This points to a viable low-resource mechanism for practical use.

------------

`[2402.15337] Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies <https://arxiv.org/abs/2402.15337>`__ 基于LLMs的概念空间维度实体排序:微调策略分析

::

    replaced with revised version Wed, 5 Jun 2024 10:42:40 GMT
    Submission history From: Nitesh Kumar [view email]
    [v1] Fri, 23 Feb 2024 14:17:01 UTC (7,877 KB)
    [v2] Wed, 5 Jun 2024 10:42:40 UTC (8,465 KB)
    Nitesh Kumar, Usashi Chatterjee, and Steven Schockaert

Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy, but existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but having at least some perceptual and subjective features in the training data seems essential for achieving the best results.

------------

`[2402.16786] Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models <https://arxiv.org/abs/2402.16786>`__ 政治指南针还是旋转的箭?在大型语言模型中对价值观和观点进行更有意义的评估

::

    replaced with revised version Wed, 5 Jun 2024 10:17:53 GMT
    Submission history From: Paul Röttger [view email]
    [v1] Mon, 26 Feb 2024 18:00:49 UTC (2,106 KB)
    [v2] Wed, 5 Jun 2024 10:17:53 UTC (2,109 KB)
    Paul R\"ottger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Sch\"utze, Dirk Hovy

Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.

------------

`[2402.17811] TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space <https://arxiv.org/abs/2402.17811>`__ TruthX:通过在真实空间中编辑大型语言模型来缓解幻觉

::

    replaced with revised version Wed, 5 Jun 2024 11:15:04 GMT
    Submission history From: Shaolei Zhang [view email]
    [v1] Tue, 27 Feb 2024 14:45:04 UTC (2,693 KB)
    [v2] Wed, 5 Jun 2024 11:15:04 UTC (2,814 KB)
    Shaolei Zhang, Tian Yu, Yang Feng

Large Language Models (LLMs) sometimes suffer from producing hallucinations, especially LLMs may generate untruthful responses despite knowing the correct knowledge. Activating the truthfulness within LLM is the key to fully unlocking LLM's knowledge potential. In this paper, we propose TruthX, an inference-time intervention method to activate the truthfulness of LLM by identifying and editing the features within LLM's internal representations that govern the truthfulness. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLM. Experiments show that TruthX improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to produce truthful or hallucinatory responses via editing only one vector in LLM's internal representations.

------------

`[2403.19121] Code Comparison Tuning for Code Large Language Models <https://arxiv.org/abs/2403.19121>`__ 代码大型语言模型的代码比较调优

::

    replaced with revised version Wed, 5 Jun 2024 04:00:08 GMT
    Submission history From: Yufan Jiang [view email]
    [v1] Thu, 28 Mar 2024 03:25:23 UTC (9,495 KB)
    [v2] Wed, 5 Jun 2024 04:00:08 UTC (9,590 KB)
    Yufan Jiang, Qiaozhi He, Xiaomin Zhuang, Zhihua Wu

We present Code Comparison Tuning (CCT), a simple and effective tuning method for code large language models (Code LLMs) to better handle subtle code errors. Specifically, we integrate the concept of comparison into instruction tuning, both at the token and sequence levels, enabling the model to discern even the slightest deviations in code. To compare the original code with an erroneous version containing manually added code errors, we use token-level preference loss for detailed token-level comparisons. Additionally, we combine code segments to create a new instruction tuning sample for sequence-level comparisons, enhancing the model's bug-fixing capability. Experimental results on the HumanEvalFix benchmark show that CCT surpasses instruction tuning in pass@1 scores by up to 4 points across diverse code LLMs, and extensive analysis demonstrates the effectiveness of our method.

------------

`[2404.01753] M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets <https://arxiv.org/abs/2404.01753>`__ M2SA:面向推文情感分析的多模态多语言模型

::

    replaced with revised version Wed, 5 Jun 2024 13:34:55 GMT
    Submission history From: Gaurish Thakkar Mr [view email]
    [v1] Tue, 2 Apr 2024 09:11:58 UTC (6,975 KB)
    [v2] Wed, 5 Jun 2024 13:34:55 UTC (6,975 KB)
    Gaurish Thakkar, Sherzod Hakimov, Marko Tadi\'c

In recent years, multimodal natural language processing, aimed at learning from diverse data types, has garnered significant attention. However, there needs to be more clarity when it comes to analysing multimodal tasks in multi-lingual contexts. While prior studies on sentiment analysis of tweets have predominantly focused on the English language, this paper addresses this gap by transforming an existing textual Twitter sentiment dataset into a multimodal format through a straightforward curation process. Our work opens up new avenues for sentiment-related research within the research community. Additionally, we conduct baseline experiments utilising this augmented dataset and report the findings. Notably, our evaluations reveal that when comparing unimodal and multimodal configurations, using a sentiment-tuned large language model as a text encoder performs exceptionally well.

------------

`[2404.03528] BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering <https://arxiv.org/abs/2404.03528>`__ BanglaAutoKG:基于语义神经图过滤的孟加拉语知识图谱自动构建

::

    replaced with revised version Wed, 5 Jun 2024 13:39:56 GMT
    Submission history From: Azmine Toushik Wasi [view email]
    [v1] Thu, 4 Apr 2024 15:31:21 UTC (293 KB)
    [v2] Fri, 5 Apr 2024 09:35:50 UTC (284 KB)
    [v3] Wed, 5 Jun 2024 13:39:56 UTC (284 KB)
    Azmine Toushik Wasi and Taki Hasan Rafi and Raima Islam and Dong-Kyu Chae

Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language. Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text. We utilize multilingual LLMs to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG. To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG. Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text.

------------

`[2404.18255] PatentGPT: A Large Language Model for Intellectual Property <https://arxiv.org/abs/2404.18255>`__ PatentGPT:面向知识产权的大型语言模型

::

    replaced with revised version Wed, 5 Jun 2024 03:02:48 GMT
    Submission history From: Zilong Bai [view email]
    [v1] Sun, 28 Apr 2024 17:36:43 UTC (749 KB)
    [v2] Tue, 30 Apr 2024 05:14:42 UTC (749 KB)
    [v3] Mon, 6 May 2024 03:00:19 UTC (749 KB)
    [v4] Tue, 7 May 2024 13:44:23 UTC (749 KB)
    [v5] Wed, 5 Jun 2024 03:02:48 UTC (753 KB)
    Zilong Bai, Ruiji Zhang, Linqing Chen, Qijun Cai, Yuan Zhong, Cong Wang, Yan Fang, Jie Fang, Jing Sun, Weikuan Wang, Lizhi Zhou, Haoran Hua, Tian Qiu, Chaochao Wang, Cheng Sun, Jianping Lu, Yixin Wang, Yubin Xia, Meng Hu, Haowen Liu, Peng Xu, Licong Xu, Fu Bian, Xiaolong Gu, Lisha Zhang, Weilei Wang, Changyang Tu

In recent years, large language models(LLMs) have attracted significant attention due to their exceptional performance across a multitude of natural language process tasks, and have been widely applied in various fields. However, the application of large language models in the Intellectual Property (IP) domain is challenging due to the strong need for specialized knowledge, privacy protection, processing of extremely long text in this field. In this technical report, we present for the first time a low-cost, standardized procedure for training IP-oriented LLMs, meeting the unique requirements of the IP domain. Using this standard process, we have trained the PatentGPT series models based on open-source pretrained models. By evaluating them on the open-source IP-oriented benchmark MOZIP, our domain-specific LLMs outperforms GPT-4, indicating the effectiveness of the proposed training procedure and the expertise of the PatentGPT models in the IP domain. Remarkably, our model surpassed GPT-4 on the 2019 China Patent Agent Qualification Examination, scoring 65 and matching human expert levels. Additionally, the PatentGPT model, which utilizes the SMoE architecture, achieves performance comparable to that of GPT-4 in the IP domain and demonstrates a better cost-performance ratio on long-text tasks, potentially serving as an alternative to GPT-4 within the IP domain.

------------

`[2405.00715] Adapting Open-Source Large Language Models for Cost-Effective, Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning <https://arxiv.org/abs/2405.00715>`__ 采用基于策略强化学习的开源大型语言模型，用于具有成本效益的专家级临床记录生成

::

    replaced with revised version Wed, 5 Jun 2024 04:03:17 GMT
    Submission history From: Hanyin Wang [view email]
    [v1] Thu, 25 Apr 2024 15:34:53 UTC (3,060 KB)
    [v2] Wed, 5 Jun 2024 04:03:17 UTC (2,847 KB)
    Hanyin Wang, Chufan Gao, Bolun Liu, Qiping Xu, Guleid Hussein, Mohamad El Labban, Kingsley Iheasirim, Hariprasad Korsapati, Chuck Outcalt, Jimeng Sun

Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have demonstrated promising capabilities in clinical text summarization tasks. However, due to patient data privacy concerns and computational costs, many healthcare providers prefer using small, locally-hosted models over external generic LLMs. This study presents a comprehensive domain- and task-specific adaptation process for the open-source LLaMA-2 13 billion parameter model, enabling it to generate high-quality clinical notes from outpatient patient-doctor dialogues. Our process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced a new approach, DistillDirect, for performing on-policy reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting model, LLaMA-Clinic, can generate clinical notes comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as "acceptable" or higher across all three criteria: real-world readiness, completeness, and accuracy. In the more challenging "Assessment and Plan" section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than physician-authored notes (4.1/5). Our cost analysis for inference shows that our LLaMA-Clinic model achieves a 4.375-fold cost reduction compared to an external generic LLM service. Additionally, we highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format, rather than relying on LLMs to determine this for clinical practice. We have made our newly created synthetic clinic dialogue-note dataset and the physician feedback dataset publicly available to foster future research.

------------

`[2405.13041] Assessing Political Bias in Large Language Models <https://arxiv.org/abs/2405.13041>`__ 大型语言模型的政治偏见评估

::

    replaced with revised version Wed, 5 Jun 2024 05:48:27 GMT
    Submission history From: Luca Rettenberger [view email]
    [v1] Fri, 17 May 2024 15:30:18 UTC (621 KB)
    [v2] Fri, 24 May 2024 07:59:31 UTC (799 KB)
    [v3] Wed, 5 Jun 2024 05:48:27 UTC (799 KB)
    Luca Rettenberger, Markus Reischl, Mark Schutera

The assessment of bias within Large Language Models (LLMs) has emerged as a critical concern in the contemporary discourse surrounding Artificial Intelligence (AI) in the context of their potential impact on societal dynamics. Recognizing and considering political bias within LLM applications is especially important when closing in on the tipping point toward performative prediction. Then, being educated about potential effects and the societal behavior LLMs can drive at scale due to their interplay with human operators. In this way, the upcoming elections of the European Parliament will not remain unaffected by LLMs. We evaluate the political bias of the currently most popular open-source LLMs (instruct or assistant models) concerning political issues within the European Union (EU) from a German voter's perspective. To do so, we use the "Wahl-O-Mat," a voting advice application used in Germany. From the voting advice of the "Wahl-O-Mat" we quantize the degree of alignment of LLMs with German political parties. We show that larger models, such as Llama3-70B, tend to align more closely with left-leaning political parties, while smaller models often remain neutral, particularly when prompted in English. The central finding is that LLMs are similarly biased, with low variances in the alignment concerning a specific party. Our findings underline the importance of rigorously assessing and making bias transparent in LLMs to safeguard the integrity and trustworthiness of applications that employ the capabilities of performative prediction and the invisible hand of machine learning prediction and language generation.

------------

`[2405.20974] SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales <https://arxiv.org/abs/2405.20974>`__ 自我:教LLMs用自我反思的理由表达自信

::

    replaced with revised version Wed, 5 Jun 2024 17:04:01 GMT
    Submission history From: Yangyi Chen [view email]
    [v1] Fri, 31 May 2024 16:21:16 UTC (10,502 KB)
    [v2] Wed, 5 Jun 2024 17:04:01 UTC (11,287 KB)
    Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao

Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at this https URL.

------------

`[2406.00832] BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling <https://arxiv.org/abs/2406.00832>`__ 大型语言模型的BoNBoN对齐和n最佳抽样的甜蜜

::

    replaced with revised version Wed, 5 Jun 2024 05:23:40 GMT
    Submission history From: Lin Gui [view email]
    [v1] Sun, 2 Jun 2024 18:42:57 UTC (875 KB)
    [v2] Wed, 5 Jun 2024 05:23:40 UTC (876 KB)
    Lin Gui, Cristina G\^arbacea, Victor Veitch

This paper concerns the problem of aligning samples from large language models to human preferences using best-of-$n$ sampling, where we draw $n$ samples, rank them, and return the best one. We consider two fundamental problems. First: what is the relationship between best-of-$n$ and approaches to alignment that train LLMs to output samples with a high expected reward (e.g., RLHF or DPO)? To answer this, we embed both the best-of-$n$ distribution and the sampling distributions learned by alignment procedures in a common class of tiltings of the base LLM distribution. We then show that, within this class, best-of-$n$ is essentially optimal in terms of the trade-off between win-rate against the base model vs KL distance from the base model. That is, best-of-$n$ is the best choice of alignment distribution if the goal is to maximize win rate. However, best-of-$n$ requires drawing $n$ samples for each inference, a substantial cost. To avoid this, the second problem we consider is how to fine-tune a LLM to mimic the best-of-$n$ sampling distribution. We derive BoNBoN Alignment to achieve this by exploiting the special structure of the best-of-$n$ distribution. Experiments show that BoNBoN alignment yields substantial improvements in producing a model that is preferred to the base policy while minimally affecting off-target aspects.

------------

`[2406.00975] Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost <https://arxiv.org/abs/2406.00975>`__ Luna:一种高精度低成本捕捉语言模型幻觉的评估基础模型

::

    replaced with revised version Wed, 5 Jun 2024 15:45:04 GMT
    Submission history From: Masha Belyi [view email]
    [v1] Mon, 3 Jun 2024 04:14:21 UTC (8,440 KB)
    [v2] Wed, 5 Jun 2024 15:45:04 UTC (8,562 KB)
    Masha Belyi, Robert Friel, Shuai Shao, Atindriyo Sanyal

Retriever Augmented Generation (RAG) systems have become pivotal in enhancing the capabilities of language models by incorporating external knowledge retrieval mechanisms. However, a significant challenge in deploying these systems in industry applications is the detection and mitigation of hallucinations: instances where the model generates information that is not grounded in the retrieved context. Addressing this issue is crucial for ensuring the reliability and accuracy of responses generated by large language models (LLMs) in diverse industry settings. Current hallucination detection techniques fail to deliver accuracy, low latency, and low cost simultaneously. We introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination detection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and commercial evaluation frameworks on the hallucination detection task, with 97% and 91% reduction in cost and latency, respectively. Luna is lightweight and generalizes across multiple industry verticals and out-of-domain data, making it an ideal candidate for industry LLM applications.

------------

`[2406.01873] CR-UTP: Certified Robustness against Universal Text Perturbations on Large Language Models <https://arxiv.org/abs/2406.01873>`__ CR-UTP:大型语言模型对普遍文本扰动的鲁棒性证明

::

    replaced with revised version Wed, 5 Jun 2024 15:53:01 GMT
    Submission history From: Mengxin Zheng [view email]
    [v1] Tue, 4 Jun 2024 01:02:22 UTC (832 KB)
    [v2] Wed, 5 Jun 2024 15:53:01 UTC (833 KB)
    Qian Lou, Xin Liang, Jiaqi Xue, Yancheng Zhang, Rui Xie, Mengxin Zheng

It is imperative to ensure the stability of every prediction made by a language model; that is, a language's prediction should remain consistent despite minor input variations, like word substitutions. In this paper, we investigate the problem of certifying a language model's robustness against Universal Text Perturbations (UTPs), which have been widely used in universal adversarial attacks and backdoor attacks. Existing certified robustness based on random smoothing has shown considerable promise in certifying the input-specific text perturbations (ISTPs), operating under the assumption that any random alteration of a sample's clean or adversarial words would negate the impact of sample-wise perturbations. However, with UTPs, masking only the adversarial words can eliminate the attack. A naive method is to simply increase the masking ratio and the likelihood of masking attack tokens, but it leads to a significant reduction in both certified accuracy and the certified radius due to input corruption by extensive masking. To solve this challenge, we introduce a novel approach, the superior prompt search method, designed to identify a superior prompt that maintains higher certified accuracy under extensive masking. Additionally, we theoretically motivate why ensembles are a particularly suitable choice as base prompts for random smoothing. The method is denoted by superior prompt ensembling technique. We also empirically confirm this technique, obtaining state-of-the-art results in multiple settings. These methodologies, for the first time, enable high certified accuracy against both UTPs and ISTPs. The source code of CR-UTP is available at \url {this https URL}.

------------

`[2406.01931] Dishonesty in Helpful and Harmless Alignment <https://arxiv.org/abs/2406.01931>`__ 有益无害的不诚实行为

::

    replaced with revised version Wed, 5 Jun 2024 07:21:19 GMT
    Submission history From: Youcheng Huang [view email]
    [v1] Tue, 4 Jun 2024 03:31:09 UTC (1,409 KB)
    [v2] Wed, 5 Jun 2024 07:21:19 UTC (1,405 KB)
    Youcheng Huang, Jingkun Tang, Duanyu Feng, Zheng Zhang, Wenqiang Lei, Jiancheng Lv, Anthony G. Cohn

People tell lies when seeking rewards. Large language models (LLMs) are aligned to human values with reinforcement learning where they get rewards if they satisfy human preference. We find that this also induces dishonesty in helpful and harmless alignment where LLMs tell lies in generating harmless responses. Using the latest interpreting tools, we detect dishonesty, show how LLMs can be harmful if their honesty is increased, and analyze such conflicts at the parameter-level. Given these preliminaries and the hypothesis that reward-seeking stimulates dishonesty, we theoretically show that the dishonesty can in-turn decrease the alignment performances and augment reward-seeking alignment with representation regularization. Extensive results, including GPT-4 annotated win-rates, perplexities, and cases studies demonstrate that we can train more honest, helpful, and harmless LLMs. We will make all our codes and results be open-sourced upon this paper's acceptance.

------------

`[2406.02050] Analyzing Social Biases in Japanese Large Language Models <https://arxiv.org/abs/2406.02050>`__ 日语大型语言模型中的社会偏见分析

::

    replaced with revised version Wed, 5 Jun 2024 07:56:11 GMT
    Submission history From: Hitomi Yanaka [view email]
    [v1] Tue, 4 Jun 2024 07:31:06 UTC (7,727 KB)
    [v2] Wed, 5 Jun 2024 07:56:11 UTC (7,727 KB)
    Hitomi Yanaka, Namgi Han, Ryoma Kumon, Jie Lu, Masashi Takeshita, Ryo Sekizawa, Taisei Kato, Hiromi Arai

With the development of Large Language Models (LLMs), social biases in the LLMs have become a crucial issue. While various benchmarks for social biases have been provided across languages, the extent to which Japanese LLMs exhibit social biases has not been fully investigated. In this study, we construct the Japanese Bias Benchmark dataset for Question Answering (JBBQ) based on the English bias benchmark BBQ, and analyze social biases in Japanese LLMs. The results show that while current Japanese LLMs improve their accuracies on JBBQ by instruction-tuning, their bias scores become larger. In addition, augmenting their prompts with warning about social biases reduces the effect of biases in some models.

------------

`[2406.02350] LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing <https://arxiv.org/abs/2406.02350>`__ LlamaCare:用于增强医疗保健知识共享的大型医疗语言模型

::

    replaced with revised version Wed, 5 Jun 2024 15:08:42 GMT
    Submission history From: Maojun Sun [view email]
    [v1] Tue, 4 Jun 2024 14:24:53 UTC (886 KB)
    [v2] Wed, 5 Jun 2024 15:08:42 UTC (886 KB)
    Maojun Sun

Large language models (LLMs) have shown amazing capabilities in knowledge memorization and the present. However, when it comes to domain-specific knowledge and downstream tasks like medical, general LLMs are often unable to give precise answers. In addition, when people want LLMs to answer classification questions, they usually go through instruction tuning first. However, LLMs do not always give a direct index of the categorization after instruction tuning. In this paper, we proposed LlamaCare, a fine-tuned medical language model, and Extended Classification Integration(ECI), a module to handle classification problems of LLMs. Our contributions are : (i) We fine-tuned a large language model of medical knowledge with very low carbon emissions and achieved similar performance with ChatGPT by a 24G GPU. (ii) We solved the problem of redundant categorical answers and improved the performance of LLMs by proposing a new module called Extended Classification Integration. (iii) We released our processed data for one-shot and few-shot training for some benchmarks such as PubMedQA and USMLE 1-3 step. Our method achieves a close performance comparable to some state-of-the-art models with the same quantity of parameters on benchmarks, while being more environmentally friendly by using less GPU computation time. Our models, codes, and datasets can be found at \url{this https URL}.

------------

`[2211.04325] Will we run out of data? Limits of LLM scaling based on human-generated data <https://arxiv.org/abs/2211.04325>`__ 我们会用完数据吗?基于人工生成数据的LLM缩放极限

::

    replaced with revised version Tue, 4 Jun 2024 22:09:46 GMT
    Submission history From: Pablo Villalobos [view email]
    [v1] Wed, 26 Oct 2022 00:28:40 UTC (4,493 KB)
    [v2] Tue, 4 Jun 2024 22:09:46 UTC (1,791 KB)
    Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, Marius Hobbhahn

We investigate the potential constraints on LLM scaling posed by the availability of public human-generated text data. We forecast the growing demand for training data based on current trends and estimate the total stock of public human text data. Our findings indicate that if current LLM development trends continue, models will be trained on datasets roughly equal in size to the available stock of public human text data between 2026 and 2032, or slightly earlier if models are overtrained. We explore how progress in language modeling can continue when human-generated text datasets cannot be scaled any further. We argue that synthetic data generation, transfer learning from data-rich domains, and data efficiency improvements might support further progress.

------------

`[2311.03285] S-LoRA: Serving Thousands of Concurrent LoRA Adapters <https://arxiv.org/abs/2311.03285>`__ S-LoRA:提供数千个并发的LoRA适配器

::

    replaced with revised version Wed, 5 Jun 2024 06:06:43 GMT
    Submission history From: Ying Sheng [view email]
    [v1] Mon, 6 Nov 2023 17:26:17 UTC (259 KB)
    [v2] Tue, 7 Nov 2023 06:59:33 UTC (274 KB)
    [v3] Wed, 5 Jun 2024 06:06:43 UTC (260 KB)
    Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, Ion Stoica

The "pretrain-then-finetune" paradigm is commonly adopted in the deployment of large language models. Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method, is often employed to adapt a base model to a multitude of tasks, resulting in a substantial collection of LoRA adapters derived from one base model. We observe that this paradigm presents significant opportunities for batched inference during serving. To capitalize on these opportunities, we present S-LoRA, a system designed for the scalable serving of many LoRA adapters. S-LoRA stores all adapters in the main memory and fetches the adapters used by the currently running queries to the GPU memory. To efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes Unified Paging. Unified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and KV cache tensors with varying sequence lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and highly optimized custom CUDA kernels for heterogeneous batching of LoRA computation. Collectively, these features enable S-LoRA to serve thousands of LoRA adapters on a single GPU or across multiple GPUs with a small overhead. Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with naive support of LoRA serving), S-LoRA can improve the throughput by up to 4 times and increase the number of served adapters by several orders of magnitude. As a result, S-LoRA enables scalable serving of many task-specific fine-tuned models and offers the potential for large-scale customized fine-tuning services. The code is available at this https URL

------------

`[2311.15983] SPIN: Sparsifying and Integrating Internal Neurons in Large Language Models for Text Classification <https://arxiv.org/abs/2311.15983>`__ SPIN:面向文本分类的大型语言模型内部神经元的稀疏化与集成

::

    replaced with revised version Wed, 5 Jun 2024 17:15:47 GMT
    Submission history From: Difan Jiao [view email]
    [v1] Mon, 27 Nov 2023 16:28:20 UTC (8,591 KB)
    [v2] Wed, 5 Jun 2024 17:15:47 UTC (11,463 KB)
    Difan Jiao, Yilun Liu, Zhenwei Tang, Daniel Matter, J\"urgen Pfeffer, Ashton Anderson

Among the many tasks that Large Language Models (LLMs) have revolutionized is text classification. Current text classification paradigms, however, rely solely on the output of the final layer in the LLM, with the rich information contained in internal neurons largely untapped. In this study, we present SPIN: a model-agnostic framework that sparsifies and integrates internal neurons of intermediate layers of LLMs for text classification. Specifically, SPIN sparsifies internal neurons by linear probing-based salient neuron selection layer by layer, avoiding noise from unrelated neurons and ensuring efficiency. The cross-layer salient neurons are then integrated to serve as multi-layered features for the classification head. Extensive experimental results show our proposed SPIN significantly improves text classification accuracy, efficiency, and interpretability.

------------

`[2402.02347] Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models <https://arxiv.org/abs/2402.02347>`__ 基于黎曼预条件LoRA的基础模型微调

::

    replaced with revised version Wed, 5 Jun 2024 06:36:45 GMT
    Submission history From: Fangzhao Zhang [view email]
    [v1] Sun, 4 Feb 2024 05:05:43 UTC (32,558 KB)
    [v2] Wed, 7 Feb 2024 06:17:13 UTC (32,558 KB)
    [v3] Wed, 5 Jun 2024 06:36:45 UTC (27,253 KB)
    Fangzhao Zhang, Mert Pilanci

Low-Rank Adaptation (LoRA) emerges as a popular parameter-efficient fine-tuning (PEFT) method, which proposes to freeze pretrained model weights and update an additive low-rank trainable matrix. In this work, we study the enhancement of LoRA training by introducing an $r \times r$ preconditioner in each gradient step where $r$ is the LoRA rank. We theoretically verify that the proposed preconditioner stabilizes feature learning with LoRA under infinite-width NN setting. Empirically, the implementation of this new preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with this new preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. The new preconditioner can be derived from a novel Riemannian metric in low-rank matrix field. Code can be accessed at this https URL.

------------

`[2402.10207] Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment <https://arxiv.org/abs/2402.10207>`__ 上下文奖励:基于动态偏好调整的基础模型多目标对齐

::

    replaced with revised version Wed, 5 Jun 2024 09:25:41 GMT
    Submission history From: Rui Yang [view email]
    [v1] Thu, 15 Feb 2024 18:58:31 UTC (1,125 KB)
    [v2] Mon, 19 Feb 2024 09:31:52 UTC (1,125 KB)
    [v3] Sun, 25 Feb 2024 03:38:48 UTC (1,125 KB)
    [v4] Fri, 24 May 2024 17:44:13 UTC (1,189 KB)
    [v5] Wed, 5 Jun 2024 09:25:41 UTC (1,190 KB)
    Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, Jianshu Chen

We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline.

------------

`[2402.12061] All Language Models Large and Small <https://arxiv.org/abs/2402.12061>`__ 所有语言模型大小不一

::

    replaced with revised version Wed, 5 Jun 2024 15:08:28 GMT
    Submission history From: Zhixun Chen [view email]
    [v1] Mon, 19 Feb 2024 11:28:20 UTC (1,924 KB)
    [v2] Wed, 5 Jun 2024 15:08:28 UTC (5,301 KB)
    Zhixun Chen, Yali Du, David Mguni

Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs (i.e. LMs require less GPU usage, but may not be able to solve the problem alone) everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove that LONDI converges to optimal solutions while also preserving budgetary constraints on LLM calls almost surely enabling it to solve various tasks while significantly lowering computational costs. We test LONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and demonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs while reducing GPU usage by up to 30%.

------------

`[2403.00932] Differentially Private Knowledge Distillation via Synthetic Text Generation <https://arxiv.org/abs/2403.00932>`__ 基于合成文本生成的差分隐私知识蒸馏

::

    replaced with revised version Wed, 5 Jun 2024 03:07:44 GMT
    Submission history From: James Flemings [view email]
    [v1] Fri, 1 Mar 2024 19:22:24 UTC (306 KB)
    [v2] Wed, 5 Jun 2024 03:07:44 UTC (307 KB)
    James Flemings and Murali Annavaram

Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy puts pressure on practitioners to train LLMs with Differential Privacy (DP) on private data. Concurrently, the exponential growth in parameter size of LLMs necessitates model compression before deployment of LLMs on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, simultaneously applying both schemes can compound the utility degradation. To this end, we propose DistilDP: a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private teacher LLM. The knowledge of a teacher LLM is transferred onto the student in two ways: one way from the synthetic data itself -- the hard labels, and the other way by the output distribution of the teacher evaluated on the synthetic data -- the soft labels. Furthermore, if the teacher and student share a similar architectural structure, we can further distill knowledge by aligning the hidden representations between both. Our experimental results demonstrate that DistilDP can substantially improve the utility over existing baselines, at least $9.0$ PPL on the Big Patent dataset, with strong privacy parameters, $\epsilon=2$. These promising results progress privacy-preserving compression of autoregressive LLMs. Our code can be accessed here: this https URL.

------------

`[2403.02419] Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems <https://arxiv.org/abs/2403.02419>`__ 你需要更多LLM电话吗?复合推理系统的缩放定律

::

    replaced with revised version Tue, 4 Jun 2024 21:20:33 GMT
    Submission history From: Lingjiao Chen [view email]
    [v1] Mon, 4 Mar 2024 19:12:48 UTC (1,401 KB)
    [v2] Tue, 4 Jun 2024 21:20:33 UTC (5,573 KB)
    Lingjiao Chen and Jared Quincy Davis and Boris Hanin and Peter Bailis and Ion Stoica and Matei Zaharia and James Zou

Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Language Model (LM) calls and aggregate their responses. However, there is little understanding of how the number of LM calls - e.g., when asking the LM to answer each question multiple times and taking a majority vote - affects such a compound system's performance. In this paper, we initiate the study of scaling properties of compound inference systems. We analyze, theoretically and empirically, how the number of LM calls affects the performance of Vote and Filter-Vote, two of the simplest compound system designs, which aggregate LM responses via majority voting, optionally applying LM filters. We find, surprisingly, that across multiple language tasks, the performance of both Vote and Filter-Vote can first increase but then decrease as a function of the number of LM calls. Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more LM calls lead to higher performance on "easy" queries, but lower performance on "hard" queries, and non-monotone behavior can emerge when a task contains both types of queries. This insight then allows us to compute, from a small number of samples, the number of LM calls that maximizes system performance, and define an analytical scaling model for both systems. Experiments show that our scaling model can accurately predict the performance of Vote and Filter-Vote systems and thus find the optimal number of LM calls to make.

------------

`[2404.01306] NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models <https://arxiv.org/abs/2404.01306>`__ NeuroPrune:一种面向大型语言模型的神经启发拓扑稀疏训练算法

::

    replaced with revised version Wed, 5 Jun 2024 16:07:13 GMT
    Submission history From: Amit Dhurandhar [view email]
    [v1] Wed, 28 Feb 2024 22:21:47 UTC (8,124 KB)
    [v2] Tue, 9 Apr 2024 14:59:10 UTC (8,124 KB)
    [v3] Wed, 5 Jun 2024 16:07:13 UTC (8,159 KB)
    Amit Dhurandhar, Tejaswini Pedapati, Ronny Luss, Soham Dan, Aurelie Lozano, Payel Das and Georgios Kollias

Transformer-based Language Models have become ubiquitous in Natural Language Processing (NLP) due to their impressive performance on various tasks. However, expensive training as well as inference remains a significant impediment to their widespread applicability. While enforcing sparsity at various levels of the model architecture has found promise in addressing scaling and efficiency issues, there remains a disconnect between how sparsity affects network topology. Inspired by brain neuronal networks, we explore sparsity approaches through the lens of network topology. Specifically, we exploit mechanisms seen in biological networks, such as preferential attachment and redundant synapse pruning, and show that principled, model-agnostic sparsity approaches are performant and efficient across diverse NLP tasks, spanning both classification (such as natural language inference) and generation (summarization, machine translation), despite our sole objective not being optimizing performance. NeuroPrune is competitive with (or sometimes superior to) baselines on performance and can be up to $10$x faster in terms of training time for a given level of sparsity, simultaneously exhibiting measurable improvements in inference time in many cases.

------------

`[2404.08819] The Illusion of State in State-Space Models <https://arxiv.org/abs/2404.08819>`__ 状态空间模型中的状态错觉

::

    replaced with revised version Tue, 4 Jun 2024 22:05:45 GMT
    Submission history From: William Merrill [view email]
    [v1] Fri, 12 Apr 2024 21:30:06 UTC (69 KB)
    [v2] Tue, 4 Jun 2024 22:05:45 UTC (76 KB)
    William Merrill and Jackson Petty and Ashish Sabharwal

State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill & Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). But do SSMs truly have an advantage (over transformers) in expressive power for state tracking? Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the "state" in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.

------------

`[2405.15362] Pipeline Parallelism with Controllable Memory <https://arxiv.org/abs/2405.15362>`__ 具有可控内存的流水线并行

::

    replaced with revised version Wed, 5 Jun 2024 08:19:02 GMT
    Submission history From: Penghui Qi [view email]
    [v1] Fri, 24 May 2024 08:54:36 UTC (1,466 KB)
    [v2] Wed, 5 Jun 2024 08:19:02 UTC (1,467 KB)
    Penghui Qi, Xinyi Wan, Nyamdavaa Amar, Min Lin

Pipeline parallelism has been widely explored, but most existing schedules lack a systematic methodology. In this paper, we propose a framework to decompose pipeline schedules as repeating a building block and we show that the lifespan of the building block decides the peak activation memory of the pipeline schedule. Guided by the observations, we find that almost all existing pipeline schedules, to the best of our knowledge, are memory inefficient. To address this, we introduce a family of memory efficient building blocks with controllable activation memory, which can reduce the peak activation memory to 1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable throughput. We can also achieve almost zero pipeline bubbles while maintaining the same activation memory as 1F1B. Our evaluations demonstrate that in pure pipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in terms of throughput. When employing a grid search over hybrid parallelism hyperparameters in practical scenarios, our proposed methods demonstrate a 16% throughput improvement over the 1F1B baseline for large language models.

------------

`[2405.19320] Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF <https://arxiv.org/abs/2405.19320>`__ 价值激励偏好优化:线上线下RLHF的统一方法

::

    replaced with revised version Tue, 4 Jun 2024 18:49:36 GMT
    Submission history From: Shicong Cen [view email]
    [v1] Wed, 29 May 2024 17:51:42 UTC (789 KB)
    [v2] Tue, 4 Jun 2024 18:49:36 UTC (1,639 KB)
    Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi, Bo Dai

Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.
In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\textit{sign}$ to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO.

------------

`[2405.20835] Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs <https://arxiv.org/abs/2405.20835>`__ 离群点和校准集对现代llm的量化影响越来越小

::

    replaced with revised version Wed, 5 Jun 2024 09:53:18 GMT
    Submission history From: Davide Paglieri [view email]
    [v1] Fri, 31 May 2024 14:24:33 UTC (14,201 KB)
    [v2] Mon, 3 Jun 2024 19:35:36 UTC (14,206 KB)
    [v3] Wed, 5 Jun 2024 09:53:18 UTC (14,206 KB)
    Davide Paglieri, Saurabh Dash, Tim Rockt\"aschel, Jack Parker-Holder

Post-Training Quantization (PTQ) enhances the efficiency of Large Language Models (LLMs) by enabling faster operation and compatibility with more accessible hardware through reduced memory usage, at the cost of small performance drops. We explore the role of calibration sets in PTQ, specifically their effect on hidden activations in various notable open-source LLMs. Calibration sets are crucial for evaluating activation magnitudes and identifying outliers, which can distort the quantization range and negatively impact performance. Our analysis reveals a marked contrast in quantization effectiveness across models. The older OPT model, upon which much of the quantization literature is based, shows significant performance deterioration and high susceptibility to outliers with varying calibration sets. In contrast, newer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and Mistral 7B demonstrate strong robustness, with Mistral 7B showing near-immunity to outliers and stable activations. These findings suggest a shift in PTQ strategies might be needed. As advancements in pre-training methods reduce the relevance of outliers, there is an emerging need to reassess the fundamentals of current quantization literature. The emphasis should pivot towards optimizing inference speed, rather than primarily focusing on outlier preservation, to align with the evolving characteristics of state-of-the-art LLMs.

------------

`[2405.21018] Improved Techniques for Optimization-Based Jailbreaking on Large Language Models <https://arxiv.org/abs/2405.21018>`__ 改进的大型语言模型优化越狱技术

::

    replaced with revised version Wed, 5 Jun 2024 16:35:49 GMT
    Submission history From: Xiaojun Jia [view email]
    [v1] Fri, 31 May 2024 17:07:15 UTC (1,362 KB)
    [v2] Wed, 5 Jun 2024 16:35:49 UTC (1,363 KB)
    Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, Min Lin

Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment. Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack's success has led to a growing interest in the study of optimization-based jailbreaking techniques. Although GCG is a significant milestone, its attacking efficiency remains unsatisfactory. In this paper, we present several improved (empirical) techniques for optimization-based jailbreaks like GCG. We first observe that the single target template of "Sure" largely limits the attacking performance of GCG; given this, we propose to apply diverse target templates containing harmful self-suggestion and/or guidance to mislead LLMs. Besides, from the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG (i.e., adaptively deciding how many tokens to replace in each step) to accelerate convergence, as well as tricks like easy-to-hard initialisation. Then, we combine these improved technologies to develop an efficient jailbreak method, dubbed I-GCG. In our experiments, we evaluate on a series of benchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved techniques can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly 100% attack success rate. The code is released at this https URL.

------------

`[2307.03941] Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions <https://arxiv.org/abs/2307.03941>`__ 大型语言模型时代被遗忘的权利:影响、挑战和解决方案

::

    replaced with revised version Wed, 5 Jun 2024 01:14:19 GMT
    Submission history From: Dawen Zhang [view email]
    [v1] Sat, 8 Jul 2023 09:28:50 UTC (94 KB)
    [v2] Fri, 8 Sep 2023 14:51:10 UTC (97 KB)
    [v3] Fri, 22 Sep 2023 01:43:33 UTC (97 KB)
    [v4] Wed, 5 Jun 2024 01:14:19 UTC (561 KB)
    Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, Xiwei Xu

The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja González, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. It was a significant emergent right as the result of the evolution of technology. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of differential privacy, machine unlearning, model editing, and guardrails. With the rapid advancement of AI and the increasing need of regulating this powerful technology, learning from the case of RTBF can provide valuable lessons for technical practitioners, legal experts, organizations, and authorities.

------------

`[2402.07408] Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples <https://arxiv.org/abs/2402.07408>`__ 大型语言模型是少样本生成器:提出混合提示算法生成Webshell逃逸样本

::

    replaced with revised version Wed, 5 Jun 2024 02:23:48 GMT
    Submission history From: Mingrui Ma [view email]
    [v1] Mon, 12 Feb 2024 04:59:58 UTC (2,248 KB)
    [v2] Wed, 5 Jun 2024 02:23:48 UTC (1,696 KB)
    Mingrui Ma, Lansheng Han, Chunjie Zhou

The frequent occurrence of cyber-attacks has made webshell attacks and defense gradually become a research hotspot in the field of network security. However, the lack of publicly available benchmark datasets and the over-reliance on manually defined rules for webshell escape sample generation have slowed down the progress of research related to webshell escape sample generation and artificial intelligence (AI)-based webshell detection. To address the drawbacks of weak webshell sample escape capabilities, the lack of webshell datasets with complex malicious features, and to promote the development of webshell detection, we propose the Hybrid Prompt algorithm for webshell escape sample generation with the help of large language models. As a prompt algorithm specifically developed for webshell sample generation, the Hybrid Prompt algorithm not only combines various prompt ideas including Chain of Thought, Tree of Thought, but also incorporates various components such as webshell hierarchical module and few-shot example to facilitate the LLM in learning and reasoning webshell escape strategies. Experimental results show that the Hybrid Prompt algorithm can work with multiple LLMs with excellent code reasoning ability to generate high-quality webshell samples with high Escape Rate (88.61% with GPT-4 model on VirusTotal detection engine) and (Survival Rate 54.98% with GPT-4 model).

------------

`[2402.14883] Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning <https://arxiv.org/abs/2402.14883>`__ 双i水印:保护LLM微调模型版权

::

    replaced with revised version Wed, 5 Jun 2024 11:30:02 GMT
    Submission history From: Shen Li [view email]
    [v1] Thu, 22 Feb 2024 04:55:14 UTC (1,510 KB)
    [v2] Wed, 29 May 2024 11:02:16 UTC (1,497 KB)
    [v3] Wed, 5 Jun 2024 11:30:02 UTC (1,497 KB)
    Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, Yaliang Li

To support various applications, a prevalent and efficient approach for business owners is leveraging their valuable datasets to fine-tune a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ``Double-I watermark''. Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed "Double-I watermark" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both quantitative and qualitative analyses.

------------

`[2404.15794] Large Language Models as In-context AI Generators for Quality-Diversity <https://arxiv.org/abs/2404.15794>`__ 大型语言模型作为上下文AI生成器的质量多样性

::

    replaced with revised version Wed, 5 Jun 2024 07:40:08 GMT
    Submission history From: Bryan Lim [view email]
    [v1] Wed, 24 Apr 2024 10:35:36 UTC (1,593 KB)
    [v2] Wed, 5 Jun 2024 07:40:08 UTC (1,593 KB)
    Bryan Lim, Manon Flageat, Antoine Cully

Quality-Diversity (QD) approaches are a promising direction to develop open-ended processes as they can discover archives of high-quality solutions across diverse niches. While already successful in many applications, QD approaches usually rely on combining only one or two solutions to generate new candidate solutions. As observed in open-ended processes such as technological evolution, wisely combining large diversity of these solutions could lead to more innovative solutions and potentially boost the productivity of QD search. In this work, we propose to exploit the pattern-matching capabilities of generative models to enable such efficient solution combinations. We introduce In-context QD, a framework of techniques that aim to elicit the in-context capabilities of pre-trained Large Language Models (LLMs) to generate interesting solutions using few-shot and many-shot prompting with quality-diverse examples from the QD archive as context. Applied to a series of common QD domains, In-context QD displays promising results compared to both QD baselines and similar strategies developed for single-objective optimization. Additionally, this result holds across multiple values of parameter sizes and archive population sizes, as well as across domains with distinct characteristics from BBO functions to policy search. Finally, we perform an extensive ablation that highlights the key prompt design considerations that encourage the generation of promising solutions for QD.

------------

`[2405.12258] Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment <https://arxiv.org/abs/2405.12258>`__ 用大型语言模型生成科学假设:乳腺癌治疗的实验室验证

::

    replaced with revised version Wed, 5 Jun 2024 08:50:51 GMT
    Submission history From: Abbi Abdel-Rehim [view email]
    [v1] Mon, 20 May 2024 11:40:23 UTC (486 KB)
    [v2] Wed, 5 Jun 2024 08:50:51 UTC (430 KB)
    Abbi Abdel-Rehim, Hector Zenil, Oghenejokpeme Orhobor, Marie Fisher, Ross J. Collins, Elizabeth Bourne, Gareth W. Fearnley, Emma Tate, Holly X. Smith, Larisa N. Soldatova, Ross D. King

Large language models (LLMs) have transformed AI and achieved breakthrough performance on a wide range of tasks that require human intelligence. In science, perhaps the most interesting application of LLMs is for hypothesis formation. A feature of LLMs, which results from their probabilistic structure, is that the output text is not necessarily a valid inference from the training text. These are 'hallucinations', and are a serious problem in many applications. However, in science, hallucinations may be useful: they are novel hypotheses whose validity may be tested by laboratory experiments. Here we experimentally test the use of LLMs as a source of scientific hypotheses using the domain of breast cancer treatment. We applied the LLM GPT4 to hypothesize novel pairs of FDA-approved non-cancer drugs that target the MCF7 breast cancer cell line relative to the non-tumorigenic breast cell line MCF10A. In the first round of laboratory experiments GPT4 succeeded in discovering three drug combinations (out of 12 tested) with synergy scores above the positive controls. These combinations were itraconazole + atenolol, disulfiram + simvastatin and dipyridamole + mebendazole. GPT4 was then asked to generate new combinations after considering its initial results. It then discovered three more combinations with positive synergy scores (out of four tested), these were disulfiram + fulvestrant, mebendazole + quinacrine and disulfiram + quinacrine. A limitation of GPT4 as a generator of hypotheses was that its explanations for them were formulaic and unconvincing. We conclude that LLMs are an exciting novel source of scientific hypotheses.

------------

-----------
Index (120)
-----------

`[2406.02791] Language Models can Infer Action Semantics for Classical Planners from Environment Feedback <https://arxiv.org/abs/2406.02791>`__ 语言模型可以从环境反馈中推断经典规划器的动作语义

`[2406.02804] $\texttt{ACCORD}$: Closing the Commonsense Measurability Gap <https://arxiv.org/abs/2406.02804>`__ $\texttt{ACCORD}$:缩小常识上的可测量性差距

`[2406.03299] The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games <https://arxiv.org/abs/2406.03299>`__ The Good, The Bad, and The Hulk-like GPT:分析合作和议价游戏中大型语言模型的情感决策

`[2406.03367] CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic Task Planning <https://arxiv.org/abs/2406.03367>`__ CLMASP:耦合大型语言模型与回答集编程的机器人任务规划

`[2406.02575] Cross-Modal Safety Alignment: Is textual unlearning all you need? <https://arxiv.org/abs/2406.02575>`__ 跨模态安全对齐:文本遗忘是你所需要的一切吗?

`[2406.02721] Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller <https://arxiv.org/abs/2406.02721>`__ 通过将后缀梯度压缩为前缀控制器实现LLM行为自控

`[2406.02756] Aligning Large Language Models via Fine-grained Supervision <https://arxiv.org/abs/2406.02756>`__ 基于细粒度监督的大型语言模型对齐

`[2406.02856] Xmodel-LM Technical Report <https://arxiv.org/abs/2406.02856>`__ Xmodel-LM技术报告

`[2406.02863] LLM as a Scorer: The Impact of Output Order on Dialogue Evaluation <https://arxiv.org/abs/2406.02863>`__ LLM作为评分者:输出顺序对对话评估的影响

`[2406.02886] PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs <https://arxiv.org/abs/2406.02886>`__ PLaD:基于伪偏好对的大规模语言模型蒸馏

`[2406.02888] HYDRA: Model Factorization Framework for Black-Box LLM Personalization <https://arxiv.org/abs/2406.02888>`__ HYDRA:黑盒LLM个性化的模型分解框架

`[2406.02919] MultifacetEval: Multifaceted Evaluation to Probe LLMs in Mastering Medical Knowledge <https://arxiv.org/abs/2406.02919>`__ MultifacetEval:从多方面探讨llm对医学知识的掌握

`[2406.02959] Adversarial Moment-Matching Distillation of Large Language Models <https://arxiv.org/abs/2406.02959>`__ 大型语言模型的对抗矩匹配蒸馏

`[2406.02962] Docs2KG: Unified Knowledge Graph Construction from Heterogeneous Documents Assisted by Large Language Models <https://arxiv.org/abs/2406.02962>`__ Docs2KG:大型语言模型辅助异构文档的统一知识图谱构建

`[2406.03004] Evaluation of data inconsistency for multi-modal sentiment analysis <https://arxiv.org/abs/2406.03004>`__ 面向多模态情感分析的数据不一致性评估

`[2406.03009] Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models <https://arxiv.org/abs/2406.03009>`__ 揭示选择偏差:探索大型语言模型中的顺序和Token敏感性

`[2406.03030] From Tarzan to Tolkien: Controlling the Language Proficiency Level of LLMs for Content Generation <https://arxiv.org/abs/2406.03030>`__ 从Tarzan到Tolkien:控制llm内容生成的语言熟练程度

`[2406.03079] Cryptocurrency Frauds for Dummies: How ChatGPT introduces us to fraud? <https://arxiv.org/abs/2406.03079>`__ 面向傻瓜的加密货币欺诈:ChatGPT如何向我们介绍欺诈?

`[2406.03151] Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation <https://arxiv.org/abs/2406.03151>`__ 你站在哪一边?用于端到端论点总结和评估的多任务数据集

`[2406.03158] CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs <https://arxiv.org/abs/2406.03158>`__ CSS: LLMs不确定性量化的对比语义相似度

`[2406.03170] StatBot.Swiss: Bilingual Open Data Exploration in Natural Language <https://arxiv.org/abs/2406.03170>`__ StatBot。Swiss:自然语言双语开放数据探索

`[2406.03181] Missci: Reconstructing Fallacies in Misrepresented Science <https://arxiv.org/abs/2406.03181>`__ Missci:重建被歪曲的科学中的谬误

`[2406.03198] The Impossibility of Fair LLMs <https://arxiv.org/abs/2406.03198>`__ 公平LLMs的不可能

`[2406.03199] Bayesian WeakS-to-Strong from Text Classification to Generation <https://arxiv.org/abs/2406.03199>`__ 贝叶斯从弱到强从文本分类到生成

`[2406.03202] ChatLang-8: An LLM-Based Synthetic Data Generation Framework for Grammatical Error Correction <https://arxiv.org/abs/2406.03202>`__ ChatLang-8:基于llm的语法纠错合成数据生成框架

`[2406.03339] The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches <https://arxiv.org/abs/2406.03339>`__ 评估LLM应用的挑战:自动化、人工和基于LLM方法的分析

`[2406.03363] LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback <https://arxiv.org/abs/2406.03363>`__ 基于llm的基于机器反馈强化学习的不恰当辩论重写

`[2406.03397] Automating Turkish Educational Quiz Generation Using Large Language Models <https://arxiv.org/abs/2406.03397>`__ 基于大型语言模型的土耳其教育测验自动生成

`[2406.03441] Cycles of Thought: Measuring LLM Confidence through Stable Explanations <https://arxiv.org/abs/2406.03441>`__ 思维周期:通过稳定的解释衡量LLM的置信度

`[2406.03450] What is the Best Way for ChatGPT to Translate Poetry? <https://arxiv.org/abs/2406.03450>`__ ChatGPT翻译诗歌的最佳方式是什么?

`[2406.03486] BIPED: Pedagogically Informed Tutoring System for ESL Education <https://arxiv.org/abs/2406.03486>`__ BIPED:基于教学信息的ESL教学系统

`[2406.03487] Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends <https://arxiv.org/abs/2406.03487>`__ 对话摘要中的LLM行为分析:揭示环境幻觉趋势

`[2406.03496] Wings: Learning Multimodal LLMs without Text-only Forgetting <https://arxiv.org/abs/2406.03496>`__ Wings:无文本遗忘的多模态llm学习

`[2406.02591] Unveiling the Potential of AI for Nanomaterial Morphology Prediction <https://arxiv.org/abs/2406.02591>`__ 揭示人工智能在纳米材料形态预测方面的潜力

`[2406.02592] LOLAMEME: Logic, Language, Memory, Mechanistic Framework <https://arxiv.org/abs/2406.02592>`__ 逻辑，语言，记忆，机械框架

`[2406.02611] LOLA: LLM-Assisted Online Learning Algorithm for Content Experiments <https://arxiv.org/abs/2406.02611>`__ LOLA: llm辅助的内容实验在线学习算法

`[2406.02613] ACCO: Accumulate while you Communicate, Hiding Communications in Distributed LLM Training <https://arxiv.org/abs/2406.02613>`__ ACCO:在交流时积累，在分布式LLM训练中隐藏交流

`[2406.02616] Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A Model-Based Reinforcement Learning Approach <https://arxiv.org/abs/2406.02616>`__ 边缘计算中无线LLM推理的自适应层分裂:基于模型的强化学习方法

`[2406.02642] E-ICL: Enhancing Fine-Grained Emotion Recognition through the Lens of Prototype Theory <https://arxiv.org/abs/2406.02642>`__ E-ICL:基于原型理论的增强细粒度情感识别

`[2406.02764] Adaptive Preference Scaling for Reinforcement Learning with Human Feedback <https://arxiv.org/abs/2406.02764>`__ 人工反馈强化学习的自适应偏好缩放

`[2406.02806] Randomized Geometric Algebra Methods for Convex Neural Networks <https://arxiv.org/abs/2406.02806>`__ 凸神经网络的随机几何代数方法

`[2406.02900] Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms <https://arxiv.org/abs/2406.02900>`__ 直接对齐算法中奖励模型过度优化的缩放定律

`[2406.02913] Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity <https://arxiv.org/abs/2406.02913>`__ 极端稀疏llm的零阶微调

`[2406.02924] Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models <https://arxiv.org/abs/2406.02924>`__ Pruner-Zero:从头开始演化大型语言模型的符号剪枝度量

`[2406.02958] PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs <https://arxiv.org/abs/2406.02958>`__ 前置文本:llm时代在私有联邦数据上训练语言模型

`[2406.02969] Filtered not Mixed: Stochastic Filtering-Based Online Gating for Mixture of Large Language Models <https://arxiv.org/abs/2406.02969>`__ 过滤而非混合:基于随机过滤的大型语言模型混合在线门控

`[2406.03324] UDQL: Bridging The Gap between MSE Loss and The Optimal Value Function in Offline Reinforcement Learning <https://arxiv.org/abs/2406.03324>`__ UDQL:弥合离线强化学习中MSE损失和最优值函数之间的差距

`[2406.03428] HelloFresh: LLM Evaluations on Streams of Real-World Human Editorial Actions across X Community Notes and Wikipedia edits <https://arxiv.org/abs/2406.03428>`__ HelloFresh: LLM对X社区笔记和维基百科编辑的真实世界人类编辑行为流进行评估

`[2406.03445] Pre-trained Large Language Models Use Fourier Features to Compute Addition <https://arxiv.org/abs/2406.03445>`__ 预训练大型语言模型使用傅里叶特征计算加法

`[2406.03476] Does your data spark joy? Performance gains from domain upsampling at the end of training <https://arxiv.org/abs/2406.03476>`__ 你的数据能带来快乐吗?训练结束时域上采样带来的性能提升

`[2406.02554] Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition <https://arxiv.org/abs/2406.02554>`__ 听我说，看我说，懂我说:视听自闭症行为识别

`[2406.03283] Enhancing Repository-Level Code Generation with Integrated Contextual Information <https://arxiv.org/abs/2406.03283>`__ 使用集成的上下文信息增强存储库级代码生成

`[2406.02795] ArguMentor: Augmenting User Experiences with Counter-Perspectives <https://arxiv.org/abs/2406.02795>`__ ArguMentor:用反视角增强用户体验

`[2406.02844] Item-Language Model for Conversational Recommendation <https://arxiv.org/abs/2406.02844>`__ 会话推荐的项目语言模型

`[2406.03248] Large Language Models as Evaluators for Recommendation Explanations <https://arxiv.org/abs/2406.03248>`__ 大型语言模型作为推荐解释的评估器

`[2406.02915] Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models <https://arxiv.org/abs/2406.02915>`__ 视觉-文本交叉对齐:改进视觉-语言模型中的相似度得分

`[2406.03230] Defending Large Language Models Against Attacks With Residual Stream Activation Analysis <https://arxiv.org/abs/2406.03230>`__ 基于残差流激活分析的大型语言模型防御攻击

`[2310.09688] Recursively-Constrained Partially Observable Markov Decision Processes <https://arxiv.org/abs/2310.09688>`__ 递归约束部分可观测马尔可夫决策过程

`[2401.00588] Fairness in Serving Large Language Models <https://arxiv.org/abs/2401.00588>`__ 大型语言模型服务的公平性

`[2402.05894] Large Language Model Meets Graph Neural Network in Knowledge Distillation <https://arxiv.org/abs/2402.05894>`__ 知识蒸馏中大型语言模型与图神经网络的相遇

`[2402.09656] The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse <https://arxiv.org/abs/2402.09656>`__ 模型编辑的蝴蝶效应:很少的编辑就会引发大型语言模型崩溃

`[2402.09836] Chain-of-Planned-Behaviour Workflow Elicits Few-Shot Mobility Generation in LLMs <https://arxiv.org/abs/2402.09836>`__ 行为规划链工作流在llm中引起少次移动生成

`[2405.20653] Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens <https://arxiv.org/abs/2405.20653>`__ 利用静默标记增强针对大型语言模型的越狱攻击

`[2303.13809] Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models <https://arxiv.org/abs/2303.13809>`__ 错误分析提示使大型语言模型的类人翻译评估成为可能

`[2304.11406] LaMP: When Large Language Models Meet Personalization <https://arxiv.org/abs/2304.11406>`__ LaMP:大型语言模型遇到个性化问题

`[2306.02796] MCTS: A Multi-Reference Chinese Text Simplification Dataset <https://arxiv.org/abs/2306.02796>`__ MCTS:一个多参考中文文本简化数据集

`[2306.07629] SqueezeLLM: Dense-and-Sparse Quantization <https://arxiv.org/abs/2306.07629>`__ SqueezeLLM:稠密稀疏量化

`[2309.08631] Large Language Models Can Infer Psychological Dispositions of Social Media Users <https://arxiv.org/abs/2309.08631>`__ 大型语言模型可以推断社交媒体用户的心理倾向

`[2310.11244] Entity Matching using Large Language Models <https://arxiv.org/abs/2310.11244>`__ 基于大型语言模型的实体匹配

`[2311.05297] Challenging the Validity of Personality Tests for Large Language Models <https://arxiv.org/abs/2311.05297>`__ 对大型语言模型人格测试有效性的挑战

`[2312.04511] An LLM Compiler for Parallel Function Calling <https://arxiv.org/abs/2312.04511>`__ 用于并行函数调用的LLM编译器

`[2312.04691] Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models <https://arxiv.org/abs/2312.04691>`__ simulm - llm:基于大型语言模型探索高质量同传的框架

`[2312.14187] WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning <https://arxiv.org/abs/2312.14187>`__ WaveCoder:通过指令调整广泛通用的代码大型语言模型增强

`[2401.07870] JumpCoder: Go Beyond Autoregressive Coder via Online Modification <https://arxiv.org/abs/2401.07870>`__ JumpCoder:通过在线修改超越自回归编码器

`[2401.11911] Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts? <https://arxiv.org/abs/2401.11911>`__ 被生成上下文蒙蔽:当知识冲突时，语言模型如何合并生成和检索上下文?

`[2401.12192] Text Embedding Inversion Security for Multilingual Language Models <https://arxiv.org/abs/2401.12192>`__ 多语言语言模型的文本嵌入倒置安全性

`[2402.09363] Copyright Traps for Large Language Models <https://arxiv.org/abs/2402.09363>`__ 大型语言模型的版权陷阱

`[2402.10058] Towards Safer Large Language Models through Machine Unlearning <https://arxiv.org/abs/2402.10058>`__ 通过机器遗忘实现更安全的大型语言模型

`[2402.10987] WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing <https://arxiv.org/abs/2402.10987>`__ WilKE:智慧层知识编辑器终身知识编辑

`[2402.11456] FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence <https://arxiv.org/abs/2402.11456>`__ FactPICO:医学证据平实语言摘要的事实性评估

`[2402.11905] Learning to Edit: Aligning LLMs with Knowledge Editing <https://arxiv.org/abs/2402.11905>`__ 学习编辑:将llm与知识编辑对齐

`[2402.12261] NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms <https://arxiv.org/abs/2402.12261>`__ NEO-BENCH:基于新词的大型语言模型鲁棒性评估

`[2402.13211] Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation <https://arxiv.org/abs/2402.13211>`__ 大型语言模型能成为良好的情感支持者吗?减轻对情感支持对话的偏好偏见

`[2402.14836] Stealthy Attack on Large Language Model based Recommendation <https://arxiv.org/abs/2402.14836>`__ 基于大型语言模型推荐的隐形攻击

`[2402.14860] Ranking Large Language Models without Ground Truth <https://arxiv.org/abs/2402.14860>`__ 没有基本事实的大型语言模型排名

`[2402.15337] Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies <https://arxiv.org/abs/2402.15337>`__ 基于LLMs的概念空间维度实体排序:微调策略分析

`[2402.16786] Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models <https://arxiv.org/abs/2402.16786>`__ 政治指南针还是旋转的箭?在大型语言模型中对价值观和观点进行更有意义的评估

`[2402.17811] TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space <https://arxiv.org/abs/2402.17811>`__ TruthX:通过在真实空间中编辑大型语言模型来缓解幻觉

`[2403.19121] Code Comparison Tuning for Code Large Language Models <https://arxiv.org/abs/2403.19121>`__ 代码大型语言模型的代码比较调优

`[2404.01753] M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets <https://arxiv.org/abs/2404.01753>`__ M2SA:面向推文情感分析的多模态多语言模型

`[2404.03528] BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering <https://arxiv.org/abs/2404.03528>`__ BanglaAutoKG:基于语义神经图过滤的孟加拉语知识图谱自动构建

`[2404.18255] PatentGPT: A Large Language Model for Intellectual Property <https://arxiv.org/abs/2404.18255>`__ PatentGPT:面向知识产权的大型语言模型

`[2405.00715] Adapting Open-Source Large Language Models for Cost-Effective, Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning <https://arxiv.org/abs/2405.00715>`__ 采用基于策略强化学习的开源大型语言模型，用于具有成本效益的专家级临床记录生成

`[2405.13041] Assessing Political Bias in Large Language Models <https://arxiv.org/abs/2405.13041>`__ 大型语言模型的政治偏见评估

`[2405.20974] SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales <https://arxiv.org/abs/2405.20974>`__ 自我:教LLMs用自我反思的理由表达自信

`[2406.00832] BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling <https://arxiv.org/abs/2406.00832>`__ 大型语言模型的BoNBoN对齐和n最佳抽样的甜蜜

`[2406.00975] Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost <https://arxiv.org/abs/2406.00975>`__ Luna:一种高精度低成本捕捉语言模型幻觉的评估基础模型

`[2406.01873] CR-UTP: Certified Robustness against Universal Text Perturbations on Large Language Models <https://arxiv.org/abs/2406.01873>`__ CR-UTP:大型语言模型对普遍文本扰动的鲁棒性证明

`[2406.01931] Dishonesty in Helpful and Harmless Alignment <https://arxiv.org/abs/2406.01931>`__ 有益无害的不诚实行为

`[2406.02050] Analyzing Social Biases in Japanese Large Language Models <https://arxiv.org/abs/2406.02050>`__ 日语大型语言模型中的社会偏见分析

`[2406.02350] LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing <https://arxiv.org/abs/2406.02350>`__ LlamaCare:用于增强医疗保健知识共享的大型医疗语言模型

`[2211.04325] Will we run out of data? Limits of LLM scaling based on human-generated data <https://arxiv.org/abs/2211.04325>`__ 我们会用完数据吗?基于人工生成数据的LLM缩放极限

`[2311.03285] S-LoRA: Serving Thousands of Concurrent LoRA Adapters <https://arxiv.org/abs/2311.03285>`__ S-LoRA:提供数千个并发的LoRA适配器

`[2311.15983] SPIN: Sparsifying and Integrating Internal Neurons in Large Language Models for Text Classification <https://arxiv.org/abs/2311.15983>`__ SPIN:面向文本分类的大型语言模型内部神经元的稀疏化与集成

`[2402.02347] Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models <https://arxiv.org/abs/2402.02347>`__ 基于黎曼预条件LoRA的基础模型微调

`[2402.10207] Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment <https://arxiv.org/abs/2402.10207>`__ 上下文奖励:基于动态偏好调整的基础模型多目标对齐

`[2402.12061] All Language Models Large and Small <https://arxiv.org/abs/2402.12061>`__ 所有语言模型大小不一

`[2403.00932] Differentially Private Knowledge Distillation via Synthetic Text Generation <https://arxiv.org/abs/2403.00932>`__ 基于合成文本生成的差分隐私知识蒸馏

`[2403.02419] Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems <https://arxiv.org/abs/2403.02419>`__ 你需要更多LLM电话吗?复合推理系统的缩放定律

`[2404.01306] NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models <https://arxiv.org/abs/2404.01306>`__ NeuroPrune:一种面向大型语言模型的神经启发拓扑稀疏训练算法

`[2404.08819] The Illusion of State in State-Space Models <https://arxiv.org/abs/2404.08819>`__ 状态空间模型中的状态错觉

`[2405.15362] Pipeline Parallelism with Controllable Memory <https://arxiv.org/abs/2405.15362>`__ 具有可控内存的流水线并行

`[2405.19320] Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF <https://arxiv.org/abs/2405.19320>`__ 价值激励偏好优化:线上线下RLHF的统一方法

`[2405.20835] Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs <https://arxiv.org/abs/2405.20835>`__ 离群点和校准集对现代llm的量化影响越来越小

`[2405.21018] Improved Techniques for Optimization-Based Jailbreaking on Large Language Models <https://arxiv.org/abs/2405.21018>`__ 改进的大型语言模型优化越狱技术

`[2307.03941] Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions <https://arxiv.org/abs/2307.03941>`__ 大型语言模型时代被遗忘的权利:影响、挑战和解决方案

`[2402.07408] Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples <https://arxiv.org/abs/2402.07408>`__ 大型语言模型是少样本生成器:提出混合提示算法生成Webshell逃逸样本

`[2402.14883] Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning <https://arxiv.org/abs/2402.14883>`__ 双i水印:保护LLM微调模型版权

`[2404.15794] Large Language Models as In-context AI Generators for Quality-Diversity <https://arxiv.org/abs/2404.15794>`__ 大型语言模型作为上下文AI生成器的质量多样性

`[2405.12258] Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment <https://arxiv.org/abs/2405.12258>`__ 用大型语言模型生成科学假设:乳腺癌治疗的实验室验证

