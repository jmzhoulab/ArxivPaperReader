240607
========

----------
Survey (6)
----------

`[2406.03712] A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions <https://arxiv.org/abs/2406.03712>`__ 医学大型语言模型综述:技术、应用、可信性及未来方向

::

    Thu, 6 Jun 2024 03:15:13 GMT
    Lei Liu, Xiaoyan Yang, Junchi Lei, Xiaoyang Liu, Yue Shen, Zhiqiang Zhang, Peng Wei, Jinjie Gu, Zhixuan Chu, Zhan Qin, Kui Ren

Large language models (LLMs), such as GPT series models, have received substantial attention due to their impressive capabilities for generating and understanding human-level language. More recently, LLMs have emerged as an innovative and powerful adjunct in the medical field, transforming traditional practices and heralding a new era of enhanced healthcare services. This survey provides a comprehensive overview of Medical Large Language Models (Med-LLMs), outlining their evolution from general to the medical-specific domain (i.e, Technology and Application), as well as their transformative impact on healthcare (e.g., Trustworthiness and Safety). Concretely, starting from the fundamental history and technology of LLMs, we first delve into the progressive adaptation and refinements of general LLM models in the medical domain, especially emphasizing the advanced algorithms that boost the LLMs' performance in handling complicated medical environments, including clinical reasoning, knowledge graph, retrieval-augmented generation, human alignment, and multi-modal learning. Secondly, we explore the extensive applications of Med-LLMs across domains such as clinical decision support, report generation, and medical education, illustrating their potential to streamline healthcare services and augment patient outcomes. Finally, recognizing the imperative and responsible innovation, we discuss the challenges of ensuring fairness, accountability, privacy, and robustness in Med-LLMs applications. Finally, we conduct a concise discussion for anticipating possible future trajectories of Med-LLMs, identifying avenues for the prudent expansion of Med-LLMs. By consolidating above-mentioned insights, this review seeks to provide a comprehensive investigation of the potential strengths and limitations of Med-LLMs for professionals and researchers, ensuring a responsible landscape in the healthcare setting.

------------

`[2406.04244] Benchmark Data Contamination of Large Language Models: A Survey <https://arxiv.org/abs/2406.04244>`__ 大型语言模型基准数据污染研究综述

::

    Thu, 6 Jun 2024 16:41:39 GMT
    Cheng Xu, Shuhao Guan, Derek Greene, M-Tahar Kechadi

The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.

------------

`[2406.00252] Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey <https://arxiv.org/abs/2406.00252>`__ 多模态和多智能体系统满足理性:综述

::

    replaced with revised version Wed, 5 Jun 2024 19:39:56 GMT
    Submission history From: Bowen Jiang [view email]
    [v1] Sat, 1 Jun 2024 01:17:25 UTC (2,332 KB)
    [v2] Wed, 5 Jun 2024 19:39:56 UTC (2,332 KB)
    Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Weijie J. Su, Camillo J. Taylor, Tanwi Mallick

Rationality is the quality of being guided by reason, characterized by logical thinking and decision-making that align with evidence and logical rules. This quality is essential for effective problem-solving, as it ensures that solutions are well-founded and systematically derived. Despite the advancements of large language models (LLMs) in generating human-like text with remarkable accuracy, they present biases inherited from the training data, inconsistency across different contexts, and difficulty understanding complex scenarios involving multiple layers of context. Therefore, recent research attempts to leverage the strength of multiple agents working collaboratively with various types of data and tools for enhanced consistency and reliability. To that end, this paper aims to understand whether multi-modal and multi-agent systems are advancing toward rationality by surveying the state-of-the-art works, identifying advancements over single-agent and single-modal systems in terms of rationality, and discussing open problems and future directions. We maintain an open repository at this https URL.

------------

`[2309.15402] Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future <https://arxiv.org/abs/2309.15402>`__ 通过谜一样的迷宫导航一个调查链的思维推理:进展，前沿和未来

::

    replaced with revised version Thu, 6 Jun 2024 01:58:54 GMT
    Submission history From: Zheng Chu [view email]
    [v1] Wed, 27 Sep 2023 04:53:10 UTC (667 KB)
    [v2] Mon, 16 Oct 2023 07:37:45 UTC (331 KB)
    [v3] Thu, 6 Jun 2024 01:58:54 UTC (501 KB)
    Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu

Reasoning, a fundamental cognitive process integral to human intelligence, has garnered substantial interest within artificial intelligence. Notably, recent studies have revealed that chain-of-thought prompting significantly enhances LLM's reasoning capabilities, which attracts widespread attention from both academics and industry. In this paper, we systematically investigate relevant research, summarizing advanced methods through a meticulous taxonomy that offers novel perspectives. Moreover, we delve into the current frontiers and delineate the challenges and future directions, thereby shedding light on future research. Furthermore, we engage in a discussion about open questions. We hope this paper serves as an introduction for beginners and fosters future research. Resources have been made publicly available at this https URL

------------

`[2404.00929] A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias <https://arxiv.org/abs/2404.00929>`__ 多语言大型语言模型综述:语料库、对齐和偏差

::

    replaced with revised version Thu, 6 Jun 2024 16:04:15 GMT
    Submission history From: Ling Hu [view email]
    [v1] Mon, 1 Apr 2024 05:13:56 UTC (3,034 KB)
    [v2] Thu, 6 Jun 2024 16:04:15 UTC (4,536 KB)
    Yuemei Xu, Ling Hu, Jiayi Zhao, Zihan Qiu, Yuqi Ye, Hanwen Gu

Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs including its category and evaluation metrics, and summarize the existing debiasing techniques. Finally, we discuss existing challenges and point out promising research directions. By demonstrating these aspects, this paper aims to facilitate a deeper understanding of MLLMs and their potentiality in various domains.

------------

`[2402.12451] The Revolution of Multimodal Large Language Models: A Survey <https://arxiv.org/abs/2402.12451>`__ 多模态大型语言模型革命综述

::

    replaced with revised version Thu, 6 Jun 2024 16:13:43 GMT
    Submission history From: Marcella Cornia [view email]
    [v1] Mon, 19 Feb 2024 19:01:01 UTC (154 KB)
    [v2] Thu, 6 Jun 2024 16:13:43 UTC (164 KB)
    Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara

Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in terms of performance and computational requirements. Overall, this survey offers a comprehensive overview of the current state of the art, laying the groundwork for future MLLMs.

------------

--------------
Benchmark (14)
--------------

`[2406.03699] M-QALM: A Benchmark to Assess Clinical Reading Comprehension and Knowledge Recall in Large Language Models via Question Answering <https://arxiv.org/abs/2406.03699>`__ M-QALM:基于问答的大型语言模型临床阅读理解和知识回忆评估基准

::

    Thu, 6 Jun 2024 02:43:21 GMT
    Anand Subramanian, Viktor Schlegel, Abhinav Ramesh Kashyap, Thanh-Tung Nguyen, Vijay Prakash Dwivedi, Stefan Winkler

There is vivid research on adapting Large Language Models (LLMs) to perform a variety of tasks in high-stakes domains such as healthcare. Despite their popularity, there is a lack of understanding of the extent and contributing factors that allow LLMs to recall relevant knowledge and combine it with presented information in the clinical and biomedical domain: a fundamental pre-requisite for success on down-stream tasks. Addressing this gap, we use Multiple Choice and Abstractive Question Answering to conduct a large-scale empirical study on 22 datasets in three generalist and three specialist biomedical sub-domains. Our multifaceted analysis of the performance of 15 LLMs, further broken down by sub-domain, source of knowledge and model architecture, uncovers success factors such as instruction tuning that lead to improved recall and comprehension. We further show that while recently proposed domain-adapted models may lack adequate knowledge, directly fine-tuning on our collected medical knowledge datasets shows encouraging results, even generalising to unseen specialist sub-domains. We complement the quantitative results with a skill-oriented manual error analysis, which reveals a significant gap between the models' capabilities to simply recall necessary knowledge and to integrate it with the presented context. To foster research and collaboration in this field we share M-QALM, our resources, standardised methodology, and evaluation results, with the research community to facilitate further advancements in clinical knowledge representation learning within language models.

------------

`[2406.03749] NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human <https://arxiv.org/abs/2406.03749>`__ NAP^2:向人类学习的自然和隐私保护文本重写基准

::

    Thu, 6 Jun 2024 05:07:44 GMT
    Shuo Huang, William MacLean, Xiaoxi Kang, Anqi Wu, Lizhen Qu, Qiongkai Xu, Zhuang Li, Xingliang Yuan, Gholamreza Haffari

Increasing concerns about privacy leakage issues in academia and industry arise when employing NLP models from third-party providers to process sensitive texts. To protect privacy before sending sensitive data to those models, we suggest sanitizing sensitive text using two common strategies used by humans: i) deleting sensitive expressions, and ii) obscuring sensitive details by abstracting them. To explore the issues and develop a tool for text rewriting, we curate the first corpus, coined NAP^2, through both crowdsourcing and the use of large language models (LLMs). Compared to the prior works based on differential privacy, which lead to a sharp drop in information utility and unnatural texts, the human-inspired approaches result in more natural rewrites and offer an improved balance between privacy protection and data utility, as demonstrated by our extensive experiments.

------------

`[2406.03855] Performance of large language models in numerical vs. semantic medical knowledge: Benchmarking on evidence-based Q&As <https://arxiv.org/abs/2406.03855>`__ 大型语言模型在数字与语义医学知识中的表现:基于证据的问答基准测试

::

    Thu, 6 Jun 2024 08:41:46 GMT
    Eden Avnat, Michal Levy, Daniel Herstain, Elia Yanko, Daniel Ben Joya, Michal Tzuchman Katz, Dafna Eshel, Sahar Laros, Yael Dagan, Shahar Barami, Joseph Mermelstein, Shahar Ovadia, Noam Shomron, Varda Shalev and Raja-Elie E. Abdulnour

Clinical problem-solving requires processing of semantic medical knowledge such as illness scripts and numerical medical knowledge of diagnostic tests for evidence-based decision-making. As large language models (LLMs) show promising results in many aspects of language-based clinical practice, their ability to generate non-language evidence-based answers to clinical questions is inherently limited by tokenization. Therefore, we evaluated LLMs' performance on two question types: numeric (correlating findings) and semantic (differentiating entities) while examining differences within and between LLMs in medical aspects and comparing their performance to humans. To generate straightforward multi-choice questions and answers (QAs) based on evidence-based medicine (EBM), we used a comprehensive medical knowledge graph (encompassed data from more than 50,00 peer-reviewed articles) and created the "EBMQA". EBMQA contains 105,000 QAs labeled with medical and non-medical topics and classified into numerical or semantic questions. We benchmarked this dataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and Claude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question types and according to sub-labeled topics. For validation, six medical experts were tested on 100 numerical EBMQA questions. We found that both LLMs excelled more in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical QAs. However, both LLMs showed inter and intra gaps in different medical aspects and remained inferior to humans. Thus, their medical advice should be addressed carefully.

------------

`[2406.04244] Benchmark Data Contamination of Large Language Models: A Survey <https://arxiv.org/abs/2406.04244>`__ 大型语言模型基准数据污染研究综述

::

    Thu, 6 Jun 2024 16:41:39 GMT
    Cheng Xu, Shuhao Guan, Derek Greene, M-Tahar Kechadi

The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.

------------

`[2406.04264] MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding <https://arxiv.org/abs/2406.04264>`__ MLVU:多任务长视频理解的综合基准

::

    Thu, 6 Jun 2024 17:09:32 GMT
    Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, Zheng Liu

The evaluation of Long Video Understanding (LVU) performance poses an important but challenging research problem. Despite previous efforts, the existing video understanding benchmarks are severely constrained by several issues, especially the insufficient lengths of videos, a lack of diversity in video types and evaluation tasks, and the inappropriateness for evaluating LVU performances. To address the above problems, we propose a new benchmark, called MLVU (Multi-task Long Video Understanding Benchmark), for the comprehensive and in-depth evaluation of LVU. MLVU presents the following critical values: 1) The substantial and flexible extension of video lengths, which enables the benchmark to evaluate LVU performance across a wide range of durations. 2) The inclusion of various video genres, e.g., movies, surveillance footage, egocentric videos, cartoons, game videos, etc., which reflects the models' LVU performances in different scenarios. 3) The development of diversified evaluation tasks, which enables a comprehensive examination of MLLMs' key abilities in long-video understanding. The empirical study with 20 latest MLLMs reveals significant room for improvement in today's technique, as all existing methods struggle with most of the evaluation tasks and exhibit severe performance degradation when handling longer videos. Additionally, it suggests that factors such as context length, image-understanding quality, and the choice of LLM backbone can play critical roles in future advancements. We anticipate that MLVU will advance the research of long video understanding by providing a comprehensive and in-depth analysis of MLLMs.

------------

`[2311.09048] GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models <https://arxiv.org/abs/2311.09048>`__ GRASP:评估多模态语言模型中语言基础和情境物理理解的新基准

::

    replaced with revised version Thu, 6 Jun 2024 09:35:53 GMT
    Submission history From: Serwan Jassim [view email]
    [v1] Wed, 15 Nov 2023 15:38:28 UTC (8,807 KB)
    [v2] Tue, 30 Jan 2024 20:21:00 UTC (14,458 KB)
    [v3] Thu, 6 Jun 2024 09:35:53 UTC (1,459 KB)
    Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, Elia Bruni

This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The first level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of "Intuitive Physics" principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models. Although they exhibit at least some grounding capabilities, particularly for colors and shapes, these capabilities depend heavily on the prompting strategy. At the same time, all models perform below or at the chance level of 50% in the Intuitive Physics tests, while human subjects are on average 80% correct. These identified limitations underline the importance of using benchmarks like GRASP to monitor the progress of future models in developing these competencies.

------------

`[2311.09562] TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction <https://arxiv.org/abs/2311.09562>`__ TextEE:事件抽取的基准、再评价、反思与未来挑战

::

    replaced with revised version Thu, 6 Jun 2024 04:24:16 GMT
    Submission history From: Kuan-Hao Huang [view email]
    [v1] Thu, 16 Nov 2023 04:43:03 UTC (143 KB)
    [v2] Fri, 16 Feb 2024 19:57:42 UTC (7,008 KB)
    [v3] Thu, 6 Jun 2024 04:24:16 UTC (7,012 KB)
    Kuan-Hao Huang, I-Hung Hsu, Tanmay Parekh, Zhiyu Xie, Zixuan Zhang, Premkumar Natarajan, Kai-Wei Chang, Nanyun Peng, Heng Ji

Event extraction has gained considerable interest due to its wide-ranging applications. However, recent studies draw attention to evaluation issues, suggesting that reported scores may not accurately reflect the true performance. In this work, we identify and address evaluation challenges, including inconsistency due to varying data assumptions or preprocessing steps, the insufficiency of current evaluation frameworks that may introduce dataset or data split bias, and the low reproducibility of some previous approaches. To address these challenges, we present TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE comprises standardized data preprocessing scripts and splits for 16 datasets spanning eight diverse domains and includes 14 recent methodologies, conducting a comprehensive benchmark reevaluation. We also evaluate five varied large language models on our TextEE benchmark and demonstrate how they struggle to achieve satisfactory performance. Inspired by our reevaluation results and findings, we discuss the role of event extraction in the current NLP era, as well as future challenges and insights derived from TextEE. We believe TextEE, the first standardized comprehensive benchmarking tool, will significantly facilitate future event extraction research.

------------

`[2401.10186] Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation <https://arxiv.org/abs/2401.10186>`__ 

::

    replaced with revised version Thu, 6 Jun 2024 12:29:44 GMT
    Submission history From: Zdeněk Kasner [view email]
    [v1] Thu, 18 Jan 2024 18:15:46 UTC (887 KB)
    [v2] Mon, 19 Feb 2024 14:35:33 UTC (841 KB)
    [v3] Thu, 6 Jun 2024 12:29:44 UTC (950 KB)
    Zden\v{e}k Kasner, Ond\v{r}ej Du\v{s}ek

We analyze the behaviors of open large language models (LLMs) on the task of data-to-text (D2T) generation, i.e., generating coherent and relevant text from structured data. To avoid the issue of LLM training data contamination with standard benchmarks, we design Quintd - a tool for collecting novel structured data records from public APIs. We find that open LLMs (Llama 2, Mistral, and Zephyr) can generate fluent and coherent texts in zero-shot settings from data in common formats collected with Quintd. However, we show that the semantic accuracy of the outputs is a major issue: both according to human annotators and our reference-free metric based on GPT-4, more than 80% of the outputs of open LLMs contain at least one semantic error. We publicly release the code, data, and model outputs.

------------

`[2402.04788] MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark <https://arxiv.org/abs/2402.04788>`__ MLLM-as-a-Judge:用视觉-语言基准评估多模态llm -as- judge

::

    replaced with revised version Thu, 6 Jun 2024 13:38:13 GMT
    Submission history From: Dongping Chen [view email]
    [v1] Wed, 7 Feb 2024 12:28:32 UTC (3,194 KB)
    [v2] Thu, 6 Jun 2024 13:38:13 UTC (3,443 KB)
    Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun

Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence of multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparison, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a closer examination reveals persistent challenges in the judgment capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: \url{this https URL}.

------------

`[2402.14008] OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems <https://arxiv.org/abs/2402.14008>`__ OlympiadBench:用奥林匹亚水平双语多模态科学问题促进AGI的挑战性基准

::

    replaced with revised version Thu, 6 Jun 2024 13:19:44 GMT
    Submission history From: Chaoqun He [view email]
    [v1] Wed, 21 Feb 2024 18:49:26 UTC (8,221 KB)
    [v2] Thu, 6 Jun 2024 13:19:44 UTC (8,589 KB)
    Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun

Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97% on OlympiadBench, with a mere 10.74% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at \url{this https URL}

------------

`[2402.14116] FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models <https://arxiv.org/abs/2402.14116>`__ FanOutQA:面向大型语言模型的多跳多文档问答基准

::

    replaced with revised version Thu, 6 Jun 2024 16:41:21 GMT
    Submission history From: Andrew Zhu [view email]
    [v1] Wed, 21 Feb 2024 20:30:45 UTC (8,421 KB)
    [v2] Thu, 6 Jun 2024 16:41:21 UTC (8,423 KB)
    Andrew Zhu and Alyssa Hwang and Liam Dugan and Chris Callison-Burch

One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at this https URL

------------

`[2404.12464] NormAd: A Benchmark for Measuring the Cultural Adaptability of Large Language Models <https://arxiv.org/abs/2404.12464>`__ NormAd:大型语言模型文化适应性度量基准

::

    replaced with revised version Thu, 6 Jun 2024 16:02:39 GMT
    Submission history From: Abhinav Rao [view email]
    [v1] Thu, 18 Apr 2024 18:48:50 UTC (8,921 KB)
    [v2] Thu, 23 May 2024 17:49:51 UTC (8,930 KB)
    [v3] Mon, 27 May 2024 00:06:31 UTC (8,930 KB)
    [v4] Thu, 6 Jun 2024 16:02:39 UTC (8,932 KB)
    Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, Maarten Sap

The integration of Large Language Models (LLMs) into various global cultures fundamentally presents a cultural challenge: LLMs must navigate interactions, respect social norms, and avoid transgressing cultural boundaries. However, it is still unclear if LLMs can adapt their outputs to diverse cultural norms. Our study focuses on this aspect. We introduce NormAd, a novel dataset, which includes 2.6k stories that represent social and cultural norms from 75 countries, to assess the ability of LLMs to adapt to different granular levels of socio-cultural contexts such as the country of origin, its associated cultural values, and prevalent social norms. Our study reveals that LLMs struggle with cultural reasoning across all contextual granularities, showing stronger adaptability to English-centric cultures over those from the Global South. Even with explicit social norms, the top-performing model, Mistral-7b-Instruct, achieves only 81.8\% accuracy, lagging behind the 95.6\% achieved by humans. Evaluation on NormAd further reveals that LLMs struggle to adapt to stories involving gift-giving across cultures. Due to inherent agreement or sycophancy biases, LLMs find it considerably easier to assess the social acceptability of stories that adhere to cultural norms than those that deviate from them. Our benchmark measures the cultural adaptability (or lack thereof) of LLMs, emphasizing the potential to make these technologies more equitable and useful for global audiences. We release the NormAd dataset and its associated code on GitHub.

------------

`[2404.16966] Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks <https://arxiv.org/abs/2404.16966>`__ 检查LLM评估对基准分布假设的鲁棒性

::

    replaced with revised version Wed, 5 Jun 2024 20:14:15 GMT
    Submission history From: Charlotte Siska [view email]
    [v1] Thu, 25 Apr 2024 18:35:54 UTC (10,192 KB)
    [v2] Wed, 5 Jun 2024 20:14:15 UTC (10,228 KB)
    Melissa Ailem and Katerina Marazopoulou and Charlotte Siska and James Bono

Benchmarks have emerged as the central approach for evaluating Large Language Models (LLMs). The research community often relies on a model's average performance across the test prompts of a benchmark to evaluate the model's performance. This is consistent with the assumption that the test prompts within a benchmark represent a random sample from a real-world distribution of interest. We note that this is generally not the case; instead, we hold that the distribution of interest varies according to the specific use case. We find that (1) the correlation in model performance across test prompts is non-random, (2) accounting for correlations across test prompts can change model rankings on major benchmarks, (3) explanatory factors for these correlations include semantic similarity and common LLM failure points.

------------

`[2402.07844] Mercury: A Code Efficiency Benchmark for LLM Code Synthesis <https://arxiv.org/abs/2402.07844>`__ Mercury: LLM代码合成的代码效率基准

::

    replaced with revised version Thu, 6 Jun 2024 09:42:17 GMT
    Submission history From: Mingzhe Du [view email]
    [v1] Mon, 12 Feb 2024 17:53:22 UTC (2,757 KB)
    [v2] Sat, 11 May 2024 06:21:01 UTC (3,629 KB)
    [v3] Thu, 6 Jun 2024 09:42:17 UTC (11,936 KB)
    Mingzhe Du, Anh Tuan Luu, Bin Ji, Qian Liu, See-Kiong Ng

Amidst the recent strides in evaluating Large Language Models for Code (Code LLMs), existing benchmarks have mainly focused on the functional correctness of generated code, neglecting the importance of their computational efficiency. To fill the gap, we present Mercury, the first code efficiency benchmark for Code LLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutions that serve as real-world efficiency baselines, enabling a comprehensive analysis of the runtime distribution. Based on the distribution, we introduce a new metric Beyond, which computes a runtime-percentile-weighted Pass score to reflect functional correctness and code efficiency simultaneously. On Mercury, leading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Given that an ideal Beyond score would be aligned with the Pass score, it indicates that while Code LLMs exhibit impressive capabilities in generating functionally correct code, there remains a notable gap in their efficiency. Finally, our empirical experiments reveal that Direct Preference Optimization (DPO) serves as a robust baseline for enhancing code efficiency compared with Supervised Fine Tuning (SFT), which paves a promising avenue for future exploration of efficient code generation. Our code and data are available on GitHub: this https URL.

------------

---------------
Accelerate (10)
---------------

`[2406.03746] Efficient Knowledge Infusion via KG-LLM Alignment <https://arxiv.org/abs/2406.03746>`__ 基于KG-LLM对齐的高效知识注入

::

    Thu, 6 Jun 2024 04:55:55 GMT
    Zhouyu Jiang, Ling Zhong, Mengshu Sun, Jun Xu, Rui Sun, Hui Cai, Shuhan Luo, Zhiqiang Zhang

To tackle the problem of domain-specific knowledge scarcity within large language models (LLMs), knowledge graph-retrievalaugmented method has been proven to be an effective and efficient technique for knowledge infusion.
However, existing approaches face two primary challenges: knowledge mismatch between public available knowledge graphs and the specific domain of the task at hand, and poor information compliance of LLMs with knowledge graphs. In this paper, we leverage a small set of labeled samples and a large-scale corpus to efficiently construct domain-specific knowledge graphs by an LLM, addressing the issue of knowledge mismatch. Additionally, we propose a three-stage KG-LLM alignment strategyto enhance the LLM's capability to utilize information from knowledge graphs. We conduct experiments with a limited-sample setting on two biomedical question-answering datasets, and the results demonstrate that our approach outperforms existing baselines.

------------

`[2406.03792] Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning <https://arxiv.org/abs/2406.03792>`__ 

::

    Thu, 6 Jun 2024 07:03:29 GMT
    Naibin Gu, Peng Fu, Xiyu Liu, Bowen Shen, Zheng Lin, Weiping Wang

Parameter-efficient fine-tuning (PEFT) has emerged as the predominant technique for fine-tuning in the era of large language models. However, existing PEFT methods still have inadequate training efficiency. Firstly, the utilization of large-scale foundation models during the training process is excessively redundant for certain fine-tuning tasks. Secondly, as the model size increases, the growth in trainable parameters of empirically added PEFT modules becomes non-negligible and redundant, leading to inefficiency. To achieve task-specific efficient fine-tuning, we propose the Light-PEFT framework, which includes two methods: Masked Early Pruning of the Foundation Model and Multi-Granularity Early Pruning of PEFT. The Light-PEFT framework allows for the simultaneous estimation of redundant parameters in both the foundation model and PEFT modules during the early stage of training. These parameters can then be pruned for more efficient fine-tuning. We validate our approach on GLUE, SuperGLUE, QA tasks, and various models. With Light-PEFT, parameters of the foundation model can be pruned by up to over 40%, while still controlling trainable parameters to be only 25% of the original PEFT method.
Compared to utilizing the PEFT method directly, Light-PEFT achieves training and inference speedup, reduces memory usage, and maintains comparable performance and the plug-and-play feature of PEFT.

------------

`[2406.03853] Speculative Decoding via Early-exiting for Faster LLM Inference with Thompson Sampling Control Mechanism <https://arxiv.org/abs/2406.03853>`__ 基于Thompson采样控制机制的早期退出推测解码以实现更快的LLM推理

::

    Thu, 6 Jun 2024 08:40:28 GMT
    Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai

The recent advancements in large language models (LLMs) have been extraordinary, yet the escalating inference costs associated with them present challenges in real-world applications. To address these challenges, we propose a novel approach called Early-exiting Speculative Decoding (EESD) with lossless acceleration. Specifically, EESD utilizes a segment of the LLM to generate draft tokens, incorporating Early-exiting structures after the first N layers.
To enhance the quality of draft tokens, a self-distillation method is integrated. This early-exiting design not only reduces deployment and training costs but also significantly accelerates the token generation speed. Moreover, we introduce a novel sampling mechanism that leverages Thompson Sampling to regulate the generation processes, automatically determining the quantity of draft tokens in each round. The original LLM is then employed to validate these draft tokens through a single forward pass, and thus guarantees that the final output text maintains a distribution consistent with vanilla auto-regressive decoding. The experimental results on both 13B and 70B models demonstrate that our approach decodes tokens at a markedly accelerated rate compared to prior methods, showing the effectiveness of our approach.

------------

`[2406.04218] Rethinking LLM and Linguistic Steganalysis: An Efficient Detection of Strongly Concealed Stego <https://arxiv.org/abs/2406.04218>`__ 重新思考LLM和语言隐写分析:高效检测强隐蔽性隐写

::

    Thu, 6 Jun 2024 16:18:02 GMT
    Yifan Tang and Yihao Wang and Ru Zhang and Jianyi Liu

To detect stego (steganographic text) in complex scenarios, linguistic steganalysis (LS) with various motivations has been proposed and achieved excellent performance. However, with the development of generative steganography, some stegos have strong concealment, especially after the emergence of LLMs-based steganography, the existing LS has low detection or even cannot detect them. We designed a novel LS with two modes called LSGC. In the generation mode, we created an LS-task "description" and used the generation ability of LLM to explain whether texts to be detected are stegos.
On this basis, we rethought the principle of LS and LLMs, and proposed the classification mode. In this mode, LSGC deleted the LS-task "description" and changed the "causalLM" LLMs to the "sequenceClassification" architecture. The LS features can be extracted by only one pass of the model, and a linear layer with initialization weights is added to obtain the classification probability.
Experiments on strongly concealed stegos show that LSGC significantly improves detection and reaches SOTA performance. Additionally, LSGC in classification mode greatly reduces training time while maintaining high performance.

------------

`[2312.07104] SGLang: Efficient Execution of Structured Language Model Programs <https://arxiv.org/abs/2312.07104>`__ 

::

    replaced with revised version Thu, 6 Jun 2024 00:10:06 GMT
    Submission history From: Ying Sheng [view email]
    [v1] Tue, 12 Dec 2023 09:34:27 UTC (380 KB)
    [v2] Thu, 6 Jun 2024 00:10:06 UTC (804 KB)
    Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, Ying Sheng

Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at this https URL

------------

`[2401.08295] SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models <https://arxiv.org/abs/2401.08295>`__ SAPT:大型语言模型参数高效持续学习的共享注意力框架

::

    replaced with revised version Thu, 6 Jun 2024 12:02:51 GMT
    Submission history From: Weixiang Zhao [view email]
    [v1] Tue, 16 Jan 2024 11:45:03 UTC (1,649 KB)
    [v2] Fri, 16 Feb 2024 10:16:52 UTC (2,099 KB)
    [v3] Thu, 6 Jun 2024 12:02:51 UTC (2,102 KB)
    Weixiang Zhao, Shilong Wang, Yulin Hu, Yanyan Zhao, Bing Qin, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che

The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two modules to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning \& Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model sizes (from 770M to 13B), different model architectures (T5 and LLaMA-2) and unseen tasks.

------------

`[2403.01165] STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models <https://arxiv.org/abs/2403.01165>`__ STAR:基于动态主动学习的LoRA约束，用于大型语言模型的数据高效微调

::

    replaced with revised version Thu, 6 Jun 2024 07:31:39 GMT
    Submission history From: Jialong Wu [view email]
    [v1] Sat, 2 Mar 2024 10:38:10 UTC (7,103 KB)
    [v2] Thu, 6 Jun 2024 07:31:39 UTC (7,160 KB)
    Linhai Zhang, Jialong Wu, Deyu Zhou, Guoqiang Xu

Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the uncertainty gap, we introduce a dynamic uncertainty measurement that combines the uncertainty of the base model and the uncertainty of the full model during the iteration of active learning. For poor model calibration, we incorporate the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation. Experimental results show that the proposed approach outperforms existing baseline models on three complex reasoning tasks.

------------

`[2406.01392] Sparsity-Accelerated Training for Large Language Models <https://arxiv.org/abs/2406.01392>`__ 大型语言模型的稀疏加速训练

::

    replaced with revised version Thu, 6 Jun 2024 16:38:34 GMT
    Submission history From: Da Ma [view email]
    [v1] Mon, 3 Jun 2024 14:56:09 UTC (1,374 KB)
    [v2] Thu, 6 Jun 2024 16:38:34 UTC (1,374 KB)
    Da Ma and Lu Chen and Pengyu Wang and Hongshen Xu and Hanqi Li and Liangtai Sun and Su Zhu and Shuai Fan and Kai Yu

Large language models (LLMs) have demonstrated proficiency across various natural language processing (NLP) tasks but often require additional training, such as continual pre-training and supervised fine-tuning. However, the costs associated with this, primarily due to their large parameter count, remain high. This paper proposes leveraging \emph{sparsity} in pre-trained LLMs to expedite this training process. By observing sparsity in activated neurons during forward iterations, we identify the potential for computational speed-ups by excluding inactive neurons. We address associated challenges by extending existing neuron importance evaluation metrics and introducing a ladder omission rate scheduler. Our experiments on Llama-2 demonstrate that Sparsity-Accelerated Training (SAT) achieves comparable or superior performance to standard training while significantly accelerating the process. Specifically, SAT achieves a $45\%$ throughput improvement in continual pre-training and saves $38\%$ training time in supervised fine-tuning in practice. It offers a simple, hardware-agnostic, and easily deployable framework for additional LLM training. Our code is available at this https URL.

------------

`[2401.10774] Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads <https://arxiv.org/abs/2401.10774>`__ Medusa:具有多个解码头的简单LLM推理加速框架

::

    replaced with revised version Wed, 5 Jun 2024 22:47:53 GMT
    Submission history From: Tianle Cai [view email]
    [v1] Fri, 19 Jan 2024 15:48:40 UTC (1,632 KB)
    [v2] Wed, 5 Jun 2024 22:47:53 UTC (3,113 KB)
    Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao

Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa substantially reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities.
Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.

------------

`[2403.09347] BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences <https://arxiv.org/abs/2403.09347>`__ burstatattention:一种高效的极长序列分布式注意力框架

::

    replaced with revised version Thu, 6 Jun 2024 05:43:52 GMT
    Submission history From: Ao Sun [view email]
    [v1] Thu, 14 Mar 2024 12:51:58 UTC (4,076 KB)
    [v2] Wed, 17 Apr 2024 10:07:14 UTC (4,076 KB)
    [v3] Thu, 16 May 2024 05:08:13 UTC (4,076 KB)
    [v4] Thu, 6 Jun 2024 05:43:52 UTC (4,072 KB)
    Ao Sun, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong Sun

Effective attention modules have played a crucial role in the success of Transformer-based large language models (LLMs), but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences. One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs). However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones. In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels. In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence processing. The experimental results under different length settings demonstrate that BurstAttention offers significant advantages for processing long sequences compared with these competitive baselines, reducing 40% communication overheads and achieving 1.37 X speedup during training 128K sequence length on 32 X A100.

------------

-----------------------
In-Context Learning (4)
-----------------------

`[2406.03730] FastGAS: Fast Graph-based Annotation Selection for In-Context Learning <https://arxiv.org/abs/2406.03730>`__ FastGAS:面向上下文学习的快速图标注选择

::

    Thu, 6 Jun 2024 04:05:54 GMT
    Zihan Chen, Song Wang, Cong Shen, Jundong Li

In-context learning (ICL) empowers large language models (LLMs) to tackle new tasks by using a series of training instances as prompts. Since generating the prompts needs to sample from a vast pool of instances and annotate them (e.g., add labels in classification task), existing methods have proposed to select a subset of unlabeled examples for annotation, thus enhancing the quality of prompts and concurrently mitigating annotation costs. However, these methods often require a long time to select instances due to their complexity, hindering their practical viability. To address this limitation, we propose a graph-based selection method, FastGAS, designed to efficiently identify high-quality instances while minimizing computational overhead. Initially, we construct a data similarity graph based on instance similarities. Subsequently, employing a graph partitioning algorithm, we partition the graph into pieces.
Within each piece (i.e., subgraph), we adopt a greedy approach to pick the most representative nodes. By aggregating nodes from diverse pieces and annotating the corresponding instances, we identify a set of diverse and representative instances for ICL. Compared to prior approaches, our method not only exhibits superior performance on different tasks but also significantly reduces selection time. In addition, we demonstrate the efficacy of our approach in LLMs of larger sizes.

------------

`[2406.03768] Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective <https://arxiv.org/abs/2406.03768>`__ 仅基于svd的权重剪枝提高上下文学习性能:理论视角

::

    Thu, 6 Jun 2024 06:15:35 GMT
    Xinhao Yao, Xiaolin Hu, Shenzhi Yang, Yong Liu

Pre-trained large language models (LLMs) based on Transformer have demonstrated striking in-context learning (ICL) abilities. With a few demonstration input-label pairs, they can predict the label for an unseen input without any parameter updates. In this paper, we show an exciting phenomenon that SVD-based weight pruning can enhance ICL performance, and more surprising, pruning weights in deep layers often results in more stable performance improvements in shallow layers. However, the underlying mechanism of those findings still remains an open question. To reveal those findings, we conduct an in-depth theoretical analysis by presenting the implicit gradient descent (GD) trajectories of ICL and giving the mutual information based generalization bounds of ICL via full implicit GD trajectories. This helps us reasonably explain the surprising experimental findings. Besides, based on all our experimental and theoretical insights, we intuitively propose a simple, model-compression and derivative-free algorithm for downstream tasks in enhancing ICL inference. Experiments on benchmark datasets and open source LLMs display the method effectiveness\footnote{The code is available at \url{https://github.com/chen123CtrlS/EnhancingICL_SVDPruning}}.

------------

`[2402.13874] $Se^2$: Sequential Example Selection for In-Context Learning <https://arxiv.org/abs/2402.13874>`__ $Se^2$:上下文学习的顺序示例选择

::

    replaced with revised version Thu, 6 Jun 2024 11:12:57 GMT
    Submission history From: Haoyu Liu [view email]
    [v1] Wed, 21 Feb 2024 15:35:04 UTC (3,520 KB)
    [v2] Wed, 6 Mar 2024 14:46:30 UTC (3,553 KB)
    [v3] Thu, 6 Jun 2024 11:12:57 UTC (5,715 KB)
    Haoyu Liu, Jianfeng Liu, Shaohan Huang, Yuefeng Zhan, Hao Sun, Weiwei Deng, Furu Wei, Qi Zhang

The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the "select then organize" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $Se$quential $Se$lection problem and introduce $Se^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $Se^2$ markedly surpasses competitive baselines and achieves 42\% relative improvement over random selection. Further in-depth analysis shows the effectiveness of proposed strategies, highlighting $Se^2$'s exceptional stability and adaptability across various scenarios. Code available at this https URL.

------------

`[2406.02847] Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers <https://arxiv.org/abs/2406.02847>`__ 线性注意力transformer中上下文学习到模型权重的精确转换

::

    replaced with revised version Thu, 6 Jun 2024 06:15:29 GMT
    Submission history From: Tianyang Hu [view email]
    [v1] Wed, 5 Jun 2024 01:47:40 UTC (120 KB)
    [v2] Thu, 6 Jun 2024 06:15:29 UTC (120 KB)
    Brian K Chen, Tianyang Hu, Hui Jin, Hwee Kuan Lee, Kenji Kawaguchi

In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years. In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates. In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner. Existing methods are not exact and require expensive parameter updates. We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer. We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized. Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms.

------------

--------------
Reasoning (12)
--------------

`[2406.03618] TACT: Advancing Complex Aggregative Reasoning with Information Extraction Tools <https://arxiv.org/abs/2406.03618>`__ TACT:利用信息抽取工具推进复杂综合推理

::

    Wed, 5 Jun 2024 20:32:56 GMT
    Avi Caciularu, Alon Jacovi, Eyal Ben-David, Sasha Goldshtein, Tal Schuster, Jonathan Herzig, Gal Elidan, Amir Globerson

Large Language Models (LLMs) often do not perform well on queries that require the aggregation of information across texts. To better evaluate this setting and facilitate modeling efforts, we introduce TACT - Text And Calculations through Tables, a dataset crafted to evaluate LLMs' reasoning and computational abilities using complex instructions. TACT contains challenging instructions that demand stitching information scattered across one or more texts, and performing complex integration on this information to generate the answer. We construct this dataset by leveraging an existing dataset of texts and their associated tables. For each such tables, we formulate new queries, and gather their respective answers. We demonstrate that all contemporary LLMs perform poorly on this dataset, achieving an accuracy below 38\%. To pinpoint the difficulties and thoroughly dissect the problem, we analyze model performance across three components: table-generation, Pandas command-generation, and execution. Unexpectedly, we discover that each component presents substantial challenges for current LLMs. These insights lead us to propose a focused modeling framework, which we refer to as IE as a tool.
Specifically, we propose to add "tools" for each of the above steps, and implement each such tool with few-shot prompting. This approach shows an improvement over existing prompting techniques, offering a promising direction for enhancing model capabilities in these tasks.

------------

`[2406.04197] DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase for Math Reasoning <https://arxiv.org/abs/2406.04197>`__ DICE:在LLM的数学推理微调阶段检测分布内污染

::

    Thu, 6 Jun 2024 15:55:53 GMT
    Shangqing Tu, Kejian Zhu, Yushi Bai, Zijun Yao, Lei Hou, Juanzi Li

The advancement of large language models (LLMs) relies on evaluation using public benchmarks, but data contamination can lead to overestimated performance. Previous researches focus on detecting contamination by determining whether the model has seen the exact same data during training. In this work, we argue that even training on data similar to benchmark data inflates performance on in-distribution tasks without improving overall capacity, which we called In-distribution contamination. To effectively detect in-distribution contamination, we propose DICE, a novel method that leverages the internal states of LLMs to locate-then-detect the contamination. DICE first identifies the most sensitive layer to contamination, then trains a classifier based on the internal states of that layer. Experiments reveal DICE's high accuracy in detecting in-distribution contamination across various LLMs and math reasoning datasets. We also show the generalization capability of the trained DICE detector, which is able to detect contamination across multiple benchmarks with similar distributions. Additionally, we find that the DICE detection scores are positively correlated with the performance of ten LLMs fine-tuned by either us or other organizations on four math reasoning datasets (with $R^2$ values between 0.6 and 0.75). This indicates that the in-distribution contamination problem potentially lead to an overestimation of the true capabilities of many existing models. The code and data are available at https://github.com/THU-KEG/DICE.

------------

`[2406.04271] Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models <https://arxiv.org/abs/2406.04271>`__ 

::

    Thu, 6 Jun 2024 17:22:08 GMT
    Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E. Gonzalez, Bin Cui

We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs). Specifically, we propose meta-buffer to store a series of informative high-level thoughts, namely thought-template, distilled from the problem-solving processes across various tasks. Then for each problem, we retrieve a relevant thought-template and adaptively instantiate it with specific reasoning structures to conduct efficient reasoning. To guarantee the scalability and stability, we further propose buffer-manager to dynamically update the meta-buffer, thus enhancing the capacity of meta-buffer as more tasks are solved. We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11% on Game of 24, 20% on Geometric Shapes and 51% on Checkmate-in-One. Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only 12% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average. Notably, we find that our Llama3-8B+BoT has the potential to surpass Llama3-70B model. Our project is available at: https://github.com/YangLing0818/buffer-of-thought-llm

------------

`[2406.03843] POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models <https://arxiv.org/abs/2406.03843>`__ POEM:增强大型语言模型多模态推理的交互式提示优化

::

    Thu, 6 Jun 2024 08:21:30 GMT
    Jianben He, Xingbo Wang, Shiyi Liu, Guande Wu, Claudio Silva, and Huamin Qu

Large language models (LLMs) have exhibited impressive abilities for multimodal content comprehension and reasoning with proper prompting in zero- or few-shot settings. Despite the proliferation of interactive systems developed to support prompt engineering for LLMs across various tasks, most have primarily focused on textual or visual inputs, thus neglecting the complex interplay between modalities within multimodal inputs. This oversight hinders the development of effective prompts that guide model multimodal reasoning processes by fully exploiting the rich context provided by multiple modalities.
In this paper, we present POEM, a visual analytics system to facilitate efficient prompt engineering for enhancing the multimodal reasoning performance of LLMs. The system enables users to explore the interaction patterns across modalities at varying levels of detail for a comprehensive understanding of the multimodal knowledge elicited by various prompts. Through diverse recommendations of demonstration examples and instructional principles, POEM supports users in iteratively crafting and refining prompts to better align and enhance model knowledge with human insights. The effectiveness and efficiency of our system are validated through two case studies and interviews with experts.

------------

`[2406.04046] ActionReasoningBench: Reasoning about Actions with and without Ramification Constraints <https://arxiv.org/abs/2406.04046>`__ 

::

    Thu, 6 Jun 2024 13:15:37 GMT
    Divij Handa, Pavel Dolin, Shrinidhi Kumbhar, Chitta Baral, Tran Cao Son

Reasoning about actions and change (RAC) has historically driven the development of many early AI challenges, such as the frame problem, and many AI disciplines, including non-monotonic and commonsense reasoning. The role of RAC remains important even now, particularly for tasks involving dynamic environments, interactive scenarios, and commonsense reasoning. Despite the progress of Large Language Models (LLMs) in various AI domains, their performance on RAC is underexplored. To address this gap, we introduce a new benchmark, ActionReasoningBench, encompassing 13 domains and rigorously evaluating LLMs across eight different areas of RAC. These include - Object Tracking, Fluent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical RAC, Hallucination Detection, and Composite Questions.
Furthermore, we also investigate the indirect effect of actions due to ramification constraints for every domain. Finally, we evaluate our benchmark using open-sourced and commercial state-of-the-art LLMs, including GPT-4o, Gemini-1.0-Pro, Llama2-7b-chat, Llama2-13b-chat, Llama3-8b-instruct, Gemma-2b-instruct, and Gemma-7b-instruct. Our findings indicate that these models face significant challenges across all categories included in our benchmark.

------------

`[2309.15402] Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future <https://arxiv.org/abs/2309.15402>`__ 通过谜一样的迷宫导航一个调查链的思维推理:进展，前沿和未来

::

    replaced with revised version Thu, 6 Jun 2024 01:58:54 GMT
    Submission history From: Zheng Chu [view email]
    [v1] Wed, 27 Sep 2023 04:53:10 UTC (667 KB)
    [v2] Mon, 16 Oct 2023 07:37:45 UTC (331 KB)
    [v3] Thu, 6 Jun 2024 01:58:54 UTC (501 KB)
    Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu

Reasoning, a fundamental cognitive process integral to human intelligence, has garnered substantial interest within artificial intelligence. Notably, recent studies have revealed that chain-of-thought prompting significantly enhances LLM's reasoning capabilities, which attracts widespread attention from both academics and industry. In this paper, we systematically investigate relevant research, summarizing advanced methods through a meticulous taxonomy that offers novel perspectives. Moreover, we delve into the current frontiers and delineate the challenges and future directions, thereby shedding light on future research. Furthermore, we engage in a discussion about open questions. We hope this paper serves as an introduction for beginners and fosters future research. Resources have been made publicly available at this https URL

------------

`[2310.03309] Concise and Organized Perception Facilitates Reasoning in Large Language Models <https://arxiv.org/abs/2310.03309>`__ 

::

    replaced with revised version Thu, 6 Jun 2024 06:28:02 GMT
    Submission history From: Shaotian Yan [view email]
    [v1] Thu, 5 Oct 2023 04:47:49 UTC (840 KB)
    [v2] Fri, 1 Mar 2024 03:47:50 UTC (840 KB)
    [v3] Thu, 6 Jun 2024 06:28:02 UTC (710 KB)
    Junjie Liu, Shaotian Yan, Chen Shen, Liang Xie, Wenxiao Wang and Jieping Ye

Exploiting large language models (LLMs) to tackle reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex logical problems, characterized by plenty of premises within the prompt and requiring multi-hop reasoning. In particular, the reasoning capabilities of LLMs are brittle to disorder and distractibility. In this work, we first examine the mechanism from the perspective of information flow and reveal that LLMs exhibit failure patterns akin to human-like cognitive biases when dealing with disordered and irrelevant content in reasoning tasks. However, in contrast to LLMs, disordered and irrelevant content does not significantly decrease human performance, as humans have a propensity to distill the most relevant information and systematically organize their thoughts, aiding them in responding to questions. Stem from that, we further propose a novel reasoning approach named Concise and Organized Perception (COP). COP carefully analyzes the given statements to identify the most pertinent information while eliminating redundancy efficiently. It then prompts the LLMs in a more organized form that adapts to the model's inference process. By perceiving concise and organized context, the reasoning abilities of LLMs can be better elicited. Extensive experimental results on several popular logical benchmarks (ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and math benchmark (DI-GSM) show that COP significantly outperforms previous state-of-the-art methods.

------------

`[2402.14328] Understanding and Patching Compositional Reasoning in LLMs <https://arxiv.org/abs/2402.14328>`__ 理解和修补llm中的组合推理

::

    replaced with revised version Thu, 6 Jun 2024 14:06:41 GMT
    Submission history From: Zhaoyi Li [view email]
    [v1] Thu, 22 Feb 2024 06:47:56 UTC (8,893 KB)
    [v2] Thu, 6 Jun 2024 14:06:41 UTC (9,058 KB)
    Zhaoyi Li, Gangwei Jiang, Hong Xie, Linqi Song, Defu Lian, Ying Wei

LLMs have marked a revolutonary shift, yet they falter when faced with compositional reasoning tasks. Our research embarks on a quest to uncover the root causes of compositional reasoning failures of LLMs, uncovering that most of them stem from the improperly generated or leveraged implicit reasoning results. Inspired by our empirical findings, we resort to Logit Lens and an intervention experiment to dissect the inner hidden states of LLMs. This deep dive reveals that implicit reasoning results indeed surface within middle layers and play a causative role in shaping the final explicit reasoning results. Our exploration further locates multi-head self-attention (MHSA) modules within these layers, which emerge as the linchpins in accurate generation and leveraing of implicit reasoning results. Grounded on the above findings, we develop CREME, a lightweight method to patch errors in compositional reasoning via editing the located MHSA modules. Our empirical evidence stands testament to CREME's effectiveness, paving the way for autonomously and continuously enhancing compositional reasoning capabilities in language models.

------------

`[2404.15522] LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models <https://arxiv.org/abs/2404.15522>`__ LogicBench:大型语言模型逻辑推理能力的系统评估

::

    replaced with revised version Thu, 6 Jun 2024 08:15:54 GMT
    Submission history From: Mihir Parmar [view email]
    [v1] Tue, 23 Apr 2024 21:08:49 UTC (1,908 KB)
    [v2] Thu, 6 Jun 2024 08:15:54 UTC (2,176 KB)
    Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral

Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really "reason" over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs. Data and code are available at this https URL.

------------

`[2404.17140] Small Language Models Need Strong Verifiers to Self-Correct Reasoning <https://arxiv.org/abs/2404.17140>`__ 小型语言模型需要强大的验证器来进行自纠正推理

::

    replaced with revised version Thu, 6 Jun 2024 03:59:24 GMT
    Submission history From: Yunxiang Zhang [view email]
    [v1] Fri, 26 Apr 2024 03:41:28 UTC (440 KB)
    [v2] Thu, 6 Jun 2024 03:59:24 UTC (1,015 KB)
    Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, Lu Wang

Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether small (<= 13B) language models (LMs) have the ability of self-correction on reasoning tasks with minimal inputs from stronger LMs. We propose a novel pipeline that prompts smaller LMs to collect self-correction data that supports the training of self-refinement abilities. First, we leverage correct solutions to guide the model in critiquing their incorrect responses. Second, the generated critiques, after filtering, are used for supervised fine-tuning of the self-correcting reasoner through solution refinement. Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.

------------

`[2402.12424] Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs <https://arxiv.org/abs/2402.12424>`__ 表格作为文本或图像:评估llm和mllm的表格推理能力

::

    replaced with revised version Thu, 6 Jun 2024 03:02:25 GMT
    Submission history From: Naihao Deng [view email]
    [v1] Mon, 19 Feb 2024 16:34:50 UTC (6,911 KB)
    [v2] Thu, 22 Feb 2024 15:34:50 UTC (6,911 KB)
    [v3] Fri, 23 Feb 2024 05:18:03 UTC (6,911 KB)
    [v4] Thu, 6 Jun 2024 03:02:25 UTC (6,916 KB)
    Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, Rada Mihalcea

In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analyses extend across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the role of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.

------------

`[2406.02061] Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models <https://arxiv.org/abs/2406.02061>`__ Alice in Wonderland:在最先进的大型语言模型中显示完全推理分解的简单任务

::

    replaced with revised version Wed, 5 Jun 2024 23:23:54 GMT
    Submission history From: Jenia Jitsev [view email]
    [v1] Tue, 4 Jun 2024 07:43:33 UTC (3,298 KB)
    [v2] Wed, 5 Jun 2024 23:23:54 UTC (3,298 KB)
    Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, Jenia Jitsev

Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale. These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models. We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans. The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical "reasoning"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible. Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs, Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks. Code for reproducing experiments in the paper and raw experiments data can be found at this https URL

------------

-----------
ToolUse (3)
-----------

`[2406.03807] Tool-Planner: Dynamic Solution Tree Planning for Large Language Model with Tool Clustering <https://arxiv.org/abs/2406.03807>`__ Tool- planner:基于工具聚类的大型语言模型动态解树规划

::

    Thu, 6 Jun 2024 07:30:14 GMT
    Yanming Liu, Xinyue Peng, Yuwei Zhang, Jiannan Cao, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du

Large language models (LLMs) have demonstrated exceptional reasoning capabilities, enabling them to solve various complex problems. Recently, this ability has been applied to the paradigm of tool learning. Tool learning involves providing examples of tool usage and their corresponding functions, allowing LLMs to formulate plans and demonstrate the process of invoking and executing each tool. LLMs can address tasks that they cannot complete independently, thereby enhancing their potential across different tasks.
However, this approach faces two key challenges. First, redundant error correction leads to unstable planning and long execution time. Additionally, designing a correct plan among multiple tools is also a challenge in tool learning. To address these issues, we propose Tool-Planner, a task-processing framework based on toolkits. Tool-Planner groups tools based on the API functions with the same function into a toolkit and allows LLMs to implement planning across the various toolkits. When a tool error occurs, the language model can reselect and adjust tools based on the toolkit. Experiments show that our approach demonstrates a high pass and win rate across different datasets and optimizes the planning scheme for tool learning in models such as GPT-4 and Claude 3, showcasing the potential of our method.

------------

`[2406.03618] TACT: Advancing Complex Aggregative Reasoning with Information Extraction Tools <https://arxiv.org/abs/2406.03618>`__ TACT:利用信息抽取工具推进复杂综合推理

::

    Wed, 5 Jun 2024 20:32:56 GMT
    Avi Caciularu, Alon Jacovi, Eyal Ben-David, Sasha Goldshtein, Tal Schuster, Jonathan Herzig, Gal Elidan, Amir Globerson

Large Language Models (LLMs) often do not perform well on queries that require the aggregation of information across texts. To better evaluate this setting and facilitate modeling efforts, we introduce TACT - Text And Calculations through Tables, a dataset crafted to evaluate LLMs' reasoning and computational abilities using complex instructions. TACT contains challenging instructions that demand stitching information scattered across one or more texts, and performing complex integration on this information to generate the answer. We construct this dataset by leveraging an existing dataset of texts and their associated tables. For each such tables, we formulate new queries, and gather their respective answers. We demonstrate that all contemporary LLMs perform poorly on this dataset, achieving an accuracy below 38\%. To pinpoint the difficulties and thoroughly dissect the problem, we analyze model performance across three components: table-generation, Pandas command-generation, and execution. Unexpectedly, we discover that each component presents substantial challenges for current LLMs. These insights lead us to propose a focused modeling framework, which we refer to as IE as a tool.
Specifically, we propose to add "tools" for each of the above steps, and implement each such tool with few-shot prompting. This approach shows an improvement over existing prompting techniques, offering a promising direction for enhancing model capabilities in these tasks.

------------

`[2405.02664] MedPromptExtract (Medical Data Extraction Tool): Anonymization and Hi-fidelity Automated data extraction using NLP and prompt engineering <https://arxiv.org/abs/2405.02664>`__ MedPromptExtract(医疗数据提取工具):基于NLP和prompt工程的匿名和高保真自动数据提取

::

    replaced with revised version Thu, 6 Jun 2024 07:39:00 GMT
    Submission history From: Roomani Srivastava PhD Scholar Centre for Digital Health [view email]
    [v1] Sat, 4 May 2024 13:25:06 UTC (1,448 KB)
    [v2] Thu, 6 Jun 2024 07:39:00 UTC (516 KB)
    Roomani Srivastava, Suraj Prasad, Lipika Bhat, Sarvesh Deshpande, Barnali Das and Kshitij Jadhav

A major roadblock in the seamless digitization of medical records remains the lack of interoperability of existing records. Extracting relevant medical information required for further treatment planning or even research is a time consuming labour intensive task involving expenditure of valuable time of doctors. In this demo paper we present, MedPromptExtract an automated tool using a combination of semi supervised learning, large language models, natural language processing and prompt engineering to convert unstructured medical records to structured data which is amenable for further analysis.

------------

-----------------------
Retrieval-Augmented (8)
-----------------------

`[2406.03592] Measuring Retrieval Complexity in Question Answering Systems <https://arxiv.org/abs/2406.03592>`__ 

::

    Wed, 5 Jun 2024 19:30:52 GMT
    Matteo Gabburo, Nicolaas Paul Jedema, Siddhant Garg, Leonardo F. R. Ribeiro, Alessandro Moschitti

In this paper, we investigate which questions are challenging for retrieval-based Question Answering (QA). We (i) propose retrieval complexity (RC), a novel metric conditioned on the completeness of retrieved documents, which measures the difficulty of answering questions, and (ii) propose an unsupervised pipeline to measure RC given an arbitrary retrieval system. Our proposed pipeline measures RC more accurately than alternative estimators, including LLMs, on six challenging QA benchmarks. Further investigation reveals that RC scores strongly correlate with both QA performance and expert judgment across five of the six studied benchmarks, indicating that RC is an effective measure of question difficulty. Subsequent categorization of high-RC questions shows that they span a broad set of question shapes, including multi-hop, compositional, and temporal QA, indicating that RC scores can categorize a new subset of complex questions. Our system can also have a major impact on retrieval-based systems by helping to identify more challenging questions on existing datasets.

------------

`[2406.04156] Pointer-Guided Pre-Training: Infusing Large Language Models with Paragraph-Level Contextual Awareness <https://arxiv.org/abs/2406.04156>`__ 

::

    Thu, 6 Jun 2024 15:17:51 GMT
    Lars Hillebrand, Prabhupad Pradhan, Christian Bauckhage, Rafet Sifa

We introduce "pointer-guided segment ordering" (SO), a novel pre-training technique aimed at enhancing the contextual understanding of paragraph-level text representations in large language models. Our methodology leverages a self-attention-driven pointer network to restore the original sequence of shuffled text segments, addressing the challenge of capturing the structural coherence and contextual dependencies within documents. This pre-training approach is complemented by a fine-tuning methodology that incorporates dynamic sampling, augmenting the diversity of training instances and improving sample efficiency for various downstream applications. We evaluate our method on a diverse set of datasets, demonstrating its efficacy in tasks requiring sequential text classification across scientific literature and financial reporting domains. Our experiments show that pointer-guided pre-training significantly enhances the model's ability to understand complex document structures, leading to state-of-the-art performance in downstream classification tasks.

------------

`[2402.07483] T-RAG: Lessons from the LLM Trenches <https://arxiv.org/abs/2402.07483>`__ T-RAG: LLM的经验教训

::

    replaced with revised version Thu, 6 Jun 2024 14:42:47 GMT
    Submission history From: Masoomali Fatehkia [view email]
    [v1] Mon, 12 Feb 2024 08:45:08 UTC (282 KB)
    [v2] Thu, 6 Jun 2024 14:42:47 UTC (283 KB)
    Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla

Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization. This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization's hierarchy. Our evaluations, including a Needle in a Haystack test, show that this combination performs better than a simple RAG or finetuning implementation. Finally, we share some lessons learned based on our experiences building an LLM application for real-world use.

------------

`[2305.14592] Meta-Tuning LLMs to Leverage Lexical Knowledge for Generalizable Language Style Understanding <https://arxiv.org/abs/2305.14592>`__ 元调优llm，以利用词汇知识实现可泛化的语言风格理解

::

    replaced with revised version Thu, 6 Jun 2024 03:20:45 GMT
    Submission history From: Ruohao Guo [view email]
    [v1] Wed, 24 May 2023 00:17:36 UTC (7,543 KB)
    [v2] Thu, 6 Jun 2024 03:20:45 UTC (7,840 KB)
    Ruohao Guo, Wei Xu, Alan Ritter

Language style is often used by writers to convey their intentions, identities, and mastery of language. In this paper, we show that current large language models struggle to capture some language styles without fine-tuning. To address this challenge, we investigate whether LLMs can be meta-trained based on representative lexicons to recognize new styles they have not been fine-tuned on. Experiments on 13 established style classification tasks, as well as 63 novel tasks generated using LLMs, demonstrate that meta-training with style lexicons consistently improves zero-shot transfer across styles. We release the code and data at this http URL .

------------

`[2403.06840] RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback <https://arxiv.org/abs/2403.06840>`__ RA-ISF:通过迭代自反馈从检索增强中学习回答和理解

::

    replaced with revised version Thu, 6 Jun 2024 11:55:36 GMT
    Submission history From: Yanming Liu [view email]
    [v1] Mon, 11 Mar 2024 16:01:05 UTC (1,582 KB)
    [v2] Thu, 6 Jun 2024 11:55:36 UTC (1,586 KB)
    Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, Tianyu Du

Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn't previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities and reducing hallucinations.

------------

`[2403.10081] DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models <https://arxiv.org/abs/2403.10081>`__ DRAGIN:基于大型语言模型信息需求的动态检索增强生成

::

    replaced with revised version Wed, 5 Jun 2024 18:44:59 GMT
    Submission history From: Weihang Su [view email]
    [v1] Fri, 15 Mar 2024 07:45:37 UTC (8,015 KB)
    [v2] Wed, 5 Jun 2024 18:44:59 UTC (8,017 KB)
    Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu

Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specifically designed to make decisions on when and what to retrieve based on the LLM's real-time information needs during the text generation process. We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method. We have open-sourced all the code, data, and models in GitHub: this https URL

------------

`[2404.13874] VALOR-EVAL: Holistic Coverage and Faithfulness Evaluation of Large Vision-Language Models <https://arxiv.org/abs/2404.13874>`__ 

::

    replaced with revised version Thu, 6 Jun 2024 02:53:37 GMT
    Submission history From: Haoyi Qiu [view email]
    [v1] Mon, 22 Apr 2024 04:49:22 UTC (5,125 KB)
    [v2] Thu, 6 Jun 2024 02:53:37 UTC (3,922 KB)
    Haoyi Qiu, Wenbo Hu, Zi-Yi Dou, Nanyun Peng

Large Vision-Language Models (LVLMs) suffer from hallucination issues, wherein the models generate plausible-sounding but factually incorrect outputs, undermining their reliability. A comprehensive quantitative evaluation is necessary to identify and understand the extent of hallucinations in these models. However, existing benchmarks are often limited in scope, focusing mainly on object hallucinations. Furthermore, current evaluation methods struggle to effectively address the subtle semantic distinctions between model outputs and reference data, as well as the balance between hallucination and informativeness. To address these issues, we introduce a multi-dimensional benchmark covering objects, attributes, and relations, with challenging images selected based on associative biases. Moreover, we propose a large language model (LLM)-based two-stage evaluation framework that generalizes the popular CHAIR metric and incorporates both faithfulness and coverage into the evaluation. Experiments on 10 established LVLMs demonstrate that our evaluation metric is more comprehensive and better correlated with humans than existing work when evaluating on our challenging human-annotated benchmark dataset. Our work also highlights the critical balance between faithfulness and coverage of model outputs, and encourages future works to address hallucinations in LVLMs while keeping their outputs informative.

------------

`[2406.00083] BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models <https://arxiv.org/abs/2406.00083>`__ BadRAG:大型语言模型检索增强生成中的漏洞识别

::

    replaced with revised version Thu, 6 Jun 2024 13:38:42 GMT
    Submission history From: Mengxin Zheng [view email]
    [v1] Mon, 3 Jun 2024 02:25:33 UTC (1,256 KB)
    [v2] Thu, 6 Jun 2024 13:38:42 UTC (1,257 KB)
    Jiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun Chen, Qian Lou

Large Language Models (LLMs) are constrained by outdated information and a tendency to generate incorrect data, commonly referred to as "hallucinations." Retrieval-Augmented Generation (RAG) addresses these limitations by combining the strengths of retrieval-based methods and generative models. This approach involves retrieving relevant information from a large, up-to-date dataset and using it to enhance the generation process, leading to more accurate and contextually appropriate responses. Despite its benefits, RAG introduces a new attack surface for LLMs, particularly because RAG databases are often sourced from public data, such as the web. In this paper, we propose \TrojRAG{} to identify the vulnerabilities and attacks on retrieval parts (RAG database) and their indirect attacks on generative parts (LLMs). Specifically, we identify that poisoning several customized content passages could achieve a retrieval backdoor, where the retrieval works well for clean queries but always returns customized poisoned adversarial queries. Triggers and poisoned passages can be highly customized to implement various attacks. For example, a trigger could be a semantic group like "The Republican Party, Donald Trump, etc." Adversarial passages can be tailored to different contents, not only linked to the triggers but also used to indirectly attack generative LLMs without modifying them. These attacks can include denial-of-service attacks on RAG and semantic steering attacks on LLM generations conditioned by the triggers. Our experiments demonstrate that by just poisoning 10 adversarial passages can induce 98.2\% success rate to retrieve the adversarial passages. Then, these passages can increase the reject ratio of RAG-based GPT-4 from 0.01\% to 74.6\% or increase the rate of negative responses from 0.22\% to 72\% for targeted queries.

------------

---------
Agent (9)
---------

`[2406.03679] On the Effects of Data Scale on Computer Control Agents <https://arxiv.org/abs/2406.03679>`__ 数据规模对计算机控制agent的影响

::

    Thu, 6 Jun 2024 01:49:29 GMT
    Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, Oriana Riva

Autonomous agents that control computer interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether fine-tuning alone is a viable approach for building real-world computer control agents. %In particularly, we investigate how performance measured on both high and low-level tasks in domain and out of domain scales as more training data is collected.
To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, AndroidControl is the most diverse computer control dataset to date, including 15,283 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data. Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance.

------------

`[2406.04151] AgentGym: Evolving Large Language Model-based Agents across Diverse Environments <https://arxiv.org/abs/2406.04151>`__ AgentGym:跨不同环境演化的基于语言模型的大型智能体

::

    Thu, 6 Jun 2024 15:15:41 GMT
    Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang

Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization. In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method. We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration. AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments. Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations. The AgentGym suite is available on https://github.com/WooooDyy/AgentGym.

------------

`[2406.04208] Aligning Agents like Large Language Models <https://arxiv.org/abs/2406.04208>`__ 像大型语言模型这样的智能体对齐

::

    Thu, 6 Jun 2024 16:05:45 GMT
    Adam Jelley, Yuhan Cao, Dave Bignell, Sam Devlin, Tabish Rashid

Training agents to behave as desired in complex 3D environments from high-dimensional sensory information is challenging. Imitation learning from diverse human behavior provides a scalable approach for training an agent with a sensible behavioral prior, but such an agent may not perform the specific behaviors of interest when deployed. To address this issue, we draw an analogy between the undesirable behaviors of imitation learning agents and the unhelpful responses of unaligned large language models (LLMs). We then investigate how the procedure for aligning LLMs can be applied to aligning agents in a 3D environment from pixels. For our analysis, we utilize an academically illustrative part of a modern console game in which the human behavior distribution is multi-modal, but we want our agent to imitate a single mode of this behavior. We demonstrate that we can align our agent to consistently perform the desired mode, while providing insights and advice for successfully applying this approach to training agents. Project webpage at https://adamjelley.github.io/aligning-agents-like-llms .

------------

`[2406.00252] Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey <https://arxiv.org/abs/2406.00252>`__ 多模态和多智能体系统满足理性:综述

::

    replaced with revised version Wed, 5 Jun 2024 19:39:56 GMT
    Submission history From: Bowen Jiang [view email]
    [v1] Sat, 1 Jun 2024 01:17:25 UTC (2,332 KB)
    [v2] Wed, 5 Jun 2024 19:39:56 UTC (2,332 KB)
    Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Weijie J. Su, Camillo J. Taylor, Tanwi Mallick

Rationality is the quality of being guided by reason, characterized by logical thinking and decision-making that align with evidence and logical rules. This quality is essential for effective problem-solving, as it ensures that solutions are well-founded and systematically derived. Despite the advancements of large language models (LLMs) in generating human-like text with remarkable accuracy, they present biases inherited from the training data, inconsistency across different contexts, and difficulty understanding complex scenarios involving multiple layers of context. Therefore, recent research attempts to leverage the strength of multiple agents working collaboratively with various types of data and tools for enhanced consistency and reliability. To that end, this paper aims to understand whether multi-modal and multi-agent systems are advancing toward rationality by surveying the state-of-the-art works, identifying advancements over single-agent and single-modal systems in terms of rationality, and discussing open problems and future directions. We maintain an open repository at this https URL.

------------

`[2402.13212] Soft Self-Consistency Improves Language Model Agents <https://arxiv.org/abs/2402.13212>`__ 软自一致性改进语言模型agent

::

    replaced with revised version Wed, 5 Jun 2024 19:50:19 GMT
    Submission history From: Han Wang [view email]
    [v1] Tue, 20 Feb 2024 18:22:38 UTC (7,997 KB)
    [v2] Wed, 5 Jun 2024 19:50:19 UTC (8,001 KB)
    Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal

Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (SOFT-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. SOFT-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, SOFT-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that SOFT-SC can be applied to both open-source and black-box models.

------------

`[2405.10150] Speaker Verification in Agent-Generated Conversations <https://arxiv.org/abs/2405.10150>`__ 代理生成对话中的说话人确认

::

    replaced with revised version Thu, 6 Jun 2024 03:36:16 GMT
    Submission history From: Yizhe Yang [view email]
    [v1] Thu, 16 May 2024 14:46:18 UTC (11,769 KB)
    [v2] Thu, 6 Jun 2024 03:36:16 UTC (12,677 KB)
    Yizhe Yang, Palakorn Achananuparp, Heyan Huang, Jing Jiang, and Ee-Peng Lim

The recent success of large language models (LLMs) has attracted widespread interest to develop role-playing conversational agents personalized to the characteristics and styles of different speakers to enhance their abilities to perform both general and special purpose dialogue tasks. However, the ability to personalize the generated utterances to speakers, whether conducted by human or LLM, has not been well studied. To bridge this gap, our study introduces a novel evaluation challenge: speaker verification in agent-generated conversations, which aimed to verify whether two sets of utterances originate from the same speaker. To this end, we assemble a large dataset collection encompassing thousands of speakers and their utterances. We also develop and evaluate speaker verification models under experiment setups. We further utilize the speaker verification models to evaluate the personalization abilities of LLM-based role-playing models. Comprehensive experiments suggest that the current role-playing models fail in accurately mimicking speakers, primarily due to their inherent linguistic characteristics.

------------

`[2405.20267] Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions <https://arxiv.org/abs/2405.20267>`__ LLM的自动竞技场:通过代理peer -battle和委员会讨论实现LLM评估的自动化

::

    replaced with revised version Thu, 6 Jun 2024 11:36:09 GMT
    Submission history From: Ruochen Zhao [view email]
    [v1] Thu, 30 May 2024 17:19:19 UTC (1,982 KB)
    [v2] Thu, 6 Jun 2024 11:36:09 UTC (2,134 KB)
    Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Deli Zhao, Lidong Bing

As LLMs evolve on a daily basis, there is an urgent need for a trustworthy evaluation method that can provide robust evaluation results in a timely fashion. Currently, as static benchmarks are prone to contamination concerns, users tend to trust human voting platforms, such as Chatbot Arena. However, human annotations require extensive manual efforts. To provide an automatic, robust, and trustworthy evaluation framework, we innovatively propose the Auto-Arena of LLMs, which automates the entire evaluation process with LLM agents. Firstly, an examiner LLM devises queries. Then, a pair of candidate LLMs engage in a multi-round peer-battle around the query, during which the LLM's true performance gaps become visible. Finally, a committee of LLM judges collectively discuss and determine the winner, which alleviates bias and promotes fairness. In our extensive experiment on the 17 newest LLMs, Auto-Arena shows the highest correlation with human preferences, providing a promising alternative to human evaluation platforms.

------------

`[2401.13649] VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks <https://arxiv.org/abs/2401.13649>`__ VisualWebArena:现实视觉Web任务上的多模态智能体评估

::

    replaced with revised version Thu, 6 Jun 2024 02:01:09 GMT
    Submission history From: Jing Yu Koh [view email]
    [v1] Wed, 24 Jan 2024 18:35:21 UTC (8,665 KB)
    [v2] Thu, 6 Jun 2024 02:01:09 UTC (10,108 KB)
    Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried

Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at this https URL.

------------

`[2402.06700] Entropy-Regularized Token-Level Policy Optimization for Language Agent Reinforcement <https://arxiv.org/abs/2402.06700>`__ 熵正则化令牌级语言智能体强化策略优化

::

    replaced with revised version Thu, 6 Jun 2024 12:29:23 GMT
    Submission history From: Muning Wen [view email]
    [v1] Fri, 9 Feb 2024 07:45:26 UTC (504 KB)
    [v2] Tue, 5 Mar 2024 05:17:21 UTC (504 KB)
    [v3] Wed, 29 May 2024 12:15:46 UTC (500 KB)
    [v4] Thu, 6 Jun 2024 12:29:23 UTC (500 KB)
    Muning Wen, Junwei Liao, Cheng Deng, Jun Wang, Weinan Zhang and Ying Wen

Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed to harmonize the RL process with the principles of language modeling. This methodology decomposes the Q-function update from a coarse action-level view to a more granular token-level perspective, backed by theoretical proof of optimization consistency. Crucially, this decomposition renders linear time complexity in action exploration. We assess the effectiveness of ETPO within a simulated environment that models data science code generation as a series of multi-step interactive tasks; results underline ETPO's potential as a robust method for refining the interactive decision-making capabilities of language agents. For a more detailed preliminary work describing our motivation for token-level decomposition and applying it in PPO methods, please refer to arXiv:2405.15821.

------------

-----------
Other (115)
-----------

`[2406.03997] HackAtari: Atari Learning Environments for Robust and Continual Reinforcement Learning <https://arxiv.org/abs/2406.03997>`__ HackAtari:用于鲁棒和持续强化学习的Atari学习环境

::

    Thu, 6 Jun 2024 12:17:05 GMT
    Quentin Delfosse, Jannis Bl\"uml, Bjarne Gregori, Kristian Kersting

Artificial agents' adaptability to novelty and alignment with intended behavior is crucial for their effective deployment. Reinforcement learning (RL) leverages novelty as a means of exploration, yet agents often struggle to handle novel situations, hindering generalization. To address these issues, we propose HackAtari, a framework introducing controlled novelty to the most common RL benchmark, the Atari Learning Environment. HackAtari allows us to create novel game scenarios (including simplification for curriculum learning), to swap the game elements' colors, as well as to introduce different reward signals for the agent. We demonstrate that current agents trained on the original environments include robustness failures, and evaluate HackAtari's efficacy in enhancing RL agents' robustness and aligning behavior through experiments using C51 and PPO. Overall, HackAtari can be used to improve the robustness of current and future RL algorithms, allowing Neuro-Symbolic RL, curriculum RL, causal RL, as well as LLM-driven RL. Our work underscores the significance of developing interpretable in RL agents.

------------

`[2406.03589] Ranking Manipulation for Conversational Search Engines <https://arxiv.org/abs/2406.03589>`__ 对话式搜索引擎的排名操纵

::

    Wed, 5 Jun 2024 19:14:21 GMT
    Samuel Pfrommer, Yatong Bai, Tanmay Gautam, Somayeh Sojoudi

Major search engine providers are rapidly incorporating Large Language Model (LLM)-generated content in response to user queries. These conversational search engines operate by loading retrieved website text into the LLM context for summarization and interpretation. Recent research demonstrates that LLMs are highly vulnerable to jailbreaking and prompt injection attacks, which disrupt the safety and quality goals of LLMs using adversarial strings. This work investigates the impact of prompt injections on the ranking order of sources referenced by conversational search engines. To this end, we introduce a focused dataset of real-world consumer product websites and formalize conversational search ranking as an adversarial problem. Experimentally, we analyze conversational search rankings in the absence of adversarial injections and show that different LLMs vary significantly in prioritizing product name, document content, and context position. We then present a tree-of-attacks-based jailbreaking technique which reliably promotes low-ranked products.
Importantly, these attacks transfer effectively to state-of-the-art conversational search engines such as perplexity.ai. Given the strong financial incentive for website owners to boost their search ranking, we argue that our problem formulation is of critical importance for future robustness work.

------------

`[2406.03600] Knowledge-Infused Legal Wisdom: Navigating LLM Consultation through the Lens of Diagnostics and Positive-Unlabeled Reinforcement Learning <https://arxiv.org/abs/2406.03600>`__ 知识注入的法律智慧:通过诊断和积极无标签强化学习的视角引导LLM咨询

::

    Wed, 5 Jun 2024 19:47:35 GMT
    Yang Wu, Chenghao Wang, Ece Gumusel, Xiaozhong Liu

The integration of generative Large Language Models (LLMs) into various applications, including the legal domain, has been accelerated by their expansive and versatile nature. However, when facing a legal case, users without a legal background often struggle to formulate professional queries and may inadvertently overlook critical legal factors when presenting their case narrative to LLMs. To address this issue, we propose the Diagnostic Legal Large Language Model (D3LM), which utilizes adaptive lawyer-like diagnostic questions to collect additional case information and then provides high-quality feedback.
D3LM incorporates an innovative graph-based Positive-Unlabeled Reinforcement Learning (PURL) algorithm, enabling the generation of critical questions and enhancing user-LLM interactions. Moreover, an integrated LLM-based stopping criterion facilitates precise Court Views Generation (CVG). Our research also introduces a new English-language CVG dataset based on the US case law database, enriching the realm of LLM research and deployment with a vital dimension. D3LM surpasses classical LLMs by delivering outstanding performance and a remarkable user experience in the legal domain.

------------

`[2406.03689] Evaluating the World Model Implicit in a Generative Model <https://arxiv.org/abs/2406.03689>`__ 评估生成模型中隐含的世界模型

::

    Thu, 6 Jun 2024 02:20:31 GMT
    Keyon Vafa, Justin Y. Chen, Jon Kleinberg, Sendhil Mullainathan, Ashesh Rambachan

Recent work suggests that large language models may implicitly learn world models. How should we assess this possibility? We formalize this question for the case where the underlying reality is governed by a deterministic finite automaton. This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry. We propose new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory. We illustrate their utility in three domains: game playing, logic puzzles, and navigation. In all domains, the generative models we consider do well on existing diagnostics for assessing world models, but our evaluation metrics reveal their world models to be far less coherent than they appear. Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead it to fail badly. Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; our results suggest new ways to assess how close a given model is to that goal.

------------

`[2406.03725] LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification <https://arxiv.org/abs/2406.03725>`__ LLMEmbed:重新思考轻量级LLM在文本分类中的真正作用

::

    Thu, 6 Jun 2024 03:46:59 GMT
    Chun Liu, Hongguang Zhang, Kainan Zhao, Xinghai Ju, Lin Yang

With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas. Recently, many attempts based on prompt-learning have been made to improve the performance of text classification. However, most of these methods are based on heuristic Chain-of-Thought (CoT), and tend to be more complex but less efficient. In this paper, we rethink the LLM-based text classification methodology, propose a simple and effective transfer learning strategy, namely LLMEmbed, to address this classical but challenging task. To illustrate, we first study how to properly extract and fuse the text embeddings via various lightweight LLMs at different network depths to improve their robustness and discrimination, then adapt such embeddings to train the classifier. We perform extensive experiments on publicly available datasets, and the results show that LLMEmbed achieves strong performance while enjoys low training overhead using lightweight LLM backbones compared to recent methods based on larger LLMs, i.e. GPT-3, and sophisticated prompt-based strategies. Our LLMEmbed achieves adequate accuracy on publicly available benchmarks without any fine-tuning while merely use 4% model parameters, 1.8% electricity consumption and 1.5% runtime compared to its counterparts. Code is available at: https://github.com/ChunLiu-cs/LLMEmbed-ACL2024.

------------

`[2406.03816] ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search <https://arxiv.org/abs/2406.03816>`__ ReST-MCTS*:基于过程奖励引导树搜索的LLM自我训练

::

    Thu, 6 Jun 2024 07:40:00 GMT
    Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, Jie Tang

Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data.
This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models.
ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\text{EM}$ and Self-Rewarding LM.

------------

`[2406.03827] Chaos with Keywords: Exposing Large Language Models Sycophancy to Misleading Keywords and Evaluating Defense Strategies <https://arxiv.org/abs/2406.03827>`__ 关键词混乱:暴露大型语言模型对误导性关键词的谄媚和评估防御策略

::

    Thu, 6 Jun 2024 08:03:05 GMT
    Aswin RRV and Nemika Tyagi and Md Nayem Uddin and Neeraj Varshney and Chitta Baral

This study explores the sycophantic tendencies of Large Language Models (LLMs), where these models tend to provide answers that match what users want to hear, even if they are not entirely correct. The motivation behind this exploration stems from the common behavior observed in individuals searching the internet for facts with partial or misleading knowledge. Similar to using web search engines, users may recall fragments of misleading keywords and submit them to an LLM, hoping for a comprehensive response. Our empirical analysis of several LLMs shows the potential danger of these models amplifying misinformation when presented with misleading keywords. Additionally, we thoroughly assess four existing hallucination mitigation strategies to reduce LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of these strategies for generating factually correct statements. Furthermore, our analyses delve into knowledge-probing experiments on factual keywords and different categories of sycophancy mitigation.

------------

`[2406.03847] Lean Workbook: A large-scale Lean problem set formalized from natural language math problems <https://arxiv.org/abs/2406.03847>`__ 精益工作簿:由自然语言数学问题形式化的大规模精益问题集

::

    Thu, 6 Jun 2024 08:25:43 GMT
    Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, Kai Chen

Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at https://github.com/InternLM/InternLM-Math and our data at https://huggingface.co/datasets/InternLM/Lean-Workbook.

------------

`[2406.03882] Spontaneous Speech-Based Suicide Risk Detection Using Whisper and Large Language Models <https://arxiv.org/abs/2406.03882>`__ 基于耳语和大型语言模型的自发语音自杀风险检测

::

    Thu, 6 Jun 2024 09:21:13 GMT
    Ziyun Cui, Chang Lei, Wen Wu, Yinan Duan, Diyang Qu, Ji Wu, Runsen Chen, Chao Zhang

The early detection of suicide risk is important since it enables the intervention to prevent potential suicide attempts. This paper studies the automatic detection of suicide risk based on spontaneous speech from adolescents, and collects a Mandarin dataset with 15 hours of suicide speech from more than a thousand adolescents aged from ten to eighteen for our experiments. To leverage the diverse acoustic and linguistic features embedded in spontaneous speech, both the Whisper speech model and textual large language models (LLMs) are used for suicide risk detection. Both all-parameter finetuning and parameter-efficient finetuning approaches are used to adapt the pre-trained models for suicide risk detection, and multiple audio-text fusion approaches are evaluated to combine the representations of Whisper and the LLM.
The proposed system achieves a detection accuracy of 0.807 and an F1-score of 0.846 on the test set with 119 subjects, indicating promising potential for real suicide risk detection applications.

------------

`[2406.03897] HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew <https://arxiv.org/abs/2406.03897>`__ HeSum:希伯来语文本摘要的新数据集

::

    Thu, 6 Jun 2024 09:36:14 GMT
    Tzuf Paz-Argaman, Itai Mondshine, Asaf Achi Mordechai, and Reut Tsarfaty

While large language models (LLMs) excel in various natural language tasks in English, their performance in lower-resourced languages like Hebrew, especially for generative tasks such as abstractive summarization, remains unclear. The high morphological richness in Hebrew adds further challenges due to the ambiguity in sentence comprehension and the complexities in meaning construction. In this paper, we address this resource and evaluation gap by introducing HeSum, a novel benchmark specifically designed for abstractive text summarization in Modern Hebrew. HeSum consists of 10,000 article-summary pairs sourced from Hebrew news websites written by professionals. Linguistic analysis confirms HeSum's high abstractness and unique morphological challenges. We show that HeSum presents distinct difficulties for contemporary state-of-the-art LLMs, establishing it as a valuable testbed for generative language technology in Hebrew, and MRLs generative challenges in general.

------------

`[2406.03949] UltraMedical: Building Specialized Generalists in Biomedicine <https://arxiv.org/abs/2406.03949>`__ 超医学:生物医学专科建设

::

    Thu, 6 Jun 2024 10:50:26 GMT
    Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Hu Jinfang, Zhiyuan Liu, Bowen Zhou

Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas. Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges. The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization. However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data. In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks. Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community.

------------

`[2406.03963] A + B: A General Generator-Reader Framework for Optimizing LLMs to Unleash Synergy Potential <https://arxiv.org/abs/2406.03963>`__ 

::

    Thu, 6 Jun 2024 11:14:27 GMT
    Wei Tang, Yixin Cao, Jiahao Ying, Bo Wang, Yuyue Zhao, Yong Liao, Pengyuan Zhou

Retrieval-Augmented Generation (RAG) is an effective solution to supplement necessary knowledge to large language models (LLMs). Targeting its bottleneck of retriever performance, "generate-then-read" pipeline is proposed to replace the retrieval stage with generation from the LLM itself. Although promising, this research direction is underexplored and still cannot work in the scenario when source knowledge is given. In this paper, we formalize a general "A + B" framework with varying combinations of foundation models and types for systematic investigation. We explore the efficacy of the base and chat versions of LLMs and found their different functionalities suitable for generator A and reader B, respectively. Their combinations consistently outperform single models, especially in complex scenarios. Furthermore, we extend the application of the "A + B" framework to scenarios involving source documents through continuous learning, enabling the direct integration of external knowledge into LLMs. This approach not only facilitates effective acquisition of new knowledge but also addresses the challenges of safety and helpfulness post-adaptation.
The paper underscores the versatility of the "A + B" framework, demonstrating its potential to enhance the practical application of LLMs across various domains.

------------

`[2406.03986] On The Persona-based Summarization of Domain-Specific Documents <https://arxiv.org/abs/2406.03986>`__ 基于人物角色的领域文档摘要研究

::

    Thu, 6 Jun 2024 12:00:41 GMT
    Ankan Mullick, Sombit Bose, Rounak Saha, Ayan Kumar Bhowmick, Pawan Goyal, Niloy Ganguly, Prasenjit Dey, Ravi Kokku

In an ever-expanding world of domain-specific knowledge, the increasing complexity of consuming, and storing information necessitates the generation of summaries from large information repositories. However, every persona of a domain has different requirements of information and hence their summarization.
For example, in the healthcare domain, a persona-based (such as Doctor, Nurse, Patient etc.) approach is imperative to deliver targeted medical information efficiently. Persona-based summarization of domain-specific information by humans is a high cognitive load task and is generally not preferred. The summaries generated by two different humans have high variability and do not scale in cost and subject matter expertise as domains and personas grow.
Further, AI-generated summaries using generic Large Language Models (LLMs) may not necessarily offer satisfactory accuracy for different domains unless they have been specifically trained on domain-specific data and can also be very expensive to use in day-to-day operations. Our contribution in this paper is two-fold: 1) We present an approach to efficiently fine-tune a domain-specific small foundation LLM using a healthcare corpus and also show that we can effectively evaluate the summarization quality using AI-based critiquing. 2) We further show that AI-based critiquing has good concordance with Human-based critiquing of the summaries. Hence, such AI-based pipelines to generate domain-specific persona-based summaries can be easily scaled to other domains such as legal, enterprise documents, education etc. in a very efficient and cost-effective manner.

------------

`[2406.03993] Assessing LLMs for Zero-shot Abstractive Summarization Through the Lens of Relevance Paraphrasing <https://arxiv.org/abs/2406.03993>`__ 通过相关复述的视角评估llm的零样本抽象摘要

::

    Thu, 6 Jun 2024 12:08:43 GMT
    Hadi Askari, Anshuman Chhabra, Muhao Chen, Prasant Mohapatra

Large Language Models (LLMs) have achieved state-of-the-art performance at zero-shot generation of abstractive summaries for given articles. However, little is known about the robustness of such a process of zero-shot summarization. To bridge this gap, we propose relevance paraphrasing, a simple strategy that can be used to measure the robustness of LLMs as summarizers. The relevance paraphrasing approach identifies the most relevant sentences that contribute to generating an ideal summary, and then paraphrases these inputs to obtain a minimally perturbed dataset. Then, by evaluating model performance for summarization on both the original and perturbed datasets, we can assess the LLM's one aspect of robustness. We conduct extensive experiments with relevance paraphrasing on 4 diverse datasets, as well as 4 LLMs of different sizes (GPT-3.5-Turbo, Llama-2-13B, Mistral-7B, and Dolly-v2-7B). Our results indicate that LLMs are not consistent summarizers for the minimally perturbed articles, necessitating further improvements.

------------

`[2406.04064] Ask LLMs Directly, "What shapes your bias?": Measuring Social Bias in Large Language Models <https://arxiv.org/abs/2406.04064>`__ 直接问llm:“是什么塑造了你的偏见?”:在大型语言模型中测量社会偏见

::

    Thu, 6 Jun 2024 13:32:09 GMT
    Jisu Shin, Hoyun Song, Huije Lee, Soyeong Jeong, and Jong C. Park

Social bias is shaped by the accumulation of social perceptions towards targets across various demographic identities. To fully understand such social bias in large language models (LLMs), it is essential to consider the composite of social perceptions from diverse perspectives among identities. Previous studies have either evaluated biases in LLMs by indirectly assessing the presence of sentiments towards demographic identities in the generated text or measuring the degree of alignment with given stereotypes. These methods have limitations in directly quantifying social biases at the level of distinct perspectives among identities. In this paper, we aim to investigate how social perceptions from various viewpoints contribute to the development of social bias in LLMs. To this end, we propose a novel strategy to intuitively quantify these social perceptions and suggest metrics that can evaluate the social biases within LLMs by aggregating diverse social perceptions. The experimental results show the quantitative demonstration of the social attitude in LLMs by examining social perception. The analysis we conducted shows that our proposed metrics capture the multi-dimensional aspects of social bias, enabling a fine-grained and comprehensive investigation of bias in LLMs.

------------

`[2406.04113] Uncovering Limitations of Large Language Models in Information Seeking from Tables <https://arxiv.org/abs/2406.04113>`__ 揭示大型语言模型在从表中查找信息方面的局限性

::

    Thu, 6 Jun 2024 14:30:59 GMT
    Chaoxu Pang, Yixuan Cao, Chunhao Yang, Ping Luo

Tables are recognized for their high information density and widespread usage, serving as essential sources of information. Seeking information from tables (TIS) is a crucial capability for Large Language Models (LLMs), serving as the foundation of knowledge-based Q&A systems. However, this field presently suffers from an absence of thorough and reliable evaluation. This paper introduces a more reliable benchmark for Table Information Seeking (TabIS). To avoid the unreliable evaluation caused by text similarity-based metrics, TabIS adopts a single-choice question format (with two options per question) instead of a text generation format. We establish an effective pipeline for generating options, ensuring their difficulty and quality. Experiments conducted on 12 LLMs reveal that while the performance of GPT-4-turbo is marginally satisfactory, both other proprietary and open-source models perform inadequately. Further analysis shows that LLMs exhibit a poor understanding of table structures, and struggle to balance between TIS performance and robustness against pseudo-relevant tables (common in retrieval-augmented systems). These findings uncover the limitations and potential challenges of LLMs in seeking information from tables. We release our data and code to facilitate further research in this field.

------------

`[2406.04127] Are We Done with MMLU? <https://arxiv.org/abs/2406.04127>`__ MMLU搞定了吗?

::

    Thu, 6 Jun 2024 14:49:06 GMT
    Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, Pasquale Minervini

Maybe not. We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, we find that 57% of the analysed questions in the Virology subset contain errors. To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error taxonomy. Then, we create MMLU-Redux, which is a subset of 3,000 manually re-annotated questions across 30 MMLU subjects. Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that were originally reported. Our results strongly advocate for revising MMLU's error-ridden questions to enhance its future utility and reliability as a benchmark. Therefore, we open up MMLU-Redux for additional annotation https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux.

------------

`[2406.04136] Legal Judgment Reimagined: PredEx and the Rise of Intelligent AI Interpretation in Indian Courts <https://arxiv.org/abs/2406.04136>`__ 重新设想法律判决:PredEx和印度法院智能人工智能解释的兴起

::

    Thu, 6 Jun 2024 14:57:48 GMT
    Shubham Kumar Nigam, Anurag Sharma, Danush Khanna, Noel Shallum, Kripabandhu Ghosh, and Arnab Bhattacharya

In the era of Large Language Models (LLMs), predicting judicial outcomes poses significant challenges due to the complexity of legal proceedings and the scarcity of expert-annotated datasets. Addressing this, we introduce \textbf{Pred}iction with \textbf{Ex}planation (\texttt{PredEx}), the largest expert-annotated dataset for legal judgment prediction and explanation in the Indian context, featuring over 15,000 annotations. This groundbreaking corpus significantly enhances the training and evaluation of AI models in legal analysis, with innovations including the application of instruction tuning to LLMs. This method has markedly improved the predictive accuracy and explanatory depth of these models for legal judgments. We employed various transformer-based models, tailored for both general and Indian legal contexts.
Through rigorous lexical, semantic, and expert assessments, our models effectively leverage \texttt{PredEx} to provide precise predictions and meaningful explanations, establishing it as a valuable benchmark for both the legal profession and the NLP community.

------------

`[2406.04143] Do Language Models Understand Morality? Towards a Robust Detection of Moral Content <https://arxiv.org/abs/2406.04143>`__ 语言模型理解道德吗?走向对道德内容的鲁棒检测

::

    Thu, 6 Jun 2024 15:08:16 GMT
    Luana Bulla, Aldo Gangemi and Misael Mongiov\`i

The task of detecting moral values in text has significant implications in various fields, including natural language processing, social sciences, and ethical decision-making. Previously proposed supervised models often suffer from overfitting, leading to hyper-specialized moral classifiers that struggle to perform well on data from different domains. To address this issue, we introduce novel systems that leverage abstract concepts and common-sense knowledge acquired from Large Language Models and Natural Language Inference models during previous stages of training on multiple data sources. By doing so, we aim to develop versatile and robust methods for detecting moral values in real-world scenarios. Our approach uses the GPT 3.5 model as a zero-shot ready-made unsupervised multi-label classifier for moral values detection, eliminating the need for explicit training on labeled data. We compare it with a smaller NLI-based zero-shot model. The results show that the NLI approach achieves competitive results compared to the Davinci model. Furthermore, we conduct an in-depth investigation of the performance of supervised systems in the context of cross-domain multi-label moral value detection. This involves training supervised models on different domains to explore their effectiveness in handling data from different sources and comparing their performance with the unsupervised methods. Our contributions encompass a thorough analysis of both supervised and unsupervised methodologies for cross-domain value detection. We introduce the Davinci model as a state-of-the-art zero-shot unsupervised moral values classifier, pushing the boundaries of moral value detection without the need for explicit training on labeled data. Additionally, we perform a comparative evaluation of our approach with the supervised models, shedding light on their respective strengths and weaknesses.

------------

`[2406.04145] Every Answer Matters: Evaluating Commonsense with Probabilistic Measures <https://arxiv.org/abs/2406.04145>`__ 每个答案都很重要:用概率测度评估常识

::

    Thu, 6 Jun 2024 15:10:27 GMT
    Qi Cheng, Michael Boratko, Pranay Kumar Yelugam, Tim O'Gorman, Nalini Singh, Andrew McCallum, Xiang Lorraine Li

Large language models have demonstrated impressive performance on commonsense tasks; however, these tasks are often posed as multiple-choice questions, allowing models to exploit systematic biases. Commonsense is also inherently probabilistic with multiple correct answers. The purpose of "boiling water" could be making tea and cooking, but it also could be killing germs. Existing tasks do not capture the probabilistic nature of common sense. To this end, we present commonsense frame completion (CFC), a new generative task that evaluates common sense via multiple open-ended generations. We also propose a method of probabilistic evaluation that strongly correlates with human judgments. Humans drastically outperform strong language model baselines on our dataset, indicating this approach is both a challenging and useful evaluation of machine common sense.

------------

`[2406.04175] Confabulation: The Surprising Value of Large Language Model Hallucinations <https://arxiv.org/abs/2406.04175>`__ 虚构:大型语言模型幻觉的惊人价值

::

    Thu, 6 Jun 2024 15:32:29 GMT
    Peiqi Sui, Eamon Duede, Sophie Wu, Richard Jean So

This paper presents a systematic defense of large language model (LLM) hallucinations or 'confabulations' as a potential resource instead of a categorically negative pitfall. The standard view is that confabulations are inherently problematic and AI research should eliminate this flaw. In this paper, we argue and empirically demonstrate that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication. In other words, it has potential value. Specifically, we analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs. This finding reveals a tension in our usually dismissive understandings of confabulation. It suggests, counter-intuitively, that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation.

------------

`[2406.04202] Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language Model <https://arxiv.org/abs/2406.04202>`__ 基于微调预训练大型语言模型的法律文件起草

::

    Thu, 6 Jun 2024 16:00:20 GMT
    Chun-Hsien Lin and Pu-Jen Cheng

With the development of large-scale Language Models (LLM), fine-tuning pre-trained LLM has become a mainstream paradigm for solving downstream tasks of natural language processing. However, training a language model in the legal field requires a large number of legal documents so that the language model can learn legal terminology and the particularity of the format of legal documents.
The typical NLP approaches usually rely on many manually annotated data sets for training. However, in the legal field application, it is difficult to obtain a large number of manually annotated data sets, which restricts the typical method applied to the task of drafting legal documents. The experimental results of this paper show that not only can we leverage a large number of annotation-free legal documents without Chinese word segmentation to fine-tune a large-scale language model, but more importantly, it can fine-tune a pre-trained LLM on the local computer to achieve the generating legal document drafts task, and at the same time achieve the protection of information privacy and to improve information security issues.

------------

`[2406.04214] ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models <https://arxiv.org/abs/2406.04214>`__ ValueBench:综合评估大型语言模型的价值取向和理解

::

    Thu, 6 Jun 2024 16:14:16 GMT
    Yuanyi Ren, Haoran Ye, Hanjun Fang, Xin Zhang, Guojie Song

Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies. This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications. This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and value understanding in LLMs. ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions. We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space. With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks. ValueBench is openly accessible at https://github.com/Value4AI/ValueBench.

------------

`[2406.04216] What Do Language Models Learn in Context? The Structured Task Hypothesis <https://arxiv.org/abs/2406.04216>`__ 语言模型在上下文中学习什么?结构化任务假说

::

    Thu, 6 Jun 2024 16:15:34 GMT
    Jiaoda Li, Yifan Hou, Mrinmaya Sachan, Ryan Cotterell

Large language models (LLMs) exhibit an intriguing ability to learn a novel task from in-context examples presented in a demonstration, termed in-context learning (ICL). Understandably, a swath of research has been dedicated to uncovering the theories underpinning ICL. One popular hypothesis explains ICL by task selection. LLMs identify the task based on the demonstration and generalize it to the prompt. Another popular hypothesis is that ICL is a form of meta-learning, i.e., the models learn a learning algorithm at pre-training time and apply it to the demonstration. Finally, a third hypothesis argues that LLMs use the demonstration to select a composition of tasks learned during pre-training to perform ICL. In this paper, we empirically explore these three hypotheses that explain LLMs' ability to learn in context with a suite of experiments derived from common text classification tasks. We invalidate the first two hypotheses with counterexamples and provide evidence in support of the last hypothesis. Our results suggest an LLM could learn a novel task in context via composing tasks learned during pre-training.

------------

`[2406.04220] BEADs: Bias Evaluation Across Domains <https://arxiv.org/abs/2406.04220>`__ BEADs:跨域偏差评估

::

    Thu, 6 Jun 2024 16:18:30 GMT
    Shaina Raza and Mizanur Rahman and Michael R. Zhang

Recent improvements in large language models (LLMs) have significantly enhanced natural language processing (NLP) applications. However, these models can also inherit and perpetuate biases from their training data. Addressing this issue is crucial, yet many existing datasets do not offer evaluation across diverse NLP tasks. To tackle this, we introduce the Bias Evaluations Across Domains (BEADs) dataset, designed to support a wide range of NLP tasks, including text classification, bias entity recognition, bias quantification, and benign language generation. BEADs uses AI-driven annotation combined with experts' verification to provide reliable labels. This method overcomes the limitations of existing datasets that typically depend on crowd-sourcing, expert-only annotations with limited bias evaluations, or unverified AI labeling. Our empirical analysis shows that BEADs is effective in detecting and reducing biases across different language models, with smaller models fine-tuned on BEADs often outperforming LLMs in bias classification tasks.
However, these models may still exhibit biases towards certain demographics.
Fine-tuning LLMs with our benign language data also reduces biases while preserving the models' knowledge. Our findings highlight the importance of comprehensive bias evaluation and the potential of targeted fine-tuning for reducing the bias of LLMs. We are making BEADs publicly available at https://huggingface.co/datasets/shainar/BEAD Warning: This paper contains examples that may be considered offensive.

------------

`[2406.04267] Transformers need glasses! Information over-squashing in language tasks <https://arxiv.org/abs/2406.04267>`__ 变形金刚需要眼镜!语言任务中的信息过度压缩

::

    Thu, 6 Jun 2024 17:14:44 GMT
    Federico Barbero, Andrea Banino, Steven Kapturowski, Dharshan Kumaran, Jo\~ao G.M. Ara\'ujo, Alex Vitvitskyi, Razvan Pascanu, Petar Veli\v{c}kovi\'c

We study how information propagates in decoder-only Transformers, which are the architectural backbone of most existing frontier large language models (LLMs). We rely on a theoretical signal propagation analysis -- specifically, we analyse the representations of the last token in the final layer of the Transformer, as this is the representation used for next-token prediction. Our analysis reveals a representational collapse phenomenon: we prove that certain distinct sequences of inputs to the Transformer can yield arbitrarily close representations in the final token. This effect is exacerbated by the low-precision floating-point formats frequently used in modern LLMs. As a result, the model is provably unable to respond to these sequences in different ways -- leading to errors in, e.g., tasks involving counting or copying.
Further, we show that decoder-only Transformer language models can lose sensitivity to specific tokens in the input, which relates to the well-known phenomenon of over-squashing in graph neural networks. We provide empirical evidence supporting our claims on contemporary LLMs. Our theory also points to simple solutions towards ameliorating these issues.

------------

`[2406.04278] Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People <https://arxiv.org/abs/2406.04278>`__ 通过对人进行采样来描述人类和llm对话语调的相似性和差异性

::

    Thu, 6 Jun 2024 17:26:00 GMT
    Dun-Ming Huang, Pol Van Rijn, Ilia Sucholutsky, Raja Marjieh, Nori Jacoby

Conversational tones -- the manners and attitudes in which speakers communicate -- are essential to effective communication. Amidst the increasing popularization of Large Language Models (LLMs) over recent years, it becomes necessary to characterize the divergences in their conversational tones relative to humans. However, existing investigations of conversational modalities rely on pre-existing taxonomies or text corpora, which suffer from experimenter bias and may not be representative of real-world distributions for the studies' psycholinguistic domains. Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone. We run 100 iterations of this process with human participants and GPT-4, then obtain a dataset of sentences and frequent conversational tones. In an additional experiment, humans and GPT-4 annotated all sentences with all tones. With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4. This work demonstrates how combining ideas from machine learning and cognitive science can address challenges in human-computer interactions.

------------

`[2406.04289] What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages <https://arxiv.org/abs/2406.04289>`__ 哪些语言易于语言建模?从学习概率正则语言的角度来看

::

    Thu, 6 Jun 2024 17:34:24 GMT
    Nadav Borenstein, Anej Svete, Robin Chan, Josef Valvoda, Franz Nowak, Isabelle Augenstein, Eleanor Chodroff, Ryan Cotterell

What can large language models learn? By definition, language models (LM) are distributions over strings. Therefore, an intuitive way of addressing the above question is to formalize it as a matter of learnability of classes of distributions over strings. While prior work in this direction focused on assessing the theoretical limits, in contrast, we seek to understand the empirical learnability. Unlike prior empirical work, we evaluate neural LMs on their home turf-learning probabilistic languages-rather than as classifiers of formal languages. In particular, we investigate the learnability of regular LMs (RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs as a function of various complexity parameters of the RLM and the hidden state size of the neural LM. We find that the RLM rank, which corresponds to the size of linear space spanned by the logits of its conditional distributions, and the expected length of sampled strings are strong and significant predictors of learnability for both RNNs and Transformers. Several other predictors also reach significance, but with differing patterns between RNNs and Transformers.

------------

`[2406.04331] PaCE: Parsimonious Concept Engineering for Large Language Models <https://arxiv.org/abs/2406.04331>`__ PaCE:大型语言模型的简约概念工程

::

    Thu, 6 Jun 2024 17:59:10 GMT
    Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, Ren\'e Vidal

Large Language Models (LLMs) are being used for a wide variety of tasks.
While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable output, via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose Parsimonious Concept Engineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Then, given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Finally, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activation as a linear combination of the benign and undesirable components. By removing the latter ones from the activation, we reorient the behavior of LLMs towards alignment goals. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities.

------------

`[2406.03505] Dynamic and Adaptive Feature Generation with LLM <https://arxiv.org/abs/2406.03505>`__ 基于LLM的动态和自适应特征生成

::

    Tue, 4 Jun 2024 20:32:14 GMT
    Xinhao Zhang, Jinghan Zhang, Banafsheh Rekabdar, Yuanchun Zhou, Pengfei Wang, Kunpeng Liu

The representation of feature space is a crucial environment where data points get vectorized and embedded for upcoming modeling. Thus the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and draws advantages over strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.

------------

`[2406.03614] Advancing Anomaly Detection: Non-Semantic Financial Data Encoding with LLMs <https://arxiv.org/abs/2406.03614>`__ 推进异常检测:基于LLMs的非语义金融数据编码

::

    Wed, 5 Jun 2024 20:19:09 GMT
    Alexander Bakumenko (1), Kate\v{r}ina Hlav\'a\v{c}kov\'a-Schindler (2), Claudia Plant (2), and Nina C. Hubig (1) ((1) Clemson University, USA, (2) University of Vienna, Austria)

Detecting anomalies in general ledger data is of utmost importance to ensure trustworthiness of financial records. Financial audits increasingly rely on machine learning (ML) algorithms to identify irregular or potentially fraudulent journal entries, each characterized by a varying number of transactions. In machine learning, heterogeneity in feature dimensions adds significant complexity to data analysis. In this paper, we introduce a novel approach to anomaly detection in financial data using Large Language Models (LLMs) embeddings. To encode non-semantic categorical data from real-world financial records, we tested 3 pre-trained general purpose sentence-transformer models. For the downstream classification task, we implemented and evaluated 5 optimized ML models including Logistic Regression, Random Forest, Gradient Boosting Machines, Support Vector Machines, and Neural Networks. Our experiments demonstrate that LLMs contribute valuable information to anomaly detection as our models outperform the baselines, in selected settings even by a large margin. The findings further underscore the effectiveness of LLMs in enhancing anomaly detection in financial journal entries, particularly by tackling feature sparsity. We discuss a promising perspective on using LLM embeddings for non-semantic data in the financial context and beyond.

------------

`[2406.03707] What Should Embeddings Embed? Autoregressive Models Represent Latent Generating Distributions <https://arxiv.org/abs/2406.03707>`__ 嵌入应该嵌入什么?自回归模型代表潜在的生成分布

::

    Thu, 6 Jun 2024 03:06:46 GMT
    Liyi Zhang, Michael Y. Li, Thomas L. Griffiths

Autoregressive language models have demonstrated a remarkable ability to extract latent structure from text. The embeddings from large language models have been shown to capture aspects of the syntax and semantics of language. But what {\em should} embeddings represent? We connect the autoregressive prediction objective to the idea of constructing predictive sufficient statistics to summarize the information contained in a sequence of observations, and use this connection to identify three settings where the optimal content of embeddings can be identified: independent identically distributed data, where the embedding should capture the sufficient statistics of the data; latent state models, where the embedding should encode the posterior distribution over states given the data; and discrete hypothesis spaces, where the embedding should reflect the posterior distribution over hypotheses given the data. We then conduct empirical probing studies to show that transformers encode these three kinds of latent generating distributions, and that they perform well in out-of-distribution cases and without token memorization in these settings.

------------

`[2406.03777] Empirical Guidelines for Deploying LLMs onto Resource-constrained Edge Devices <https://arxiv.org/abs/2406.03777>`__ 

::

    Thu, 6 Jun 2024 06:41:53 GMT
    Ruiyang Qin, Dancheng Liu, Zheyu Yan, Zhaoxuan Tan, Zixuan Pan, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Jinjun Xiong, Yiyu Shi

The scaling laws have become the de facto guidelines for designing large language models (LLMs), but they were studied under the assumption of unlimited computing resources for both training and inference. As LLMs are increasingly used as personalized intelligent assistants, their customization (i.e., learning through fine-tuning) and deployment onto resource-constrained edge devices will become more and more prevalent. An urging but open question is how a resource-constrained computing environment would affect the design choices for a personalized LLM. We study this problem empirically in this work. In particular, we consider the tradeoffs among a number of key design factors and their intertwined impacts on learning efficiency and accuracy. The factors include the learning methods for LLM customization, the amount of personalized data used for learning customization, the types and sizes of LLMs, the compression methods of LLMs, the amount of time afforded to learn, and the difficulty levels of the target use cases. Through extensive experimentation and benchmarking, we draw a number of surprisingly insightful guidelines for deploying LLMs onto resource-constrained devices. For example, an optimal choice between parameter learning and RAG may vary depending on the difficulty of the downstream task, the longer fine-tuning time does not necessarily help the model, and a compressed LLM may be a better choice than an uncompressed LLM to learn from limited personalized data.

------------

`[2406.04081] Bootstrapping Expectiles in Reinforcement Learning <https://arxiv.org/abs/2406.04081>`__ 强化学习中的自助法预期

::

    Thu, 6 Jun 2024 13:51:39 GMT
    Pierre Clavier, Emmanuel Rachelson, Erwan Le Pennec, Matthieu Geist

Many classic Reinforcement Learning (RL) algorithms rely on a Bellman operator, which involves an expectation over the next states, leading to the concept of bootstrapping. To introduce a form of pessimism, we propose to replace this expectation with an expectile. In practice, this can be very simply done by replacing the $L_2$ loss with a more general expectile loss for the critic. Introducing pessimism in RL is desirable for various reasons, such as tackling the overestimation problem (for which classic solutions are double Q-learning or the twin-critic approach of TD3) or robust RL (where transitions are adversarial). We study empirically these two cases. For the overestimation problem, we show that the proposed approach, ExpectRL, provides better results than a classic twin-critic. On robust RL benchmarks, involving changes of the environment, we show that our approach is more robust than classic RL algorithms. We also introduce a variation of ExpectRL combined with domain randomization which is competitive with state-of-the-art robust RL agents.
Eventually, we also extend \ExpectRL with a mechanism for choosing automatically the expectile value, that is the degree of pessimism

------------

`[2406.04088] Deterministic Uncertainty Propagation for Improved Model-Based Offline Reinforcement Learning <https://arxiv.org/abs/2406.04088>`__ 基于确定性不确定性传播的改进模型离线强化学习

::

    Thu, 6 Jun 2024 13:58:41 GMT
    Abdullah Akg\"ul, Manuel Hau{\ss}mann, Melih Kandemir

Current approaches to model-based offline Reinforcement Learning (RL) often incorporate uncertainty-based reward penalization to address the distributional shift problem. While these approaches have achieved some success, we argue that this penalization introduces excessive conservatism, potentially resulting in suboptimal policies through underestimation. We identify as an important cause of over-penalization the lack of a reliable uncertainty estimator capable of propagating uncertainties in the Bellman operator. The common approach to calculating the penalty term relies on sampling-based uncertainty estimation, resulting in high variance. To address this challenge, we propose a novel method termed Moment Matching Offline Model-Based Policy Optimization (MOMBO).
MOMBO learns a Q-function using moment matching, which allows us to deterministically propagate uncertainties through the Q-function. We evaluate MOMBO's performance across various environments and demonstrate empirically that MOMBO is a more stable and sample-efficient approach.

------------

`[2406.04274] Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models <https://arxiv.org/abs/2406.04274>`__ 对抗性评论的自玩:可证明和可扩展的语言模型离线对齐

::

    Thu, 6 Jun 2024 17:23:49 GMT
    Xiang Ji, Sanjeev Kulkarni, Mengdi Wang, Tengyang Xie

This work studies the challenge of aligning large language models (LLMs) with offline preference data. We focus on alignment by Reinforcement Learning from Human Feedback (RLHF) in particular. While popular preference optimization methods exhibit good empirical performance in practice, they are not theoretically guaranteed to converge to the optimal policy and can provably fail when the data coverage is sparse by classical offline reinforcement learning (RL) results. On the other hand, a recent line of work has focused on theoretically motivated preference optimization methods with provable guarantees, but these are not computationally efficient for large-scale applications like LLM alignment. To bridge this gap, we propose SPAC, a new offline preference optimization method with self-play, inspired by the on-average pessimism technique from the offline RL literature, to be the first provable and scalable approach to LLM alignment. We both provide theoretical analysis for its convergence under single-policy concentrability for the general function approximation setting and demonstrate its competitive empirical performance for LLM alignment on a 7B Mistral model with Open LLM Leaderboard evaluations.

------------

`[2406.04276] Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next Generation Networks <https://arxiv.org/abs/2406.04276>`__ 生成式AI-in-the-loop:将llm和GPTs集成到下一代网络中

::

    Thu, 6 Jun 2024 17:25:07 GMT
    Han Zhang, Akram Bin Sediq, Ali Afana and Melike Erol-Kantarci

In recent years, machine learning (ML) techniques have created numerous opportunities for intelligent mobile networks and have accelerated the automation of network operations. However, complex network tasks may involve variables and considerations even beyond the capacity of traditional ML algorithms. On the other hand, large language models (LLMs) have recently emerged, demonstrating near-human-level performance in cognitive tasks across various fields. However, they remain prone to hallucinations and often lack common sense in basic tasks. Therefore, they are regarded as assistive tools for humans. In this work, we propose the concept of "generative AI-in-the-loop" and utilize the semantic understanding, context awareness, and reasoning abilities of LLMs to assist humans in handling complex or unforeseen situations in mobile communication networks. We believe that combining LLMs and ML models allows both to leverage their respective capabilities and achieve better results than either model alone. To support this idea, we begin by analyzing the capabilities of LLMs and compare them with traditional ML algorithms. We then explore potential LLM-based applications in line with the requirements of next-generation networks. We further examine the integration of ML and LLMs, discussing how they can be used together in mobile networks. Unlike existing studies, our research emphasizes the fusion of LLMs with traditional ML-driven next-generation networks and serves as a comprehensive refinement of existing surveys. Finally, we provide a case study to enhance ML-based network intrusion detection with synthesized data generated by LLMs. Our case study further demonstrates the advantages of our proposed idea.

------------

`[2406.04306] Semantically Diverse Language Generation for Uncertainty Estimation in Language Models <https://arxiv.org/abs/2406.04306>`__ 面向语言模型不确定性估计的语义多样性语言生成

::

    Thu, 6 Jun 2024 17:53:34 GMT
    Lukas Aichberger, Kajetan Schweighofer, Mykyta Ielanskyi, Sepp Hochreiter

Large language models (LLMs) can suffer from hallucinations when generating text. These hallucinations impede various applications in society and industry by making LLMs untrustworthy. Current LLMs generate text in an autoregressive fashion by predicting and appending text tokens. When an LLM is uncertain about the semantic meaning of the next tokens to generate, it is likely to start hallucinating. Thus, it has been suggested that hallucinations stem from predictive uncertainty. We introduce Semantically Diverse Language Generation (SDLG) to quantify predictive uncertainty in LLMs. SDLG steers the LLM to generate semantically diverse yet likely alternatives for an initially generated text. This approach provides a precise measure of aleatoric semantic uncertainty, detecting whether the initial text is likely to be hallucinated.
Experiments on question-answering tasks demonstrate that SDLG consistently outperforms existing methods while being the most computationally efficient, setting a new standard for uncertainty estimation in LLMs.

------------

`[2406.04344] Verbalized Machine Learning: Revisiting Machine Learning with Language Models <https://arxiv.org/abs/2406.04344>`__ 语言化机器学习:用语言模型重新审视机器学习

::

    Thu, 6 Jun 2024 17:59:56 GMT
    Tim Z. Xiao, Robert Bamler, Bernhard Sch\"olkopf, Weiyang Liu

Motivated by the large progress made by large language models (LLMs), we introduce the framework of verbalized machine learning (VML). In contrast to conventional machine learning models that are typically optimized over a continuous parameter space, VML constrains the parameter space to be human-interpretable natural language. Such a constraint leads to a new perspective of function approximation, where an LLM with a text prompt can be viewed as a function parameterized by the text prompt. Guided by this perspective, we revisit classical machine learning problems, such as regression and classification, and find that these problems can be solved by an LLM-parameterized learner and optimizer. The major advantages of VML include (1) easy encoding of inductive bias: prior knowledge about the problem and hypothesis class can be encoded in natural language and fed into the LLM-parameterized learner; (2) automatic model class selection: the optimizer can automatically select a concrete model class based on data and verbalized prior knowledge, and it can update the model class during training; and (3) interpretable learner updates: the LLM-parameterized optimizer can provide explanations for why each learner update is performed. We conduct several studies to empirically evaluate the effectiveness of VML, and hope that VML can serve as a stepping stone to stronger interpretability and trustworthiness in ML.

------------

`[2406.03718] Generalization-Enhanced Code Vulnerability Detection via Multi-Task Instruction Fine-Tuning <https://arxiv.org/abs/2406.03718>`__ 基于多任务指令微调的泛化增强代码漏洞检测

::

    Thu, 6 Jun 2024 03:29:05 GMT
    Xiaohu Du, Ming Wen, Jiahao Zhu, Zifan Xie, Bin Ji, Huijun Liu, Xuanhua Shi and Hai Jin

Code Pre-trained Models (CodePTMs) based vulnerability detection have achieved promising results over recent years. However, these models struggle to generalize as they typically learn superficial mapping from source code to labels instead of understanding the root causes of code vulnerabilities, resulting in poor performance in real-world scenarios beyond the training instances. To tackle this challenge, we introduce VulLLM, a novel framework that integrates multi-task learning with Large Language Models (LLMs) to effectively mine deep-seated vulnerability features. Specifically, we construct two auxiliary tasks beyond the vulnerability detection task. First, we utilize the vulnerability patches to construct a vulnerability localization task.
Second, based on the vulnerability features extracted from patches, we leverage GPT-4 to construct a vulnerability interpretation task. VulLLM innovatively augments vulnerability classification by leveraging generative LLMs to understand complex vulnerability patterns, thus compelling the model to capture the root causes of vulnerabilities rather than overfitting to spurious features of a single task. The experiments conducted on six large datasets demonstrate that VulLLM surpasses seven state-of-the-art models in terms of effectiveness, generalization, and robustness.

------------

`[2406.04337] Coherent Zero-Shot Visual Instruction Generation <https://arxiv.org/abs/2406.04337>`__ 连贯零样本视觉指令生成

::

    Thu, 6 Jun 2024 17:59:44 GMT
    Quynh Phung, Songwei Ge, Jia-Bin Huang

Despite the advances in text-to-image synthesis, particularly with diffusion models, generating visual instructions that require consistent representation and smooth state transitions of objects across sequential steps remains a formidable challenge. This paper introduces a simple, training-free framework to tackle the issues, capitalizing on the advancements in diffusion models and large language models (LLMs). Our approach systematically integrates text comprehension and image generation to ensure visual instructions are visually appealing and maintain consistency and accuracy throughout the instruction sequence. We validate the effectiveness by testing multi-step instructions and comparing the text alignment and consistency with several baselines. Our experiments show that our approach can visualize coherent and visually pleasing instructions

------------

`[2406.03706] Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model <https://arxiv.org/abs/2406.03706>`__ 利用多模态上下文和大型语言模型改进基于音频编解码器的零样本文本到语音合成

::

    Thu, 6 Jun 2024 03:06:45 GMT
    Jinlong Xue, Yayue Deng, Yicheng Han, Yingming Gao, Ya Li

Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt.
However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios.
In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.

------------

`[2406.03243] Llumnix: Dynamic Scheduling for Large Language Model Serving <https://arxiv.org/abs/2406.03243>`__ lumnix:大型语言模型服务的动态调度

::

    Wed, 5 Jun 2024 13:20:18 GMT
    Biao Sun, Ziming Huang, Hanyu Zhao, Wencong Xiao, Xinyi Zhang, Yong Li, Wei Lin

Inference serving for large language models (LLMs) is the key to unleashing their potential in people's daily lives. However, efficient LLM serving remains challenging today because the requests are inherently heterogeneous and unpredictable in terms of resource and latency requirements, as a result of the diverse applications and the dynamic execution nature of LLMs. Existing systems are fundamentally limited in handling these characteristics and cause problems such as severe queuing delays, poor tail latencies, and SLO violations.
We introduce Llumnix, an LLM serving system that reacts to such heterogeneous and unpredictable requests by runtime rescheduling across multiple model instances. Similar to context switching across CPU cores in modern operating systems, Llumnix reschedules requests to improve load balancing and isolation, mitigate resource fragmentation, and differentiate request priorities and SLOs.
Llumnix implements the rescheduling with an efficient and scalable live migration mechanism for requests and their in-memory states, and exploits it in a dynamic scheduling policy that unifies the multiple rescheduling scenarios elegantly. Our evaluations show that Llumnix improves tail latencies by an order of magnitude, accelerates high-priority requests by up to 1.5x, and delivers up to 36% cost savings while achieving similar tail latencies, compared against state-of-the-art LLM serving systems. Llumnix is publicly available at https://github.com/AlibabaPAI/llumnix.

------------

`[2406.03628] Synthetic Oversampling: Theory and A Practical Approach Using LLMs to Address Data Imbalance <https://arxiv.org/abs/2406.03628>`__ 合成过采样:用llm解决数据不平衡的理论和实践方法

::

    Wed, 5 Jun 2024 21:24:26 GMT
    Ryumei Nakada, Yichen Xu, Lexin Li, Linjun Zhang

Imbalanced data and spurious correlations are common challenges in machine learning and data science. Oversampling, which artificially increases the number of instances in the underrepresented classes, has been widely adopted to tackle these challenges. In this article, we introduce OPAL (\textbf{O}versam\textbf{P}ling with \textbf{A}rtificial \textbf{L}LM-generated data), a systematic oversampling approach that leverages the capabilities of large language models (LLMs) to generate high-quality synthetic data for minority groups. Recent studies on synthetic data generation using deep generative models mostly target prediction tasks. Our proposal differs in that we focus on handling imbalanced data and spurious correlations. More importantly, we develop a novel theory that rigorously characterizes the benefits of using the synthetic data, and shows the capacity of transformers in generating high-quality synthetic data for both labels and covariates. We further conduct intensive numerical experiments to demonstrate the efficacy of our proposed approach compared to some representative alternative solutions.

------------

`[2406.03636] Synthetic Programming Elicitation and Repair for Text-to-Code in Very Low-Resource Programming Languages <https://arxiv.org/abs/2406.03636>`__ 低资源编程语言文本到代码的综合编程启发与修复

::

    Wed, 5 Jun 2024 22:16:19 GMT
    Federico Mora, Justin Wong, Haley Lepe, Sahil Bhatia, Karim Elmaaroufi, George Varghese, Joseph E. Gonzalez, Elizabeth Polgreen, Sanjit A. Seshia

Recent advances in large language models (LLMs) for code applications have demonstrated remarkable zero-shot fluency and instruction following on challenging code related tasks ranging from test case generation to self-repair. Unsurprisingly, however, models struggle to compose syntactically valid programs in programming languages unrepresented in pre-training, referred to as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial settings including domain-specific languages for internal to tools and tool-chains and legacy languages. Inspired by an HCI technique called natural program elicitation, we propose designing an intermediate language that LLMs ``naturally'' know how to use and which can be automatically compiled to the target VLPL. Specifically, we introduce synthetic programming elicitation and compilation (SPEAK), an approach that enables LLMs to generate syntactically valid code even for VLPLs. We empirically evaluate the performance of SPEAK in a case study and find that, compared to existing retrieval and fine-tuning baselines, SPEAK produces syntactically correct programs more frequently without sacrificing semantic correctness.

------------

`[2406.03757] RoboCoder: Robotic Learning from Basic Skills to General Tasks with Large Language Models <https://arxiv.org/abs/2406.03757>`__ RoboCoder:基于大型语言模型的从基本技能到一般任务的机器人学习

::

    Thu, 6 Jun 2024 05:41:47 GMT
    Jingyao Li, Pengguang Chen, Sitong Wu, Chuanyang Zheng, Hong Xu, Jiaya Jia

The emergence of Large Language Models (LLMs) has improved the prospects for robotic tasks. However, existing benchmarks are still limited to single tasks with limited generalization capabilities. In this work, we introduce a comprehensive benchmark and an autonomous learning framework, RoboCoder aimed at enhancing the generalization capabilities of robots in complex environments.
Unlike traditional methods that focus on single-task learning, our research emphasizes the development of a general-purpose robotic coding algorithm that enables robots to leverage basic skills to tackle increasingly complex tasks.
The newly proposed benchmark consists of 80 manually designed tasks across 7 distinct entities, testing the models' ability to learn from minimal initial mastery. Initial testing revealed that even advanced models like GPT-4 could only achieve a 47% pass rate in three-shot scenarios with humanoid entities. To address these limitations, the RoboCoder framework integrates Large Language Models (LLMs) with a dynamic learning system that uses real-time environmental feedback to continuously update and refine action codes. This adaptive method showed a remarkable improvement, achieving a 36% relative improvement. Our codes will be released.

------------

`[2309.11361] Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG) Using LLM <https://arxiv.org/abs/2309.11361>`__ 材料科学知识图谱问答(KGQA4MAT):用LLM为金属有机框架知识图谱(MOF-KG)开发自然语言界面

::

    replaced with revised version Thu, 6 Jun 2024 15:35:09 GMT
    Submission history From: Yuan An [view email]
    [v1] Wed, 20 Sep 2023 14:43:43 UTC (323 KB)
    [v2] Thu, 6 Jun 2024 15:35:09 UTC (323 KB)
    Yuan An, Jane Greenberg, Alex Kalinowski, Xintong Zhao, Xiaohua Hu, Fernando J. Uribe-Romo, Kyle Langlois, Jacob Furst, Diego A. G\'omez-Gualdr\'on

We present a comprehensive benchmark dataset for Knowledge Graph Question Answering in Materials Science (KGQA4MAT), with a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has been constructed by integrating structured databases and knowledge extracted from the literature. To enhance MOF-KG accessibility for domain experts, we aim to develop a natural language interface for querying the knowledge graph. We have developed a benchmark comprised of 161 complex questions involving comparison, aggregation, and complicated graph structures. Each question is rephrased in three additional variations, resulting in 644 questions and 161 KG queries. To evaluate the benchmark, we have developed a systematic approach for utilizing the LLM, ChatGPT, to translate natural language questions into formal KG queries. We also apply the approach to the well-known QALD-9 dataset, demonstrating ChatGPT's potential in addressing KGQA issues for different platforms and query languages. The benchmark and the proposed approach aim to stimulate further research and development of user-friendly and efficient interfaces for querying domain-specific materials science knowledge graphs, thereby accelerating the discovery of novel materials.

------------

`[2405.04776] Chain of Thoughtlessness? An Analysis of CoT in Planning <https://arxiv.org/abs/2405.04776>`__ 轻率的连锁反应?规划中的CoT分析

::

    replaced with revised version Thu, 6 Jun 2024 02:44:52 GMT
    Submission history From: Karthik Valmeekam [view email]
    [v1] Wed, 8 May 2024 02:48:28 UTC (1,475 KB)
    [v2] Thu, 6 Jun 2024 02:44:52 UTC (8,983 KB)
    Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati

Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting-a method of demonstrating solution procedures-with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem. This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples. We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes. Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.

------------

`[2405.17345] Exploring and steering the moral compass of Large Language Models <https://arxiv.org/abs/2405.17345>`__ 探索和引导大型语言模型的道德指南针

::

    replaced with revised version Thu, 6 Jun 2024 11:53:23 GMT
    Submission history From: Alejandro Tlaie [view email]
    [v1] Mon, 27 May 2024 16:49:22 UTC (749 KB)
    [v2] Thu, 6 Jun 2024 11:53:23 UTC (801 KB)
    Alejandro Tlaie

Large Language Models (LLMs) have become central to advancing automation and decision-making across various sectors, raising significant ethical questions. This study proposes a comprehensive comparative analysis of the most advanced LLMs to assess their moral profiles. We subjected several state-of-the-art models to a selection of ethical dilemmas and found that all the proprietary ones are mostly utilitarian and all of the open-weights ones align mostly with values-based ethics. Furthermore, when using the Moral Foundations Questionnaire, all models we probed - except for Llama 2-7B - displayed a strong liberal bias. Lastly, in order to causally intervene in one of the studied models, we propose a novel similarity-specific activation steering technique. Using this method, we were able to reliably steer the model's moral compass to different ethical schools. All of these results showcase that there is an ethical dimension in already deployed LLMs, an aspect that is generally overlooked.

------------

`[2306.09782] Full Parameter Fine-tuning for Large Language Models with Limited Resources <https://arxiv.org/abs/2306.09782>`__ 资源有限的大型语言模型的全参数微调

::

    replaced with revised version Thu, 6 Jun 2024 13:22:26 GMT
    Submission history From: Kai Lv [view email]
    [v1] Fri, 16 Jun 2023 11:37:15 UTC (1,128 KB)
    [v2] Thu, 6 Jun 2024 13:22:26 UTC (8,677 KB)
    Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, Xipeng Qiu

Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.Code and data are available at this https URL.

------------

`[2309.08047] Bias in News Summarization: Measures, Pitfalls and Corpora <https://arxiv.org/abs/2309.08047>`__ 新闻摘要中的偏见:措施、陷阱和语料库

::

    replaced with revised version Thu, 6 Jun 2024 11:49:26 GMT
    Submission history From: Julius Steen [view email]
    [v1] Thu, 14 Sep 2023 22:20:27 UTC (43 KB)
    [v2] Fri, 16 Feb 2024 12:56:31 UTC (92 KB)
    [v3] Thu, 6 Jun 2024 11:49:26 UTC (95 KB)
    Julius Steen, Katja Markert

Summarization is an important application of large language models (LLMs). Most previous evaluation of summarization models has focused on their content selection, faithfulness, grammaticality and coherence. However, it is well known that LLMs can reproduce and reinforce harmful social biases. This raises the question: Do biases affect model outputs in a constrained setting like summarization? To help answer this question, we first motivate and introduce a number of definitions for biased behaviours in summarization models, along with practical operationalizations. Since we find that biases inherent to input documents can confound bias analysis in summaries, we propose a method to generate input documents with carefully controlled demographic attributes. This allows us to study summarizer behavior in a controlled setting, while still working with realistic input documents. We measure gender bias in English summaries generated by both purpose-built summarization models and general purpose chat models as a case study. We find content selection in single document summarization to be largely unaffected by gender bias, while hallucinations exhibit evidence of bias. To demonstrate the generality of our approach, we additionally investigate racial bias, including intersectional settings.

------------

`[2310.00160] Self-Specialization: Uncovering Latent Expertise within Large Language Models <https://arxiv.org/abs/2310.00160>`__ 自我专业化:揭示大型语言模型中的潜在专业知识

::

    replaced with revised version Wed, 5 Jun 2024 19:48:45 GMT
    Submission history From: Junmo Kang [view email]
    [v1] Fri, 29 Sep 2023 21:53:46 UTC (3,674 KB)
    [v2] Wed, 5 Jun 2024 19:48:45 UTC (3,337 KB)
    Junmo Kang, Hongyin Luo, Yada Zhu, Jacob Hansen, James Glass, David Cox, Alan Ritter, Rogerio Feris, Leonid Karlinsky

Recent works have demonstrated the effectiveness of self-alignment in which a large language model is aligned to follow general instructions using instructional data generated from the model itself starting from a handful of human-written seeds. Instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine, finance). As a preliminary, we quantitively show the marginal effect that generic instruction-following training has on downstream expert domains' performance. To remedy this, we propose self-specialization - allowing for effective model specialization while achieving cross-task generalization by leveraging only a few labeled seeds. Self-specialization offers a data- and parameter-efficient way of "carving out" an expert model out of a generalist pre-trained LLM. Exploring a variety of popular open large models as a base for specialization, our experimental results in both biomedical and financial domains show that our self-specialized models outperform their base models by a large margin, and even larger models that are generally instruction-tuned or that have been adapted to the target domain by other means.

------------

`[2310.13571] Why Can Large Language Models Generate Correct Chain-of-Thoughts? <https://arxiv.org/abs/2310.13571>`__ 为什么大型语言模型可以生成正确的思维链?

::

    replaced with revised version Thu, 6 Jun 2024 12:18:56 GMT
    Submission history From: Haitham Bou Ammar PhD [view email]
    [v1] Fri, 20 Oct 2023 15:09:46 UTC (62 KB)
    [v2] Mon, 30 Oct 2023 09:47:39 UTC (63 KB)
    [v3] Wed, 29 May 2024 09:23:59 UTC (63 KB)
    [v4] Thu, 6 Jun 2024 12:18:56 UTC (64 KB)
    Rasul Tutunov, Antoine Grosnit, Juliusz Ziomek, Jun Wang, Haitham Bou-Ammar

This paper delves into the capabilities of large language models (LLMs), specifically focusing on advancing the theoretical comprehension of chain-of-thought prompting. We investigate how LLMs can be effectively induced to generate a coherent chain of thoughts. To achieve this, we introduce a two-level hierarchical graphical model tailored for natural language generation. Within this framework, we establish a compelling geometrical convergence rate that gauges the likelihood of an LLM-generated chain of thoughts compared to those originating from the true language. Our findings provide a theoretical justification for the ability of LLMs to produce the correct sequence of thoughts (potentially) explaining performance gains in tasks demanding reasoning skills.

------------

`[2311.09033] MELA: Multilingual Evaluation of Linguistic Acceptability <https://arxiv.org/abs/2311.09033>`__ MELA:语言可接受性的多语言评估

::

    replaced with revised version Thu, 6 Jun 2024 12:31:51 GMT
    Submission history From: Yikang Liu [view email]
    [v1] Wed, 15 Nov 2023 15:25:28 UTC (472 KB)
    [v2] Sun, 3 Mar 2024 06:37:11 UTC (677 KB)
    [v3] Thu, 6 Jun 2024 12:31:51 UTC (559 KB)
    Ziyin Zhang and Yikang Liu and Weifang Huang and Junyu Mao and Rui Wang and Hai Hu

In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability -- MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language -- Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks. Our data is available at this https URL.

------------

`[2311.09213] GENEVA: GENErating and Visualizing branching narratives using LLMs <https://arxiv.org/abs/2311.09213>`__ GENEVA:使用llm生成和可视化分支叙述

::

    replaced with revised version Wed, 5 Jun 2024 20:59:27 GMT
    Submission history From: Sudha Rao [view email]
    [v1] Wed, 15 Nov 2023 18:55:45 UTC (7,969 KB)
    [v2] Mon, 3 Jun 2024 21:01:51 UTC (8,842 KB)
    [v3] Wed, 5 Jun 2024 20:59:27 UTC (8,842 KB)
    Jorge Leandro, Sudha Rao, Michael Xu, Weijia Xu, Nebosja Jojic, Chris Brockett, and Bill Dolan

Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The narratives of these may take years to write and typically involve a large creative team. In this work, we demonstrate the potential of large generative text models to assist this process. \textbf{GENEVA}, a prototype tool, generates a rich narrative graph with branching and reconverging storylines that match a high-level narrative description and constraints provided by the designer. A large language model (LLM), GPT-4, is used to generate the branching narrative and to render it in a graph format in a two-step process. We illustrate the use of GENEVA in generating new branching narratives for four well-known stories under different contextual constraints. This tool has the potential to assist in game development, simulations, and other applications with game-like properties.

------------

`[2311.09832] WatME: Towards Lossless Watermarking Through Lexical Redundancy <https://arxiv.org/abs/2311.09832>`__ WatME:通过词汇冗余实现无损水印

::

    replaced with revised version Thu, 6 Jun 2024 13:17:43 GMT
    Submission history From: Liang Chen [view email]
    [v1] Thu, 16 Nov 2023 11:58:31 UTC (7,144 KB)
    [v2] Fri, 16 Feb 2024 14:58:53 UTC (8,152 KB)
    [v3] Thu, 6 Jun 2024 13:17:43 UTC (7,881 KB)
    Liang Chen, Yatao Bian, Yang Deng, Deng Cai, Shuaiyi Li, Peilin Zhao, Kam-fai Wong

Text watermarking has emerged as a pivotal technique for identifying machine-generated text. However, existing methods often rely on arbitrary vocabulary partitioning during decoding to embed watermarks, which compromises the availability of suitable tokens and significantly degrades the quality of responses. This study assesses the impact of watermarking on different capabilities of large language models (LLMs) from a cognitive science lens. Our finding highlights a significant disparity; knowledge recall and logical reasoning are more adversely affected than language generation. These results suggest a more profound effect of watermarking on LLMs than previously understood. To address these challenges, we introduce Watermarking with Mutual Exclusion (WatME), a novel approach leveraging linguistic prior knowledge of inherent lexical redundancy in LLM vocabularies to seamlessly integrate watermarks. Specifically, WatME dynamically optimizes token usage during the decoding process by applying a mutually exclusive rule to the identified lexical redundancies. This strategy effectively prevents the unavailability of appropriate tokens and preserves the expressive power of LLMs. We provide both theoretical analysis and empirical evidence showing that WatME effectively preserves the diverse capabilities of LLMs while ensuring watermark detectability.

------------

`[2312.08800] Evaluating Large Language Models for Health-related Queries with Presuppositions <https://arxiv.org/abs/2312.08800>`__ 基于预设的健康相关查询大型语言模型评估

::

    replaced with revised version Thu, 6 Jun 2024 09:27:29 GMT
    Submission history From: Navreet Kaur [view email]
    [v1] Thu, 14 Dec 2023 10:35:13 UTC (2,173 KB)
    [v2] Thu, 6 Jun 2024 09:27:29 UTC (2,579 KB)
    Navreet Kaur and Monojit Choudhury and Danish Pruthi

As corporations rush to integrate large language models (LLMs) to their search offerings, it is critical that they provide factually accurate information that is robust to any presuppositions that a user may express. In this work, we introduce UPHILL, a dataset consisting of health-related queries with varying degrees of presuppositions. Using UPHILL, we evaluate the factual accuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find that while model responses rarely disagree with true health claims (posed as questions), they often fail to challenge false claims: responses from InstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%. As we increase the extent of presupposition in input queries, the responses from InstructGPT and ChatGPT agree with the claim considerably more often, regardless of its veracity. Responses from BingChat, which rely on retrieved webpages, are not as susceptible. Given the moderate factual accuracy, and the inability of models to consistently correct false assumptions, our work calls for a careful assessment of current LLMs for use in high-stakes scenarios.

------------

`[2312.14591] Reasons to Reject? Aligning Language Models with Judgments <https://arxiv.org/abs/2312.14591>`__ 拒绝的理由?将语言模型与判断对齐

::

    replaced with revised version Thu, 6 Jun 2024 04:16:54 GMT
    Submission history From: Weiwen Xu [view email]
    [v1] Fri, 22 Dec 2023 10:29:43 UTC (449 KB)
    [v2] Mon, 27 May 2024 12:22:14 UTC (472 KB)
    [v3] Mon, 3 Jun 2024 02:24:38 UTC (484 KB)
    [v4] Thu, 6 Jun 2024 04:16:54 UTC (484 KB)
    Weiwen Xu, Deng Cai, Zhisong Zhang, Wai Lam, Shuming Shi

As humans, we consistently interact with our peers and receive feedback in the form of natural language. This language feedback allows us to maintain appropriate behavior, and rectify potential errors. The question arises naturally: can we use language feedback to align large language models (LLMs)? In contrast to previous research that aligns LLMs with scalar rewards, we present the first systematic exploration of alignment through the lens of language feedback (i.e., judgment). We start with an in-depth investigation of potential methods that can be adapted for aligning LLMs with judgments, revealing that these methods cannot fully capitalize on judgments. To facilitate more effective utilization of judgments, we propose a novel framework, Contrastive Unlikelihood Training (CUT), that allows for fine-grained inappropriate content detection and correction based on judgments. Our results show that, with merely 1317 off-the-shelf judgment data, CUT (LLaMA2-13b) can beat the 175B DaVinci003 and surpass the best baseline by 50.84 points on AlpacaEval. CUT (LLaMA2-chat-13b) can also align LLMs in an iterative fashion using up-to-date model-specific judgments, improving performance from 81.09 to 91.68 points on AlpacaEval. Further analysis suggests that judgments hold greater potential than rewards in LLM alignment.

------------

`[2401.05749] A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism <https://arxiv.org/abs/2401.05749>`__ 惊人数量的Web是机器翻译的:来自多路并行的见解

::

    replaced with revised version Wed, 5 Jun 2024 20:49:57 GMT
    Submission history From: Brian Thompson [view email]
    [v1] Thu, 11 Jan 2024 08:56:13 UTC (76 KB)
    [v2] Wed, 5 Jun 2024 20:49:57 UTC (108 KB)
    Brian Thompson, Mehak Preet Dhaliwal, Peter Frisch, Tobias Domhan, Marcello Federico

We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT). Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT. Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web.

------------

`[2401.06568] Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation <https://arxiv.org/abs/2401.06568>`__ 迷失在源语言中:大型语言模型如何评估机器翻译质量

::

    replaced with revised version Thu, 6 Jun 2024 07:51:16 GMT
    Submission history From: Xu Huang [view email]
    [v1] Fri, 12 Jan 2024 13:23:21 UTC (91 KB)
    [v2] Thu, 6 Jun 2024 07:51:16 UTC (7,803 KB)
    Xu Huang, Zhirui Zhang, Xiang Geng, Yichao Du, Jiajun Chen, Shujian Huang

This study investigates how Large Language Models (LLMs) leverage source and reference data in machine translation evaluation task, aiming to better understand the mechanisms behind their remarkable performance in this task. We design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information. We find that reference information significantly enhances the evaluation accuracy, while surprisingly, source information sometimes is counterproductive, indicating LLMs' inability to fully leverage the cross-lingual capability when evaluating translations. Further analysis of the fine-grained evaluation and fine-tuning experiments show similar results. These findings also suggest a potential research direction for LLMs that fully exploits the cross-lingual capability of LLMs to achieve better performance in machine translation evaluation tasks.

------------

`[2401.06688] Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation <https://arxiv.org/abs/2401.06688>`__ 不要排名，要结合!用质量估计结合机器翻译假设

::

    replaced with revised version Thu, 6 Jun 2024 17:45:39 GMT
    Submission history From: Giorgos Vernikos [view email]
    [v1] Fri, 12 Jan 2024 16:52:41 UTC (8,848 KB)
    [v2] Thu, 6 Jun 2024 17:45:39 UTC (8,625 KB)
    Giorgos Vernikos and Andrei Popescu-Belis

Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method that synthesizes translations using a quality estimation metric (QE), which correlates better with human judgments. QE-fusion leverages a pool of candidates sampled from a model, combining spans from different candidates using a QE metric such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, Mistral, ALMA, and Tower) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generates novel translations in over half of the cases and consistently outperforms other methods across varying numbers of candidates (5-200). Furthermore, we empirically establish that QE-fusion scales linearly with the number of candidates in the pool.

------------

`[2401.11382] Using Large Language Model for End-to-End Chinese ASR and NER <https://arxiv.org/abs/2401.11382>`__ 基于大型语言模型的端到端中文识别和NER

::

    replaced with revised version Thu, 6 Jun 2024 05:39:50 GMT
    Submission history From: Yuang Li [view email]
    [v1] Sun, 21 Jan 2024 03:15:05 UTC (2,639 KB)
    [v2] Thu, 6 Jun 2024 05:39:50 UTC (2,199 KB)
    Yuang Li, Jiawei Yu, Min Zhang, Mengxin Ren, Yanqing Zhao, Xiaofeng Zhao, Shimin Tao, Jinsong Su, Hao Yang

Mapping speech tokens to the same feature space as text tokens has become the paradigm for the integration of speech modality into decoder-only large language models (LLMs). An alternative approach is to use an encoder-decoder architecture that incorporates speech features through cross-attention. This approach, however, has received less attention in the literature. In this work, we connect the Whisper encoder with ChatGLM3 and provide in-depth comparisons of these two approaches using Chinese automatic speech recognition (ASR) and name entity recognition (NER) tasks. We evaluate them not only by conventional metrics like the F1 score but also by a novel fine-grained taxonomy of ASR-NER errors. Our experiments reveal that encoder-decoder architecture outperforms decoder-only architecture with a short context, while decoder-only architecture benefits from a long context as it fully exploits all layers of the LLM. By using LLM, we significantly reduced the entity omission errors and improved the entity ASR accuracy compared to the Conformer baseline. Additionally, we obtained a state-of-the-art (SOTA) F1 score of 0.805 on the AISHELL-NER test set by using chain-of-thought (CoT) NER which first infers long-form ASR transcriptions and then predicts NER labels.

------------

`[2401.14556] Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling <https://arxiv.org/abs/2401.14556>`__ 右看有时是对的:研究仅解码器的llm进行序列标记的能力

::

    replaced with revised version Thu, 6 Jun 2024 12:33:19 GMT
    Submission history From: David Dukić [view email]
    [v1] Thu, 25 Jan 2024 22:50:48 UTC (9,694 KB)
    [v2] Tue, 27 Feb 2024 16:48:17 UTC (10,825 KB)
    [v3] Thu, 6 Jun 2024 12:33:19 UTC (8,552 KB)
    David Duki\'c, Jan \v{S}najder

Pre-trained language models based on masked language modeling (MLM) excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, recent decoder-only large language models (LLMs) perform on par with smaller MLM-based encoders. Although their performance improves with scale, LLMs fall short of achieving state-of-the-art results in information extraction (IE) tasks, many of which are formulated as sequence labeling (SL). We hypothesize that LLMs' poor SL performance stems from causal masking, which prevents the model from attending to tokens on the right of the current token. Yet, how exactly and to what extent LLMs' performance on SL can be improved remains unclear. We explore techniques for improving the SL performance of open LLMs on IE tasks by applying layer-wise removal of the causal mask (CM) during LLM fine-tuning. This approach yields performance gains competitive with state-of-the-art SL models, matching or outperforming the results of CM removal from all blocks. Our findings hold for diverse SL tasks, demonstrating that open LLMs with layer-dependent CM removal outperform strong MLM-based encoders and even instruction-tuned LLMs.

------------

`[2401.18046] Multipath parsing in the brain <https://arxiv.org/abs/2401.18046>`__ 大脑中的多路径解析

::

    replaced with revised version Thu, 6 Jun 2024 13:40:47 GMT
    Submission history From: John Hale [view email]
    [v1] Wed, 31 Jan 2024 18:07:12 UTC (1,302 KB)
    [v2] Thu, 6 Jun 2024 13:40:47 UTC (1,308 KB)
    Berta Franzluebbers, Donald Dunagan, Milo\v{s} Stanojevi\'c, Jan Buys, John T. Hale

Humans understand sentences word-by-word, in the order that they hear them. This incrementality entails resolving temporary ambiguities about syntactic relationships. We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook. In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one. This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset. In both English and Chinese data, we find evidence for multipath parsing. Brain regions associated with this multipath effect include bilateral superior temporal gyrus.

------------

`[2402.06733] NICE: To Optimize In-Context Examples or Not? <https://arxiv.org/abs/2402.06733>`__ 很好:是否优化上下文中的示例?

::

    replaced with revised version Thu, 6 Jun 2024 12:16:55 GMT
    Submission history From: Satvik Golechha [view email]
    [v1] Fri, 9 Feb 2024 19:09:19 UTC (7,174 KB)
    [v2] Fri, 16 Feb 2024 12:08:38 UTC (7,162 KB)
    [v3] Thu, 6 Jun 2024 12:16:55 UTC (7,176 KB)
    Pragya Srivastava, Satvik Golechha, Amit Deshpande, Amit Sharma

Recent work shows that in-context learning and optimization of in-context examples (ICE) can significantly improve the accuracy of large language models (LLMs) on a wide range of tasks, leading to an apparent consensus that ICE optimization is crucial for better performance. However, most of these studies assume a fixed or no instruction provided in the prompt. We challenge this consensus by investigating the necessity of optimizing ICE when task-specific instructions are provided and find that there are many tasks for which it yields diminishing returns. In particular, using a diverse set of tasks and a systematically created instruction set with gradually added details, we find that as the prompt instruction becomes more detailed, the returns on ICE optimization diminish. To characterize this behavior, we introduce a task-specific metric called Normalized Invariability to Choice of Examples (NICE) that quantifies the learnability of tasks from a given instruction, and provides a heuristic to help decide whether to optimize instructions or ICE for a new task. Given a task, the proposed metric can reliably predict the utility of optimizing ICE compared to using random ICE. Our code is available at this https URL.

------------

`[2402.10073] Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence <https://arxiv.org/abs/2402.10073>`__ 两者都很重要:在不影响一般智力的情况下提高大型语言模型的情商

::

    replaced with revised version Thu, 6 Jun 2024 02:13:27 GMT
    Submission history From: Weixiang Zhao [view email]
    [v1] Thu, 15 Feb 2024 16:36:04 UTC (838 KB)
    [v2] Thu, 6 Jun 2024 02:13:27 UTC (840 KB)
    Weixiang Zhao, Zhuojun Li, Shilong Wang, Yang Wang, Yulin Hu, Yanyan Zhao, Chen Wei, Bing Qin

Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel \underline{\textbf{Mo}}dular \underline{\textbf{E}}motional \underline{\textbf{I}}ntelligence enhancement method (\textbf{MoEI}), consisting of Modular Parameter Expansion and intra-inter modulation, is proposed to comprehensively enhance the EI of LLMs without compromise their GI. Extensive experiments on two representative LLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness of MoEI to improving EI while maintain GI.

------------

`[2402.10571] Direct Preference Optimization with an Offset <https://arxiv.org/abs/2402.10571>`__ 带偏移量的直接偏好优化

::

    replaced with revised version Thu, 6 Jun 2024 12:02:37 GMT
    Submission history From: Afra Amini [view email]
    [v1] Fri, 16 Feb 2024 10:55:38 UTC (878 KB)
    [v2] Thu, 6 Jun 2024 12:02:37 UTC (1,110 KB)
    Afra Amini, Tim Vieira, Ryan Cotterell

Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal. Sometimes, the preferred response is only slightly better than the dispreferred one. In other cases, the preference is much stronger. For instance, if a response contains harmful or toxic content, the annotator will have a strong preference for that response. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.

------------

`[2402.10890] When is Tree Search Useful for LLM Planning? It Depends on the Discriminator <https://arxiv.org/abs/2402.10890>`__ 什么时候树搜索对LLM规划有用?这取决于判别器

::

    replaced with revised version Thu, 6 Jun 2024 14:55:40 GMT
    Submission history From: Ziru Chen [view email]
    [v1] Fri, 16 Feb 2024 18:45:58 UTC (2,011 KB)
    [v2] Thu, 6 Jun 2024 14:55:40 UTC (1,630 KB)
    Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, Huan Sun

In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications. Code and data are available at this https URL.

------------

`[2402.11138] Contrastive Instruction Tuning <https://arxiv.org/abs/2402.11138>`__ 对比指令调优

::

    replaced with revised version Thu, 6 Jun 2024 06:03:34 GMT
    Submission history From: Tianyi Yan [view email]
    [v1] Sat, 17 Feb 2024 00:09:32 UTC (1,439 KB)
    [v2] Thu, 6 Jun 2024 06:03:34 UTC (1,470 KB)
    Tianyi Lorena Yan, Fei Wang, James Y. Huang, Wenxuan Zhou, Fan Yin, Aram Galstyan, Wenpeng Yin, Muhao Chen

Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructions with variations across character, word, sentence, and semantic levels by an average of +2.5% in accuracy. Code is available at this https URL.

------------

`[2402.11349] Language Models Don't Learn the Physical Manifestation of Language <https://arxiv.org/abs/2402.11349>`__ 语言模型不学习语言的物理表现形式

::

    replaced with revised version Thu, 6 Jun 2024 17:20:21 GMT
    Submission history From: Bruce W. Lee [view email]
    [v1] Sat, 17 Feb 2024 17:52:24 UTC (2,959 KB)
    [v2] Thu, 6 Jun 2024 17:20:21 UTC (3,811 KB)
    Bruce W. Lee and JaeHyuk Lim

We argue that language-only models don't learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test. These tasks highlight a fundamental gap between human linguistic understanding and the sensory-deprived linguistic understanding of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -> LLaMA 2 70B) has no significant effect on H-Test performance.
We bring in the philosophical case of Mary, who learns about the world in a sensory-deprived environment as a useful conceptual framework to understand how language-only models learn about the world (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limitations of linguistic knowledge acquired in the absence of sensory experience. Our code and data are available at <this http URL.

------------

`[2402.11485] LEIA: Facilitating Cross-lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation <https://arxiv.org/abs/2402.11485>`__ LEIA:基于实体的数据增强促进语言模型中的跨语言知识迁移

::

    replaced with revised version Thu, 6 Jun 2024 05:30:59 GMT
    Submission history From: Ikuya Yamada [view email]
    [v1] Sun, 18 Feb 2024 07:24:34 UTC (123 KB)
    [v2] Thu, 6 Jun 2024 05:30:59 UTC (124 KB)
    Ikuya Yamada and Ryokan Ri

Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at this https URL.

------------

`[2402.11517] Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM <https://arxiv.org/abs/2402.11517>`__ 知识到SQL:使用数据专家LLM增强SQL生成

::

    replaced with revised version Thu, 6 Jun 2024 13:56:59 GMT
    Submission history From: Zijin Hong [view email]
    [v1] Sun, 18 Feb 2024 09:10:04 UTC (333 KB)
    [v2] Thu, 7 Mar 2024 13:43:03 UTC (421 KB)
    [v3] Thu, 6 Jun 2024 13:56:59 UTC (785 KB)
    Zijin Hong, Zheng Yuan, Hao Chen, Qinggang Zhang, Feiran Huang, Xiao Huang

Generating accurate SQL queries for user questions (text-to-SQL) has been a long-standing challenge since it requires a deep understanding of both the user's question and the corresponding database schema in order to retrieve the desired content accurately. Existing methods rely on the comprehensive capability of large language models (LLMs) to generate the SQL. However, some necessary knowledge is not explicitly included in the database schema and user question or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient questions may be inaccurate, negatively influencing the text-to-SQL models' performance and robustness. To address this challenge, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all text-to-SQL models. Specifically, we introduce the detailed implementation of DELLM regarding table reading and the basic fine-tuning process. We further propose a Preference Learning via Database Feedback (PLDBF) strategy, refining the DELLM to generate more helpful knowledge for LLMs. Extensive experiments verify that DELLM can enhance the state-of-the-art approaches for text-to-SQL tasks. The corresponding code of DELLM is released for further research.

------------

`[2402.11548] KMMLU: Measuring Massive Multitask Language Understanding in Korean <https://arxiv.org/abs/2402.11548>`__ KMMLU:大规模韩语多任务语言理解测试

::

    replaced with revised version Thu, 6 Jun 2024 12:23:58 GMT
    Submission history From: Guijin Son [view email]
    [v1] Sun, 18 Feb 2024 11:41:07 UTC (1,554 KB)
    [v2] Thu, 6 Jun 2024 12:23:58 UTC (1,620 KB)
    Guijin Son and Hanwool Lee and Sungdong Kim and Seungone Kim and Niklas Muennighoff and Taekyoon Choi and Cheonbok Park and Kang Min Yoo and Stella Biderman

We propose KMMLU, a new Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. While prior Korean benchmarks are translated from existing English benchmarks, KMMLU is collected from original Korean exams, capturing linguistic and cultural aspects of the Korean language. We test 27 public and proprietary LLMs and observe the best public model to score 50.5%, leaving significant room for improvement. This model was primarily trained for English and Chinese, not Korean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X do not exceed 60%. This suggests that further work is needed to improve LLMs for Korean, and we believe KMMLU offers the appropriate tool to track this progress. We make our dataset publicly available on the Hugging Face Hub and integrate the benchmark into EleutherAI's Language Model Evaluation Harness.

------------

`[2402.11597] Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once? <https://arxiv.org/abs/2402.11597>`__ 多任务推理:大型语言模型能否同时遵循多条指令?

::

    replaced with revised version Thu, 6 Jun 2024 12:55:17 GMT
    Submission history From: Guijin Son [view email]
    [v1] Sun, 18 Feb 2024 14:25:19 UTC (8,516 KB)
    [v2] Thu, 6 Jun 2024 12:55:17 UTC (8,516 KB)
    Guijin Son and Sangwon Baek and Sangdae Nam and Ilgyun Jeong and Seungone Kim

Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench dataset and our code at this link this https URL.

------------

`[2402.11894] Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models <https://arxiv.org/abs/2402.11894>`__ 自动化数据集更新以实现大型语言模型的可靠和及时评估

::

    replaced with revised version Thu, 6 Jun 2024 12:49:44 GMT
    Submission history From: Jiahao Ying [view email]
    [v1] Mon, 19 Feb 2024 07:15:59 UTC (9,700 KB)
    [v2] Wed, 28 Feb 2024 04:21:09 UTC (9,700 KB)
    [v3] Thu, 6 Jun 2024 12:49:44 UTC (1,609 KB)
    Jiahao Ying, Yixin Cao, Yushi Bai, Qianru Sun, Bo Wang, Wei Tang, Zhaojun Ding, Yizhe Yang, Xuanjing Huang, Shuicheng Yan

Large language models (LLMs) have achieved impressive performance across various natural language benchmarks, prompting a continual need to curate more difficult datasets for larger LLMs, which is costly and time-consuming. In this paper, we propose to automate dataset updating and provide systematic analysis regarding its effectiveness in dealing with benchmark leakage issue, difficulty control, and stability. Thus, once the current benchmark has been mastered or leaked, we can update it for timely and reliable evaluation. There are two updating strategies: 1) mimicking strategy to generate similar samples based on original data, preserving stylistic and contextual essence, and 2) extending strategy that further expands existing samples at varying cognitive levels by adapting Bloom's taxonomy of educational objectives. Extensive experiments on updated MMLU and BIG-Bench demonstrate the stability of the proposed strategies and find that the mimicking strategy can effectively alleviate issues of overestimation from benchmark leakage. In cases where the efficient mimicking strategy fails, our extending strategy still shows promising results. Additionally, by controlling the difficulty, we can better discern the models' performance and enable fine-grained analysis neither too difficult nor too easy an exam can fairly judge students' learning status. To the best of our knowledge, we are the first to automate updating benchmarks for reliable and timely evaluation. Our demo leaderboard can be found at this https URL.

------------

`[2402.12343] Emulated Disalignment: Safety Alignment for Large Language Models May Backfire! <https://arxiv.org/abs/2402.12343>`__ 模拟错位:大型语言模型的安全对齐可能会适得其反!

::

    replaced with revised version Thu, 6 Jun 2024 12:54:48 GMT
    Submission history From: Zhanhui Zhou [view email]
    [v1] Mon, 19 Feb 2024 18:16:51 UTC (471 KB)
    [v2] Wed, 21 Feb 2024 16:29:18 UTC (471 KB)
    [v3] Wed, 3 Apr 2024 12:25:47 UTC (1,278 KB)
    [v4] Thu, 6 Jun 2024 12:54:48 UTC (1,269 KB)
    Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, Yu Qiao

Large language models (LLMs) undergo safety alignment to ensure safe conversations with humans. However, this paper introduces a training-free attack method capable of reversing safety alignment, converting the outcomes of stronger alignment into greater potential for harm by accessing only LLM output token distributions. Specifically, our method achieves this reversal by contrasting the output token distribution of a safety-aligned language model (e.g., Llama-2-chat) against its pre-trained version (e.g., Llama-2), so that the token predictions are shifted towards the opposite direction of safety alignment. We name this method emulated disalignment (ED) because sampling from this contrastive distribution provably emulates the result of fine-tuning to minimize a safety reward. Our experiments with ED across three evaluation datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rates in 43 out of 48 evaluation subsets by a large margin. Eventually, given ED's reliance on language model output token distributions, which particularly compromises open-source models, our findings highlight the need to reassess the open accessibility of language models, even if they have been safety-aligned. Code is available at this https URL.

------------

`[2402.15637] Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models <https://arxiv.org/abs/2402.15637>`__ 因果语言模型中上下文演示示例的顺序敏感性问题

::

    replaced with revised version Thu, 6 Jun 2024 12:01:09 GMT
    Submission history From: Yanzheng Xiang [view email]
    [v1] Fri, 23 Feb 2024 22:39:12 UTC (248 KB)
    [v2] Thu, 6 Jun 2024 12:01:09 UTC (211 KB)
    Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He

In-context learning has become a popular paradigm in natural language processing. However, its performance can be significantly influenced by the order of in-context demonstration examples. In this paper, we found that causal language models (CausalLMs) are more sensitive to this order compared to prefix language models (PrefixLMs). We attribute this phenomenon to the auto-regressive attention masks within CausalLMs, which restrict each token from accessing information from subsequent tokens. This results in different receptive fields for samples at different positions, thereby leading to representation disparities across positions. To tackle this challenge, we introduce an unsupervised fine-tuning method, termed the Information-Augmented and Consistency-Enhanced approach. This approach utilizes contrastive learning to align representations of in-context examples across different positions and introduces a consistency loss to ensure similar representations for inputs with different permutations. This enhances the model's predictive consistency across permutations. Experimental results on five benchmarks suggest that our proposed method can reduce the sensitivity of CausalLMs to the order of in-context examples and exhibit robust generalizability, particularly when demonstrations are sourced from a candidate pool different from that used in the training phase, or when the number of in-context examples differs from what is used during training.

------------

`[2402.16438] Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models <https://arxiv.org/abs/2402.16438>`__ 特定语言神经元:大型语言模型多语言能力的关键

::

    replaced with revised version Thu, 6 Jun 2024 08:14:46 GMT
    Submission history From: Tianyi Tang [view email]
    [v1] Mon, 26 Feb 2024 09:36:05 UTC (273 KB)
    [v2] Thu, 6 Jun 2024 08:14:46 UTC (281 KB)
    Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, Ji-Rong Wen

Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora. It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts. In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs. Based on LAPE, we conduct comprehensive experiments on several representative LLMs, such as LLaMA-2, BLOOM, and Mistral. Our findings indicate that LLMs' proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models' top and bottom layers. Furthermore, we showcase the feasibility to "steer" the output language of LLMs by selectively activating or deactivating language-specific neurons. Our research provides important evidence to the understanding and exploration of the multilingual capabilities of LLMs.

------------

`[2402.16775] A Comprehensive Evaluation of Quantization Strategies for Large Language Models <https://arxiv.org/abs/2402.16775>`__ 大型语言模型量化策略的综合评估

::

    replaced with revised version Thu, 6 Jun 2024 13:38:26 GMT
    Submission history From: Renren Jin [view email]
    [v1] Mon, 26 Feb 2024 17:45:36 UTC (306 KB)
    [v2] Thu, 6 Jun 2024 13:38:26 UTC (302 KB)
    Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, Deyi Xiong

Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \& capacity, (2) alignment, and (3) efficiency, and conduct extensive experiments across ten diverse benchmarks. Our experimental results indicate that LLMs with 4-bit quantization can retain performance comparable to their non-quantized counterparts, and perplexity can serve as a proxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs with larger parameter scales can outperform smaller LLMs. Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs. Consequently, substantial engineering efforts and hardware support are imperative to achieve a balanced optimization of decoding speed and memory consumption in the context of quantized LLMs.

------------

`[2402.17447] Deep Learning Based Named Entity Recognition Models for Recipes <https://arxiv.org/abs/2402.17447>`__ 基于深度学习的菜谱命名实体识别模型

::

    replaced with revised version Thu, 6 Jun 2024 07:41:21 GMT
    Submission history From: Ganesh Bagler Prof [view email]
    [v1] Tue, 27 Feb 2024 12:03:56 UTC (938 KB)
    [v2] Thu, 6 Jun 2024 07:41:21 UTC (938 KB)
    Mansi Goel, Ayush Agarwal, Shubham Agrawal, Janak Kapuriya, Akhil Vamshi Konam, Rishabh Gupta, Shrey Rastogi, Niharika, and Ganesh Bagler

Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity to create the machine-annotated dataset. A thorough investigation of NER approaches on these three datasets involving statistical, fine-tuning of deep learning-based language models and few-shot prompting on large language models (LLMs) provides deep insights. We conclude that few-shot prompting on LLMs has abysmal performance, whereas the fine-tuned spaCy-transformer emerges as the best model with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated, augmented, and machine-annotated datasets, respectively.

------------

`[2402.18158] Evaluating Quantized Large Language Models <https://arxiv.org/abs/2402.18158>`__ 量化大型语言模型的评估

::

    replaced with revised version Thu, 6 Jun 2024 07:30:44 GMT
    Submission history From: Shiyao Li [view email]
    [v1] Wed, 28 Feb 2024 08:43:05 UTC (1,007 KB)
    [v2] Thu, 6 Jun 2024 07:30:44 UTC (25,095 KB)
    Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang

Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their applicability. Based on the extensive experiments, we systematically summarize the effect of quantization, provide recommendations to apply quantization techniques, and point out future directions. The code can be found in this https URL.

------------

`[2402.18334] Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation <https://arxiv.org/abs/2402.18334>`__ 学习生成指令调优数据集的零样本任务自适应

::

    replaced with revised version Thu, 6 Jun 2024 13:50:26 GMT
    Submission history From: Nihal V. Nayak [view email]
    [v1] Wed, 28 Feb 2024 13:54:57 UTC (342 KB)
    [v2] Thu, 6 Jun 2024 13:50:26 UTC (361 KB)
    Nihal V. Nayak, Yiyang Nan, Avi Trost, Stephen H. Bach

We introduce Bonito, an open-source model for conditional task generation that converts unannotated text into task-specific training datasets for instruction tuning. We aim to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito by fine-tuning a pretrained large language model on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains with unannotated text across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points. We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators. Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains. The model, dataset, and code are available at this https URL.

------------

`[2403.03167] PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset <https://arxiv.org/abs/2403.03167>`__ PARADISE:用程序警告和提示数据集评估语言模型的隐式规划技能

::

    replaced with revised version Thu, 6 Jun 2024 08:22:16 GMT
    Submission history From: Arda Uzunoglu [view email]
    [v1] Tue, 5 Mar 2024 18:01:59 UTC (12,605 KB)
    [v2] Wed, 6 Mar 2024 21:01:10 UTC (12,605 KB)
    [v3] Thu, 6 Jun 2024 08:22:16 UTC (12,605 KB)
    Arda Uzunoglu, Abdalfatah Rashid Safa, G\"ozde G\"ul \c{S}ahin

Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q\&A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and zero-shot prompting, reveal the effectiveness of task-specific small models over large language models in most scenarios. Despite advancements, all models fall short of human performance. Notably, our analysis uncovers intriguing insights, such as variations in model behavior with dropped keywords, struggles of BERT-family and GPT-4 with physical and abstract goals, and the proposed tasks offering valuable prior knowledge for other unseen procedural tasks. The PARADISE dataset and associated resources are publicly available for further research exploration with this https URL.

------------

`[2403.06932] ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis <https://arxiv.org/abs/2403.06932>`__ ERA-CoT:通过实体关系分析改进思维链

::

    replaced with revised version Thu, 6 Jun 2024 11:54:56 GMT
    Submission history From: Yanming Liu [view email]
    [v1] Mon, 11 Mar 2024 17:18:53 UTC (1,079 KB)
    [v2] Thu, 6 Jun 2024 11:54:56 UTC (1,828 KB)
    Yanming Liu, Xinyue Peng, Tianyu Du, Jianwei Yin, Weihao Liu, Xuhong Zhang

Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs.

------------

`[2403.18680] Non-Linear Inference Time Intervention: Improving LLM Truthfulness <https://arxiv.org/abs/2403.18680>`__ 非线性推理时间干预:改善LLM真实性

::

    replaced with revised version Thu, 6 Jun 2024 13:58:20 GMT
    Submission history From: Jakub Hościłowicz [view email]
    [v1] Wed, 27 Mar 2024 15:22:16 UTC (282 KB)
    [v2] Thu, 6 Jun 2024 13:58:20 UTC (624 KB)
    Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki

In this work, we explore LLM's internal representation space to identify attention heads that contain the most truthful and accurate information. We further developed the Inference Time Intervention (ITI) framework, which lets bias LLM without the need for fine-tuning. The improvement manifests in introducing a non-linear multi-token probing and multi-token intervention: Non-Linear ITI (NL-ITI), which significantly enhances performance on evaluation benchmarks. NL-ITI is tested on diverse multiple-choice datasets, including TruthfulQA, on which we report over 16% relative MC1 (accuracy of model pointing to the correct answer) improvement with respect to the baseline ITI results. Moreover, we achieved a 10% relative improvement over the recently released Truth Forest (TrFf) method that also focused on ITI improvement.

------------

`[2404.14461] Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs <https://arxiv.org/abs/2404.14461>`__ 竞赛报告:在对齐的llm中寻找通用的越狱后门

::

    replaced with revised version Thu, 6 Jun 2024 12:45:52 GMT
    Submission history From: Javier Rando [view email]
    [v1] Mon, 22 Apr 2024 05:08:53 UTC (183 KB)
    [v2] Thu, 6 Jun 2024 12:45:52 UTC (172 KB)
    Javier Rando and Francesco Croce and Kry\v{s}tof Mitka and Stepan Shabalin and Maksym Andriushchenko and Nicolas Flammarion and Florian Tram\`er

Large language models are aligned to be safe, preventing users from generating harmful content like misinformation or instructions for illegal activities. However, previous work has shown that the alignment process is vulnerable to poisoning attacks. Adversaries can manipulate the safety training data to inject backdoors that act like a universal sudo command: adding the backdoor string to any prompt enables harmful responses from models that, otherwise, behave safely. Our competition, co-located at IEEE SaTML 2024, challenged participants to find universal backdoors in several large language models. This report summarizes the key findings and promising ideas for future research.

------------

`[2405.00301] Enhanced Language Model Truthfulness with Learnable Intervention and Uncertainty Expression <https://arxiv.org/abs/2405.00301>`__ 基于可学习干预和不确定性表达的增强语言模型真实性

::

    replaced with revised version Thu, 6 Jun 2024 07:32:15 GMT
    Submission history From: Farima Fatahi Bayat [view email]
    [v1] Wed, 1 May 2024 03:50:09 UTC (7,435 KB)
    [v2] Thu, 6 Jun 2024 07:32:15 UTC (7,436 KB)
    Farima Fatahi Bayat, Xin Liu, H. V. Jagadish, Lu Wang

Large language models (LLMs) can generate long-form and coherent text, yet they often hallucinate facts, which undermines their reliability. To mitigate this issue, inference-time methods steer LLM representations toward the "truthful directions" previously learned for truth elicitation. However, applying these truthful directions with the same intensity fails to generalize across different query contexts. We propose LITO, a Learnable Intervention method for Truthfulness Optimization that automatically identifies the optimal intervention intensity tailored to each specific context. LITO explores a sequence of model generations based on increasing levels of intervention intensities. It selects the most accurate response or refuses to answer when the predictions are highly uncertain. Experiments on multiple LLMs and question-answering datasets demonstrate that LITO improves truthfulness while preserving task accuracy. The adaptive nature of LITO counters the limitations of one-size-fits-all intervention methods, maximizing truthfulness by reflecting the model's internal knowledge only when it is confident. Our code is available at this https URL.

------------

`[2405.09482] Beyond Flesch-Kincaid: Prompt-based Metrics Improve Difficulty Classification of Educational Texts <https://arxiv.org/abs/2405.09482>`__ 超越Flesch-Kincaid:基于提示的指标提高教育文本分类的难度

::

    replaced with revised version Thu, 6 Jun 2024 07:40:58 GMT
    Submission history From: Donya Rooein [view email]
    [v1] Wed, 15 May 2024 16:22:16 UTC (2,709 KB)
    [v2] Thu, 6 Jun 2024 07:40:58 UTC (2,710 KB)
    Donya Rooein, Paul Rottger, Anastassia Shaitarova, Dirk Hovy

Using large language models (LLMs) for educational applications like dialogue-based teaching is a hot topic. Effective teaching, however, requires teachers to adapt the difficulty of content and explanations to the education level of their students. Even the best LLMs today struggle to do this well. If we want to improve LLMs on this adaptation task, we need to be able to measure adaptation success reliably. However, current Static metrics for text difficulty, like the Flesch-Kincaid Reading Ease score, are known to be crude and brittle. We, therefore, introduce and evaluate a new set of Prompt-based metrics for text difficulty. Based on a user study, we create Prompt-based metrics as inputs for LLMs. They leverage LLM's general language understanding capabilities to capture more abstract and complex features than Static metrics. Regression experiments show that adding our Prompt-based metrics significantly improves text difficulty classification over Static metrics alone. Our results demonstrate the promise of using LLMs to evaluate text adaptation to different education levels.

------------

`[2405.13034] Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards Mixed Reality <https://arxiv.org/abs/2405.13034>`__ 面向混合现实的多模态细粒度训练助手自主工作流

::

    replaced with revised version Wed, 5 Jun 2024 21:47:37 GMT
    Submission history From: Jiahuan Pei [view email]
    [v1] Thu, 16 May 2024 14:20:30 UTC (9,092 KB)
    [v2] Wed, 5 Jun 2024 21:47:37 UTC (9,091 KB)
    Jiahuan Pei, Irene Viola, Haochen Huang, Junxiao Wang, Moonisa Ahsan, Fanghua Ye, Jiang Yiming, Yao Sai, Di Wang, Zhumin Chen, Pengjie Ren, Pablo Cesar

Autonomous artificial intelligence (AI) agents have emerged as promising protocols for automatically understanding the language-based environment, particularly with the exponential development of large language models (LLMs). However, a fine-grained, comprehensive understanding of multimodal environments remains under-explored. This work designs an autonomous workflow tailored for integrating AI agents seamlessly into extended reality (XR) applications for fine-grained training. We present a demonstration of a multimodal fine-grained training assistant for LEGO brick assembly in a pilot XR environment. Specifically, we design a cerebral language agent that integrates LLM with memory, planning, and interaction with XR tools and a vision-language agent, enabling agents to decide their actions based on past experiences. Furthermore, we introduce LEGO-MRTA, a multimodal fine-grained assembly dialogue dataset synthesized automatically in the workflow served by a commercial LLM. This dataset comprises multimodal instruction manuals, conversations, XR responses, and vision question answering. Last, we present several prevailing open-resource LLMs as benchmarks, assessing their performance with and without fine-tuning on the proposed dataset. We anticipate that the broader impact of this workflow will advance the development of smarter assistants for seamless user interaction in XR environments, fostering research in both AI and HCI communities.

------------

`[2406.01026] Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors <https://arxiv.org/abs/2406.01026>`__ 加强的符号绑定使大型语言模型成为可靠的多项选择器

::

    replaced with revised version Thu, 6 Jun 2024 06:32:45 GMT
    Submission history From: Xue Mengge [view email]
    [v1] Mon, 3 Jun 2024 06:20:12 UTC (8,089 KB)
    [v2] Thu, 6 Jun 2024 06:32:45 UTC (8,089 KB)
    Mengge Xue, Zhenyu Hu, Liqun Liu, Kuo Liao, Shuang Li, Honglin Han, Meng Zhao, Chengguo Yin

Multiple-Choice Questions (MCQs) constitute a critical area of research in the study of Large Language Models (LLMs). Previous works have investigated the selection bias problem in MCQs within few-shot scenarios, in which the LLM's performance may be influenced by the presentation of answer choices, leaving the selection bias during Supervised Fine-Tuning (SFT) unexplored. In this paper, we reveal that selection bias persists in the SFT phase , primarily due to the LLM's inadequate Multiple Choice Symbol Binding (MCSB) ability. This limitation implies that the model struggles to associate the answer options with their corresponding symbols (e.g., A/B/C/D) effectively. To enhance the model's MCSB capability, we first incorporate option contents into the loss function and subsequently adjust the weights of the option symbols and contents, guiding the model to understand the option content of the current symbol. Based on this, we introduce an efficient SFT algorithm for MCQs, termed Point-wise Intelligent Feedback (PIF). PIF constructs negative instances by randomly combining the incorrect option contents with all candidate symbols, and proposes a point-wise loss to provide feedback on these negative samples into LLMs. Our experimental results demonstrate that PIF significantly reduces the model's selection bias by improving its MCSB capability. Remarkably, PIF exhibits a substantial enhancement in the accuracy for MCQs.

------------

`[2406.01514] Decoupled Alignment for Robust Plug-and-Play Adaptation <https://arxiv.org/abs/2406.01514>`__ 面向健壮即插即用适应的解耦对齐

::

    replaced with revised version Thu, 6 Jun 2024 04:25:40 GMT
    Submission history From: Haozheng Luo [view email]
    [v1] Mon, 3 Jun 2024 16:46:18 UTC (2,711 KB)
    [v2] Tue, 4 Jun 2024 03:04:09 UTC (2,711 KB)
    [v3] Thu, 6 Jun 2024 04:25:40 UTC (3,980 KB)
    Haozheng Luo, Jiahao Yu, Wenxin Zhang, Jialong Li, Jerry Yao-Chieh Hu, Xinyu Xing, Han Liu

We introduce a low-resource safety enhancement method for aligning large language models (LLMs) without the need for supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF). Our main idea is to exploit knowledge distillation to extract the alignment information from existing well-aligned LLMs and integrate it into unaligned LLMs in a plug-and-play fashion. Methodology, we employ delta debugging to identify the critical components of knowledge necessary for effective distillation. On the harmful question dataset, our method significantly enhances the average defense success rate by approximately 14.41%, reaching as high as 51.39%, in 17 unaligned pre-trained LLMs, without compromising performance.

------------

`[2406.02886] PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs <https://arxiv.org/abs/2406.02886>`__ PLaD:基于伪偏好对的大规模语言模型蒸馏

::

    replaced with revised version Thu, 6 Jun 2024 12:47:31 GMT
    Submission history From: Rongzhi Zhang [view email]
    [v1] Wed, 5 Jun 2024 03:08:25 UTC (1,196 KB)
    [v2] Thu, 6 Jun 2024 12:47:31 UTC (1,197 KB)
    Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Haorui Wang, Zhen Qin, Feng Han, Jialu Liu, Simon Baumgartner, Michael Bendersky, Chao Zhang

Large Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings. Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models. However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue. In this work, we present PLaD, a novel preference-based LLM distillation framework. PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs. Then, PLaD leverages a ranking loss to re-calibrate student's estimation of sequence likelihood, which steers the student's focus towards understanding the relative quality of outputs instead of simply imitating the teacher. PLaD bypasses the need for access to teacher LLM's internal states, tackles the student's expressivity limitations, and mitigates the student mis-calibration issue. Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our proposed PLaD framework.

------------

`[2406.03151] Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation <https://arxiv.org/abs/2406.03151>`__ 你站在哪一边?用于端到端论点总结和评估的多任务数据集

::

    replaced with revised version Thu, 6 Jun 2024 09:30:11 GMT
    Submission history From: Hao Li [view email]
    [v1] Wed, 5 Jun 2024 11:15:45 UTC (8,103 KB)
    [v2] Thu, 6 Jun 2024 09:30:11 UTC (8,105 KB)
    Hao Li, Yuping Wu, Viktor Schlegel, Riza Batista-Navarro, Tharindu Madusanka, Iqra Zahid, Jiayan Zeng, Xiaochi Wang, Xinran He, Yizhi Li and Goran Nenadic

With the recent advances of large language models (LLMs), it is no longer infeasible to build an automated debate system that helps people to synthesise persuasive arguments. Previous work attempted this task by integrating multiple components. In our work, we introduce an argument mining dataset that captures the end-to-end process of preparing an argumentative essay for a debate, which covers the tasks of claim and evidence identification (Task 1 ED), evidence convincingness ranking (Task 2 ECR), argumentative essay summarisation and human preference ranking (Task 3 ASR) and metric learning for automated evaluation of resulting essays, based on human feedback along argument quality dimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are fully annotated with the various properties supporting the aforementioned tasks. We evaluate multiple generative baselines for each of these tasks, including representative LLMs. We find, that while they show promising results on individual tasks in our benchmark, their end-to-end performance on all four tasks in succession deteriorates significantly, both in automated measures as well as in human-centred evaluation. This challenge presented by our proposed dataset motivates future research on end-to-end argument mining and summarisation. The repository of this project is available at this https URL

------------

`[2406.03170] StatBot.Swiss: Bilingual Open Data Exploration in Natural Language <https://arxiv.org/abs/2406.03170>`__ StatBot。Swiss:自然语言双语开放数据探索

::

    replaced with revised version Thu, 6 Jun 2024 08:29:23 GMT
    Submission history From: Farhad Nooralahzadeh [view email]
    [v1] Wed, 5 Jun 2024 12:03:19 UTC (1,273 KB)
    [v2] Thu, 6 Jun 2024 08:29:23 UTC (1,273 KB)
    Farhad Nooralahzadeh, Yi Zhang, Ellery Smith, Sabine Maennel, Cyril Matthey-Doret, Rapha\"el de Fondville, Kurt Stockinger

The potential for improvements brought by Large Language Models (LLMs) in Text-to-SQL systems is mostly assessed on monolingual English datasets. However, LLMs' performance for other languages remains vastly unexplored. In this work, we release the StatBot.Swiss dataset, the first bilingual benchmark for evaluating Text-to-SQL systems based on real-world applications. The StatBot.Swiss dataset contains 455 natural language/SQL-pairs over 35 big databases with varying level of complexity for both English and German.
We evaluate the performance of state-of-the-art LLMs such as GPT-3.5-Turbo and mixtral-8x7b-instruct for the Text-to-SQL translation task using an in-context learning approach. Our experimental analysis illustrates that current LLMs struggle to generalize well in generating SQL queries on our novel bilingual dataset.

------------

`[2310.07579] In-Context Unlearning: Language Models as Few Shot Unlearners <https://arxiv.org/abs/2310.07579>`__ 

::

    replaced with revised version Thu, 6 Jun 2024 06:31:08 GMT
    Submission history From: Martin Pawelczyk [view email]
    [v1] Wed, 11 Oct 2023 15:19:31 UTC (23,361 KB)
    [v2] Thu, 12 Oct 2023 14:15:24 UTC (23,361 KB)
    [v3] Tue, 4 Jun 2024 12:35:56 UTC (24,582 KB)
    [v4] Thu, 6 Jun 2024 06:31:08 UTC (24,582 KB)
    Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju

Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.

------------

`[2310.09639] DPZero: Private Fine-Tuning of Language Models without Backpropagation <https://arxiv.org/abs/2310.09639>`__ DPZero:无反向传播的语言模型私有微调

::

    replaced with revised version Thu, 6 Jun 2024 14:31:03 GMT
    Submission history From: Liang Zhang [view email]
    [v1] Sat, 14 Oct 2023 18:42:56 UTC (44 KB)
    [v2] Wed, 14 Feb 2024 09:42:42 UTC (3,420 KB)
    [v3] Thu, 6 Jun 2024 14:31:03 UTC (3,426 KB)
    Liang Zhang, Bingcong Li, Kiran Koshy Thekumparampil, Sewoong Oh, Niao He

The widespread practice of fine-tuning large language models (LLMs) on domain-specific data faces two major challenges in memory and privacy. First, as the size of LLMs continues to grow, the memory demands of gradient-based training methods via backpropagation become prohibitively high. Second, given the tendency of LLMs to memorize training data, it is important to protect potentially sensitive information in the fine-tuning data from being regurgitated. Zeroth-order methods, which rely solely on forward passes, substantially reduce memory consumption during training. However, directly combining them with standard differentially private gradient descent suffers more as model size grows. To bridge this gap, we introduce DPZero, a novel private zeroth-order algorithm with nearly dimension-independent rates. The memory efficiency of DPZero is demonstrated in privately fine-tuning RoBERTa and OPT on several downstream tasks. Our code is available at this https URL.

------------

`[2310.10195] AdaLomo: Low-memory Optimization with Adaptive Learning Rate <https://arxiv.org/abs/2310.10195>`__ AdaLomo:自适应学习率的低内存优化

::

    replaced with revised version Thu, 6 Jun 2024 13:22:25 GMT
    Submission history From: Kai Lv [view email]
    [v1] Mon, 16 Oct 2023 09:04:28 UTC (1,692 KB)
    [v2] Sun, 22 Oct 2023 13:37:33 UTC (1,692 KB)
    [v3] Thu, 6 Jun 2024 13:22:25 UTC (9,396 KB)
    Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu

Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at this https URL.

------------

`[2401.00793] SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models <https://arxiv.org/abs/2401.00793>`__ SecFormer:面向大型语言模型的快速准确隐私保护推理

::

    replaced with revised version Thu, 6 Jun 2024 05:22:44 GMT
    Submission history From: Jinglong Luo [view email]
    [v1] Mon, 1 Jan 2024 15:40:35 UTC (252 KB)
    [v2] Sat, 6 Jan 2024 10:05:23 UTC (270 KB)
    [v3] Thu, 6 Jun 2024 05:22:44 UTC (1,512 KB)
    Jinglong Luo, Yehong Zhang, Zhuo Zhang, Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu, Zenglin Xu

With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and maximum operations in PPI without sacrificing model performance. Additionally, we have developed a suite of efficient SMPC protocols that utilize segmented polynomials, Fourier series and Goldschmidt's method to handle other complex nonlinear functions within PPI, such as GeLU, LayerNorm, and Softmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer in performance, showing improvements of $5.6\%$ and $24.2\%$ for BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, respectively. In terms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, demonstrating its effectiveness and speed.

------------

`[2401.17263] Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks <https://arxiv.org/abs/2401.17263>`__ 针对语言模型越狱攻击的鲁棒提示优化

::

    replaced with revised version Wed, 5 Jun 2024 23:39:54 GMT
    Submission history From: Andy Zhou [view email]
    [v1] Tue, 30 Jan 2024 18:56:08 UTC (1,578 KB)
    [v2] Fri, 2 Feb 2024 21:18:57 UTC (2,198 KB)
    [v3] Wed, 5 Jun 2024 23:39:54 UTC (3,161 KB)
    Andy Zhou and Bo Li and Haohan Wang

Despite advances in AI alignment, large language models (LLMs) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries can modify prompts to induce unwanted behavior. While some defenses have been proposed, they have not been adapted to newly proposed attacks and more challenging threat models. To address this, we propose an optimization-based objective for defending LLMs against jailbreaking attacks and an algorithm, Robust Prompt Optimization (RPO) to create robust system-level defenses. Our approach directly incorporates the adversary into the defensive objective and optimizes a lightweight and transferable suffix, enabling RPO to adapt to worst-case adaptive attacks. Our theoretical and experimental results show improved robustness to both jailbreaks seen during optimization and unknown jailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2 to 0% on JailbreakBench, setting the state-of-the-art. Code can be found at this https URL

------------

`[2402.10450] PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control <https://arxiv.org/abs/2402.10450>`__ PRISE:用于学习控制时序动作抽象的llm风格序列压缩

::

    replaced with revised version Thu, 6 Jun 2024 04:47:52 GMT
    Submission history From: Ruijie Zheng [view email]
    [v1] Fri, 16 Feb 2024 04:55:09 UTC (15,009 KB)
    [v2] Thu, 9 May 2024 20:43:44 UTC (9,255 KB)
    [v3] Thu, 6 Jun 2024 04:47:52 UTC (16,338 KB)
    Ruijie Zheng, Ching-An Cheng, Hal Daum\'e III, Furong Huang, Andrey Kolobov

Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of both multitask imitation learning as well as few-shot imitation learning on unseen tasks. Our code is released at this https URL.

------------

`[2402.12991] TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification <https://arxiv.org/abs/2402.12991>`__ 陷阱:用于黑盒识别的定向随机对抗性提示蜜罐

::

    replaced with revised version Thu, 6 Jun 2024 17:46:48 GMT
    Submission history From: Martin Gubri [view email]
    [v1] Tue, 20 Feb 2024 13:20:39 UTC (3,136 KB)
    [v2] Thu, 6 Jun 2024 17:46:48 UTC (2,045 KB)
    Martin Gubri, Dennis Ulmer, Hwaran Lee, Sangdoo Yun and Seong Joon Oh

Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel fingerprinting problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the original function.

------------

`[2402.14979] Optimizing Language Models for Human Preferences is a Causal Inference Problem <https://arxiv.org/abs/2402.14979>`__ 根据人类偏好优化语言模型是一个因果推理问题

::

    replaced with revised version Wed, 5 Jun 2024 23:19:19 GMT
    Submission history From: Victoria Lin [view email]
    [v1] Thu, 22 Feb 2024 21:36:07 UTC (169 KB)
    [v2] Wed, 5 Jun 2024 23:19:19 UTC (170 KB)
    Victoria Lin, Eli Ben-Michael, Louis-Philippe Morency

As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarantees on bias. Finally, we empirically demonstrate the effectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human preferences on direct outcome data, and we validate the robustness of DR-CPO under difficult confounding conditions.

------------

`[2402.17641] Variational Learning is Effective for Large Deep Networks <https://arxiv.org/abs/2402.17641>`__ 变分学习对于大型深度网络是有效的

::

    replaced with revised version Thu, 6 Jun 2024 04:31:43 GMT
    Submission history From: Thomas Möllenhoff [view email]
    [v1] Tue, 27 Feb 2024 16:11:05 UTC (2,142 KB)
    [v2] Thu, 6 Jun 2024 04:31:43 UTC (3,518 KB)
    Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi, Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz Khan, Thomas M\"ollenhoff

We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve finetuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence that variational learning is effective.

------------

`[2402.18059] Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models <https://arxiv.org/abs/2402.18059>`__ 具有增强可检测性和语义一致性的大型语言模型特定标记水印

::

    replaced with revised version Thu, 6 Jun 2024 04:49:37 GMT
    Submission history From: Mingjia Huo [view email]
    [v1] Wed, 28 Feb 2024 05:43:22 UTC (328 KB)
    [v2] Thu, 7 Mar 2024 05:47:49 UTC (325 KB)
    [v3] Thu, 6 Jun 2024 04:49:37 UTC (292 KB)
    Mingjia Huo, Sai Ashish Somayajula, Youwei Liang, Ruisi Zhang, Farinaz Koushanfar, Pengtao Xie

Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Achieving both the detectability of inserted watermarks and the semantic quality of generated texts is challenging. While current watermarking algorithms have made promising progress in this direction, there remains significant scope for improvement. To address these challenges, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at this https URL.

------------

`[2405.13902] LOGIN: A Large Language Model Consulted Graph Neural Network Training Framework <https://arxiv.org/abs/2405.13902>`__ LOGIN:一个大型语言模型参考图神经网络训练框架

::

    replaced with revised version Thu, 6 Jun 2024 08:29:31 GMT
    Submission history From: Yiran Qiao [view email]
    [v1] Wed, 22 May 2024 18:17:20 UTC (1,865 KB)
    [v2] Thu, 6 Jun 2024 08:29:31 UTC (1,549 KB)
    Yiran Qiao, Xiang Ao, Yang Liu, Jiarong Xu, Xiaoqian Sun, Qing He

Recent prevailing works on graph machine learning typically follow a similar methodology that involves designing advanced variants of graph neural networks (GNNs) to maintain the superior performance of GNNs on different graphs. In this paper, we aim to streamline the GNN design process and leverage the advantages of Large Language Models (LLMs) to improve the performance of GNNs on downstream tasks. We formulate a new paradigm, coined "LLMs-as-Consultants," which integrates LLMs with GNNs in an interactive manner. A framework named LOGIN (LLM Consulted GNN training) is instantiated, empowering the interactive utilization of LLMs within the GNN training process. First, we attentively craft concise prompts for spotted nodes, carrying comprehensive semantic and topological information, and serving as input to LLMs. Second, we refine GNNs by devising a complementary coping mechanism that utilizes the responses from LLMs, depending on their correctness. We empirically evaluate the effectiveness of LOGIN on node classification tasks across both homophilic and heterophilic graphs. The results illustrate that even basic GNN architectures, when employed within the proposed LLMs-as-Consultants paradigm, can achieve comparable performance to advanced GNNs with intricate designs. Our codes are available at this https URL.

------------

`[2406.02290] A Study of Optimizations for Fine-tuning Large Language Models <https://arxiv.org/abs/2406.02290>`__ 大型语言模型微调优化研究

::

    replaced with revised version Thu, 6 Jun 2024 16:09:31 GMT
    Submission history From: Nikhil Pandey [view email]
    [v1] Tue, 4 Jun 2024 13:05:47 UTC (477 KB)
    [v2] Thu, 6 Jun 2024 16:09:31 UTC (479 KB)
    Arjun Singh, Nikhil Pandey, Anup Shirgaonkar, Pavan Manoj, Vijay Aski

Fine-tuning large language models is a popular choice among users trying to adapt them for specific applications. However, fine-tuning these models is a demanding task because the user has to examine several factors, such as resource budget, runtime, model size and context length among others. A specific challenge is that fine-tuning is memory intensive, imposing constraints on the required hardware memory and context length of training data that can be handled. In this work, we share a detailed study on a variety of fine-tuning optimizations across different fine-tuning scenarios. In particular, we assess Gradient Checkpointing, Low-Rank Adaptation, DeepSpeed's Zero Redundancy Optimizer and FlashAttention. With a focus on memory and runtime, we examine the impact of different optimization combinations on GPU memory usage and execution runtime during fine-tuning phase. We provide our recommendation on the best default optimization for balancing memory and runtime across diverse model sizes. We share effective strategies for fine-tuning very large models with tens or hundreds of billions of parameters and enabling large context lengths during fine-tuning. Furthermore, we propose the appropriate optimization mixtures for fine-tuning under GPU resource limitations.

------------

`[2406.02616] Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A Model-Based Reinforcement Learning Approach <https://arxiv.org/abs/2406.02616>`__ 边缘计算中无线LLM推理的自适应层分裂:基于模型的强化学习方法

::

    replaced with revised version Thu, 6 Jun 2024 09:07:59 GMT
    Submission history From: Yuxuan Chen [view email]
    [v1] Mon, 3 Jun 2024 09:41:42 UTC (2,382 KB)
    [v2] Thu, 6 Jun 2024 09:07:59 UTC (2,393 KB)
    Yuxuan Chen, Rongpeng Li, Xiaoxue Yu, Zhifeng Zhao, and Honggang Zhang

Optimizing the deployment of large language models (LLMs) in edge computing environments is critical for enhancing privacy and computational efficiency. Toward efficient wireless LLM inference in edge computing, this study comprehensively analyzes the impact of different splitting points in mainstream open-source LLMs. On this basis, this study introduces a framework taking inspiration from model-based reinforcement learning (MBRL) to determine the optimal splitting point across the edge and user equipment (UE). By incorporating a reward surrogate model, our approach significantly reduces the computational cost of frequent performance evaluations. Extensive simulations demonstrate that this method effectively balances inference performance and computational load under varying network conditions, providing a robust solution for LLM deployment in decentralized settings.

------------

`[2312.03668] Integrating Pre-Trained Speech and Language Models for End-to-End Speech Recognition <https://arxiv.org/abs/2312.03668>`__ 

::

    replaced with revised version Thu, 6 Jun 2024 15:24:16 GMT
    Submission history From: Yukiya Hono [view email]
    [v1] Wed, 6 Dec 2023 18:34:42 UTC (172 KB)
    [v2] Thu, 6 Jun 2024 15:24:16 UTC (239 KB)
    Yukiya Hono, Koh Mitsuda, Tianyu Zhao, Kentaro Mitsui, Toshiaki Wakatsuki, Kei Sawada

Advances in machine learning have made it possible to perform various text and speech processing tasks, such as automatic speech recognition (ASR), in an end-to-end (E2E) manner. E2E approaches utilizing pre-trained models are gaining attention for conserving training data and resources. However, most of their applications in ASR involve only one of either a pre-trained speech or a language model. This paper proposes integrating a pre-trained speech representation model and a large language model (LLM) for E2E ASR. The proposed model enables the optimization of the entire ASR process, including acoustic feature extraction and acoustic and language modeling, by combining pre-trained models with a bridge network and also enables the application of remarkable developments in LLM utilization, such as parameter-efficient domain adaptation and inference optimization. Experimental results demonstrate that the proposed model achieves a performance comparable to that of modern E2E ASR models by utilizing powerful pre-training models with the proposed integrated approach.

------------

`[2401.04621] DebugBench: Evaluating Debugging Capability of Large Language Models <https://arxiv.org/abs/2401.04621>`__ DebugBench:大型语言模型调试能力评估

::

    replaced with revised version Thu, 6 Jun 2024 06:10:00 GMT
    Submission history From: Runchu Tian [view email]
    [v1] Tue, 9 Jan 2024 15:46:38 UTC (2,288 KB)
    [v2] Thu, 11 Jan 2024 11:48:36 UTC (2,288 KB)
    [v3] Thu, 6 Jun 2024 06:10:00 UTC (2,291 KB)
    Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Haotian Hui, Weichuan Liu, Zhiyuan Liu, Maosong Sun

Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and four open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models relatively lower pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.

------------

`[2401.16467] ReGAL: Refactoring Programs to Discover Generalizable Abstractions <https://arxiv.org/abs/2401.16467>`__ ReGAL:重构程序以发现可通用的抽象

::

    replaced with revised version Thu, 6 Jun 2024 17:31:07 GMT
    Submission history From: Elias Stengel-Eskin [view email]
    [v1] Mon, 29 Jan 2024 18:45:30 UTC (498 KB)
    [v2] Thu, 6 Jun 2024 17:31:07 UTC (3,275 KB)
    Elias Stengel-Eskin, Archiki Prasad, Mohit Bansal

While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e., restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On five datasets -- LOGO graphics generation, Date reasoning, TextCraft (a Minecraft-based text-game) MATH, and TabMWP -- both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL functions. For CodeLlama-13B, ReGAL results in absolute accuracy increases of 11.5% on LOGO, 26.1% on date understanding, and 8.1% on TextCraft, outperforming GPT-3.5 in two of three domains. Our analysis reveals ReGAL's abstractions encapsulate frequently-used subroutines as well as environment dynamics.

------------

`[2405.00899] Characterising the Creative Process in Humans and Large Language Models <https://arxiv.org/abs/2405.00899>`__ 描述人类和大型语言模型的创造过程

::

    replaced with revised version Wed, 5 Jun 2024 19:15:43 GMT
    Submission history From: Surabhi S Nath [view email]
    [v1] Wed, 1 May 2024 23:06:46 UTC (1,024 KB)
    [v2] Wed, 5 Jun 2024 19:15:43 UTC (1,024 KB)
    Surabhi S. Nath, Peter Dayan and Claire Stevenson

Large language models appear quite creative, often performing on par with the average human on creative tasks. However, research on LLM creativity has focused solely on \textit{products}, with little attention on the creative \textit{process}. Process analyses of human creativity often require hand-coded categories or exploit response times, which do not apply to LLMs. We provide an automated method to characterise how humans and LLMs explore semantic spaces on the Alternate Uses Task, and contrast with behaviour in a Verbal Fluency Task. We use sentence embeddings to identify response categories and compute semantic similarities, which we use to generate jump profiles. Our results corroborate earlier work in humans reporting both persistent (deep search in few semantic spaces) and flexible (broad search across multiple semantic spaces) pathways to creativity, where both pathways lead to similar creativity scores. LLMs were found to be biased towards either persistent or flexible paths, that varied across tasks. Though LLMs as a population match human profiles, their relationship with creativity is different, where the more flexible models score higher on creativity. Our dataset and scripts are available on \href{this https URL}{GitHub}.

------------

`[2403.07974] LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code <https://arxiv.org/abs/2403.07974>`__ LiveCodeBench:大型代码语言模型的整体无污染评估

::

    replaced with revised version Thu, 6 Jun 2024 17:41:21 GMT
    Submission history From: Naman Jain [view email]
    [v1] Tue, 12 Mar 2024 17:58:04 UTC (6,890 KB)
    [v2] Thu, 6 Jun 2024 17:41:21 UTC (7,772 KB)
    Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica

Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and May 2024. We have evaluated 18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model

------------

`[2405.19732] Two Optimizers Are Better Than One: LLM Catalyst Empowers Gradient-Based Optimization for Prompt Tuning <https://arxiv.org/abs/2405.19732>`__ 两个优化器比一个好:LLM Catalyst可实现基于梯度的优化，以进行及时调整

::

    replaced with revised version Thu, 6 Jun 2024 04:59:27 GMT
    Submission history From: Zixian Guo [view email]
    [v1] Thu, 30 May 2024 06:24:14 UTC (442 KB)
    [v2] Fri, 31 May 2024 08:13:34 UTC (442 KB)
    [v3] Thu, 6 Jun 2024 04:59:27 UTC (442 KB)
    Zixian Guo, Ming Liu, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo

Learning a skill generally relies on both practical experience by doer and insightful high-level guidance by instructor. Will this strategy also work well for solving complex non-convex optimization problems? Here, a common gradient-based optimizer acts like a disciplined doer, making locally optimal update at each step. Recent methods utilize large language models (LLMs) to optimize solutions for concrete problems by inferring from natural language instructions, akin to a high-level instructor. In this paper, we show that these two optimizers are complementary to each other, suggesting a collaborative optimization approach. The gradient-based optimizer and LLM-based optimizer are combined in an interleaved manner. We instruct LLMs using task descriptions and timely optimization trajectories recorded during gradient-based optimization. Inferred results from LLMs are used as restarting points for the next stage of gradient optimization. By leveraging both the locally rigorous gradient-based optimizer and the high-level deductive LLM-based optimizer, our combined optimization method consistently yields improvements over competitive baseline prompt tuning methods. Our results demonstrate the synergistic effect of conventional gradient-based optimization and the inference ability of LLMs. The code is released at this https URL.

------------

`[2406.03248] Large Language Models as Evaluators for Recommendation Explanations <https://arxiv.org/abs/2406.03248>`__ 大型语言模型作为推荐解释的评估器

::

    replaced with revised version Thu, 6 Jun 2024 04:31:37 GMT
    Submission history From: Xiaoyu Zhang [view email]
    [v1] Wed, 5 Jun 2024 13:23:23 UTC (3,067 KB)
    [v2] Thu, 6 Jun 2024 04:31:37 UTC (3,067 KB)
    Xiaoyu Zhang, Yishan Li, Jiayin Wang, Bowen Sun, Weizhi Ma, Peijie Sun, Min Zhang

The explainability of recommender systems has attracted significant attention in academia and industry. Many efforts have been made for explainable recommendations, yet evaluating the quality of the explanations remains a challenging and unresolved issue. In recent years, leveraging LLMs as evaluators presents a promising avenue in Natural Language Processing tasks (e.g., sentiment classification, information extraction), as they perform strong capabilities in instruction following and common-sense reasoning. However, evaluating recommendation explanatory texts is different from these NLG tasks, as its criteria are related to human perceptions and are usually subjective. In this paper, we investigate whether LLMs can serve as evaluators of recommendation explanations. To answer the question, we utilize real user feedback on explanations given from previous work and additionally collect third-party annotations and LLM evaluations. We design and apply a 3-level meta evaluation strategy to measure the correlation between evaluator labels and the ground truth provided by users. Our experiments reveal that LLMs, such as GPT4, can provide comparable evaluations with appropriate prompts and settings. We also provide further insights into combining human labels with the LLM evaluation process and utilizing ensembles of multiple heterogeneous LLM evaluators to enhance the accuracy and stability of evaluations. Our study verifies that utilizing LLMs as evaluators can be an accurate, reproducible and cost-effective solution for evaluating recommendation explanation texts. Our code is available at this https URL.

------------

`[2309.00169] RepCodec: A Speech Representation Codec for Speech Tokenization <https://arxiv.org/abs/2309.00169>`__ RepCodec:用于语音标记化的语音表示编解码器

::

    replaced with revised version Thu, 6 Jun 2024 07:36:46 GMT
    Submission history From: Zhichao Huang [view email]
    [v1] Thu, 31 Aug 2023 23:26:10 UTC (740 KB)
    [v2] Thu, 6 Jun 2024 07:36:46 UTC (1,825 KB)
    Zhichao Huang, Chutong Meng, Tom Ko

With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.

------------

-----------
Index (115)
-----------

`[2406.03997] HackAtari: Atari Learning Environments for Robust and Continual Reinforcement Learning <https://arxiv.org/abs/2406.03997>`__ HackAtari:用于鲁棒和持续强化学习的Atari学习环境

`[2406.03589] Ranking Manipulation for Conversational Search Engines <https://arxiv.org/abs/2406.03589>`__ 对话式搜索引擎的排名操纵

`[2406.03600] Knowledge-Infused Legal Wisdom: Navigating LLM Consultation through the Lens of Diagnostics and Positive-Unlabeled Reinforcement Learning <https://arxiv.org/abs/2406.03600>`__ 知识注入的法律智慧:通过诊断和积极无标签强化学习的视角引导LLM咨询

`[2406.03689] Evaluating the World Model Implicit in a Generative Model <https://arxiv.org/abs/2406.03689>`__ 评估生成模型中隐含的世界模型

`[2406.03725] LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification <https://arxiv.org/abs/2406.03725>`__ LLMEmbed:重新思考轻量级LLM在文本分类中的真正作用

`[2406.03816] ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search <https://arxiv.org/abs/2406.03816>`__ ReST-MCTS*:基于过程奖励引导树搜索的LLM自我训练

`[2406.03827] Chaos with Keywords: Exposing Large Language Models Sycophancy to Misleading Keywords and Evaluating Defense Strategies <https://arxiv.org/abs/2406.03827>`__ 关键词混乱:暴露大型语言模型对误导性关键词的谄媚和评估防御策略

`[2406.03847] Lean Workbook: A large-scale Lean problem set formalized from natural language math problems <https://arxiv.org/abs/2406.03847>`__ 精益工作簿:由自然语言数学问题形式化的大规模精益问题集

`[2406.03882] Spontaneous Speech-Based Suicide Risk Detection Using Whisper and Large Language Models <https://arxiv.org/abs/2406.03882>`__ 基于耳语和大型语言模型的自发语音自杀风险检测

`[2406.03897] HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew <https://arxiv.org/abs/2406.03897>`__ HeSum:希伯来语文本摘要的新数据集

`[2406.03949] UltraMedical: Building Specialized Generalists in Biomedicine <https://arxiv.org/abs/2406.03949>`__ 超医学:生物医学专科建设

`[2406.03963] A + B: A General Generator-Reader Framework for Optimizing LLMs to Unleash Synergy Potential <https://arxiv.org/abs/2406.03963>`__

`[2406.03986] On The Persona-based Summarization of Domain-Specific Documents <https://arxiv.org/abs/2406.03986>`__ 基于人物角色的领域文档摘要研究

`[2406.03993] Assessing LLMs for Zero-shot Abstractive Summarization Through the Lens of Relevance Paraphrasing <https://arxiv.org/abs/2406.03993>`__ 通过相关复述的视角评估llm的零样本抽象摘要

`[2406.04064] Ask LLMs Directly, "What shapes your bias?": Measuring Social Bias in Large Language Models <https://arxiv.org/abs/2406.04064>`__ 直接问llm:“是什么塑造了你的偏见?”:在大型语言模型中测量社会偏见

`[2406.04113] Uncovering Limitations of Large Language Models in Information Seeking from Tables <https://arxiv.org/abs/2406.04113>`__ 揭示大型语言模型在从表中查找信息方面的局限性

`[2406.04127] Are We Done with MMLU? <https://arxiv.org/abs/2406.04127>`__ MMLU搞定了吗?

`[2406.04136] Legal Judgment Reimagined: PredEx and the Rise of Intelligent AI Interpretation in Indian Courts <https://arxiv.org/abs/2406.04136>`__ 重新设想法律判决:PredEx和印度法院智能人工智能解释的兴起

`[2406.04143] Do Language Models Understand Morality? Towards a Robust Detection of Moral Content <https://arxiv.org/abs/2406.04143>`__ 语言模型理解道德吗?走向对道德内容的鲁棒检测

`[2406.04145] Every Answer Matters: Evaluating Commonsense with Probabilistic Measures <https://arxiv.org/abs/2406.04145>`__ 每个答案都很重要:用概率测度评估常识

`[2406.04175] Confabulation: The Surprising Value of Large Language Model Hallucinations <https://arxiv.org/abs/2406.04175>`__ 虚构:大型语言模型幻觉的惊人价值

`[2406.04202] Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language Model <https://arxiv.org/abs/2406.04202>`__ 基于微调预训练大型语言模型的法律文件起草

`[2406.04214] ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models <https://arxiv.org/abs/2406.04214>`__ ValueBench:综合评估大型语言模型的价值取向和理解

`[2406.04216] What Do Language Models Learn in Context? The Structured Task Hypothesis <https://arxiv.org/abs/2406.04216>`__ 语言模型在上下文中学习什么?结构化任务假说

`[2406.04220] BEADs: Bias Evaluation Across Domains <https://arxiv.org/abs/2406.04220>`__ BEADs:跨域偏差评估

`[2406.04267] Transformers need glasses! Information over-squashing in language tasks <https://arxiv.org/abs/2406.04267>`__ 变形金刚需要眼镜!语言任务中的信息过度压缩

`[2406.04278] Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People <https://arxiv.org/abs/2406.04278>`__ 通过对人进行采样来描述人类和llm对话语调的相似性和差异性

`[2406.04289] What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages <https://arxiv.org/abs/2406.04289>`__ 哪些语言易于语言建模?从学习概率正则语言的角度来看

`[2406.04331] PaCE: Parsimonious Concept Engineering for Large Language Models <https://arxiv.org/abs/2406.04331>`__ PaCE:大型语言模型的简约概念工程

`[2406.03505] Dynamic and Adaptive Feature Generation with LLM <https://arxiv.org/abs/2406.03505>`__ 基于LLM的动态和自适应特征生成

`[2406.03614] Advancing Anomaly Detection: Non-Semantic Financial Data Encoding with LLMs <https://arxiv.org/abs/2406.03614>`__ 推进异常检测:基于LLMs的非语义金融数据编码

`[2406.03707] What Should Embeddings Embed? Autoregressive Models Represent Latent Generating Distributions <https://arxiv.org/abs/2406.03707>`__ 嵌入应该嵌入什么?自回归模型代表潜在的生成分布

`[2406.03777] Empirical Guidelines for Deploying LLMs onto Resource-constrained Edge Devices <https://arxiv.org/abs/2406.03777>`__

`[2406.04081] Bootstrapping Expectiles in Reinforcement Learning <https://arxiv.org/abs/2406.04081>`__ 强化学习中的自助法预期

`[2406.04088] Deterministic Uncertainty Propagation for Improved Model-Based Offline Reinforcement Learning <https://arxiv.org/abs/2406.04088>`__ 基于确定性不确定性传播的改进模型离线强化学习

`[2406.04274] Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models <https://arxiv.org/abs/2406.04274>`__ 对抗性评论的自玩:可证明和可扩展的语言模型离线对齐

`[2406.04276] Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next Generation Networks <https://arxiv.org/abs/2406.04276>`__ 生成式AI-in-the-loop:将llm和GPTs集成到下一代网络中

`[2406.04306] Semantically Diverse Language Generation for Uncertainty Estimation in Language Models <https://arxiv.org/abs/2406.04306>`__ 面向语言模型不确定性估计的语义多样性语言生成

`[2406.04344] Verbalized Machine Learning: Revisiting Machine Learning with Language Models <https://arxiv.org/abs/2406.04344>`__ 语言化机器学习:用语言模型重新审视机器学习

`[2406.03718] Generalization-Enhanced Code Vulnerability Detection via Multi-Task Instruction Fine-Tuning <https://arxiv.org/abs/2406.03718>`__ 基于多任务指令微调的泛化增强代码漏洞检测

`[2406.04337] Coherent Zero-Shot Visual Instruction Generation <https://arxiv.org/abs/2406.04337>`__ 连贯零样本视觉指令生成

`[2406.03706] Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model <https://arxiv.org/abs/2406.03706>`__ 利用多模态上下文和大型语言模型改进基于音频编解码器的零样本文本到语音合成

`[2406.03243] Llumnix: Dynamic Scheduling for Large Language Model Serving <https://arxiv.org/abs/2406.03243>`__ lumnix:大型语言模型服务的动态调度

`[2406.03628] Synthetic Oversampling: Theory and A Practical Approach Using LLMs to Address Data Imbalance <https://arxiv.org/abs/2406.03628>`__ 合成过采样:用llm解决数据不平衡的理论和实践方法

`[2406.03636] Synthetic Programming Elicitation and Repair for Text-to-Code in Very Low-Resource Programming Languages <https://arxiv.org/abs/2406.03636>`__ 低资源编程语言文本到代码的综合编程启发与修复

`[2406.03757] RoboCoder: Robotic Learning from Basic Skills to General Tasks with Large Language Models <https://arxiv.org/abs/2406.03757>`__ RoboCoder:基于大型语言模型的从基本技能到一般任务的机器人学习

`[2309.11361] Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG) Using LLM <https://arxiv.org/abs/2309.11361>`__ 材料科学知识图谱问答(KGQA4MAT):用LLM为金属有机框架知识图谱(MOF-KG)开发自然语言界面

`[2405.04776] Chain of Thoughtlessness? An Analysis of CoT in Planning <https://arxiv.org/abs/2405.04776>`__ 轻率的连锁反应?规划中的CoT分析

`[2405.17345] Exploring and steering the moral compass of Large Language Models <https://arxiv.org/abs/2405.17345>`__ 探索和引导大型语言模型的道德指南针

`[2306.09782] Full Parameter Fine-tuning for Large Language Models with Limited Resources <https://arxiv.org/abs/2306.09782>`__ 资源有限的大型语言模型的全参数微调

`[2309.08047] Bias in News Summarization: Measures, Pitfalls and Corpora <https://arxiv.org/abs/2309.08047>`__ 新闻摘要中的偏见:措施、陷阱和语料库

`[2310.00160] Self-Specialization: Uncovering Latent Expertise within Large Language Models <https://arxiv.org/abs/2310.00160>`__ 自我专业化:揭示大型语言模型中的潜在专业知识

`[2310.13571] Why Can Large Language Models Generate Correct Chain-of-Thoughts? <https://arxiv.org/abs/2310.13571>`__ 为什么大型语言模型可以生成正确的思维链?

`[2311.09033] MELA: Multilingual Evaluation of Linguistic Acceptability <https://arxiv.org/abs/2311.09033>`__ MELA:语言可接受性的多语言评估

`[2311.09213] GENEVA: GENErating and Visualizing branching narratives using LLMs <https://arxiv.org/abs/2311.09213>`__ GENEVA:使用llm生成和可视化分支叙述

`[2311.09832] WatME: Towards Lossless Watermarking Through Lexical Redundancy <https://arxiv.org/abs/2311.09832>`__ WatME:通过词汇冗余实现无损水印

`[2312.08800] Evaluating Large Language Models for Health-related Queries with Presuppositions <https://arxiv.org/abs/2312.08800>`__ 基于预设的健康相关查询大型语言模型评估

`[2312.14591] Reasons to Reject? Aligning Language Models with Judgments <https://arxiv.org/abs/2312.14591>`__ 拒绝的理由?将语言模型与判断对齐

`[2401.05749] A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism <https://arxiv.org/abs/2401.05749>`__ 惊人数量的Web是机器翻译的:来自多路并行的见解

`[2401.06568] Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation <https://arxiv.org/abs/2401.06568>`__ 迷失在源语言中:大型语言模型如何评估机器翻译质量

`[2401.06688] Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation <https://arxiv.org/abs/2401.06688>`__ 不要排名，要结合!用质量估计结合机器翻译假设

`[2401.11382] Using Large Language Model for End-to-End Chinese ASR and NER <https://arxiv.org/abs/2401.11382>`__ 基于大型语言模型的端到端中文识别和NER

`[2401.14556] Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling <https://arxiv.org/abs/2401.14556>`__ 右看有时是对的:研究仅解码器的llm进行序列标记的能力

`[2401.18046] Multipath parsing in the brain <https://arxiv.org/abs/2401.18046>`__ 大脑中的多路径解析

`[2402.06733] NICE: To Optimize In-Context Examples or Not? <https://arxiv.org/abs/2402.06733>`__ 很好:是否优化上下文中的示例?

`[2402.10073] Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence <https://arxiv.org/abs/2402.10073>`__ 两者都很重要:在不影响一般智力的情况下提高大型语言模型的情商

`[2402.10571] Direct Preference Optimization with an Offset <https://arxiv.org/abs/2402.10571>`__ 带偏移量的直接偏好优化

`[2402.10890] When is Tree Search Useful for LLM Planning? It Depends on the Discriminator <https://arxiv.org/abs/2402.10890>`__ 什么时候树搜索对LLM规划有用?这取决于判别器

`[2402.11138] Contrastive Instruction Tuning <https://arxiv.org/abs/2402.11138>`__ 对比指令调优

`[2402.11349] Language Models Don't Learn the Physical Manifestation of Language <https://arxiv.org/abs/2402.11349>`__ 语言模型不学习语言的物理表现形式

`[2402.11485] LEIA: Facilitating Cross-lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation <https://arxiv.org/abs/2402.11485>`__ LEIA:基于实体的数据增强促进语言模型中的跨语言知识迁移

`[2402.11517] Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM <https://arxiv.org/abs/2402.11517>`__ 知识到SQL:使用数据专家LLM增强SQL生成

`[2402.11548] KMMLU: Measuring Massive Multitask Language Understanding in Korean <https://arxiv.org/abs/2402.11548>`__ KMMLU:大规模韩语多任务语言理解测试

`[2402.11597] Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once? <https://arxiv.org/abs/2402.11597>`__ 多任务推理:大型语言模型能否同时遵循多条指令?

`[2402.11894] Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models <https://arxiv.org/abs/2402.11894>`__ 自动化数据集更新以实现大型语言模型的可靠和及时评估

`[2402.12343] Emulated Disalignment: Safety Alignment for Large Language Models May Backfire! <https://arxiv.org/abs/2402.12343>`__ 模拟错位:大型语言模型的安全对齐可能会适得其反!

`[2402.15637] Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models <https://arxiv.org/abs/2402.15637>`__ 因果语言模型中上下文演示示例的顺序敏感性问题

`[2402.16438] Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models <https://arxiv.org/abs/2402.16438>`__ 特定语言神经元:大型语言模型多语言能力的关键

`[2402.16775] A Comprehensive Evaluation of Quantization Strategies for Large Language Models <https://arxiv.org/abs/2402.16775>`__ 大型语言模型量化策略的综合评估

`[2402.17447] Deep Learning Based Named Entity Recognition Models for Recipes <https://arxiv.org/abs/2402.17447>`__ 基于深度学习的菜谱命名实体识别模型

`[2402.18158] Evaluating Quantized Large Language Models <https://arxiv.org/abs/2402.18158>`__ 量化大型语言模型的评估

`[2402.18334] Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation <https://arxiv.org/abs/2402.18334>`__ 学习生成指令调优数据集的零样本任务自适应

`[2403.03167] PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset <https://arxiv.org/abs/2403.03167>`__ PARADISE:用程序警告和提示数据集评估语言模型的隐式规划技能

`[2403.06932] ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis <https://arxiv.org/abs/2403.06932>`__ ERA-CoT:通过实体关系分析改进思维链

`[2403.18680] Non-Linear Inference Time Intervention: Improving LLM Truthfulness <https://arxiv.org/abs/2403.18680>`__ 非线性推理时间干预:改善LLM真实性

`[2404.14461] Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs <https://arxiv.org/abs/2404.14461>`__ 竞赛报告:在对齐的llm中寻找通用的越狱后门

`[2405.00301] Enhanced Language Model Truthfulness with Learnable Intervention and Uncertainty Expression <https://arxiv.org/abs/2405.00301>`__ 基于可学习干预和不确定性表达的增强语言模型真实性

`[2405.09482] Beyond Flesch-Kincaid: Prompt-based Metrics Improve Difficulty Classification of Educational Texts <https://arxiv.org/abs/2405.09482>`__ 超越Flesch-Kincaid:基于提示的指标提高教育文本分类的难度

`[2405.13034] Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards Mixed Reality <https://arxiv.org/abs/2405.13034>`__ 面向混合现实的多模态细粒度训练助手自主工作流

`[2406.01026] Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors <https://arxiv.org/abs/2406.01026>`__ 加强的符号绑定使大型语言模型成为可靠的多项选择器

`[2406.01514] Decoupled Alignment for Robust Plug-and-Play Adaptation <https://arxiv.org/abs/2406.01514>`__ 面向健壮即插即用适应的解耦对齐

`[2406.02886] PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs <https://arxiv.org/abs/2406.02886>`__ PLaD:基于伪偏好对的大规模语言模型蒸馏

`[2406.03151] Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation <https://arxiv.org/abs/2406.03151>`__ 你站在哪一边?用于端到端论点总结和评估的多任务数据集

`[2406.03170] StatBot.Swiss: Bilingual Open Data Exploration in Natural Language <https://arxiv.org/abs/2406.03170>`__ StatBot。Swiss:自然语言双语开放数据探索

`[2310.07579] In-Context Unlearning: Language Models as Few Shot Unlearners <https://arxiv.org/abs/2310.07579>`__

`[2310.09639] DPZero: Private Fine-Tuning of Language Models without Backpropagation <https://arxiv.org/abs/2310.09639>`__ DPZero:无反向传播的语言模型私有微调

`[2310.10195] AdaLomo: Low-memory Optimization with Adaptive Learning Rate <https://arxiv.org/abs/2310.10195>`__ AdaLomo:自适应学习率的低内存优化

`[2401.00793] SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models <https://arxiv.org/abs/2401.00793>`__ SecFormer:面向大型语言模型的快速准确隐私保护推理

`[2401.17263] Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks <https://arxiv.org/abs/2401.17263>`__ 针对语言模型越狱攻击的鲁棒提示优化

`[2402.10450] PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control <https://arxiv.org/abs/2402.10450>`__ PRISE:用于学习控制时序动作抽象的llm风格序列压缩

`[2402.12991] TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification <https://arxiv.org/abs/2402.12991>`__ 陷阱:用于黑盒识别的定向随机对抗性提示蜜罐

`[2402.14979] Optimizing Language Models for Human Preferences is a Causal Inference Problem <https://arxiv.org/abs/2402.14979>`__ 根据人类偏好优化语言模型是一个因果推理问题

`[2402.17641] Variational Learning is Effective for Large Deep Networks <https://arxiv.org/abs/2402.17641>`__ 变分学习对于大型深度网络是有效的

`[2402.18059] Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models <https://arxiv.org/abs/2402.18059>`__ 具有增强可检测性和语义一致性的大型语言模型特定标记水印

`[2405.13902] LOGIN: A Large Language Model Consulted Graph Neural Network Training Framework <https://arxiv.org/abs/2405.13902>`__ LOGIN:一个大型语言模型参考图神经网络训练框架

`[2406.02290] A Study of Optimizations for Fine-tuning Large Language Models <https://arxiv.org/abs/2406.02290>`__ 大型语言模型微调优化研究

`[2406.02616] Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A Model-Based Reinforcement Learning Approach <https://arxiv.org/abs/2406.02616>`__ 边缘计算中无线LLM推理的自适应层分裂:基于模型的强化学习方法

`[2312.03668] Integrating Pre-Trained Speech and Language Models for End-to-End Speech Recognition <https://arxiv.org/abs/2312.03668>`__

`[2401.04621] DebugBench: Evaluating Debugging Capability of Large Language Models <https://arxiv.org/abs/2401.04621>`__ DebugBench:大型语言模型调试能力评估

`[2401.16467] ReGAL: Refactoring Programs to Discover Generalizable Abstractions <https://arxiv.org/abs/2401.16467>`__ ReGAL:重构程序以发现可通用的抽象

`[2405.00899] Characterising the Creative Process in Humans and Large Language Models <https://arxiv.org/abs/2405.00899>`__ 描述人类和大型语言模型的创造过程

`[2403.07974] LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code <https://arxiv.org/abs/2403.07974>`__ LiveCodeBench:大型代码语言模型的整体无污染评估

`[2405.19732] Two Optimizers Are Better Than One: LLM Catalyst Empowers Gradient-Based Optimization for Prompt Tuning <https://arxiv.org/abs/2405.19732>`__ 两个优化器比一个好:LLM Catalyst可实现基于梯度的优化，以进行及时调整

`[2406.03248] Large Language Models as Evaluators for Recommendation Explanations <https://arxiv.org/abs/2406.03248>`__ 大型语言模型作为推荐解释的评估器

`[2309.00169] RepCodec: A Speech Representation Codec for Speech Tokenization <https://arxiv.org/abs/2309.00169>`__ RepCodec:用于语音标记化的语音表示编解码器

