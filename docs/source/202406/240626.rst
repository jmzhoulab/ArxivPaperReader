240626
========

----------
Survey (2)
----------

`[2406.16891] Survey on Reasoning Capabilities and Accessibility of Large Language Models Using Biology-related Questions <https://arxiv.org/abs/2406.16891>`__ 基于生物学相关问题的大型语言模型推理能力与可及性综述

::

    Sat, 11 May 2024 20:25:40 GMT
    Michael Ackerman

This research paper discusses the advances made in the past decade in biomedicine and Large Language Models. To understand how the advances have been made hand-in-hand with one another, the paper also discusses the integration of Natural Language Processing techniques and tools into biomedicine. Finally, the goal of this paper is to expand on a survey conducted last year (2023) by introducing a new list of questions and prompts for the top two language models. Through this survey, this paper seeks to quantify the improvement made in the reasoning abilities in LLMs and to what extent those improvements are felt by the average user. Additionally, this paper seeks to extend research on retrieval of biological literature by prompting the LLM to answer open-ended questions in great depth.

------------

`[2406.16937] A Complete Survey on LLM-based AI Chatbots <https://arxiv.org/abs/2406.16937>`__ 基于llm的AI聊天机器人完整综述

::

    Mon, 17 Jun 2024 09:39:34 GMT
    Sumit Kumar Dam, Choong Seon Hong, Yu Qiao, and Chaoning Zhang

The past few decades have witnessed an upsurge in data, forming the foundation for data-hungry, learning-based AI technology. Conversational agents, often referred to as AI chatbots, rely heavily on such data to train large language models (LLMs) and generate new content (knowledge) in response to user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have set new standards in the AI community. This paper presents a complete survey of the evolution and deployment of LLM-based chatbots in various sectors. We first summarize the development of foundational chatbots, followed by the evolution of LLMs, and then provide an overview of LLM-based chatbots currently in use and those in the development phase. Recognizing AI chatbots as tools for generating new knowledge, we explore their diverse applications across various industries. We then discuss the open challenges, considering how the data used to train the LLMs and the misuse of the generated knowledge can cause several issues. Finally, we explore the future outlook to augment their efficiency and reliability in numerous applications. By addressing key milestones and the present-day context of LLM-based chatbots, our survey invites readers to delve deeper into this realm, reflecting on how their next generation will reshape conversational AI.

------------

--------------
Benchmark (10)
--------------

`[2406.17158] DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs <https://arxiv.org/abs/2406.17158>`__ DEXTER:使用llm的开放域复杂问答基准

::

    Mon, 24 Jun 2024 22:09:50 GMT
    Venktesh V. Deepali Prabhu and Avishek Anand

Open-domain complex Question Answering (QA) is a difficult task with challenges in evidence retrieval and reasoning. The complexity of such questions could stem from questions being compositional, hybrid evidence, or ambiguity in questions. While retrieval performance for classical QA tasks is well explored, their capabilities for heterogeneous complex retrieval tasks, especially in an open-domain setting, and the impact on downstream QA performance, are relatively unexplored. To address this, in this work, we propose a benchmark composing diverse complex QA tasks and provide a toolkit to evaluate state-of-the-art pre-trained dense and sparse retrieval models in an open-domain setting. We observe that late interaction models and surprisingly lexical models like BM25 perform well compared to other pre-trained dense retrieval models. In addition, since context-based reasoning is critical for solving complex QA tasks, we also evaluate the reasoning capabilities of LLMs and the impact of retrieval performance on their reasoning capabilities.
Through experiments, we observe that much progress is to be made in retrieval for complex QA to improve downstream QA performance. Our software and related data can be accessed at https://github.com/VenkteshV/DEXTER

------------

`[2406.17419] Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA <https://arxiv.org/abs/2406.17419>`__ 

::

    Tue, 25 Jun 2024 09:42:56 GMT
    Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, Yongbin Li

Long-context modeling capabilities have garnered widespread attention, leading to the emergence of Large Language Models (LLMs) with ultra-context windows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong's test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model's long-context modeling capabilities.

------------

`[2406.17535] Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian Benchmark <https://arxiv.org/abs/2406.17535>`__ Disce aut赤字:在INVALSI意大利基准上评估LLMs的熟练程度

::

    Tue, 25 Jun 2024 13:20:08 GMT
    Fabio Mercorio, Mario Mezzanzanica, Daniele Potert\`i, Antonio Serino, Andrea Seveso

Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to generate and manipulate human language, highlighting their potential across various applications. Evaluating LLMs in languages other than English is crucial for ensuring their linguistic versatility, cultural relevance, and applicability in diverse global contexts, thus broadening their usability and effectiveness. We tackle this challenge by introducing a structured benchmark using the INVALSI tests, a set of well-established assessments designed to measure educational competencies across Italy. Our study makes three primary contributions: Firstly, we adapt the INVALSI benchmark for automated LLM evaluation, which involves rigorous adaptation of the test format to suit automated processing while retaining the essence of the original tests. Secondly, we provide a detailed assessment of current LLMs, offering a crucial reference point for the academic community. Finally, we visually compare the performance of these models against human results.
Additionally, researchers are invited to submit their models for ongoing evaluation, ensuring the benchmark remains a current and valuable resource.

------------

`[2406.17566] FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating Toxicity in French Texts <https://arxiv.org/abs/2406.17566>`__ 法语毒性提示:评估和减轻法语文本毒性的大型基准

::

    Tue, 25 Jun 2024 14:02:11 GMT
    Caroline Brun, Vassilina Nikoulina

Large language models (LLMs) are increasingly popular but are also prone to generating bias, toxic or harmful language, which can have detrimental effects on individuals and communities. Although most efforts is put to assess and mitigate toxicity in generated content, it is primarily concentrated on English, while it's essential to consider other languages as well. For addressing this issue, we create and release FrenchToxicityPrompts, a dataset of 50K naturally occurring French prompts and their continuations, annotated with toxicity scores from a widely used toxicity classifier. We evaluate 14 different models from four prevalent open-sourced families of LLMs against our dataset to assess their potential toxicity across various dimensions. We hope that our contribution will foster future research on toxicity detection and mitigation beyond Englis

------------

`[2406.17675] Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models <https://arxiv.org/abs/2406.17675>`__ 量化AI心理学:大型语言模型的心理测量基准

::

    Tue, 25 Jun 2024 16:09:08 GMT
    Yuan Li, Yue Huang, Hongyi Wang, Xiangliang Zhang, James Zou, Lichao Sun

Large Language Models (LLMs) have demonstrated exceptional task-solving capabilities, increasingly adopting roles akin to human-like assistants. The broader integration of LLMs into society has sparked interest in whether they manifest psychological attributes, and whether these attributes are stable-inquiries that could deepen the understanding of their behaviors.
Inspired by psychometrics, this paper presents a framework for investigating psychology in LLMs, including psychological dimension identification, assessment dataset curation, and assessment with results validation. Following this framework, we introduce a comprehensive psychometrics benchmark for LLMs that covers six psychological dimensions: personality, values, emotion, theory of mind, motivation, and intelligence. This benchmark includes thirteen datasets featuring diverse scenarios and item types. Our findings indicate that LLMs manifest a broad spectrum of psychological attributes. We also uncover discrepancies between LLMs' self-reported traits and their behaviors in real-world scenarios. This paper demonstrates a thorough psychometric assessment of LLMs, providing insights into reliable evaluation and potential applications in AI and social sciences.

------------

`[2406.17681] VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation <https://arxiv.org/abs/2406.17681>`__ VarBench:基于动态变量扰动的鲁棒语言模型基准测试

::

    Tue, 25 Jun 2024 16:13:53 GMT
    Kun Qian, Shunji Wan, Claudia Tang, Youzhi Wang, Xuanming Zhang, Maximillian Chen, Zhou Yu

As large language models achieve impressive scores on traditional benchmarks, an increasing number of researchers are becoming concerned about benchmark data leakage during pre-training, commonly known as the data contamination problem.
To ensure fair evaluation, recent benchmarks release only the training and validation sets, keeping the test set labels closed-source. They require anyone wishing to evaluate his language model to submit the model's predictions for centralized processing and then publish the model's result on their leaderboard. However, this submission process is inefficient and prevents effective error analysis. To address this issue, we propose to variabilize benchmarks and evaluate language models dynamically. Specifically, we extract variables from each test case and define a value range for each variable. For each evaluation, we sample new values from these value ranges to create unique test cases, thus ensuring a fresh evaluation each time. We applied this variable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and TruthfulQA, which cover mathematical generation and multiple-choice tasks. Our experimental results demonstrate that this approach provides a more accurate assessment of the true capabilities of language models, effectively mitigating the contamination problem.

------------

`[2406.17753] Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language <https://arxiv.org/abs/2406.17753>`__ 

::

    Tue, 25 Jun 2024 17:40:47 GMT
    Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent

We are exposed to much information trying to influence us, such as teaser messages, debates, politically framed news, and propaganda - all of which use persuasive language. With the recent interest in Large Language Models (LLMs), we study the ability of LLMs to produce persuasive text. As opposed to prior work which focuses on particular domains or types of persuasion, we conduct a general study across various domains to measure and benchmark to what degree LLMs produce persuasive text - both when explicitly instructed to rewrite text to be more or less persuasive and when only instructed to paraphrase. To this end, we construct a new dataset, Persuasive-Pairs, of pairs each consisting of a short text and of a text rewritten by an LLM to amplify or diminish persuasive language. We multi-annotate the pairs on a relative scale for persuasive language. This data is not only a valuable resource in itself, but we also show that it can be used to train a regression model to predict a score of persuasive language between text pairs. This model can score and benchmark new LLMs across domains, thereby facilitating the comparison of different LLMs.
Finally, we discuss effects observed for different system prompts. Notably, we find that different 'personas' in the system prompt of LLaMA3 change the persuasive language in the text substantially, even when only instructed to paraphrase. These findings underscore the importance of investigating persuasive language in LLM generated text.

------------

`[2402.14762] MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues <https://arxiv.org/abs/2402.14762>`__ MT-Bench-101:评估多轮对话中大型语言模型的细粒度基准

::

    replaced with revised version Tue, 25 Jun 2024 13:38:41 GMT
    Submission history From: Xingyuan Bu [view email]
    [v1] Thu, 22 Feb 2024 18:21:59 UTC (1,258 KB)
    [v2] Tue, 25 Jun 2024 13:38:41 UTC (9,344 KB)
    Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang

The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities. The data and code are available at \url{this https URL}.

------------

`[2405.00716] Large Language Models in Healthcare: A Comprehensive Benchmark <https://arxiv.org/abs/2405.00716>`__ 医疗保健中的大型语言模型:一个全面的基准

::

    replaced with revised version Tue, 25 Jun 2024 17:23:22 GMT
    Submission history From: Hongjian Zhou [view email]
    [v1] Thu, 25 Apr 2024 15:51:06 UTC (124 KB)
    [v2] Tue, 25 Jun 2024 17:23:22 UTC (392 KB)
    [v3] Wed, 26 Jun 2024 17:48:18 UTC (392 KB)
    Andrew Liu, Hongjian Zhou, Yining Hua, Omid Rohanian, Anshul Thakur, Lei Clifton, David A. Clifton

The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench. We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and complex clinical tasks that are close to real-world practice, i.e., referral QA, treatment recommendation, hospitalization (long document) summarization, patient education, pharmacology QA and drug interaction for emerging drugs. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs.

------------

`[2406.16020] AudioBench: A Universal Benchmark for Audio Large Language Models <https://arxiv.org/abs/2406.16020>`__ audibench:音频大型语言模型通用基准

::

    replaced with revised version Tue, 25 Jun 2024 12:27:06 GMT
    Submission history From: Bin Wang [view email]
    [v1] Sun, 23 Jun 2024 05:40:26 UTC (8,423 KB)
    [v2] Tue, 25 Jun 2024 12:27:06 UTC (8,423 KB)
    Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, Nancy F. Chen

We introduce AudioBench, a new benchmark designed to evaluate audio large language models (AudioLLMs). AudioBench encompasses 8 distinct tasks and 26 carefully selected or newly curated datasets, focusing on speech understanding, voice interpretation, and audio scene understanding. Despite the rapid advancement of large language models, including multimodal versions, a significant gap exists in comprehensive benchmarks for thoroughly evaluating their capabilities. AudioBench addresses this gap by providing relevant datasets and evaluation metrics. In our study, we evaluated the capabilities of four models across various aspects and found that no single model excels consistently across all tasks. We outline the research outlook for AudioLLMs and anticipate that our open-source code, data, and leaderboard will offer a robust testbed for future model developments.

------------

--------------
Accelerate (8)
--------------

`[2406.17519] Entropy-Based Decoding for Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2406.17519>`__ 基于熵的检索增强大型语言模型解码

::

    Tue, 25 Jun 2024 12:59:38 GMT
    Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, Irwin King

Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective for improving the factual accuracy of generated responses.
Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the generated responses are negatively influenced by noise from both external and internal knowledge sources. In this paper, we introduce a novel, training-free decoding method guided by entropy considerations to mitigate this issue. Our approach utilizes entropy-based document-parallel ensemble decoding to prioritize low-entropy distributions from retrieved documents, thereby enhancing the extraction of relevant information of context.
Additionally, it incorporates a contrastive decoding mechanism that contrasts the obtained low-entropy ensemble distribution with the high-entropy distribution derived from the model's internal knowledge across layers, which ensures a greater emphasis on reliable external information. Extensive experiments on open-domain question answering datasets demonstrate the superiority of our method.

------------

`[2406.17755] Accelerating Clinical Evidence Synthesis with Large Language Models <https://arxiv.org/abs/2406.17755>`__ 用大型语言模型加速临床证据合成

::

    Tue, 25 Jun 2024 17:41:52 GMT
    Zifeng Wang, Lang Cao, Benjamin Danek, Yichi Zhang, Qiao Jin, Zhiyong Lu, Jimeng Sun

Automatic medical discovery by AI is a dream of many. One step toward that goal is to create an AI model to understand clinical studies and synthesize clinical evidence from the literature. Clinical evidence synthesis currently relies on systematic reviews of clinical trials and retrospective analyses from medical literature. However, the rapid expansion of publications presents challenges in efficiently identifying, summarizing, and updating evidence. We introduce TrialMind, a generative AI-based pipeline for conducting medical systematic reviews, encompassing study search, screening, and data extraction phases. We utilize large language models (LLMs) to drive each pipeline component while incorporating human expert oversight to minimize errors. To facilitate evaluation, we also create a benchmark dataset TrialReviewBench, a custom dataset with 870 annotated clinical studies from 25 meta-analysis papers across various medical treatments. Our results demonstrate that TrialMind significantly improves the literature review process, achieving high recall rates (0.897-1.000) in study searching from over 20 million PubMed studies and outperforming traditional language model embeddings-based methods in screening (Recall@20 of 0.227-0.246 vs. 0.000-0.102). Furthermore, our approach surpasses direct GPT-4 performance in result extraction, with accuracy ranging from 0.65 to 0.84. We also support clinical evidence synthesis in forest plots, as validated by eight human annotators who preferred TrialMind over the GPT-4 baseline with a winning rate of 62.5%-100% across the involved reviews. Our findings suggest that an LLM-based clinical evidence synthesis approach, such as TrialMind, can enable reliable and high-quality clinical evidence synthesis to improve clinical research efficiency.

------------

`[2406.17296] BlockLLM: Memory-Efficient Adaptation of LLMs by Selecting and Optimizing the Right Coordinate Blocks <https://arxiv.org/abs/2406.17296>`__ BlockLLM:通过选择和优化正确的坐标块实现llm的内存高效自适应

::

    Tue, 25 Jun 2024 05:45:12 GMT
    Amrutha Varshini Ramesh, Vignesh Ganapathiraman, Issam H. Laradji, Mark Schmidt

Training large language models (LLMs) for pretraining or adapting to new tasks and domains has become increasingly critical as their applications expand. However, as the model and the data sizes grow, the training process presents significant memory challenges, often requiring a prohibitive amount of GPU memory that may not be readily available. Existing methods such as low-rank adaptation (LoRA) add trainable low-rank matrix factorizations, altering the training dynamics and limiting the model's parameter search to a low-rank subspace. GaLore, a more recent method, employs Gradient Low-Rank Projection to reduce the memory footprint, in the full parameter training setting. However GaLore can only be applied to a subset of the LLM layers that satisfy the "reversibility" property, thus limiting their applicability. In response to these challenges, we introduce BlockLLM, an approach inspired by block coordinate descent. Our method carefully selects and updates a very small subset of the trainable parameters without altering any part of its architecture and training procedure. BlockLLM achieves state-of-the-art performance in both finetuning and pretraining tasks, while reducing the memory footprint of the underlying optimization process. Our experiments demonstrate that fine-tuning with only less than 5% of the parameters, BlockLLM achieves state-of-the-art perplexity scores on the GLUE benchmarks. On Llama model pretrained on C4 dataset, BlockLLM is able to train with significantly less memory than the state-of-the-art, while still maintaining competitive performance.

------------

`[2406.17660] Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients <https://arxiv.org/abs/2406.17660>`__ Grass:使用结构化稀疏梯度计算高效的低内存LLM训练

::

    Tue, 25 Jun 2024 15:50:32 GMT
    Aashiq Muhamed, Oscar Li, David Woodruff, Mona Diab, Virginia Smith

Large language model (LLM) training and finetuning are often bottlenecked by limited GPU memory. While existing projection-based optimization methods address this by projecting gradients into a lower-dimensional subspace to reduce optimizer state memory, they typically rely on dense projection matrices, which can introduce computational and memory overheads. In this work, we propose Grass (GRAdient Stuctured Sparsification), a novel approach that leverages sparse projections to transform gradients into structured sparse updates. This design not only significantly reduces memory usage for optimizer states but also minimizes gradient memory footprint, computation, and communication costs, leading to substantial throughput improvements. Extensive experiments on pretraining and finetuning tasks demonstrate that Grass achieves competitive performance to full-rank training and existing projection-based methods. Notably, Grass enables half-precision pretraining of a 13B parameter LLaMA model on a single 40GB A100 GPU--a feat infeasible for previous methods--and yields up to a $2\times$ throughput improvement on an 8-GPU system. Code can be found at https://github.com/aashiqmuhamed/GRASS .

------------

`[2406.16976] Efficient Evolutionary Search Over Chemical Space with Large Language Models <https://arxiv.org/abs/2406.16976>`__ 基于大型语言模型的化学空间高效进化搜索

::

    Sun, 23 Jun 2024 06:22:49 GMT
    Haorui Wang, Marta Skreta, Cher-Tian Ser, Wenhao Gao, Lingkai Kong, Felix Streith-Kalthoff, Chenru Duan, Yuchen Zhuang, Yue Yu, Yanqiao Zhu, Yuanqi Du, Al\'an Aspuru-Guzik, Kirill Neklyudov, Chao Zhang

Molecular discovery, when formulated as an optimization problem, presents significant computational challenges because optimization objectives can be non-differentiable. Evolutionary Algorithms (EAs), often used to optimize black-box objectives in molecular discovery, traverse chemical space by performing random mutations and crossovers, leading to a large number of expensive objective evaluations. In this work, we ameliorate this shortcoming by incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely, we redesign crossover and mutation operations in EAs using LLMs trained on large corpora of chemical information. We perform extensive empirical studies on both commercial and open-source models on multiple tasks involving property optimization, molecular rediscovery, and structure-based drug design, demonstrating that the joint usage of LLMs with EAs yields superior performance over all baseline models across single- and multi-objective settings. We demonstrate that our algorithm improves both the quality of the final solution and convergence speed, thereby reducing the number of required objective evaluations. Our code is available at http://github.com/zoom-wang112358/MOLLEO

------------

`[2312.05795] Large Multimodal Model Compression via Efficient Pruning and Distillation at AntGroup <https://arxiv.org/abs/2312.05795>`__ AntGroup基于高效剪枝和蒸馏的大规模多模态模型压缩

::

    replaced with revised version Tue, 25 Jun 2024 03:53:28 GMT
    Submission history From: Maolin Wang [view email]
    [v1] Sun, 10 Dec 2023 06:57:48 UTC (7,417 KB)
    [v2] Tue, 25 Jun 2024 03:53:28 UTC (2,465 KB)
    Maolin Wang, Yao Zhao, Jiajia Liu, Jingdong Chen, Chenyi Zhuang, Jinjie Gu, Ruocheng Guo, Xiangyu Zhao

The deployment of Large Multimodal Models (LMMs) within AntGroup has significantly advanced multimodal tasks in payment, security, and advertising, notably enhancing advertisement audition tasks in Alipay. However, the deployment of such sizable models introduces challenges, particularly in increased latency and carbon emissions, which are antithetical to the ideals of Green AI. This paper introduces a novel multi-stage compression strategy for our proprietary LLM, AntGMM. Our methodology pivots on three main aspects: employing small training sample sizes, addressing multi-level redundancy through multi-stage pruning, and introducing an advanced distillation loss design. In our research, we constructed a dataset, the Multimodal Advertisement Audition Dataset (MAAD), from real-world scenarios within Alipay, and conducted experiments to validate the reliability of our proposed strategy. Furthermore, the effectiveness of our strategy is evident in its operational success in Alipay's real-world multimodal advertisement audition for three months from September 2023. Notably, our approach achieved a substantial reduction in latency, decreasing it from 700ms to 90ms, while maintaining online performance with only a slight performance decrease. Moreover, our compressed model is estimated to reduce electricity consumption by approximately 75 million kWh annually compared to the direct deployment of AntGMM, demonstrating our commitment to green AI initiatives. We will publicly release our code and the MAAD dataset after some reviews\footnote{this https URL\_$Pruning}.

------------

`[2402.02416] Aligner: Efficient Alignment by Learning to Correct <https://arxiv.org/abs/2402.02416>`__ Aligner:通过学习纠正有效的对齐

::

    replaced with revised version Mon, 24 Jun 2024 18:55:16 GMT
    Submission history From: Jiaming Ji [view email]
    [v1] Sun, 4 Feb 2024 09:24:51 UTC (2,207 KB)
    [v2] Tue, 6 Feb 2024 18:02:01 UTC (2,206 KB)
    [v3] Mon, 3 Jun 2024 14:33:45 UTC (2,808 KB)
    [v4] Mon, 24 Jun 2024 18:55:16 UTC (2,801 KB)
    Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, Tianyi Qiu, Yaodong Yang

With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-and-play module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model's performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of 68.9% in helpfulness and 23.8% in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0% to 58.3%, surpassing GPT-4 Omni's 57.5% Win Rate (community report).

------------

`[2406.15193] Reward Steering with Evolutionary Heuristics for Decoding-time Alignment <https://arxiv.org/abs/2406.15193>`__ 基于进化启发式的解码时间对齐奖励导向

::

    replaced with revised version Tue, 25 Jun 2024 16:55:03 GMT
    Submission history From: Soujanya Poria [view email]
    [v1] Fri, 21 Jun 2024 14:35:16 UTC (2,673 KB)
    [v2] Mon, 24 Jun 2024 17:36:11 UTC (2,674 KB)
    [v3] Tue, 25 Jun 2024 16:55:03 UTC (1,832 KB)
    Chia-Yu Hung, Navonil Majumder, Ambuj Mehrish, Soujanya Poria

The widespread applicability and increasing omnipresence of LLMs have instigated a need to align LLM responses to user and stakeholder preferences. Many preference optimization approaches have been proposed that fine-tune LLM parameters to achieve good alignment. However, such parameter tuning is known to interfere with model performance on many tasks. Moreover, keeping up with shifting user preferences is tricky in such a situation. Decoding-time alignment with reward model guidance solves these issues at the cost of increased inference time. However, most of such methods fail to strike the right balance between exploration and exploitation of reward -- often due to the conflated formulation of these two aspects - to give well-aligned responses. To remedy this we decouple these two aspects and implement them in an evolutionary fashion: exploration is enforced by decoding from mutated instructions and exploitation is represented as the periodic replacement of poorly-rewarded generations with well-rewarded ones. Empirical evidences indicate that this strategy outperforms many preference optimization and decode-time alignment approaches on two widely accepted alignment benchmarks AlpacaEval 2 and MT-Bench. Our implementation will be available at: this https URL.

------------

-----------------------
In-Context Learning (2)
-----------------------

`[2406.17534] Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification <https://arxiv.org/abs/2406.17534>`__ 基于检索式上下文学习的小样本层次文本分类

::

    Tue, 25 Jun 2024 13:19:41 GMT
    Huiyao Chen, Yu Zhao, Zulong Chen, Mengjia Wang, Liangyue Li, Meishan Zhang, Min Zhang

Hierarchical text classification (HTC) is an important task with broad applications, while few-shot HTC has gained increasing interest recently. While in-context learning (ICL) with large language models (LLMs) has achieved significant success in few-shot learning, it is not as effective for HTC because of the expansive hierarchical label sets and extremely-ambiguous labels. In this work, we introduce the first ICL-based framework with LLM for few-shot HTC. We exploit a retrieval database to identify relevant demonstrations, and an iterative policy to manage multi-layer hierarchical labels. Particularly, we equip the retrieval database with HTC label-aware representations for the input texts, which is achieved by continual training on a pretrained language model with masked language modeling (MLM), layer-wise classification (CLS, specifically for HTC), and a novel divergent contrastive learning (DCL, mainly for adjacent semantically-similar labels) objective.
Experimental results on three benchmark datasets demonstrate superior performance of our method, and we can achieve state-of-the-art results in few-shot HTC.

------------

`[2406.17764] BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning <https://arxiv.org/abs/2406.17764>`__ BMIKE-53:基于语境学习的跨语言知识编辑研究

::

    Tue, 25 Jun 2024 17:48:56 GMT
    Ercong Nie, Bo Shao, Zifeng Ding, Mingyang Wang, Helmut Schmid, Hinrich Sch\"utze

Large language models (LLMs) possess extensive parametric knowledge, but this knowledge is difficult to update with new information because retraining is very expensive and infeasible for closed-source models. Knowledge editing (KE) has emerged as a viable solution for updating the knowledge of LLMs without compromising their overall performance. On-the-fly KE methods, inspired by in-context learning (ICL), have shown great promise and allow LLMs to be treated as black boxes. In the past, KE was primarily employed in English contexts, whereas the potential for cross-lingual KE in current English-centric LLMs has not been fully explored. To foster more research in this direction, we introduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverse languages across three KE task types. We also propose a gradient-free KE method called Multilingual In-context Knowledge Editing (MIKE) and evaluate it on BMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in terms of reliability, generality, locality, and portability, offering valuable insights and a framework for future research in cross-lingual KE. Our code and data are publicly accessible via the anonymous repository at https://anonymous.4open.science/r/MIKE.

------------

-------------
Reasoning (9)
-------------

`[2406.16891] Survey on Reasoning Capabilities and Accessibility of Large Language Models Using Biology-related Questions <https://arxiv.org/abs/2406.16891>`__ 基于生物学相关问题的大型语言模型推理能力与可及性综述

::

    Sat, 11 May 2024 20:25:40 GMT
    Michael Ackerman

This research paper discusses the advances made in the past decade in biomedicine and Large Language Models. To understand how the advances have been made hand-in-hand with one another, the paper also discusses the integration of Natural Language Processing techniques and tools into biomedicine. Finally, the goal of this paper is to expand on a survey conducted last year (2023) by introducing a new list of questions and prompts for the top two language models. Through this survey, this paper seeks to quantify the improvement made in the reasoning abilities in LLMs and to what extent those improvements are felt by the average user. Additionally, this paper seeks to extend research on retrieval of biological literature by prompting the LLM to answer open-ended questions in great depth.

------------

`[2406.17169] Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models <https://arxiv.org/abs/2406.17169>`__ Multi-LogiEval:大型语言模型多步逻辑推理能力评估研究

::

    Mon, 24 Jun 2024 23:02:56 GMT
    Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral

As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning. Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multi-step reasoning with a limited set of inference rules. Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning. To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multi-step logical reasoning with various inference rules and depths. Multi-LogiEval covers three logic types--propositional, first-order, and non-monotonic--consisting of more than 30 inference rules and more than 60 of their combinations with various depths. Leveraging this dataset, we conduct evaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca, and Mistral, employing a zero-shot chain-of-thought. Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).
We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings. We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs. Data is available at https://github.com/Mihir3009/Multi-LogiEval.

------------

`[2406.17271] DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph <https://arxiv.org/abs/2406.17271>`__ DARG:基于自适应推理图的大型语言模型动态评估

::

    Tue, 25 Jun 2024 04:27:53 GMT
    Zhehao Zhang, Jiaao Chen, Diyi Yang

The current paradigm of evaluating Large Language Models (LLMs) through static benchmarks comes with significant limitations, such as vulnerability to data contamination and a lack of adaptability to the evolving capabilities of LLMs. Therefore, evaluation methods that can adapt and generate evaluation data with controlled complexity are urgently needed. In this work, we introduce Dynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to dynamically extend current benchmarks with controlled complexity and diversity.
Specifically, we first extract the reasoning graphs of data points in current benchmarks and then perturb the reasoning graphs to generate novel testing data. Such newly generated test samples can have different levels of complexity while maintaining linguistic diversity similar to the original benchmarks. We further use a code-augmented LLM to ensure the label correctness of newly generated data. We apply our DARG framework to diverse reasoning tasks in four domains with 15 state-of-the-art LLMs. Experimental results show that almost all LLMs experience a performance decrease with increased complexity and certain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit more biases when being evaluated via the data generated by DARG with higher complexity levels. These observations provide useful insights into how to dynamically and adaptively evaluate LLMs. The code is available at https://github.com/SALT-NLP/DARG.

------------

`[2406.17294] Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models <https://arxiv.org/abs/2406.17294>`__ math - lava:多模态大型语言模型的自助数学推理

::

    Tue, 25 Jun 2024 05:43:21 GMT
    Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, Roy Ka-Wei Lee

Large language models (LLMs) have demonstrated impressive reasoning capabilities, particularly in textual mathematical problem-solving. However, existing open-source image instruction fine-tuning datasets, containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap, we address the lack of high-quality, diverse multimodal mathematical datasets by collecting 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs, creating the MathV360K dataset, which enhances both the breadth and depth of multimodal mathematical questions. We introduce Math-LLaVA, a LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced generalizability, showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs' mathematical reasoning abilities. The code and data are available at: \url{https://github.com/HZQ950419/Math-LLaVA}.

------------

`[2406.17663] LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic <https://arxiv.org/abs/2406.17663>`__ LLM-ARC:用自动推理评价器增强llm

::

    Tue, 25 Jun 2024 15:52:15 GMT
    Aditya Kalyanpur, Kailash Saravanakumar, Victor Barres, Jennifer Chu-Carroll, David Melville, David Ferrucci

We introduce LLM-ARC, a neuro-symbolic framework designed to enhance the logical reasoning capabilities of Large Language Models (LLMs), by combining them with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Critic method where the LLM Actor generates declarative logic programs along with tests for semantic correctness, while the Automated Reasoning Critic evaluates the code, runs the tests and provides feedback on test failures for iterative refinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves a new state-of-the-art accuracy of 88.32% on the FOLIO benchmark which tests complex logical reasoning capabilities. Our experiments demonstrate significant improvements over LLM-only baselines, highlighting the importance of logic test generation and iterative self-refinement. We achieve our best result using a fully automated self-supervised training loop where the Actor is trained on end-to-end dialog traces with Critic feedback. We discuss potential enhancements and provide a detailed error analysis, showcasing the robustness and efficacy of LLM-ARC for complex natural language reasoning tasks.

------------

`[2403.15297] Sphere Neural-Networks for Rational Reasoning <https://arxiv.org/abs/2403.15297>`__ 用于理性推理的球面神经网络

::

    replaced with revised version Mon, 24 Jun 2024 19:45:42 GMT
    Submission history From: Tiansi Dong [view email]
    [v1] Fri, 22 Mar 2024 15:44:59 UTC (29,986 KB)
    [v2] Wed, 17 Apr 2024 20:02:20 UTC (29,981 KB)
    [v3] Mon, 24 Jun 2024 19:45:42 UTC (15,734 KB)
    Tiansi Dong, Mateja Jamnik, Pietro Li\`o

The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by their planetary popularity, their capability of human-like communication, and also by their steadily improved reasoning performance. However, it remains unclear whether LLMs reason. It is an open problem how traditional neural networks can be qualitatively extended to go beyond the statistic paradigm and achieve high-level cognition. Here, we present a novel qualitative extension by generalising computational building blocks from vectors to spheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning through model construction and inspection, and develop SphNN for syllogistic reasoning, a microcosm of human rationality. SphNN is a hierarchical neuro-symbolic Kolmogorov-Arnold geometric GNN, and uses a neuro-symbolic transition map of neighbourhood spatial relations to transform the current sphere configuration towards the target. SphNN is the first neural model that can determine the validity of long-chained syllogistic reasoning in one epoch without training data, with the worst computational complexity of O(N). SphNN can evolve into various types of reasoning, such as spatio-temporal reasoning, logical reasoning with negation and disjunction, event reasoning, neuro-symbolic unification, and humour understanding (the highest level of cognition). All these suggest a new kind of Herbert A. Simon's scissors with two neural blades. SphNNs will tremendously enhance interdisciplinary collaborations to develop the two neural blades and realise deterministic neural reasoning and human-bounded rationality and elevate LLMs to reliable psychological AI. This work suggests that the non-zero radii of spheres are the missing components that prevent traditional deep-learning systems from reaching the realm of rational reasoning and cause LLMs to be trapped in the swamp of hallucination.

------------

`[2310.01290] Knowledge Crosswords: Geometric Knowledge Reasoning with Large Language Models <https://arxiv.org/abs/2310.01290>`__ 知识填字游戏:基于大型语言模型的几何知识推理

::

    replaced with revised version Tue, 25 Jun 2024 06:25:41 GMT
    Submission history From: Wenxuan Ding [view email]
    [v1] Mon, 2 Oct 2023 15:43:53 UTC (295 KB)
    [v2] Tue, 25 Jun 2024 06:25:41 UTC (833 KB)
    Wenxuan Ding, Shangbin Feng, Yuhan Liu, Zhaoxuan Tan, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov

We propose Knowledge Crosswords, a geometric knowledge reasoning benchmark consisting of incomplete knowledge networks bounded by structured factual constraints, where LLMs are tasked with inferring the missing facts to meet all constraints. The novel setting of geometric knowledge reasoning necessitates new LM abilities beyond existing atomic/linear multi-hop QA, such as backtracking, verifying facts and constraints, reasoning with uncertainty, and more. Knowledge Crosswords contains 2,101 individual problems, covering diverse knowledge domains, and is further divided into three difficulty levels. We conduct extensive experiments to evaluate existing LLMs and approaches on Knowledge Crosswords. Results demonstrate that baseline approaches struggle with larger knowledge networks and semantically-equivalent entity distractors. In light of their limitations, we propose two new approaches, Staged Prompting and Verify-All, to augment LLMs' abilities for error-aware backtracking and constraint verification. Our Verify-All significantly outperforms prior methods and is more robust towards problems in the hard subset. Further analysis shows that geometric knowledge reasoning poses new challenges to LLMs' knowledge abilities, particularly in robustness towards varying option orders, complex structural constraints in knowledge networks, "none of the above" scenarios, and more.

------------

`[2402.10963] GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements <https://arxiv.org/abs/2402.10963>`__ GLoRe:何时，何地，以及如何通过全局和局部细化来改进LLM推理

::

    replaced with revised version Tue, 25 Jun 2024 03:14:10 GMT
    Submission history From: Alex Havrilla [view email]
    [v1] Tue, 13 Feb 2024 20:16:29 UTC (3,789 KB)
    [v2] Tue, 25 Jun 2024 03:14:10 UTC (3,789 KB)
    Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Roberta Raileanu

State-of-the-art language models can exhibit impressive reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify \textit{when and where to refine} without access to external feedback. Outcome-based Reward Models (\textbf{ORMs}), trained to predict correctness of the final answer indicating when to refine, offer one convenient solution for deciding when to refine. Process Based Reward Models (\textbf{PRMs}), trained to predict correctness of intermediate steps, can then be used to indicate where to refine. But they are expensive to train, requiring extensive human annotations. In this paper, we propose Stepwise ORMs (\textbf{SORMs}) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\star}$. More specifically, SORMs are trained to predict the correctness of the final answer when sampling the current policy many times (rather than only once as in the case of ORMs). Our experiments show that SORMs can more accurately detect incorrect reasoning steps compared to ORMs, thus improving downstream accuracy when doing refinements. We then train \textit{global} refinement models, which take only the question and a draft solution as input and predict a corrected solution, and \textit{local} refinement models which also take as input a critique indicating the location of the first reasoning error. We generate training data for both models synthetically by reusing data used to train the SORM. We find combining global and local refinements, using the ORM as a reranker, significantly outperforms either one individually, as well as a best of three sample baseline. With this strategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned with RL) on GSM8K from 53\% to 65\% when greedily sampled.

------------

`[2406.14425] SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource Languages <https://arxiv.org/abs/2406.14425>`__ SynDARin:面向低资源语言自动推理的数据集合成

::

    replaced with revised version Tue, 25 Jun 2024 13:48:41 GMT
    Submission history From: Erik Arakelyan [view email]
    [v1] Thu, 20 Jun 2024 15:49:28 UTC (757 KB)
    [v2] Tue, 25 Jun 2024 13:48:41 UTC (818 KB)
    Gayane Ghazaryan, Erik Arakelyan, Pasquale Minervini, Isabelle Augenstein

Question Answering (QA) datasets have been instrumental in developing and evaluating Large Language Model (LLM) capabilities. However, such datasets are scarce for languages other than English due to the cost and difficulties of collection and manual annotation. This means that producing novel models and measuring the performance of multilingual LLMs in low-resource languages is challenging. To mitigate this, we propose $\textbf{S}$yn$\textbf{DAR}$in, a method for generating and validating QA datasets for low-resource languages. We utilize parallel content mining to obtain $\textit{human-curated}$ paragraphs between English and the target language. We use the English data as context to $\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which are automatically translated and further validated for quality. Combining these with their designated non-English $\textit{human-curated}$ paragraphs form the final QA dataset. The method allows to maintain the content quality, reduces the likelihood of factual errors, and circumvents the need for costly annotation. To test the method, we created a QA dataset with $1.2$K samples for the Armenian language. The human evaluation shows that $98\%$ of the generated English data maintains quality and diversity in the question types and topics, while the translation validation pipeline can filter out $\sim70\%$ of data with poor quality. We use the dataset to benchmark state-of-the-art LLMs, showing their inability to achieve human accuracy with some model performances closer to random chance. This shows that the generated dataset is non-trivial and can be used to evaluate reasoning capabilities in low-resource language.

------------

-----------
ToolUse (3)
-----------

`[2406.17465] Enhancing Tool Retrieval with Iterative Feedback from Large Language Models <https://arxiv.org/abs/2406.17465>`__ 利用大型语言模型的迭代反馈增强工具检索

::

    Tue, 25 Jun 2024 11:12:01 GMT
    Qiancheng Xu, Yongqi Li, Heming Xia, Wenjie Li

Tool learning aims to enhance and expand large language models' (LLMs) capabilities with external tools, which has gained significant attention recently. Current methods have shown that LLMs can effectively handle a certain amount of tools through in-context learning or fine-tuning. However, in real-world scenarios, the number of tools is typically extensive and irregularly updated, emphasizing the necessity for a dedicated tool retrieval component. Tool retrieval is nontrivial due to the following challenges: 1) complex user instructions and tool descriptions; 2) misalignment between tool retrieval and tool usage models. To address the above issues, we propose to enhance tool retrieval with iterative feedback from the large language model.
Specifically, we prompt the tool usage model, i.e., the LLM, to provide feedback for the tool retriever model in multi-round, which could progressively improve the tool retriever's understanding of instructions and tools and reduce the gap between the two standalone components. We build a unified and comprehensive benchmark to evaluate tool retrieval models. The extensive experiments indicate that our proposed approach achieves advanced performance in both in-domain evaluation and out-of-domain evaluation.

------------

`[2406.16903] Towards a copilot in BIM authoring tool using a large language model-based agent for intelligent human-machine interaction <https://arxiv.org/abs/2406.16903>`__ 基于大型语言模型agent的BIM创作工具中copilot的实现

::

    Sun, 2 Jun 2024 17:47:57 GMT
    Changyu Du, Stavros Nousias, Andr\'e Borrmann

Facing increasingly complex BIM authoring software and the accompanying expensive learning costs, designers often seek to interact with the software in a more intelligent and lightweight manner. They aim to automate modeling workflows, avoiding obstacles and difficulties caused by software usage, thereby focusing on the design process itself. To address this issue, we proposed an LLM-based autonomous agent framework that can function as a copilot in the BIM authoring tool, answering software usage questions, understanding the user's design intentions from natural language, and autonomously executing modeling tasks by invoking the appropriate tools. In a case study based on the BIM authoring software Vectorworks, we implemented a software prototype to integrate the proposed framework seamlessly into the BIM authoring scenario. We evaluated the planning and reasoning capabilities of different LLMs within this framework when faced with complex instructions. Our work demonstrates the significant potential of LLM-based agents in design automation and intelligent interaction.

------------

`[2406.17215] Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of Daline <https://arxiv.org/abs/2406.17215>`__ 用以前未见过的工具使大型语言模型执行电力系统模拟:Daline的一个案例

::

    Tue, 25 Jun 2024 02:05:26 GMT
    Mengshuo Jia, Zeyu Cui, Gabriela Hug

The integration of experiment technologies with large language models (LLMs) is transforming scientific research, offering AI capabilities beyond specialized problem-solving to becoming research assistants for human scientists. In power systems, simulations are essential for research. However, LLMs face significant challenges in power system simulations due to limited pre-existing knowledge and the complexity of power grids. To address this issue, this work proposes a modular framework that integrates expertise from both the power system and LLM domains. This framework enhances LLMs' ability to perform power system simulations on previously unseen tools. Validated using 34 simulation tasks in Daline, a (optimal) power flow simulation and linearization toolbox not yet exposed to LLMs, the proposed framework improved GPT-4o's simulation coding accuracy from 0% to 96.07%, also outperforming the ChatGPT-4o web interface's 33.8% accuracy (with the entire knowledge base uploaded). These results highlight the potential of LLMs as research assistants in power systems.

------------

-----------------------
Retrieval-Augmented (9)
-----------------------

`[2406.17304] Leveraging LLMs for Dialogue Quality Measurement <https://arxiv.org/abs/2406.17304>`__ 利用llm进行对话质量度量

::

    Tue, 25 Jun 2024 06:19:47 GMT
    Jinghan Jia, Abi Komma, Timothy Leffel, Xujun Peng, Ajay Nagesh, Tamer Soliman, Aram Galstyan, Anoop Kumar

In task-oriented conversational AI evaluation, unsupervised methods poorly correlate with human judgments, and supervised approaches lack generalization.
Recent advances in large language models (LLMs) show robust zeroshot and few-shot capabilities across NLP tasks. This paper explores using LLMs for automated dialogue quality evaluation, experimenting with various configurations on public and proprietary datasets. Manipulating factors such as model size, in-context examples, and selection techniques, we examine "chain-of-thought" (CoT) reasoning and label extraction procedures. Our results show that (1) larger models yield more accurate dialogue labels; (2) algorithmic selection of in-context examples outperforms random selection; (3) CoT reasoning where an LLM is asked to provide justifications before outputting final labels improves performance; and (4) fine-tuned LLMs outperform out-of-the-box ones. Our results indicate that LLMs that are suitably fine-tuned and have sufficient reasoning capabilities can be leveraged for automated dialogue evaluation.

------------

`[2406.17305] Retrieval Augmented Instruction Tuning for Open NER with Large Language Models <https://arxiv.org/abs/2406.17305>`__ 

::

    Tue, 25 Jun 2024 06:24:50 GMT
    Tingyu Xie, Jian Zhang, Yan Zhang, Yuanyuan Liang, Qi Li, Hongwei Wang

The strong capability of large language models (LLMs) has been applied to information extraction (IE) through either retrieval augmented prompting or instruction tuning (IT). However, the best way to incorporate information with LLMs for IE remains an open question. In this paper, we explore Retrieval Augmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named entity recognition (NER). Specifically, for each training sample, we retrieve semantically similar examples from the training dataset as the context and prepend them to the input of the original instruction. To evaluate our RA-IT approach more thoroughly, we construct a Chinese IT dataset for open NER and evaluate RA-IT in both English and Chinese scenarios. Experimental results verify the effectiveness of RA-IT across various data sizes and in both English and Chinese scenarios. We also conduct thorough studies to explore the impacts of various retrieval strategies in the proposed RA-IT framework. Code and data are available at: https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER

------------

`[2406.17465] Enhancing Tool Retrieval with Iterative Feedback from Large Language Models <https://arxiv.org/abs/2406.17465>`__ 利用大型语言模型的迭代反馈增强工具检索

::

    Tue, 25 Jun 2024 11:12:01 GMT
    Qiancheng Xu, Yongqi Li, Heming Xia, Wenjie Li

Tool learning aims to enhance and expand large language models' (LLMs) capabilities with external tools, which has gained significant attention recently. Current methods have shown that LLMs can effectively handle a certain amount of tools through in-context learning or fine-tuning. However, in real-world scenarios, the number of tools is typically extensive and irregularly updated, emphasizing the necessity for a dedicated tool retrieval component. Tool retrieval is nontrivial due to the following challenges: 1) complex user instructions and tool descriptions; 2) misalignment between tool retrieval and tool usage models. To address the above issues, we propose to enhance tool retrieval with iterative feedback from the large language model.
Specifically, we prompt the tool usage model, i.e., the LLM, to provide feedback for the tool retriever model in multi-round, which could progressively improve the tool retriever's understanding of instructions and tools and reduce the gap between the two standalone components. We build a unified and comprehensive benchmark to evaluate tool retrieval models. The extensive experiments indicate that our proposed approach achieves advanced performance in both in-domain evaluation and out-of-domain evaluation.

------------

`[2406.17519] Entropy-Based Decoding for Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2406.17519>`__ 基于熵的检索增强大型语言模型解码

::

    Tue, 25 Jun 2024 12:59:38 GMT
    Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, Irwin King

Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective for improving the factual accuracy of generated responses.
Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the generated responses are negatively influenced by noise from both external and internal knowledge sources. In this paper, we introduce a novel, training-free decoding method guided by entropy considerations to mitigate this issue. Our approach utilizes entropy-based document-parallel ensemble decoding to prioritize low-entropy distributions from retrieved documents, thereby enhancing the extraction of relevant information of context.
Additionally, it incorporates a contrastive decoding mechanism that contrasts the obtained low-entropy ensemble distribution with the high-entropy distribution derived from the model's internal knowledge across layers, which ensures a greater emphasis on reliable external information. Extensive experiments on open-domain question answering datasets demonstrate the superiority of our method.

------------

`[2406.17534] Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification <https://arxiv.org/abs/2406.17534>`__ 基于检索式上下文学习的小样本层次文本分类

::

    Tue, 25 Jun 2024 13:19:41 GMT
    Huiyao Chen, Yu Zhao, Zulong Chen, Mengjia Wang, Liangyue Li, Meishan Zhang, Min Zhang

Hierarchical text classification (HTC) is an important task with broad applications, while few-shot HTC has gained increasing interest recently. While in-context learning (ICL) with large language models (LLMs) has achieved significant success in few-shot learning, it is not as effective for HTC because of the expansive hierarchical label sets and extremely-ambiguous labels. In this work, we introduce the first ICL-based framework with LLM for few-shot HTC. We exploit a retrieval database to identify relevant demonstrations, and an iterative policy to manage multi-layer hierarchical labels. Particularly, we equip the retrieval database with HTC label-aware representations for the input texts, which is achieved by continual training on a pretrained language model with masked language modeling (MLM), layer-wise classification (CLS, specifically for HTC), and a novel divergent contrastive learning (DCL, mainly for adjacent semantically-similar labels) objective.
Experimental results on three benchmark datasets demonstrate superior performance of our method, and we can achieve state-of-the-art results in few-shot HTC.

------------

`[2406.17553] Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft <https://arxiv.org/abs/2406.17553>`__ 

::

    Tue, 25 Jun 2024 13:43:24 GMT
    Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen

In the Minecraft Collaborative Building Task, two players collaborate: an Architect (A) provides instructions to a Builder (B) to assemble a specified structure using 3D blocks. In this work, we investigate the use of large language models (LLMs) to predict the sequence of actions taken by the Builder.
Leveraging LLMs' in-context learning abilities, we use few-shot prompting techniques, that significantly improve performance over baseline methods.
Additionally, we present a detailed analysis of the gaps in performance for future work

------------

`[2406.16989] Retrieval-Augmented Mixture of LoRA Experts for Uploadable Machine Learning <https://arxiv.org/abs/2406.16989>`__ 用于可上传机器学习的LoRA专家检索增强混合

::

    Mon, 24 Jun 2024 05:24:41 GMT
    Ziyu Zhao, Leilei Gan, Guoyin Wang, Yuwei Hu, Tao Shen, Hongxia Yang, Kun Kuang, Fei Wu

Low-Rank Adaptation (LoRA) offers an efficient way to fine-tune large language models (LLMs). Its modular and plug-and-play nature allows the integration of various domain-specific LoRAs, enhancing LLM capabilities.
Open-source platforms like Huggingface and Modelscope have introduced a new computational paradigm, Uploadable Machine Learning (UML). In UML, contributors use decentralized data to train specialized adapters, which are then uploaded to a central platform to improve LLMs. This platform uses these domain-specific adapters to handle mixed-task requests requiring personalized service. Previous research on LoRA composition either focuses on specific tasks or fixes the LoRA selection during training. However, in UML, the pool of LoRAs is dynamically updated with new uploads, requiring a generalizable selection mechanism for unseen LoRAs. Additionally, the mixed-task nature of downstream requests necessitates personalized services. To address these challenges, we propose Retrieval-Augmented Mixture of LoRA Experts (RAMoLE), a framework that adaptively retrieves and composes multiple LoRAs based on input prompts. RAMoLE has three main components: LoraRetriever for identifying and retrieving relevant LoRAs, an on-the-fly MoLE mechanism for coordinating the retrieved LoRAs, and efficient batch inference for handling heterogeneous requests.
Experimental results show that RAMoLE consistently outperforms baselines, highlighting its effectiveness and scalability.

------------

`[2406.17651] Leveraging Large Language Models for Software Model Completion: Results from Industrial and Public Datasets <https://arxiv.org/abs/2406.17651>`__ 利用大型语言模型完成软件模型:来自工业和公共数据集的结果

::

    Tue, 25 Jun 2024 15:43:20 GMT
    Christof Tinnes, Alisa Welter, Sven Apel

Modeling structure and behavior of software systems plays a crucial role in the industrial practice of software engineering. As with other software engineering artifacts, software models are subject to evolution. Supporting modelers in evolving software models with recommendations for model completions is still an open problem, though. In this paper, we explore the potential of large language models for this task. In particular, we propose an approach, retrieval-augmented generation, leveraging large language models, model histories, and retrieval-augmented generation for model completion. Through experiments on three datasets, including an industrial application, one public open-source community dataset, and one controlled collection of simulated model repositories, we evaluate the potential of large language models for model completion with retrieval-augmented generation. We found that large language models are indeed a promising technology for supporting software model evolution (62.30% semantically correct completions on real-world industrial data and up to 86.19% type-correct completions). The general inference capabilities of large language models are particularly useful when dealing with concepts for which there are few, noisy, or no examples at all.

------------

`[2405.02659] R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2405.02659>`__ R4:检索增强大型语言模型的强化检索器-重序响应器

::

    replaced with revised version Tue, 25 Jun 2024 09:24:01 GMT
    Submission history From: Taolin Zhang [view email]
    [v1] Sat, 4 May 2024 12:59:10 UTC (823 KB)
    [v2] Tue, 25 Jun 2024 09:24:01 UTC (1 KB) (withdrawn)
    Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, Longtao Huang, Hui Xue, Xiaofeng He, Jun Huang

Retrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods typically append relevant documents to the prompt of LLMs to perform text generation tasks without considering the interaction of fine-grained structural semantics between the retrieved documents and the LLMs. This issue is particularly important for accurate response generation as LLMs tend to "lose in the middle" when dealing with input prompts augmented with lengthy documents. In this work, we propose a new pipeline named "Reinforced Retriever-Reorder-Responder" (R$^4$) to learn document orderings for retrieval-augmented LLMs, thereby further enhancing their generation abilities while the large numbers of parameters of LLMs remain frozen. The reordering learning process is divided into two steps according to the quality of the generated responses: document order adjustment and document representation enhancement. Specifically, document order adjustment aims to organize retrieved document orderings into beginning, middle, and end positions based on graph attention learning, which maximizes the reinforced reward of response quality. Document representation enhancement further refines the representations of retrieved documents for responses of poor quality via document-level gradient adversarial learning. Extensive experiments demonstrate that our proposed pipeline achieves better factual question-answering performance on knowledge-intensive tasks compared to strong baselines across various public datasets. The source codes and trained models will be released upon paper acceptance.

------------

---------
Agent (3)
---------

`[2406.17232] Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks <https://arxiv.org/abs/2406.17232>`__ 超越人口统计学:使用人类信念网络对齐基于llm的角色扮演代理

::

    Tue, 25 Jun 2024 02:37:29 GMT
    Yun-Shiuan Chuang, Zach Studdiford, Krirk Nirunwiroj, Agam Goyal, Vincent V. Frigo, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers

Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 18 topics loading on two non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.

------------

`[2406.16903] Towards a copilot in BIM authoring tool using a large language model-based agent for intelligent human-machine interaction <https://arxiv.org/abs/2406.16903>`__ 基于大型语言模型agent的BIM创作工具中copilot的实现

::

    Sun, 2 Jun 2024 17:47:57 GMT
    Changyu Du, Stavros Nousias, Andr\'e Borrmann

Facing increasingly complex BIM authoring software and the accompanying expensive learning costs, designers often seek to interact with the software in a more intelligent and lightweight manner. They aim to automate modeling workflows, avoiding obstacles and difficulties caused by software usage, thereby focusing on the design process itself. To address this issue, we proposed an LLM-based autonomous agent framework that can function as a copilot in the BIM authoring tool, answering software usage questions, understanding the user's design intentions from natural language, and autonomously executing modeling tasks by invoking the appropriate tools. In a case study based on the BIM authoring software Vectorworks, we implemented a software prototype to integrate the proposed framework seamlessly into the BIM authoring scenario. We evaluated the planning and reasoning capabilities of different LLMs within this framework when faced with complex instructions. Our work demonstrates the significant potential of LLM-based agents in design automation and intelligent interaction.

------------

`[2406.14868] Direct Multi-Turn Preference Optimization for Language Agents <https://arxiv.org/abs/2406.14868>`__ 语言智能体的直接多轮偏好优化

::

    replaced with revised version Tue, 25 Jun 2024 08:44:24 GMT
    Submission history From: Wentao Shi [view email]
    [v1] Fri, 21 Jun 2024 05:13:20 UTC (3,961 KB)
    [v2] Tue, 25 Jun 2024 08:44:24 UTC (3,961 KB)
    Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, Fuli Feng

Adapting Large Language Models (LLMs) for agent tasks is critical in developing language agents. Direct Preference Optimization (DPO) is a promising technique for this adaptation with the alleviation of compounding errors, offering a means to directly optimize Reinforcement Learning (RL) objectives. However, applying DPO to multi-turn tasks presents challenges due to the inability to cancel the partition function. Overcoming this obstacle involves making the partition function independent of the current state and addressing length disparities between preferred and dis-preferred trajectories. In this light, we replace the policy constraint with the state-action occupancy measure constraint in the RL objective and add length normalization to the Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn agent tasks with theoretical explanations. Extensive experiments on three multi-turn agent task datasets confirm the effectiveness and superiority of the DMPO loss.

------------

----------
Other (90)
----------

`[2406.17224] Large Language Models are Interpretable Learners <https://arxiv.org/abs/2406.17224>`__ 大型语言模型是可解释的学习者

::

    Tue, 25 Jun 2024 02:18:15 GMT
    Ruochen Wang and Si Si and Felix Yu and Dorothea Wiesmann and Cho-Jui Hsieh and Inderjit Dhillon

The trade-off between expressiveness and interpretability remains a core challenge when building human-centric predictive models for classification and decision-making. While symbolic rules offer interpretability, they often lack expressiveness, whereas neural networks excel in performance but are known for being black boxes. In this paper, we show a combination of Large Language Models (LLMs) and symbolic programs can bridge this gap. In the proposed LLM-based Symbolic Programs (LSPs), the pretrained LLM with natural language prompts provides a massive set of interpretable modules that can transform raw input into natural language concepts. Symbolic programs then integrate these modules into an interpretable decision rule. To train LSPs, we develop a divide-and-conquer approach to incrementally build the program from scratch, where the learning process of each step is guided by LLMs. To evaluate the effectiveness of LSPs in extracting interpretable and accurate knowledge from data, we introduce IL-Bench, a collection of diverse tasks, including both synthetic and real-world scenarios across different modalities. Empirical results demonstrate LSP's superior performance compared to traditional neurosymbolic programs and vanilla automatic prompt tuning methods. Moreover, as the knowledge learned by LSP is a combination of natural language descriptions and symbolic rules, it is easily transferable to humans (interpretable), and other LLMs, and generalizes well to out-of-distribution samples.

------------

`[2406.17532] Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study <https://arxiv.org/abs/2406.17532>`__ 大型语言模型能理解DL-Lite本体吗?实证研究

::

    Tue, 25 Jun 2024 13:16:34 GMT
    Keyu Wang, Guilin Qi, Jiaqi Li, Songlin Zhai

Large language models (LLMs) have shown significant achievements in solving a wide range of tasks. Recently, LLMs' capability to store, retrieve and infer with symbolic knowledge has drawn a great deal of attention, showing their potential to understand structured information. However, it is not yet known whether LLMs can understand Description Logic (DL) ontologies. In this work, we empirically analyze the LLMs' capability of understanding DL-Lite ontologies covering 6 representative tasks from syntactic and semantic aspects. With extensive experiments, we demonstrate both the effectiveness and limitations of LLMs in understanding DL-Lite ontologies. We find that LLMs can understand formal syntax and model-theoretic semantics of concepts and roles. However, LLMs struggle with understanding TBox NI transitivity and handling ontologies with large ABoxes. We hope that our experiments and analyses provide more insights into LLMs and inspire to build more faithful knowledge engineering solutions.

------------

`[2406.16899] Prompt-based vs. Fine-tuned LLMs Toward Causal Graph Verification <https://arxiv.org/abs/2406.16899>`__ 面向因果图验证的提示型和微调型llm

::

    Wed, 29 May 2024 09:06:18 GMT
    Yuni Susanti, Nina Holsmoelle

This work aims toward an application of natural language processing (NLP) technology for automatic verification of causal graphs using text sources. A causal graph is often derived from unsupervised causal discovery methods and requires manual evaluation from human experts. NLP technologies, i.e., Large Language Models (LLMs) such as BERT and ChatGPT, can potentially be used to verify the resulted causal graph by predicting if causal relation can be observed between node pairs based on the textual context. In this work, we compare the performance of two types of NLP models: (1) Pre-trained language models fine-tuned for causal relation classification task and, (2) prompt-based LLMs. Contrasted to previous studies where prompt-based LLMs work relatively well over a set of diverse tasks, preliminary experiments on biomedical and open-domain datasets suggest that the fine-tuned models far outperform the prompt-based LLMs, up to 20.5 points improvement of F1 score. We shared the code and the pre-processed datasets in our repository.

------------

`[2406.17055] Large Language Models Assume People are More Rational than We Really are <https://arxiv.org/abs/2406.17055>`__ 大型语言模型假设人们比我们实际更理性

::

    Mon, 24 Jun 2024 18:15:27 GMT
    Ryan Liu, Jiayi Geng, Joshua C. Peterson, Ilia Sucholutsky, Thomas L. Griffiths

In order for AI systems to communicate effectively with people, they must understand how we make decisions. However, people's decisions are not always rational, so the implicit internal models of human decision-making in Large Language Models (LLMs) must account for this. Previous empirical evidence seems to suggest that these implicit models are accurate -- LLMs offer believable proxies of human behavior, acting how we expect humans would in everyday interactions. However, by comparing LLM behavior and predictions to a large dataset of human decisions, we find that this is actually not the case: when both simulating and predicting people's choices, a suite of cutting-edge LLMs (GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more rational than we really are. Specifically, these models deviate from human behavior and align more closely with a classic model of rational choice -- expected value theory. Interestingly, people also tend to assume that other people are rational when interpreting their behavior. As a consequence, when we compare the inferences that LLMs and people draw from the decisions of others using another psychological dataset, we find that these inferences are highly correlated. Thus, the implicit decision-making models of LLMs appear to be aligned with the human expectation that other people will act rationally, rather than with how people actually act.

------------

`[2406.17095] Attention Instruction: Amplifying Attention in the Middle via Prompting <https://arxiv.org/abs/2406.17095>`__ 

::

    Mon, 24 Jun 2024 19:35:11 GMT
    Meiru Zhang, Zaiqiao Meng, Nigel Collier

The context window of large language models has been extended to 128k tokens or more. However, language models still suffer from position bias and have difficulty in accessing and using the middle part of the context due to the lack of attention. We examine the relative position awareness of LLMs and the feasibility of mitigating disproportional attention through prompting. We augment the original task instruction with $\texttt{attention instructions}$ that direct language models to allocate more attention towards a selected segment of the context. We conduct a comprehensive investigation on multi-document question answering task with both position-based and index-based instructions. We find that language models do not have relative position awareness of the context. Nevertheless, they demonstrate the capacity to adapt attention to a specific segment using matching indexes. Our analysis contributes to a deeper understanding of position bias in LLMs and provides a pathway to mitigate this bias by instruction, thus benefiting LLMs in locating and utilizing relevant information from retrieved documents in RAG applications.

------------

`[2406.17104] Automated Adversarial Discovery for Safety Classifiers <https://arxiv.org/abs/2406.17104>`__ 安全分类器的自动对抗发现

::

    Mon, 24 Jun 2024 19:45:12 GMT
    Yash Kumar Lal, Preethi Lahoti, Aradhana Sinha, Yao Qin, Ananth Balashankar

Safety classifiers are critical in mitigating toxicity on online forums such as social media and in chatbots. Still, they continue to be vulnerable to emergent, and often innumerable, adversarial attacks. Traditional automated adversarial data generation methods, however, tend to produce attacks that are not diverse, but variations of previously observed harm types. We formalize the task of automated adversarial discovery for safety classifiers - to find new attacks along previously unseen harm dimensions that expose new weaknesses in the classifier. We measure progress on this task along two key axes (1) adversarial success: does the attack fool the classifier? and (2) dimensional diversity: does the attack represent a previously unseen harm type? Our evaluation of existing attack generation methods on the CivilComments toxicity task reveals their limitations: Word perturbation attacks fail to fool classifiers, while prompt-based LLM attacks have more adversarial success, but lack dimensional diversity. Even our best-performing prompt-based method finds new successful attacks on unseen harm dimensions of attacks only 5\% of the time. Automatically finding new harmful dimensions of attack is crucial and there is substantial headroom for future research on our new task.

------------

`[2406.17163] Paraphrase and Aggregate with Large Language Models for Minimizing Intent Classification Errors <https://arxiv.org/abs/2406.17163>`__ 用大型语言模型进行复述和聚合以最小化意图分类错误

::

    Mon, 24 Jun 2024 22:30:26 GMT
    Vikas Yadav and Zheng Tang and Vijay Srinivasan

Large language models (LLM) have achieved remarkable success in natural language generation but lesser focus has been given to their applicability in decision making tasks such as classification. We show that LLMs like LLaMa can achieve high performance on large multi-class classification tasks but still make classification errors and worse, generate out-of-vocabulary class labels.
To address these critical issues, we introduce Paraphrase and AGgregate (PAG)-LLM approach wherein an LLM generates multiple paraphrases of the input query (parallel queries), performs multi-class classification for the original query and each paraphrase, and at the end aggregate all the classification labels based on their confidence scores. We evaluate PAG-LLM on two large multi-class classication datasets: CLINC, and Banking and show 22.7% and 15.1% error reduction. We show that PAG-LLM is especially effective for hard examples where LLM is uncertain, and reduces the critical misclassification and hallucinated label generation errors

------------

`[2406.17231] CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph <https://arxiv.org/abs/2406.17231>`__ CogMG:大型语言模型与知识图谱的协同增强

::

    Tue, 25 Jun 2024 02:37:12 GMT
    Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao

Large language models have become integral to question-answering applications despite their propensity for generating hallucinations and factually inaccurate content. Querying knowledge graphs to reduce hallucinations in LLM meets the challenge of incomplete knowledge coverage in knowledge graphs. On the other hand, updating knowledge graphs by information extraction and knowledge graph completion faces the knowledge update misalignment issue. In this work, we introduce a collaborative augmentation framework, CogMG, leveraging knowledge graphs to address the limitations of LLMs in QA scenarios, explicitly targeting the problems of incomplete knowledge coverage and knowledge update misalignment. The LLMs identify and decompose required knowledge triples that are not present in the KG, enriching them and aligning updates with real-world demands. We demonstrate the efficacy of this approach through a supervised fine-tuned LLM within an agent framework, showing significant improvements in reducing hallucinations and enhancing factual accuracy in QA responses. Our code and video are publicly available.

------------

`[2406.17253] How Well Can Knowledge Edit Methods Edit Perplexing Knowledge? <https://arxiv.org/abs/2406.17253>`__ 知识编辑方法如何编辑令人困惑的知识?

::

    Tue, 25 Jun 2024 03:41:02 GMT
    Huaizhi Ge, Frank Rudzicz, Zining Zhu

As large language models (LLMs) are widely deployed, targeted editing of their knowledge has become a critical challenge. Recently, advancements in model editing techniques, such as Rank-One Model Editing (ROME), have paved the way for updating LLMs with new knowledge. However, the efficacy of these methods varies across different types of knowledge. This study investigates the capability of knowledge editing methods to incorporate new knowledge with varying degrees of "perplexingness", a term we use to describe the initial difficulty LLMs have in understanding new concepts. We begin by quantifying the "perplexingness" of target knowledge using pre-edit conditional probabilities, and assess the efficacy of edits through post-edit conditional probabilities.
Utilizing the widely-used CounterFact dataset, we find significant negative correlations between the "perplexingness" of the new knowledge and the edit efficacy across all 12 scenarios. To dive deeper into this phenomenon, we introduce a novel dataset, HierarchyData, consisting of 99 hyponym-hypernym pairs across diverse categories. Our analysis reveal that more abstract concepts (hypernyms) tend to be more perplexing than their specific counterparts (hyponyms). Further exploration into the influence of knowledge hierarchy on editing outcomes indicates that knowledge positioned at higher hierarchical levels is more challenging to modify in some scenarios. Our research highlights a previously overlooked aspect of LLM editing: the variable efficacy of editing methods in handling perplexing knowledge. By revealing how hierarchical relationships can influence editing outcomes, our findings offer new insights into the challenges of updating LLMs and pave the way for more nuanced approaches to model editing in the future.

------------

`[2406.17255] MPCODER: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning <https://arxiv.org/abs/2406.17255>`__ 

::

    Tue, 25 Jun 2024 03:45:28 GMT
    Zhenlong Dai, Chang Yao, WenKang Han, Ying Yuan, Zhipeng Gao, Jingyuan Chen

Large Language Models (LLMs) have demonstrated great potential for assisting developers in their daily development. However, most research focuses on generating correct code, how to use LLMs to generate personalized code has seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user Personalized Code Generator) to generate personalized code for multiple users.
To better learn coding style features, we utilize explicit coding style residual learning to capture the syntax code style standards and implicit style learning to capture the semantic code style conventions. We train a multi-user style adapter to better differentiate the implicit feature representations of different users through contrastive learning, ultimately enabling personalized code generation for multiple users. We further propose a novel evaluation metric for estimating similarities between codes of different coding styles.
The experimental results show the effectiveness of our approach for this novel task.

------------

`[2406.17260] Mitigating Hallucination in Fictional Character Role-Play <https://arxiv.org/abs/2406.17260>`__ 减轻虚构角色角色扮演中的幻觉

::

    Tue, 25 Jun 2024 03:56:33 GMT
    Nafis Sadeq, Zhouhang Xie, Byungkyu Kang, Prarit Lamba, Xiang Gao, Julian McAuley

Role-playing has wide-ranging applications in customer support, embodied agents, computational social science, etc. The influence of parametric world knowledge of large language models (LLMs) often causes role-playing characters to act out of character and hallucinate about things outside the scope of their knowledge. In this work, we focus on the evaluation and mitigation of hallucination in fictional character role-play. We introduce a dataset with more than 2,000 characters and 72,000 interviews, including 18,000 adversarial questions. We propose RoleFact, a role-playing method that mitigates hallucination by modulating the influence of parametric knowledge using a pre-calibrated confidence threshold. Experiments show that the proposed method improves the factual precision of generated responses by 18% for adversarial questions with a 44% reduction in temporal hallucination for time-sensitive interviews. The code and the dataset will be available at https://github.com/NafisSadeq/rolefact.git.

------------

`[2406.17261] TRAWL: Tensor Reduced and Approximated Weights for Large Language Models <https://arxiv.org/abs/2406.17261>`__ 拖网:大型语言模型的张量约简和近似权重

::

    Tue, 25 Jun 2024 04:01:32 GMT
    Yiran Luo, Het Patel, Yu Fu, Dawon Ahn, Jia Chen, Yue Dong, Evangelos E. Papalexakis

Large language models (LLMs) have fundamentally transformed artificial intelligence, catalyzing recent advancements while imposing substantial environmental and computational burdens. We introduce TRAWL (Tensor Reduced and Approximated Weights for Large Language Models), a novel methodology for optimizing LLMs through tensor decomposition. TRAWL leverages diverse strategies to exploit matrices within transformer-based architectures, realizing notable performance enhancements without necessitating retraining.
The most significant improvements were observed through a layer-by-layer intervention strategy, particularly when applied to fully connected weights of the final layers, yielding up to 16% enhancement in accuracy without the need for additional data or fine-tuning. These results underscore the importance of targeted and adaptive techniques in increasing the efficiency and effectiveness of large language model optimization, thereby promoting the development of more sustainable and accessible AI systems.

------------

`[2406.17262] D2LLM: Decomposed and Distilled Large Language Models for Semantic Search <https://arxiv.org/abs/2406.17262>`__ D2LLM:面向语义搜索的大型语言模型分解和提炼

::

    Tue, 25 Jun 2024 04:03:04 GMT
    Zihan Liao, Hang Yu, Jianguo Li, Jun Wang, Wei Zhang

The key challenge in semantic search is to create models that are both accurate and efficient in pinpointing relevant sentences for queries. While BERT-style bi-encoders excel in efficiency with pre-computed embeddings, they often miss subtle nuances in search tasks. Conversely, GPT-style LLMs with cross-encoder designs capture these nuances but are computationally intensive, hindering real-time applications. In this paper, we present D2LLMs-Decomposed and Distilled LLMs for semantic search-that combines the best of both worlds.
We decompose a cross-encoder into an efficient bi-encoder integrated with Pooling by Multihead Attention and an Interaction Emulation Module, achieving nuanced understanding and pre-computability. Knowledge from the LLM is distilled into this model using contrastive, rank, and feature imitation techniques. Our experiments show that D2LLM surpasses five leading baselines in terms of all metrics across three tasks, particularly improving NLI task performance by at least 6.45%. The source code is available at https://github.com/codefuse-ai/D2LLM.

------------

`[2406.17274] Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization? <https://arxiv.org/abs/2406.17274>`__ 文本摘要中不确定性估计方法的性能评估是否可信?

::

    Tue, 25 Jun 2024 04:41:17 GMT
    Jianfeng He, Runing Yang, Linlin Yu, Changbin Li, Ruoxi Jia, Feng Chen, Ming Jin, Chang-Tien Lu

Text summarization, a key natural language generation (NLG) task, is vital in various domains. However, the high cost of inaccurate summaries in risk-critical applications, particularly those involving human-in-the-loop decision-making, raises concerns about the reliability of uncertainty estimation on text summarization (UE-TS) evaluation methods. This concern stems from the dependency of uncertainty model metrics on diverse and potentially conflicting NLG metrics. To address this issue, we introduce a comprehensive UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The benchmark evaluates the uncertainty estimation capabilities of two large language models and one pre-trained language model on three datasets, with human-annotation analysis incorporated where applicable. We also assess the performance of 14 common uncertainty estimation methods within this benchmark.
Our findings emphasize the importance of considering multiple uncorrelated NLG metrics and diverse uncertainty estimation methods to ensure reliable and efficient evaluation of UE-TS techniques.

------------

`[2406.17287] Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models <https://arxiv.org/abs/2406.17287>`__ 用大型语言模型预测汉语心理咨询对话中的五大人格特征

::

    Tue, 25 Jun 2024 05:30:55 GMT
    Yang Yan, Lizhi Ma, Anqi Li, Jingsong Ma, Zhenzhong Lan

Accurate assessment of personality traits is crucial for effective psycho-counseling, yet traditional methods like self-report questionnaires are time-consuming and biased. This study exams whether Large Language Models (LLMs) can predict the Big Five personality traits directly from counseling dialogues and introduces an innovative framework to perform the task. Our framework applies role-play and questionnaire-based prompting to condition LLMs on counseling sessions, simulating client responses to the Big Five Inventory.
We evaluated our framework on 853 real-world counseling sessions, finding a significant correlation between LLM-predicted and actual Big Five traits, proving the validity of framework. Moreover, ablation studies highlight the importance of role-play simulations and task simplification via questionnaires in enhancing prediction accuracy. Meanwhile, our fine-tuned Llama3-8B model, utilizing Direct Preference Optimization with Supervised Fine-Tuning, achieves a 130.95\% improvement, surpassing the state-of-the-art Qwen1.5-110B by 36.94\% in personality prediction validity. In conclusion, LLMs can predict personality based on counseling dialogues. Our code and model are publicly available at \url{https://github.com/kuri-leo/BigFive-LLM-Predictor}, providing a valuable tool for future research in computational psychometrics.

------------

`[2406.17324] Delving into the Utilisation of ChatGPT in Scientific Publications in Astronomy <https://arxiv.org/abs/2406.17324>`__ 研究ChatGPT在天文学科学出版物中的应用

::

    Tue, 25 Jun 2024 07:15:10 GMT
    Simone Astarita, Sandor Kruk, Jan Reerink, Pablo G\'omez

Rapid progress in the capabilities of machine learning approaches in natural language processing has culminated in the rise of large language models over the last two years. Recent works have shown unprecedented adoption of these for academic writing, especially in some fields, but their pervasiveness in astronomy has not been studied sufficiently. To remedy this, we extract words that ChatGPT uses more often than humans when generating academic text and search a total of 1 million articles for them. This way, we assess the frequency of word occurrence in published works in astronomy tracked by the NASA Astrophysics Data System since 2000. We then perform a statistical analysis of the occurrences. We identify a list of words favoured by ChatGPT and find a statistically significant increase for these words against a control group in 2024, which matches the trend in other disciplines. These results suggest a widespread adoption of these models in the writing of astronomy papers. We encourage organisations, publishers, and researchers to work together to identify ethical and pragmatic guidelines to maximise the benefits of these systems while maintaining scientific rigour.

------------

`[2406.17328] Dual-Space Knowledge Distillation for Large Language Models <https://arxiv.org/abs/2406.17328>`__ 面向大型语言模型的双空间知识蒸馏

::

    Tue, 25 Jun 2024 07:25:15 GMT
    Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng Chen and Jinan Xu

Knowledge distillation (KD) is known as a promising solution to compress large language models (LLMs) via transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output distributions of the two models so that more knowledge can be transferred. However, in the current white-box KD framework, the output distributions are from the respective output spaces of the two models, using their own prediction heads. We argue that the space discrepancy will lead to low similarity between the teacher model and the student model on both representation and distribution levels. Furthermore, this discrepancy also hinders the KD process between models with different vocabularies, which is common for current LLMs. To address these issues, we propose a dual-space knowledge distillation (DSKD) framework that unifies the output spaces of the two models for KD. On the basis of DSKD, we further develop a cross-model attention mechanism, which can automatically align the representations of the two models with different vocabularies. Thus, our framework is not only compatible with various distance functions for KD (e.g., KL divergence) like the current framework, but also supports KD between any two LLMs regardless of their vocabularies. Experiments on task-agnostic instruction-following benchmarks show that DSKD significantly outperforms the current white-box KD framework with various distance functions, and also surpasses existing KD methods for LLMs with different vocabularies.

------------

`[2406.17377] A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual LLMs <https://arxiv.org/abs/2406.17377>`__ 基于多语言llm的跨语言自适应三管齐下方法

::

    Tue, 25 Jun 2024 08:53:46 GMT
    Vaibhav Singh, Amrith Krishna, Karthika NJ, Ganesh Ramakrishnan

Low-resource languages, by its very definition, tend to be under represented in the pre-training corpora of Large Language Models. In this work, we investigate three low-resource cross-lingual approaches that enable an LLM adapt to tasks in previously unseen languages. Llama-2 is an LLM where Indic languages, among many other language families, contribute to less than $0.005\%$ of the total $2$ trillion token pre-training corpora. In this work, we experiment with the English-dominated Llama-2 for cross-lingual transfer to three Indic languages, Bengali, Hindi, and Tamil as target languages. We study three approaches for cross-lingual transfer, under ICL and fine-tuning. One, we find that adding additional supervisory signals via a dominant language in the LLM, leads to improvements, both under in-context learning and fine-tuning.
Two, adapting the target languages to word reordering may be beneficial under ICL, but its impact diminishes with fine tuning. Finally, continued pre-training in one low-resource language can improve model performance for other related low-resource languages.

------------

`[2406.17378] A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens <https://arxiv.org/abs/2406.17378>`__ 一个文本值几个标记:来自llm的文本嵌入秘密地与关键标记对齐

::

    Tue, 25 Jun 2024 08:55:12 GMT
    Zhijie Nie, Richong Zhang, Zhanyu Wu

Text embeddings from large language models (LLMs) have achieved excellent results in tasks such as information retrieval, semantic textual similarity, etc. In this work, we show an interesting finding: when feeding a text into the embedding LLMs, the obtained text embedding will be able to be aligned with the key tokens in the input text. We first fully analyze this phenomenon on eight embedding LLMs and show that this phenomenon is universal and is not affected by model architecture, training strategy, and embedding method. With a deeper analysis, we then find that the main change in embedding space between the embedding LLMs and their original generative LLMs is in the first principal component. By adjusting the first principal component, we can align text embedding with the key tokens. Finally, we give several examples to demonstrate the vast application potential of this finding: (1) we propose a simple and practical sparse retrieval method based on the aligned tokens, which can achieve 80\% of the dense retrieval effect of the same model while reducing the computation significantly; (2) we show that our findings provide a fresh perspective to help understand fuzzy concepts (e.g., semantic relatedness vs.
semantic similarity) and emerging technologies (e.g., instruction-following embedding) in this field.

------------

`[2406.17385] Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance <https://arxiv.org/abs/2406.17385>`__ 母语设计偏差:研究英语母语性对语言模型性能的影响

::

    Tue, 25 Jun 2024 09:04:21 GMT
    Manon Reusens, Philipp Borchert, Jochen De Weerdt, Bart Baesens

Large Language Models (LLMs) excel at providing information acquired during pretraining on large-scale corpora and following instructions through user prompts. This study investigates whether the quality of LLM responses varies depending on the demographic profile of users. Considering English as the global lingua franca, along with the diversity of its dialects among speakers of different native languages, we explore whether non-native English speakers receive lower-quality or even factually incorrect responses from LLMs more frequently. Our results show that performance discrepancies occur when LLMs are prompted by native versus non-native English speakers and persist when comparing native speakers from Western countries with others. Additionally, we find a strong anchoring effect when the model recognizes or is made aware of the user's nativeness, which further degrades the response quality when interacting with non-native speakers. Our analysis is based on a newly collected dataset with over 12,000 unique annotations from 124 annotators, including information on their native language and English proficiency.

------------

`[2406.17404] Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training <https://arxiv.org/abs/2406.17404>`__ 制造噪声:通过噪声训练解锁语言模型并行推理能力

::

    Tue, 25 Jun 2024 09:25:39 GMT
    Yixuan Wang, Xianzhen Luo, Fuxuan Wei, Yijun Liu, Qingfu Zhu, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che

Existing speculative decoding methods typically require additional model structure and training processes to assist the model for draft token generation. This makes the migration of acceleration methods to the new model more costly and more demanding on device memory. To address this problem, we propose the Make Some Noise (MSN) training framework as a replacement for the supervised fine-tuning stage of the large language model. The training method simply introduces some noise at the input for the model to learn the denoising task. It significantly enhances the parallel decoding capability of the model without affecting the original task capability. In addition, we propose a tree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further improve the inference speed of MSN models. Experiments in both the general and code domains have shown that MSN can improve inference speed by 2.3-2.7x times without compromising model performance. The MSN model also achieves comparable acceleration ratios to the SOTA model with additional model structure on Spec-Bench.

------------

`[2406.17415] Variable Layer-Wise Quantization: A Simple and Effective Approach to Quantize LLMs <https://arxiv.org/abs/2406.17415>`__ 可变分层量化:一种简单有效的llm量化方法

::

    Tue, 25 Jun 2024 09:37:15 GMT
    Razvan-Gabriel Dumitru, Vikas Yadav, Rishabh Maheshwary, Paul-Ioan Clotan, Sathwik Tejaswi Madhusudhan, Mihai Surdeanu

We present a simple variable quantization approach that quantizes different layers of a large language model (LLM) at different bit levels. Specifically, we quantize the most important layers to higher bit precision and less important layers to lower bits to achieve floating point quantization levels.
We propose two effective strategies to measure the importance of layers within LLMs: the first measures the importance of a layer based on how different its output embeddings are from the input embeddings (the higher the better); the second estimates the importance of a layer using the number of layer weights that are much larger than average (the smaller the better). We show that quantizing different layers at varying bits according to our importance scores results in minimal performance drop with a far more compressed model size.
Finally, we present several practical key takeaways from our variable layer-wise quantization experiments: (a) LLM performance under variable quantization remains close to the original model until 25-50% of layers are moved in lower quantization using our proposed ordering but only until 5-10% if moved using no specific ordering; (b) Quantizing LLMs to lower bits performs substantially better than pruning unless extreme quantization (2-bit) is used; and (c) Layer-wise quantization to lower bits works better in the case of larger LLMs with more layers compared to smaller LLMs with fewer layers. The code used to run the experiments is available at: https://github.com/RazvanDu/LayerwiseQuant.

------------

`[2406.17453] Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain <https://arxiv.org/abs/2406.17453>`__ 学习提出信息性问题:用偏好优化和期望信息增益增强llm

::

    Tue, 25 Jun 2024 10:44:01 GMT
    Davide Mazzaccara, Alberto Testoni, Raffaella Bernardi

Questions are essential tools for acquiring the necessary information to complete information-seeking tasks. However, large language models (LLMs), especially open-source models, often perform poorly in generating informative questions, as measured by expected information gain (EIG). In this paper, we propose a method to enhance the informativeness of LLM-generated questions in 20-question game dialogues. We sample multiple questions from the same model (LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG questions to apply a Direct Preference Optimization (DPO) algorithm. Our results show that this method produces more effective questions (in terms of EIG), even in domains different from those used to train the DPO model.

------------

`[2406.17484] MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation <https://arxiv.org/abs/2406.17484>`__ 医疗保健:通过解耦临床对齐和知识聚合推进医学LLMs

::

    Tue, 25 Jun 2024 12:05:56 GMT
    Yusheng Liao, Shuyang Jiang, Yanfeng Wang, Yu Wang

Large language models (LLMs) have shown substantial progress in natural language understanding and generation, proving valuable especially in the medical field. Despite advancements, challenges persist due to the complexity and diversity inherent in medical tasks, which can be categorized as knowledge-intensive tasks and alignment-required tasks. Previous approaches either ignore the latter task or focus on a minority of tasks and hence lose generalization. To address these drawbacks, we propose a progressive fine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise aggregator to encode diverse knowledge in the first stage and filter out detrimental information. In the second stage, we drop the Noise Aggregator to avoid the interference of suboptimal representation and leverage an additional alignment module optimized towards an orthogonal direction to the knowledge space to mitigate knowledge forgetting. Based on this two-stage paradigm, we proposed a Medical LLM through decoupling Clinical Alignment and Knowledge Aggregation (MedCare), which is designed to achieve state-of-the-art (SOTA) performance on over 20 medical tasks, as well as SOTA results on specific medical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all demonstrate significant improvements over existing models with similar model sizes.

------------

`[2406.17526] LumberChunker: Long-Form Narrative Document Segmentation <https://arxiv.org/abs/2406.17526>`__ LumberChunker:长记叙文文档分割

::

    Tue, 25 Jun 2024 13:08:35 GMT
    Andr\'e V. Duarte, Jo\~ao Marques, Miguel Gra\c{c}a, Miguel Freire, Lei Li, Arlindo L. Oliveira

Modern NLP tasks increasingly rely on dense retrieval methods to access up-to-date and relevant contextual information. We are motivated by the premise that retrieval benefits from segments that can vary in size such that a content's semantic independence is better captured. We propose LumberChunker, a method leveraging an LLM to dynamically segment documents, which iteratively prompts the LLM to identify the point within a group of sequential passages where the content begins to shift. To evaluate our method, we introduce GutenQA, a benchmark with 3000 "needle in a haystack" type of question-answer pairs derived from 100 public domain narrative books available on Project Gutenberg. Our experiments show that LumberChunker not only outperforms the most competitive baseline by 7.37% in retrieval performance (DCG@20) but also that, when integrated into a RAG pipeline, LumberChunker proves to be more effective than other chunking methods and competitive baselines, such as the Gemini 1.5M Pro. Our Code and Data are available at https://github.com/joaodsmarques/LumberChunker

------------

`[2406.17557] The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale <https://arxiv.org/abs/2406.17557>`__ FineWeb数据集:从网络中提取最精细的大规模文本数据

::

    Tue, 25 Jun 2024 13:50:56 GMT
    Guilherme Penedo, Hynek Kydl\'i\v{c}ek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf

The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.

------------

`[2406.17563] Multi-property Steering of Large Language Models with Dynamic Activation Composition <https://arxiv.org/abs/2406.17563>`__ 基于动态激活组合的大型语言模型多属性转向

::

    Tue, 25 Jun 2024 14:00:42 GMT
    Daniel Scalena, Gabriele Sarti, Malvina Nissim

Activation steering methods were shown to be effective in conditioning language model generation by additively intervening over models' intermediate representations. However, the evaluation of these techniques has so far been limited to single conditioning properties and synthetic settings. In this work, we conduct a comprehensive evaluation of various activation steering strategies, highlighting the property-dependent nature of optimal parameters to ensure a robust effect throughout generation. To address this issue, we propose Dynamic Activation Composition, an information-theoretic approach to modulate the steering intensity of one or more properties throughout generation. Our experiments on multi-property steering show that our method successfully maintains high conditioning while minimizing the impact of conditioning on generation fluency.

------------

`[2406.17574] Beyond Text-to-SQL for IoT Defense: A Comprehensive Framework for Querying and Classifying IoT Threats <https://arxiv.org/abs/2406.17574>`__ Beyond Text-to-SQL用于物联网防御:查询和分类物联网威胁的综合框架

::

    Tue, 25 Jun 2024 14:14:35 GMT
    Ryan Pavlich, Nima Ebadi, Richard Tarbell, Billy Linares, Adrian Tan, Rachael Humphreys, Jayanta Kumar Das, Rambod Ghandiparsi, Hannah Haley, Jerris George, Rocky Slavin, Kim-Kwang Raymond Choo, Glenn Dietrich, Anthony Rios

Recognizing the promise of natural language interfaces to databases, prior studies have emphasized the development of text-to-SQL systems. While substantial progress has been made in this field, existing research has concentrated on generating SQL statements from text queries. The broader challenge, however, lies in inferring new information about the returned data.
Our research makes two major contributions to address this gap. First, we introduce a novel Internet-of-Things (IoT) text-to-SQL dataset comprising 10,985 text-SQL pairs and 239,398 rows of network traffic activity. The dataset contains additional query types limited in prior text-to-SQL datasets, notably temporal-related queries. Our dataset is sourced from a smart building's IoT ecosystem exploring sensor read and network traffic data. Second, our dataset allows two-stage processing, where the returned data (network traffic) from a generated SQL can be categorized as malicious or not. Our results show that joint training to query and infer information about the data can improve overall text-to-SQL performance, nearly matching substantially larger models.
We also show that current large language models (e.g., GPT3.5) struggle to infer new information about returned data, thus our dataset provides a novel test bed for integrating complex domain-specific reasoning into LLMs.

------------

`[2406.17588] LongIns: A Challenging Long-context Instruction-based Exam for LLMs <https://arxiv.org/abs/2406.17588>`__ LongIns:一种具有挑战性的基于长上下文指令的LLMs考试

::

    Tue, 25 Jun 2024 14:31:26 GMT
    Shawn Gavin, Tuney Zheng, Jiaheng Liu, Quehry Que, Noah Wang, Jian Yang, Chenchen Zhang, Wenhao Huang, Wenhu Chen, Ge Zhang

The long-context capabilities of large language models (LLMs) have been a hot topic in recent years. To evaluate the performance of LLMs in different scenarios, various assessment benchmarks have emerged. However, as most of these benchmarks focus on identifying key information to answer questions, which mainly requires the retrieval ability of LLMs, these benchmarks can partially represent the reasoning performance of LLMs from large amounts of information. Meanwhile, although LLMs often claim to have context windows of 32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual supported length of these LLMs. To address these issues, we propose the LongIns benchmark dataset, a challenging long-context instruction-based exam for LLMs, which is built based on the existing instruction datasets. Specifically, in our LongIns, we introduce three evaluation settings: Global Instruction & Single Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction & Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations on existing LLMs and have the following important findings: (1). The top-performing GPT-4 with 128k context length performs poorly on the evaluation context window of 16k in our LongIns. (2). For the multi-hop reasoning ability of many existing LLMs, significant efforts are still needed under short context windows (less than 4k).

------------

`[2406.17600] "Seeing the Big through the Small": Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations? <https://arxiv.org/abs/2406.17600>`__ “以小见大”:llm能从一些解释中近似人类对NLI的判断分布吗?

::

    Tue, 25 Jun 2024 14:42:17 GMT
    Beiduo Chen, Xinpeng Wang, Siyao Peng, Robert Litschko, Anna Korhonen, Barbara Plank

Human label variation (HLV) is a valuable source of information that arises when multiple human annotators provide different labels for valid reasons. In Natural Language Inference (NLI) earlier approaches to capturing HLV involve either collecting annotations from many crowd workers to represent human judgment distribution (HJD) or use expert linguists to provide detailed explanations for their chosen labels. While the former method provides denser HJD information, obtaining it is resource-intensive. In contrast, the latter offers richer textual information but it is challenging to scale up to many human judges. Besides, large language models (LLMs) are increasingly used as evaluators (``LLM judges'') but with mixed results, and few works aim to study HJDs. This study proposes to exploit LLMs to approximate HJDs using a small number of expert labels and explanations. Our experiments show that a few explanations significantly improve LLMs' ability to approximate HJDs with and without explicit labels, thereby providing a solution to scale up annotations for HJD. However, fine-tuning smaller soft-label aware models with the LLM-generated model judgment distributions (MJDs) presents partially inconsistent results: while similar in distance, their resulting fine-tuned models and visualized distributions differ substantially. We show the importance of complementing instance-level distance measures with a global-level shape metric and visualization to more effectively evaluate MJDs against human judgment distributions.

------------

`[2406.17624] Self-assessment, Exhibition, and Recognition: a Review of Personality in Large Language Models <https://arxiv.org/abs/2406.17624>`__ 自我评估、展示和识别:大型语言模型中的个性综述

::

    Tue, 25 Jun 2024 15:08:44 GMT
    Zhiyuan Wen, Yu Yang, Jiannong Cao, Haoming Sun, Ruosong Yang, Shuaiqi Liu

As large language models (LLMs) appear to behave increasingly human-like in text-based interactions, more and more researchers become interested in investigating personality in LLMs. However, the diversity of psychological personality research and the rapid development of LLMs have led to a broad yet fragmented landscape of studies in this interdisciplinary field. Extensive studies across different research focuses, different personality psychometrics, and different LLMs make it challenging to have a holistic overview and further pose difficulties in applying findings to real-world applications. In this paper, we present a comprehensive review by categorizing current studies into three research problems: self-assessment, exhibition, and recognition, based on the intrinsic characteristics and external manifestations of personality in LLMs. For each problem, we provide a thorough analysis and conduct in-depth comparisons of their corresponding solutions. Besides, we summarize research findings and open challenges from current studies and further discuss their underlying causes. We also collect extensive publicly available resources to facilitate interested researchers and developers. Lastly, we discuss the potential future research directions and application scenarios. Our paper is the first comprehensive survey of up-to-date literature on personality in LLMs.
By presenting a clear taxonomy, in-depth analysis, promising future directions, and extensive resource collections, we aim to provide a better understanding and facilitate further advancements in this emerging field.

------------

`[2406.17626] CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference <https://arxiv.org/abs/2406.17626>`__ 

::

    Tue, 25 Jun 2024 15:13:02 GMT
    Erxin Yu, Jing Li, Ming Liao, Siqi Wang, Zuchen Gao, Fei Mi, Lanqing Hong

As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research problem. Previous red-teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our knowledge, we are the first to study LLM safety in multi-turn dialogue coreference. We created a dataset of 1,400 questions across 14 categories, each featuring multi-turn coreference safety attacks. We then conducted detailed evaluations on five widely used open-source LLMs. The results indicated that under multi-turn coreference safety attacks, the highest attack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was 13.9% with the Mistral-7B-Instruct model. These findings highlight the safety vulnerabilities in LLMs during dialogue coreference interactions.

------------

`[2406.17633] Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels <https://arxiv.org/abs/2406.17633>`__ 自动标注中的知识蒸馏:基于llm生成训练标签的监督文本分类

::

    Tue, 25 Jun 2024 15:20:25 GMT
    Nicholas Pangakis and Samuel Wolken

Computational social science (CSS) practitioners often rely on human-labeled data to fine-tune supervised text classifiers. We assess the potential for researchers to augment or replace human-generated training data with surrogate training labels from generative large language models (LLMs). We introduce a recommended workflow and test this LLM application by replicating 14 classification tasks and measuring performance. We employ a novel corpus of English-language text classification data sets from recent CSS articles in high-impact journals. Because these data sets are stored in password-protected archives, our analyses are less prone to issues of contamination. For each task, we compare supervised classifiers fine-tuned using GPT-4 labels against classifiers fine-tuned with human annotations and against labels from GPT-4 and Mistral-7B with few-shot in-context learning. Our findings indicate that supervised classification models fine-tuned on LLM-generated labels perform comparably to models fine-tuned with labels from human annotators. Fine-tuning models using LLM-generated labels can be a fast, efficient and cost-effective method of building supervised text classifiers.

------------

`[2406.17642] Banishing LLM Hallucinations Requires Rethinking Generalization <https://arxiv.org/abs/2406.17642>`__ 消除LLM幻觉需要重新思考泛化

::

    Tue, 25 Jun 2024 15:31:01 GMT
    Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, and Gregory Diamos

Despite their powerful chat, coding, and reasoning abilities, Large Language Models (LLMs) frequently hallucinate. Conventional wisdom suggests that hallucinations are a consequence of a balance between creativity and factuality, which can be mitigated, but not eliminated, by grounding the LLM in external knowledge sources. Through extensive systematic experiments, we show that these traditional approaches fail to explain why LLMs hallucinate in practice. Specifically, we show that LLMs augmented with a massive Mixture of Memory Experts (MoME) can easily memorize large datasets of random numbers. We corroborate these experimental findings with a theoretical construction showing that simple neural networks trained to predict the next token hallucinate when the training loss is above a threshold as it usually does in practice when training on internet scale data. We interpret our findings by comparing against traditional retrieval methods for mitigating hallucinations. We use our findings to design a first generation model for removing hallucinations -- Lamini-1 -- that stores facts in a massive mixture of millions of memory experts that are retrieved dynamically.

------------

`[2406.17692] From Distributional to Overton Pluralism: Investigating Large Language Model Alignment <https://arxiv.org/abs/2406.17692>`__ 从分布式到欧弗顿多元化:大型语言模型对齐研究

::

    Tue, 25 Jun 2024 16:32:33 GMT
    Thom Lake, Eunsol Choi, Greg Durrett

The alignment process changes several properties of a large language model's (LLM's) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that alignment suppresses useful information, it is natural to ask the opposite question: do aligned models surface information that cannot be recovered from base models? Our second investigation shows this is not the case and the behavior of aligned models is recoverable from base models without fine-tuning. A combination of in-context examples and lower-resolution semantic hints about response content can elicit responses from base LLMs that are as similar to alignment-tuned LLM responses as alignment-tuned LLM responses are to each other. Taken together, these results indicate that current alignment techniques capture but do not extend the useful subset of assistant-like base LLM behavior, providing further evidence for the Superficial Alignment Hypothesis. They also show that in-context alignment can go surprisingly far as a strategy for imitating aligned LLMs without fine-tuning. Our code and data is available at https://github.com/thomlake/investigating-alignment.

------------

`[2406.17737] LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users <https://arxiv.org/abs/2406.17737>`__ 针对性能低下的LLM不成比例地影响易受攻击的用户

::

    Tue, 25 Jun 2024 17:24:07 GMT
    Elinor Poole-Dayan, Deb Roy, Jad Kabbara

While state-of-the-art Large Language Models (LLMs) have shown impressive performance on many tasks, there has been extensive research on undesirable model behavior such as hallucinations and bias. In this work, we investigate how the quality of LLM responses changes in terms of information accuracy, truthfulness, and refusals depending on three user traits: English proficiency, education level, and country of origin. We present extensive experimentation on three state-of-the-art LLMs and two different datasets targeting truthfulness and factuality. Our findings suggest that undesirable behaviors in state-of-the-art LLMs occur disproportionately more for users with lower English proficiency, of lower education status, and originating from outside the US, rendering these models unreliable sources of information towards their most vulnerable users.

------------

`[2406.17761] CaLMQA: Exploring culturally specific long-form question answering across 23 languages <https://arxiv.org/abs/2406.17761>`__ calqa:跨23种语言探索特定文化的长篇问答

::

    Tue, 25 Jun 2024 17:45:26 GMT
    Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, Eunsol Choi

Large language models (LLMs) are commonly used for long-form question answering, which requires them to generate paragraph-length answers to complex questions. While long-form QA has been well-studied in English via many different datasets and evaluation metrics, this research has not been extended to cover most other languages. To bridge this gap, we introduce CaLMQA, a collection of 2.6K complex questions spanning 23 languages, including under-resourced, rarely-studied languages such as Fijian and Kirundi. Our dataset includes both naturally-occurring questions collected from community web forums as well as questions written by native speakers, whom we hire for this purpose. Our process yields diverse, complex questions that reflect cultural topics (e.g. traditions, laws, news) and the language usage of native speakers. We conduct automatic evaluation across a suite of open- and closed-source models using our novel metric CaLMScore, which detects incorrect language and token repetitions in answers, and observe that the quality of LLM-generated answers degrades significantly for some low-resource languages.
We perform human evaluation on a subset of models and see that model performance is significantly worse for culturally specific questions than for culturally agnostic questions. Our findings highlight the need for further research in LLM multilingual capabilities and non-English LFQA evaluation.

------------

`[2406.16963] Large Language Models for Link Stealing Attacks Against Graph Neural Networks <https://arxiv.org/abs/2406.16963>`__ 面向图神经网络链接窃取攻击的大型语言模型

::

    Sat, 22 Jun 2024 02:47:24 GMT
    Faqian Guan, Tianqing Zhu, Hui Sun, Wanlei Zhou, and Philip S. Yu

Graph data contains rich node features and unique edge information, which have been applied across various domains, such as citation networks or recommendation systems. Graph Neural Networks (GNNs) are specialized for handling such data and have shown impressive performance in many applications.
However, GNNs may contain of sensitive information and susceptible to privacy attacks. For example, link stealing is a type of attack in which attackers infer whether two nodes are linked or not. Previous link stealing attacks primarily relied on posterior probabilities from the target GNN model, neglecting the significance of node features. Additionally, variations in node classes across different datasets lead to different dimensions of posterior probabilities. The handling of these varying data dimensions posed a challenge in using a single model to effectively conduct link stealing attacks on different datasets. To address these challenges, we introduce Large Language Models (LLMs) to perform link stealing attacks on GNNs. LLMs can effectively integrate textual features and exhibit strong generalizability, enabling attacks to handle diverse data dimensions across various datasets. We design two distinct LLM prompts to effectively combine textual features and posterior probabilities of graph nodes. Through these designed prompts, we fine-tune the LLM to adapt to the link stealing attack task. Furthermore, we fine-tune the LLM using multiple datasets and enable the LLM to learn features from different datasets simultaneously. Experimental results show that our approach significantly enhances the performance of existing link stealing attack tasks in both white-box and black-box scenarios. Our method can execute link stealing attacks across different datasets using only a single model, making link stealing attacks more applicable to real-world scenarios.

------------

`[2406.16964] Are Language Models Actually Useful for Time Series Forecasting? <https://arxiv.org/abs/2406.16964>`__ 语言模型对时间序列预测真的有用吗?

::

    Sat, 22 Jun 2024 03:33:38 GMT
    Mingtian Tan, Mike A. Merrill, Vinayak Gupta, Tim Althoff and Thomas Hartvigsen

Large language models (LLMs) are being applied to time series tasks, particularly time series forecasting. However, are language models actually useful for time series? After a series of ablation studies on three recent and popular LLM-based time series forecasting methods, we find that removing the LLM component or replacing it with a basic attention layer does not degrade the forecasting results -- in most cases the results even improved. We also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, we explore time series encoders and reveal that patching and attention structures perform similarly to state-of-the-art LLM-based forecasters.

------------

`[2406.16985] Unveiling LLM Mechanisms Through Neural ODEs and Control Theory <https://arxiv.org/abs/2406.16985>`__ 用神经常微分方程和控制理论揭示LLM机制

::

    Sun, 23 Jun 2024 22:56:34 GMT
    Yukun Zhang

This study presents a novel approach that leverages Neural Ordinary Differential Equations (Neural ODEs) to unravel the intricate relationships between inputs and outputs in Large Language Models (LLMs), and employs robust control to fine-tune outputs to meet predefined standards. Central to our methodology is the transformation of LLM inputs and outputs into a lower-dimensional latent space, facilitating a detailed examination of the information processing pathways within LLMs. Neural ODEs play a pivotal role in this investigation by providing a dynamic model that captures the continuous evolution of data within the LLMs. Additionally, robust control mechanisms are applied to strategically adjust the model's outputs, ensuring they not only maintain high quality and reliability but also adhere to specific performance criteria. This fusion of Neural ODEs and robust control represents a significant advancement in LLM interpretability, offering a comprehensive framework that elucidates the previously opaque mechanisms of these complex models. Our empirical results validate the effectiveness of this integrated approach, making a substantial contribution to the field of explainable AI by merging advanced machine learning techniques with the critical need for transparency and control in AI outputs.

------------

`[2406.17216] Machine Unlearning Fails to Remove Data Poisoning Attacks <https://arxiv.org/abs/2406.17216>`__ 机器遗忘无法消除数据中毒攻击

::

    Tue, 25 Jun 2024 02:05:29 GMT
    Martin Pawelczyk, Jimmy Z. Di, Yiwei Lu, Gautam Kamath, Ayush Sekhari, Seth Neel

We revisit the efficacy of several practical methods for approximate machine unlearning developed for large-scale deep learning. In addition to complying with data deletion requests, one often-cited potential application for unlearning methods is to remove the effects of training on poisoned data. We experimentally demonstrate that, while existing unlearning methods have been demonstrated to be effective in a number of evaluation settings (e.g., alleviating membership inference attacks), they fail to remove the effects of data poisoning, across a variety of types of poisoning attacks (indiscriminate, targeted, and a newly-introduced Gaussian poisoning attack) and models (image classifiers and LLMs); even when granted a relatively large compute budget. In order to precisely characterize unlearning efficacy, we introduce new evaluation metrics for unlearning based on data poisoning. Our results suggest that a broader perspective, including a wider variety of evaluations, is required to avoid a false sense of confidence in machine unlearning procedures for deep learning without provable guarantees. Moreover, while unlearning methods show some signs of being useful to efficiently remove poisoned datapoints without having to retrain, our work suggests that these methods are not yet "ready for prime time", and currently provide limited benefit over retraining.

------------

`[2406.17272] A Comprehensive Solution to Connect Speech Encoder and Large Language Model for ASR <https://arxiv.org/abs/2406.17272>`__ 

::

    Tue, 25 Jun 2024 04:35:50 GMT
    Van Tung Pham, Yist Lin, Tao Han, Wei Li, Jun Zhang, Lu Lu, Yuxuan Wang

Recent works have shown promising results in connecting speech encoders to large language models (LLMs) for speech recognition. However, several limitations persist, including limited fine-tuning options, a lack of mechanisms to enforce speech-text alignment, and high insertion errors especially in domain mismatch conditions. This paper presents a comprehensive solution to address these issues. We begin by investigating more thoughtful fine-tuning schemes. Next, we propose a matching loss to enhance alignment between modalities. Finally, we explore training and inference methods to mitigate high insertion errors. Experimental results on the Librispeech corpus demonstrate that partially fine-tuning the encoder and LLM using parameter-efficient methods, such as LoRA, is the most cost-effective approach.
Additionally, the matching loss improves modality alignment, enhancing performance. The proposed training and inference methods significantly reduce insertion errors.

------------

`[2406.17467] Early learning of the optimal constant solution in neural networks and humans <https://arxiv.org/abs/2406.17467>`__ 神经网络和人类中最优常数解的早期学习

::

    Tue, 25 Jun 2024 11:12:52 GMT
    Jirko Rubruck, Jan P. Bauer, Andrew Saxe, Christopher Summerfield

Deep neural networks learn increasingly complex functions over the course of training. Here, we show both empirically and theoretically that learning of the target function is preceded by an early phase in which networks learn the optimal constant solution (OCS) - that is, initial model responses mirror the distribution of target labels, while entirely ignoring information provided in the input. Using a hierarchical category learning task, we derive exact solutions for learning dynamics in deep linear networks trained with bias terms. Even when initialized to zero, this simple architectural feature induces substantial changes in early dynamics. We identify hallmarks of this early OCS phase and illustrate how these signatures are observed in deep linear networks and larger, more complex (and nonlinear) convolutional neural networks solving a hierarchical learning task based on MNIST and CIFAR10. We explain these observations by proving that deep linear networks necessarily learn the OCS during early learning. To further probe the generality of our results, we train human learners over the course of three days on the category learning task. We then identify qualitative signatures of this early OCS phase in terms of the dynamics of true negative (correct-rejection) rates. Surprisingly, we find the same early reliance on the OCS in the behaviour of human learners. Finally, we show that learning of the OCS can emerge even in the absence of bias terms and is equivalently driven by generic correlations in the input data. Overall, our work suggests the OCS as a universal learning principle in supervised, error-corrective learning, and the mechanistic reasons for its prevalence.

------------

`[2406.17542] CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained Models using Greedy Coordinate Descent <https://arxiv.org/abs/2406.17542>`__ CDQuant:使用贪婪坐标下降对大型预训练模型进行精确的训练后权重量化

::

    Tue, 25 Jun 2024 13:29:14 GMT
    Pranav Ajit Nair and Arun Sai Suggala

Large language models (LLMs) have recently demonstrated remarkable performance across diverse language tasks. But their deployment is often constrained by their substantial computational and storage requirements.
Quantization has emerged as a key technique for addressing this challenge, enabling the compression of large models with minimal impact on performance.
The recent GPTQ algorithm, a post-training quantization (PTQ) method, has proven highly effective for compressing LLMs, sparking a wave of research that leverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the PTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ with improved performance. CDQuant uses coordinate descent to minimize the layer-wise reconstruction loss to achieve high-quality quantized weights. Our algorithm is easy to implement and scales efficiently to models with hundreds of billions of parameters. Through extensive evaluation on the PaLM2 model family, we demonstrate that CDQuant consistently outperforms GPTQ across diverse model sizes and quantization levels. In particular, for INT2 quantization of PaLM2-Otter, CDQuant achieves a 10% reduction in perplexity compared to GPTQ.

------------

`[2406.17706] FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model <https://arxiv.org/abs/2406.17706>`__ FedBiOT:无全模型联邦学习中的LLM局部微调

::

    Tue, 25 Jun 2024 16:45:47 GMT
    Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, Jing Gao

Large language models (LLMs) show amazing performance on many domain-specific tasks after fine-tuning with some appropriate data. However, many domain-specific data are privately distributed across multiple owners. Thus, this dilemma raises the interest in how to perform LLM fine-tuning in federated learning (FL). However, confronted with limited computation and communication capacities, FL clients struggle to fine-tune an LLM effectively. To this end, we introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL.
Specifically, our method involves the server generating a compressed LLM and aligning its performance with the full model. Subsequently, the clients fine-tune a lightweight yet important part of the compressed model, referred to as an adapter. Notice that as the server has no access to the private data owned by the clients, the data used for alignment by the server has a different distribution from the one used for fine-tuning by clients. We formulate the problem into a bi-level optimization problem to minimize the negative effect of data discrepancy and derive the updating rules for the server and clients. We conduct extensive experiments on LLaMA-2, empirically showing that the adapter has exceptional performance when reintegrated into the global LLM. The results also indicate that the proposed FedBiOT significantly reduces resource consumption compared to existing benchmarks, all while achieving comparable performance levels.

------------

`[2406.16990] AND: Audio Network Dissection for Interpreting Deep Acoustic <https://arxiv.org/abs/2406.16990>`__ 

::

    Mon, 24 Jun 2024 06:02:07 GMT
    Tung-Yu Wu, Yu-Xiang Lin, and Tsui-Wei Weng

Neuron-level interpretations aim to explain network behaviors and properties by investigating neurons responsive to specific perceptual or structural input patterns. Although there is emerging work in the vision and language domains, none is explored for acoustic models. To bridge the gap, we introduce $\textit{AND}$, the first $\textbf{A}$udio $\textbf{N}$etwork $\textbf{D}$issection framework that automatically establishes natural language explanations of acoustic neurons based on highly-responsive audio.
$\textit{AND}$ features the use of LLMs to summarize mutual acoustic features and identities among audio. Extensive experiments are conducted to verify $\textit{AND}$'s precise and informative descriptions. In addition, we demonstrate a potential use of $\textit{AND}$ for audio machine unlearning by conducting concept-specific pruning based on the generated descriptions.
Finally, we highlight two acoustic model behaviors with analysis by $\textit{AND}$: (i) models discriminate audio with a combination of basic acoustic features rather than high-level abstract concepts; (ii) training strategies affect model behaviors and neuron interpretability -- supervised training guides neurons to gradually narrow their attention, while self-supervised learning encourages neurons to be polysemantic for exploring high-level features.

------------

`[2406.16995] A large language model for predicting T cell receptor-antigen binding specificity <https://arxiv.org/abs/2406.16995>`__ 预测T细胞受体-抗原结合特异性的大型语言模型

::

    Mon, 24 Jun 2024 08:36:40 GMT
    Xing Fang, Chenpeng Yu, Shiye Tian, Hui Liu

The human immune response depends on the binding of T-cell receptors (TCRs) to antigens (pTCR), which elicits the T cells to eliminate viruses, tumor cells, and other pathogens. The ability of human immunity system responding to unknown viruses and bacteria stems from the TCR diversity. However, this vast diversity poses challenges on the TCR-antigen binding prediction methods. In this study, we propose a Masked Language Model (MLM), referred to as tcrLM, to overcome limitations in model generalization. Specifically, we randomly masked sequence segments and train tcrLM to infer the masked segment, thereby extract expressive feature from TCR sequences. Meanwhile, we introduced virtual adversarial training techniques to enhance the model's robustness. We built the largest TCR CDR3 sequence dataset to date (comprising 2,277,773,840 residuals), and pre-trained tcrLM on this dataset. Our extensive experimental results demonstrate that tcrLM achieved AUC values of 0.937 and 0.933 on independent test sets and external validation sets, respectively, which remarkably outperformed four previously published prediction methods. On a large-scale COVID-19 pTCR binding test set, our method outperforms the current state-of-the-art method by at least 8%, highlighting the generalizability of our method. Furthermore, we validated that our approach effectively predicts immunotherapy response and clinical outcomes on a clinical cohorts. These findings clearly indicate that tcrLM exhibits significant potential in predicting antigenic immunogenicity.

------------

`[2406.17092] BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models <https://arxiv.org/abs/2406.17092>`__ bear:基于嵌入的指令调优语言模型安全后门对抗移除

::

    Mon, 24 Jun 2024 19:29:47 GMT
    Yi Zeng, Weiyu Sun, Tran Ngoc Huynh, Dawn Song, Bo Li, Ruoxi Jia

Safety backdoor attacks in large language models (LLMs) enable the stealthy triggering of unsafe behaviors while evading detection during normal interactions. The high dimensionality of potential triggers in the token space and the diverse range of malicious behaviors make this a critical challenge. We present BEEAR, a mitigation approach leveraging the insight that backdoor triggers induce relatively uniform drifts in the model's embedding space. Our bi-level optimization method identifies universal embedding perturbations that elicit unwanted behaviors and adjusts the model parameters to reinforce safe behaviors against these perturbations. Experiments show BEEAR reduces the success rate of RLHF time backdoor attacks from >95% to <1% and from 47% to 0% for instruction-tuning time backdoors targeting malicious code generation, without compromising model utility. Requiring only defender-defined safe and unwanted behaviors, BEEAR represents a step towards practical defenses against safety backdoors in LLMs, providing a foundation for further advancements in AI safety and security.

------------

`[2406.17531] Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness <https://arxiv.org/abs/2406.17531>`__ 以细微差别增强基于llm的人-机器人交互以增强多样性感知

::

    Tue, 25 Jun 2024 13:15:36 GMT
    Lucrezia Grassi, Carmine Tommaso Recchiuto, Antonio Sgorbissa

This paper presents a system for diversity-aware autonomous conversation leveraging the capabilities of large language models (LLMs). The system adapts to diverse populations and individuals, considering factors like background, personality, age, gender, and culture. The conversation flow is guided by the structure of the system's pre-established knowledge base, while LLMs are tasked with various functions, including generating diversity-aware sentences.
Achieving diversity-awareness involves providing carefully crafted prompts to the models, incorporating comprehensive information about users, conversation history, contextual details, and specific guidelines. To assess the system's performance, we conducted both controlled and real-world experiments, measuring a wide range of performance indicators.

------------

`[2406.17233] Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement <https://arxiv.org/abs/2406.17233>`__ 增强细粒度对齐的自构造上下文反编译

::

    Tue, 25 Jun 2024 02:37:53 GMT
    Yunlong Feng, Yang Xu, Dechuan Teng, Honglin Mu, Xiao Xu, Libo Qin, Wanxiang Che, Qingfu Zhu

Decompilation transforms compiled code back into a high-level programming language for analysis when source code is unavailable. Previous work has primarily focused on enhancing decompilation performance by increasing the scale of model parameters or training data for pre-training. Based on the characteristics of the decompilation task, we propose two methods: (1) Without fine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method recompiles the LLM's decompilation results to construct pairs for in-context learning, helping the model improve decompilation performance. (2) Fine-grained Alignment Enhancement (FAE), which meticulously aligns assembly code with source code at the statement level by leveraging debugging information, is employed during the fine-tuning phase to achieve further improvements in decompilation. By integrating these two methods, we achieved a Re-Executability performance improvement of approximately 7.35\% on the Decompile-Eval benchmark, establishing a new state-of-the-art performance of 55.03\%.

------------

`[2406.17126] MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs <https://arxiv.org/abs/2406.17126>`__ MM-SpuBench:更好地理解多模态llm中的虚假偏差

::

    Mon, 24 Jun 2024 20:29:16 GMT
    Wenqian Ye, Guangtao Zheng, Yunsheng Ma, Xu Cao, Bolin Lai, James M. Rehg, Aidong Zhang

Spurious bias, a tendency to use spurious correlations between non-essential input attributes and target variables for predictions, has revealed a severe robustness pitfall in deep learning models trained on single modality data.
Multimodal Large Language Models (MLLMs), which integrate both vision and language models, have demonstrated strong capability in joint vision-language understanding. However, whether spurious biases are prevalent in MLLMs remains under-explored. We mitigate this gap by analyzing the spurious biases in a multimodal setting, uncovering the specific test data patterns that can manifest this problem when biases in the vision model cascade into the alignment between visual and text tokens in MLLMs. To better understand this problem, we introduce MM-SpuBench, a comprehensive visual question-answering (VQA) benchmark designed to evaluate MLLMs' reliance on nine distinct categories of spurious correlations from five open-source image datasets. The VQA dataset is built from human-understandable concept information (attributes). Leveraging this benchmark, we conduct a thorough evaluation of current state-of-the-art MLLMs. Our findings illuminate the persistence of the reliance on spurious correlations from these models and underscore the urge for new methodologies to mitigate spurious biases. To support the MLLM robustness research, we release our VQA benchmark at https://huggingface.co/datasets/mmbench/MM-SpuBench.

------------

`[2406.17228] Greedy equivalence search for nonparametric graphical models <https://arxiv.org/abs/2406.17228>`__ 非参数图模型的贪婪等价搜索

::

    Tue, 25 Jun 2024 02:31:32 GMT
    Bryon Aragam

One of the hallmark achievements of the theory of graphical models and Bayesian model selection is the celebrated greedy equivalence search (GES) algorithm due to Chickering and Meek. GES is known to consistently estimate the structure of directed acyclic graph (DAG) models in various special cases including Gaussian and discrete models, which are in particular curved exponential families. A general theory that covers general nonparametric DAG models, however, is missing. Here, we establish the consistency of greedy equivalence search for general families of DAG models that satisfy smoothness conditions on the Markov factorization, and hence may not be curved exponential families, or even parametric. The proof leverages recent advances in nonparametric Bayes to construct a test for comparing misspecified DAG models that avoids arguments based on the Laplace approximation. Nonetheless, when the Laplace approximation is valid and a consistent scoring function exists, we recover the classical result. As a result, we obtain a general consistency theorem for GES applied to general DAG models.

------------

`[2406.17295] MatText: Do Language Models Need More than Text & Scale for Materials Modeling? <https://arxiv.org/abs/2406.17295>`__ 

::

    Tue, 25 Jun 2024 05:45:07 GMT
    Nawaf Alampara, Santiago Miret, Kevin Maik Jablonka

Effectively representing materials as text has the potential to leverage the vast advancements of large language models (LLMs) for discovering new materials. While LLMs have shown remarkable success in various domains, their application to materials science remains underexplored. A fundamental challenge is the lack of understanding of how to best utilize text-based representations for materials modeling. This challenge is further compounded by the absence of a comprehensive benchmark to rigorously evaluate the capabilities and limitations of these text representations in capturing the complexity of material systems. To address this gap, we propose MatText, a suite of benchmarking tools and datasets designed to systematically evaluate the performance of language models in modeling materials. MatText encompasses nine distinct text-based representations for material systems, including several novel representations. Each representation incorporates unique inductive biases that capture relevant information and integrate prior physical knowledge about materials. Additionally, MatText provides essential tools for training and benchmarking the performance of language models in the context of materials science. These tools include standardized dataset splits for each representation, probes for evaluating sensitivity to geometric factors, and tools for seamlessly converting crystal structures into text. Using MatText, we conduct an extensive analysis of the capabilities of language models in modeling materials. Our findings reveal that current language models consistently struggle to capture the geometric information crucial for materials modeling across all representations. Instead, these models tend to leverage local information, which is emphasized in some of our novel representations. Our analysis underscores MatText's ability to reveal shortcomings of text-based methods for materials design.

------------

`[2309.05519] NExT-GPT: Any-to-Any Multimodal LLM <https://arxiv.org/abs/2309.05519>`__ NExT-GPT:任意对任意多模态LLM

::

    replaced with revised version Tue, 25 Jun 2024 05:01:09 GMT
    Submission history From: Hao Fei [view email]
    [v1] Mon, 11 Sep 2023 15:02:25 UTC (6,480 KB)
    [v2] Wed, 13 Sep 2023 16:49:34 UTC (6,480 KB)
    [v3] Tue, 25 Jun 2024 05:01:09 UTC (8,351 KB)
    Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua

While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: this https URL

------------

`[2406.14343] iWISDM: Assessing instruction following in multimodal models at scale <https://arxiv.org/abs/2406.14343>`__ iWISDM:评估大规模多模态模型的指令跟随

::

    replaced with revised version Tue, 25 Jun 2024 15:12:01 GMT
    Submission history From: Xiaoxuan Lei [view email]
    [v1] Thu, 20 Jun 2024 14:09:54 UTC (14,117 KB)
    [v2] Sun, 23 Jun 2024 01:51:53 UTC (14,118 KB)
    [v3] Tue, 25 Jun 2024 15:12:01 UTC (14,118 KB)
    [v4] Wed, 3 Jul 2024 21:44:23 UTC (14,118 KB)
    Xiaoxuan Lei and Lucas Gomez and Hao Yuan Bai and Pouya Bashivan

The ability to perform complex tasks from detailed instructions is a key to many remarkable achievements of our species. As humans, we are not only capable of performing a wide variety of tasks but also very complex ones that may entail hundreds or thousands of steps to complete. Large language models and their more recent multimodal counterparts that integrate textual and visual inputs have achieved unprecedented success in performing complex tasks. Yet, most existing benchmarks are largely confined to single-modality inputs (either text or vision), narrowing the scope of multimodal assessments, particularly for instruction-following in multimodal contexts. To bridge this gap, we introduce the instructed-Virtual VISual Decision Making (iWISDM) environment engineered to generate a limitless array of vision-language tasks of varying complexity. Using iWISDM, we compiled three distinct benchmarks of instruction following visual tasks across varying complexity levels and evaluated several newly developed multimodal models on these benchmarks. Our findings establish iWISDM as a robust benchmark for assessing the instructional adherence of both existing and emergent multimodal models and highlight a large gap between these models' ability to precisely follow instructions with that of humans.The code of iWISDM is available on GitHub at this https URL.

------------

`[2210.04359] Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates <https://arxiv.org/abs/2210.04359>`__ 在155年的德国议会辩论中，细致地发现了妇女和移民的团结

::

    replaced with revised version Mon, 24 Jun 2024 20:01:19 GMT
    Submission history From: Steffen Eger [view email]
    [v1] Sun, 9 Oct 2022 22:02:58 UTC (408 KB)
    [v2] Mon, 24 Jun 2024 20:01:19 UTC (1,498 KB)
    Aida Kostikova, Benjamin Paassen, Dominik Beese, Ole P\"utz, Gregor Wiedemann, Steffen Eger

Solidarity is a crucial concept to understand social relations in societies. In this paper, we explore fine-grained solidarity frames to study solidarity towards women and migrants in German parliamentary debates between 1867 and 2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k Euro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and GPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation quality. Using GPT-4, we automatically annotate more than 18k further instances (with a cost of around 500 Euro) across 155 years and find that solidarity with migrants outweighs anti-solidarity but that frequencies and solidarity types shift over time. Most importantly, group-based notions of (anti-)solidarity fade in favor of compassionate solidarity, focusing on the vulnerability of migrant groups, and exchange-based anti-solidarity, focusing on the lack of (economic) contribution. Our study highlights the interplay of historical events, socio-economic needs, and political ideologies in shaping migration discourse and social cohesion. We also show that powerful LLMs, if carefully prompted, can be cost-effective alternatives to human annotation for hard social scientific tasks.

------------

`[2305.11725] S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering <https://arxiv.org/abs/2305.11725>`__ S$^3$HQA:一种三阶段的多跳文本表混合问答方法

::

    replaced with revised version Tue, 25 Jun 2024 09:53:44 GMT
    Submission history From: Fangyu Lei [view email]
    [v1] Fri, 19 May 2023 15:01:48 UTC (7,075 KB)
    [v2] Tue, 25 Jun 2024 09:53:44 UTC (7,075 KB)
    Fangyu Lei, Xiang Li, Yifan Wei, Shizhu He, Yiming Huang, Jun Zhao, Kang Liu

Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever with refinement training to solve the noisy labeling problem. Then, a hybrid selector considers the linked relationships between heterogeneous data to select the most relevant factual knowledge. For the final stage, instead of adapting a reading comprehension module like in previous methods, we employ a generation-based reasoner to obtain answers. This includes two approaches: a row-wise generator and an LLM prompting generator~(first time used in this task). The experimental results demonstrate that our method achieves competitive results in the few-shot setting. When trained on the full dataset, our approach outperforms all baseline methods, ranking first on the HybridQA leaderboard.

------------

`[2309.00789] LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models <https://arxiv.org/abs/2309.00789>`__ LinkTransformer:用于与Transformer语言模型进行记录链接的统一包

::

    replaced with revised version Mon, 24 Jun 2024 21:01:58 GMT
    Submission history From: Melissa Dell [view email]
    [v1] Sat, 2 Sep 2023 01:45:27 UTC (119 KB)
    [v2] Mon, 24 Jun 2024 21:01:58 UTC (215 KB)
    Abhishek Arora, Melissa Dell

Linking information across sources is fundamental to a variety of analyses in social science, business, and government. While large language models (LLMs) offer enormous promise for improving record linkage in noisy datasets, in many domains approximate string matching packages in popular softwares such as R and Stata remain predominant. These packages have clean, simple interfaces and can be easily extended to a diversity of languages. Our open-source package LinkTransformer aims to extend the familiarity and ease-of-use of popular string matching methods to deep learning. It is a general purpose package for record linkage with transformer LLMs that treats record linkage as a text retrieval problem. At its core is an off-the-shelf toolkit for applying transformer models to record linkage with four lines of code. LinkTransformer contains a rich repository of pre-trained transformer semantic similarity models for multiple languages and supports easy integration of any transformer language model from Hugging Face or OpenAI. It supports standard functionality such as blocking and linking on multiple noisy fields. LinkTransformer APIs also perform other common text data processing tasks, e.g., aggregation, noisy de-duplication, and translation-free cross-lingual linkage. Importantly, LinkTransformer also contains comprehensive tools for efficient model tuning, to facilitate different levels of customization when off-the-shelf models do not provide the required accuracy. Finally, to promote reusability, reproducibility, and extensibility, LinkTransformer makes it easy for users to contribute their custom-trained models to its model hub. By combining transformer language models with intuitive APIs that will be familiar to many users of popular string matching packages, LinkTransformer aims to democratize the benefits of LLMs among those who may be less familiar with deep learning frameworks.

------------

`[2311.06062] Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration <https://arxiv.org/abs/2311.06062>`__ 基于自提示校准的针对微调大型语言模型的实用成员推理攻击

::

    replaced with revised version Tue, 25 Jun 2024 12:36:02 GMT
    Submission history From: Wenjie Fu [view email]
    [v1] Fri, 10 Nov 2023 13:55:05 UTC (324 KB)
    [v2] Tue, 12 Dec 2023 03:44:04 UTC (313 KB)
    [v3] Tue, 25 Jun 2024 12:36:02 UTC (282 KB)
    Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang

Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Prior attempts have quantified the privacy risks of language models (LMs) via MIAs, but there is still no consensus on whether existing MIA algorithms can cause remarkable privacy leakage on practical Large Language Models (LLMs). Existing MIAs designed for LMs can be classified into two categories: reference-free and reference-based attacks. They are both based on the hypothesis that training records consistently strike a higher probability of being sampled. Nevertheless, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. The reference-based attack seems to achieve promising effectiveness in LLMs, which measures a more reliable membership signal by comparing the probability discrepancy between the target model and the reference model. However, the performance of reference-based attack is highly dependent on a reference dataset that closely resembles the training dataset, which is usually inaccessible in the practical scenario. Overall, existing MIAs are unable to effectively unveil privacy leakage over practical fine-tuned LLMs that are overfitting-free and private. We propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, since memorization in LLMs is inevitable during the training process and occurs before overfitting, we introduce a more reliable membership signal, probabilistic variation, which is based on memorization rather than overfitting. Furthermore, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs.

------------

`[2402.09910] DE-COP: Detecting Copyrighted Content in Language Models Training Data <https://arxiv.org/abs/2402.09910>`__ DE-COP:语言模型训练数据中的版权内容检测

::

    replaced with revised version Tue, 25 Jun 2024 10:33:41 GMT
    Submission history From: André Duarte [view email]
    [v1] Thu, 15 Feb 2024 12:17:15 UTC (1,017 KB)
    [v2] Tue, 25 Jun 2024 10:33:41 UTC (1,014 KB)
    Andr\'e V. Duarte, Xuandong Zhao, Arlindo L. Oliveira and Lei Li

How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give approximately 4% accuracy. The code and datasets are available at this https URL.

------------

`[2402.11532] Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models <https://arxiv.org/abs/2402.11532>`__ 指令链:大型语言模型的组合式指令调优

::

    replaced with revised version Mon, 24 Jun 2024 22:43:57 GMT
    Submission history From: Shirley Anugrah Hayati [view email]
    [v1] Sun, 18 Feb 2024 10:10:40 UTC (3,092 KB)
    [v2] Mon, 24 Jun 2024 22:43:57 UTC (4,752 KB)
    Shirley Anugrah Hayati, Taehee Jung, Tristan Bodding-Long, Sudipta Kar, Abhinav Sethy, Joo-Kyung Kim, Dongyeop Kang

Fine-tuning large language models (LLMs) with a collection of large and diverse instructions has improved the model's generalization to different tasks, even for unseen tasks. However, most existing instruction datasets include only single instructions, and they struggle to follow complex instructions composed of multiple subtasks. In this work, we propose a novel concept of compositional instructions called chain-of-instructions (CoI), where the output of one instruction becomes an input for the next like a chain. Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached. CoI-tuning (i.e., fine-tuning with CoI instructions) improves the model's ability to handle instructions composed of multiple subtasks as well as unseen composite tasks such as multilingual summarization. Overall, our study find that simple CoI tuning of existing instruction data can provide consistent generalization to solve more complex, unseen, and longer chains of instructions.

------------

`[2402.15422] A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models <https://arxiv.org/abs/2402.15422>`__ 一种以数据为中心的方法，用大型语言模型生成忠实和高质量的患者摘要

::

    replaced with revised version Tue, 25 Jun 2024 17:02:10 GMT
    Submission history From: Stefan Hegselmann [view email]
    [v1] Fri, 23 Feb 2024 16:32:28 UTC (1,549 KB)
    [v2] Tue, 25 Jun 2024 17:02:10 UTC (1,595 KB)
    Stefan Hegselmann, Shannon Zejiang Shen, Florian Gierse, Monica Agrawal, David Sontag, Xiaoyi Jiang

Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we release (i) a rigorous labeling protocol for errors in medical texts and (ii) a publicly available dataset of annotated hallucinations in 100 doctor-written and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. We observe a similar effect on GPT-4 (0.70 to 0.40), when the few-shot examples are hallucination-free. We also conduct a qualitative evaluation using hallucination-free and improved training data. We find that common quantitative metrics do not correlate well with faithfulness and quality. Finally, we test GPT-4 for automatic hallucination detection, which clearly outperforms common baselines.

------------

`[2403.04666] Telecom Language Models: Must They Be Large? <https://arxiv.org/abs/2403.04666>`__ 电信语言模型:必须很大吗?

::

    replaced with revised version Tue, 25 Jun 2024 09:28:43 GMT
    Submission history From: Nicola Piovesan PhD [view email]
    [v1] Thu, 7 Mar 2024 17:13:12 UTC (199 KB)
    [v2] Tue, 25 Jun 2024 09:28:43 UTC (5,664 KB)
    Nicola Piovesan, Antonio De Domenico, Fadhel Ayed

The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2's intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2's capabilities through a Retrieval-Augmented Generation approach, meticulously integrating an extensive knowledge base specifically curated with telecom standard specifications. The enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering questions about telecom standards with a precision that closely rivals the more resource-intensive GPT-3.5. The paper further explores the refined capabilities of Phi-2 in addressing problem-solving scenarios within the telecom sector, highlighting its potential and limitations.

------------

`[2403.16512] LLMs Are Few-Shot In-Context Low-Resource Language Learners <https://arxiv.org/abs/2403.16512>`__ llm是少样本的上下文低资源语言学习者

::

    replaced with revised version Tue, 25 Jun 2024 11:54:23 GMT
    Submission history From: Samuel Cahyawijaya [view email]
    [v1] Mon, 25 Mar 2024 07:55:29 UTC (10,055 KB)
    [v2] Wed, 27 Mar 2024 06:25:10 UTC (10,057 KB)
    [v3] Sat, 18 May 2024 08:51:10 UTC (10,065 KB)
    [v4] Mon, 24 Jun 2024 12:41:52 UTC (10,067 KB)
    [v5] Tue, 25 Jun 2024 11:54:23 UTC (10,067 KB)
    Samuel Cahyawijaya, Holy Lovenia, Pascale Fung

In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-context information on enhancing the low-resource understanding quality of LLMs through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted low-resource and the high-resource language that the model is proficient in. Our work highlights the importance of advancing ICL research, particularly for low-resource languages. Our code is publicly released at this https URL

------------

`[2404.07900] High-Dimension Human Value Representation in Large Language Models <https://arxiv.org/abs/2404.07900>`__ 大型语言模型中的高维人类价值表示

::

    replaced with revised version Tue, 25 Jun 2024 12:23:00 GMT
    Submission history From: Samuel Cahyawijaya [view email]
    [v1] Thu, 11 Apr 2024 16:39:00 UTC (9,717 KB)
    [v2] Tue, 25 Jun 2024 12:23:00 UTC (38,804 KB)
    Samuel Cahyawijaya, Delong Chen, Yejin Bang, Leila Khalatbari, Bryan Wilie, Ziwei Ji, Etsuko Ishii, Pascale Fung

The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, ranging from Reinforcement Learning with Human Feedback (RLHF), to constitutional learning, etc. there is an urgent need to understand the scope and nature of human values injected into these models before their release. There is also a need for model alignment without a costly large scale human annotation effort. We propose UniVaR, a high-dimensional representation of human value distributions in LLMs, orthogonal to model architecture and training data. Trained from the value-relevant output of eight multilingual LLMs and tested on the output from four multilingual LLMs, namely LlaMA2, ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the distribution of human values embedded in different LLMs with different langauge sources. Through UniVaR, we explore how different LLMs prioritize various values in different languages and cultures, shedding light on the complex interplay between human values and language modeling.

------------

`[2404.13071] Modeling Emotions and Ethics with Large Language Models <https://arxiv.org/abs/2404.13071>`__ 基于大型语言模型的情感和伦理建模

::

    replaced with revised version Tue, 25 Jun 2024 04:36:08 GMT
    Submission history From: Edward Chang [view email]
    [v1] Mon, 15 Apr 2024 05:30:26 UTC (5,827 KB)
    [v2] Tue, 25 Jun 2024 04:36:08 UTC (6,390 KB)
    Edward Y. Chang

This paper explores the integration of human-like emotions and ethical considerations into Large Language Models (LLMs). We first model eight fundamental human emotions, presented as opposing pairs, and employ collaborative LLMs to reinterpret and express these emotions across a spectrum of intensity. Our focus extends to embedding a latent ethical dimension within LLMs, guided by a novel self-supervised learning algorithm with human feedback (SSHF). This approach enables LLMs to perform self-evaluations and adjustments concerning ethical guidelines, enhancing their capability to generate content that is not only emotionally resonant but also ethically aligned. The methodologies and case studies presented herein illustrate the potential of LLMs to transcend mere text and image generation, venturing into the realms of empathetic interaction and principled decision-making, thereby setting a new precedent in the development of emotionally aware and ethically conscious AI systems.

------------

`[2405.05506] Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias <https://arxiv.org/abs/2405.05506>`__ 交叉护理:评估预训练数据对语言模型偏差的医疗保健影响

::

    replaced with revised version Mon, 24 Jun 2024 23:17:52 GMT
    Submission history From: Shan Chen [view email]
    [v1] Thu, 9 May 2024 02:33:14 UTC (6,850 KB)
    [v2] Mon, 24 Jun 2024 23:17:52 UTC (8,430 KB)
    Shan Chen, Jack Gallifant, Mingye Gao, Pedro Moreira, Nikolaj Munch, Ajay Muthukkumar, Arvind Rajan, Jaya Kolluri, Amelia Fiske, Janna Hastings, Hugo Aerts, Brian Anthony, Leo Anthony Celi, William G. La Cava, Danielle S. Bitterman

Large language models (LLMs) are increasingly essential in processing natural languages, yet their application is frequently compromised by biases and inaccuracies originating in their training data. In this study, we introduce Cross-Care, the first benchmark framework dedicated to assessing biases and real world knowledge in LLMs, specifically focusing on the representation of disease prevalence across diverse demographic groups. We systematically evaluate how demographic biases embedded in pre-training corpora like $ThePile$ influence the outputs of LLMs. We expose and quantify discrepancies by juxtaposing these biases against actual disease prevalences in various U.S. demographic groups. Our results highlight substantial misalignment between LLM representation of disease prevalence and real disease prevalence rates across demographic subgroups, indicating a pronounced risk of bias propagation and a lack of real-world grounding for medical applications of LLMs. Furthermore, we observe that various alignment methods minimally resolve inconsistencies in the models' representation of disease prevalence across different languages. For further exploration and analysis, we make all data and a data visualization tool available at: this http URL.

------------

`[2406.00697] Comprehensive Evaluation of Large Language Models for Topic Modeling <https://arxiv.org/abs/2406.00697>`__ 面向主题建模的大型语言模型综合评估

::

    replaced with revised version Tue, 25 Jun 2024 08:42:53 GMT
    Submission history From: Tomoki Doi [view email]
    [v1] Sun, 2 Jun 2024 10:25:02 UTC (7,916 KB)
    [v2] Tue, 25 Jun 2024 08:42:53 UTC (7,922 KB)
    Tomoki Doi, Masaru Isonuma, Hitomi Yanaka

Recent work utilizes Large Language Models (LLMs) for topic modeling, generating comprehensible topic labels for given documents. However, their performance has mainly been evaluated qualitatively, and there remains room for quantitative investigation of their capabilities. In this paper, we quantitatively evaluate LLMs from multiple perspectives: the quality of topics, the impact of LLM-specific concerns, such as hallucination and shortcuts for limited documents, and LLMs' controllability of topic categories via prompts. Our findings show that LLMs can identify coherent and diverse topics with few hallucinations but may take shortcuts by focusing only on parts of documents. We also found that their controllability is limited.

------------

`[2406.06369] Annotation alignment: Comparing LLM and human annotations of conversational safety <https://arxiv.org/abs/2406.06369>`__ 注释对齐:比较LLM和会话安全性的人工注释

::

    replaced with revised version Mon, 24 Jun 2024 23:23:22 GMT
    Submission history From: Rajiv Movva [view email]
    [v1] Mon, 10 Jun 2024 15:30:13 UTC (7,947 KB)
    [v2] Fri, 21 Jun 2024 04:58:40 UTC (8,201 KB)
    [v3] Mon, 24 Jun 2024 23:23:22 UTC (8,202 KB)
    Rajiv Movva, Pang Wei Koh, Emma Pierson

To what extent do LLMs align with human perceptions of safety? We study this question via *annotation alignment*, the extent to which LLMs and humans agree when annotating the safety of user-chatbot conversations. We leverage the recent DICES dataset (Aroyo et al., 2023), in which 350 conversations are each rated for safety by 112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson correlation of $r = 0.59$ with the average annotator rating, higher than the median annotator's correlation with the average ($r=0.51$). We show that larger datasets are needed to resolve whether GPT-4 exhibits disparities in how well it correlates with demographic groups. Also, there is substantial idiosyncratic variation in correlation *within* groups, suggesting that race & gender do not fully capture differences in alignment. Finally, we find that GPT-4 cannot predict when one demographic group finds a conversation more unsafe than another.

------------

`[2406.06582] Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing <https://arxiv.org/abs/2406.06582>`__ 基于预训练大型语言模型的离散多模态transformer混合监督语音处理

::

    replaced with revised version Tue, 25 Jun 2024 17:44:00 GMT
    Submission history From: Viet Anh Trinh [view email]
    [v1] Tue, 4 Jun 2024 20:08:25 UTC (5,232 KB)
    [v2] Tue, 25 Jun 2024 17:44:00 UTC (5,232 KB)
    Viet Anh Trinh, Rosy Southwell, Yiwen Guan, Xinlu He, Zhiyong Wang, Jacob Whitehill

Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.

------------

`[2406.06589] PatentEval: Understanding Errors in Patent Generation <https://arxiv.org/abs/2406.06589>`__ PatentEval:理解专利生成中的错误

::

    replaced with revised version Tue, 25 Jun 2024 08:23:03 GMT
    Submission history From: You Zuo [view email] [via CCSD proxy]
    [v1] Wed, 5 Jun 2024 13:55:27 UTC (8,666 KB)
    [v2] Tue, 25 Jun 2024 08:23:03 UTC (8,780 KB)
    You Zuo (ALMAnaCH), Kim Gerdes (LISN), Eric Villemonte de La Clergerie (ALMAnaCH), Beno\^it Sagot (ALMAnaCH)

In this work, we introduce a comprehensive error typology specifically designed for evaluating two distinct tasks in machine-generated patent texts: claims-to-abstract generation, and the generation of the next claim given previous ones. We have also developed a benchmark, PatentEval, for systematically assessing language models in this context. Our study includes a comparative analysis, annotated by humans, of various models. These range from those specifically adapted during training for tasks within the patent domain to the latest general-purpose large language models (LLMs). Furthermore, we explored and evaluated some metrics to approximate human judgments in patent text evaluation, analyzing the extent to which these metrics align with expert assessments. These approaches provide valuable insights into the capabilities and limitations of current language models in the specialized field of patent text generation.

------------

`[2406.12036] MedCalc-Bench: Evaluating Large Language Models for Medical Calculations <https://arxiv.org/abs/2406.12036>`__ MedCalc-Bench:医学计算大型语言模型评估

::

    replaced with revised version Tue, 25 Jun 2024 13:45:49 GMT
    Submission history From: Nikhil Khandekar [view email]
    [v1] Mon, 17 Jun 2024 19:07:21 UTC (1,319 KB)
    [v2] Tue, 25 Jun 2024 13:45:49 UTC (1,320 KB)
    [v3] Thu, 27 Jun 2024 15:25:25 UTC (1,320 KB)
    [v4] Sun, 30 Jun 2024 15:12:10 UTC (1,320 KB)
    Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina S Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad W Safranek, Abid A Anwar, Andrew Zhang, Aidan Gilson, Maxwell B Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, and Zhiyong Lu

As opposed to evaluating computation and logic-based reasoning, current benchmarks for evaluating large language models (LLMs) in medicine are primarily focused on question-answering involving domain knowledge and descriptive reasoning. While such qualitative capabilities are vital to medical diagnosis, in real-world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. To this end, we propose MedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical calculation capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks. Each instance in MedCalc-Bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. While our evaluation results show the potential of LLMs in this area, none of them are effective enough for clinical settings. Common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. We hope our study highlights the quantitative knowledge and reasoning gaps in LLMs within medical settings, encouraging future improvements of LLMs for various clinical calculation tasks.

------------

`[2406.13476] LLMs Are Zero-Shot Context-Aware Simultaneous Translators <https://arxiv.org/abs/2406.13476>`__ llm是零样本的上下文感知同声传译

::

    replaced with revised version Tue, 25 Jun 2024 04:45:19 GMT
    Submission history From: Roman Koshkin [view email]
    [v1] Wed, 19 Jun 2024 11:57:42 UTC (7,065 KB)
    [v2] Fri, 21 Jun 2024 07:21:28 UTC (7,067 KB)
    [v3] Tue, 25 Jun 2024 04:45:19 UTC (7,067 KB)
    Roman Koshkin, Katsuhito Sudoh, Satoshi Nakamura

The advent of transformers has fueled progress in machine translation. More recently large language models (LLMs) have come to the spotlight thanks to their generality and strong performance in a wide range of language tasks, including translation. Here we show that open-source LLMs perform on par with or better than some state-of-the-art baselines in simultaneous machine translation (SiMT) tasks, zero-shot. We also demonstrate that injection of minimal background information, which is easy with an LLM, brings further performance gains, especially on challenging technical subject-matter. This highlights LLMs' potential for building next generation of massively multilingual, context-aware and terminologically accurate SiMT systems that require no resource-intensive training or fine-tuning.

------------

`[2406.16797] Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs <https://arxiv.org/abs/2406.16797>`__ 彩票适配:降低llm中的破坏性干扰

::

    replaced with revised version Tue, 25 Jun 2024 13:46:41 GMT
    Submission history From: Ashwinee Panda [view email]
    [v1] Mon, 24 Jun 2024 16:58:23 UTC (261 KB)
    [v2] Tue, 25 Jun 2024 13:46:41 UTC (261 KB)
    Ashwinee Panda, Berivan Isik, Xiangyu Qi, Sanmi Koyejo, Tsachy Weissman, Prateek Mittal

Existing methods for adapting large language models (LLMs) to new tasks are not suited to multi-task adaptation because they modify all the model weights -- causing destructive interference between tasks. The resulting effects, such as catastrophic forgetting of earlier tasks, make it challenging to obtain good performance on multiple tasks at the same time. To mitigate this, we propose Lottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies and optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide range of challenging tasks such as instruction following, reasoning, math, and summarization. LoTA obtains better performance than full fine-tuning and low-rank adaptation (LoRA), and maintains good performance even after training on other tasks -- thus, avoiding catastrophic forgetting. By extracting and fine-tuning over lottery tickets (or sparse task vectors), LoTA also enables model merging over highly dissimilar tasks. Our code is made publicly available at this https URL.

------------

`[2308.13049] Bayesian Exploration Networks <https://arxiv.org/abs/2308.13049>`__ 贝叶斯探索网络

::

    replaced with revised version Tue, 25 Jun 2024 13:06:13 GMT
    Submission history From: Mattie Fellows [view email]
    [v1] Thu, 24 Aug 2023 19:35:58 UTC (6,418 KB)
    [v2] Tue, 19 Sep 2023 18:36:08 UTC (6,757 KB)
    [v3] Mon, 3 Jun 2024 08:23:19 UTC (12,544 KB)
    [v4] Tue, 25 Jun 2024 13:06:13 UTC (12,541 KB)
    Mattie Fellows, Brandon Kaplowitz, Christian Schroeder de Witt and Shimon Whiteson

Bayesian reinforcement learning (RL) offers a principled and elegant approach for sequential decision making under uncertainty. Most notably, Bayesian agents do not face an exploration/exploitation dilemma, a major pathology of frequentist methods. However theoretical understanding of model-free approaches is lacking. In this paper, we introduce a novel Bayesian model-free formulation and the first analysis showing that model-free approaches can yield Bayes-optimal policies. We show all existing model-free approaches make approximations that yield policies that can be arbitrarily Bayes-suboptimal. As a first step towards model-free Bayes optimality, we introduce the Bayesian exploration network (BEN) which uses normalising flows to model both the aleatoric uncertainty (via density estimation) and epistemic uncertainty (via variational inference) in the Bellman operator. In the limit of complete optimisation, BEN learns true Bayes-optimal policies, but like in variational expectation-maximisation, partial optimisation renders our approach tractable. Empirical results demonstrate that BEN can learn true Bayes-optimal policies in tasks where existing model-free approaches fail.

------------

`[2402.11253] Aligning Large Language Models by On-Policy Self-Judgment <https://arxiv.org/abs/2402.11253>`__ 基于策略自我判断的大型语言模型对齐

::

    replaced with revised version Tue, 25 Jun 2024 13:39:52 GMT
    Submission history From: Sangkyu Lee [view email]
    [v1] Sat, 17 Feb 2024 11:25:26 UTC (371 KB)
    [v2] Sun, 3 Mar 2024 21:37:16 UTC (370 KB)
    [v3] Tue, 25 Jun 2024 13:39:52 UTC (372 KB)
    Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min Yoo, Youngjae Yu

Existing approaches for aligning large language models with human preferences face a trade-off that requires a separate reward model (RM) for on-policy learning. In this paper, we present a novel alignment framework, SELF-JUDGE that (1) does on-policy learning and 2) is parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model to act as both a policy and a judge. Specifically, we view the pairwise judgment task, choosing the better response from a response pair, as a special case of the instruction-following task. The resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines in preference benchmarks. We also show that the rejecting sampling by itself can improve performance further without an additional evaluator.

------------

`[2402.13414] Harnessing Large Language Models as Post-hoc Correctors <https://arxiv.org/abs/2402.13414>`__ 利用大型语言模型作为事后校正器

::

    replaced with revised version Tue, 25 Jun 2024 08:26:19 GMT
    Submission history From: Zhiqiang Zhong [view email]
    [v1] Tue, 20 Feb 2024 22:50:41 UTC (866 KB)
    [v2] Tue, 25 Jun 2024 08:26:19 UTC (1,264 KB)
    Zhiqiang Zhong and Kuangyu Zhou and Davide Mottin

As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can transfer its acquired knowledge to suggest corrections for the ML model's predictions. Our experimental results on text analysis and the challenging molecular predictions show that \model improves the performance of a number of models by up to 39%.

------------

`[2404.18239] SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning <https://arxiv.org/abs/2404.18239>`__ 灵魂:解锁LLM忘却的二阶优化力量

::

    replaced with revised version Mon, 24 Jun 2024 20:24:53 GMT
    Submission history From: Jinghan Jia [view email]
    [v1] Sun, 28 Apr 2024 16:31:32 UTC (332 KB)
    [v2] Fri, 31 May 2024 17:38:51 UTC (331 KB)
    [v3] Mon, 3 Jun 2024 01:10:53 UTC (332 KB)
    [v4] Mon, 24 Jun 2024 20:24:53 UTC (354 KB)
    Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, Sijia Liu

Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning. Codes are available at this https URL.

------------

`[2405.15756] Sparse Expansion and Neuronal Disentanglement <https://arxiv.org/abs/2405.15756>`__ 稀疏膨胀和神经元解缠

::

    replaced with revised version Mon, 24 Jun 2024 22:14:42 GMT
    Submission history From: Shashata Sawmya [view email]
    [v1] Fri, 24 May 2024 17:51:39 UTC (5,263 KB)
    [v2] Mon, 24 Jun 2024 22:14:42 UTC (5,263 KB)
    Shashata Sawmya, Linghao Kong, Ilia Markov, Dan Alistarh, Nir Shavit

We show how to improve the inference efficiency of an LLM by expanding it into a mixture of sparse experts, where each expert is a copy of the original weights, one-shot pruned for a specific cluster of input values. We call this approach $\textit{Sparse Expansion}$. We show that, for models such as Llama 2 70B, as we increase the number of sparse experts, Sparse Expansion outperforms all other one-shot sparsification approaches for the same inference FLOP budget per token, and that this gap grows as sparsity increases, leading to inference speedups.
But why? To answer this, we provide strong evidence that the mixture of sparse experts is effectively $\textit{disentangling}$ the input-output relationship of every individual neuron across clusters of inputs. Specifically, sparse experts approximate the dense neuron output distribution with fewer weights by decomposing the distribution into a collection of simpler ones, each with a separate sparse dot product covering it. Interestingly, we show that the Wasserstein distance between a neuron's output distribution and a Gaussian distribution is an indicator of its entanglement level and contribution to the accuracy of the model. Every layer of an LLM has a fraction of highly entangled Wasserstein neurons, and model performance suffers more when these are sparsified as opposed to others. The code for Sparse Expansion is available at: this https URL .

------------

`[2405.18641] Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning <https://arxiv.org/abs/2405.18641>`__ 针对有害微调的大型语言模型的惰性安全对齐

::

    replaced with revised version Mon, 24 Jun 2024 18:59:50 GMT
    Submission history From: Tiansheng Huang [view email]
    [v1] Tue, 28 May 2024 22:53:43 UTC (1,592 KB)
    [v2] Thu, 30 May 2024 20:03:37 UTC (1,592 KB)
    [v3] Mon, 24 Jun 2024 18:59:50 UTC (1,592 KB)
    [v4] Wed, 26 Jun 2024 18:54:59 UTC (1,592 KB)
    Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu

Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data. First time in the literature, we show that the jail-broken effect can be mitigated by separating states in the finetuning stage to optimize the alignment and user datasets. Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance. By statistical analysis, we show that the \textit{excess drift} towards consensus could be a probable reason for the instability. To remedy this issue, we propose \textbf{L}azy(\textbf{i}) \textbf{s}afety \textbf{a}lignment (\textbf{Lisa}), which introduces a proximal term to constraint the drift of each state. Theoretically, the benefit of the proximal term is supported by the convergence analysis, wherein we show that a sufficient large proximal factor is necessary to guarantee Lisa's convergence. Empirically, our results on four downstream finetuning tasks show that Lisa with a proximal term can significantly increase alignment performance while maintaining the LLM's accuracy on the user tasks. Code is available at \url{this https URL}.

------------

`[2406.02500] Demystifying the Compression of Mixture-of-Experts Through a Unified Framework <https://arxiv.org/abs/2406.02500>`__ 通过统一的框架揭开了专家混合压缩的神秘面纱

::

    replaced with revised version Mon, 24 Jun 2024 21:51:23 GMT
    Submission history From: Shwai He [view email]
    [v1] Tue, 4 Jun 2024 17:18:40 UTC (1,121 KB)
    [v2] Mon, 24 Jun 2024 21:51:23 UTC (1,122 KB)
    Shwai He, Daize Dong, Liang Ding, Ang Li

Scaling large language models has revolutionized the performance across diverse domains, yet the continual growth in model size poses significant challenges for real-world deployment. The Mixture of Experts (MoE) approach addresses this by dynamically selecting and activating only a subset of experts, significantly reducing computational costs while maintaining high performance. However, MoE introduces potential redundancy (e.g., parameters) and extra costs (e.g., communication overhead). Despite numerous compression techniques developed for mitigating the redundancy in dense models, the compression of MoE remains under-explored. We first bridge this gap with a cutting-edge unified framework that not only seamlessly integrates mainstream compression methods but also helps systematically understand MoE compression. This framework approaches compression from two perspectives: Expert Slimming which compresses individual experts and Expert Trimming which removes structured modules. Within this framework, we explore the optimization space unexplored by existing methods,and further introduce aggressive Expert Trimming techniques, i.e., Layer Drop and Block Drop, to eliminate redundancy at larger scales. Based on these insights,we present a comprehensive recipe to guide practitioners in compressing MoE effectively. Extensive experimental results demonstrate the effectiveness of the compression methods under our framework and the proposed recipe, achieving a 6.05x speedup and only 20.0GB memory usage while maintaining over 92% of performance on Mixtral-8x7B. Code is released at \url{this https URL}.

------------

`[2406.10918] Embodied Question Answering via Multi-LLM Systems <https://arxiv.org/abs/2406.10918>`__ 基于多llm系统的具身问答

::

    replaced with revised version Tue, 25 Jun 2024 10:50:09 GMT
    Submission history From: Bhrij Patel [view email]
    [v1] Sun, 16 Jun 2024 12:46:40 UTC (4,384 KB)
    [v2] Tue, 18 Jun 2024 01:18:46 UTC (4,384 KB)
    [v3] Tue, 25 Jun 2024 10:50:09 UTC (4,863 KB)
    Bhrij Patel, Vishnu Sashank Dorbala, Dinesh Manocha, Amrit Singh Bedi

Embodied Question Answering (EQA) is an important problem, which involves an agent exploring the environment to answer user queries. In the existing literature, EQA has exclusively been studied in single-agent scenarios, where exploration can be time-consuming and costly. In this work, we consider EQA in a multi-agent framework involving multiple large language models (LLM) based agents independently answering queries about a household environment. To generate one answer for each query, we use the individual responses to train a Central Answer Model (CAM) that aggregates responses for a robust answer. Using CAM, we observe a $50\%$ higher EQA accuracy when compared against aggregation methods for ensemble LLM, such as voting schemes and debates. CAM does not require any form of agent communication, alleviating it from the associated costs. We ablate CAM with various nonlinear (neural network, random forest, decision tree, XGBoost) and linear (logistic regression classifier, SVM) algorithms. Finally, we present a feature importance analysis for CAM via permutation feature importance (PFI), quantifying CAMs reliance on each independent agent and query context.

------------

`[2406.16252] Graph-Augmented LLMs for Personalized Health Insights: A Case Study in Sleep Analysis <https://arxiv.org/abs/2406.16252>`__ 面向个性化健康见解的图增强llm:睡眠分析案例研究

::

    replaced with revised version Tue, 25 Jun 2024 03:17:40 GMT
    Submission history From: Ajan Subramanian [view email]
    [v1] Mon, 24 Jun 2024 01:22:54 UTC (605 KB)
    [v2] Tue, 25 Jun 2024 03:17:40 UTC (606 KB)
    Ajan Subramanian, Zhongqi Yang, Iman Azimi and Amir M. Rahmani

Health monitoring systems have revolutionized modern healthcare by enabling the continuous capture of physiological and behavioral data, essential for preventive measures and early health intervention. While integrating this data with Large Language Models (LLMs) has shown promise in delivering interactive health advice, traditional methods like Retrieval-Augmented Generation (RAG) and fine-tuning often fail to fully utilize the complex, multi-dimensional, and temporally relevant data from wearable devices. These conventional approaches typically provide limited actionable and personalized health insights due to their inadequate capacity to dynamically integrate and interpret diverse health data streams. In response, this paper introduces a graph-augmented LLM framework designed to significantly enhance the personalization and clarity of health insights. Utilizing a hierarchical graph structure, the framework captures inter and intra-patient relationships, enriching LLM prompts with dynamic feature importance scores derived from a Random Forest Model. The effectiveness of this approach is demonstrated through a sleep analysis case study involving 20 college students during the COVID-19 lockdown, highlighting the potential of our model to generate actionable and personalized health insights efficiently. We leverage another LLM to evaluate the insights for relevance, comprehensiveness, actionability, and personalization, addressing the critical need for models that process and interpret complex health data effectively. Our findings show that augmenting prompts with our framework yields significant improvements in all 4 criteria. Through our framework, we can elicit well-crafted, more thoughtful responses tailored to a specific patient.

------------

`[2311.13721] Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning <https://arxiv.org/abs/2311.13721>`__ Nova:基于分层注意力和对比学习的汇编代码生成语言模型

::

    replaced with revised version Mon, 24 Jun 2024 18:18:59 GMT
    Submission history From: Nan Jiang [view email]
    [v1] Wed, 22 Nov 2023 22:27:54 UTC (29,457 KB)
    [v2] Mon, 27 Nov 2023 18:22:55 UTC (29,457 KB)
    [v3] Tue, 18 Jun 2024 02:48:16 UTC (7,039 KB)
    [v4] Mon, 24 Jun 2024 18:18:59 UTC (7,034 KB)
    Nan Jiang, Chengxiao Wang, Kevin Liu, Xiangzhe Xu, Lin Tan, Xiangyu Zhang

Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code due to the unique challenges of assembly: (1) the low information density of assembly and (2) the diverse optimizations in assembly code. To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively, and designs contrastive learning objectives to train LLMs to learn assembly optimization. Equipped with these techniques, this work develops Nova, a generative LLM for assembly code. Nova outperforms existing techniques on binary code decompilation by up to 146.54%, and outperforms the latest binary code similarity detection techniques by up to 6.17%, showing promising abilities on both assembly generation and understanding tasks.

------------

`[2402.15368] Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction <https://arxiv.org/abs/2402.15368>`__ 基于保形预测的语言指导多机器人系统安全任务规划

::

    replaced with revised version Mon, 24 Jun 2024 18:27:35 GMT
    Submission history From: Yiannis Kantaros [view email]
    [v1] Fri, 23 Feb 2024 15:02:44 UTC (25,850 KB)
    [v2] Mon, 24 Jun 2024 18:27:35 UTC (25,370 KB)
    Jun Wang, Guocheng He, Yiannis Kantaros

This paper addresses task planning problems for language-instructed robot teams. Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities at various locations and semantic objects. Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans. However, these approaches lack mission completion guarantees. To address this challenge, we introduce a new decentralized LLM-based planner, called S-ATLAS for Safe plAnning for Teams of Language-instructed AgentS, that is capable of achieving user-defined mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models. CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a decentralized fashion, enabling robots to make individual decisions when they are sufficiently certain and seek help otherwise. We show, both theoretically and empirically, that the proposed planner can achieve user-specified task success rates while minimizing the overall number of help requests. We provide comparative experiments against related works showing that our method is significantly more computational efficient and achieves lower help rates. The advantage of our algorithm over baselines becomes more pronounced with increasing robot team size.

------------

`[2405.05905] Truthful Aggregation of LLMs with an Application to Online Advertising <https://arxiv.org/abs/2405.05905>`__ llm与在线广告应用的真实聚合

::

    replaced with revised version Tue, 25 Jun 2024 16:31:31 GMT
    Submission history From: Ermis Soumalias Mr. [view email]
    [v1] Thu, 9 May 2024 17:01:31 UTC (806 KB)
    [v2] Tue, 25 Jun 2024 16:31:31 UTC (1,625 KB)
    [v3] Wed, 26 Jun 2024 06:29:32 UTC (1,625 KB)
    Ermis Soumalias, Michael J. Curry, Sven Seuken

Online platforms generate hundreds of billions of dollars in revenue per year by showing advertisements alongside their own content. Currently, these platforms are integrating Large Language Models (LLMs) into their services. This makes revenue generation from LLM-generated content the next major challenge in online advertising. We consider a scenario where advertisers aim to influence the responses of an LLM to align with their interests, while platforms seek to maximize advertiser value and ensure user satisfaction. We introduce an auction mechanism for this problem that operates without LLM fine-tuning or access to model weights and provably converges to the output of the optimally fine-tuned LLM for the platform's objective as computational resources increase. Our mechanism ensures that truthful reporting is a dominant strategy for advertisers and it aligns each advertiser's utility with their contribution to social welfare - an essential feature for long-term viability. Additionally, it can incorporate contextual information about the advertisers, significantly accelerating convergence. Via experiments with a publicly available LLM, we show that our mechanism significantly boosts advertiser value and platform revenue, with low computational overhead. While our motivating application is online advertising, our mechanism can be applied in any setting with monetary transfers, making it a general-purpose solution for truthfully aggregating the preferences of self-interested agents over LLM-generated replies.

------------

`[2405.17441] When Large Language Models Meet Optical Networks: Paving the Way for Automation <https://arxiv.org/abs/2405.17441>`__ 当大型语言模型遇到光学网络:为自动化铺平道路

::

    replaced with revised version Tue, 25 Jun 2024 03:23:00 GMT
    Submission history From: Danshi Wang [view email]
    [v1] Tue, 14 May 2024 10:46:33 UTC (1,582 KB)
    [v2] Tue, 25 Jun 2024 03:23:00 UTC (1,714 KB)
    Danshi Wang, Yidi Wang, Xiaotian Jiang, Yao Zhang, Yue Pang, and Min Zhang

Since the advent of GPT, large language models (LLMs) have brought about revolutionary advancements in all walks of life. As a superior natural language processing (NLP) technology, LLMs have consistently achieved state-of-the-art performance on numerous areas. However, LLMs are considered to be general-purpose models for NLP tasks, which may encounter challenges when applied to complex tasks in specialized fields such as optical networks. In this study, we propose a framework of LLM-empowered optical networks, facilitating intelligent control of the physical layer and efficient interaction with the application layer through an LLM-driven agent (AI-Agent) deployed in the control layer. The AI-Agent can leverage external tools and extract domain knowledge from a comprehensive resource library specifically established for optical networks. This is achieved through user input and well-crafted prompts, enabling the generation of control instructions and result representations for autonomous operation and maintenance in optical networks. To improve LLM's capability in professional fields and stimulate its potential on complex tasks, the details of performing prompt engineering, establishing domain knowledge library, and implementing complex tasks are illustrated in this study. Moreover, the proposed framework is verified on two typical tasks: network alarm analysis and network performance optimization. The good response accuracies and sematic similarities of 2,400 test situations exhibit the great potential of LLM in optical networks.

------------

`[2406.16224] From Text to Test: AI-Generated Control Software for Materials Science Instruments <https://arxiv.org/abs/2406.16224>`__ 从文本到测试:材料科学仪器ai生成控制软件

::

    replaced with revised version Tue, 25 Jun 2024 11:34:15 GMT
    Submission history From: Davi Febba [view email]
    [v1] Sun, 23 Jun 2024 21:32:57 UTC (2,019 KB)
    [v2] Tue, 25 Jun 2024 11:34:15 UTC (2,019 KB)
    Davi M F\'ebba, Kingsley Egbo, William A. Callahan, Andriy Zakutayev

Large language models (LLMs) are transforming the landscape of chemistry and materials science. Recent examples of LLM-accelerated experimental research include virtual assistants for parsing synthesis recipes from the literature, or using the extracted knowledge to guide synthesis and characterization. Despite these advancements, their application is constrained to labs with automated instruments and control software, leaving much of materials science reliant on manual processes. Here, we demonstrate the rapid deployment of a Python-based control module for a Keithley 2400 electrical source measure unit using ChatGPT-4. Through iterative refinement, we achieved effective instrument management with minimal human intervention. Additionally, a user-friendly graphical user interface (GUI) was created, effectively linking all instrument controls to interactive screen elements. Finally, we integrated this AI-crafted instrument control software with a high-performance stochastic optimization algorithm to facilitate rapid and automated extraction of electronic device parameters related to semiconductor charge transport mechanisms from current-voltage (IV) measurement data. This integration resulted in a comprehensive open-source toolkit for semiconductor device characterization and analysis using IV curve measurements. We demonstrate the application of these tools by acquiring, analyzing, and parameterizing IV data from a Pt/Cr$_2$O$_3$:Mg/$\beta$-Ga$_2$O$_3$ heterojunction diode, a novel stack for high-power and high-temperature electronic devices. This approach underscores the powerful synergy between LLMs and the development of instruments for scientific inquiry, showcasing a path for further acceleration in materials science.

------------

`[2310.07710] A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models <https://arxiv.org/abs/2310.07710>`__ 面向大型语言模型的弹性可访问分布保持水印

::

    replaced with revised version Tue, 25 Jun 2024 07:08:17 GMT
    Submission history From: Yihan Wu [view email]
    [v1] Wed, 11 Oct 2023 17:57:35 UTC (622 KB)
    [v2] Tue, 25 Jun 2024 07:08:17 UTC (865 KB)
    Yihan Wu, Zhengmian Hu, Junfeng Guo, Hongyang Zhang, Heng Huang

Watermarking techniques offer a promising way to identify machine-generated content via embedding covert information into the contents generated from language models. A challenge in the domain lies in preserving the distribution of original generated content after watermarking. Our research extends and improves upon existing watermarking framework, placing emphasis on the importance of a \textbf{Di}stribution-\textbf{P}reserving (DiP) watermark. Contrary to the current strategies, our proposed DiPmark simultaneously preserves the original token distribution during watermarking (distribution-preserving), is detectable without access to the language model API and prompts (accessible), and is provably robust to moderate changes of tokens (resilient). DiPmark operates by selecting a random set of tokens prior to the generation of a word, then modifying the token distribution through a distribution-preserving reweight function to enhance the probability of these selected tokens during the sampling process. Extensive empirical evaluation on various language models and tasks demonstrates our approach's distribution-preserving property, accessibility, and resilience, making it a effective solution for watermarking tasks that demand impeccable quality preservation.

------------

`[2406.13275] Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding <https://arxiv.org/abs/2406.13275>`__ 通过优化音频编码的大型语言模型增强自动音频描述

::

    replaced with revised version Tue, 25 Jun 2024 08:07:36 GMT
    Submission history From: Gang Li [view email]
    [v1] Wed, 19 Jun 2024 07:09:46 UTC (144 KB)
    [v2] Tue, 25 Jun 2024 08:07:36 UTC (144 KB)
    Jizhong Liu, Gang Li, Junbo Zhang, Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Yujun Wang, Bin Wang

Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.

------------

----------
Index (90)
----------

`[2406.17224] Large Language Models are Interpretable Learners <https://arxiv.org/abs/2406.17224>`__ 大型语言模型是可解释的学习者

`[2406.17532] Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study <https://arxiv.org/abs/2406.17532>`__ 大型语言模型能理解DL-Lite本体吗?实证研究

`[2406.16899] Prompt-based vs. Fine-tuned LLMs Toward Causal Graph Verification <https://arxiv.org/abs/2406.16899>`__ 面向因果图验证的提示型和微调型llm

`[2406.17055] Large Language Models Assume People are More Rational than We Really are <https://arxiv.org/abs/2406.17055>`__ 大型语言模型假设人们比我们实际更理性

`[2406.17095] Attention Instruction: Amplifying Attention in the Middle via Prompting <https://arxiv.org/abs/2406.17095>`__

`[2406.17104] Automated Adversarial Discovery for Safety Classifiers <https://arxiv.org/abs/2406.17104>`__ 安全分类器的自动对抗发现

`[2406.17163] Paraphrase and Aggregate with Large Language Models for Minimizing Intent Classification Errors <https://arxiv.org/abs/2406.17163>`__ 用大型语言模型进行复述和聚合以最小化意图分类错误

`[2406.17231] CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph <https://arxiv.org/abs/2406.17231>`__ CogMG:大型语言模型与知识图谱的协同增强

`[2406.17253] How Well Can Knowledge Edit Methods Edit Perplexing Knowledge? <https://arxiv.org/abs/2406.17253>`__ 知识编辑方法如何编辑令人困惑的知识?

`[2406.17255] MPCODER: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning <https://arxiv.org/abs/2406.17255>`__

`[2406.17260] Mitigating Hallucination in Fictional Character Role-Play <https://arxiv.org/abs/2406.17260>`__ 减轻虚构角色角色扮演中的幻觉

`[2406.17261] TRAWL: Tensor Reduced and Approximated Weights for Large Language Models <https://arxiv.org/abs/2406.17261>`__ 拖网:大型语言模型的张量约简和近似权重

`[2406.17262] D2LLM: Decomposed and Distilled Large Language Models for Semantic Search <https://arxiv.org/abs/2406.17262>`__ D2LLM:面向语义搜索的大型语言模型分解和提炼

`[2406.17274] Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization? <https://arxiv.org/abs/2406.17274>`__ 文本摘要中不确定性估计方法的性能评估是否可信?

`[2406.17287] Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models <https://arxiv.org/abs/2406.17287>`__ 用大型语言模型预测汉语心理咨询对话中的五大人格特征

`[2406.17324] Delving into the Utilisation of ChatGPT in Scientific Publications in Astronomy <https://arxiv.org/abs/2406.17324>`__ 研究ChatGPT在天文学科学出版物中的应用

`[2406.17328] Dual-Space Knowledge Distillation for Large Language Models <https://arxiv.org/abs/2406.17328>`__ 面向大型语言模型的双空间知识蒸馏

`[2406.17377] A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual LLMs <https://arxiv.org/abs/2406.17377>`__ 基于多语言llm的跨语言自适应三管齐下方法

`[2406.17378] A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens <https://arxiv.org/abs/2406.17378>`__ 一个文本值几个标记:来自llm的文本嵌入秘密地与关键标记对齐

`[2406.17385] Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance <https://arxiv.org/abs/2406.17385>`__ 母语设计偏差:研究英语母语性对语言模型性能的影响

`[2406.17404] Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training <https://arxiv.org/abs/2406.17404>`__ 制造噪声:通过噪声训练解锁语言模型并行推理能力

`[2406.17415] Variable Layer-Wise Quantization: A Simple and Effective Approach to Quantize LLMs <https://arxiv.org/abs/2406.17415>`__ 可变分层量化:一种简单有效的llm量化方法

`[2406.17453] Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain <https://arxiv.org/abs/2406.17453>`__ 学习提出信息性问题:用偏好优化和期望信息增益增强llm

`[2406.17484] MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation <https://arxiv.org/abs/2406.17484>`__ 医疗保健:通过解耦临床对齐和知识聚合推进医学LLMs

`[2406.17526] LumberChunker: Long-Form Narrative Document Segmentation <https://arxiv.org/abs/2406.17526>`__ LumberChunker:长记叙文文档分割

`[2406.17557] The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale <https://arxiv.org/abs/2406.17557>`__ FineWeb数据集:从网络中提取最精细的大规模文本数据

`[2406.17563] Multi-property Steering of Large Language Models with Dynamic Activation Composition <https://arxiv.org/abs/2406.17563>`__ 基于动态激活组合的大型语言模型多属性转向

`[2406.17574] Beyond Text-to-SQL for IoT Defense: A Comprehensive Framework for Querying and Classifying IoT Threats <https://arxiv.org/abs/2406.17574>`__ Beyond Text-to-SQL用于物联网防御:查询和分类物联网威胁的综合框架

`[2406.17588] LongIns: A Challenging Long-context Instruction-based Exam for LLMs <https://arxiv.org/abs/2406.17588>`__ LongIns:一种具有挑战性的基于长上下文指令的LLMs考试

`[2406.17600] "Seeing the Big through the Small": Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations? <https://arxiv.org/abs/2406.17600>`__ “以小见大”:llm能从一些解释中近似人类对NLI的判断分布吗?

`[2406.17624] Self-assessment, Exhibition, and Recognition: a Review of Personality in Large Language Models <https://arxiv.org/abs/2406.17624>`__ 自我评估、展示和识别:大型语言模型中的个性综述

`[2406.17626] CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference <https://arxiv.org/abs/2406.17626>`__

`[2406.17633] Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels <https://arxiv.org/abs/2406.17633>`__ 自动标注中的知识蒸馏:基于llm生成训练标签的监督文本分类

`[2406.17642] Banishing LLM Hallucinations Requires Rethinking Generalization <https://arxiv.org/abs/2406.17642>`__ 消除LLM幻觉需要重新思考泛化

`[2406.17692] From Distributional to Overton Pluralism: Investigating Large Language Model Alignment <https://arxiv.org/abs/2406.17692>`__ 从分布式到欧弗顿多元化:大型语言模型对齐研究

`[2406.17737] LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users <https://arxiv.org/abs/2406.17737>`__ 针对性能低下的LLM不成比例地影响易受攻击的用户

`[2406.17761] CaLMQA: Exploring culturally specific long-form question answering across 23 languages <https://arxiv.org/abs/2406.17761>`__ calqa:跨23种语言探索特定文化的长篇问答

`[2406.16963] Large Language Models for Link Stealing Attacks Against Graph Neural Networks <https://arxiv.org/abs/2406.16963>`__ 面向图神经网络链接窃取攻击的大型语言模型

`[2406.16964] Are Language Models Actually Useful for Time Series Forecasting? <https://arxiv.org/abs/2406.16964>`__ 语言模型对时间序列预测真的有用吗?

`[2406.16985] Unveiling LLM Mechanisms Through Neural ODEs and Control Theory <https://arxiv.org/abs/2406.16985>`__ 用神经常微分方程和控制理论揭示LLM机制

`[2406.17216] Machine Unlearning Fails to Remove Data Poisoning Attacks <https://arxiv.org/abs/2406.17216>`__ 机器遗忘无法消除数据中毒攻击

`[2406.17272] A Comprehensive Solution to Connect Speech Encoder and Large Language Model for ASR <https://arxiv.org/abs/2406.17272>`__

`[2406.17467] Early learning of the optimal constant solution in neural networks and humans <https://arxiv.org/abs/2406.17467>`__ 神经网络和人类中最优常数解的早期学习

`[2406.17542] CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained Models using Greedy Coordinate Descent <https://arxiv.org/abs/2406.17542>`__ CDQuant:使用贪婪坐标下降对大型预训练模型进行精确的训练后权重量化

`[2406.17706] FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model <https://arxiv.org/abs/2406.17706>`__ FedBiOT:无全模型联邦学习中的LLM局部微调

`[2406.16990] AND: Audio Network Dissection for Interpreting Deep Acoustic <https://arxiv.org/abs/2406.16990>`__

`[2406.16995] A large language model for predicting T cell receptor-antigen binding specificity <https://arxiv.org/abs/2406.16995>`__ 预测T细胞受体-抗原结合特异性的大型语言模型

`[2406.17092] BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models <https://arxiv.org/abs/2406.17092>`__ bear:基于嵌入的指令调优语言模型安全后门对抗移除

`[2406.17531] Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness <https://arxiv.org/abs/2406.17531>`__ 以细微差别增强基于llm的人-机器人交互以增强多样性感知

`[2406.17233] Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement <https://arxiv.org/abs/2406.17233>`__ 增强细粒度对齐的自构造上下文反编译

`[2406.17126] MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs <https://arxiv.org/abs/2406.17126>`__ MM-SpuBench:更好地理解多模态llm中的虚假偏差

`[2406.17228] Greedy equivalence search for nonparametric graphical models <https://arxiv.org/abs/2406.17228>`__ 非参数图模型的贪婪等价搜索

`[2406.17295] MatText: Do Language Models Need More than Text & Scale for Materials Modeling? <https://arxiv.org/abs/2406.17295>`__

`[2309.05519] NExT-GPT: Any-to-Any Multimodal LLM <https://arxiv.org/abs/2309.05519>`__ NExT-GPT:任意对任意多模态LLM

`[2406.14343] iWISDM: Assessing instruction following in multimodal models at scale <https://arxiv.org/abs/2406.14343>`__ iWISDM:评估大规模多模态模型的指令跟随

`[2210.04359] Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates <https://arxiv.org/abs/2210.04359>`__ 在155年的德国议会辩论中，细致地发现了妇女和移民的团结

`[2305.11725] S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering <https://arxiv.org/abs/2305.11725>`__ S$^3$HQA:一种三阶段的多跳文本表混合问答方法

`[2309.00789] LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models <https://arxiv.org/abs/2309.00789>`__ LinkTransformer:用于与Transformer语言模型进行记录链接的统一包

`[2311.06062] Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration <https://arxiv.org/abs/2311.06062>`__ 基于自提示校准的针对微调大型语言模型的实用成员推理攻击

`[2402.09910] DE-COP: Detecting Copyrighted Content in Language Models Training Data <https://arxiv.org/abs/2402.09910>`__ DE-COP:语言模型训练数据中的版权内容检测

`[2402.11532] Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models <https://arxiv.org/abs/2402.11532>`__ 指令链:大型语言模型的组合式指令调优

`[2402.15422] A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models <https://arxiv.org/abs/2402.15422>`__ 一种以数据为中心的方法，用大型语言模型生成忠实和高质量的患者摘要

`[2403.04666] Telecom Language Models: Must They Be Large? <https://arxiv.org/abs/2403.04666>`__ 电信语言模型:必须很大吗?

`[2403.16512] LLMs Are Few-Shot In-Context Low-Resource Language Learners <https://arxiv.org/abs/2403.16512>`__ llm是少样本的上下文低资源语言学习者

`[2404.07900] High-Dimension Human Value Representation in Large Language Models <https://arxiv.org/abs/2404.07900>`__ 大型语言模型中的高维人类价值表示

`[2404.13071] Modeling Emotions and Ethics with Large Language Models <https://arxiv.org/abs/2404.13071>`__ 基于大型语言模型的情感和伦理建模

`[2405.05506] Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias <https://arxiv.org/abs/2405.05506>`__ 交叉护理:评估预训练数据对语言模型偏差的医疗保健影响

`[2406.00697] Comprehensive Evaluation of Large Language Models for Topic Modeling <https://arxiv.org/abs/2406.00697>`__ 面向主题建模的大型语言模型综合评估

`[2406.06369] Annotation alignment: Comparing LLM and human annotations of conversational safety <https://arxiv.org/abs/2406.06369>`__ 注释对齐:比较LLM和会话安全性的人工注释

`[2406.06582] Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing <https://arxiv.org/abs/2406.06582>`__ 基于预训练大型语言模型的离散多模态transformer混合监督语音处理

`[2406.06589] PatentEval: Understanding Errors in Patent Generation <https://arxiv.org/abs/2406.06589>`__ PatentEval:理解专利生成中的错误

`[2406.12036] MedCalc-Bench: Evaluating Large Language Models for Medical Calculations <https://arxiv.org/abs/2406.12036>`__ MedCalc-Bench:医学计算大型语言模型评估

`[2406.13476] LLMs Are Zero-Shot Context-Aware Simultaneous Translators <https://arxiv.org/abs/2406.13476>`__ llm是零样本的上下文感知同声传译

`[2406.16797] Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs <https://arxiv.org/abs/2406.16797>`__ 彩票适配:降低llm中的破坏性干扰

`[2308.13049] Bayesian Exploration Networks <https://arxiv.org/abs/2308.13049>`__ 贝叶斯探索网络

`[2402.11253] Aligning Large Language Models by On-Policy Self-Judgment <https://arxiv.org/abs/2402.11253>`__ 基于策略自我判断的大型语言模型对齐

`[2402.13414] Harnessing Large Language Models as Post-hoc Correctors <https://arxiv.org/abs/2402.13414>`__ 利用大型语言模型作为事后校正器

`[2404.18239] SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning <https://arxiv.org/abs/2404.18239>`__ 灵魂:解锁LLM忘却的二阶优化力量

`[2405.15756] Sparse Expansion and Neuronal Disentanglement <https://arxiv.org/abs/2405.15756>`__ 稀疏膨胀和神经元解缠

`[2405.18641] Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning <https://arxiv.org/abs/2405.18641>`__ 针对有害微调的大型语言模型的惰性安全对齐

`[2406.02500] Demystifying the Compression of Mixture-of-Experts Through a Unified Framework <https://arxiv.org/abs/2406.02500>`__ 通过统一的框架揭开了专家混合压缩的神秘面纱

`[2406.10918] Embodied Question Answering via Multi-LLM Systems <https://arxiv.org/abs/2406.10918>`__ 基于多llm系统的具身问答

`[2406.16252] Graph-Augmented LLMs for Personalized Health Insights: A Case Study in Sleep Analysis <https://arxiv.org/abs/2406.16252>`__ 面向个性化健康见解的图增强llm:睡眠分析案例研究

`[2311.13721] Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning <https://arxiv.org/abs/2311.13721>`__ Nova:基于分层注意力和对比学习的汇编代码生成语言模型

`[2402.15368] Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction <https://arxiv.org/abs/2402.15368>`__ 基于保形预测的语言指导多机器人系统安全任务规划

`[2405.05905] Truthful Aggregation of LLMs with an Application to Online Advertising <https://arxiv.org/abs/2405.05905>`__ llm与在线广告应用的真实聚合

`[2405.17441] When Large Language Models Meet Optical Networks: Paving the Way for Automation <https://arxiv.org/abs/2405.17441>`__ 当大型语言模型遇到光学网络:为自动化铺平道路

`[2406.16224] From Text to Test: AI-Generated Control Software for Materials Science Instruments <https://arxiv.org/abs/2406.16224>`__ 从文本到测试:材料科学仪器ai生成控制软件

`[2310.07710] A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models <https://arxiv.org/abs/2310.07710>`__ 面向大型语言模型的弹性可访问分布保持水印

`[2406.13275] Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding <https://arxiv.org/abs/2406.13275>`__ 通过优化音频编码的大型语言模型增强自动音频描述

