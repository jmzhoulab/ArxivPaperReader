240624
========

----------
Survey (2)
----------

`[2406.14644] Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation <https://arxiv.org/abs/2406.14644>`__ 揭示语言模型中的数据污染谱:从检测到修复的综述

::

    Thu, 20 Jun 2024 18:07:26 GMT
    Chunyuan Deng, Yilun Zhao, Yuzhao Heng, Yitong Li, Jiannan Cao, Xiangru Tang, Arman Cohan

Data contamination has garnered increased attention in the era of large language models (LLMs) due to the reliance on extensive internet-derived training corpora. The issue of training corpus overlap with evaluation benchmarks--referred to as contamination--has been the focus of significant recent research. This body of work aims to identify contamination, understand its impacts, and explore mitigation strategies from diverse perspectives.
However, comprehensive studies that provide a clear pathway from foundational concepts to advanced insights are lacking in this nascent field. Therefore, we present a comprehensive survey in the field of data contamination, laying out the key issues, methodologies, and findings to date, and highlighting areas in need of further research and development. In particular, we begin by examining the effects of data contamination across various stages and forms. We then provide a detailed analysis of current contamination detection methods, categorizing them to highlight their focus, assumptions, strengths, and limitations. We also discuss mitigation strategies, offering a clear guide for future research. This survey serves as a succinct overview of the most recent advancements in data contamination research, providing a straightforward guide for the benefit of future research endeavors.

------------

`[2406.15126] On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey <https://arxiv.org/abs/2406.15126>`__ llms驱动的合成数据生成、策划和评估综述

::

    Fri, 14 Jun 2024 07:47:09 GMT
    Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, Haobo Wang

Within the evolving landscape of deep learning, the dilemma of data quantity and quality has been a long-standing problem. The recent advent of Large Language Models (LLMs) offers a data-centric solution to alleviate the limitations of real-world data with synthetic data generation. However, current investigations into this field lack a unified framework and mostly stay on the surface. Therefore, this paper provides an organization of relevant studies based on a generic workflow of synthetic data generation. By doing so, we highlight the gaps within existing research and outline prospective avenues for future study. This work aims to shepherd the academic and industrial communities towards deeper, more methodical inquiries into the capabilities and applications of LLMs-driven synthetic data generation.

------------

-------------
Benchmark (8)
-------------

`[2406.14780] ACR: A Benchmark for Automatic Cohort Retrieval <https://arxiv.org/abs/2406.14780>`__ ACR:自动队列检索基准

::

    Thu, 20 Jun 2024 23:04:06 GMT
    Dung Ngoc Thai, Victor Ardulov, Jose Ulises Mena, Simran Tiwari, Gleb Erofeev, Ramy Eskander, Karim Tarabishy, Ravi B Parikh, Wael Salloum

Identifying patient cohorts is fundamental to numerous healthcare tasks, including clinical trial recruitment and retrospective studies. Current cohort retrieval methods in healthcare organizations rely on automated queries of structured data combined with manual curation, which are time-consuming, labor-intensive, and often yield low-quality results. Recent advancements in large language models (LLMs) and information retrieval (IR) offer promising avenues to revolutionize these systems. Major challenges include managing extensive eligibility criteria and handling the longitudinal nature of unstructured Electronic Medical Records (EMRs) while ensuring that the solution remains cost-effective for real-world application. This paper introduces a new task, Automatic Cohort Retrieval (ACR), and evaluates the performance of LLMs and commercial, domain-specific neuro-symbolic approaches. We provide a benchmark task, a query dataset, an EMR dataset, and an evaluation framework.
Our findings underscore the necessity for efficient, high-quality ACR systems capable of longitudinal reasoning across extensive patient databases.

------------

`[2406.15187] UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis <https://arxiv.org/abs/2406.15187>`__ UDA:真实世界文档分析中检索增强生成的基准套件

::

    Fri, 21 Jun 2024 14:29:39 GMT
    Yulong Hui, Yao Lu, Huanchen Zhang

The use of Retrieval-Augmented Generation (RAG) has improved Large Language Models (LLMs) in collaborating with external data, yet significant challenges exist in real-world scenarios. In areas such as academic literature and finance question answering, data are often found in raw text and tables in HTML or PDF formats, which can be lengthy and highly unstructured. In this paper, we introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We revisit popular LLM- and RAG-based solutions for document analysis and evaluate the design choices and answer qualities across multiple document domains and diverse query types. Our evaluation yields interesting findings and highlights the importance of data parsing and retrieval. We hope our benchmark can shed light and better serve real-world document analysis applications. The benchmark suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.

------------

`[2406.14884] FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents <https://arxiv.org/abs/2406.14884>`__ FlowBench:基于llm代理的工作流指导规划的重新审视和基准测试

::

    Fri, 21 Jun 2024 06:13:00 GMT
    Ruixuan Xiao, Wentao Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang, Fei Huang, Yongbin Li

LLM-based agents have emerged as promising tools, which are crafted to fulfill complex tasks by iterative planning and action. However, these agents are susceptible to undesired planning hallucinations when lacking specific knowledge for expertise-intensive tasks. To address this, preliminary attempts are made to enhance planning reliability by incorporating external workflow-related knowledge. Despite the promise, such infused knowledge is mostly disorganized and diverse in formats, lacking rigorous formalization and comprehensive comparisons. Motivated by this, we formalize different formats of workflow knowledge and present FlowBench, the first benchmark for workflow-guided planning. FlowBench covers 51 different scenarios from 6 domains, with knowledge presented in diverse formats. To assess different LLMs on FlowBench, we design a multi-tiered evaluation framework. We evaluate the efficacy of workflow knowledge across multiple formats, and the results indicate that current LLM agents need considerable improvements for satisfactory planning. We hope that our challenging benchmark can pave the way for future agent planning research.

------------

`[2406.15019] MedOdyssey: A Medical Domain Benchmark for Long Context Evaluation Up to 200K Tokens <https://arxiv.org/abs/2406.15019>`__ MedOdyssey:长达200K token的医学领域长上下文评估基准

::

    Fri, 21 Jun 2024 09:46:57 GMT
    Yongqi Fan, Hongli Sun, Kui Xue, Xiaofan Zhang, Shaoting Zhang, Tong Ruan

Numerous advanced Large Language Models (LLMs) now support context lengths up to 128K, and some extend to 200K. Some benchmarks in the generic domain have also followed up on evaluating long-context capabilities. In the medical domain, tasks are distinctive due to the unique contexts and need for domain expertise, necessitating further evaluation. However, despite the frequent presence of long texts in medical scenarios, evaluation benchmarks of long-context capabilities for LLMs in this field are still rare. In this paper, we propose MedOdyssey, the first medical long-context benchmark with seven length levels ranging from 4K to 200K tokens. MedOdyssey consists of two primary components: the medical-context "needles in a haystack" task and a series of tasks specific to medical applications, together comprising 10 datasets. The first component includes challenges such as counter-intuitive reasoning and novel (unknown) facts injection to mitigate knowledge leakage and data contamination of LLMs. The second component confronts the challenge of requiring professional medical expertise. Especially, we design the ``Maximum Identical Context'' principle to improve fairness by guaranteeing that different LLMs observe as many identical contexts as possible. Our experiment evaluates advanced proprietary and open-source LLMs tailored for processing long contexts and presents detailed performance analyses. This highlights that LLMs still face challenges and need for further research in this area. Our code and data are released in the repository: \url{https://github.com/JOHNNY-fans/MedOdyssey.}

------------

`[2406.15341] GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians <https://arxiv.org/abs/2406.15341>`__ 

::

    Fri, 21 Jun 2024 17:55:24 GMT
    Haoyang Liu, Haohan Wang

Recent advancements in machine learning have significantly improved the identification of disease-associated genes from gene expression datasets.
However, these processes often require extensive expertise and manual effort, limiting their scalability. Large Language Model (LLM)-based agents have shown promise in automating these tasks due to their increasing problem-solving abilities. To support the evaluation and development of such methods, we introduce GenoTEX, a benchmark dataset for the automatic exploration of gene expression data, involving the tasks of dataset selection, preprocessing, and statistical analysis. GenoTEX provides annotated code and results for solving a wide range of gene identification problems, in a full analysis pipeline that follows the standard of computational genomics. These annotations are curated by human bioinformaticians who carefully analyze the datasets to ensure accuracy and reliability. To provide baselines for these tasks, we present GenoAgents, a team of LLM-based agents designed with context-aware planning, iterative correction, and domain expert consultation to collaboratively explore gene datasets. Our experiments with GenoAgents demonstrate the potential of LLM-based approaches in genomics data analysis, while error analysis highlights the challenges and areas for future improvement. We propose GenoTEX as a promising resource for benchmarking and enhancing AI-driven methods for genomics data analysis. We make our benchmark publicly available at \url{https://github.com/Liu-Hy/GenoTex}.

------------

`[2406.14712] Qiskit HumanEval: An Evaluation Benchmark For Quantum Code Generative Models <https://arxiv.org/abs/2406.14712>`__ Qiskit HumanEval:量子代码生成模型评估基准

::

    Thu, 20 Jun 2024 20:14:22 GMT
    Sanjay Vishwakarma, Francis Harkins, Siddharth Golecha, Vishal Sharathchandra Bajpe, Nicolas Dupuis, Luca Buratti, David Kremer, Ismael Faro, Ruchir Puri and Juan Cruz-Benito

Quantum programs are typically developed using quantum Software Development Kits (SDKs). The rapid advancement of quantum computing necessitates new tools to streamline this development process, and one such tool could be Generative Artificial intelligence (GenAI). In this study, we introduce and use the Qiskit HumanEval dataset, a hand-curated collection of tasks designed to benchmark the ability of Large Language Models (LLMs) to produce quantum code using Qiskit - a quantum SDK. This dataset consists of more than 100 quantum computing tasks, each accompanied by a prompt, a canonical solution, a comprehensive test case, and a difficulty scale to evaluate the correctness of the generated solutions.
We systematically assess the performance of a set of LLMs against the Qiskit HumanEval dataset's tasks and focus on the models ability in producing executable quantum code. Our findings not only demonstrate the feasibility of using LLMs for generating quantum code but also establish a new benchmark for ongoing advancements in the field and encourage further exploration and development of GenAI-driven tools for quantum code generation.

------------

`[2306.04959] FedSecurity: Benchmarking Attacks and Defenses in Federated Learning and Federated LLMs <https://arxiv.org/abs/2306.04959>`__ FedSecurity:联邦学习和联邦llm中的攻击和防御基准测试

::

    replaced with revised version Fri, 21 Jun 2024 00:01:52 GMT
    Submission history From: Shanshan Han [view email]
    [v1] Thu, 8 Jun 2023 06:21:35 UTC (3,262 KB)
    [v2] Mon, 25 Sep 2023 07:17:27 UTC (7,073 KB)
    [v3] Fri, 6 Oct 2023 07:58:13 UTC (7,073 KB)
    [v4] Fri, 9 Feb 2024 19:57:05 UTC (7,378 KB)
    [v5] Fri, 21 Jun 2024 00:01:52 UTC (7,517 KB)
    Shanshan Han, Baturalp Buyukates, Zijian Hu, Han Jin, Weizhao Jin, Lichao Sun, Xiaoyang Wang, Wenxuan Wu, Chulin Xie, Yuhang Yao, Kai Zhang, Qifan Zhang, Yuhui Zhang, Carlee Joe-Wong, Salman Avestimehr and Chaoyang He

This paper introduces FedSecurity, an end-to-end benchmark that serves as a supplementary component of the FedML library for simulating adversarial attacks and corresponding defense mechanisms in Federated Learning (FL). FedSecurity eliminates the need for implementing the fundamental FL procedures, e.g., FL training and data loading, from scratch, thus enables users to focus on developing their own attack and defense strategies. It contains two key components, including FedAttacker that conducts a variety of attacks during FL training, and FedDefender that implements defensive mechanisms to counteract these attacks. FedSecurity has the following features: i) It offers extensive customization options to accommodate a broad range of machine learning models (e.g., Logistic Regression, ResNet, and GAN) and FL optimizers (e.g., FedAVG, FedOPT, and FedNOVA); ii) it enables exploring the effectiveness of attacks and defenses across different datasets and models; and iii) it supports flexible configuration and customization through a configuration file and some APIs. We further demonstrate FedSecurity's utility and adaptability through federated training of Large Language Models (LLMs) to showcase its potential on a wide range of complex applications.

------------

`[2406.14294] DASB -- Discrete Audio and Speech Benchmark <https://arxiv.org/abs/2406.14294>`__ DASB—离散音频和语音基准

::

    replaced with revised version Fri, 21 Jun 2024 17:07:17 GMT
    Submission history From: Pooneh Mousavi [view email]
    [v1] Thu, 20 Jun 2024 13:23:27 UTC (936 KB)
    [v2] Fri, 21 Jun 2024 17:07:17 UTC (936 KB)
    Pooneh Mousavi, Luca Della Libera, Jarod Duret, Artem Ploujnikov, Cem Subakan, Mirco Ravanelli

Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.

------------

--------------
Accelerate (9)
--------------

`[2406.14635] Harvesting Efficient On-Demand Order Pooling from Skilled Couriers: Enhancing Graph Representation Learning for Refining Real-time Many-to-One Assignments <https://arxiv.org/abs/2406.14635>`__ 从熟练的快递员那里获取高效的按需订单池:增强图表示学习以细化实时多对一分配

::

    Thu, 20 Jun 2024 18:03:27 GMT
    Yile Liang, Jiuxia Zhao, Donghui Li, Jie Feng, Chen Zhang, Xuetao Ding, Jinghua Hao, Renqing He

The recent past has witnessed a notable surge in on-demand food delivery (OFD) services, offering delivery fulfillment within dozens of minutes after an order is placed. In OFD, pooling multiple orders for simultaneous delivery in real-time order assignment is a pivotal efficiency source, which may in turn extend delivery time. Constructing high-quality order pooling to harmonize platform efficiency with the experiences of consumers and couriers, is crucial to OFD platforms. However, the complexity and real-time nature of order assignment, making extensive calculations impractical, significantly limit the potential for order consolidation. Moreover, offline environment is frequently riddled with unknown factors, posing challenges for the platform's perceptibility and pooling decisions. Nevertheless, delivery behaviors of skilled couriers (SCs) who know the environment well, can improve system awareness and effectively inform decisions. Hence a SC delivery network (SCDN) is constructed, based on an enhanced attributed heterogeneous network embedding approach tailored for OFD. It aims to extract features from rich temporal and spatial information, and uncover the latent potential for order combinations embedded within SC trajectories. Accordingly, the vast search space of order assignment can be effectively pruned through scalable similarity calculations of low-dimensional vectors, making comprehensive and high-quality pooling outcomes more easily identified in real time. SCDN has now been deployed in Meituan dispatch system. Online tests reveal that with SCDN, the pooling quality and extent have been greatly improved. And our system can boost couriers'efficiency by 45-55% during noon peak hours, while upholding the timely delivery commitment.

------------

`[2406.14833] Efficient Continual Pre-training by Mitigating the Stability Gap <https://arxiv.org/abs/2406.14833>`__ 通过减轻稳定性差距进行有效的持续预训练

::

    Fri, 21 Jun 2024 02:28:37 GMT
    Yiduo Guo, Jie Fu, Huishuai Zhang, Dongyan Zhao, Yikang Shen

Continual pre-training has increasingly become the predominant approach for adapting Large Language Models (LLMs) to new domains. This process involves updating the pre-trained LLM with a corpus from a new domain, resulting in a shift in the training distribution. To study the behavior of LLMs during this shift, we measured the model's performance throughout the continual pre-training process. we observed a temporary performance drop at the beginning, followed by a recovery phase, a phenomenon known as the "stability gap," previously noted in vision models classifying new classes. To address this issue and enhance LLM performance within a fixed compute budget, we propose three effective strategies: (1) Continually pre-training the LLM on a subset with a proper size for multiple epochs, resulting in faster performance recovery than pre-training the LLM on a large corpus in a single epoch; (2) Pre-training the LLM only on high-quality sub-corpus, which rapidly boosts domain performance; and (3) Using a data mixture similar to the pre-training data to reduce distribution gap. We conduct various experiments on Llama-family models to validate the effectiveness of our strategies in both medical continual pre-training and instruction tuning. For example, our strategies improve the average medical task performance of the OpenLlama-3B model from 36.2% to 40.7% with only 40% of the original training budget and enhance the average general task performance without causing forgetting. Furthermore, we apply our strategies to the Llama-3-8B model. The resulting model, Llama-3-Physician, achieves the best medical performance among current open-source models, and performs comparably to or even better than GPT-4 on several medical benchmarks. We release our models at \url{https://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct}.

------------

`[2406.14848] Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models <https://arxiv.org/abs/2406.14848>`__ 利用段落嵌入对大型语言模型进行有效的列表排序

::

    Fri, 21 Jun 2024 03:33:51 GMT
    Qi Liu, Bo Wang, Nan Wang, Jiaxin Mao

Recent studies have demonstrated the effectiveness of using large language language models (LLMs) in passage ranking. The listwise approaches, such as RankGPT, have become new state-of-the-art in this task. However, the efficiency of RankGPT models is limited by the maximum context length and relatively high latency of LLM inference. To address these issues, in this paper, we propose PE-Rank, leveraging the single passage embedding as a good context compression for efficient listwise passage reranking. By treating each passage as a special token, we can directly input passage embeddings into LLMs, thereby reducing input length. Additionally, we introduce an inference method that dynamically constrains the decoding space to these special tokens, accelerating the decoding process. For adapting the model to reranking, we employ listwise learning to rank loss for training. Evaluation results on multiple benchmarks demonstrate that PE-Rank significantly improves efficiency in both prefilling and decoding, while maintaining competitive ranking effectiveness. {The Code is available at \url{https://github.com/liuqi6777/pe_rank}.}

------------

`[2406.15193] Reward Steering with Evolutionary Heuristics for Decoding-time Alignment <https://arxiv.org/abs/2406.15193>`__ 基于进化启发式的解码时间对齐奖励导向

::

    Fri, 21 Jun 2024 14:35:16 GMT
    Chia-Yu Hung, Navonil Majumder, Ambuj Mehrish, Soujanya Poria

The widespread applicability and increasing omnipresence of LLMs have instigated a need to align LLM responses to user and stakeholder preferences.
Many preference optimization approaches have been proposed that fine-tune LLM parameters to achieve good alignment. However, such parameter tuning is known to interfere with model performance on many tasks. Moreover, keeping up with shifting user preferences is tricky in such a situation. Decoding-time alignment with reward model guidance solves these issues at the cost of increased inference time. However, most of such methods fail to strike the right balance between exploration and exploitation of reward -- often due to the conflated formulation of these two aspects - to give well-aligned responses. To remedy this we decouple these two aspects and implement them in an evolutionary fashion: exploration is enforced by decoding from mutated instructions and exploitation is represented as the periodic replacement of poorly-rewarded generations with well-rewarded ones. Empirical evidences indicate that this strategy outperforms many preference optimization and decode-time alignment approaches on two widely accepted alignment benchmarks AlpacaEval 2 and MT-Bench. Our implementation will be available at: https://darwin-alignment.github.io.

------------

`[2402.10552] Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models <https://arxiv.org/abs/2402.10552>`__ 会话模拟:基于大型语言模型的高效同声翻译

::

    replaced with revised version Fri, 21 Jun 2024 07:18:13 GMT
    Submission history From: Minghan Wang [view email]
    [v1] Fri, 16 Feb 2024 10:32:16 UTC (7,921 KB)
    [v2] Sun, 16 Jun 2024 09:25:13 UTC (8,047 KB)
    [v3] Fri, 21 Jun 2024 07:18:13 UTC (8,047 KB)
    Minghan Wang, Thuy-Trang Vu, Yuxia Wang, Ehsan Shareghi, Gholamreza Haffari

Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.

------------

`[2405.01121] Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts <https://arxiv.org/abs/2405.01121>`__ 基于源的信息搜索对话的高效数据生成:会议记录用例

::

    replaced with revised version Fri, 21 Jun 2024 09:10:28 GMT
    Submission history From: Filippo Galgani [view email]
    [v1] Thu, 2 May 2024 09:35:06 UTC (8,470 KB)
    [v2] Fri, 21 Jun 2024 09:10:28 UTC (9,057 KB)
    Lotem Golany, Filippo Galgani, Maya Mamo, Nimrod Parasol, Omer Vandsburger, Nadav Bar, Ido Dagan

Automating data generation with Large Language Models (LLMs) has become increasingly popular. In this work, we investigate the feasibility and effectiveness of LLM-based data generation in the challenging setting of source-grounded information-seeking dialogs, with response attribution, over long documents. Our source texts consist of long and noisy meeting transcripts, adding to the task complexity. Since automating attribution remains difficult, we propose a semi-automatic approach: dialog queries and responses are generated with LLMs, followed by human verification and identification of attribution spans. Using this approach, we created MISeD -- Meeting Information Seeking Dialogs dataset -- a dataset of information-seeking dialogs focused on meeting transcripts. Models finetuned with MISeD demonstrate superior performance compared to off-the-shelf models, even those of larger size. Finetuning on MISeD gives comparable response generation quality to finetuning on fully manual data, while improving attribution quality and reducing time and effort.

------------

`[2405.19715] SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths <https://arxiv.org/abs/2405.19715>`__ specdec++:自适应候选长度增强推测解码

::

    replaced with revised version Fri, 21 Jun 2024 01:01:42 GMT
    Submission history From: Kaixuan Huang [view email]
    [v1] Thu, 30 May 2024 05:49:38 UTC (218 KB)
    [v2] Fri, 21 Jun 2024 01:01:42 UTC (218 KB)
    Kaixuan Huang, Xudong Guo, Mengdi Wang

Speculative decoding reduces the inference latency of a target large language model via utilizing a smaller and faster draft model. Its performance depends on a hyperparameter K -- the candidate length, i.e., the number of candidate tokens for the target model to verify in each round. However, previous methods often use simple heuristics to choose K, which may result in sub-optimal performance. We study the choice of the candidate length K and formulate it as a Markov Decision Process. We theoretically show that the optimal policy of this Markov decision process takes the form of a threshold policy, i.e., the current speculation should stop and be verified when the probability of getting a rejection exceeds a threshold value. Motivated by this theory, we propose SpecDec++, an enhanced version of speculative decoding that adaptively determines the candidate length on the fly. We augment the draft model with a trained acceptance prediction head to predict the conditional acceptance probability of the candidate tokens. SpecDec++ will stop the current speculation when the predicted probability that at least one token gets rejected exceeds a threshold. We implement SpecDec++ and apply it to the llama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup on the Alpaca dataset (an additional 7.2% improvement over the baseline speculative decoding). On the GSM8K and HumanEval datasets, our method achieves a 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement), respectively.

------------

`[2406.04218] Linguistic Steganalysis via LLMs: Two Modes for Efficient Detection of Strongly Concealed Stego <https://arxiv.org/abs/2406.04218>`__ 基于LLMs的语言隐写分析:两种高效检测强隐蔽性隐写的模式

::

    replaced with revised version Fri, 21 Jun 2024 05:17:53 GMT
    Submission history From: Yifan Tang [view email]
    [v1] Thu, 6 Jun 2024 16:18:02 UTC (127 KB)
    [v2] Fri, 21 Jun 2024 05:17:53 UTC (117 KB)
    Yifan Tang and Yihao Wang and Ru Zhang and Jianyi Liu

To detect stego (steganographic text) in complex scenarios, linguistic steganalysis (LS) with various motivations has been proposed and achieved excellent performance. However, with the development of generative steganography, some stegos have strong concealment, especially after the emergence of LLMs-based steganography, the existing LS has low detection or cannot detect them. We designed a novel LS with two modes called LSGC. In the generation mode, we created an LS-task "description" and used the generation ability of LLM to explain whether texts to be detected are stegos. On this basis, we rethought the principle of LS and LLMs, and proposed the classification mode. In this mode, LSGC deleted the LS-task "description" and used the "causalLM" LLMs to extract steganographic features. The LS features can be extracted by only one pass of the model, and a linear layer with initialization weights is added to obtain the classification probability. Experiments on strongly concealed stegos show that LSGC significantly improves detection and reaches SOTA performance. Additionally, LSGC in classification mode greatly reduces training time while maintaining high performance.

------------

`[2312.16862] TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones <https://arxiv.org/abs/2312.16862>`__ TinyGPT-V:基于小骨干的高效多模态大型语言模型

::

    replaced with revised version Fri, 21 Jun 2024 07:08:59 GMT
    Submission history From: Zhengqing Yuan [view email]
    [v1] Thu, 28 Dec 2023 07:11:41 UTC (2,300 KB)
    [v2] Thu, 4 Apr 2024 18:53:58 UTC (5,205 KB)
    [v3] Fri, 21 Jun 2024 07:08:59 UTC (5,207 KB)
    Zhengqing Yuan, Zhaoxu Li, Weiran Huang, Yanfang Ye, Lichao Sun

In recent years, multimodal large language models (MLLMs) such as GPT-4V have demonstrated remarkable advancements, excelling in a variety of vision-language tasks. Despite their prowess, the closed-source nature and computational demands of such models limit their accessibility and applicability. This study introduces TinyGPT-V, a novel open-source MLLM, designed for efficient training and inference across various vision-language tasks, including image captioning (IC) and visual question answering (VQA). Leveraging a compact yet powerful architecture, TinyGPT-V integrates the Phi-2 language model with pre-trained vision encoders, utilizing a unique mapping module for visual and linguistic information fusion. With a training regimen optimized for small backbones and employing a diverse dataset amalgam, TinyGPT-V requires significantly lower computational resources 24GB for training and as little as 8GB for inference without compromising on performance. Our experiments demonstrate that TinyGPT-V, with its language model 2.8 billion parameters, achieves comparable results in VQA and image inference tasks to its larger counterparts while being uniquely suited for deployment on resource-constrained devices through innovative quantization techniques. This work not only paves the way for more accessible and efficient MLLMs but also underscores the potential of smaller, optimized models in bridging the gap between high performance and computational efficiency in real-world applications. Additionally, this paper introduces a new approach to multimodal large language models using smaller backbones. Our code and training weights are available in the supplementary material.

------------

-----------------------
In-Context Learning (2)
-----------------------

`[2406.14739] Learning to Retrieve Iteratively for In-Context Learning <https://arxiv.org/abs/2406.14739>`__ 面向上下文学习的迭代检索学习

::

    Thu, 20 Jun 2024 21:07:55 GMT
    Yunmo Chen, Tongfei Chen, Harsh Jhamtani, Patrick Xia, Richard Shin, Jason Eisner, Benjamin Van Durme

We introduce iterative retrieval, a novel framework that empowers retrievers to make iterative decisions through policy optimization. Finding an optimal portfolio of retrieved items is a combinatorial optimization problem, generally considered NP-hard. This approach provides a learned approximation to such a solution, meeting specific task requirements under a given family of large language models (LLMs). We propose a training procedure based on reinforcement learning, incorporating feedback from LLMs. We instantiate an iterative retriever for composing in-context learning (ICL) exemplars and apply it to various semantic parsing tasks that demand synthesized programs as outputs. By adding only 4M additional parameters for state encoding, we convert an off-the-shelf dense retriever into a stateful iterative retriever, outperforming previous methods in selecting ICL exemplars on semantic parsing datasets such as CalFlow, TreeDST, and MTOP. Additionally, the trained iterative retriever generalizes across different inference LLMs beyond the one used during training.

------------

`[2406.14955] ICLEval: Evaluating In-Context Learning Ability of Large Language Models <https://arxiv.org/abs/2406.14955>`__ ICLEval:大型语言模型的上下文学习能力评估

::

    Fri, 21 Jun 2024 08:06:10 GMT
    Wentong Chen, Yankai Lin, ZhenHao Zhou, HongYun Huang, Yantao Jia, Zhao Cao, Ji-Rong Wen

In-Context Learning (ICL) is a critical capability of Large Language Models (LLMs) as it empowers them to comprehend and reason across interconnected inputs. Evaluating the ICL ability of LLMs can enhance their utilization and deepen our understanding of how this ability is acquired at the training stage.
However, existing evaluation frameworks primarily focus on language abilities and knowledge, often overlooking the assessment of ICL ability. In this work, we introduce the ICLEval benchmark to evaluate the ICL abilities of LLMs, which encompasses two key sub-abilities: exact copying and rule learning. Through the ICLEval benchmark, we demonstrate that ICL ability is universally present in different LLMs, and model size is not the sole determinant of ICL efficacy.
Surprisingly, we observe that ICL abilities, particularly copying, develop early in the pretraining process and stabilize afterward. Our source codes and benchmark are released at https://github.com/yiye3/ICLEval.

------------

-------------
Reasoning (6)
-------------

`[2406.14732] TTQA-RS- A break-down prompting approach for Multi-hop Table-Text Question Answering with Reasoning and Summarization <https://arxiv.org/abs/2406.14732>`__ TTQA-RS——一种支持推理和摘要的多跳表-文本问答分解提示方法

::

    Thu, 20 Jun 2024 20:55:38 GMT
    Jayetri Bardhan, Bushi Xiao, Daisy Zhe Wang

Question answering (QA) over tables and text has gained much popularity over the years. Multi-hop table-text QA requires multiple hops between the table and text, making it a challenging QA task. Although several works have attempted to solve the table-text QA task, most involve training the models and requiring labeled data. In this paper, we have proposed a model - TTQA-RS: A break-down prompting approach for Multi-hop Table-Text Question Answering with Reasoning and Summarization. Our model uses augmented knowledge including table-text summary with decomposed sub-question with answer for a reasoning-based table-text QA. Using open-source language models our model outperformed all existing prompting methods for table-text QA tasks on existing table-text QA datasets like HybridQA and OTT-QA's development set. Our results are comparable with the training-based state-of-the-art models, demonstrating the potential of prompt-based approaches using open-source LLMs. Additionally, by using GPT-4 with LLaMA3-70B, our model achieved state-of-the-art performance for prompting-based methods on multi-hop table-text QA.

------------

`[2406.14852] Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models <https://arxiv.org/abs/2406.14852>`__ 一张图片是否胜过千言万语?深入研究视觉语言模型的空间推理

::

    Fri, 21 Jun 2024 03:53:37 GMT
    Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Neel Joshi

Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains.
Despite this promise, spatial understanding and reasoning -- a fundamental component of human cognition -- remains under-explored. We develop novel benchmarks that cover diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided.
Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence.

------------

`[2403.06294] ArgMed-Agents: Explainable Clinical Decision Reasoning with LLM Disscusion via Argumentation Schemes <https://arxiv.org/abs/2403.06294>`__ 论证主体:基于论证方案的可解释临床决策推理

::

    replaced with revised version Thu, 20 Jun 2024 21:57:15 GMT
    Submission history From: Shengxin Hong [view email]
    [v1] Sun, 10 Mar 2024 19:47:00 UTC (906 KB)
    [v2] Thu, 20 Jun 2024 21:57:15 UTC (972 KB)
    Shengxin Hong, Liang Xiao, Xin Zhang, Jianxia Chen

There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Discussion (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, use symbolic solver to identify a series of rational and coherent arguments to support decision. We construct a formal model of ArgMed-Agents and present conjectures for theoretical guarantees. ArgMed-Agents enables LLMs to mimic the process of clinical argumentative reasoning by generating explanations of reasoning in a self-directed manner. The setup experiments show that ArgMed-Agents not only improves accuracy in complex clinical decision reasoning problems compared to other prompt methods, but more importantly, it provides users with decision explanations that increase their confidence.

------------

`[2305.14318] CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models <https://arxiv.org/abs/2305.14318>`__ CREATOR:用于分解大型语言模型的抽象和具体推理的工具创建

::

    replaced with revised version Fri, 21 Jun 2024 16:51:22 GMT
    Submission history From: Cheng Qian [view email]
    [v1] Tue, 23 May 2023 17:51:52 UTC (8,324 KB)
    [v2] Sun, 8 Oct 2023 14:12:12 UTC (9,273 KB)
    [v3] Fri, 21 Jun 2024 16:51:22 UTC (9,273 KB)
    Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, Heng Ji

Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance. We evaluate CREATOR on MATH and TabMWP benchmarks, respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably, CREATOR outperforms existing chain-of-thought, program-of-thought, and tool-using baselines. Additionally, we introduce the Creation Challenge dataset, featuring 2K diverse questions, to emphasize the necessity and benefits of LLMs' tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to adapt to diverse situations. The tool creation ability revolutionizes the LLM's problem-solving paradigm, driving us closer to the next frontier of artificial intelligence. All the codes and data are released.

------------

`[2311.08097] Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts <https://arxiv.org/abs/2311.08097>`__ 通过思想树实现跨语言的多步推理

::

    replaced with revised version Fri, 21 Jun 2024 06:06:51 GMT
    Submission history From: Leonardo Ranaldi Mr [view email]
    [v1] Tue, 14 Nov 2023 11:49:43 UTC (9,186 KB)
    [v2] Fri, 19 Apr 2024 15:49:21 UTC (9,411 KB)
    [v3] Wed, 19 Jun 2024 13:07:54 UTC (9,411 KB)
    [v4] Fri, 21 Jun 2024 06:06:51 UTC (9,403 KB)
    Leonardo Ranaldi, Giulia Pucci, Federico Ranaldi, Elena Sofia Ruzzetti, Fabio Massimo Zanzotto

Reasoning methods, best exemplified by the well-known Chain-of-Thought (CoT), empower the reasoning abilities of Large Language Models (LLMs) by eliciting them to solve complex tasks in a step-by-step manner. Although they are achieving significant success, the ability to deliver multi-step reasoning remains limited to English because of the imbalance in the distribution of pre-training data, which makes other languages a barrier. In this paper, we propose Cross-lingual Tree-of-Thoughts (Cross-ToT), a method for aligning Cross-lingual CoT reasoning across languages. The proposed method, through a self-consistent cross-lingual prompting mechanism inspired by the Tree-of-Thoughts approach, provides multi-step reasoning paths in different languages that, during the steps, lead to the final solution. Experimental evaluations show that our method significantly outperforms existing prompting methods by reducing the number of interactions and achieving state-of-the-art performance.

------------

`[2406.13808] Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning? <https://arxiv.org/abs/2406.13808>`__ llm中的低秩知识蒸馏对微电子推理有用吗?

::

    replaced with revised version Fri, 21 Jun 2024 02:25:57 GMT
    Submission history From: Nirjhor Tahmidur Rouf [view email]
    [v1] Wed, 19 Jun 2024 20:14:39 UTC (1,127 KB)
    [v2] Fri, 21 Jun 2024 02:25:57 UTC (1,127 KB)
    Nirjhor Rouf, Fin Amin, Paul D. Franzon

In this work, we present empirical results regarding the feasibility of using offline large language models (LLMs) in the context of electronic design automation (EDA). The goal is to investigate and evaluate a contemporary language model's (Llama-2-7B) ability to function as a microelectronic Q & A expert as well as its reasoning, and generation capabilities in solving microelectronic-related problems. Llama-2-7B was tested across a variety of adaptation methods, including introducing a novel low-rank knowledge distillation (LoRA-KD) scheme. Our experiments produce both qualitative and quantitative results.

------------

-----------
ToolUse (3)
-----------

`[2305.14318] CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models <https://arxiv.org/abs/2305.14318>`__ CREATOR:用于分解大型语言模型的抽象和具体推理的工具创建

::

    replaced with revised version Fri, 21 Jun 2024 16:51:22 GMT
    Submission history From: Cheng Qian [view email]
    [v1] Tue, 23 May 2023 17:51:52 UTC (8,324 KB)
    [v2] Sun, 8 Oct 2023 14:12:12 UTC (9,273 KB)
    [v3] Fri, 21 Jun 2024 16:51:22 UTC (9,273 KB)
    Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, Heng Ji

Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance. We evaluate CREATOR on MATH and TabMWP benchmarks, respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably, CREATOR outperforms existing chain-of-thought, program-of-thought, and tool-using baselines. Additionally, we introduce the Creation Challenge dataset, featuring 2K diverse questions, to emphasize the necessity and benefits of LLMs' tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to adapt to diverse situations. The tool creation ability revolutionizes the LLM's problem-solving paradigm, driving us closer to the next frontier of artificial intelligence. All the codes and data are released.

------------

`[2406.12266] Towards a Client-Centered Assessment of LLM Therapists by Client Simulation <https://arxiv.org/abs/2406.12266>`__ 通过客户模拟对LLM治疗师进行以客户为中心的评估

::

    replaced with revised version Thu, 20 Jun 2024 22:06:47 GMT
    Submission history From: Jiashuo Wang [view email]
    [v1] Tue, 18 Jun 2024 04:46:55 UTC (2,203 KB)
    [v2] Thu, 20 Jun 2024 22:06:47 UTC (2,203 KB)
    Jiashuo Wang, Yang Xiao, Yanran Li, Changhe Song, Chunpu Xu, Chenhao Tan, Wenjie Li

Although there is a growing belief that LLMs can be used as therapists, exploring LLMs' capabilities and inefficacy, particularly from the client's perspective, is limited. This work focuses on a client-centered assessment of LLM therapists with the involvement of simulated clients, a standard approach in clinical medical education. However, there are two challenges when applying the approach to assess LLM therapists at scale. Ethically, asking humans to frequently mimic clients and exposing them to potentially harmful LLM outputs can be risky and unsafe. Technically, it can be difficult to consistently compare the performances of different LLM therapists interacting with the same client. To this end, we adopt LLMs to simulate clients and propose ClientCAST, a client-centered approach to assessing LLM therapists by client simulation. Specifically, the simulated client is utilized to interact with LLM therapists and complete questionnaires related to the interaction. Based on the questionnaire results, we assess LLM therapists from three client-centered aspects: session outcome, therapeutic alliance, and self-reported feelings. We conduct experiments to examine the reliability of ClientCAST and use it to evaluate LLMs therapists implemented by Claude-3, GPT-3.5, LLaMA3-70B, and Mixtral 8*7B. Codes are released at this https URL.

------------

`[2402.05147] ApiQ: Finetuning of 2-Bit Quantized Large Language Model <https://arxiv.org/abs/2402.05147>`__ ApiQ: 2-Bit量化大型语言模型的微调

::

    replaced with revised version Fri, 21 Jun 2024 14:03:48 GMT
    Submission history From: Baohao Liao [view email]
    [v1] Wed, 7 Feb 2024 09:36:54 UTC (4,435 KB)
    [v2] Mon, 12 Feb 2024 15:09:39 UTC (17,690 KB)
    [v3] Fri, 21 Jun 2024 14:03:48 UTC (5,318 KB)
    Baohao Liao, Christian Herold, Shahram Khadivi, Christof Monz

Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the effectiveness of these methods compared to full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework, ApiQ, designed to restore the lost information from quantization by concurrently initializing the LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error propagation from shallower into deeper layers. Through comprehensive evaluations conducted on a spectrum of language tasks with various LLMs, ApiQ demonstrably minimizes activation error during quantization. Consequently, it consistently achieves superior finetuning results across various bit-widths.

------------

------------------------
Retrieval-Augmented (12)
------------------------

`[2406.14780] ACR: A Benchmark for Automatic Cohort Retrieval <https://arxiv.org/abs/2406.14780>`__ ACR:自动队列检索基准

::

    Thu, 20 Jun 2024 23:04:06 GMT
    Dung Ngoc Thai, Victor Ardulov, Jose Ulises Mena, Simran Tiwari, Gleb Erofeev, Ramy Eskander, Karim Tarabishy, Ravi B Parikh, Wael Salloum

Identifying patient cohorts is fundamental to numerous healthcare tasks, including clinical trial recruitment and retrospective studies. Current cohort retrieval methods in healthcare organizations rely on automated queries of structured data combined with manual curation, which are time-consuming, labor-intensive, and often yield low-quality results. Recent advancements in large language models (LLMs) and information retrieval (IR) offer promising avenues to revolutionize these systems. Major challenges include managing extensive eligibility criteria and handling the longitudinal nature of unstructured Electronic Medical Records (EMRs) while ensuring that the solution remains cost-effective for real-world application. This paper introduces a new task, Automatic Cohort Retrieval (ACR), and evaluates the performance of LLMs and commercial, domain-specific neuro-symbolic approaches. We provide a benchmark task, a query dataset, an EMR dataset, and an evaluation framework.
Our findings underscore the necessity for efficient, high-quality ACR systems capable of longitudinal reasoning across extensive patient databases.

------------

`[2406.15187] UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis <https://arxiv.org/abs/2406.15187>`__ UDA:真实世界文档分析中检索增强生成的基准套件

::

    Fri, 21 Jun 2024 14:29:39 GMT
    Yulong Hui, Yao Lu, Huanchen Zhang

The use of Retrieval-Augmented Generation (RAG) has improved Large Language Models (LLMs) in collaborating with external data, yet significant challenges exist in real-world scenarios. In areas such as academic literature and finance question answering, data are often found in raw text and tables in HTML or PDF formats, which can be lengthy and highly unstructured. In this paper, we introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We revisit popular LLM- and RAG-based solutions for document analysis and evaluate the design choices and answer qualities across multiple document domains and diverse query types. Our evaluation yields interesting findings and highlights the importance of data parsing and retrieval. We hope our benchmark can shed light and better serve real-world document analysis applications. The benchmark suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.

------------

`[2406.14745] Relation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks <https://arxiv.org/abs/2406.14745>`__ 检索增强生成框架中基于微调大型语言模型的关系抽取

::

    Thu, 20 Jun 2024 21:27:57 GMT
    Sefika Efeoglu and Adrian Paschke

Information Extraction (IE) is crucial for converting unstructured data into structured formats like Knowledge Graphs (KGs). A key task within IE is Relation Extraction (RE), which identifies relationships between entities in text. Various RE methods exist, including supervised, unsupervised, weakly supervised, and rule-based approaches. Recent studies leveraging pre-trained language models (PLMs) have shown significant success in this area. In the current era dominated by Large Language Models (LLMs), fine-tuning these models can overcome limitations associated with zero-shot LLM prompting-based RE methods, especially regarding domain adaptation challenges and identifying implicit relations between entities in sentences. These implicit relations, which cannot be easily extracted from a sentence's dependency tree, require logical inference for accurate identification. This work explores the performance of fine-tuned LLMs and their integration into the Retrieval Augmented-based (RAG) RE approach to address the challenges of identifying implicit relations at the sentence level, particularly when LLMs act as generators within the RAG framework. Empirical evaluations on the TACRED, TACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant performance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B, and T5 (Large). Notably, our approach achieves substantial gains on SemEVAL, where implicit relations are common, surpassing previous results on this dataset. Additionally, our method outperforms previous works on TACRED, TACREV, and Re-TACRED, demonstrating exceptional performance across diverse evaluation scenarios.

------------

`[2406.14848] Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models <https://arxiv.org/abs/2406.14848>`__ 利用段落嵌入对大型语言模型进行有效的列表排序

::

    Fri, 21 Jun 2024 03:33:51 GMT
    Qi Liu, Bo Wang, Nan Wang, Jiaxin Mao

Recent studies have demonstrated the effectiveness of using large language language models (LLMs) in passage ranking. The listwise approaches, such as RankGPT, have become new state-of-the-art in this task. However, the efficiency of RankGPT models is limited by the maximum context length and relatively high latency of LLM inference. To address these issues, in this paper, we propose PE-Rank, leveraging the single passage embedding as a good context compression for efficient listwise passage reranking. By treating each passage as a special token, we can directly input passage embeddings into LLMs, thereby reducing input length. Additionally, we introduce an inference method that dynamically constrains the decoding space to these special tokens, accelerating the decoding process. For adapting the model to reranking, we employ listwise learning to rank loss for training. Evaluation results on multiple benchmarks demonstrate that PE-Rank significantly improves efficiency in both prefilling and decoding, while maintaining competitive ranking effectiveness. {The Code is available at \url{https://github.com/liuqi6777/pe_rank}.}

------------

`[2406.14891] Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering <https://arxiv.org/abs/2406.14891>`__ 多跳问答的检索增强生成中先生成后地面

::

    Fri, 21 Jun 2024 06:26:38 GMT
    Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, Zhaochun Ren

Multi-Hop Question Answering (MHQA) tasks present a significant challenge for large language models (LLMs) due to the intensive knowledge required. Current solutions, like Retrieval-Augmented Generation, typically retrieve potential documents from an external corpus to read an answer. However, the performance of this retrieve-then-read paradigm is constrained by the retriever and the inevitable noise in the retrieved documents. To mitigate these challenges, we introduce a novel generate-then-ground (GenGround) framework, synergizing the parametric knowledge of LLMs and external documents to solve a multi-hop question. GenGround empowers LLMs to alternate two phases until the final answer is derived: (1) formulate a simpler, single-hop question and directly generate the answer; (2) ground the question-answer pair in retrieved documents, amending any wrong predictions in the answer. We also propose an instructional grounding distillation method to generalize our method into smaller models. Extensive experiments conducted on four datasets illustrate the superiority of our method.

------------

`[2406.14938] Towards Retrieval Augmented Generation over Large Video Libraries <https://arxiv.org/abs/2406.14938>`__ 

::

    Fri, 21 Jun 2024 07:52:01 GMT
    Yannis Tevissen, Khalil Guetari, Fr\'ed\'eric Petitpont

Video content creators need efficient tools to repurpose content, a task that often requires complex manual or automated searches. Crafting a new video from large video libraries remains a challenge. In this paper we introduce the task of Video Library Question Answering (VLQA) through an interoperable architecture that applies Retrieval Augmented Generation (RAG) to video libraries. We propose a system that uses large language models (LLMs) to generate search queries, retrieving relevant video moments indexed by speech and visual metadata. An answer generation module then integrates user queries with this metadata to produce responses with specific video timestamps. This approach shows promise in multimedia content retrieval, and AI-assisted video content creation.

------------

`[2406.14972] A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems <https://arxiv.org/abs/2406.14972>`__ 一个关于信任和准确性的故事:RAG系统中的Base与Instruct llm

::

    Fri, 21 Jun 2024 08:31:02 GMT
    Florin Cuconasu, Giovanni Trappolini, Nicola Tonellotto, Fabrizio Silvestri

Retrieval Augmented Generation (RAG) represents a significant advancement in artificial intelligence combining a retrieval phase with a generative phase, with the latter typically being powered by large language models (LLMs). The current common practices in RAG involve using "instructed" LLMs, which are fine-tuned with supervised training to enhance their ability to follow instructions and are aligned with human preferences using state-of-the-art techniques. Contrary to popular belief, our study demonstrates that base models outperform their instructed counterparts in RAG tasks by 20% on average under our experimental settings. This finding challenges the prevailing assumptions about the superiority of instructed LLMs in RAG applications. Further investigations reveal a more nuanced situation, questioning fundamental aspects of RAG and suggesting the need for broader discussions on the topic; or, as Fromm would have it, "Seldom is a glance at the statistics enough to understand the meaning of the figures".

------------

`[2406.15045] Harnessing Knowledge Retrieval with Large Language Models for Clinical Report Error Correction <https://arxiv.org/abs/2406.15045>`__ 利用大规模语言模型进行知识检索的临床报告纠错

::

    Fri, 21 Jun 2024 10:48:21 GMT
    Jinge Wu, Zhaolong Wu, Abul Hasan, Yunsoo Kim, Jason P.Y. Cheung, Teng Zhang, Honghan Wu

This study proposes an approach for error correction in clinical radiology reports, leveraging large language models (LLMs) and retrieval-augmented generation (RAG) techniques. The proposed framework employs internal and external retrieval mechanisms to extract relevant medical entities and relations from the report and external knowledge sources. A three-stage inference process is introduced, decomposing the task into error detection, localization, and correction subtasks, which enhances the explainability and performance of the system. The effectiveness of the approach is evaluated using a benchmark dataset created by corrupting real-world radiology reports with realistic errors, guided by domain experts. Experimental results demonstrate the benefits of the proposed methods, with the combination of internal and external retrieval significantly improving the accuracy of error detection, localization, and correction across various state-of-the-art LLMs. The findings contribute to the development of more robust and reliable error correction systems for clinical documentation.

------------

`[2406.15319] LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs <https://arxiv.org/abs/2406.15319>`__ longag:用长上下文llm增强检索增强生成

::

    Fri, 21 Jun 2024 17:23:21 GMT
    Ziyan Jiang, Xueguang Ma, Wenhu Chen

In traditional RAG framework, the basic retrieval units are normally short.
The common retrievers like DPR normally work with 100-word Wikipedia paragraphs. Such a design forces the retriever to search over a large corpus to find the `needle' unit. In contrast, the readers only need to extract answers from the short retrieved units. Such an imbalanced `heavy' retriever and `light' reader design can lead to sub-optimal performance. In order to alleviate the imbalance, we propose a new framework LongRAG, consisting of a `long retriever' and a `long reader'. LongRAG processes the entire Wikipedia into 4K-token units, which is 30x longer than before. By increasing the unit size, we significantly reduce the total units from 22M to 700K. This significantly lowers the burden of retriever, which leads to a remarkable retrieval score: answer recall@1=71% on NQ (previously 52%) and answer recall@2=72% (previously 47%) on HotpotQA (full-wiki). Then we feed the top-k retrieved units ($\approx$ 30K tokens) to an existing long-context LLM to perform zero-shot answer extraction. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ, which is the best known result. LongRAG also achieves 64.3% on HotpotQA (full-wiki), which is on par of the SoTA model. Our study offers insights into the future roadmap for combining RAG with long-context LLMs.

------------

`[2406.14764] RE-AdaptIR: Improving Information Retrieval through Reverse Engineered Adaptation <https://arxiv.org/abs/2406.14764>`__ RE-AdaptIR:通过逆向工程适应改进信息检索

::

    Thu, 20 Jun 2024 22:28:11 GMT
    William Fleshman and Benjamin Van Durme

Large language models (LLMs) fine-tuned for text-retrieval have demonstrated state-of-the-art results across several information retrieval (IR) benchmarks.
However, supervised training for improving these models requires numerous labeled examples, which are generally unavailable or expensive to acquire. In this work, we explore the effectiveness of extending reverse engineered adaptation to the context of information retrieval (RE-AdaptIR). We use RE-AdaptIR to improve LLM-based IR models using only unlabeled data. We demonstrate improved performance both in training domains as well as zero-shot in domains where the models have seen no queries. We analyze performance changes in various fine-tuning scenarios and offer findings of immediate use to practitioners.

------------

`[2406.14783] Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework <https://arxiv.org/abs/2406.14783>`__ 用RAGElo评估RAG-Fusion:一个基于elo的自动化框架

::

    Thu, 20 Jun 2024 23:20:34 GMT
    Zackary Rackauckas, Arthur C\^amara, Jakub Zavrel

Challenges in the automated evaluation of Retrieval-Augmented Generation (RAG) Question-Answering (QA) systems include hallucination problems in domain-specific knowledge and the lack of gold standard benchmarks for company internal tasks. This results in difficulties in evaluating RAG variations, like RAG-Fusion (RAGF), in the context of a product QA task at Infineon Technologies. To solve these problems, we propose a comprehensive evaluation framework, which leverages Large Language Models (LLMs) to generate large datasets of synthetic queries based on real user queries and in-domain documents, uses LLM-as-a-judge to rate retrieved documents and answers, evaluates the quality of answers, and ranks different variants of Retrieval-Augmented Generation (RAG) agents with RAGElo's automated Elo-based competition. LLM-as-a-judge rating of a random sample of synthetic queries shows a moderate, positive correlation with domain expert scoring in relevance, accuracy, completeness, and precision. While RAGF outperformed RAG in Elo score, a significance analysis against expert annotations also shows that RAGF significantly outperforms RAG in completeness, but underperforms in precision.
In addition, Infineon's RAGF assistant demonstrated slightly higher performance in document relevance based on MRR@5 scores. We find that RAGElo positively aligns with the preferences of human annotators, though due caution is still required. Finally, RAGF's approach leads to more complete answers based on expert annotations and better answers overall based on RAGElo's evaluation criteria.

------------

`[2311.17696] How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation <https://arxiv.org/abs/2311.17696>`__ 如何使用大型语言模型和检索增强生成构建一个能够适应任何课程并提供准确答案的AI导师

::

    replaced with revised version Fri, 21 Jun 2024 09:29:07 GMT
    Submission history From: Chenxi Dong [view email]
    [v1] Wed, 29 Nov 2023 15:02:46 UTC (1,141 KB)
    [v2] Thu, 30 Nov 2023 06:28:22 UTC (539 KB)
    [v3] Fri, 21 Jun 2024 09:29:07 UTC (966 KB)
    Chenxi Dong

This paper proposes a low-code solution to build an AI tutor that leverages advanced AI techniques to provide accurate and contextually relevant responses in a personalized learning environment. The OpenAI Assistants API allows AI Tutor to easily embed, store, retrieve, and manage files and chat history, enabling a low-code solution. Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) technology generate sophisticated answers based on course-specific materials. The application efficiently organizes and retrieves relevant information through vector embedding and similarity-based retrieval algorithms. The AI Tutor prototype demonstrates its ability to generate relevant, accurate answers with source citations. It represents a significant advancement in technology-enhanced tutoring systems, democratizing access to high-quality, customized educational support in higher education.

------------

----------
Agent (10)
----------

`[2406.14928] Autonomous Agents for Collaborative Task under Information Asymmetry <https://arxiv.org/abs/2406.14928>`__ 信息不对称下的协作任务自治agent

::

    Fri, 21 Jun 2024 07:37:19 GMT
    Wei Liu, Chenxi Wang, Yifei Wang, Zihao Xie, Rennai Qiu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Chen Qian

Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great progress in solving complex tasks. It performs communication among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' communication is leveraged to enhance human cooperation, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents' communication towards effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes.

------------

`[2406.14711] MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate <https://arxiv.org/abs/2406.14711>`__ 多智能体协作攻击:基于辩论的大型语言模型协作对抗攻击研究

::

    Thu, 20 Jun 2024 20:09:37 GMT
    Alfonso Amayuelas, Xianjun Yang, Antonis Antoniades, Wenyue Hua, Liangming Pan, William Wang

Large Language Models (LLMs) have shown exceptional results on current benchmarks when working individually. The advancement in their capabilities, along with a reduction in parameter size and inference times, has facilitated the use of these models as agents, enabling interactions among multiple models to execute complex tasks. Such collaborations offer several advantages, including the use of specialized models (e.g. coding), improved confidence through multiple computations, and enhanced divergent thinking, leading to more diverse outputs. Thus, the collaborative use of language models is expected to grow significantly in the coming years. In this work, we evaluate the behavior of a network of models collaborating through debate under the influence of an adversary. We introduce pertinent metrics to assess the adversary's effectiveness, focusing on system accuracy and model agreement. Our findings highlight the importance of a model's persuasive ability in influencing others.
Additionally, we explore inference-time methods to generate more compelling arguments and evaluate the potential of prompt-based mitigation as a defensive strategy.

------------

`[2406.14868] Direct Multi-Turn Preference Optimization for Language Agents <https://arxiv.org/abs/2406.14868>`__ 

::

    Fri, 21 Jun 2024 05:13:20 GMT
    Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, Fuli Feng

Adapting Large Language Models (LLMs) for agent tasks is critical in developing language agents. Direct Preference Optimization (DPO) is a promising technique for this adaptation with the alleviation of compounding errors, offering a means to directly optimize Reinforcement Learning (RL) objectives.
However, applying DPO to multi-turn tasks presents challenges due to the inability to cancel the partition function. Overcoming this obstacle involves making the partition function independent of the current state and addressing length disparities between preferred and dis-preferred trajectories. In this light, we replace the policy constraint with the state-action occupancy measure constraint in the RL objective and add length normalization to the Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn agent tasks with theoretical explanations. Extensive experiments on three multi-turn agent task datasets confirm the effectiveness and superiority of the DMPO loss.

------------

`[2406.14884] FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents <https://arxiv.org/abs/2406.14884>`__ FlowBench:基于llm代理的工作流指导规划的重新审视和基准测试

::

    Fri, 21 Jun 2024 06:13:00 GMT
    Ruixuan Xiao, Wentao Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang, Fei Huang, Yongbin Li

LLM-based agents have emerged as promising tools, which are crafted to fulfill complex tasks by iterative planning and action. However, these agents are susceptible to undesired planning hallucinations when lacking specific knowledge for expertise-intensive tasks. To address this, preliminary attempts are made to enhance planning reliability by incorporating external workflow-related knowledge. Despite the promise, such infused knowledge is mostly disorganized and diverse in formats, lacking rigorous formalization and comprehensive comparisons. Motivated by this, we formalize different formats of workflow knowledge and present FlowBench, the first benchmark for workflow-guided planning. FlowBench covers 51 different scenarios from 6 domains, with knowledge presented in diverse formats. To assess different LLMs on FlowBench, we design a multi-tiered evaluation framework. We evaluate the efficacy of workflow knowledge across multiple formats, and the results indicate that current LLM agents need considerable improvements for satisfactory planning. We hope that our challenging benchmark can pave the way for future agent planning research.

------------

`[2406.14596] ICAL: Continual Learning of Multimodal Agents by Transforming Trajectories into Actionable Insights <https://arxiv.org/abs/2406.14596>`__ ICAL:通过将轨迹转化为可操作见解的多模态智能体持续学习

::

    Thu, 20 Jun 2024 17:45:02 GMT
    Gabriel Sarch and Lawrence Jang and Michael J. Tarr and William W. Cohen and Kenneth Marino and Katerina Fragkiadaki

Large-scale generative language and vision-language models (LLMs and VLMs) excel in few-shot in-context learning for decision making and instruction following. However, they require high-quality exemplar demonstrations to be included in their context window. In this work, we ask: Can LLMs and VLMs generate their own prompt examples from generic, sub-optimal demonstrations? We propose In-Context Abstraction Learning (ICAL), a method that builds a memory of multimodal experience insights from sub-optimal demonstrations and human feedback. Given a noisy demonstration in a new domain, VLMs abstract the trajectory into a general program by fixing inefficient actions and annotating cognitive abstractions: task relationships, object state changes, temporal subgoals, and task construals. These abstractions are refined and adapted interactively through human feedback while the agent attempts to execute the trajectory in a similar environment. The resulting abstractions, when used as exemplars in the prompt, significantly improve decision-making in retrieval-augmented LLM and VLM agents. Our ICAL agent surpasses the state-of-the-art in dialogue-based instruction following in TEACh, multimodal web agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6% improvement in goal-condition success. In VisualWebArena, our task success rate improves over the SOTA from 14.3% to 22.7%. In Ego4D action forecasting, we improve over few-shot GPT-4V and remain competitive with supervised models. We show finetuning our retrieval-augmented in-context agent yields additional improvements. Our approach significantly reduces reliance on expert-crafted examples and consistently outperforms in-context learning from action plans that lack such insights.

------------

`[2403.06294] ArgMed-Agents: Explainable Clinical Decision Reasoning with LLM Disscusion via Argumentation Schemes <https://arxiv.org/abs/2403.06294>`__ 论证主体:基于论证方案的可解释临床决策推理

::

    replaced with revised version Thu, 20 Jun 2024 21:57:15 GMT
    Submission history From: Shengxin Hong [view email]
    [v1] Sun, 10 Mar 2024 19:47:00 UTC (906 KB)
    [v2] Thu, 20 Jun 2024 21:57:15 UTC (972 KB)
    Shengxin Hong, Liang Xiao, Xin Zhang, Jianxia Chen

There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Discussion (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, use symbolic solver to identify a series of rational and coherent arguments to support decision. We construct a formal model of ArgMed-Agents and present conjectures for theoretical guarantees. ArgMed-Agents enables LLMs to mimic the process of clinical argumentative reasoning by generating explanations of reasoning in a self-directed manner. The setup experiments show that ArgMed-Agents not only improves accuracy in complex clinical decision reasoning problems compared to other prompt methods, but more importantly, it provides users with decision explanations that increase their confidence.

------------

`[2405.16334] Devil's Advocate: Anticipatory Reflection for LLM Agents <https://arxiv.org/abs/2405.16334>`__ 魔鬼代言人:LLM代理的预期反思

::

    replaced with revised version Thu, 20 Jun 2024 19:41:48 GMT
    Submission history From: Tao Li [view email]
    [v1] Sat, 25 May 2024 19:20:15 UTC (1,067 KB)
    [v2] Tue, 28 May 2024 03:22:44 UTC (1,083 KB)
    [v3] Wed, 29 May 2024 14:12:53 UTC (1,067 KB)
    [v4] Thu, 20 Jun 2024 19:41:48 UTC (2,092 KB)
    Haoyu Wang and Tao Li and Zhiwei Deng and Dan Roth and Yang Li

In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks. Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions. %; and when necessary, to explore ``the road not taken.'' We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement. By deploying and experimenting with this methodology -- a zero-shot approach -- within WebArena for practical tasks in web environments, our agent demonstrates superior performance with a success rate of 23.5% over existing zero-shot methods by 3.5%. The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions by 45% needed to achieve a task.

------------

`[2310.12821] GestureGPT: Toward Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents <https://arxiv.org/abs/2310.12821>`__ GestureGPT:基于大型语言模型智能体的零样本交互式手势理解和基础

::

    replaced with revised version Fri, 21 Jun 2024 10:12:41 GMT
    Submission history From: Xin Zeng [view email]
    [v1] Thu, 19 Oct 2023 15:17:34 UTC (17,523 KB)
    [v2] Fri, 20 Oct 2023 04:13:17 UTC (17,523 KB)
    [v3] Mon, 30 Oct 2023 03:04:07 UTC (17,523 KB)
    [v4] Fri, 21 Jun 2024 10:12:41 UTC (5,690 KB)
    Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, Yiqiang Chen

Current gesture interfaces typically demand users to learn and perform gestures from a predefined set, which leads to a less natural experience. Interfaces supporting user-defined gestures eliminate the learning process, but users still need to demonstrate and associate the gesture to a specific system function themselves. We introduce GestureGPT, a free-form hand gesture understanding framework that does not require users to learn, demonstrate, or associate gestures. Our framework leverages the large language model's (LLM) astute common sense and strong inference ability to understand a spontaneously performed gesture from its natural language descriptions, and automatically maps it to a function provided by the interface. More specifically, our triple-agent framework involves a Gesture Description Agent that automatically segments and formulates natural language descriptions of hand poses and movements based on hand landmark coordinates. The description is deciphered by a Gesture Inference Agent through self-reasoning and querying about the interaction context (e.g., interaction history, gaze data), which a Context Management Agent organizes and provides. Following iterative exchanges, the Gesture Inference Agent discerns user intent, grounding it to an interactive function. We validated our conceptual framework under two real-world scenarios: smart home controlling and online video streaming. The average zero-shot Top-5 grounding accuracies are 83.59% for smart home tasks and 73.44% for video streaming. We also provided an extensive discussion of our framework including model selection rationale, generated description quality, generalizability etc.

------------

`[2406.10478] From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent <https://arxiv.org/abs/2406.10478>`__ 从文字到世界:用交流的LLM Agent将单行提示转换为沉浸式多模态数字故事

::

    replaced with revised version Fri, 21 Jun 2024 08:09:17 GMT
    Submission history From: Danrui Li [view email]
    [v1] Sat, 15 Jun 2024 03:03:43 UTC (42,822 KB)
    [v2] Fri, 21 Jun 2024 08:09:17 UTC (42,822 KB)
    Samuel S. Sohn and Danrui Li and Sen Zhang and Che-Jui Chang and Mubbasir Kapadia

Digital storytelling, essential in entertainment, education, and marketing, faces challenges in production scalability and flexibility. The StoryAgent framework, introduced in this paper, utilizes Large Language Models and generative tools to automate and refine digital storytelling. Employing a top-down story drafting and bottom-up asset generation approach, StoryAgent tackles key issues such as manual intervention, interactive scene orchestration, and narrative consistency. This framework enables efficient production of interactive and consistent narratives across multiple modalities, democratizing content creation and enhancing engagement. Our results demonstrate the framework's capability to produce coherent digital stories without reference videos, marking a significant advancement in automated digital storytelling.

------------

`[2406.01893] Large Language Model-Enabled Multi-Agent Manufacturing Systems <https://arxiv.org/abs/2406.01893>`__ 大型语言模型支持的多智能体制造系统

::

    replaced with revised version Fri, 21 Jun 2024 14:54:46 GMT
    Submission history From: Jonghan Lim [view email]
    [v1] Tue, 4 Jun 2024 01:57:37 UTC (577 KB)
    [v2] Fri, 21 Jun 2024 14:54:46 UTC (580 KB)
    Jonghan Lim, Birgit Vogel-Heuser, Ilya Kovalenko

Traditional manufacturing faces challenges adapting to dynamic environments and quickly responding to manufacturing changes. The use of multi-agent systems has improved adaptability and coordination but requires further advancements in rapid human instruction comprehension, operational adaptability, and coordination through natural language integration. Large language models like GPT-3.5 and GPT-4 enhance multi-agent manufacturing systems by enabling agents to communicate in natural language and interpret human instructions for decision-making. This research introduces a novel framework where large language models enhance the capabilities of agents in manufacturing, making them more adaptable, and capable of processing context-specific instructions. A case study demonstrates the practical application of this framework, showing how agents can effectively communicate, understand tasks, and execute manufacturing processes, including precise G-code allocation among agents. The findings highlight the importance of continuous large language model integration into multi-agent manufacturing systems and the development of sophisticated agent communication protocols for a more flexible manufacturing system.

------------

----------
Other (92)
----------

`[2406.14598] SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors <https://arxiv.org/abs/2406.14598>`__ SORRY-Bench:系统评估大型语言模型的安全拒绝行为

::

    Thu, 20 Jun 2024 17:56:07 GMT
    Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, Ruoxi Jia, Bo Li, Kai Li, Danqi Chen, Peter Henderson, Prateek Mittal

Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments.
Existing evaluation efforts, however, face three limitations that we address with SORRY-Bench, our proposed benchmark. First, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. SORRY-Bench improves on this by using a fine-grained taxonomy of 45 potentially unsafe topics, and 450 class-balanced unsafe instructions, compiled through human-in-the-loop methods.
Second, linguistic characteristics and formatting of prompts are often overlooked, like different languages, dialects, and more -- which are only implicitly considered in many evaluations. We supplement SORRY-Bench with 20 diverse linguistic augmentations to systematically examine these effects.
Third, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs, we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost. Putting these together, we evaluate over 40 proprietary and open-source LLMs on SORRY-Bench, analyzing their distinctive refusal behaviors. We hope our effort provides a building block for systematic evaluations of LLMs' safety refusal capabilities, in a balanced, granular, and efficient manner.

------------

`[2406.14701] Speech Prefix-Tuning with RNNT Loss for Improving LLM Predictions <https://arxiv.org/abs/2406.14701>`__ 基于RNNT损失的语音前缀微调以改善LLM预测

::

    Thu, 20 Jun 2024 19:50:49 GMT
    Murali Karthick Baskar, Andrew Rosenberg, Bhuvana Ramabhadran, Neeraj Gaur and Zhong Meng

In this paper, we focus on addressing the constraints faced when applying LLMs to ASR. Recent works utilize prefixLM-type models, which directly apply speech as a prefix to LLMs for ASR. We have found that optimizing speech prefixes leads to better ASR performance and propose applying RNNT loss to perform speech prefix-tuning. This is a simple approach and does not increase the model complexity or alter the inference pipeline. We also propose language-based soft prompting to further improve with frozen LLMs. Empirical analysis on realtime testset from 10 Indic languages demonstrate that our proposed speech prefix-tuning yields improvements with both frozen and fine-tuned LLMs. Our recognition results on an average of 10 Indics show that the proposed prefix-tuning with RNNT loss results in a 12\% relative improvement in WER over the baseline with a fine-tuned LLM. Our proposed approches with the frozen LLM leads to a 31\% relative improvement over basic soft-prompting prefixLM.

------------

`[2406.14722] Does GPT Really Get It? A Hierarchical Scale to Quantify Human vs AI's Understanding of Algorithms <https://arxiv.org/abs/2406.14722>`__ GPT真的能理解吗?一个层级尺度来量化人类vs AI对算法的理解

::

    Thu, 20 Jun 2024 20:37:55 GMT
    Mirabel Reid, Santosh S. Vempala

As Large Language Models (LLMs) perform (and sometimes excel at) more and more complex cognitive tasks, a natural question is whether AI really understands. The study of understanding in LLMs is in its infancy, and the community has yet to incorporate well-trodden research in philosophy, psychology, and education. We initiate this, specifically focusing on understanding algorithms, and propose a hierarchy of levels of understanding.
We use the hierarchy to design and conduct a study with human subjects (undergraduate and graduate students) as well as large language models (generations of GPT), revealing interesting similarities and differences. We expect that our rigorous criteria will be useful to keep track of AI's progress in such cognitive domains.

------------

`[2406.14757] A Large Language Model Outperforms Other Computational Approaches to the High-Throughput Phenotyping of Physician Notes <https://arxiv.org/abs/2406.14757>`__ 在医生病历的高通量表型方面，大型语言模型的性能优于其他计算方法

::

    Thu, 20 Jun 2024 22:05:34 GMT
    Syed I. Munzir, Daniel B. Hier, Chelsea Oommen, Michael D. Carrithers

High-throughput phenotyping, the automated mapping of patient signs and symptoms to standardized ontology concepts, is essential to gaining value from electronic health records (EHR) in the support of precision medicine. Despite technological advances, high-throughput phenotyping remains a challenge. This study compares three computational approaches to high-throughput phenotyping: a Large Language Model (LLM) incorporating generative AI, a Natural Language Processing (NLP) approach utilizing deep learning for span categorization, and a hybrid approach combining word vectors with machine learning. The approach that implemented GPT-4 (a Large Language Model) demonstrated superior performance, suggesting that Large Language Models are poised to be the preferred method for high-throughput phenotyping of physician notes.

------------

`[2406.14769] How critically can an AI think? A framework for evaluating the quality of thinking of generative artificial intelligence <https://arxiv.org/abs/2406.14769>`__ 

::

    Thu, 20 Jun 2024 22:46:56 GMT
    Luke Zaphir, Jason M. Lodge, Jacinta Lisec, Dom McGrath, Hassan Khosravi

Generative AI such as those with large language models have created opportunities for innovative assessment design practices. Due to recent technological developments, there is a need to know the limits and capabilities of generative AI in terms of simulating cognitive skills. Assessing student critical thinking skills has been a feature of assessment for time immemorial, but the demands of digital assessment create unique challenges for equity, academic integrity and assessment authorship. Educators need a framework for determining their assessments vulnerability to generative AI to inform assessment design practices. This paper presents a framework that explores the capabilities of the LLM ChatGPT4 application, which is the current industry benchmark. This paper presents the Mapping of questions, AI vulnerability testing, Grading, Evaluation (MAGE) framework to methodically critique their assessments within their own disciplinary contexts. This critique will provide specific and targeted indications of their questions vulnerabilities in terms of the critical thinking skills. This can go on to form the basis of assessment design for their tasks.

------------

`[2406.14903] GIEBench: Towards Holistic Evaluation of Group Indentity-based Empathy for Large Language Models <https://arxiv.org/abs/2406.14903>`__ GIEBench:基于群体身份的大型语言模型共情的整体评估

::

    Fri, 21 Jun 2024 06:50:42 GMT
    Leyan Wang, Yonggang Jin, Tianhao Shen, Tianyu Zheng, Xinrun Du, Chenchen Zhang, Wenhao Huang, Jiaheng Liu, Shi Wang, Ge Zhang, Liuyu Xiang, Zhaofeng He

As large language models (LLMs) continue to develop and gain widespread application, the ability of LLMs to exhibit empathy towards diverse group identities and understand their perspectives is increasingly recognized as critical. Most existing benchmarks for empathy evaluation of LLMs focus primarily on universal human emotions, such as sadness and pain, often overlooking the context of individuals' group identities. To address this gap, we introduce GIEBench, a comprehensive benchmark that includes 11 identity dimensions, covering 97 group identities with a total of 999 single-choice questions related to specific group identities. GIEBench is designed to evaluate the empathy of LLMs when presented with specific group identities such as gender, age, occupation, and race, emphasizing their ability to respond from the standpoint of the identified group. This supports the ongoing development of empathetic LLM applications tailored to users with different identities. Our evaluation of 23 LLMs revealed that while these LLMs understand different identity standpoints, they fail to consistently exhibit equal empathy across these identities without explicit instructions to adopt those perspectives.
This highlights the need for improved alignment of LLMs with diverse values to better accommodate the multifaceted nature of human identities. Our datasets are available at https://github.com/GIEBench/GIEBench.

------------

`[2406.14917] LLM2FEA: Discover Novel Designs with Generative Evolutionary Multitasking <https://arxiv.org/abs/2406.14917>`__ LLM2FEA:基于生成进化多任务处理的新颖设计

::

    Fri, 21 Jun 2024 07:20:51 GMT
    Melvin Wong, Jiao Liu, Thiago Rios, Stefan Menzel, Yew Soon Ong

The rapid research and development of generative artificial intelligence has enabled the generation of high-quality images, text, and 3D models from text prompts. This advancement impels an inquiry into whether these models can be leveraged to create digital artifacts for both creative and engineering applications. Drawing on innovative designs from other domains may be one answer to this question, much like the historical practice of ``bionics", where humans have sought inspiration from nature's exemplary designs. This raises the intriguing possibility of using generative models to simultaneously tackle design tasks across multiple domains, facilitating cross-domain learning and resulting in a series of innovative design solutions. In this paper, we propose LLM2FEA as the first attempt to discover novel designs in generative models by transferring knowledge across multiple domains. By utilizing a multi-factorial evolutionary algorithm (MFEA) to drive a large language model, LLM2FEA integrates knowledge from various fields to generate prompts that guide the generative model in discovering novel and practical objects. Experimental results in the context of 3D aerodynamic design verify the discovery capabilities of the proposed LLM2FEA. The designs generated by LLM2FEA not only satisfy practicality requirements to a certain degree but also feature novel and aesthetically pleasing shapes, demonstrating the potential applications of LLM2FEA in discovery tasks.

------------

`[2406.14981] Human-AI collectives produce the most accurate differential diagnoses <https://arxiv.org/abs/2406.14981>`__ 人类- ai集体产生最准确的鉴别诊断

::

    Fri, 21 Jun 2024 08:46:30 GMT
    N. Z\"oller, J. Berger, I. Lin, N. Fu, J. Komarneni, G. Barabucci, K. Laskowski, V. Shia, B. Harack, E. A. Chu, V. Trianni, R. H.J.M. Kurvers, S. M. Herzog

Artificial intelligence systems, particularly large language models (LLMs), are increasingly being employed in high-stakes decisions that impact both individuals and society at large, often without adequate safeguards to ensure safety, quality, and equity. Yet LLMs hallucinate, lack common sense, and are biased - shortcomings that may reflect LLMs' inherent limitations and thus may not be remedied by more sophisticated architectures, more data, or more human feedback. Relying solely on LLMs for complex, high-stakes decisions is therefore problematic. Here we present a hybrid collective intelligence system that mitigates these risks by leveraging the complementary strengths of human experience and the vast information processed by LLMs. We apply our method to open-ended medical diagnostics, combining 40,762 differential diagnoses made by physicians with the diagnoses of five state-of-the art LLMs across 2,133 medical cases. We show that hybrid collectives of physicians and LLMs outperform both single physicians and physician collectives, as well as single LLMs and LLM ensembles. This result holds across a range of medical specialties and professional experience, and can be attributed to humans' and LLMs' complementary contributions that lead to different kinds of errors. Our approach highlights the potential for collective human and machine intelligence to improve accuracy in complex, open-ended domains like medical diagnostics.

------------

`[2406.14986] Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers <https://arxiv.org/abs/2406.14986>`__ 

::

    Fri, 21 Jun 2024 08:56:35 GMT
    Manuel Mondal, Ljiljana Dolamic, G\'er\^ome Bovet, Philippe Cudr\'e-Mauroux

Prompting and Multiple Choices Questions (MCQ) have become the preferred approach to assess the capabilities of Large Language Models (LLMs), due to their ease of manipulation and evaluation. Such experimental appraisals have pointed toward the LLMs' apparent ability to perform causal reasoning or to grasp uncertainty. In this paper, we investigate whether these abilities are measurable outside of tailored prompting and MCQ by reformulating these issues as direct text completion - the foundation of LLMs. To achieve this goal, we define scenarios with multiple possible outcomes and we compare the prediction made by the LLM through prompting (their Stated Answer) to the probability distributions they compute over these outcomes during next token prediction (their Revealed Belief). Our findings suggest that the Revealed Belief of LLMs significantly differs from their Stated Answer and hint at multiple biases and misrepresentations that their beliefs may yield in many scenarios and outcomes.
As text completion is at the core of LLMs, these results suggest that common evaluation methods may only provide a partial picture and that more research is needed to assess the extent and nature of their capabilities.

------------

`[2406.15198] Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms <https://arxiv.org/abs/2406.15198>`__ 探讨ChatGPT和Claude机器人助手在加强ADHD治疗中的功效:创新治疗范式

::

    Fri, 21 Jun 2024 14:38:25 GMT
    Santiago Berrezueta-Guzman, Mohanad Kandil, Mar\'ia-Luisa Mart\'in-Ruiz, Iv\'an Pau-de-la-Cruz, Stephan Krusche

Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental condition characterized by inattention, hyperactivity, and impulsivity, which can significantly impact an individual's daily functioning and quality of life.
Occupational therapy plays a crucial role in managing ADHD by fostering the development of skills needed for daily living and enhancing an individual's ability to participate fully in school, home, and social situations. Recent studies highlight the potential of integrating Large Language Models (LLMs) like ChatGPT and Socially Assistive Robots (SAR) to improve psychological treatments. This integration aims to overcome existing limitations in mental health therapy by providing tailored support and adapting to the unique needs of this sensitive group. However, there remains a significant gap in research exploring the combined use of these advanced technologies in ADHD therapy, suggesting an opportunity for novel therapeutic approaches.
Thus, we integrated two advanced language models, ChatGPT-4 Turbo and Claude-3 Opus, into a robotic assistant to explore how well each model performs in robot-assisted interactions. Additionally, we have compared their performance in a simulated therapy scenario to gauge their effectiveness against a clinically validated customized model. The results of this study show that ChatGPT-4 Turbo excelled in performance and responsiveness, making it suitable for time-sensitive applications. Claude-3 Opus, on the other hand, showed strengths in understanding, coherence, and ethical considerations, prioritizing safe and engaging interactions. Both models demonstrated innovation and adaptability, but ChatGPT-4 Turbo offered greater ease of integration and broader language support. The selection between them hinges on the specific demands of ADHD therapy.

------------

`[2406.15325] Bug In the Code Stack: Can LLMs Find Bugs in Large Python Code Stacks <https://arxiv.org/abs/2406.15325>`__ 代码栈中的Bug: llm能否在大型Python代码栈中找到Bug

::

    Fri, 21 Jun 2024 17:37:10 GMT
    Hokyung Lee, Sumanyu Sharma, Bing Hu

Recent research in Needle-in-a-Haystack (NIAH) benchmarks has explored the capabilities of Large Language Models (LLMs) in retrieving contextual information from large text documents. However, as LLMs become increasingly integrated into software development processes, it is crucial to evaluate their performance in code-based environments. As LLMs are further developed for program synthesis, we need to ensure that LLMs can understand syntax and write syntactically correct code. As a step in ensuring LLMs understand syntax, LLMs can be evaluated in their ability to find and detect syntax bugs. Our benchmark, Bug In The Code Stack (BICS), is designed to assess the ability of LLMs to identify simple syntax bugs within large source code. Our findings reveal three key insights: (1) code-based environments pose significantly more challenge compared to text-based environments for retrieval tasks, (2) there is a substantial performance disparity among different models, and (3) there is a notable correlation between longer context lengths and performance degradation, though the extent of this degradation varies between models.

------------

`[2406.15330] Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance <https://arxiv.org/abs/2406.15330>`__ 梯度掩码调优提高了LLM性能的上限

::

    Fri, 21 Jun 2024 17:42:52 GMT
    Haoling Li, Xin Zhang, Xiao Liu, Yeyun Gong, Yifan Wang, Yujiu Yang, Qi Chen, Peng Cheng

Large language models (LLMs) have revolutionized lots of fields of research.
Although it is well-known that fine-tuning is essential for enhancing the capabilities of LLMs, existing research suggests that there is potential redundancy in the fine-tuning process and therefore proposes to update only a subset of parameters. However, these methods fail to leverage the task-specific information to identify important parameters during training. Based on the insight that gradients inherently contain information on task-specific data, we propose Gradient-Mask Tuning (GMT), a method that selectively updates parameters during training based on their gradient information. Specifically, we compute the absolute values of the gradients and apply masking to those with relatively smaller magnitudes. Our empirical results across various tasks demonstrate that GMT not only outperforms traditional fine-tuning methods but also elevates the upper limits of LLM performance. Further analysis indicates that GMT exhibits insensitivity to mask ratio and possesses computational efficiency comparable to vanilla SFT.

------------

`[2406.14629] Can LLMs Learn by Teaching? A Preliminary Study <https://arxiv.org/abs/2406.14629>`__ LLMs可以通过教学来学习吗?初步研究

::

    Thu, 20 Jun 2024 18:00:17 GMT
    Xuefei Ning, Zifu Wang, Shiyao Li, Zinan Lin, Peiran Yao, Tianyu Fu, Matthew B. Blaschko, Guohao Dai, Huazhong Yang, Yu Wang

Teaching to improve student models (e.g., knowledge distillation) is an extensively studied methodology in LLMs. However, for humans, teaching not only improves students but also improves teachers. We ask: Can LLMs also learn by teaching (LbT)? If yes, we can potentially unlock the possibility of continuously advancing the models without solely relying on human-produced data or stronger models. In this paper, we provide a preliminary exploration of this ambitious agenda. We show that LbT ideas can be incorporated into existing LLM training/prompting pipelines and provide noticeable improvements. Specifically, we design three methods, each mimicking one of the three levels of LbT in humans: observing students' feedback, learning from the feedback, and learning iteratively, with the goals of improving answer accuracy without training and improving models' inherent capability with fine-tuning. The findings are encouraging. For example, similar to LbT in human, we see that: (1) LbT can induce weak-to-strong generalization: strong models can improve themselves by teaching other weak models; (2) Diversity in students might help: teaching multiple students could be better than teaching one student or the teacher itself. We hope that this early promise can inspire future research on LbT and more broadly adopting the advanced techniques in education to improve LLMs. The code is available at https://github.com/imagination-research/lbt.

------------

`[2406.14654] Major Entity Identification: A Generalizable Alternative to Coreference Resolution <https://arxiv.org/abs/2406.14654>`__ 主要实体识别:共指消解的可泛化替代方案

::

    Thu, 20 Jun 2024 18:17:58 GMT
    Kawshik Manikantan (1), Shubham Toshniwal (2), Makarand Tapaswi (1), Vineet Gandhi (1) ((1) CVIT, IIIT Hyderabad, (2) NVIDIA)

The limited generalization of coreference resolution (CR) models has been a major bottleneck in the task's broad application. Prior work has identified annotation differences, especially for mention detection, as one of the main reasons for the generalization gap and proposed using additional annotated target domain data. Rather than relying on this additional annotation, we propose an alternative formulation of the CR task, Major Entity Identification (MEI), where we: (a) assume the target entities to be specified in the input, and (b) limit the task to only the frequent entities. Through extensive experiments, we demonstrate that MEI models generalize well across domains on multiple datasets with supervised models and LLM-based few-shot prompting.
Additionally, the MEI task fits the classification framework, which enables the use of classification-based metrics that are more robust than the current CR metrics. Finally, MEI is also of practical use as it allows a user to search for all mentions of a particular entity or a group of entities of interest.

------------

`[2406.14657] OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset <https://arxiv.org/abs/2406.14657>`__ OpenDebateEvidence:大规模论据挖掘和摘要数据集

::

    Thu, 20 Jun 2024 18:22:59 GMT
    Allen Roush, Yusuf Shabazz, Arvind Balaji, Peter Zhang, Stefano Mezza, Markus Zhang, Sanjay Basu, Sriram Vishwanath, Mehdi Fatemi, Ravid Schwartz-Ziv

We introduce OpenDebateEvidence, a comprehensive dataset for argument mining and summarization sourced from the American Competitive Debate community. This dataset includes over 3.5 million documents with rich metadata, making it one of the most extensive collections of debate evidence. OpenDebateEvidence captures the complexity of arguments in high school and college debates, providing valuable resources for training and evaluation. Our extensive experiments demonstrate the efficacy of fine-tuning state-of-the-art large language models for argumentative abstractive summarization across various methods, models, and datasets. By providing this comprehensive resource, we aim to advance computational argumentation and support practical applications for debaters, educators, and researchers. OpenDebateEvidence is publicly available to support further research and innovation in computational argumentation.
Access it here: https://huggingface.co/datasets/Yusuf5/OpenCaselist

------------

`[2406.14670] Exploring Design Choices for Building Language-Specific LLMs <https://arxiv.org/abs/2406.14670>`__ 

::

    Thu, 20 Jun 2024 18:47:43 GMT
    Atula Tejaswi, Nilesh Gupta, Eunsol Choi

Despite rapid progress in large language models (LLMs), their performance on a vast majority of languages remain unsatisfactory. In this paper, we study building language-specific LLMs by adapting monolingual and multilingual LLMs.
We conduct systematic experiments on how design choices (base model selection, vocabulary extension, and continued fine-tuning) impact the adapted LLM, both in terms of efficiency (how many tokens are needed to encode the same amount of information) and end task performance. We find that (1) the initial performance before the adaptation is not always indicative of the final performance. (2) Efficiency can easily improved with simple vocabulary extension and continued fine-tuning in most LLMs we study, and (3) The optimal adaptation method is highly language-dependent, and the simplest approach works well across various experimental settings. Adapting English-centric models can yield better results than adapting multilingual models despite their worse initial performance on low-resource languages. Together, our work lays foundations on efficiently building language-specific LLMs by adapting existing LLMs.

------------

`[2406.14673] Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell <https://arxiv.org/abs/2406.14673>`__ 对LLM长上下文故障的洞察:transformer何时知道但不告诉

::

    Thu, 20 Jun 2024 18:50:44 GMT
    Taiming Lu, Muhan Gao, Kuai Yu, Adam Byerly, Daniel Khashabi

Large Language Models (LLMs) exhibit positional bias, struggling to utilize information from the middle or end of long contexts. Our study explores LLMs' long-context reasoning by probing their hidden representations. We find that while LLMs encode the position of target information, they often fail to leverage this in generating accurate responses. This reveals a disconnect between information retrieval and utilization, a "know but don't tell" phenomenon. We further analyze the relationship between extraction time and final accuracy, offering insights into the underlying mechanics of transformer models.

------------

`[2406.14678] Bidirectional Transformer Representations of (Spanish) Ambiguous Words in Context: A New Lexical Resource and Empirical Analysis <https://arxiv.org/abs/2406.14678>`__ 语境中(西班牙语)歧义词的双向Transformer表示:一种新的词汇资源和实证分析

::

    Thu, 20 Jun 2024 18:58:11 GMT
    Pamela D. Rivi\`ere (1), Anne L. Beatty-Mart\'inez (1) and Sean Trott (1 and 2) ((1) Department of Cognitive Science UC San Diego, (2) Computational Social Science UC San Diego)

Lexical ambiguity -- where a single wordform takes on distinct, context-dependent meanings -- serves as a useful tool to compare across different large language models' (LLMs') ability to form distinct, contextualized representations of the same stimulus. Few studies have systematically compared LLMs' contextualized word embeddings for languages beyond English. Here, we evaluate multiple bidirectional transformers' (BERTs') semantic representations of Spanish ambiguous nouns in context. We develop a novel dataset of minimal-pair sentences evoking the same or different sense for a target ambiguous noun. In a pre-registered study, we collect contextualized human relatedness judgments for each sentence pair. We find that various BERT-based LLMs' contextualized semantic representations capture some variance in human judgments but fall short of the human benchmark, and for Spanish -- unlike English -- model scale is uncorrelated with performance. We also identify stereotyped trajectories of target noun disambiguation as a proportion of traversal through a given LLM family's architecture, which we partially replicate in English. We contribute (1) a dataset of controlled, Spanish sentence stimuli with human relatedness norms, and (2) to our evolving understanding of the impact that LLM specification (architectures, training protocols) exerts on contextualized embeddings.

------------

`[2406.14703] Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics <https://arxiv.org/abs/2406.14703>`__ 法学硕士有鲜明一致的个性吗?特质:为法学硕士设计的心理测量人格测试集

::

    Thu, 20 Jun 2024 19:50:56 GMT
    Seungbeen Lee, Seungwon Lim, Seungju Han, Giyeong Oh, Hyungjoo Chae, Jiwan Chung, Minju Kim, Beong-woo Kwak, Yeonsoo Lee, Dongha Lee, Jinyoung Yeo, Youngjae Yu

The idea of personality in descriptive psychology, traditionally defined through observable behavior, has now been extended to Large Language Models (LLMs) to better understand their behavior. This raises a question: do LLMs exhibit distinct and consistent personality traits, similar to humans? Existing self-assessment personality tests, while applicable, lack the necessary validity and reliability for precise personality measurements. To address this, we introduce TRAIT, a new tool consisting of 8K multi-choice questions designed to assess the personality of LLMs with validity and reliability. TRAIT is built on the psychometrically validated human questionnaire, Big Five Inventory (BFI) and Short Dark Triad (SD-3), enhanced with the ATOMIC10X knowledge graph for testing personality in a variety of real scenarios. TRAIT overcomes the reliability and validity issues when measuring personality of LLM with self-assessment, showing the highest scores across three metrics: refusal rate, prompt sensitivity, and option order sensitivity. It reveals notable insights into personality of LLM: 1) LLMs exhibit distinct and consistent personality, which is highly influenced by their training data (i.e., data used for alignment tuning), and 2) current prompting techniques have limited effectiveness in eliciting certain traits, such as high psychopathy or low conscientiousness, suggesting the need for further research in this direction.

------------

`[2406.14709] Factual Dialogue Summarization via Learning from Large Language Models <https://arxiv.org/abs/2406.14709>`__ 基于大型语言模型学习的事实对话摘要

::

    Thu, 20 Jun 2024 20:03:37 GMT
    Rongxin Zhu, Jey Han Lau, Jianzhong Qi

Factual consistency is an important quality in dialogue summarization. Large language model (LLM)-based automatic text summarization models generate more factually consistent summaries compared to those by smaller pretrained language models, but they face deployment challenges in real-world applications due to privacy or resource constraints. In this paper, we investigate the use of symbolic knowledge distillation to improve the factual consistency of smaller pretrained models for dialogue summarization. We employ zero-shot learning to extract symbolic knowledge from LLMs, generating both factually consistent (positive) and inconsistent (negative) summaries. We then apply two contrastive learning objectives on these summaries to enhance smaller summarization models.
Experiments with BART, PEGASUS, and Flan-T5 indicate that our approach surpasses strong baselines that rely on complex data augmentation strategies.
Our approach achieves better factual consistency while maintaining coherence, fluency, and relevance, as confirmed by various automatic evaluation metrics.
We also provide access to the data and code to facilitate future research.

------------

`[2406.14721] 1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators? <https://arxiv.org/abs/2406.14721>`__ 1+1>2:大型语言模型能否作为跨语言知识聚合器?

::

    Thu, 20 Jun 2024 20:32:53 GMT
    Yue Huang, Chenrui Fan, Yuan Li, Siyuan Wu, Tianyi Zhou, Xiangliang Zhang, Lichao Sun

Large Language Models (LLMs) have garnered significant attention due to their remarkable ability to process information across various languages. Despite their capabilities, they exhibit inconsistencies in handling identical queries in different languages, presenting challenges for further advancement. This paper introduces a method to enhance the multilingual performance of LLMs by aggregating knowledge from diverse languages. This approach incorporates a low-resource knowledge detector specific to a language, a language selection process, and mechanisms for answer replacement and integration. Our experiments demonstrate notable performance improvements, particularly in reducing language performance disparity. An ablation study confirms that each component of our method significantly contributes to these enhancements. This research highlights the inherent potential of LLMs to harmonize multilingual capabilities and offers valuable insights for further exploration.

------------

`[2406.14737] Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial Alterations to the False Belief Task? <https://arxiv.org/abs/2406.14737>`__ 

::

    Thu, 20 Jun 2024 21:02:30 GMT
    Zhiqiang Pi, Annapurna Vadaparty, Benjamin K. Bergen, and Cameron R. Jones

Recent empirical results have sparked a debate about whether or not Large Language Models (LLMs) are capable of Theory of Mind (ToM). While some have found LLMs to be successful on ToM evaluations such as the False Belief task (Kosinski, 2023), others have argued that LLMs solve these tasks by exploiting spurious correlations -- not representing beliefs -- since they fail on trivial alterations to these tasks (Ullman, 2023). In this paper, we introduce SCALPEL: a technique to generate targeted modifications for False Belief tasks to test different specific hypotheses about why LLMs fail. We find that modifications which make explicit common inferences -- such as that looking at a transparent object implies recognizing its contents -- preserve LLMs' performance. This suggests that LLMs' failures on modified ToM tasks could result from a lack of more general commonsense reasoning, rather than a failure to represent mental states. We argue that SCALPEL could be helpful for explaining LLM successes and failures in other cases.

------------

`[2406.14760] An LLM Feature-based Framework for Dialogue Constructiveness Assessment <https://arxiv.org/abs/2406.14760>`__ 基于LLM特征的对话建设性评估框架

::

    Thu, 20 Jun 2024 22:10:52 GMT
    Lexin Zhou, Youmna Farag and Andreas Vlachos

Research on dialogue constructiveness assessment focuses on (i) analysing conversational factors that influence individuals to take specific actions, win debates, change their perspectives or broaden their open-mindedness and (ii) predicting constructive outcomes following dialogues for such use cases. These objectives can be achieved by training either interpretable feature-based models (which often involve costly human annotations) or neural models such as pre-trained language models (which have empirically shown higher task accuracy but lack interpretability). We propose a novel LLM feature-based framework that combines the strengths of feature-based and neural approaches while mitigating their downsides, in assessing dialogue constructiveness. The framework first defines a set of dataset-independent and interpretable linguistic features, which can be extracted by both prompting an LLM and simple heuristics. Such features are then used to train LLM feature-based models. We apply this framework to three datasets of dialogue constructiveness and find that our LLM feature-based models significantly outperform standard feature-based models and neural models, and tend to learn more robust prediction rules instead of relying on superficial shortcuts (as seen with neural models). Further, we demonstrate that interpreting these LLM feature-based models can yield valuable insights into what makes a dialogue constructive.

------------

`[2406.14763] A Learn-Then-Reason Model Towards Generalization in Knowledge Base Question Answering <https://arxiv.org/abs/2406.14763>`__ 面向知识库问答泛化的“先学习后推理”模型

::

    Thu, 20 Jun 2024 22:22:41 GMT
    Lingxi Zhang and Jing Zhang and Yanling Wang and Cuiping Li and Hong Chen

Large-scale knowledge bases (KBs) like Freebase and Wikidata house millions of structured knowledge. Knowledge Base Question Answering (KBQA) provides a user-friendly way to access these valuable KBs via asking natural language questions. In order to improve the generalization capabilities of KBQA models, extensive research has embraced a retrieve-then-reason framework to retrieve relevant evidence for logical expression generation. These multi-stage efforts prioritize acquiring external sources but overlook the incorporation of new knowledge into their model parameters. In effect, even advanced language models and retrievers have knowledge boundaries, thereby limiting the generalization capabilities of previous KBQA models. Therefore, this paper develops KBLLaMA, which follows a learn-then-reason framework to inject new KB knowledge into a large language model for flexible end-to-end KBQA. At the core of KBLLaMA, we study (1) how to organize new knowledge about KBQA and (2) how to facilitate the learning of the organized knowledge. Extensive experiments on various KBQA generalization tasks showcase the state-of-the-art performance of KBLLaMA.
Especially on the general benchmark GrailQA and domain-specific benchmark Bio-chemical, KBLLaMA respectively derives a performance gain of up to 3.8% and 9.8% compared to the baselines.

------------

`[2406.14805] How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions <https://arxiv.org/abs/2406.14805>`__ llm在多大程度上代表了跨文化的价值观?基于Hofstede文化维度的LLM回应实证分析

::

    Fri, 21 Jun 2024 00:58:01 GMT
    Julia Kharchenko, Tanya Roosta, Aman Chadha, Chirag Shah

Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values.
However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user's known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs' cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.

------------

`[2406.14828] Word Matters: What Influences Domain Adaptation in Summarization? <https://arxiv.org/abs/2406.14828>`__ 词的重要性:什么影响摘要中的领域适应?

::

    Fri, 21 Jun 2024 02:15:49 GMT
    Yinghao Li, Siyu Miao, Heyan Huang, Yang Gao

Domain adaptation aims to enable Large Language Models (LLMs) to generalize domain datasets unseen effectively during the training phase. However, factors such as the size of the model parameters and the scale of training data are general influencers and do not reflect the nuances of domain adaptation performance. This paper investigates the fine-grained factors affecting domain adaptation performance, analyzing the specific impact of `words' in training data on summarization tasks. We propose quantifying dataset learning difficulty as the learning difficulty of generative summarization, which is determined by two indicators: word-based compression rate and abstraction level. Our experiments conclude that, when considering dataset learning difficulty, the cross-domain overlap and the performance gain in summarization tasks exhibit an approximate linear relationship, which is not directly related to the number of words. Based on this finding, predicting a model's performance on unknown domain datasets is possible without undergoing training.

------------

`[2406.14859] From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking <https://arxiv.org/abs/2406.14859>`__ 从llm到mllm:探索多模态越狱景观

::

    Fri, 21 Jun 2024 04:33:48 GMT
    Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei

The rapid development of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has exposed vulnerabilities to various adversarial attacks. This paper provides a comprehensive overview of jailbreaking research targeting both LLMs and MLLMs, highlighting recent advancements in evaluation benchmarks, attack techniques and defense strategies. Compared to the more advanced state of unimodal jailbreaking, multimodal domain remains underexplored. We summarize the limitations and potential research directions of multimodal jailbreaking, aiming to inspire future research and further enhance the robustness and security of MLLMs.

------------

`[2406.14877] Sports Intelligence: Assessing the Sports Understanding Capabilities of Language Models through Question Answering from Text to Video <https://arxiv.org/abs/2406.14877>`__ 运动智能:通过从文本到视频的问答评估语言模型的运动理解能力

::

    Fri, 21 Jun 2024 05:57:50 GMT
    Zhengbang Yang, Haotian Xia, Jingxi Li, Zezhi Chen, Zhuangdi Zhu, Weining Shen

Understanding sports is crucial for the advancement of Natural Language Processing (NLP) due to its intricate and dynamic nature. Reasoning over complex sports scenarios has posed significant challenges to current NLP technologies which require advanced cognitive capabilities. Toward addressing the limitations of existing benchmarks on sports understanding in the NLP field, we extensively evaluated mainstream large language models for various sports tasks. Our evaluation spans from simple queries on basic rules and historical facts to complex, context-specific reasoning, leveraging strategies from zero-shot to few-shot learning, and chain-of-thought techniques. In addition to unimodal analysis, we further assessed the sports reasoning capabilities of mainstream video language models to bridge the gap in multimodal sports understanding benchmarking. Our findings highlighted the critical challenges of sports understanding for NLP. We proposed a new benchmark based on a comprehensive overview of existing sports datasets and provided extensive error analysis which we hope can help identify future research priorities in this field.

------------

`[2406.14882] 70B-parameter large language models in Japanese medical question-answering <https://arxiv.org/abs/2406.14882>`__ 日语医疗问答中的70b参数大型语言模型

::

    Fri, 21 Jun 2024 06:04:10 GMT
    Issey Sukeda, Risa Kishikawa, Satoshi Kodera

Since the rise of large language models (LLMs), the domain adaptation has been one of the hot topics in various domains. Many medical LLMs trained with English medical dataset have made public recently. However, Japanese LLMs in medical domain still lack its research. Here we utilize multiple 70B-parameter LLMs for the first time and show that instruction tuning using Japanese medical question-answering dataset significantly improves the ability of Japanese LLMs to solve Japanese medical license exams, surpassing 50\% in accuracy. In particular, the Japanese-centric models exhibit a more significant leap in improvement through instruction tuning compared to their English-centric counterparts. This underscores the importance of continual pretraining and the adjustment of the tokenizer in our local language. We also examine two slightly different prompt formats, resulting in non-negligible performance improvement.

------------

`[2406.14883] OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants <https://arxiv.org/abs/2406.14883>`__ 誓言框架:与LLM助理描述对无家可归者的在线态度

::

    Fri, 21 Jun 2024 06:09:47 GMT
    Jaspreet Ranjit, Brihi Joshi, Rebecca Dorn, Laura Petry, Olga Koumoundouros, Jayne Bottarini, Peichen Liu, Eric Rice, Swabha Swayamdipta

Warning: Contents of this paper may be upsetting.
Public attitudes towards key societal issues, expressed on online media, are of immense value in policy and reform efforts, yet challenging to understand at scale. We study one such social issue: homelessness in the U.S., by leveraging the remarkable capabilities of large language models to assist social work experts in analyzing millions of posts from Twitter. We introduce a framing typology: Online Attitudes Towards Homelessness (OATH) Frames: nine hierarchical frames capturing critiques, responses and perceptions. We release annotations with varying degrees of assistance from language models, with immense benefits in scaling: 6.5x speedup in annotation time while only incurring a 3 point F1 reduction in performance with respect to the domain experts. Our experiments demonstrate the value of modeling OATH-Frames over existing sentiment and toxicity classifiers. Our large-scale analysis with predicted OATH-Frames on 2.4M posts on homelessness reveal key trends in attitudes across states, time periods and vulnerable populations, enabling new insights on the issue. Our work provides a general framework to understand nuanced public attitudes at scale, on issues beyond homelessness.

------------

`[2406.14887] InternLM-Law: An Open Source Chinese Legal Large Language Model <https://arxiv.org/abs/2406.14887>`__ InternLM-Law:一个开源的中文法律大型语言模型

::

    Fri, 21 Jun 2024 06:19:03 GMT
    Zhiwei Fei, Songyang Zhang, Xiaoyu Shen, Dawei Zhu, Xiao Wang, Maosong Cao, Fengzhe Zhou, Yining Li, Wenwei Zhang, Dahua Lin, Kai Chen, Jidong Ge

While large language models (LLMs) have showcased impressive capabilities, they struggle with addressing legal queries due to the intricate complexities and specialized expertise required in the legal field. In this paper, we introduce InternLM-Law, a specialized LLM tailored for addressing diverse legal queries related to Chinese laws, spanning from responding to standard legal questions (e.g., legal exercises in textbooks) to analyzing complex real-world legal situations. We meticulously construct a dataset in the Chinese legal domain, encompassing over 1 million queries, and implement a data filtering and processing pipeline to ensure its diversity and quality. Our training approach involves a novel two-stage process: initially fine-tuning LLMs on both legal-specific and general-purpose content to equip the models with broad knowledge, followed by exclusive fine-tuning on high-quality legal data to enhance structured output generation. InternLM-Law achieves the highest average performance on LawBench, outperforming state-of-the-art models, including GPT-4, on 13 out of 20 subtasks. We make InternLM-Law and our dataset publicly available to facilitate future research in applying LLMs within the legal domain.

------------

`[2406.14894] Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition <https://arxiv.org/abs/2406.14894>`__ 说到做到并不需要做到做到:论词汇蕴涵识别中的大型语言模型的限制

::

    Fri, 21 Jun 2024 06:30:16 GMT
    Candida M. Greco, Lucio La Cava, Andrea Tagarelli

Verbs form the backbone of language, providing the structure and meaning to sentences. Yet, their intricate semantic nuances pose a longstanding challenge.
Understanding verb relations through the concept of lexical entailment is crucial for comprehending sentence meanings and grasping verb dynamics. This work investigates the capabilities of eight Large Language Models in recognizing lexical entailment relations among verbs through differently devised prompting strategies and zero-/few-shot settings over verb pairs from two lexical databases, namely WordNet and HyperLex. Our findings unveil that the models can tackle the lexical entailment recognition task with moderately good performance, although at varying degree of effectiveness and under different conditions. Also, utilizing few-shot prompting can enhance the models' performance. However, perfectly solving the task arises as an unmet challenge for all examined LLMs, which raises an emergence for further research developments on this topic.

------------

`[2406.14952] ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models <https://arxiv.org/abs/2406.14952>`__ ESC-Eval:大型语言模型中情感支持对话的评估

::

    Fri, 21 Jun 2024 08:03:33 GMT
    Haiquan Zhao and Lingyu Li and Shisong Chen and Shuqi Kong and Jiaan Wang and Kexing Huang and Tianle Gu and Yixu Wang and Dandan Liang and Zhixu Li and Tan Teng and Yanghua Xiao and Yingchun Wang

Emotion Support Conversation (ESC) is a crucial application, which aims to reduce human stress, offer emotional guidance, and ultimately enhance human mental and physical well-being. With the advancement of Large Language Models (LLMs), many researchers have employed LLMs as the ESC models. However, the evaluation of these LLM-based ESCs remains uncertain. Inspired by the awesome development of role-playing agents, we propose an ESC Evaluation framework (ESC-Eval), which uses a role-playing agent to interact with ESC models, followed by a manual evaluation of the interactive dialogues. In detail, we first re-organize 2,801 role-playing cards from seven existing datasets to define the roles of the role-playing agent. Second, we train a specific role-playing model called ESC-Role which behaves more like a confused person than GPT-4. Third, through ESC-Role and organized role cards, we systematically conduct experiments using 14 LLMs as the ESC models, including general AI-assistant LLMs (ChatGPT) and ESC-oriented LLMs (ExTES-Llama). We conduct comprehensive human annotations on interactive multi-turn dialogues of different ESC models. The results show that ESC-oriented LLMs exhibit superior ESC abilities compared to general AI-assistant LLMs, but there is still a gap behind human performance. Moreover, to automate the scoring process for future ESC models, we developed ESC-RANK, which trained on the annotated data, achieving a scoring performance surpassing 35 points of GPT-4. Our data and code are available at https://github.com/haidequanbu/ESC-Eval.

------------

`[2406.14979] Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation <https://arxiv.org/abs/2406.14979>`__ 检索-计划-生成:一种面向知识密集型LLM生成的迭代规划和回答框架

::

    Fri, 21 Jun 2024 08:45:52 GMT
    Yuanjie Lyu, Zihan Niu, Zheyong Xie, Chao Zhang, Tong Xu, Yang Wang, Enhong Chen

Despite the significant progress of large language models (LLMs) in various tasks, they often produce factual errors due to their limited internal knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with external knowledge sources, offers a promising solution. However, these methods can be misled by irrelevant paragraphs in retrieved documents. Due to the inherent uncertainty in LLM generation, inputting the entire document may introduce off-topic information, causing the model to deviate from the central topic and affecting the relevance of the generated content. To address these issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates plan tokens to guide subsequent generation in the plan stage. In the answer stage, the model selects relevant fine-grained paragraphs based on the plan and uses them for further answer generation. This plan-answer process is repeated iteratively until completion, enhancing generation relevance by focusing on specific topics. To implement this framework efficiently, we utilize a simple but effective multi-task prompt-tuning method, enabling the existing LLMs to handle both planning and answering. We comprehensively compare RPG with baselines across 5 knowledge-intensive generation tasks, demonstrating the effectiveness of our approach.

------------

`[2406.14991] SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation <https://arxiv.org/abs/2406.14991>`__ 电子表格工作台:挑战现实世界电子表格操作

::

    Fri, 21 Jun 2024 09:06:45 GMT
    Zeyao Ma, Bohan Zhang, Jing Zhang, Jifan Yu, Xiaokang Zhang, Xiaohan Zhang, Sijia Luo, Xi Wang, Jie Tang

We introduce SpreadsheetBench, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SpreadsheetBench is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users.
The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values. Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-of-the-art (SOTA) models and human performance, highlighting the benchmark's difficulty.

------------

`[2406.15000] Unveiling the Impact of Multi-Modal Interactions on User Engagement: A Comprehensive Evaluation in AI-driven Conversations <https://arxiv.org/abs/2406.15000>`__ 揭示多模态交互对用户参与度的影响:人工智能驱动对话的综合评估

::

    Fri, 21 Jun 2024 09:26:55 GMT
    Lichao Zhang, Jia Yu, Shuai Zhang, Long Li, Yangyang Zhong, Guanbao Liang, Yuming Yan, Qing Ma, Fangsheng Weng, Fayu Pan, Jing Li, Renjun Xu and Zhenzhong Lan

Large Language Models (LLMs) have significantly advanced user-bot interactions, enabling more complex and coherent dialogues. However, the prevalent text-only modality might not fully exploit the potential for effective user engagement. This paper explores the impact of multi-modal interactions, which incorporate images and audio alongside text, on user engagement in chatbot conversations. We conduct a comprehensive analysis using a diverse set of chatbots and real-user interaction data, employing metrics such as retention rate and conversation length to evaluate user engagement. Our findings reveal a significant enhancement in user engagement with multi-modal interactions compared to text-only dialogues. Notably, the incorporation of a third modality significantly amplifies engagement beyond the benefits observed with just two modalities. These results suggest that multi-modal interactions optimize cognitive processing and facilitate richer information comprehension.
This study underscores the importance of multi-modality in chatbot design, offering valuable insights for creating more engaging and immersive AI communication experiences and informing the broader AI community about the benefits of multi-modal interactions in enhancing user engagement.

------------

`[2406.15053] PARIKSHA : A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data <https://arxiv.org/abs/2406.15053>`__ 

::

    Fri, 21 Jun 2024 11:00:38 GMT
    Ishaan Watts, Varun Gumma, Aditya Yadavalli, Vivek Seshadri, Manohar Swaminathan and Sunayana Sitaram

Evaluation of multilingual Large Language Models (LLMs) is challenging due to a variety of factors -- the lack of benchmarks with sufficient linguistic diversity, contamination of popular benchmarks into LLM pre-training data and the lack of local, cultural nuances in translated benchmarks. In this work, we study human and LLM-based evaluation in a multilingual, multi-cultural setting.
We evaluate 30 models across 10 Indic languages by conducting 90K human evaluations and 30K LLM-based evaluations and find that models such as GPT-4o and Llama-3 70B consistently perform best for most Indic languages. We build leaderboards for two evaluation settings - pairwise comparison and direct assessment and analyse the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. We also check for various biases in human and LLM-based evaluation and find evidence of self-bias in the GPT-based evaluator. Our work presents a significant step towards scaling up multilingual evaluation of LLMs.

------------

`[2406.15109] Brain-Like Language Processing via a Shallow Untrained Multihead Attention Network <https://arxiv.org/abs/2406.15109>`__ 基于未经训练的浅层多头注意力网络的类脑语言处理

::

    Fri, 21 Jun 2024 12:54:03 GMT
    Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf

Large Language Models (LLMs) have been shown to be effective models of the human language system, with some models predicting most explainable variance of brain activity in current datasets. Even in untrained models, the representations induced by architectural priors can exhibit reasonable alignment to brain data. In this work, we investigate the key architectural components driving the surprising alignment of untrained models. To estimate LLM-to-brain similarity, we first select language-selective units within an LLM, similar to how neuroscientists identify the language network in the human brain. We then benchmark the brain alignment of these LLM units across five different brain recording datasets. By isolating critical components of the Transformer architecture, we identify tokenization strategy and multihead attention as the two major components driving brain alignment. A simple form of recurrence further improves alignment. We further demonstrate this quantitative brain alignment of our model by reproducing landmark studies in the language neuroscience field, showing that localized model units -- just like language voxels measured empirically in the human brain -- discriminate more reliably between lexical than syntactic differences, and exhibit similar response profiles under the same experimental conditions. Finally, we demonstrate the utility of our model's representations for language modeling, achieving improved sample and parameter efficiency over comparable architectures. Our model's estimates of surprisal sets a new state-of-the-art in the behavioral alignment to human reading times. Taken together, we propose a highly brain- and behaviorally-aligned model that conceptualizes the human language system as an untrained shallow feature encoder, with structural priors, combined with a trained decoder to achieve efficient and performant language processing.

------------

`[2406.15130] Assessing Good, Bad and Ugly Arguments Generated by ChatGPT: a New Dataset, its Methodology and Associated Tasks <https://arxiv.org/abs/2406.15130>`__ 评估ChatGPT生成的好、坏和丑陋的参数:一个新的数据集，它的方法和相关的任务

::

    Fri, 21 Jun 2024 13:27:10 GMT
    Victor Hugo Nascimento Rocha, Igor Cataneo Silveira, Paulo Pirozelli, Denis Deratani Mau\'a, Fabio Gagliardi Cozman

The recent success of Large Language Models (LLMs) has sparked concerns about their potential to spread misinformation. As a result, there is a pressing need for tools to identify ``fake arguments'' generated by such models. To create these tools, examples of texts generated by LLMs are needed. This paper introduces a methodology to obtain good, bad and ugly arguments from argumentative essays produced by ChatGPT, OpenAI's LLM. We then describe a novel dataset containing a set of diverse arguments, ArGPT. We assess the effectiveness of our dataset and establish baselines for several argumentation-related tasks. Finally, we show that the artificially generated data relates well to human argumentation and thus is useful as a tool to train and test systems for the defined tasks.

------------

`[2406.15178] Hybrid Alignment Training for Large Language Models <https://arxiv.org/abs/2406.15178>`__ 大型语言模型的混合对齐训练

::

    Fri, 21 Jun 2024 14:23:57 GMT
    Chenglong Wang, Hang Zhou, Kaiyan Chang, Bei Li, Yongyu Mu, Tong Xiao, Tongran Liu and Jingbo Zhu

Alignment training is crucial for enabling large language models (LLMs) to cater to human intentions and preferences. It is typically performed based on two stages with different objectives: instruction-following alignment and human-preference alignment. However, aligning LLMs with these objectives in sequence suffers from an inherent problem: the objectives may conflict, and the LLMs cannot guarantee to simultaneously align with the instructions and human preferences well. To response to these, in this work, we propose a Hybrid Alignment Training (Hbat) approach, based on alternating alignment and modified elastic weight consolidation methods. The basic idea is to alternate between different objectives during alignment training, so that better collaboration can be achieved between the two alignment tasks.We experiment with Hbat on summarization and dialogue tasks. Experimental results show that the proposed \textsc{Hbat} can significantly outperform all baselines. Notably, Hbat yields consistent performance gains over the traditional two-stage alignment training when using both proximal policy optimization and direct preference optimization.

------------

`[2406.15214] Unsupervised Extraction of Dialogue Policies from Conversations <https://arxiv.org/abs/2406.15214>`__ 对话策略的无监督抽取

::

    Fri, 21 Jun 2024 14:57:25 GMT
    Makesh Narsimhan Sreedhar, Traian Rebedea and Christopher Parisien

Dialogue policies play a crucial role in developing task-oriented dialogue systems, yet their development and maintenance are challenging and typically require substantial effort from experts in dialogue modeling. While in many situations, large amounts of conversational data are available for the task at hand, people lack an effective solution able to extract dialogue policies from this data. In this paper, we address this gap by first illustrating how Large Language Models (LLMs) can be instrumental in extracting dialogue policies from datasets, through the conversion of conversations into a unified intermediate representation consisting of canonical forms. We then propose a novel method for generating dialogue policies utilizing a controllable and interpretable graph-based methodology. By combining canonical forms across conversations into a flow network, we find that running graph traversal algorithms helps in extracting dialogue flows. These flows are a better representation of the underlying interactions than flows extracted by prompting LLMs. Our technique focuses on giving conversation designers greater control, offering a productivity tool to improve the process of developing dialogue policies.

------------

`[2406.15227] A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation <https://arxiv.org/abs/2406.15227>`__ 一种基于llm的自动反叙事生成评估排序方法

::

    Fri, 21 Jun 2024 15:11:33 GMT
    Irune Zubiaga, Aitor Soroa, Rodrigo Agerri

The proliferation of misinformation and harmful narratives in online discourse has underscored the critical need for effective Counter Narrative (CN) generation techniques. However, existing automatic evaluation methods often lack interpretability and fail to capture the nuanced relationship between generated CNs and human perception. Aiming to achieve a higher correlation with human judgments, this paper proposes a novel approach to asses generated CNs that consists on the use of a Large Language Model (LLM) as a evaluator. By comparing generated CNs pairwise in a tournament-style format, we establish a model ranking pipeline that achieves a correlation of $0.88$ with human preference. As an additional contribution, we leverage LLMs as zero-shot (ZS) CN generators and conduct a comparative analysis of chat, instruct, and base models, exploring their respective strengths and limitations. Through meticulous evaluation, including fine-tuning experiments, we elucidate the differences in performance and responsiveness to domain-specific data. We conclude that chat-aligned models in ZS are the best option for carrying out the task, provided they do not refuse to generate an answer due to security concerns.

------------

`[2406.15231] Detecting Synthetic Lyrics with Few-Shot Inference <https://arxiv.org/abs/2406.15231>`__ 

::

    Fri, 21 Jun 2024 15:19:21 GMT
    Yanis Labrak, Gabriel Meseguer-Brocal and Elena V. Epure

In recent years, generated content in music has gained significant popularity, with large language models being effectively utilized to produce human-like lyrics in various styles, themes, and linguistic structures. This technological advancement supports artists in their creative processes but also raises issues of authorship infringement, consumer satisfaction and content spamming. To address these challenges, methods for detecting generated lyrics are necessary. However, existing works have not yet focused on this specific modality or on creative text in general regarding machine-generated content detection methods and datasets. In response, we have curated the first dataset of high-quality synthetic lyrics and conducted a comprehensive quantitative evaluation of various few-shot content detection approaches, testing their generalization capabilities and complementing this with a human evaluation. Our best few-shot detector, based on LLM2Vec, surpasses stylistic and statistical methods, which are shown competitive in other domains at distinguishing human-written from machine-generated content. It also shows good generalization capabilities to new artists and models, and effectively detects post-generation paraphrasing. This study emphasizes the need for further research on creative content detection, particularly in terms of generalization and scalability with larger song catalogs. All datasets, pre-processing scripts, and code are available publicly on GitHub and Hugging Face under the Apache 2.0 license.

------------

`[2406.15267] Evaluating Diversity in Automatic Poetry Generation <https://arxiv.org/abs/2406.15267>`__ 诗歌自动生成的多样性评估

::

    Fri, 21 Jun 2024 16:03:21 GMT
    Yanran Chen, Hannes Gr\"oner, Sina Zarrie{\ss}, Steffen Eger

Natural Language Generation (NLG), and more generally generative AI, are among the currently most impactful research fields. Creative NLG, such as automatic poetry generation, is a fascinating niche in this area. While most previous research has focused on forms of the Turing test when evaluating automatic poetry generation - can humans distinguish between automatic and human generated poetry - we evaluate the diversity of automatically generated poetry, by comparing distributions of generated poetry to distributions of human poetry along structural, lexical, semantic and stylistic dimensions, assessing different model types (word vs. character-level, general purpose LLMs vs. poetry-specific models), including the very recent LLaMA3, and types of fine-tuning (conditioned vs. unconditioned). We find that current automatic poetry systems are considerably underdiverse along multiple dimensions - they often do not rhyme sufficiently, are semantically too uniform and even do not match the length distribution of human poetry. Our experiments reveal, however, that style-conditioning and character-level modeling clearly increases diversity across virtually all dimensions we explore. Our identified limitations may serve as the basis for more genuinely diverse future poetry generation models.

------------

`[2406.15352] A SMART Mnemonic Sounds like "Glue Tonic": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick <https://arxiv.org/abs/2406.15352>`__ 一个聪明的助记器听起来像“胶水补剂”:混合LLMs与学生的反馈，使助记器学习粘

::

    Fri, 21 Jun 2024 17:59:51 GMT
    Nishant Balepur, Matthew Shu, Alexander Hoyle, Alison Robey, Shi Feng, Seraphina Goldfarb-Tarrant, Jordan Boyd-Graber

Keyword mnemonics are memorable explanations that link new terms to simpler keywords. Prior works generate mnemonics for students, but they do not guide models toward mnemonics students prefer and aid learning. We build SMART, a mnemonic generator trained on feedback from real students learning new terms.
To train SMART, we first fine-tune LLaMA-2 on a curated set of user-written mnemonics. We then use LLM alignment to enhance SMART: we deploy mnemonics generated by SMART in a flashcard app to find preferences on mnemonics students favor. We gather 2684 preferences from 45 students across two types: expressed (inferred from ratings) and observed (inferred from student learning), yielding three key findings. First, expressed and observed preferences disagree; what students think is helpful does not fully capture what is truly helpful. Second, Bayesian models can synthesize complementary data from multiple preference types into a single effectiveness signal. SMART is tuned via Direct Preference Optimization on this signal, which we show resolves ties and missing labels in the typical method of pairwise comparisons, augmenting data for LLM output quality gains. Third, mnemonic experts assess SMART as matching GPT-4, at much lower deployment costs, showing the utility of capturing diverse student feedback to align LLMs in education.

------------

`[2406.14662] Advantage Alignment Algorithms <https://arxiv.org/abs/2406.14662>`__ 优势对齐算法

::

    Thu, 20 Jun 2024 18:30:09 GMT
    Juan Agustin Duque, Milad Aghajohari, Tim Cooijmans, Tianyu Zhang, Aaron Courville

The growing presence of artificially intelligent agents in everyday decision-making, from LLM assistants to autonomous vehicles, hints at a future in which conflicts may arise from each agent optimizing individual interests.
In general-sum games these conflicts are apparent, where naive Reinforcement Learning agents get stuck in Pareto-suboptimal Nash equilibria. Consequently, opponent shaping has been introduced as a method with success at finding socially beneficial equilibria in social dilemmas. In this work, we introduce Advantage Alignment, a family of algorithms derived from first principles that perform opponent shaping efficiently and intuitively. This is achieved by aligning the advantages of conflicting agents in a given game by increasing the probability of mutually-benefiting actions. We prove that existing opponent shaping methods, including LOLA and LOQA, implicitly perform Advantage Alignment. Compared to these works, Advantage Alignment mathematically simplifies the formulation of opponent shaping and seamlessly works for continuous action domains. We also demonstrate the effectiveness of our algorithm in a wide range of social dilemmas, achieving state of the art results in each case, including a social dilemma version of the Negotiation Game.

------------

`[2406.14867] DistiLRR: Transferring Code Repair for Low-Resource Programming Languages <https://arxiv.org/abs/2406.14867>`__ DistiLRR:面向低资源编程语言的代码修复传输

::

    Fri, 21 Jun 2024 05:05:39 GMT
    Kyle Wong, Alfonso Amayuelas, Liangming Pan, William Yang Wang

Large language models (LLMs) have shown remarkable performance on code generation tasks. A recent application of LLMs for code generation is iterative code repair, where a model fixes an incorrect program by rationalizing about errors and generating a new program. However, code repair is primarily studied on high-resource languages like Python, and the framework's efficacy is under-explored on low-resource languages. To apply code repair for low-resource languages, we propose Distilling Low-Resource Repairs (DistiLRR), an approach that transfers the reasoning and code generation ability from a teacher model to a student model. Our results show that DistiLRR consistently outperforms baselines on low-resource languages, but has similar performance on high-resource languages. To investigate this behavior, we perform a further analysis and find that the correlation between rationale quality and code correctness is weaker than previously perceived. We hypothesize this weakness is magnified in low-resource settings where base models lack deep knowledge of a programming language, leading to wavering benefits of code repair between high-resource and low-resource languages.

------------

`[2406.14909] MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression <https://arxiv.org/abs/2406.14909>`__ MoA:面向大规模语言模型自动压缩的混合稀疏注意力算法

::

    Fri, 21 Jun 2024 06:58:37 GMT
    Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang

Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers.
MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models.
Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\%-36\%$ to within $5\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.

------------

`[2406.14956] Unlocking the Global Synergies in Low-Rank Adapters <https://arxiv.org/abs/2406.14956>`__ 在低等级适配器中释放全球协同效应

::

    Fri, 21 Jun 2024 08:10:03 GMT
    Zixi Zhang, Cheng Zhang, Xitong Gao, Robert D. Mullins, George A. Constantinides, Yiren Zhao

Low-rank Adaption (LoRA) has been the de-facto parameter-efficient fine-tuning technique for large language models. We present HeteroLoRA, a light-weight search algorithm that leverages zero-cost proxies to allocate the limited LoRA trainable parameters across the model for better fine-tuned performance. In addition to the allocation for the standard LoRA-adapted models, we also demonstrate the efficacy of HeteroLoRA by performing the allocation in a more challenging search space that includes LoRA modules and LoRA-adapted shortcut connections. Experiments show that HeteroLoRA enables improvements in model performance given the same parameter budge. For example, on MRPC, we see an improvement of 1.6% in accuracy with similar training parameter budget. We will open-source our algorithm once the paper is accepted.

------------

`[2406.14963] Optimised Grouped-Query Attention Mechanism for Transformers <https://arxiv.org/abs/2406.14963>`__ transformer优化的分组查询注意力机制

::

    Fri, 21 Jun 2024 08:20:06 GMT
    Yuang Chen, Cheng Zhang, Xitong Gao, Robert D. Mullins, George A. Constantinides, Yiren Zhao

Grouped-query attention (GQA) has been widely adopted in LLMs to mitigate the complexity of multi-head attention (MHA). To transform an MHA to a GQA, neighbour queries in MHA are evenly split into groups where each group shares the value and key layers. In this work, we propose AsymGQA, an activation-informed approach to asymmetrically grouping an MHA to a GQA for better model performance. Our AsymGQA outperforms the GQA within the same model size budget. For example, AsymGQA LLaMA-2-7B has an accuracy increase of 7.5% on MMLU compared to neighbour grouping. Our approach addresses the GQA's trade-off problem between model performance and hardware efficiency.

------------

`[2406.14653] LLM Granularity for On-the-Fly Robot Control <https://arxiv.org/abs/2406.14653>`__ 用于实时机器人控制的LLM粒度

::

    Thu, 20 Jun 2024 18:17:48 GMT
    Peng Wang, Mattia Robbiani, Zhihao Guo

Assistive robots have attracted significant attention due to their potential to enhance the quality of life for vulnerable individuals like the elderly. The convergence of computer vision, large language models, and robotics has introduced the `visuolinguomotor' mode for assistive robots, where visuals and linguistics are incorporated into assistive robots to enable proactive and interactive assistance. This raises the question: \textit{In circumstances where visuals become unreliable or unavailable, can we rely solely on language to control robots, i.e., the viability of the `linguomotor` mode for assistive robots?} This work takes the initial steps to answer this question by: 1) evaluating the responses of assistive robots to language prompts of varying granularities; and 2) exploring the necessity and feasibility of controlling the robot on-the-fly. We have designed and conducted experiments on a Sawyer cobot to support our arguments. A Turtlebot robot case is designed to demonstrate the adaptation of the solution to scenarios where assistive robots need to maneuver to assist. Codes will be released on GitHub soon to benefit the community.

------------

`[2406.14655] HYPERmotion: Learning Hybrid Behavior Planning for Autonomous Loco-manipulation <https://arxiv.org/abs/2406.14655>`__ HYPERmotion:自主位置操纵混合行为规划学习

::

    Thu, 20 Jun 2024 18:21:24 GMT
    Jin Wang, Rui Dai, Weijie Wang, Luca Rossini, Francesco Ruscelli, Nikos Tsagarakis

Enabling robots to autonomously perform hybrid motions in diverse environments can be beneficial for long-horizon tasks such as material handling, household chores, and work assistance. This requires extensive exploitation of intrinsic motion capabilities, extraction of affordances from rich environmental information, and planning of physical interaction behaviors.
Despite recent progress has demonstrated impressive humanoid whole-body control abilities, they struggle to achieve versatility and adaptability for new tasks.
In this work, we propose HYPERmotion, a framework that learns, selects and plans behaviors based on tasks in different scenarios. We combine reinforcement learning with whole-body optimization to generate motion for 38 actuated joints and create a motion library to store the learned skills. We apply the planning and reasoning features of the large language models (LLMs) to complex loco-manipulation tasks, constructing a hierarchical task graph that comprises a series of primitive behaviors to bridge lower-level execution with higher-level planning. By leveraging the interaction of distilled spatial geometry and 2D observation with a visual language model (VLM) to ground knowledge into a robotic morphology selector to choose appropriate actions in single- or dual-arm, legged or wheeled locomotion. Experiments in simulation and real-world show that learned motions can efficiently adapt to new tasks, demonstrating high autonomy from free-text commands in unstructured scenes.
Videos and website: hy-motion.github.io/

------------

`[2406.14871] I don't trust you (anymore)! -- The effect of students' LLM use on Lecturer-Student-Trust in Higher Education <https://arxiv.org/abs/2406.14871>`__ 我不再相信你了!——学生LLM使用对高等教育师生信任的影响

::

    Fri, 21 Jun 2024 05:35:57 GMT
    Simon Kloker, Matthew Bazanya, Twaha Kateete

Trust plays a pivotal role in Lecturer-Student-Collaboration, encompassing teaching and research aspects. The advent of Large Language Models (LLMs) in platforms like Open AI's ChatGPT, coupled with their cost-effectiveness and high-quality results, has led to their rapid adoption among university students. However, discerning genuine student input from LLM-generated output poses a challenge for lecturers. This dilemma jeopardizes the trust relationship between lecturers and students, potentially impacting university downstream activities, particularly collaborative research initiatives. Despite attempts to establish guidelines for student LLM use, a clear framework mutually beneficial for lecturers and students in higher education remains elusive. This study addresses the research question: How does the use of LLMs by students impact Informational and Procedural Justice, influencing Team Trust and Expected Team Performance? Methodically, we applied a quantitative construct-based survey, evaluated using techniques of Structural Equation Modelling (PLS- SEM) to examine potential relationships among these constructs.
Our findings based on 23 valid respondents from Ndejje University indicate that lecturers are less concerned about the fairness of LLM use per se but are more focused on the transparency of student utilization, which significantly influences Team Trust positively. This research contributes to the global discourse on integrating and regulating LLMs and subsequent models in education. We propose that guidelines should support LLM use while enforcing transparency in Lecturer-Student- Collaboration to foster Team Trust and Performance. The study contributes valuable insights for shaping policies enabling ethical and transparent LLMs usage in education to ensure effectiveness of collaborative learning environments.

------------

`[2406.15173] \'Evaluation des capacit\'es de r\'eponse de larges mod\`eles de langage (LLM) pour des questions d'historiens <https://arxiv.org/abs/2406.15173>`__ 对历史学家的评价、评价、评价、评价、评价、评价、评价、评价、评价、评价、评价、评价、评价、评价

::

    Fri, 21 Jun 2024 14:19:57 GMT
    Mathieu Chartier, Nabil Dakkoune, Guillaume Bourgeois, St\'ephane Jean

Large Language Models (LLMs) like ChatGPT or Bard have revolutionized information retrieval and captivated the audience with their ability to generate custom responses in record time, regardless of the topic. In this article, we assess the capabilities of various LLMs in producing reliable, comprehensive, and sufficiently relevant responses about historical facts in French. To achieve this, we constructed a testbed comprising numerous history-related questions of varying types, themes, and levels of difficulty.
Our evaluation of responses from ten selected LLMs reveals numerous shortcomings in both substance and form. Beyond an overall insufficient accuracy rate, we highlight uneven treatment of the French language, as well as issues related to verbosity and inconsistency in the responses provided by LLMs.

------------

`[2406.15259] V-RECS, a Low-Cost LLM4VIS Recommender with Explanations, Captioning and Suggestions <https://arxiv.org/abs/2406.15259>`__ 

::

    Fri, 21 Jun 2024 15:50:10 GMT
    Luca Podo, Marco Angelini, Paola Velardi

NL2VIS (natural language to visualization) is a promising and recent research area that involves interpreting natural language queries and translating them into visualizations that accurately represent the underlying data. As we navigate the era of big data, NL2VIS holds considerable application potential since it greatly facilitates data exploration by non-expert users. Following the increasingly widespread usage of generative AI in NL2VIS applications, in this paper we present V-RECS, the first LLM-based Visual Recommender augmented with explanations(E), captioning(C), and suggestions(S) for further data exploration. V-RECS' visualization narratives facilitate both response verification and data exploration by non-expert users. Furthermore, our proposed solution mitigates computational, controllability, and cost issues associated with using powerful LLMs by leveraging a methodology to effectively fine-tune small models. To generate insightful visualization narratives, we use Chain-of-Thoughts (CoT), a prompt engineering technique to help LLM identify and generate the logical steps to produce a correct answer. Since CoT is reported to perform poorly with small LLMs, we adopted a strategy in which a large LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to fine-tune a small model, Llama-2-7B, which plays the role of a Student.
Extensive experiments-based on a framework for the quantitative evaluation of AI-based visualizations and on manual assessment by a group of participants-show that V-RECS achieves performance scores comparable to GPT-4, at a much lower cost. The efficacy of the V-RECS teacher-student paradigm is also demonstrated by the fact that the un-tuned Llama fails to perform the task in the vast majority of test cases. We release V-RECS for the visualization community to assist visualization designers throughout the entire visualization generation process.

------------

`[2406.14898] Safely Learning with Private Data: A Federated Learning Framework for Large Language Model <https://arxiv.org/abs/2406.14898>`__ 私有数据安全学习:大型语言模型联邦学习框架

::

    Fri, 21 Jun 2024 06:43:15 GMT
    JiaYing Zheng, HaiNan Zhang, LingXiang Wang, WangJie Qiu, HongWei Zheng, ZhiMing Zheng

Private data, being larger and quality-higher than public data, can greatly improve large language models (LLM). However, due to privacy concerns, this data is often dispersed in multiple silos, making its secure utilization for LLM training a challenge. Federated learning (FL) is an ideal solution for training models with distributed private data, but traditional frameworks like FedAvg are unsuitable for LLM due to their high computational demands on clients. An alternative, split learning, offloads most training parameters to the server while training embedding and output layers locally, making it more suitable for LLM. Nonetheless, it faces significant challenges in security and efficiency. Firstly, the gradients of embeddings are prone to attacks, leading to potential reverse engineering of private data. Furthermore, the server's limitation of handle only one client's training request at a time hinders parallel training, severely impacting training efficiency. In this paper, we propose a Federated Learning framework for LLM, named FL-GLM, which prevents data leakage caused by both server-side and peer-client attacks while improving training efficiency. Specifically, we first place the input block and output block on local client to prevent embedding gradient attacks from server.
Secondly, we employ key-encryption during client-server communication to prevent reverse engineering attacks from peer-clients. Lastly, we employ optimization methods like client-batching or server-hierarchical, adopting different acceleration methods based on the actual computational capabilities of the server. Experimental results on NLU and generation tasks demonstrate that FL-GLM achieves comparable metrics to centralized chatGLM model, validating the effectiveness of our federated learning framework.

------------

`[2406.15264] Towards Fine-Grained Citation Evaluation in Generated Text: A Comparative Analysis of Faithfulness Metrics <https://arxiv.org/abs/2406.15264>`__ 生成文本中的细粒度引文评价:忠实度指标比较分析

::

    Fri, 21 Jun 2024 15:57:24 GMT
    Weijia Zhang, Mohammad Aliannejadi, Yifei Yuan, Jiahuan Pei, Jia-Hong Huang, Evangelos Kanoulas

Large language models (LLMs) often produce unsupported or unverifiable information, known as "hallucinations." To mitigate this, retrieval-augmented LLMs incorporate citations, grounding the content in verifiable sources.
Despite such developments, manually assessing how well a citation supports the associated statement remains a major challenge. Previous studies use faithfulness metrics to estimate citation support automatically but are limited to binary classification, overlooking fine-grained citation support in practical scenarios. To investigate the effectiveness of faithfulness metrics in fine-grained scenarios, we propose a comparative evaluation framework that assesses the metric effectiveness in distinguishinging citations between three-category support levels: full, partial, and no support. Our framework employs correlation analysis, classification evaluation, and retrieval evaluation to measure the alignment between metric scores and human judgments comprehensively. Our results show no single metric consistently excels across all evaluations, revealing the complexity of assessing fine-grained support.
Based on the findings, we provide practical recommendations for developing more effective metrics.

------------

`[2312.01678] Jellyfish: A Large Language Model for Data Preprocessing <https://arxiv.org/abs/2312.01678>`__ Jellyfish:用于数据预处理的大型语言模型

::

    replaced with revised version Fri, 21 Jun 2024 09:39:31 GMT
    Submission history From: Chuan Xiao [view email]
    [v1] Mon, 4 Dec 2023 07:01:54 UTC (180 KB)
    [v2] Tue, 5 Dec 2023 18:02:46 UTC (180 KB)
    [v3] Tue, 26 Dec 2023 13:51:29 UTC (105 KB)
    [v4] Wed, 13 Mar 2024 13:02:57 UTC (206 KB)
    [v5] Fri, 21 Jun 2024 09:39:31 UTC (214 KB)
    Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada

This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 -- 13B models) as universal DP task solvers that operate on a local, single, and low-priced GPU, ensuring data security and enabling further customization. We select a collection of datasets across four representative DP tasks and construct instruction tuning data using data configuration, knowledge injection, and reasoning data distillation techniques tailored to DP. By tuning Mistral-7B, Llama 3-8B, and OpenOrca-Platypus2-13B, our models, namely, Jellyfish-7B/8B/13B, deliver competitiveness compared to GPT-3.5/4 models and strong generalizability to unseen tasks while barely compromising the base models' abilities in NLP tasks. Meanwhile, Jellyfish offers enhanced reasoning capabilities compared to GPT-3.5.
Our models are available at: this https URL .
Our instruction dataset is available at: this https URL .

------------

`[2406.11160] Contextual Knowledge Graph <https://arxiv.org/abs/2406.11160>`__ 语境知识图谱

::

    replaced with revised version Fri, 21 Jun 2024 08:33:10 GMT
    Submission history From: Chengjin Xu [view email]
    [v1] Mon, 17 Jun 2024 02:59:19 UTC (3,392 KB)
    [v2] Fri, 21 Jun 2024 08:33:10 UTC (3,393 KB)
    Chengjin Xu, Muzhi Li, Cehao Yang, Xuhui Jiang, Lumingyuan Tang, Yiyan Qi, Jian Guo

Knowledge Graphs (KGs) are foundational structures in many AI applications, representing entities and their interrelations through triples. However, triple-based KGs lack the contextual information of relational knowledge, like temporal dynamics and provenance details, which are crucial for comprehensive knowledge representation and effective reasoning. Instead, \textbf{Contextual Knowledge Graphs} (CKGs) expand upon the conventional structure by incorporating additional information such as time validity, geographic location, and source provenance. This integration provides a more nuanced and accurate understanding of knowledge, enabling KGs to offer richer insights and support more sophisticated reasoning processes. In this work, we first discuss the inherent limitations of triple-based KGs and introduce the concept of contextual KGs, highlighting their advantages in knowledge representation and reasoning. We then present \textbf{KGR$^3$, a context-enriched KG reasoning paradigm} that leverages large language models (LLMs) to retrieve candidate entities and related contexts, rank them based on the retrieved information, and reason whether sufficient information has been obtained to answer a query. Our experimental results demonstrate that KGR$^3$ significantly improves performance on KG completion (KGC) and KG question answering (KGQA) tasks, validating the effectiveness of incorporating contextual information on KG representation and reasoning.

------------

`[2406.12000] Look Further Ahead: Testing the Limits of GPT-4 in Path Planning <https://arxiv.org/abs/2406.12000>`__ 进一步展望:测试GPT-4在路径规划中的极限

::

    replaced with revised version Thu, 20 Jun 2024 19:53:52 GMT
    Submission history From: Mohamed Aghzal [view email]
    [v1] Mon, 17 Jun 2024 18:12:56 UTC (1,476 KB)
    [v2] Thu, 20 Jun 2024 19:53:52 UTC (1,475 KB)
    Mohamed Aghzal, Erion Plaku, Ziyu Yao

Large Language Models (LLMs) have shown impressive capabilities across a wide variety of tasks. However, they still face challenges with long-horizon planning. To study this, we propose path planning tasks as a platform to evaluate LLMs' ability to navigate long trajectories under geometric constraints. Our proposed benchmark systematically tests path-planning skills in complex settings. Using this, we examined GPT-4's planning abilities using various task representations and prompting approaches. We found that framing prompts as Python code and decomposing long trajectory tasks improve GPT-4's path planning effectiveness. However, while these approaches show some promise toward improving the planning ability of the model, they do not obtain optimal paths and fail at generalizing over extended horizons.

------------

`[2406.12043] Grade Score: Quantifying LLM Performance in Option Selection <https://arxiv.org/abs/2406.12043>`__ 等级分数:量化LLM在选项选择中的表现

::

    replaced with revised version Thu, 20 Jun 2024 21:58:07 GMT
    Submission history From: Dmitri Iourovitski [view email]
    [v1] Mon, 17 Jun 2024 19:29:39 UTC (1,047 KB)
    [v2] Thu, 20 Jun 2024 21:58:07 UTC (1,047 KB)
    Dmitri Iourovitski

This study introduces the "Grade Score", a novel metric designed to evaluate the consistency and fairness of Large Language Models (LLMs) when used as multiple-choice judges with respect to order bias and choice consistency. The Grade Score combines Entropy, which measures order bias, and Mode Frequency, which assesses choice stability, offering insights into LLMs' reliability and impartiality. The study explores techniques such as prompt engineering and option sampling strategies to optimize the Grade Score, demonstrating their effectiveness in enhancing LLMs' performance. Results showcase varying performances among LLMs with respect to prompts and highlight the positive impact of including irrelevant options. The study also identifies an emergent behavior in instruction-following models, where they adapt to instructions targeting specific biases, demonstrating their adaptability. The Grade Score facilitates comparisons between LLMs and encourages ongoing research towards optimizing their decision-making processes, with potential implications for improving their reliability and fairness in various applications. All code is available on GitHub this https URL

------------

`[2406.13919] SPL: A Socratic Playground for Learning Powered by Large Language Model <https://arxiv.org/abs/2406.13919>`__ SPL:由大型语言模型驱动的苏格拉底式学习游乐场

::

    replaced with revised version Fri, 21 Jun 2024 02:36:10 GMT
    Submission history From: Liang Zhang [view email]
    [v1] Thu, 20 Jun 2024 01:18:52 UTC (1,630 KB)
    [v2] Fri, 21 Jun 2024 02:36:10 UTC (1,630 KB)
    Liang Zhang, Jionghao Lin, Ziyi Kuang, Sheng Xu, Mohammed Yeasin, Xiangen Hu

Dialogue-based Intelligent Tutoring Systems (ITSs) have significantly advanced adaptive and personalized learning by automating sophisticated human tutoring strategies within interactive dialogues. However, replicating the nuanced patterns of expert human communication remains a challenge in Natural Language Processing (NLP). Recent advancements in NLP, particularly Large Language Models (LLMs) such as OpenAI's GPT-4, offer promising solutions by providing human-like and context-aware responses based on extensive pre-trained knowledge. Motivated by the effectiveness of LLMs in various educational tasks (e.g., content creation and summarization, problem-solving, and automated feedback provision), our study introduces the Socratic Playground for Learning (SPL), a dialogue-based ITS powered by the GPT-4 model, which employs the Socratic teaching method to foster critical thinking among learners. Through extensive prompt engineering, SPL can generate specific learning scenarios and facilitates efficient multi-turn tutoring dialogues. The SPL system aims to enhance personalized and adaptive learning experiences tailored to individual needs, specifically focusing on improving critical thinking skills. Our pilot experimental results from essay writing tasks demonstrate SPL has the potential to improve tutoring interactions and further enhance dialogue-based ITS functionalities. Our study, exemplified by SPL, demonstrates how LLMs enhance dialogue-based ITSs and expand the accessibility and efficacy of educational technologies.

------------

`[2406.14124] Measuring Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective <https://arxiv.org/abs/2406.14124>`__ 

::

    replaced with revised version Fri, 21 Jun 2024 02:30:32 GMT
    Submission history From: Minsang Kim [view email]
    [v1] Thu, 20 Jun 2024 09:09:34 UTC (1,360 KB)
    [v2] Fri, 21 Jun 2024 02:30:32 UTC (1,360 KB)
    Minsang Kim, Seungjun Baek

Compute-efficient training of large language models (LLMs) has become an important research problem. In this work, we consider data pruning as a method of data-efficient training of LLMs, where we take a data compression view on data pruning. We argue that the amount of information of a sample, or the achievable compression on its description length, represents its sample importance. The key idea is that, less informative samples are likely to contain redundant information, and thus should be pruned first. We leverage log-likelihood function of trained models as a surrogate to measure information content of samples. Experiments reveal a surprising insight that information-based pruning can enhance the generalization capability of the model, improves upon language modeling and downstream tasks as compared to the model trained on the entire dataset.

------------

`[2406.14408] FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving <https://arxiv.org/abs/2406.14408>`__ FVEL:基于定理证明的大型语言模型交互式形式化验证环境

::

    replaced with revised version Fri, 21 Jun 2024 02:51:41 GMT
    Submission history From: Yinya Huang [view email]
    [v1] Thu, 20 Jun 2024 15:31:05 UTC (3,587 KB)
    [v2] Fri, 21 Jun 2024 02:51:41 UTC (3,585 KB)
    Xiaohan Lin, Qingxing Cao, Yinya Huang, Haiming Wang, Jianqiao Lu, Zhengying Liu, Linqi Song, Xiaodan Liang

Formal verification (FV) has witnessed growing significance with current emerging program synthesis by the evolving large language models (LLMs). However, current formal verification mainly resorts to symbolic verifiers or hand-craft rules, resulting in limitations for extensive and flexible verification. On the other hand, formal languages for automated theorem proving, such as Isabelle, as another line of rigorous verification, are maintained with comprehensive rules and theorems. In this paper, we propose FVEL, an interactive Formal Verification Environment with LLMs. Specifically, FVEL transforms a given code to be verified into Isabelle, and then conducts verification via neural automated theorem proving with an LLM. The joined paradigm leverages the rigorous yet abundant formulated and organized rules in Isabelle and is also convenient for introducing and adjusting cutting-edge LLMs. To achieve this goal, we extract a large-scale FVELER3. The FVELER dataset includes code dependencies and verification processes that are formulated in Isabelle, containing 758 theories, 29,125 lemmas, and 200,646 proof steps in total with in-depth dependencies. We benchmark FVELER in the FVEL environment by first fine-tuning LLMs with FVELER and then evaluating them on Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned Llama3- 8B solves 17.39% (69 -> 81) more problems, and Mistral-7B 12% (75 -> 84) more problems in SV-COMP. And the proportion of proof errors is reduced. Project page: this https URL.

------------

`[2305.13712] Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models <https://arxiv.org/abs/2305.13712>`__ 知识中的知识:用大型语言模型探索已知-未知的不确定性

::

    replaced with revised version Thu, 20 Jun 2024 20:40:51 GMT
    Submission history From: Alfonso Amayuelas [view email]
    [v1] Tue, 23 May 2023 05:59:21 UTC (826 KB)
    [v2] Thu, 20 Jun 2024 20:40:51 UTC (4,788 KB)
    Alfonso Amayuelas, Liangming Pan, Wenhu Chen, William Wang

This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their knowledge and uncertainty over questions. Specifically, we focus on addressing known-unknown questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a new dataset with Known-Unknown Questions (KUQ) and establish a categorization framework to clarify the origins of uncertainty in such queries. Subsequently, we examine the performance of open-source LLMs, fine-tuned using this dataset, in distinguishing between known and unknown queries within open-ended question-answering scenarios. The fine-tuned models demonstrated a significant improvement, achieving a considerable increase in F1-score relative to their pre-fine-tuning state. Through a comprehensive analysis, we reveal insights into the models' improved uncertainty articulation and their consequent efficacy in multi-agent debates. These findings help us understand how LLMs can be trained to identify and express uncertainty, improving our knowledge of how they understand and express complex or unclear information.

------------

`[2308.01846] XNLP: An Interactive Demonstration System for Universal Structured NLP <https://arxiv.org/abs/2308.01846>`__ XNLP:通用结构化NLP交互演示系统

::

    replaced with revised version Fri, 21 Jun 2024 15:26:05 GMT
    Submission history From: Hao Fei [view email]
    [v1] Thu, 3 Aug 2023 16:13:05 UTC (1,076 KB)
    [v2] Fri, 21 Jun 2024 15:26:05 UTC (1,075 KB)
    Hao Fei, Meishan Zhang, Min Zhang, Tat-Seng Chua

Structured Natural Language Processing (XNLP) is an important subset of NLP that entails understanding the underlying semantic or syntactic structure of texts, which serves as a foundational component for many downstream applications. Despite certain recent efforts to explore universal solutions for specific categories of XNLP tasks, a comprehensive and effective approach for unifying all XNLP tasks long remains underdeveloped. In the meanwhile, while XNLP demonstration systems are vital for researchers exploring various XNLP tasks, existing platforms can be limited to, e.g., supporting few XNLP tasks, lacking interactivity and universalness. To this end, we propose an advanced XNLP demonstration platform, where we propose leveraging LLM to achieve universal XNLP, with one model for all with high generalizability. Overall, our system advances in multiple aspects, including universal XNLP modeling, high performance, interpretability, scalability, and interactivity, providing a unified platform for exploring diverse XNLP tasks in the community. XNLP is online: https://xnlp.haofei.vip

------------

`[2311.04155] Black-Box Prompt Optimization: Aligning Large Language Models without Model Training <https://arxiv.org/abs/2311.04155>`__ 黑盒提示优化:无需模型训练即可对齐大型语言模型

::

    replaced with revised version Fri, 21 Jun 2024 06:06:07 GMT
    Submission history From: Jiale Cheng [view email]
    [v1] Tue, 7 Nov 2023 17:31:50 UTC (3,751 KB)
    [v2] Wed, 8 Nov 2023 04:21:41 UTC (3,751 KB)
    [v3] Fri, 21 Jun 2024 06:06:07 UTC (3,764 KB)
    Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang

Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them; that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods primarily focus on further training them. However, the extra training of LLMs is usually expensive in terms of GPU computing; even worse, some LLMs are not accessible for user-demanded training, such as GPTs. In this work, we take a different perspective -- Black-Box Prompt Optimization (BPO) -- to perform alignments. The idea is to optimize user prompts to suit LLMs' input understanding, so as to best realize users' intents without updating LLMs' parameters. BPO leverages human preferences to optimize prompts, thus making it superior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is model-agnostic, and the empirical results demonstrate that the BPO-aligned ChatGPT yields a 22% increase in the win rate against its original version and 10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO. Code and datasets are released at this https URL.

------------

`[2401.01301] Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models <https://arxiv.org/abs/2401.01301>`__ 大型法律小说:用大型语言模型分析法律幻觉

::

    replaced with revised version Fri, 21 Jun 2024 15:32:27 GMT
    Submission history From: Matthew Dahl [view email]
    [v1] Tue, 2 Jan 2024 17:28:06 UTC (4,258 KB)
    [v2] Fri, 21 Jun 2024 15:32:27 UTC (4,420 KB)
    Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E. Ho

Do large language models (LLMs) know the law? These models are increasingly being used to augment legal practice, education, and research, yet their revolutionary potential is threatened by the presence of hallucinations -- textual output that is not consistent with legal facts. We present the first systematic evidence of these hallucinations, documenting LLMs' varying performance across jurisdictions, courts, time periods, and cases. Our work makes four key contributions. First, we develop a typology of legal hallucinations, providing a conceptual framework for future research in this area. Second, we find that legal hallucinations are alarmingly prevalent, occurring between 58% of the time with ChatGPT 4 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases. Third, we illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup. Fourth, we provide evidence that LLMs cannot always predict, or do not always know, when they are producing legal hallucinations. Taken together, our findings caution against the rapid and unsupervised integration of popular LLMs into legal tasks. Even experienced lawyers must remain wary of legal hallucinations, and the risks are highest for those who stand to benefit from LLMs the most -- pro se litigants or those without access to traditional legal resources.

------------

`[2401.06408] AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters <https://arxiv.org/abs/2401.06408>`__ 关于我:在网页中使用自我描述来记录英语预训练数据过滤器的效果

::

    replaced with revised version Thu, 20 Jun 2024 18:21:49 GMT
    Submission history From: Li Lucy [view email]
    [v1] Fri, 12 Jan 2024 07:10:10 UTC (4,379 KB)
    [v2] Tue, 16 Jan 2024 19:35:28 UTC (4,379 KB)
    [v3] Thu, 20 Jun 2024 18:21:49 UTC (4,313 KB)
    Li Lucy, Suchin Gururangan, Luca Soldaini, Emma Strubell, David Bamman, Lauren F. Klein, Jesse Dodge

Large language models' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage are under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten "quality" and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications.

------------

`[2402.11442] Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs <https://arxiv.org/abs/2402.11442>`__ llm可以用规则进行推理吗?为压力测试和改进LLMs的逻辑脚手架

::

    replaced with revised version Fri, 21 Jun 2024 02:36:55 GMT
    Submission history From: Siyuan Wang [view email]
    [v1] Sun, 18 Feb 2024 03:38:51 UTC (9,163 KB)
    [v2] Tue, 21 May 2024 22:51:49 UTC (9,164 KB)
    [v3] Fri, 21 Jun 2024 02:36:55 UTC (9,165 KB)
    Siyuan Wang, Zhongyu Wei, Yejin Choi, Xiang Ren

Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reasoning tasks. Overall, our work sheds light on LLMs' limitations in grasping inferential rule and suggests ways to enhance their logical reasoning abilities~\footnote{Code and data are available at \url{this https URL}.}.

------------

`[2402.11943] LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation <https://arxiv.org/abs/2402.11943>`__ 

::

    replaced with revised version Thu, 20 Jun 2024 20:20:30 GMT
    Submission history From: Keyang Xuan [view email]
    [v1] Mon, 19 Feb 2024 08:32:27 UTC (8,572 KB)
    [v2] Thu, 20 Jun 2024 20:20:30 UTC (9,629 KB)
    Keyang Xuan, Li Yi, Fan Yang, Ruochen Wu, Yi R. Fung, Heng Ji

The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. In this paper, we first investigate the potential of LVLM on multimodal misinformation detection. We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation. LEMMA leverages LVLM intuition and reasoning capabilities while augmenting them with external knowledge to enhance the accuracy of misinformation detection. Our method improves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and Fakeddit datasets respectively.

------------

`[2402.14897] Chain-of-Thought Unfaithfulness as Disguised Accuracy <https://arxiv.org/abs/2402.14897>`__ 用思维链的不忠来伪装准确

::

    replaced with revised version Fri, 21 Jun 2024 13:39:14 GMT
    Submission history From: Oliver Bentham [view email]
    [v1] Thu, 22 Feb 2024 17:23:53 UTC (3,936 KB)
    [v2] Wed, 19 Jun 2024 17:49:54 UTC (2,776 KB)
    [v3] Fri, 21 Jun 2024 13:39:14 UTC (2,776 KB)
    Oliver Bentham, Nathan Stringham, Ana Marasovi\'c

Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, Lanham et al. (2023) propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate the experimental setup in their section focused on scaling experiments with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, after normalizing the metric to account for a model's bias toward certain answer choices, unfaithfulness drops significantly for smaller less-capable models. This normalized faithfulness metric is also strongly correlated ($R^2$=0.74) with accuracy, raising doubts about its validity for evaluating faithfulness.

------------

`[2402.16379] TEaR: Improving LLM-based Machine Translation with Systematic Self-Refinement <https://arxiv.org/abs/2402.16379>`__ TEaR:系统的自细化改进基于llm的机器翻译

::

    replaced with revised version Fri, 21 Jun 2024 07:35:53 GMT
    Submission history From: Zhaopeng Feng [view email]
    [v1] Mon, 26 Feb 2024 07:58:12 UTC (5,006 KB)
    [v2] Mon, 4 Mar 2024 03:14:11 UTC (5,006 KB)
    [v3] Fri, 21 Jun 2024 07:35:53 UTC (1,965 KB)
    Zhaopeng Feng, Yan Zhang, Hao Li, Bei Wu, Jiayu Liao, Wenqiang Liu, Jun Lang, Yang Feng, Jian Wu, Zuozhu Liu

Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors. Importantly, feeding back such error information into the LLMs can lead to self-refinement and result in improved translation performance. Motivated by these insights, we introduce a systematic LLM-based self-refinement translation framework, named \textbf{TEaR}, which stands for \textbf{T}ranslate, \textbf{E}stimate, \textbf{a}nd \textbf{R}efine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self-refinement framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it's from high-resource languages to low-resource ones or whether it's English-centric or centered around other languages; 2) TEaR exhibits superior systematicity and interpretability; 3) different estimation strategies yield varied impacts, directly affecting the effectiveness of the final corrections. Additionally, traditional neural translation models and evaluation models operate separately, often focusing on singular tasks due to their limited capabilities, while general-purpose LLMs possess the capability to undertake both tasks simultaneously. We further conduct cross-model correction experiments to investigate the potential relationship between the translation and evaluation capabilities of general-purpose LLMs. Our code and data are available at this https URL

------------

`[2403.00794] Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models <https://arxiv.org/abs/2403.00794>`__ 认真对待幽默:用无趣的大型语言模型制作幽默数据集

::

    replaced with revised version Fri, 21 Jun 2024 17:12:35 GMT
    Submission history From: Zachary Horvitz [view email]
    [v1] Fri, 23 Feb 2024 02:58:12 UTC (280 KB)
    [v2] Fri, 21 Jun 2024 17:12:35 UTC (288 KB)
    Zachary Horvitz, Jingru Chen, Rahul Aditya, Harshvardhan Srivastava, Robert West, Zhou Yu, Kathleen McKeown

Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. In our work, we investigate whether large language models (LLMs), can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to 'unfun' jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset, where we find that GPT-4's synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.

------------

`[2404.03820] CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues <https://arxiv.org/abs/2404.03820>`__ CantTalkAboutThis:对齐语言模型以保持对话中的主题

::

    replaced with revised version Fri, 21 Jun 2024 13:57:11 GMT
    Submission history From: Traian Rebedea [view email]
    [v1] Thu, 4 Apr 2024 22:31:58 UTC (348 KB)
    [v2] Fri, 21 Jun 2024 13:57:11 UTC (8,104 KB)
    Makesh Narsimhan Sreedhar, Traian Rebedea, Shaona Ghosh, Jiaqi Zeng and Christopher Parisien

Recent advancements in instruction-tuning datasets have predominantly focused on specific tasks like mathematical or logical reasoning. There has been a notable gap in data designed for aligning language models to maintain topic relevance in conversations - a critical aspect for deploying chatbots to production. We introduce the CantTalkAboutThis dataset to help language models remain focused on the subject at hand during task-oriented interactions. It consists of synthetic dialogues on a wide range of conversation topics from different domains. These dialogues are interspersed with distractor turns that intentionally divert the chatbot from the predefined topic. Fine-tuning language models on this dataset helps make them resilient to deviating from the role assigned and improves their ability to maintain topical coherence compared to general-purpose instruction-tuned LLMs like GPT-4-turbo and Mixtral-Instruct. Additionally, preliminary observations suggest that training models on this dataset also enhance their performance on fine-grained instruction following tasks, including safety alignment.

------------

`[2404.12038] Uncovering Safety Risks of Large Language Models through Concept Activation Vector <https://arxiv.org/abs/2404.12038>`__ 通过概念激活向量揭示大型语言模型的安全风险

::

    replaced with revised version Fri, 21 Jun 2024 01:08:36 GMT
    Submission history From: Ruixuan Huang [view email]
    [v1] Thu, 18 Apr 2024 09:46:25 UTC (769 KB)
    [v2] Fri, 21 Jun 2024 01:08:36 UTC (340 KB)
    Zhihao Xu, Ruixuan Huang, Shuai Wang, Xiting Wang

Despite careful safety alignment, current large language models (LLMs) remain vulnerable to various attacks. To further unveil the safety risks of LLMs, we introduce a Safety Concept Activation Vector (SCAV) framework, which effectively guides the attacks by accurately interpreting LLMs' safety mechanisms. We then develop an SCAV-guided attack method that can generate both attack prompts and embedding-level attacks with automatically selected perturbation hyperparameters. Both automatic and human evaluations demonstrate that our attack method significantly improves the attack success rate and response quality while requiring less training data. Additionally, we find that our generated attack prompts may be transferable to GPT-4, and the embedding-level attacks may also be transferred to other white-box LLMs whose parameters are known. Our experiments further uncover the safety risks present in current LLMs. For example, we find that six out of seven open-source LLMs that we attack consistently provide relevant answers to more than 85\% malicious instructions. Finally, we provide insights into the safety mechanism of LLMs.

------------

`[2404.15949] CORM: Cache Optimization with Recent Message for Large Language Model Inference <https://arxiv.org/abs/2404.15949>`__ CORM:基于最新消息的大型语言模型推理缓存优化

::

    replaced with revised version Fri, 21 Jun 2024 11:44:17 GMT
    Submission history From: Jincheng Dai [view email]
    [v1] Wed, 24 Apr 2024 16:11:54 UTC (11,375 KB)
    [v2] Fri, 21 Jun 2024 11:44:17 UTC (15,608 KB)
    Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei Bi, Shuming Shi

Large Language Models (LLMs), despite their remarkable performance across a wide range of tasks, necessitate substantial GPU memory and consume significant computational resources. Beyond the memory taken up by model weights, the memory used by the KV cache rises linearly with sequence length, becoming a primary bottleneck for inference. In this paper, we introduce an innovative method for optimizing the KV cache, which considerably minimizes its memory footprint. Upon thorough investigation, we discover that in most Transformer models, (i) there is a striking similarity between adjacent tokens' query vectors, and (ii) the attention calculation of the current query can rely exclusively on the attention information of a small fraction of preceding queries. Based on these observations, we present CORM, a KV cache eviction policy that dynamically retains essential key-value pairs for inference without the need for model fine-tuning. Our validation shows that CORM reduces the inference memory usage of KV cache by up to 70\% with negligible performance degradation across six tasks in LongBench. Furthermore, we demonstrate that CORM is compatible with GQA for further compression rate.

------------

`[2405.20654] Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models <https://arxiv.org/abs/2405.20654>`__ 基于大型语言模型问答中篇章重排序的篇章特定提示调优

::

    replaced with revised version Fri, 21 Jun 2024 03:52:30 GMT
    Submission history From: Xuyang Wu [view email]
    [v1] Fri, 31 May 2024 07:43:42 UTC (174 KB)
    [v2] Fri, 21 Jun 2024 03:52:30 UTC (172 KB)
    Xuyang Wu, Zhiyuan Peng, Krishna Sravanthi Rajanala Sai, Hsin-Tai Wu, Yi Fang

Effective passage retrieval and reranking methods have been widely utilized to identify suitable candidates in open-domain question answering tasks, recent studies have resorted to LLMs for reranking the retrieved passages by the log-likelihood of the question conditioned on each passage. Although these methods have demonstrated promising results, the performance is notably sensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs can be computationally intensive and time-consuming. Furthermore, this approach limits the leverage of question-passage relevance pairs and passage-specific knowledge to enhance the ranking capabilities of LLMs. In this paper, we propose passage-specific prompt tuning for reranking in open-domain question answering (PSPT): a parameter-efficient method that fine-tunes learnable passage-specific soft prompts, incorporating passage-specific knowledge from a limited set of question-passage relevance pairs. The method involves ranking retrieved passages based on the log-likelihood of the model generating the question conditioned on each passage and the learned soft prompt. We conducted extensive experiments utilizing the Llama-2-chat-7B model across three publicly available open-domain question answering datasets and the results demonstrate the effectiveness of the proposed approach.

------------

`[2406.01538] What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores <https://arxiv.org/abs/2406.01538>`__ 大型语言模型在大脑中映射到什么?反对过度依赖大脑分数的案例

::

    replaced with revised version Thu, 20 Jun 2024 20:35:36 GMT
    Submission history From: Ebrahim Feghhi [view email]
    [v1] Mon, 3 Jun 2024 17:13:27 UTC (3,705 KB)
    [v2] Thu, 20 Jun 2024 20:35:36 UTC (4,049 KB)
    Ebrahim Feghhi, Nima Hadidi, Bryan Song, Idan A. Blank, Jonathan C. Kao

Given the remarkable capabilities of large language models (LLMs), there has been a growing interest in evaluating their similarity to the human brain. One approach towards quantifying this similarity is by measuring how well a model predicts neural signals, also called "brain score". Internal representations from LLMs achieve state-of-the-art brain scores, leading to speculation that they share computational principles with human language processing. This inference is only valid if the subset of neural activity predicted by LLMs reflects core elements of language processing. Here, we question this assumption by analyzing three neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages. We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain. We therefore use contiguous splits moving forward. Second, we explain the surprisingly high brain scores of untrained LLMs by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position. This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like. Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence length, position, and pronoun-dereferenced static word embeddings; a small, additional amount is explained by sense-specific embeddings and contextual representations of sentence structure. We conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.

------------

`[2406.06369] Annotation alignment: Comparing LLM and human annotations of conversational safety <https://arxiv.org/abs/2406.06369>`__ 注释对齐:比较LLM和会话安全性的人工注释

::

    replaced with revised version Fri, 21 Jun 2024 04:58:40 GMT
    Submission history From: Rajiv Movva [view email]
    [v1] Mon, 10 Jun 2024 15:30:13 UTC (7,947 KB)
    [v2] Fri, 21 Jun 2024 04:58:40 UTC (8,201 KB)
    Rajiv Movva, Pang Wei Koh, Emma Pierson

To what extent to do LLMs align with human perceptions of safety? We study this question via *annotation alignment*, the extent to which LLMs and humans agree when annotating the safety of user-chatbot conversations. We leverage the recent DICES dataset (Aroyo et al., 2023), in which 350 conversations are each rated for safety by 112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson correlation of $r = 0.59$ with the average annotator rating, higher than the median annotator's correlation with the average ($r=0.51$). We show that larger datasets are needed to resolve whether GPT-4 exhibits disparities in how well it correlates with demographic groups. Also, there is substantial idiosyncratic variation in correlation *within* groups, suggesting that race & gender do not fully capture differences in alignment. Finally, we find that GPT-4 cannot predict when one demographic group finds a conversation more unsafe than another.

------------

`[2406.07232] DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms <https://arxiv.org/abs/2406.07232>`__ Dual - reflect:利用双重学习反馈机制增强大型语言模型的反思性翻译

::

    replaced with revised version Fri, 21 Jun 2024 16:49:33 GMT
    Submission history From: Andong Chen [view email]
    [v1] Tue, 11 Jun 2024 13:10:39 UTC (13,440 KB)
    [v2] Fri, 21 Jun 2024 16:49:33 UTC (9,427 KB)
    Andong Chen, Lianzhang Lou, Kehai Chen, Xuefeng Bai, Yang Xiang, Muyun Yang, Tiejun Zhao, Min Zhang

Recently, large language models (LLMs) enhanced by self-reflection have achieved promising performance on machine translation. The key idea is guiding LLMs to generate translation with human-like feedback. However, existing self-reflection methods lack effective feedback information, limiting the translation performance. To address this, we introduce a DUAL-REFLECT framework, leveraging the dual learning of translation tasks to provide effective feedback, thereby enhancing the models' self-reflective abilities and improving translation performance. The application of this method across various translation tasks has proven its effectiveness in improving translation accuracy and eliminating ambiguities, especially in translation tasks with low-resource language pairs.

------------

`[2406.07835] SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature <https://arxiv.org/abs/2406.07835>`__ SciRIFF:增强科学文献语言模型指导遵循的资源

::

    replaced with revised version Tue, 18 Jun 2024 23:43:34 GMT
    Submission history From: David Wadden [view email]
    [v1] Mon, 10 Jun 2024 21:22:08 UTC (1,261 KB)
    [v2] Tue, 18 Jun 2024 23:43:34 UTC (3,188 KB)
    David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, Arman Cohan

We present SciRIFF (Scientific Resource for Instruction-Following and Finetuning), a dataset of 137K instruction-following demonstrations for 54 tasks covering five essential scientific literature understanding capabilities: information extraction, summarization, question answering, claim verification, and classification. SciRIFF demonstrations are notable for their long input contexts, detailed task specifications, and complex structured outputs. While instruction-following resources are available in specific domains such as clinical medicine and chemistry, SciRIFF is the first dataset focused on extracting and synthesizing information from research literature across a wide range of scientific fields. To demonstrate the utility of SciRIFF, we develop a sample-efficient strategy to adapt a general instruction-following model for science by performing additional finetuning on a mix of general-domain and SciRIFF demonstrations. In evaluations on nine held-out scientific tasks, our model -- called SciTulu -- improves over a strong LLM baseline by 28.1% and 6.5% at the 7B and 70B scales respectively, while maintaining general instruction-following performance within 2% of the baseline. We are optimistic that SciRIFF will facilitate the development and evaluation of LLMs to help researchers navigate the ever-growing body of scientific literature. We release our dataset, model checkpoints, and data processing and evaluation code to enable further research.

------------

`[2406.10248] On the Worst Prompt Performance of Large Language Models <https://arxiv.org/abs/2406.10248>`__ 大型语言模型的最差提示性能研究

::

    replaced with revised version Fri, 21 Jun 2024 08:55:37 GMT
    Submission history From: Bowen Cao [view email]
    [v1] Sat, 8 Jun 2024 13:40:38 UTC (2,482 KB)
    [v2] Fri, 21 Jun 2024 08:55:37 UTC (2,481 KB)
    Bowen Cao, Deng Cai, Zhisong Zhang, Yuexian Zou, Wai Lam

The performance of large language models (LLMs) is acutely sensitive to the phrasing of prompts, which raises significant concerns about their reliability in real-world scenarios. Existing studies often divide prompts into task-level instructions and case-level inputs and primarily focus on evaluating and improving robustness against variations in tasks-level instructions. However, this setup fails to fully address the diversity of real-world user queries and assumes the existence of task-specific datasets. To address these limitations, we introduce RobustAlpacaEval, a new benchmark that consists of semantically equivalent case-level queries and emphasizes the importance of using the worst prompt performance to gauge the lower bound of model performance. Extensive experiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the Llama, Mistral, and Gemma families uncover substantial variability in model performance; for instance, a difference of 45.48% between the worst and best performance for the Llama-2-70B-chat model, with its worst performance dipping as low as 9.38%. We further illustrate the difficulty in identifying the worst prompt from both model-agnostic and model-dependent perspectives, emphasizing the absence of a shortcut to characterize the worst prompt. We also attempt to enhance the worst prompt performance using existing prompt engineering and prompt consistency methods, but find that their impact is limited. These findings underscore the need to create more resilient LLMs that can maintain high performance across diverse prompts. Data and code are available at this https URL Performance-of-LLMs.

------------

`[2406.12702] [WIP] Jailbreak Paradox: The Achilles' Heel of LLMs <https://arxiv.org/abs/2406.12702>`__ 

::

    replaced with revised version Fri, 21 Jun 2024 00:05:35 GMT
    Submission history From: Abhinav Rao [view email]
    [v1] Tue, 18 Jun 2024 15:14:35 UTC (339 KB)
    [v2] Fri, 21 Jun 2024 00:05:35 UTC (342 KB)
    Abhinav Rao, Monojit Choudhury, Somak Aditya

We introduce two paradoxes concerning jailbreak of foundation models: First, it is impossible to construct a perfect jailbreak classifier, and second, a weaker model cannot consistently detect whether a stronger (in a pareto-dominant sense) model is jailbroken or not. We provide formal proofs for these paradoxes and a short case study on Llama and GPT4-o to demonstrate this. We discuss broader theoretical and practical repercussions of these results.

------------

`[2406.13476] LLMs Are Zero-Shot Context-Aware Simultaneous Translators <https://arxiv.org/abs/2406.13476>`__ llm是零样本的上下文感知同声传译

::

    replaced with revised version Fri, 21 Jun 2024 07:21:28 GMT
    Submission history From: Roman Koshkin [view email]
    [v1] Wed, 19 Jun 2024 11:57:42 UTC (7,065 KB)
    [v2] Fri, 21 Jun 2024 07:21:28 UTC (7,067 KB)
    Roman Koshkin, Katsuhito Sudoh, Satoshi Nakamura

The advent of transformers has fueled progress in machine translation. More recently large language models (LLMs) have come to the spotlight thanks to their generality and strong performance in a wide range of language tasks, including translation. Here we show that open-source LLMs perform on par with or better than some state-of-the-art baselines in simultaneous machine translation (SiMT) tasks, zero-shot. We also demonstrate that injection of minimal background information, which is easy with an LLM, brings further performance gains, especially on challenging technical subject-matter. This highlights LLMs' potential for building next generation of massively multilingual, context-aware and terminologically accurate SiMT systems that require no resource-intensive training or fine-tuning.

------------

`[2402.00795] LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law <https://arxiv.org/abs/2402.00795>`__ llm学习动态系统的控制原理，揭示了上下文神经缩放定律

::

    replaced with revised version Thu, 20 Jun 2024 19:12:26 GMT
    Submission history From: Jianbang Liu [view email]
    [v1] Thu, 1 Feb 2024 17:28:10 UTC (7,108 KB)
    [v2] Thu, 20 Jun 2024 19:12:26 UTC (13,018 KB)
    Toni J.B. Liu, Nicolas Boull\'e, Rapha\"el Sarfati, Christopher J. Earls

Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. We study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.

------------

`[2402.10517] Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs <https://arxiv.org/abs/2402.10517>`__ 任意精度LLM:多个不同大小的LLM的低成本部署

::

    replaced with revised version Fri, 21 Jun 2024 05:20:56 GMT
    Submission history From: Yeonhong Park [view email]
    [v1] Fri, 16 Feb 2024 09:06:06 UTC (1,643 KB)
    [v2] Sun, 5 May 2024 11:09:04 UTC (1,643 KB)
    [v3] Tue, 7 May 2024 02:44:25 UTC (1,643 KB)
    [v4] Fri, 21 Jun 2024 05:20:56 UTC (1,514 KB)
    Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, Jae W. Lee

Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit LLM. All the supported LLMs with varying bit-widths demonstrate state-of-the-art model quality and inference throughput, proving itself to be a compelling option for deployment of multiple, different-sized LLMs. Our code is open-sourced and available online.

------------

`[2405.18039] Large Language Model-Driven Curriculum Design for Mobile Networks <https://arxiv.org/abs/2405.18039>`__ 大型语言模型驱动的移动网络课程设计

::

    replaced with revised version Fri, 21 Jun 2024 07:06:30 GMT
    Submission history From: Omar Erak [view email]
    [v1] Tue, 28 May 2024 10:50:35 UTC (1,413 KB)
    [v2] Fri, 21 Jun 2024 07:06:30 UTC (1,413 KB)
    Omar Erak, Omar Alhussein, Shimaa Naser, Nouf Alabbasi, De Mi, Sami Muhaidat

This study introduces an innovative framework that employs large language models (LLMs) to automate the design and generation of curricula for reinforcement learning (RL). As mobile networks evolve towards the 6G era, managing their increasing complexity and dynamic nature poses significant challenges. Conventional RL approaches often suffer from slow convergence and poor generalization due to conflicting objectives and the large state and action spaces associated with mobile networks. To address these shortcomings, we introduce curriculum learning, a method that systematically exposes the RL agent to progressively challenging tasks, improving convergence and generalization. However, curriculum design typically requires extensive domain knowledge and manual human effort. Our framework mitigates this by utilizing the generative capabilities of LLMs to automate the curriculum design process, significantly reducing human effort while improving the RL agent's convergence and performance. We deploy our approach within a simulated mobile network environment and demonstrate improved RL convergence rates, generalization to unseen scenarios, and overall performance enhancements. As a case study, we consider autonomous coordination and user association in mobile networks. Our obtained results highlight the potential of combining LLM-based curriculum generation with RL for managing next-generation wireless networks, marking a significant step towards fully autonomous network operations.

------------

`[2406.14541] Are LLMs Naturally Good at Synthetic Tabular Data Generation? <https://arxiv.org/abs/2406.14541>`__ llm天生擅长合成表格数据生成吗?

::

    replaced with revised version Fri, 21 Jun 2024 14:00:02 GMT
    Submission history From: Shengzhe Xu [view email]
    [v1] Thu, 20 Jun 2024 17:52:29 UTC (3,117 KB)
    [v2] Fri, 21 Jun 2024 14:00:02 UTC (3,117 KB)
    Shengzhe Xu, Cho-Ting Lee, Mandar Sharma, Raquib Bin Yousuf, Nikhil Muralidhar, Naren Ramakrishnan

Large language models (LLMs) have demonstrated their prowess in generating synthetic text and images; however, their potential for generating tabular data -- arguably the most common data type in business and scientific applications -- is largely underexplored. This paper demonstrates that LLMs, used as-is, or after traditional fine-tuning, are severely inadequate as synthetic table generators. Due to the autoregressive nature of LLMs, fine-tuning with random order permutation runs counter to the importance of modeling functional dependencies, and renders LLMs unable to model conditional mixtures of distributions (key to capturing real world constraints). We showcase how LLMs can be made to overcome some of these deficiencies by making them permutation-aware.

------------

`[2306.11232] Eight challenges in developing theory of intelligence <https://arxiv.org/abs/2306.11232>`__ 发展智力理论的八个挑战

::

    replaced with revised version Fri, 21 Jun 2024 08:26:30 GMT
    Submission history From: Haiping Huang [view email]
    [v1] Tue, 20 Jun 2023 01:45:42 UTC (18 KB)
    [v2] Fri, 21 Jun 2024 08:26:30 UTC (148 KB)
    Haiping Huang

A good theory of mathematical beauty is more practical than any current observation, as new predictions of physical reality can be verified self-consistently. This belief applies to the current status of understanding deep neural networks including large language models and even the biological intelligence. Toy models provide a metaphor of physical reality, allowing mathematically formulating that reality (i.e., the so-called theory), which can be updated as more conjectures are justified or refuted. One does not need to pack all details into a model, but rather, more abstract models are constructed, as complex systems like brains or deep networks have many sloppy dimensions but much less stiff dimensions that strongly impact macroscopic observables. This kind of bottom-up mechanistic modeling is still promising in the modern era of understanding the natural or artificial intelligence. Here, we shed light on eight challenges in developing theory of intelligence following this theoretical paradigm. Theses challenges are representation learning, generalization, adversarial robustness, continual learning, causal learning, internal model of the brain, next-token prediction, and finally the mechanics of subjective experience.

------------

`[2312.01797] LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics <https://arxiv.org/abs/2312.01797>`__ LLM A*: Human in the Loop大型语言模型实现了对机器人的搜索

::

    replaced with revised version Thu, 20 Jun 2024 18:50:52 GMT
    Submission history From: Peng Wang Dr. [view email]
    [v1] Mon, 4 Dec 2023 10:37:58 UTC (2,766 KB)
    [v2] Thu, 20 Jun 2024 18:50:52 UTC (28,973 KB)
    Hengjia Xiao and Peng Wang

This research focuses on how Large Language Models (LLMs) can help with (path) planning for mobile embodied agents such as robots, in a human-in-the-loop and interactive manner. A novel framework named LLM A*, aims to leverage the commonsense of LLMs, and the utility-optimal A* is proposed to facilitate few-shot near-optimal path planning. Prompts are used for two main purposes: 1) to provide LLMs with essential information like environments, costs, heuristics, etc.; 2) to communicate human feedback on intermediate planning results to LLMs. This approach takes human feedback on board and renders the entire planning process transparent (akin to a `white box') to humans. Moreover, it facilitates code-free path planning, thereby fostering the accessibility and inclusiveness of artificial intelligence techniques to communities less proficient in coding. Comparative analysis against A* and RL demonstrates that LLM A* exhibits greater efficiency in terms of search space and achieves paths comparable to A* while outperforming RL. The interactive nature of LLM A* also makes it a promising tool for deployment in collaborative human-robot tasks. Codes and Supplemental Materials can be found at GitHub: this https URL.

------------

`[2403.03230] Large language models surpass human experts in predicting neuroscience results <https://arxiv.org/abs/2403.03230>`__ 大型语言模型在预测神经科学结果方面超过了人类专家

::

    replaced with revised version Fri, 21 Jun 2024 17:35:46 GMT
    Submission history From: Xiaoliang Luo [view email]
    [v1] Mon, 4 Mar 2024 15:27:59 UTC (3,592 KB)
    [v2] Thu, 14 Mar 2024 23:32:15 UTC (3,575 KB)
    [v3] Fri, 21 Jun 2024 17:35:46 UTC (2,845 KB)
    Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe Y\'a\~nez, Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina Borghesani, Anton Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro Salatiello, Ilia Sucholutsky, Pasquale Minervini, Sepehr Razavi, Roberta Rocca, Elkhan Yusifov, Tereza Okalova, Nianlong Gu, Martin Ferianc, Mikail Khona, Kaustubh R. Patil, Pui-Shee Lee, Rui Mata, Nicholas E. Myers, Jennifer K Bizley, Sebastian Musslick, Isil Poyraz Bilgin, Guiomar Niso, Justin M. Ales, Michael Gaebler, N Apurva Ratan Murty, Leyla Loued-Khenissi, Anna Behler, Chloe M. Hall, Jessica Dafflon, Sherry Dongqi Bao, Bradley C. Love

Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.

------------

----------
Index (92)
----------

`[2406.14598] SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors <https://arxiv.org/abs/2406.14598>`__ SORRY-Bench:系统评估大型语言模型的安全拒绝行为

`[2406.14701] Speech Prefix-Tuning with RNNT Loss for Improving LLM Predictions <https://arxiv.org/abs/2406.14701>`__ 基于RNNT损失的语音前缀微调以改善LLM预测

`[2406.14722] Does GPT Really Get It? A Hierarchical Scale to Quantify Human vs AI's Understanding of Algorithms <https://arxiv.org/abs/2406.14722>`__ GPT真的能理解吗?一个层级尺度来量化人类vs AI对算法的理解

`[2406.14757] A Large Language Model Outperforms Other Computational Approaches to the High-Throughput Phenotyping of Physician Notes <https://arxiv.org/abs/2406.14757>`__ 在医生病历的高通量表型方面，大型语言模型的性能优于其他计算方法

`[2406.14769] How critically can an AI think? A framework for evaluating the quality of thinking of generative artificial intelligence <https://arxiv.org/abs/2406.14769>`__

`[2406.14903] GIEBench: Towards Holistic Evaluation of Group Indentity-based Empathy for Large Language Models <https://arxiv.org/abs/2406.14903>`__ GIEBench:基于群体身份的大型语言模型共情的整体评估

`[2406.14917] LLM2FEA: Discover Novel Designs with Generative Evolutionary Multitasking <https://arxiv.org/abs/2406.14917>`__ LLM2FEA:基于生成进化多任务处理的新颖设计

`[2406.14981] Human-AI collectives produce the most accurate differential diagnoses <https://arxiv.org/abs/2406.14981>`__ 人类- ai集体产生最准确的鉴别诊断

`[2406.14986] Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers <https://arxiv.org/abs/2406.14986>`__

`[2406.15198] Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms <https://arxiv.org/abs/2406.15198>`__ 探讨ChatGPT和Claude机器人助手在加强ADHD治疗中的功效:创新治疗范式

`[2406.15325] Bug In the Code Stack: Can LLMs Find Bugs in Large Python Code Stacks <https://arxiv.org/abs/2406.15325>`__ 代码栈中的Bug: llm能否在大型Python代码栈中找到Bug

`[2406.15330] Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance <https://arxiv.org/abs/2406.15330>`__ 梯度掩码调优提高了LLM性能的上限

`[2406.14629] Can LLMs Learn by Teaching? A Preliminary Study <https://arxiv.org/abs/2406.14629>`__ LLMs可以通过教学来学习吗?初步研究

`[2406.14654] Major Entity Identification: A Generalizable Alternative to Coreference Resolution <https://arxiv.org/abs/2406.14654>`__ 主要实体识别:共指消解的可泛化替代方案

`[2406.14657] OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset <https://arxiv.org/abs/2406.14657>`__ OpenDebateEvidence:大规模论据挖掘和摘要数据集

`[2406.14670] Exploring Design Choices for Building Language-Specific LLMs <https://arxiv.org/abs/2406.14670>`__

`[2406.14673] Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell <https://arxiv.org/abs/2406.14673>`__ 对LLM长上下文故障的洞察:transformer何时知道但不告诉

`[2406.14678] Bidirectional Transformer Representations of (Spanish) Ambiguous Words in Context: A New Lexical Resource and Empirical Analysis <https://arxiv.org/abs/2406.14678>`__ 语境中(西班牙语)歧义词的双向Transformer表示:一种新的词汇资源和实证分析

`[2406.14703] Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics <https://arxiv.org/abs/2406.14703>`__ 法学硕士有鲜明一致的个性吗?特质:为法学硕士设计的心理测量人格测试集

`[2406.14709] Factual Dialogue Summarization via Learning from Large Language Models <https://arxiv.org/abs/2406.14709>`__ 基于大型语言模型学习的事实对话摘要

`[2406.14721] 1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators? <https://arxiv.org/abs/2406.14721>`__ 1+1>2:大型语言模型能否作为跨语言知识聚合器?

`[2406.14737] Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial Alterations to the False Belief Task? <https://arxiv.org/abs/2406.14737>`__

`[2406.14760] An LLM Feature-based Framework for Dialogue Constructiveness Assessment <https://arxiv.org/abs/2406.14760>`__ 基于LLM特征的对话建设性评估框架

`[2406.14763] A Learn-Then-Reason Model Towards Generalization in Knowledge Base Question Answering <https://arxiv.org/abs/2406.14763>`__ 面向知识库问答泛化的“先学习后推理”模型

`[2406.14805] How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions <https://arxiv.org/abs/2406.14805>`__ llm在多大程度上代表了跨文化的价值观?基于Hofstede文化维度的LLM回应实证分析

`[2406.14828] Word Matters: What Influences Domain Adaptation in Summarization? <https://arxiv.org/abs/2406.14828>`__ 词的重要性:什么影响摘要中的领域适应?

`[2406.14859] From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking <https://arxiv.org/abs/2406.14859>`__ 从llm到mllm:探索多模态越狱景观

`[2406.14877] Sports Intelligence: Assessing the Sports Understanding Capabilities of Language Models through Question Answering from Text to Video <https://arxiv.org/abs/2406.14877>`__ 运动智能:通过从文本到视频的问答评估语言模型的运动理解能力

`[2406.14882] 70B-parameter large language models in Japanese medical question-answering <https://arxiv.org/abs/2406.14882>`__ 日语医疗问答中的70b参数大型语言模型

`[2406.14883] OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants <https://arxiv.org/abs/2406.14883>`__ 誓言框架:与LLM助理描述对无家可归者的在线态度

`[2406.14887] InternLM-Law: An Open Source Chinese Legal Large Language Model <https://arxiv.org/abs/2406.14887>`__ InternLM-Law:一个开源的中文法律大型语言模型

`[2406.14894] Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition <https://arxiv.org/abs/2406.14894>`__ 说到做到并不需要做到做到:论词汇蕴涵识别中的大型语言模型的限制

`[2406.14952] ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models <https://arxiv.org/abs/2406.14952>`__ ESC-Eval:大型语言模型中情感支持对话的评估

`[2406.14979] Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation <https://arxiv.org/abs/2406.14979>`__ 检索-计划-生成:一种面向知识密集型LLM生成的迭代规划和回答框架

`[2406.14991] SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation <https://arxiv.org/abs/2406.14991>`__ 电子表格工作台:挑战现实世界电子表格操作

`[2406.15000] Unveiling the Impact of Multi-Modal Interactions on User Engagement: A Comprehensive Evaluation in AI-driven Conversations <https://arxiv.org/abs/2406.15000>`__ 揭示多模态交互对用户参与度的影响:人工智能驱动对话的综合评估

`[2406.15053] PARIKSHA : A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data <https://arxiv.org/abs/2406.15053>`__

`[2406.15109] Brain-Like Language Processing via a Shallow Untrained Multihead Attention Network <https://arxiv.org/abs/2406.15109>`__ 基于未经训练的浅层多头注意力网络的类脑语言处理

`[2406.15130] Assessing Good, Bad and Ugly Arguments Generated by ChatGPT: a New Dataset, its Methodology and Associated Tasks <https://arxiv.org/abs/2406.15130>`__ 评估ChatGPT生成的好、坏和丑陋的参数:一个新的数据集，它的方法和相关的任务

`[2406.15178] Hybrid Alignment Training for Large Language Models <https://arxiv.org/abs/2406.15178>`__ 大型语言模型的混合对齐训练

`[2406.15214] Unsupervised Extraction of Dialogue Policies from Conversations <https://arxiv.org/abs/2406.15214>`__ 对话策略的无监督抽取

`[2406.15227] A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation <https://arxiv.org/abs/2406.15227>`__ 一种基于llm的自动反叙事生成评估排序方法

`[2406.15231] Detecting Synthetic Lyrics with Few-Shot Inference <https://arxiv.org/abs/2406.15231>`__

`[2406.15267] Evaluating Diversity in Automatic Poetry Generation <https://arxiv.org/abs/2406.15267>`__ 诗歌自动生成的多样性评估

`[2406.15352] A SMART Mnemonic Sounds like "Glue Tonic": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick <https://arxiv.org/abs/2406.15352>`__ 一个聪明的助记器听起来像“胶水补剂”:混合LLMs与学生的反馈，使助记器学习粘

`[2406.14662] Advantage Alignment Algorithms <https://arxiv.org/abs/2406.14662>`__ 优势对齐算法

`[2406.14867] DistiLRR: Transferring Code Repair for Low-Resource Programming Languages <https://arxiv.org/abs/2406.14867>`__ DistiLRR:面向低资源编程语言的代码修复传输

`[2406.14909] MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression <https://arxiv.org/abs/2406.14909>`__ MoA:面向大规模语言模型自动压缩的混合稀疏注意力算法

`[2406.14956] Unlocking the Global Synergies in Low-Rank Adapters <https://arxiv.org/abs/2406.14956>`__ 在低等级适配器中释放全球协同效应

`[2406.14963] Optimised Grouped-Query Attention Mechanism for Transformers <https://arxiv.org/abs/2406.14963>`__ transformer优化的分组查询注意力机制

`[2406.14653] LLM Granularity for On-the-Fly Robot Control <https://arxiv.org/abs/2406.14653>`__ 用于实时机器人控制的LLM粒度

`[2406.14655] HYPERmotion: Learning Hybrid Behavior Planning for Autonomous Loco-manipulation <https://arxiv.org/abs/2406.14655>`__ HYPERmotion:自主位置操纵混合行为规划学习

`[2406.14871] I don't trust you (anymore)! -- The effect of students' LLM use on Lecturer-Student-Trust in Higher Education <https://arxiv.org/abs/2406.14871>`__ 我不再相信你了!——学生LLM使用对高等教育师生信任的影响

`[2406.15173] \'Evaluation des capacit\'es de r\'eponse de larges mod\`eles de langage (LLM) pour des questions d'historiens <https://arxiv.org/abs/2406.15173>`__ 对历史学家的评价、评价、评价、评价、评价、评价、评价、评价、评价、评价、评价、评价、评价、评价

`[2406.15259] V-RECS, a Low-Cost LLM4VIS Recommender with Explanations, Captioning and Suggestions <https://arxiv.org/abs/2406.15259>`__

`[2406.14898] Safely Learning with Private Data: A Federated Learning Framework for Large Language Model <https://arxiv.org/abs/2406.14898>`__ 私有数据安全学习:大型语言模型联邦学习框架

`[2406.15264] Towards Fine-Grained Citation Evaluation in Generated Text: A Comparative Analysis of Faithfulness Metrics <https://arxiv.org/abs/2406.15264>`__ 生成文本中的细粒度引文评价:忠实度指标比较分析

`[2312.01678] Jellyfish: A Large Language Model for Data Preprocessing <https://arxiv.org/abs/2312.01678>`__ Jellyfish:用于数据预处理的大型语言模型

`[2406.11160] Contextual Knowledge Graph <https://arxiv.org/abs/2406.11160>`__ 语境知识图谱

`[2406.12000] Look Further Ahead: Testing the Limits of GPT-4 in Path Planning <https://arxiv.org/abs/2406.12000>`__ 进一步展望:测试GPT-4在路径规划中的极限

`[2406.12043] Grade Score: Quantifying LLM Performance in Option Selection <https://arxiv.org/abs/2406.12043>`__ 等级分数:量化LLM在选项选择中的表现

`[2406.13919] SPL: A Socratic Playground for Learning Powered by Large Language Model <https://arxiv.org/abs/2406.13919>`__ SPL:由大型语言模型驱动的苏格拉底式学习游乐场

`[2406.14124] Measuring Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective <https://arxiv.org/abs/2406.14124>`__

`[2406.14408] FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving <https://arxiv.org/abs/2406.14408>`__ FVEL:基于定理证明的大型语言模型交互式形式化验证环境

`[2305.13712] Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models <https://arxiv.org/abs/2305.13712>`__ 知识中的知识:用大型语言模型探索已知-未知的不确定性

`[2308.01846] XNLP: An Interactive Demonstration System for Universal Structured NLP <https://arxiv.org/abs/2308.01846>`__ XNLP:通用结构化NLP交互演示系统

`[2311.04155] Black-Box Prompt Optimization: Aligning Large Language Models without Model Training <https://arxiv.org/abs/2311.04155>`__ 黑盒提示优化:无需模型训练即可对齐大型语言模型

`[2401.01301] Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models <https://arxiv.org/abs/2401.01301>`__ 大型法律小说:用大型语言模型分析法律幻觉

`[2401.06408] AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters <https://arxiv.org/abs/2401.06408>`__ 关于我:在网页中使用自我描述来记录英语预训练数据过滤器的效果

`[2402.11442] Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs <https://arxiv.org/abs/2402.11442>`__ llm可以用规则进行推理吗?为压力测试和改进LLMs的逻辑脚手架

`[2402.11943] LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation <https://arxiv.org/abs/2402.11943>`__

`[2402.14897] Chain-of-Thought Unfaithfulness as Disguised Accuracy <https://arxiv.org/abs/2402.14897>`__ 用思维链的不忠来伪装准确

`[2402.16379] TEaR: Improving LLM-based Machine Translation with Systematic Self-Refinement <https://arxiv.org/abs/2402.16379>`__ TEaR:系统的自细化改进基于llm的机器翻译

`[2403.00794] Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models <https://arxiv.org/abs/2403.00794>`__ 认真对待幽默:用无趣的大型语言模型制作幽默数据集

`[2404.03820] CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues <https://arxiv.org/abs/2404.03820>`__ CantTalkAboutThis:对齐语言模型以保持对话中的主题

`[2404.12038] Uncovering Safety Risks of Large Language Models through Concept Activation Vector <https://arxiv.org/abs/2404.12038>`__ 通过概念激活向量揭示大型语言模型的安全风险

`[2404.15949] CORM: Cache Optimization with Recent Message for Large Language Model Inference <https://arxiv.org/abs/2404.15949>`__ CORM:基于最新消息的大型语言模型推理缓存优化

`[2405.20654] Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models <https://arxiv.org/abs/2405.20654>`__ 基于大型语言模型问答中篇章重排序的篇章特定提示调优

`[2406.01538] What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores <https://arxiv.org/abs/2406.01538>`__ 大型语言模型在大脑中映射到什么?反对过度依赖大脑分数的案例

`[2406.06369] Annotation alignment: Comparing LLM and human annotations of conversational safety <https://arxiv.org/abs/2406.06369>`__ 注释对齐:比较LLM和会话安全性的人工注释

`[2406.07232] DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms <https://arxiv.org/abs/2406.07232>`__ Dual - reflect:利用双重学习反馈机制增强大型语言模型的反思性翻译

`[2406.07835] SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature <https://arxiv.org/abs/2406.07835>`__ SciRIFF:增强科学文献语言模型指导遵循的资源

`[2406.10248] On the Worst Prompt Performance of Large Language Models <https://arxiv.org/abs/2406.10248>`__ 大型语言模型的最差提示性能研究

`[2406.12702] [WIP] Jailbreak Paradox: The Achilles' Heel of LLMs <https://arxiv.org/abs/2406.12702>`__

`[2406.13476] LLMs Are Zero-Shot Context-Aware Simultaneous Translators <https://arxiv.org/abs/2406.13476>`__ llm是零样本的上下文感知同声传译

`[2402.00795] LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law <https://arxiv.org/abs/2402.00795>`__ llm学习动态系统的控制原理，揭示了上下文神经缩放定律

`[2402.10517] Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs <https://arxiv.org/abs/2402.10517>`__ 任意精度LLM:多个不同大小的LLM的低成本部署

`[2405.18039] Large Language Model-Driven Curriculum Design for Mobile Networks <https://arxiv.org/abs/2405.18039>`__ 大型语言模型驱动的移动网络课程设计

`[2406.14541] Are LLMs Naturally Good at Synthetic Tabular Data Generation? <https://arxiv.org/abs/2406.14541>`__ llm天生擅长合成表格数据生成吗?

`[2306.11232] Eight challenges in developing theory of intelligence <https://arxiv.org/abs/2306.11232>`__ 发展智力理论的八个挑战

`[2312.01797] LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics <https://arxiv.org/abs/2312.01797>`__ LLM A*: Human in the Loop大型语言模型实现了对机器人的搜索

`[2403.03230] Large language models surpass human experts in predicting neuroscience results <https://arxiv.org/abs/2403.03230>`__ 大型语言模型在预测神经科学结果方面超过了人类专家

