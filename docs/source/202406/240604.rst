240604
========

----------
Survey (1)
----------

`[2312.06717] Privacy Issues in Large Language Models: A Survey <https://arxiv.org/abs/2312.06717>`__ 大型语言模型中的隐私问题综述

::

    replaced with revised version Thu, 30 May 2024 19:26:05 GMT
    Submission history From: Peter Chang [view email]
    [v1] Mon, 11 Dec 2023 01:26:53 UTC (5,762 KB)
    [v2] Tue, 23 Jan 2024 21:56:31 UTC (5,764 KB)
    [v3] Tue, 20 Feb 2024 18:26:08 UTC (6,349 KB)
    [v4] Thu, 30 May 2024 19:26:05 UTC (6,364 KB)
    Seth Neel and Peter Chang

This is the first survey of the active area of AI research that focuses on privacy issues in Large Language Models (LLMs). Specifically, we focus on work that red-teams models to highlight privacy risks, attempts to build privacy into the training or inference process, enables efficient data deletion from trained models to comply with existing privacy regulations, and tries to mitigate copyright issues. Our focus is on summarizing technical research that develops algorithms, proves theorems, and runs empirical evaluations. While there is an extensive body of legal and policy work addressing these challenges from a different angle, that is not the focus of our survey. Nevertheless, these works, along with recent legal developments do inform how these technical problems are formalized, and so we discuss them briefly in Section 1. While we have made our best effort to include all the relevant work, due to the fast moving nature of this research we may have missed some recent work. If we have missed some of your work please contact us, as we will attempt to keep this survey relatively up to date. We are maintaining a repository with the list of papers covered in this survey and any relevant code that was publicly available at this https URL.

------------

-------------
Benchmark (7)
-------------

`[2405.20574] Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark <https://arxiv.org/abs/2405.20574>`__ Open Ko-LLM Leaderboard:用Ko-H5基准评估韩语大型语言模型

::

    Fri, 31 May 2024 02:05:45 GMT
    Chanjun Park, Hyeonwoo Kim, Dahyun Kim, Seonghwan Cho, Sanghoon Kim, Sukyung Lee, Yungi Kim, Hwalsuk Lee

This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean.
Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.

------------

`[2405.20859] clembench-2024: A Challenging, Dynamic, Complementary, Multilingual Benchmark and Underlying Flexible Framework for LLMs as Multi-Action Agents <https://arxiv.org/abs/2405.20859>`__ clembench-2024:一个具有挑战性的、动态的、互补的多语言基准和作为多动作智能体的llm的底层灵活框架

::

    Fri, 31 May 2024 14:43:31 GMT
    Anne Beyer, Kranti Chalamalasetti, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, David Schlangen

It has been established in recent work that Large Language Models (LLMs) can be prompted to "self-play" conversational games that probe certain capabilities (general instruction following, strategic goal orientation, language understanding abilities), where the resulting interactive game play can be automatically scored. In this paper, we take one of the proposed frameworks for setting up such game-play environments, and further test its usefulness as an evaluation instrument, along a number of dimensions: We show that it can easily keep up with new developments while avoiding data contamination, we show that the tests implemented within it are not yet saturated (human performance is substantially higher than that of even the best models), and we show that it lends itself to investigating additional questions, such as the impact of the prompting language on performance. We believe that the approach forms a good basis for making decisions on model choice for building applied interactive systems, and perhaps ultimately setting up a closed-loop development environment of system and simulated evaluator.

------------

`[2405.20947] OR-Bench: An Over-Refusal Benchmark for Large Language Models <https://arxiv.org/abs/2405.20947>`__ OR-Bench:大型语言模型的过度拒绝基准

::

    Fri, 31 May 2024 15:44:33 GMT
    Justin Cui, Wei-Lin Chiang, Ion Stoica, Cho-Jui Hsieh

Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where the LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that appear harmful but are benign. This study proposes a novel method for automatically generating large-scale sets of ``seemingly toxic prompts'' (benign prompts likely rejected by LLMs). Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 seemingly toxic prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 25 popular LLMs across 8 model families. Our datasets are available at https://huggingface.co/datasets/bench-llm/OR-Bench and the corresponding demo can be found at https://huggingface.co/spaces/bench-llm/or-bench. We hope this benchmark can help the community develop better safety aligned models.

------------

`[2405.20441] SECURE: Benchmarking Generative Large Language Models for Cybersecurity Advisory <https://arxiv.org/abs/2405.20441>`__ SECURE:面向网络安全咨询的生成式大型语言模型基准测试

::

    Thu, 30 May 2024 19:35:06 GMT
    Dipkamal Bhusal, Md Tanvirul Alam, Le Nguyen, Ashim Mahara, Zachary Lightcap, Rodney Frazier, Romy Fieblinger, Grace Long Torales, Nidhi Rastogi

Large Language Models (LLMs) have demonstrated potential in cybersecurity applications but have also caused lower confidence due to problems like hallucinations and a lack of truthfulness. Existing benchmarks provide general evaluations but do not sufficiently address the practical and applied aspects of LLM performance in cybersecurity-specific tasks. To address this gap, we introduce the SECURE (Security Extraction, Understanding \& Reasoning Evaluation), a benchmark designed to assess LLMs performance in realistic cybersecurity scenarios. SECURE includes six datasets focussed on the Industrial Control System sector to evaluate knowledge extraction, understanding, and reasoning based on industry-standard sources. Our study evaluates seven state-of-the-art models on these tasks, providing insights into their strengths and weaknesses in cybersecurity contexts, and offer recommendations for improving LLMs reliability as cyber advisory tools.

------------

`[2405.21075] Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis <https://arxiv.org/abs/2405.21075>`__ Video- mme:视频分析中第一个多模态llm的综合评估基准

::

    Fri, 31 May 2024 17:59:47 GMT
    Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, Xing Sun

In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However, the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs in processing sequential visual data is still insufficiently explored, highlighting the absence of a comprehensive, high-quality assessment of their performance. In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. 900 videos with a total of 256 hours are manually selected and annotated by repeatedly viewing all the video content, resulting in 2,700 question-answer pairs. With Video-MME, we extensively evaluate various state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as open-source image models like InternVL-Chat-V1.5 and video models like LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data. Project Page: https://video-mme.github.io

------------

`[2310.00835] TRAM: Benchmarking Temporal Reasoning for Large Language Models <https://arxiv.org/abs/2310.00835>`__ TRAM:大型语言模型时序推理基准测试

::

    replaced with revised version Fri, 31 May 2024 15:36:09 GMT
    Submission history From: Yuqing Wang [view email]
    [v1] Mon, 2 Oct 2023 00:59:07 UTC (442 KB)
    [v2] Tue, 3 Oct 2023 13:54:02 UTC (442 KB)
    [v3] Fri, 31 May 2024 15:36:09 UTC (493 KB)
    Yuqing Wang, Yun Zhao

Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the TeR capabilities of large language models (LLMs). We evaluate popular LLMs like GPT-4 and Llama2 in zero-shot and few-shot scenarios, and establish baselines with BERT-based and domain-specific models. Our findings indicate that the best-performing model lags significantly behind human performance. It is our aspiration that TRAM will spur further progress in enhancing the TeR capabilities of LLMs.

------------

`[2405.07960] AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments <https://arxiv.org/abs/2405.07960>`__ AgentClinic:在模拟临床环境中评估人工智能的多模态智能体基准

::

    replaced with revised version Thu, 30 May 2024 22:56:17 GMT
    Submission history From: Samuel Schmidgall [view email]
    [v1] Mon, 13 May 2024 17:38:53 UTC (9,883 KB)
    [v2] Wed, 22 May 2024 01:57:23 UTC (9,914 KB)
    [v3] Thu, 30 May 2024 22:56:17 UTC (9,638 KB)
    Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, Michael Moor

Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information -- such as which tests to perform -- and to act upon it. Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care. However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work. Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments. In our benchmark, the doctor agent must uncover the patient's diagnosis through dialogue and active data collection. We present two open medical agent benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents. We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents. Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA. We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark. We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents. The code and data for this work is publicly available at this https URL.

------------

--------------
Accelerate (8)
--------------

`[2405.21047] Grammar-Aligned Decoding <https://arxiv.org/abs/2405.21047>`__ Grammar-Aligned解码

::

    Fri, 31 May 2024 17:39:15 GMT
    Kanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick, Nadia Polikarpova, Loris D'Antoni

Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.

------------

`[2405.20495] Transfer Q Star: Principled Decoding for LLM Alignment <https://arxiv.org/abs/2405.20495>`__ Transfer Q Star: LLM对齐的原则性解码

::

    Thu, 30 May 2024 21:36:12 GMT
    Souradip Chakraborty, Soumya Suvra Ghosal, Ming Yin, Dinesh Manocha, Mengdi Wang, Amrit Singh Bedi, and Furong Huang

Aligning foundation models is essential for their safe and trustworthy deployment. However, traditional fine-tuning methods are computationally intensive and require updating billions of model parameters. A promising alternative, alignment via decoding, adjusts the response distribution directly without model updates to maximize a target reward $r$, thus providing a lightweight and adaptable framework for alignment. However, principled decoding methods rely on oracle access to an optimal Q-function ($Q^*$), which is often unavailable in practice. Hence, prior SoTA methods either approximate this $Q^*$ using $Q^{\pi_{\texttt{sft}}}$ (derived from the reference $\texttt{SFT}$ model) or rely on short-term rewards, resulting in sub-optimal decoding performance. In this work, we propose Transfer $Q^*$, which implicitly estimates the optimal value function for a target reward $r$ through a baseline model $\rho_{\texttt{BL}}$ aligned with a baseline reward $\rho_{\texttt{BL}}$ (which can be different from the target reward $r$). Theoretical analyses of Transfer $Q^*$ provide a rigorous characterization of its optimality, deriving an upper bound on the sub-optimality gap and identifying a hyperparameter to control the deviation from the pre-trained reference $\texttt{SFT}$ model based on user needs. Our approach significantly reduces the sub-optimality gap observed in prior SoTA methods and demonstrates superior empirical performance across key metrics such as coherence, diversity, and quality in extensive tests on several synthetic and real datasets.

------------

`[2405.21046] Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF <https://arxiv.org/abs/2405.21046>`__ 探索性偏好优化:利用隐式Q*近似实现样本高效RLHF

::

    Fri, 31 May 2024 17:39:06 GMT
    Tengyang Xie, Dylan J. Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, Alexander Rakhlin

Reinforcement learning from human feedback (RLHF) has emerged as a central tool for language model alignment. We consider online exploration in RLHF, which exploits interactive access to human or AI feedback by deliberately encouraging the model to produce diverse, maximally informative responses. By allowing RLHF to confidently stray from the pre-trained model, online exploration offers the possibility of novel, potentially super-human capabilities, but its full potential as a paradigm for language model training has yet to be realized, owing to computational and statistical bottlenecks in directly adapting existing reinforcement learning techniques. We propose a new algorithm for online exploration in RLHF, Exploratory Preference Optimization (XPO), which is simple and practical -- a one-line change to (online) Direct Preference Optimization (DPO; Rafailov et al., 2023) -- yet enjoys the strongest known provable guarantees and promising empirical performance. XPO augments the DPO objective with a novel and principled exploration bonus, empowering the algorithm to explore outside the support of the initial model and human feedback data. In theory, we show that XPO is provably sample-efficient and converges to a near-optimal language model policy under natural exploration conditions, irrespective of whether the initial model has good coverage. Our analysis, which builds on the observation that DPO implicitly performs a form of $Q^{\star}$-approximation (or, Bellman error minimization), combines previously disparate techniques from language modeling and theoretical reinforcement learning in a serendipitous fashion through the perspective of KL-regularized Markov decision processes. Empirically, we find that XPO is more sample-efficient than non-exploratory DPO variants in a preliminary evaluation.

------------

`[2305.15805] Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers <https://arxiv.org/abs/2305.15805>`__ 高效可解释的自回归transformer的动态上下文修剪

::

    replaced with revised version Fri, 31 May 2024 14:02:24 GMT
    Submission history From: Sotiris Anagnostidis [view email]
    [v1] Thu, 25 May 2023 07:39:41 UTC (2,041 KB)
    [v2] Sun, 28 May 2023 12:11:11 UTC (2,042 KB)
    [v3] Fri, 31 May 2024 14:02:24 UTC (2,041 KB)
    Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, Thomas Hofmann

Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\times$ increase in inference throughput and even greater memory savings.

------------

`[2310.18339] When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications <https://arxiv.org/abs/2310.18339>`__ 当MOE遇到LLMs:多任务医疗应用的参数高效微调

::

    replaced with revised version Fri, 31 May 2024 07:56:08 GMT
    Submission history From: Qidong Liu [view email]
    [v1] Sat, 21 Oct 2023 17:18:09 UTC (4,582 KB)
    [v2] Fri, 31 May 2024 07:56:08 UTC (4,707 KB)
    Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, Yefeng Zheng

The recent surge in Large Language Models (LLMs) has garnered significant attention across numerous fields. Fine-tuning is often required to fit general LLMs for a specific domain, like the web-based healthcare system. However, two problems arise during fine-tuning LLMs for medical applications. One is the task variety problem, which involves distinct tasks in real-world medical scenarios. The variety often leads to sub-optimal fine-tuning for data imbalance and seesaw problems. Besides, the large amount of parameters in LLMs leads to huge time and computation consumption by fine-tuning. To address these two problems, we propose a novel parameter efficient fine-tuning framework for multi-task medical applications, dubbed as MOELoRA. The designed framework aims to absorb both the benefits of mixture-of-expert (MOE) for multi-task learning and low-rank adaptation (LoRA) for parameter efficient fine-tuning. For unifying MOE and LoRA, we devise multiple experts as the trainable parameters, where each expert consists of a pair of low-rank matrices to retain the small size of trainable parameters. Then, a task-motivated gate function for all MOELoRA layers is proposed, which can control the contributions of each expert and produce distinct parameters for various tasks. We conduct experiments on a multi-task medical dataset, indicating MOELoRA outperforms the existing parameter efficient fine-tuning methods. The code is available online.

------------

`[2405.19325] Nearest Neighbor Speculative Decoding for LLM Generation and Attribution <https://arxiv.org/abs/2405.19325>`__ 用于LLM生成和归因的最近邻推测解码

::

    replaced with revised version Fri, 31 May 2024 01:41:49 GMT
    Submission history From: Minghan Li [view email]
    [v1] Wed, 29 May 2024 17:55:03 UTC (4,395 KB)
    [v2] Fri, 31 May 2024 01:41:49 UTC (1,458 KB)
    Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, Xi Victoria Lin

Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts. In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B.

------------

`[2402.04513] Online Cascade Learning for Efficient Inference over Streams <https://arxiv.org/abs/2402.04513>`__ 基于在线级联学习的流高效推理

::

    replaced with revised version Fri, 31 May 2024 15:59:34 GMT
    Submission history From: Lunyiu Nie [view email]
    [v1] Wed, 7 Feb 2024 01:46:50 UTC (1,975 KB)
    [v2] Fri, 31 May 2024 15:59:34 UTC (2,078 KB)
    Lunyiu Nie, Zhimin Ding, Erdong Hu, Christopher Jermaine, Swarat Chaudhuri

Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to address this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regression) and ending with a powerful LLM, along with a deferral policy that determines the model to be used on a given input. We formulate the task of learning cascades online as an imitation-learning problem, where smaller models are updated over time imitating the collected LLM demonstrations, and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90% with strong robustness against input distribution shifts, underscoring its efficacy and adaptability in stream processing.

------------

`[2402.09723] Efficient Prompt Optimization Through the Lens of Best Arm Identification <https://arxiv.org/abs/2402.09723>`__ 通过最佳手臂识别的镜头高效快速优化

::

    replaced with revised version Thu, 30 May 2024 19:40:21 GMT
    Submission history From: Chengshuai Shi [view email]
    [v1] Thu, 15 Feb 2024 05:31:13 UTC (2,609 KB)
    [v2] Tue, 20 Feb 2024 06:35:36 UTC (3,044 KB)
    [v3] Thu, 30 May 2024 19:40:21 UTC (1,391 KB)
    Chengshuai Shi, Kun Yang, Zihan Chen, Jundong Li, Jing Yang and Cong Shen

The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically finding good prompts, i.e., prompt optimization. Most existing works follow the scheme of selecting from a pre-generated pool of candidate prompts. However, these designs mainly focus on the generation strategy, while limited attention has been paid to the selection method. Especially, the cost incurred during the selection (e.g., accessing LLM and evaluating the responses) is rarely explicitly considered. To overcome this limitation, this work provides a principled framework, TRIPLE, to efficiently perform prompt selection under an explicit budget constraint. TRIPLE is built on a novel connection established between prompt optimization and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB); thus, it is capable of leveraging the rich toolbox from BAI-FB systematically and also incorporating unique characteristics of prompt optimization. Extensive experiments on multiple well-adopted tasks using various LLMs demonstrate the remarkable performance improvement of TRIPLE over baselines while satisfying the limited budget constraints. As an extension, variants of TRIPLE are proposed to efficiently select examples for few-shot prompts, also achieving superior empirical performance.

------------

-------------
Reasoning (4)
-------------

`[2405.20535] Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning <https://arxiv.org/abs/2405.20535>`__ 编码数据指令微调对大型语言模型推理的影响

::

    Thu, 30 May 2024 23:20:25 GMT
    Xinlu Zhang, Zhiyu Zoey Chen, Xi Ye, Xianjun Yang, Lichang Chen, William Yang Wang, Linda Ruth Petzold

Instruction Fine-Tuning (IFT) significantly enhances the zero-shot capabilities of pretrained Large Language Models (LLMs). While coding data is known to boost reasoning abilities during LLM pretraining, its role in activating internal reasoning capacities during IFT remains understudied. This paper investigates a key question: How does coding data impact LLMs' reasoning capacities during the IFT stage? To explore this, we thoroughly examine the impact of coding data across different coding data proportions, model families, sizes, and reasoning domains, from various perspectives. Specifically, we create three IFT datasets with increasing coding data proportions, fine-tune six LLM backbones across different families and scales on these datasets, evaluate the tuned models' performance across twelve tasks in three reasoning domains, and analyze the outcomes from three broad-to-granular perspectives: overall, domain-level, and task-specific. Our holistic analysis provides valuable insights in each perspective. First, coding data tuning enhances the overall reasoning capabilities of LLMs across different model families and scales. Moreover, the effect of coding data varies among different domains but shows consistent trends across model families and scales within each domain.
Additionally, coding data generally yields comparable task-specific benefits across different model families, with the optimal coding data proportions in IFT datasets being task-specific.

------------

`[2405.20902] Preemptive Answer "Attacks" on Chain-of-Thought Reasoning <https://arxiv.org/abs/2405.20902>`__ 先发制人的回答“攻击”思维链推理

::

    Fri, 31 May 2024 15:15:04 GMT
    Rongwu Xu, Zehan Qi, Wei Xu

Large language models (LLMs) showcase impressive reasoning capabilities when coupled with Chain-of-Thought (CoT) prompting. However, the robustness of this approach warrants further investigation. In this paper, we introduce a novel scenario termed preemptive answers, where the LLM obtains an answer before engaging in reasoning. This situation can arise inadvertently or induced by malicious users by prompt injection attacks. Experiments reveal that preemptive answers significantly impair the model's reasoning capability across various CoT methods and a broad spectrum of datasets. To bolster the robustness of reasoning, we propose two measures aimed at mitigating this issue to some extent.

------------

`[2310.00835] TRAM: Benchmarking Temporal Reasoning for Large Language Models <https://arxiv.org/abs/2310.00835>`__ TRAM:大型语言模型时序推理基准测试

::

    replaced with revised version Fri, 31 May 2024 15:36:09 GMT
    Submission history From: Yuqing Wang [view email]
    [v1] Mon, 2 Oct 2023 00:59:07 UTC (442 KB)
    [v2] Tue, 3 Oct 2023 13:54:02 UTC (442 KB)
    [v3] Fri, 31 May 2024 15:36:09 UTC (493 KB)
    Yuqing Wang, Yun Zhao

Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the TeR capabilities of large language models (LLMs). We evaluate popular LLMs like GPT-4 and Llama2 in zero-shot and few-shot scenarios, and establish baselines with BERT-based and domain-specific models. Our findings indicate that the best-performing model lags significantly behind human performance. It is our aspiration that TRAM will spur further progress in enhancing the TeR capabilities of LLMs.

------------

`[2309.05660] Hypothesis Search: Inductive Reasoning with Language Models <https://arxiv.org/abs/2309.05660>`__ 假设搜索:基于语言模型的归纳推理

::

    replaced with revised version Thu, 30 May 2024 23:10:00 GMT
    Submission history From: Ruocheng Wang [view email]
    [v1] Mon, 11 Sep 2023 17:56:57 UTC (278 KB)
    [v2] Thu, 30 May 2024 23:10:00 UTC (343 KB)
    Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D. Goodman

Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which robustly generalize to novel scenarios. Recent work evaluates large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding "in context learning." This works well for straightforward inductive tasks but performs poorly on complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be verified by running on observed examples and generalized to novel inputs. To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, string transformation dataset SyGuS, and list transformation dataset List Functions. On a random 100-problem subset of ARC, our automated pipeline using LLM summaries achieves 30% accuracy, outperforming the direct prompting baseline (accuracy of 17%). With the minimal human input of selecting from LLM-generated candidates, performance is boosted to 33%. Our ablations show that both abstract hypothesis generation and concrete program representations benefit LLMs on inductive reasoning tasks.

------------

-----------
ToolUse (5)
-----------

`[2405.20529] An Automatic Question Usability Evaluation Toolkit <https://arxiv.org/abs/2405.20529>`__ 自动问题可用性评估工具包

::

    Thu, 30 May 2024 23:04:53 GMT
    Steven Moore, Eamon Costello, Huy A. Nguyen, John Stamper

Evaluating multiple-choice questions (MCQs) involves either labor intensive human assessments or automated methods that prioritize readability, often overlooking deeper question design flaws. To address this issue, we introduce the Scalable Automatic Question Usability Evaluation Toolkit (SAQUET), an open-source tool that leverages the Item-Writing Flaws (IWF) rubric for a comprehensive and automated quality evaluation of MCQs. By harnessing the latest in large language models such as GPT-4, advanced word embeddings, and Transformers designed to analyze textual complexity, SAQUET effectively pinpoints and assesses a wide array of flaws in MCQs. We first demonstrate the discrepancy between commonly used automated evaluation metrics and the human assessment of MCQ quality. Then we evaluate SAQUET on a diverse dataset of MCQs across the five domains of Chemistry, Statistics, Computer Science, Humanities, and Healthcare, showing how it effectively distinguishes between flawed and flawless questions, providing a level of analysis beyond what is achievable with traditional metrics. With an accuracy rate of over 94% in detecting the presence of flaws identified by human evaluators, our findings emphasize the limitations of existing evaluation methods and showcase potential in improving the quality of educational assessments.

------------

`[2405.20956] A Robot Walks into a Bar: Can Language Models Serve asCreativity Support Tools for Comedy? An Evaluation of LLMs' Humour Alignment with Comedians <https://arxiv.org/abs/2405.20956>`__ 机器人走进酒吧:语言模型可以作为喜剧的创造力支持工具吗?llm与喜剧演员的幽默一致性评估

::

    Fri, 31 May 2024 15:55:51 GMT
    Piotr Wojciech Mirowski, Juliette Love, Kory W. Mathewson, Shakir Mohamed

We interviewed twenty professional comedians who perform live shows in front of audiences and who use artificial intelligence in their artistic process as part of 3-hour workshops on ``AI x Comedy'' conducted at the Edinburgh Festival Fringe in August 2023 and online. The workshop consisted of a comedy writing session with large language models (LLMs), a human-computer interaction questionnaire to assess the Creativity Support Index of AI as a writing tool, and a focus group interrogating the comedians' motivations for and processes of using AI, as well as their ethical concerns about bias, censorship and copyright. Participants noted that existing moderation strategies used in safety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by erasing minority groups and their perspectives, and qualified this as a form of censorship. At the same time, most participants felt the LLMs did not succeed as a creativity support tool, by producing bland and biased comedy tropes, akin to ``cruise ship comedy material from the 1950s, but a bit less racist''. Our work extends scholarship about the subtle difference between, one the one hand, harmful speech, and on the other hand, ``offensive'' language as a practice of resistance, satire and ``punching up''. We also interrogate the global value alignment behind such language models, and discuss the importance of community-based value alignment and data ownership to build AI tools that better suit artists' needs.

------------

`[2405.20362] Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools <https://arxiv.org/abs/2405.20362>`__ Hallucination-Free吗?评估领先的AI法律研究工具的可靠性

::

    Thu, 30 May 2024 17:56:05 GMT
    Varun Magesh, Faiz Surani, Matthew Dahl, Mirac Suzgun, Christopher D. Manning, Daniel E. Ho

Legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. But the large language models used in these tools are prone to "hallucinate," or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation (RAG) as "eliminating" (Casetext, 2023) or "avoid[ing]" hallucinations (Thomson Reuters, 2023), or guaranteeing "hallucination-free" legal citations (LexisNexis, 2023). Because of the closed nature of these systems, systematically assessing these claims is challenging. In this article, we design and report on the first preregistered empirical evaluation of AI-driven legal research tools. We demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots (GPT-4), we find that the AI research tools made by LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and Ask Practical Law AI) each hallucinate between 17% and 33% of the time. We also document substantial differences between systems in responsiveness and accuracy. Our article makes four key contributions. It is the first to assess and report the performance of RAG-based proprietary legal AI tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying AI outputs, which remains a central open question for the responsible integration of AI into law.

------------

`[2402.09615] API Pack: A Massive Multi-Programming Language Dataset for API Call Generation <https://arxiv.org/abs/2402.09615>`__ API包:用于API调用生成的大规模多编程语言数据集

::

    replaced with revised version Fri, 31 May 2024 17:31:38 GMT
    Submission history From: Zhen Guo [view email]
    [v1] Wed, 14 Feb 2024 23:09:15 UTC (1,561 KB)
    [v2] Fri, 16 Feb 2024 13:58:38 UTC (1,561 KB)
    [v3] Fri, 31 May 2024 17:31:38 UTC (708 KB)
    Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda

We introduce API Pack, a massive multi-programming language dataset containing more than 1 million instruction-API call pairs to improve the API call generation capabilities of large language models. By fine-tuning CodeLlama-13B on 20,000 Python instances from API Pack, we achieved around 10% and 5% higher accuracy compared to GPT-3.5 and GPT-4, respectively, in generating unseen API calls. Fine-tuning on API Pack enables cross-programming language generalization by leveraging a large amount of data in one language and small amounts of data from other languages. Scaling the training data to 1 million instances further improves the model's generalization to new APIs not encountered during training. We open-source the API Pack dataset, trained models, and associated source code at this https URL to facilitate further research.

------------

`[2403.03031] Learning to Use Tools via Cooperative and Interactive Agents <https://arxiv.org/abs/2403.03031>`__ 通过合作和交互式代理学习使用工具

::

    replaced with revised version Fri, 31 May 2024 07:42:44 GMT
    Submission history From: Zhengliang Shi [view email]
    [v1] Tue, 5 Mar 2024 15:08:16 UTC (1,443 KB)
    [v2] Sun, 26 May 2024 11:49:56 UTC (1,446 KB)
    [v3] Fri, 31 May 2024 07:42:44 UTC (1,443 KB)
    Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, Zhaochun Ren

Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the superiority of our ConAgents (e.g., 6 point improvement over the SOTA baseline). We further provide fine-granularity analysis for the efficiency and consistency of our framework.

------------

-----------------------
Retrieval-Augmented (6)
-----------------------

`[2405.20680] Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2405.20680>`__ 检索增强大型语言模型中检索器不一致性的解开和缓解

::

    Fri, 31 May 2024 08:22:49 GMT
    Mingda Li, Xinyu Li, Yifan Chen, Wenfeng Xuan, Weinan Zhang

Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their superiority in terms of factuality, they do not consistently outperform the original retrieval-free Language Models (LMs). Our experiments reveal that this example-level performance inconsistency exists not only between retrieval-augmented and retrieval-free LM but also among different retrievers.
To understand this phenomenon, we investigate the degeneration behavior of RALMs and theoretically decompose it into four categories. Further analysis based on our decomposition reveals that the innate difference in knowledge sources and the unpredictable degeneration of the reader model contribute most to the inconsistency. Drawing from our analysis, we introduce Ensemble of Retrievers (EoR), a trainable framework that can adaptively retrieve from different knowledge sources and effectively decrease unpredictable reader errors. Our experiments on Open Domain Question Answering show that EoR substantially improves performance over the RALM with a single retriever by considerably reducing inconsistent behaviors.

------------

`[2405.20978] Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training <https://arxiv.org/abs/2405.20978>`__ 基于自适应对抗训练增强检索增强语言模型的噪声鲁棒性

::

    Fri, 31 May 2024 16:24:53 GMT
    Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen and Ruifeng Xu

Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses.
Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT).
RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT.

------------

`[2405.20624] Leveraging Large Language Models for Entity Matching <https://arxiv.org/abs/2405.20624>`__ 利用大型语言模型进行实体匹配

::

    Fri, 31 May 2024 05:22:07 GMT
    Qianyu Huang and Tongfang Zhao

Entity matching (EM) is a critical task in data integration, aiming to identify records across different datasets that refer to the same real-world entities. Traditional methods often rely on manually engineered features and rule-based systems, which struggle with diverse and unstructured data. The emergence of Large Language Models (LLMs) such as GPT-4 offers transformative potential for EM, leveraging their advanced semantic understanding and contextual capabilities. This vision paper explores the application of LLMs to EM, discussing their advantages, challenges, and future research directions.
Additionally, we review related work on applying weak supervision and unsupervised approaches to EM, highlighting how LLMs can enhance these methods.

------------

`[2405.20485] Phantom: General Trigger Attacks on Retrieval Augmented Language Generation <https://arxiv.org/abs/2405.20485>`__ 幻影:检索增强语言生成的一般触发攻击

::

    Thu, 30 May 2024 21:19:24 GMT
    Harsh Chaudhari, Giorgio Severi, John Abascal, Matthew Jagielski, Christopher A. Choquette-Choo, Milad Nasr, Cristina Nita-Rotaru, Alina Oprea

Retrieval Augmented Generation (RAG) expands the capabilities of modern large language models (LLMs) in chatbot applications, enabling developers to adapt and personalize the LLM output without expensive training or fine-tuning. RAG systems use an external knowledge database to retrieve the most relevant documents for a given query, providing this context to the LLM generator. While RAG achieves impressive utility in many applications, its adoption to enable personalized generative models introduces new security risks. In this work, we propose new attack surfaces for an adversary to compromise a victim's RAG system, by injecting a single malicious document in its knowledge database. We design Phantom, general two-step attack framework against RAG augmented LLMs.
The first step involves crafting a poisoned document designed to be retrieved by the RAG system within the top-k results only when an adversarial trigger, a specific sequence of words acting as backdoor, is present in the victim's queries. In the second step, a specially crafted adversarial string within the poisoned document triggers various adversarial attacks in the LLM generator, including denial of service, reputation damage, privacy violations, and harmful behaviors. We demonstrate our attacks on multiple LLM architectures, including Gemma, Vicuna, and Llama.

------------

`[2405.19670] One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2405.19670>`__ 一个代币就能帮上忙!为检索增强的大型语言模型学习可扩展和可插拔的虚拟token

::

    replaced with revised version Fri, 31 May 2024 02:56:56 GMT
    Submission history From: Yutao Zhu [view email]
    [v1] Thu, 30 May 2024 03:44:54 UTC (264 KB)
    [v2] Fri, 31 May 2024 02:56:56 UTC (264 KB)
    Yutao Zhu, Zhaoheng Huang, Zhicheng Dou, Ji-Rong Wen

Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune the LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capacities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across nine question-answering tasks demonstrate the superiority of our approach.

------------

`[2405.13401] TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models <https://arxiv.org/abs/2405.13401>`__ TrojanRAG:检索增强生成可以成为大型语言模型的后门驱动程序

::

    replaced with revised version Fri, 31 May 2024 16:59:17 GMT
    Submission history From: Pengzhou Cheng [view email]
    [v1] Wed, 22 May 2024 07:21:32 UTC (11,333 KB)
    [v2] Fri, 24 May 2024 06:12:51 UTC (11,333 KB)
    [v3] Fri, 31 May 2024 16:59:17 UTC (12,698 KB)
    Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Ping Yi, Zhuosheng Zhang, Gongshen Liu

Large language models (LLMs) have raised concerns about potential security threats despite performing significantly in Natural Language Processing (NLP). Backdoor attacks initially verified that LLM is doing substantial harm at all stages, but the cost and robustness have been criticized. Attacking LLMs is inherently risky in security review, while prohibitively expensive. Besides, the continuous iteration of LLMs will degrade the robustness of backdoors. In this paper, we propose TrojanRAG, which employs a joint backdoor attack in the Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack scenarios. Specifically, the adversary constructs elaborate target contexts and trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized by contrastive learning, thus constraining the triggering conditions to a parameter subspace to improve the matching. To improve the recall of the RAG for the target contexts, we introduce a knowledge graph to construct structured data to achieve hard matching at a fine-grained level. Moreover, we normalize the backdoor scenarios in LLMs to analyze the real harm caused by backdoors from both attackers' and users' perspectives and further verify whether the context is a favorable tool for jailbreaking models. Extensive experimental results on truthfulness, language understanding, and harmfulness show that TrojanRAG exhibits versatility threats while maintaining retrieval capabilities on normal queries.

------------

---------
Agent (7)
---------

`[2405.20770] Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent <https://arxiv.org/abs/2405.20770>`__ 大型语言模型哨兵:利用LLM Agent提高对抗性鲁棒性

::

    Fri, 24 May 2024 07:23:56 GMT
    Guang Lin and Qibin Zhao

Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs.
Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for defense, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.

------------

`[2405.20859] clembench-2024: A Challenging, Dynamic, Complementary, Multilingual Benchmark and Underlying Flexible Framework for LLMs as Multi-Action Agents <https://arxiv.org/abs/2405.20859>`__ clembench-2024:一个具有挑战性的、动态的、互补的多语言基准和作为多动作智能体的llm的底层灵活框架

::

    Fri, 31 May 2024 14:43:31 GMT
    Anne Beyer, Kranti Chalamalasetti, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, David Schlangen

It has been established in recent work that Large Language Models (LLMs) can be prompted to "self-play" conversational games that probe certain capabilities (general instruction following, strategic goal orientation, language understanding abilities), where the resulting interactive game play can be automatically scored. In this paper, we take one of the proposed frameworks for setting up such game-play environments, and further test its usefulness as an evaluation instrument, along a number of dimensions: We show that it can easily keep up with new developments while avoiding data contamination, we show that the tests implemented within it are not yet saturated (human performance is substantially higher than that of even the best models), and we show that it lends itself to investigating additional questions, such as the impact of the prompting language on performance. We believe that the approach forms a good basis for making decisions on model choice for building applied interactive systems, and perhaps ultimately setting up a closed-loop development environment of system and simulated evaluator.

------------

`[2401.12459] Towards Socially and Morally Aware RL agent: Reward Design With LLM <https://arxiv.org/abs/2401.12459>`__ 具有社会和道德意识的RL智能体:LLM奖励设计

::

    replaced with revised version Thu, 30 May 2024 20:40:30 GMT
    Submission history From: Zhaoyue Wang [view email]
    [v1] Tue, 23 Jan 2024 03:00:03 UTC (2,913 KB)
    [v2] Thu, 30 May 2024 20:40:30 UTC (2,912 KB)
    Zhaoyue Wang

When we design and deploy an Reinforcement Learning (RL) agent, reward functions motivates agents to achieve an objective. An incorrect or incomplete specification of the objective can result in behavior that does not align with human values - failing to adhere with social and moral norms that are ambiguous and context dependent, and cause undesired outcomes such as negative side effects and exploration that is unsafe. Previous work have manually defined reward functions to avoid negative side effects, use human oversight for safe exploration, or use foundation models as planning tools. This work studies the ability of leveraging Large Language Models (LLM)' understanding of morality and social norms on safe exploration augmented RL methods. This work evaluates language model's result against human feedbacks and demonstrates language model's capability as direct reward signals.

------------

`[2311.09510] Tailoring with Targeted Precision: Edit-Based Agents for Open-Domain Procedure Customization <https://arxiv.org/abs/2311.09510>`__ 基于编辑的开放域过程定制agent

::

    replaced with revised version Fri, 31 May 2024 01:32:23 GMT
    Submission history From: Yash Kumar Lal [view email]
    [v1] Thu, 16 Nov 2023 02:25:36 UTC (9,178 KB)
    [v2] Fri, 1 Mar 2024 16:04:28 UTC (9,654 KB)
    [v3] Fri, 31 May 2024 01:32:23 UTC (9,658 KB)
    Yash Kumar Lal and Li Zhang and Faeze Brahman and Bodhisattwa Prasad Majumder and Peter Clark and Niket Tandon

How-to procedures, such as how to plant a garden, are now used by millions of users, but sometimes need customizing to meet a user's specific needs, e.g., planting a garden without pesticides. Our goal is to measure and improve an LLM's ability to perform such customization. Our approach is to test several simple multi-LLM-agent architectures for customization, as well as an end-to-end LLM, using a new evaluation set, called CustomPlans, of over 200 WikiHow procedures each with a customization need. We find that a simple architecture with two LLM agents used sequentially performs best, one that edits a generic how-to procedure and one that verifies its executability, significantly outperforming (10.5% absolute) an end-to-end prompted LLM. This suggests that LLMs can be configured reasonably effectively for procedure customization. This also suggests that multi-agent editing architectures may be worth exploring further for other customization applications (e.g. coding, creative writing) in the future.

------------

`[2403.03031] Learning to Use Tools via Cooperative and Interactive Agents <https://arxiv.org/abs/2403.03031>`__ 通过合作和交互式代理学习使用工具

::

    replaced with revised version Fri, 31 May 2024 07:42:44 GMT
    Submission history From: Zhengliang Shi [view email]
    [v1] Tue, 5 Mar 2024 15:08:16 UTC (1,443 KB)
    [v2] Sun, 26 May 2024 11:49:56 UTC (1,446 KB)
    [v3] Fri, 31 May 2024 07:42:44 UTC (1,443 KB)
    Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, Zhaochun Ren

Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the superiority of our ConAgents (e.g., 6 point improvement over the SOTA baseline). We further provide fine-granularity analysis for the efficiency and consistency of our framework.

------------

`[2404.16698] Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents <https://arxiv.org/abs/2404.16698>`__ 合作还是崩溃:LLM主体社会中可持续性行为的出现

::

    replaced with revised version Fri, 31 May 2024 14:03:10 GMT
    Submission history From: Giorgio Piatti [view email]
    [v1] Thu, 25 Apr 2024 15:59:16 UTC (616 KB)
    [v2] Fri, 31 May 2024 14:03:10 UTC (887 KB)
    Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Sch\"olkopf, Mrinmaya Sachan, Rada Mihalcea

As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions is a significant challenge. This paper introduces the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. Using GovSim, we investigate the dynamics of sustainable resource sharing in a society of AI agents. This environment allows us to study the influence of ethical considerations, strategic planning, and negotiation skills on cooperative outcomes for AI agents. We develop an LLM-based agent architecture designed for these social dilemmas and test it with a variety of LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage ``Universalization''-based reasoning, a theory of moral thinking, are able to achieve significantly greater sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with significant specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.

------------

`[2405.07960] AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments <https://arxiv.org/abs/2405.07960>`__ AgentClinic:在模拟临床环境中评估人工智能的多模态智能体基准

::

    replaced with revised version Thu, 30 May 2024 22:56:17 GMT
    Submission history From: Samuel Schmidgall [view email]
    [v1] Mon, 13 May 2024 17:38:53 UTC (9,883 KB)
    [v2] Wed, 22 May 2024 01:57:23 UTC (9,914 KB)
    [v3] Thu, 30 May 2024 22:56:17 UTC (9,638 KB)
    Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, Michael Moor

Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information -- such as which tests to perform -- and to act upon it. Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care. However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work. Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments. In our benchmark, the doctor agent must uncover the patient's diagnosis through dialogue and active data collection. We present two open medical agent benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents. We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents. Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA. We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark. We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents. The code and data for this work is publicly available at this https URL.

------------

----------
Other (73)
----------

`[2405.20519] Diffusion On Syntax Trees For Program Synthesis <https://arxiv.org/abs/2405.20519>`__ 面向程序合成的语法树扩散

::

    Thu, 30 May 2024 22:31:16 GMT
    Shreyas Kapur, Erik Jenner, Stuart Russell

Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program's output.
Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts ``noise'' applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches.

------------

`[2405.20526] Automated Generation and Tagging of Knowledge Components from Multiple-Choice Questions <https://arxiv.org/abs/2405.20526>`__ 多项选择题中知识成分的自动生成与标注

::

    Thu, 30 May 2024 22:57:49 GMT
    Steven Moore, Robin Schmucker, Tom Mitchell, John Stamper

Knowledge Components (KCs) linked to assessments enhance the measurement of student learning, enrich analytics, and facilitate adaptivity. However, generating and linking KCs to assessment items requires significant effort and domain-specific knowledge. To streamline this process for higher-education courses, we employed GPT-4 to generate KCs for multiple-choice questions (MCQs) in Chemistry and E-Learning. We analyzed discrepancies between the KCs generated by the Large Language Model (LLM) and those made by humans through evaluation from three domain experts in each subject area. This evaluation aimed to determine whether, in instances of non-matching KCs, evaluators showed a preference for the LLM-generated KCs over their human-created counterparts.
We also developed an ontology induction algorithm to cluster questions that assess similar KCs based on their content. Our most effective LLM strategy accurately matched KCs for 56% of Chemistry and 35% of E-Learning MCQs, with even higher success when considering the top five KC suggestions. Human evaluators favored LLM-generated KCs, choosing them over human-assigned ones approximately two-thirds of the time, a preference that was statistically significant across both domains. Our clustering algorithm successfully grouped questions by their underlying KCs without needing explicit labels or contextual information. This research advances the automation of KC generation and classification for assessment items, alleviating the need for student data or predefined KC labels.

------------

`[2405.20625] Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning <https://arxiv.org/abs/2405.20625>`__ LLM-Modulo框架的鲁棒规划:旅行规划案例研究

::

    Fri, 31 May 2024 05:23:35 GMT
    Atharva Gundawar, Mudit Verma, Lin Guan, Karthik Valmeekam, Siddhant Bhambri, Subbarao Kambhampati

As the applicability of Large Language Models (LLMs) extends beyond traditional text processing tasks, there is a burgeoning interest in their potential to excel in planning and reasoning assignments, realms traditionally reserved for System 2 cognitive competencies. Despite their perceived versatility, the research community is still unraveling effective strategies to harness these models in such complex domains. The recent discourse introduced by the paper on LLM Modulo marks a significant stride, proposing a conceptual framework that enhances the integration of LLMs into diverse planning and reasoning activities. This workshop paper delves into the practical application of this framework within the domain of travel planning, presenting a specific instance of its implementation. We are using the Travel Planning benchmark by the OSU NLP group, a benchmark for evaluating the performance of LLMs in producing valid itineraries based on user queries presented in natural language. While popular methods of enhancing the reasoning abilities of LLMs such as Chain of Thought, ReAct, and Reflexion achieve a meager 0%, 0.6%, and 0% with GPT3.5-Turbo respectively, our operationalization of the LLM-Modulo framework for TravelPlanning domain provides a remarkable improvement, enhancing baseline performances by 4.6x for GPT4-Turbo and even more for older models like GPT3.5-Turbo from 0% to 5%. Furthermore, we highlight the other useful roles of LLMs in the planning pipeline, as suggested in LLM-Modulo, which can be reliably operationalized such as extraction of useful critics and reformulator for critics.

------------

`[2405.20628] ToxVidLLM: A Multimodal LLM-based Framework for Toxicity Detection in Code-Mixed Videos <https://arxiv.org/abs/2405.20628>`__ ToxVidLLM:基于多模态llm的混合视频毒性检测框架

::

    Fri, 31 May 2024 05:40:56 GMT
    Krishanu Maity, A.S. Poornash, Sriparna Saha and Pushpak Bhattacharyya

In an era of rapidly evolving internet technology, the surge in multimodal content, including videos, has expanded the horizons of online communication.
However, the detection of toxic content in this diverse landscape, particularly in low-resource code-mixed languages, remains a critical challenge. While substantial research has addressed toxic content detection in textual data, the realm of video content, especially in non-English languages, has been relatively underexplored. This paper addresses this research gap by introducing a benchmark dataset, the first of its kind, consisting of 931 videos with 4021 code-mixed Hindi-English utterances collected from YouTube. Each utterance within this dataset has been meticulously annotated for toxicity, severity, and sentiment labels. We have developed an advanced Multimodal Multitask framework built for Toxicity detection in Video Content by leveraging Large Language Models (LLMs), crafted for the primary objective along with the additional tasks of conducting sentiment and severity analysis. ToxVidLLM incorporates three key modules the Encoder module, Cross-Modal Synchronization module, and Multitask module crafting a generic multimodal LLM customized for intricate video classification tasks. Our experiments reveal that incorporating multiple modalities from the videos substantially enhances the performance of toxic content detection by achieving an Accuracy and Weighted F1 score of 94.29% and 94.35%, respectively.

------------

`[2405.20653] Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens <https://arxiv.org/abs/2405.20653>`__ 利用静默标记增强针对大型语言模型的越狱攻击

::

    Fri, 31 May 2024 07:41:03 GMT
    Jiahao Yu, Haozheng Luo, Jerry Yao-Chieh, Wenbo Guo, Han Liu, Xinyu Xing

Along with the remarkable successes of Language language models, recent research also started to explore the security threats of LLMs, including jailbreaking attacks. Attackers carefully craft jailbreaking prompts such that a target LLM will respond to the harmful question. Existing jailbreaking attacks require either human experts or leveraging complicated algorithms to craft jailbreaking prompts. In this paper, we introduce BOOST, a simple attack that leverages only the eos tokens. We demonstrate that rather than constructing complicated jailbreaking prompts, the attacker can simply append a few eos tokens to the end of a harmful question. It will bypass the safety alignment of LLMs and lead to successful jailbreaking attacks. We further apply BOOST to four representative jailbreak methods and show that the attack success rates of these methods can be significantly enhanced by simply adding eos tokens to the prompt. To understand this simple but novel phenomenon, we conduct empirical analyses. Our analysis reveals that adding eos tokens makes the target LLM believe the input is much less harmful, and eos tokens have low attention values and do not affect LLM's understanding of the harmful questions, leading the model to actually respond to the questions. Our findings uncover how fragile an LLM is against jailbreak attacks, motivating the development of strong safety alignment approaches.

------------

`[2405.21030] Standards for Belief Representations in LLMs <https://arxiv.org/abs/2405.21030>`__ llm中的信念表示标准

::

    Fri, 31 May 2024 17:21:52 GMT
    Daniel A. Herrmann and Benjamin A. Levinstein

As large language models (LLMs) continue to demonstrate remarkable abilities across various domains, computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and if) LLMs internally represent their beliefs about the world. However, this field currently lacks a unified theoretical foundation to underpin the study of belief in LLMs. This article begins filling this gap by proposing adequacy conditions for a representation in an LLM to count as belief-like. We argue that, while the project of belief measurement in LLMs shares striking features with belief measurement as carried out in decision theory and formal epistemology, it also differs in ways that should change how we measure belief.
Thus, drawing from insights in philosophy and contemporary practices of machine learning, we establish four criteria that balance theoretical considerations with practical constraints. Our proposed criteria include accuracy, coherence, uniformity, and use, which together help lay the groundwork for a comprehensive understanding of belief representation in LLMs. We draw on empirical work showing the limitations of using various criteria in isolation to identify belief representations.

------------

`[2405.20404] XPrompt:Explaining Large Language Model's Generation via Joint Prompt Attribution <https://arxiv.org/abs/2405.20404>`__ XPrompt:基于联合提示归因的大型语言模型生成解释

::

    Thu, 30 May 2024 18:16:41 GMT
    Yurui Chang, Bochuan Cao, Yujia Wang, Jinghui Chen, Lu Lin

Large Language Models (LLMs) have demonstrated impressive performances in complex text generation tasks. However, the contribution of the input prompt to the generated content still remains obscure to humans, underscoring the necessity of elucidating and explaining the causality between input and output pairs. Existing works for providing prompt-specific explanation often confine model output to be classification or next-word prediction. Few initial attempts aiming to explain the entire language generation often treat input prompt texts independently, ignoring their combinatorial effects on the follow-up generation. In this study, we introduce a counterfactual explanation framework based on joint prompt attribution, XPrompt, which aims to explain how a few prompt texts collaboratively influences the LLM's complete generation.
Particularly, we formulate the task of prompt attribution for generation interpretation as a combinatorial optimization problem, and introduce a probabilistic algorithm to search for the casual input combination in the discrete space. We define and utilize multiple metrics to evaluate the produced explanations, demonstrating both faithfulness and efficiency of our framework.

------------

`[2405.20477] Automated Focused Feedback Generation for Scientific Writing Assistance <https://arxiv.org/abs/2405.20477>`__ 科学写作辅助的自动聚焦反馈生成

::

    Thu, 30 May 2024 20:56:41 GMT
    Eric Chamoun, Michael Schlichktrull, Andreas Vlachos

Scientific writing is a challenging task, particularly for novice researchers who often rely on feedback from experienced peers. Recent work has primarily focused on improving surface form and style rather than manuscript content. In this paper, we propose a novel task: automated focused feedback generation for scientific writing assistance. We present SWIF$^{2}$T: a Scientific WrIting Focused Feedback Tool. It is designed to generate specific, actionable and coherent comments, which identify weaknesses in a scientific paper and/or propose revisions to it. Our approach consists of four components - planner, investigator, reviewer and controller - leveraging multiple Large Language Models (LLMs) to implement them. We compile a dataset of 300 peer reviews citing weaknesses in scientific papers and conduct human evaluation. The results demonstrate the superiority in specificity, reading comprehension, and overall helpfulness of SWIF$^{2}$T's feedback compared to other approaches. In our analysis, we also identified cases where automatically generated reviews were judged better than human ones, suggesting opportunities for integration of AI-generated feedback in scientific writing.

------------

`[2405.20505] SPOT: Text Source Prediction from Originality Score Thresholding <https://arxiv.org/abs/2405.20505>`__ SPOT:基于原创分数阈值的文本来源预测

::

    Thu, 30 May 2024 21:51:01 GMT
    Edouard Yvinec, Gabriel Kasser

The wide acceptance of large language models (LLMs) has unlocked new applications and social risks. Popular countermeasures aim at detecting misinformation, usually involve domain specific models trained to recognize the relevance of any information. Instead of evaluating the validity of the information, we propose to investigate LLM generated text from the perspective of trust. In this study, we define trust as the ability to know if an input text was generated by a LLM or a human. To do so, we design SPOT, an efficient method, that classifies the source of any, standalone, text input based on originality score. This score is derived from the prediction of a given LLM to detect other LLMs. We empirically demonstrate the robustness of the method to the architecture, training data, evaluation data, task and compression of modern LLMs.

------------

`[2405.20512] How Multilingual Are Large Language Models Fine-Tuned for Translation? <https://arxiv.org/abs/2405.20512>`__ 如何对多语言大型语言模型进行翻译微调?

::

    Thu, 30 May 2024 22:08:20 GMT
    Aquia Richburg and Marine Carpuat

A new paradigm for machine translation has recently emerged: fine-tuning large language models (LLM) on parallel text has been shown to outperform dedicated translation systems trained in a supervised fashion on much larger amounts of parallel data (Xu et al., 2024a; Alves et al., 2024). However, it remains unclear whether this paradigm can enable massively multilingual machine translation or whether it requires fine-tuning dedicated models for a small number of language pairs. How does translation fine-tuning impact the MT capabilities of LLMs for zero-shot languages, zero-shot language pairs, and translation tasks that do not involve English? To address these questions, we conduct an extensive empirical evaluation of the translation quality of the TOWER family of language models (Alves et al., 2024) on 132 translation tasks from the multi-parallel FLORES-200 data. We find that translation fine-tuning improves translation quality even for zero-shot languages on average, but that the impact is uneven depending on the language pairs involved. These results call for further research to effectively enable massively multilingual translation with LLMs.

------------

`[2405.20527] Towards Ontology-Enhanced Representation Learning for Large Language Models <https://arxiv.org/abs/2405.20527>`__ 本体增强的大型语言模型表示学习研究

::

    Thu, 30 May 2024 23:01:10 GMT
    Francesco Ronzano and Jay Nanavati

Taking advantage of the widespread use of ontologies to organise and harmonize knowledge across several distinct domains, this paper proposes a novel approach to improve an embedding-Large Language Model (embedding-LLM) of interest by infusing the knowledge formalized by a reference ontology: ontological knowledge infusion aims at boosting the ability of the considered LLM to effectively model the knowledge domain described by the infused ontology. The linguistic information (i.e. concept synonyms and descriptions) and structural information (i.e. is-a relations) formalized by the ontology are utilized to compile a comprehensive set of concept definitions, with the assistance of a powerful generative LLM (i.e. GPT-3.5-turbo). These concept definitions are then employed to fine-tune the target embedding-LLM using a contrastive learning framework. To demonstrate and evaluate the proposed approach, we utilize the biomedical disease ontology MONDO. The results show that embedding-LLMs enhanced by ontological disease knowledge exhibit an improved capability to effectively evaluate the similarity of in-domain sentences from biomedical documents mentioning diseases, without compromising their out-of-domain performance.

------------

`[2405.20582] The Point of View of a Sentiment: Towards Clinician Bias Detection in Psychiatric Notes <https://arxiv.org/abs/2405.20582>`__ 从情感的角度看:精神病学笔记中对临床医生偏见的检测

::

    Fri, 31 May 2024 02:28:41 GMT
    Alissa A. Valentine, Lauren A. Lepow, Alexander W. Charney, and Isotta Landi

In psychiatry, negative patient descriptions and stigmatizing language can contribute to healthcare disparities in two ways: (1) read by patients they can harm their trust and engagement with the medical center; (2) read by future providers they may negatively influence the future perspective of a patient. By leveraging large language models, this work aims to identify the sentiment expressed in psychiatric clinical notes based on the reader's point of view.
Extracting sentences from the Mount Sinai Health System's large and diverse clinical notes, we used prompts and in-context learning to adapt three large language models (GPT-3.5, Llama 2, Mistral) to classify the sentiment conveyed by the sentences according to the provider or non-provider point of view.
Results showed that GPT-3.5 aligns best to provider point of view, whereas Mistral aligns best to non-provider point of view.

------------

`[2405.20585] GAMedX: Generative AI-based Medical Entity Data Extractor Using Large Language Models <https://arxiv.org/abs/2405.20585>`__ GAMedX:基于生成式人工智能的大型语言模型医疗实体数据提取器

::

    Fri, 31 May 2024 02:53:22 GMT
    Mohammed-Khalil Ghali, Abdelrahman Farrag, Hajar Sakai, Hicham El Baz, Yu Jin, Sarah Lam

In the rapidly evolving field of healthcare and beyond, the integration of generative AI in Electronic Health Records (EHRs) represents a pivotal advancement, addressing a critical gap in current information extraction techniques. This paper introduces GAMedX, a Named Entity Recognition (NER) approach utilizing Large Language Models (LLMs) to efficiently extract entities from medical narratives and unstructured text generated throughout various phases of the patient hospital visit. By addressing the significant challenge of processing unstructured medical text, GAMedX leverages the capabilities of generative AI and LLMs for improved data extraction. Employing a unified approach, the methodology integrates open-source LLMs for NER, utilizing chained prompts and Pydantic schemas for structured output to navigate the complexities of specialized medical jargon. The findings reveal significant ROUGE F1 score on one of the evaluation datasets with an accuracy of 98\%. This innovation enhances entity extraction, offering a scalable, cost-effective solution for automated forms filling from unstructured data. As a result, GAMedX streamlines the processing of unstructured narratives, and sets a new standard in NER applications, contributing significantly to theoretical and practical advancements beyond the medical technology sphere.

------------

`[2405.20588] DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models <https://arxiv.org/abs/2405.20588>`__ DAFNet:大型语言模型中序列模型编辑的动态辅助融合

::

    Fri, 31 May 2024 02:56:49 GMT
    Taolin Zhang, Qizhou Chen, Dongyang Li, Chengyu Wang, Xiaofeng He, Longtao Huang, Hui Xue, Jun Huang

Recently, while large language models (LLMs) have demonstrated impressive results, they still suffer from hallucination, i.e., the generation of false information. Model editing is the task of fixing factual mistakes in LLMs; yet, most previous works treat it as a one-time task, paying little attention to ever-emerging mistakes generated by LLMs. We address the task of sequential model editing (SME) that aims to rectify mistakes continuously. A Dynamic Auxiliary Fusion Network (DAFNet) is designed to enhance the semantic interaction among the factual knowledge within the entire sequence, preventing catastrophic forgetting during the editing process of multiple knowledge triples. Specifically, (1) for semantic fusion within a relation triple, we aggregate the intra-editing attention flow into auto-regressive self-attention with token-level granularity in LLMs. We further leverage multi-layer diagonal inter-editing attention flow to update the weighted representations of the entire sequence-level granularity. (2) Considering that auxiliary parameters are required to store the knowledge for sequential editing, we construct a new dataset named \textbf{DAFSet}, fulfilling recent, popular, long-tail and robust properties to enhance the generality of sequential editing. Experiments show DAFNet significantly outperforms strong baselines in single-turn and sequential editing. The usage of DAFSet also consistently improves the performance of other auxiliary network-based methods in various scenarios

------------

`[2405.20612] UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation <https://arxiv.org/abs/2405.20612>`__ UniBias:通过内部注意力和FFN操纵揭开和减轻LLM偏见

::

    Fri, 31 May 2024 03:59:15 GMT
    Hanzhang Zhou, Zijian Feng, Zixiao Zhu, Junlang Qian, Kezhi Mao

Large language models (LLMs) have demonstrated impressive capabilities in various tasks using the in-context learning (ICL) paradigm. However, their effectiveness is often compromised by inherent bias, leading to prompt brittleness, i.e., sensitivity to design settings such as example selection, order, and prompt formatting. Previous studies have addressed LLM bias through external adjustment of model outputs, but the internal mechanisms that lead to such bias remain unexplored. Our work delves into these mechanisms, particularly investigating how feedforward neural networks (FFNs) and attention heads result in the bias of LLMs. By Interpreting the contribution of individual FFN vectors and attention heads, we identify the biased LLM components that skew LLMs' prediction toward specific labels. To mitigate these biases, we introduce UniBias, an inference-only method that effectively identifies and eliminates biased FFN vectors and attention heads. Extensive experiments across 12 NLP datasets demonstrate that UniBias significantly enhances ICL performance and alleviates prompt brittleness of LLMs.

------------

`[2405.20613] FineRadScore: A Radiology Report Line-by-Line Evaluation Technique Generating Corrections with Severity Scores <https://arxiv.org/abs/2405.20613>`__ FineRadScore:一种放射科报告逐行评估技术，根据严重程度评分生成修正

::

    Fri, 31 May 2024 04:05:09 GMT
    Alyssa Huang, Oishi Banerjee, Kay Wu, Eduardo Pontes Reis, Pranav Rajpurkar

The current gold standard for evaluating generated chest x-ray (CXR) reports is through radiologist annotations. However, this process can be extremely time-consuming and costly, especially when evaluating large numbers of reports.
In this work, we present FineRadScore, a Large Language Model (LLM)-based automated evaluation metric for generated CXR reports. Given a candidate report and a ground-truth report, FineRadScore gives the minimum number of line-by-line corrections required to go from the candidate to the ground-truth report. Additionally, FineRadScore provides an error severity rating with each correction and generates comments explaining why the correction was needed. We demonstrate that FineRadScore's corrections and error severity scores align with radiologist opinions. We also show that, when used to judge the quality of the report as a whole, FineRadScore aligns with radiologists as well as current state-of-the-art automated CXR evaluation metrics. Finally, we analyze FineRadScore's shortcomings to provide suggestions for future improvements.

------------

`[2405.20654] Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models <https://arxiv.org/abs/2405.20654>`__ 基于大型语言模型问答中篇章重排序的篇章特定提示调优

::

    Fri, 31 May 2024 07:43:42 GMT
    Xuyang Wu, Zhiyuan Peng, Sravanthi Rajanala, Hsin-Tai Wu, Yi Fang

Effective passage retrieval and reranking methods have been widely utilized to identify suitable candidates in open-domain question answering tasks, recent studies have resorted to LLMs for reranking the retrieved passages by the log-likelihood of the question conditioned on each passage. Although these methods have demonstrated promising results, the performance is notably sensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs can be computationally intensive and time-consuming. Furthermore, this approach limits the leverage of question-passage relevance pairs and passage-specific knowledge to enhance the ranking capabilities of LLMs. In this paper, we propose passage-specific prompt tuning for reranking in open-domain question answering (PSPT): a parameter-efficient method that fine-tunes learnable passage-specific soft prompts, incorporating passage-specific knowledge from a limited set of question-passage relevance pairs. The method involves ranking retrieved passages based on the log-likelihood of the model generating the question conditioned on each passage and the learned soft prompt. We conducted extensive experiments utilizing the Llama-2-chat-7B model across three publicly available open-domain question answering datasets and the results demonstrate the effectiveness of the proposed approach.

------------

`[2405.20657] DORY: Deliberative Prompt Recovery for LLM <https://arxiv.org/abs/2405.20657>`__ 多莉:对LLM的审慎迅速恢复

::

    Fri, 31 May 2024 07:51:16 GMT
    Lirong Gao, Ru Peng, Yiming Zhang, Junbo Zhao

Prompt recovery in large language models (LLMs) is crucial for understanding how LLMs work and addressing concerns regarding privacy, copyright, etc. The trend towards inference-only APIs complicates this task by restricting access to essential outputs for recovery. To tackle this challenge, we extract prompt-related information from limited outputs and identify a strong(negative) correlation between output probability-based uncertainty and the success of prompt recovery. This finding led to the development of Deliberative PrOmpt RecoverY (DORY), our novel approach that leverages uncertainty to recover prompts accurately. DORY involves reconstructing drafts from outputs, refining these with hints, and filtering out noise based on uncertainty. Our evaluation across diverse LLMs and prompt benchmarks shows that DORY outperforms existing baselines, improving performance by approximately 10.82% and establishing a new state-of-the-art record in prompt recovery tasks. Significantly, DORY operates using a single LLM without any external resources or model, offering a cost-effective, user-friendly prompt recovery solution.

------------

`[2405.20701] Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement <https://arxiv.org/abs/2405.20701>`__ 揭示LLMs的词法敏感性:快速增强的组合优化

::

    Fri, 31 May 2024 08:53:59 GMT
    Pengwei Zhan, Zhen Xu, Qian Tan, Jie Song, Ru Xie

Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE).
COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence.
Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.

------------

`[2405.20787] PGA-SciRE: Harnessing LLM on Data Augmentation for Enhancing Scientific Relation Extraction <https://arxiv.org/abs/2405.20787>`__ PGA-SciRE:利用LLM增强数据增强科学关系抽取

::

    Thu, 30 May 2024 13:07:54 GMT
    Yang Zhou, Shimin Shan, Hongkui Wei, Zhehuan Zhao, Wenshuo Feng

Relation Extraction (RE) aims at recognizing the relation between pairs of entities mentioned in a text. Advances in LLMs have had a tremendous impact on NLP. In this work, we propose a textual data augmentation framework called PGA for improving the performance of models for RE in the scientific domain. The framework introduces two ways of data augmentation, utilizing a LLM to obtain pseudo-samples with the same sentence meaning but with different representations and forms by paraphrasing the original training set samples. As well as instructing LLM to generate sentences that implicitly contain information about the corresponding labels based on the relation and entity of the original training set samples. These two kinds of pseudo-samples participate in the training of the RE model together with the original dataset, respectively. The PGA framework in the experiment improves the F1 scores of the three mainstream models for RE within the scientific domain. Also, using a LLM to obtain samples can effectively reduce the cost of manually labeling data.

------------

`[2405.20805] Multilingual Text Style Transfer: Datasets & Models for Indian Languages <https://arxiv.org/abs/2405.20805>`__ 多语言文本风格迁移:印度语言的数据集和模型

::

    Fri, 31 May 2024 14:05:27 GMT
    Sourabrata Mukherjee, Atul Kr. Ojha, Akanksha Bansal, Deepak Alok, John P. McCrae, Ond\v{r}ej Du\v{s}ek

Text style transfer (TST) involves altering the linguistic style of a text while preserving its core content. This paper focuses on sentiment transfer, a vital TST subtask (Mukherjee et al., 2022a), across a spectrum of Indian languages: Hindi, Magahi, Malayalam, Marathi, Punjabi, Odia, Telugu, and Urdu, expanding upon previous work on English-Bangla sentiment transfer (Mukherjee et al., 2023). We introduce dedicated datasets of 1,000 positive and 1,000 negative style-parallel sentences for each of these eight languages. We then evaluate the performance of various benchmark models categorized into parallel, non-parallel, cross-lingual, and shared learning approaches, including the Llama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the significance of parallel data in TST and demonstrate the effectiveness of the Masked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel techniques. Moreover, cross-lingual and joint multilingual learning methods show promise, offering insights into selecting optimal models tailored to the specific language and task requirements. To the best of our knowledge, this work represents the first comprehensive exploration of the TST task as sentiment transfer across a diverse set of languages.

------------

`[2405.20830] Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment <https://arxiv.org/abs/2405.20830>`__ 自增强偏好优化:语言模型对齐的非策略范式

::

    Fri, 31 May 2024 14:21:04 GMT
    Yueqin Yin and Zhendong Wang and Yujia Xie and Weizhu Chen and Mingyuan Zhou

Traditional language model alignment methods, such as Direct Preference Optimization (DPO), are limited by their dependence on static, pre-collected paired preference data, which hampers their adaptability and practical applicability. To overcome this limitation, we introduce Self-Augmented Preference Optimization (SAPO), an effective and scalable training paradigm that does not require existing paired data. Building on the self-play concept, which autonomously generates negative responses, we further incorporate an off-policy learning pipeline to enhance data exploration and exploitation.
Specifically, we employ an Exponential Moving Average (EMA) model in conjunction with a replay buffer to enable dynamic updates of response segments, effectively integrating real-time feedback with insights from historical data. Our comprehensive evaluations of the LLaMA3-8B and Mistral-7B models across benchmarks, including the Open LLM Leaderboard, IFEval, AlpacaEval 2.0, and MT-Bench, demonstrate that SAPO matches or surpasses established offline contrastive baselines, such as DPO and Odds Ratio Preference Optimization, and outperforms offline self-play methods like SPIN.
Our code is available at https://github.com/yinyueqin/SAPO

------------

`[2405.20833] That's Optional: A Contemporary Exploration of "that" Omission in English Subordinate Clauses <https://arxiv.org/abs/2405.20833>`__ 这是可选的:对英语从句中省略“That”的当代探索

::

    Fri, 31 May 2024 14:23:30 GMT
    Ella Rabinovich

The Uniform Information Density (UID) hypothesis posits that speakers optimize the communicative properties of their utterances by avoiding spikes in information, thereby maintaining a relatively uniform information profile over time. This paper investigates the impact of UID principles on syntactic reduction, specifically focusing on the optional omission of the connector "that" in English subordinate clauses. Building upon previous research, we extend our investigation to a larger corpus of written English, utilize contemporary large language models (LLMs) and extend the information-uniformity principles by the notion of entropy, to estimate the UID manifestations in the usecase of syntactic reduction choices.

------------

`[2405.20850] Improving Reward Models with Synthetic Critiques <https://arxiv.org/abs/2405.20850>`__ 用综合评论改进奖励模型

::

    Fri, 31 May 2024 14:33:07 GMT
    Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, Matthias Gall\'e

Reward models (RM) play a critical role in aligning language models through the process of reinforcement learning from human feedback. RMs are trained to predict a score reflecting human preference, which requires significant time and cost for human annotation. Additionally, RMs tend to quickly overfit on superficial features in the training set, hindering their generalization performance on unseen distributions. We propose a novel approach using synthetic natural language critiques generated by large language models to provide additional feedback, evaluating aspects such as instruction following, correctness, and style. This offers richer signals and more robust features for RMs to assess and score on. We demonstrate that high-quality critiques improve the performance and data efficiency of RMs initialized from different pretrained models. Conversely, we also show that low-quality critiques negatively impact performance. Furthermore, incorporating critiques enhances the interpretability and robustness of RM training.

------------

`[2405.20900] Large Language Models: A New Approach for Privacy Policy Analysis at Scale <https://arxiv.org/abs/2405.20900>`__ 大型语言模型:一种大规模隐私策略分析的新方法

::

    Fri, 31 May 2024 15:12:33 GMT
    David Rodriguez, Ian Yang, Jose M. Del Alamo, Norman Sadeh

The number and dynamic nature of web and mobile applications presents significant challenges for assessing their compliance with data protection laws. In this context, symbolic and statistical Natural Language Processing (NLP) techniques have been employed for the automated analysis of these systems' privacy policies. However, these techniques typically require labor-intensive and potentially error-prone manually annotated datasets for training and validation. This research proposes the application of Large Language Models (LLMs) as an alternative for effectively and efficiently extracting privacy practices from privacy policies at scale. Particularly, we leverage well-known LLMs such as ChatGPT and Llama 2, and offer guidance on the optimal design of prompts, parameters, and models, incorporating advanced strategies such as few-shot learning. We further illustrate its capability to detect detailed and varied privacy practices accurately. Using several renowned datasets in the domain as a benchmark, our evaluation validates its exceptional performance, achieving an F1 score exceeding 93%. Besides, it does so with reduced costs, faster processing times, and fewer technical knowledge requirements. Consequently, we advocate for LLM-based solutions as a sound alternative to traditional NLP techniques for the automated analysis of privacy policies at scale.

------------

`[2405.20974] SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales <https://arxiv.org/abs/2405.20974>`__ 自我:教LLMs用自我反思的理由表达自信

::

    Fri, 31 May 2024 16:21:16 GMT
    Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao

Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at \url{https://github.com/xu1868/SaySelf}.

------------

`[2405.21028] LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models <https://arxiv.org/abs/2405.21028>`__ LACIE:基于听者感知的大型语言模型置信度校准微调

::

    Fri, 31 May 2024 17:16:38 GMT
    Elias Stengel-Eskin, Peter Hase, Mohit Bansal

When answering questions, LLMs can convey not only an answer, but a level of confidence about the answer being correct. This includes explicit confidence markers (e.g. giving a numeric score) as well as implicit markers, like an authoritative tone or elaborating with additional knowledge. For LLMs to be trustworthy knowledge sources, the confidence they convey should match their actual expertise; however, most current models tend towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. We cast calibration as preference optimization, creating data via a two-agent game, where a speaker model's outputs are judged by a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the resulting models are better calibrated w.r.t. a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM's answers, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers.
Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better confidence separation between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more and implicitly signals certainty when it is correct by using an authoritative tone or including details. Finally, LACIE finetuning leads to an emergent increase in model abstention (e.g. saying "I don't know") for answers that are likely wrong.

------------

`[2405.21040] Direct Alignment of Language Models via Quality-Aware Self-Refinement <https://arxiv.org/abs/2405.21040>`__ 基于质量感知自我细化的语言模型直接对齐

::

    Fri, 31 May 2024 17:31:18 GMT
    Runsheng Yu, Yong Wang, Xiaoqi Jiao, Youzhi Zhang, James T. Kwok

Reinforcement Learning from Human Feedback (RLHF) has been commonly used to align the behaviors of Large Language Models (LLMs) with human preferences.
Recently, a popular alternative is Direct Policy Optimization (DPO), which replaces an LLM-based reward model with the policy itself, thus obviating the need for extra memory and training time to learn the reward model. However, DPO does not consider the relative qualities of the positive and negative responses, and can lead to sub-optimal training outcomes. To alleviate this problem, we investigate the use of intrinsic knowledge within the on-the-fly fine-tuning LLM to obtain relative qualities and help to refine the loss function. Specifically, we leverage the knowledge of the LLM to design a refinement function to estimate the quality of both the positive and negative responses. We show that the constructed refinement function can help self-refine the loss function under mild assumptions. The refinement function is integrated into DPO and its variant Identity Policy Optimization (IPO).
Experiments across various evaluators indicate that they can improve the performance of the fine-tuned models over DPO and IPO.

------------

`[2405.20835] Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs <https://arxiv.org/abs/2405.20835>`__ 离群点和校准集对现代llm的量化影响越来越小

::

    Fri, 31 May 2024 14:24:33 GMT
    Davide Paglieri, Saurabh Dash, Tim Rockt\"aschel, Jack Parker-Holder

Post-Training Quantization (PTQ) enhances the efficiency of Large Language Models (LLMs) by enabling faster operation and compatibility with more accessible hardware through reduced memory usage, at the cost of small performance drops. We explore the role of calibration sets in PTQ, specifically their effect on hidden activations in various notable open-source LLMs.
Calibration sets are crucial for evaluating activation magnitudes and identifying outliers, which can distort the quantization range and negatively impact performance. Our analysis reveals a marked contrast in quantization effectiveness across models. The older OPT model, which much of the quantization literature is based on, shows significant performance deterioration and high susceptibility to outliers with varying calibration sets. In contrast, newer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and Mistral 7B demonstrate strong robustness, with Mistral 7B showing near-immunity to outliers and stable activations. These findings suggest a shift in PTQ strategies might be needed. As advancements in pre-training methods reduce the relevance of outliers, there is an emerging need to reassess the fundamentals of current quantization literature. The emphasis should pivot towards optimizing inference speed, rather than primarily focusing on outlier preservation, to align with the evolving characteristics of state-of-the-art LLMs.

------------

`[2405.20971] Amortizing intractable inference in diffusion models for vision, language, and control <https://arxiv.org/abs/2405.20971>`__ 视觉、语言和控制扩散模型中的难解推理

::

    Fri, 31 May 2024 16:18:46 GMT
    Siddarth Venkatraman, Moksh Jain, Luca Scimeca, Minsu Kim, Marcin Sendera, Mohsin Hasan, Luke Rowe, Sarthak Mittal, Pablo Lemos, Emmanuel Bengio, Alexandre Adam, Jarrid Rector-Brooks, Yoshua Bengio, Glen Berseth, Nikolay Malkin

Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem. This paper studies amortized sampling of the posterior over data, $\mathbf{x}\sim p^{\rm post}(\mathbf{x})\propto p(\mathbf{x})r(\mathbf{x})$, in a model that consists of a diffusion generative model prior $p(\mathbf{x})$ and a black-box constraint or likelihood function $r(\mathbf{x})$. We state and prove the asymptotic correctness of a data-free learning objective, relative trajectory balance, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases.
Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage. Experiments illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors: in vision (classifier guidance), language (infilling under a discrete diffusion LLM), and multimodal data (text-to-image generation). Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning.

------------

`[2405.20973] LCQ: Low-Rank Codebook based Quantization for Large Language Models <https://arxiv.org/abs/2405.20973>`__ LCQ:基于低秩码本的大型语言模型量化

::

    Fri, 31 May 2024 16:21:05 GMT
    Wen-Pu Cai, Wu-Jun Li

Large language models~(LLMs) have recently demonstrated promising performance in many tasks. However, the high storage and computational cost of LLMs has become a challenge for deploying LLMs. Weight quantization has been widely used for model compression, which can reduce both storage and computational cost.
Most existing weight quantization methods for LLMs use a rank-one codebook for quantization, which results in substantial accuracy loss when the compression ratio is high. In this paper, we propose a novel weight quantization method, called low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a low-rank codebook, the rank of which can be larger than one, for quantization.
Experiments show that LCQ can achieve better accuracy than existing methods with a negligibly extra storage cost.

------------

`[2405.21018] Improved Techniques for Optimization-Based Jailbreaking on Large Language Models <https://arxiv.org/abs/2405.21018>`__ 改进的大型语言模型优化越狱技术

::

    Fri, 31 May 2024 17:07:15 GMT
    Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, Min Lin

Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment. Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack's success has led to a growing interest in the study of optimization-based jailbreaking techniques. Although GCG is a significant milestone, its attacking efficiency remains unsatisfactory. In this paper, we present several improved (empirical) techniques for optimization-based jailbreaks like GCG. We first observe that the single target template of "Sure" largely limits the attacking performance of GCG; given this, we propose to apply diverse target templates containing harmful self-suggestion and/or guidance to mislead LLMs. Besides, from the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG (i.e., adaptively deciding how many tokens to replace in each step) to accelerate convergence, as well as tricks like easy-to-hard initialisation. Then, we combine these improved technologies to develop an efficient jailbreak method, dubbed $\mathcal{I}$-GCG. In our experiments, we evaluate on a series of benchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved techniques can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly 100% attack success rate. The code is released at https://github.com/jiaxiaojunQAQ/I-GCG.

------------

`[2405.20389] Designing an Evaluation Framework for Large Language Models in Astronomy Research <https://arxiv.org/abs/2405.20389>`__ 天文学研究中大型语言模型评估框架的设计

::

    Thu, 30 May 2024 18:00:21 GMT
    John F. Wu, Alina Hyk, Kiera McCormick, Christine Ye, Simone Astarita, Elina Baral, Jo Ciuca, Jesse Cranney, Anjalie Field, Kartheik Iyer, Philipp Koehn, Jenn Kotler, Sandor Kruk, Michelle Ntampaka, Charles O'Neill, Joshua E.G. Peek, Sanjib Sharma, Mikaeel Yunus

Large Language Models (LLMs) are shifting how scientific research is done. It is imperative to understand how researchers interact with these models and how scientific sub-communities like astronomy might benefit from them. However, there is currently no standard for evaluating the use of LLMs in astronomy.
Therefore, we present the experimental design for an evaluation study on how astronomy researchers interact with LLMs. We deploy a Slack chatbot that can answer queries from users via Retrieval-Augmented Generation (RAG); these responses are grounded in astronomy papers from arXiv. We record and anonymize user questions and chatbot answers, user upvotes and downvotes to LLM responses, user feedback to the LLM, and retrieved documents and similarity scores with the query. Our data collection method will enable future dynamic evaluations of LLM tools for astronomy.

------------

`[2405.20434] Facilitating Human-LLM Collaboration through Factuality Scores and Source Attributions <https://arxiv.org/abs/2405.20434>`__ 通过事实性得分和来源归因促进人类- llm协作

::

    Thu, 30 May 2024 19:23:14 GMT
    Hyo Jin Do, Rachel Ostrand, Justin D. Weisz, Casey Dugan, Prasanna Sattigeri, Dennis Wei, Keerthiram Murugesan, Werner Geyer

While humans increasingly rely on large language models (LLMs), they are susceptible to generating inaccurate or false information, also known as "hallucinations". Technical advancements have been made in algorithms that detect hallucinated content by assessing the factuality of the model's responses and attributing sections of those responses to specific source documents. However, there is limited research on how to effectively communicate this information to users in ways that will help them appropriately calibrate their trust toward LLMs. To address this issue, we conducted a scenario-based study (N=104) to systematically compare the impact of various design strategies for communicating factuality and source attribution on participants' ratings of trust, preferences, and ease in validating response accuracy. Our findings reveal that participants preferred a design in which phrases within a response were color-coded based on the computed factuality scores. Additionally, participants increased their trust ratings when relevant sections of the source material were highlighted or responses were annotated with reference numbers corresponding to those sources, compared to when they received no annotation in the source material. Our study offers practical design guidelines to facilitate human-LLM collaboration and it promotes a new human role to carefully evaluate and take responsibility for their use of LLM outputs.

------------

`[2405.20450] Decentralized AI: Permissionless LLM Inference on POKT Network <https://arxiv.org/abs/2405.20450>`__ 去中心化人工智能:POKT网络上的无许可LLM推理

::

    Thu, 30 May 2024 19:50:07 GMT
    Daniel Olshansky, Ramiro Rodriguez Colmeiro, Bowen Li

POKT Network's decentralized Remote Procedure Call (RPC) infrastructure, surpassing 740 billion requests since launching on MainNet in 2020, is well-positioned to extend into providing AI inference services with minimal design or implementation modifications. This litepaper illustrates how the network's open-source and permissionless design aligns incentives among model researchers, hardware operators, API providers and users whom we term model Sources, Suppliers, Gateways and Applications respectively. Through its Relay Mining algorithm, POKT creates a transparent marketplace where costs and earnings directly reflect cryptographically verified usage. This decentralized framework offers large model AI researchers a new avenue to disseminate their work and generate revenue without the complexities of maintaining infrastructure or building end-user products. Supply scales naturally with demand, as evidenced in recent years and the protocol's free market dynamics.
POKT Gateways facilitate network growth, evolution, adoption, and quality by acting as application-facing load balancers, providing value-added features without managing LLM nodes directly. This vertically decoupled network, battle tested over several years, is set up to accelerate the adoption, operation, innovation and financialization of open-source models. It is the first mature permissionless network whose quality of service competes with centralized entities set up to provide application grade inference.

------------

`[2405.20681] No Free Lunch Theorem for Privacy-Preserving LLM Inference <https://arxiv.org/abs/2405.20681>`__ 隐私保护LLM推理没有免费的午餐定理

::

    Fri, 31 May 2024 08:22:53 GMT
    Xiaojin Zhang, Yulin Fei, Yan Kang, Wei Chen, Lixin Fan, Hai Jin, Qiang Yang

Individuals and businesses have been significantly benefited by Large Language Models (LLMs) including PaLM, Gemini and ChatGPT in various ways. For example, LLMs enhance productivity, reduce costs, and enable us to focus on more valuable tasks. Furthermore, LLMs possess the capacity to sift through extensive datasets, uncover underlying patterns, and furnish critical insights that propel the frontiers of technology and science. However, LLMs also pose privacy concerns. Users' interactions with LLMs may expose their sensitive personal or company information. A lack of robust privacy safeguards and legal frameworks could permit the unwarranted intrusion or improper handling of individual data, thereby risking infringements of privacy and the theft of personal identities. To ensure privacy, it is essential to minimize the dependency between shared prompts and private information. Various randomization approaches have been proposed to protect prompts' privacy, but they may incur utility loss compared to unprotected LLMs prompting. Therefore, it is essential to evaluate the balance between the risk of privacy leakage and loss of utility when conducting effective protection mechanisms. The current study develops a framework for inferring privacy-protected Large Language Models (LLMs) and lays down a solid theoretical basis for examining the interplay between privacy preservation and utility. The core insight is encapsulated within a theorem that is called as the NFL (abbreviation of the word No-Free-Lunch) Theorem.

------------

`[2405.20773] Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Characte <https://arxiv.org/abs/2405.20773>`__ visual - role -play:基于角色扮演图像特征的多模态大规模语言模型通用越狱攻击

::

    Sat, 25 May 2024 17:17:18 GMT
    Siyuan Ma, Weidi Luo, Yu Wang, Xiaogeng Liu, Muhao Chen, Bo Li, Chaowei Xiao

With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), ensuring their safety has become increasingly critical. To achieve this objective, it requires us to proactively discover the vulnerability of MLLMs by exploring the attack methods. Thus, structure-based jailbreak attacks, where harmful semantic content is embedded within images, have been proposed to mislead the models. However, previous structure-based jailbreak methods mainly focus on transforming the format of malicious queries, such as converting harmful content into images through typography, which lacks sufficient jailbreak effectiveness and generalizability. To address these limitations, we first introduce the concept of "Role-play" into MLLM jailbreak attacks and propose a novel and effective method called Visual Role-play (VRP).
Specifically, VRP leverages Large Language Models to generate detailed descriptions of high-risk characters and create corresponding images based on the descriptions. When paired with benign role-play instruction texts, these high-risk character images effectively mislead MLLMs into generating malicious responses by enacting characters with negative attributes. We further extend our VRP method into a universal setup to demonstrate its generalizability.
Extensive experiments on popular benchmarks show that VRP outperforms the strongest baseline, Query relevant and FigStep, by an average Attack Success Rate (ASR) margin of 14.3% across all models.

------------

`[2405.20774] Exploring Backdoor Attacks against Large Language Model-based Decision Making <https://arxiv.org/abs/2405.20774>`__ 探索针对基于大型语言模型决策的后门攻击

::

    Mon, 27 May 2024 17:59:43 GMT
    Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu

Large Language Models (LLMs) have shown significant promise in decision-making tasks when fine-tuned on specific applications, leveraging their inherent common sense and reasoning abilities learned from vast amounts of data. However, these systems are exposed to substantial safety and security risks during the fine-tuning phase. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-enabled Decision-making systems (BALD), systematically exploring how such attacks can be introduced during the fine-tuning phase across various channels.
Specifically, we propose three attack mechanisms and corresponding backdoor optimization methods to attack different components in the LLM-based decision-making pipeline: word injection, scenario manipulation, and knowledge injection. Word injection embeds trigger words directly into the query prompt.
Scenario manipulation occurs in the physical environment, where a high-level backdoor semantic scenario triggers the attack. Knowledge injection conducts backdoor attacks on retrieval augmented generation (RAG)-based LLM systems, strategically injecting word triggers into poisoned knowledge while ensuring the information remains factually accurate for stealthiness. We conduct extensive experiments with three popular LLMs (GPT-3.5, LLaMA2, PaLM2), using two datasets (HighwayEnv, nuScenes), and demonstrate the effectiveness and stealthiness of our backdoor triggers and mechanisms. Finally, we critically assess the strengths and weaknesses of our proposed approaches, highlight the inherent vulnerabilities of LLMs in decision-making tasks, and evaluate potential defenses to safeguard LLM-based decision making systems.

------------

`[2405.20775] Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models <https://arxiv.org/abs/2405.20775>`__ 医学多模态大型语言模型的跨模态越狱与错配攻击

::

    Sun, 26 May 2024 19:11:21 GMT
    Xijie Huang, Xinyuan Wang, Hantao Zhang, Jiawen Xi, Jingkun An, Hao Wang, Chengwei Pan

Security concerns related to Large Language Models (LLMs) have been extensively explored, yet the safety implications for Multimodal Large Language Models (MLLMs), particularly in medical contexts (MedMLLMs), remain insufficiently studied. This paper delves into the underexplored security vulnerabilities of MedMLLMs, especially when deployed in clinical environments where the accuracy and relevance of question-and-answer interactions are critically tested against complex medical challenges. By combining existing clinical medical data with atypical natural phenomena, we redefine two types of attacks: mismatched malicious attack (2M-attack) and optimized mismatched malicious attack (O2M-attack). Using our own constructed voluminous 3MAD dataset, which covers a wide range of medical image modalities and harmful medical scenarios, we conduct a comprehensive analysis and propose the MCM optimization method, which significantly enhances the attack success rate on MedMLLMs. Evaluations with this dataset and novel attack methods, including white-box attacks on LLaVA-Med and transfer attacks on four other state-of-the-art models, indicate that even MedMLLMs designed with enhanced security features are vulnerable to security breaches. Our work underscores the urgent need for a concerted effort to implement robust security measures and enhance the safety and efficacy of open-source MedMLLMs, particularly given the potential severity of jailbreak attacks and other malicious or clinically significant exploits in medical settings. For further research and replication, anonymous access to our code is available at https://github.com/dirtycomputer/O2M_attack. Warning: Medical large model jailbreaking may generate content that includes unverified diagnoses and treatment recommendations. Always consult professional medical advice.

------------

`[2405.20797] Ovis: Structural Embedding Alignment for Multimodal Large Language Model <https://arxiv.org/abs/2405.20797>`__ Ovis:多模态大型语言模型的结构嵌入对齐

::

    Fri, 31 May 2024 13:59:18 GMT
    Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Han-Jia Ye

Current Multimodal Large Language Models (MLLMs) typically integrate a pre-trained LLM with another pre-trained vision transformer through a connector, such as an MLP, endowing the LLM with visual capabilities. However, the misalignment between two embedding strategies in MLLMs -- the structural textual embeddings based on an embedding look-up table and the continuous embeddings generated directly by the vision encoder -- makes challenges for a more seamless fusion of visual and textual information. We propose Ovis, a novel MLLM architecture designed to structurally align visual and textual embeddings. Ovis integrates an additional learnable visual embedding table into the visual encoder's process. To capture rich visual semantics, each image patch indexes the visual embedding table multiple times, resulting in a final visual embedding that is a probabilistic combination of the indexed embeddings.
This structural approach mirrors the method used for generating textual embeddings. Empirical evaluations on various multimodal benchmarks demonstrate that Ovis outperforms open-source MLLMs of similar parameter scales and even surpasses the proprietary model Qwen-VL-Plus overall. These results highlight the potential of Ovis' structured visual representation for advancing MLLM architectural design and promoting more effective multimodal learning. Both the source code and the training dataset of Ovis will be made publicly available.

------------

`[2405.20962] Large Language Models are Zero-Shot Next Location Predictors <https://arxiv.org/abs/2405.20962>`__ 大型语言模型是零样本下一位置预测器

::

    Fri, 31 May 2024 16:07:33 GMT
    Ciro Beneduce, Bruno Lepri, Massimiliano Luca

Predicting the locations an individual will visit in the future is crucial for solving many societal issues like disease diffusion and reduction of pollution among many others. The models designed to tackle next-location prediction, however, require a significant amount of individual-level information to be trained effectively. Such data may be scarce or even unavailable in some geographic regions or peculiar scenarios (e.g., cold-start in recommendation systems). Moreover, the design of a next-location predictor able to generalize or geographically transfer knowledge is still an open research challenge. Recent advances in natural language processing have led to a rapid diffusion of Large Language Models (LLMs) which have shown good generalization and reasoning capabilities. These insights, coupled with the recent findings that LLMs are rich in geographical knowledge, allowed us to believe that these models can act as zero-shot next-location predictors. This paper evaluates the capabilities of many popular LLMs in this role, specifically Llama, GPT-3.5 and Mistral 7B. After designing a proper prompt, we tested the models on three real-world mobility datasets. The results show that LLMs can obtain accuracies up to 32.4%, a significant relative improvement of over 600% when compared to sophisticated DL models specifically designed for human mobility. Moreover, we show that other LLMs are unable to perform the task properly. To prevent positively biased results, we also propose a framework inspired by other studies to test data contamination. Finally, we explored the possibility of using LLMs as text-based explainers for next-location prediction showing that can effectively provide an explanation for their decision. Notably, 7B models provide more generic, but still reliable, explanations compared to larger counterparts. Code: github.com/ssai-trento/LLM-zero-shot-NL

------------

`[2405.20413] Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters <https://arxiv.org/abs/2405.20413>`__ 通过密码字符破解大型语言模型的适度防护护栏

::

    Thu, 30 May 2024 18:38:36 GMT
    Haibo Jin, Andy Zhou, Joe D. Menke, Haohan Wang

Large Language Models (LLMs) are typically harmless but remain vulnerable to carefully crafted prompts known as ``jailbreaks'', which can bypass protective measures and induce harmful behavior. Recent advancements in LLMs have incorporated moderation guardrails that can filter outputs, which trigger processing errors for certain malicious questions. Existing red-teaming benchmarks often neglect to include questions that trigger moderation guardrails, making it difficult to evaluate jailbreak effectiveness. To address this issue, we introduce JAMBench, a harmful behavior benchmark designed to trigger and evaluate moderation guardrails. JAMBench involves 160 manually crafted instructions covering four major risk categories at multiple severity levels. Furthermore, we propose a jailbreak method, JAM (Jailbreak Against Moderation), designed to attack moderation guardrails using jailbreak prefixes to bypass input-level filters and a fine-tuned shadow model functionally equivalent to the guardrail model to generate cipher characters to bypass output-level filters. Our extensive experiments on four LLMs demonstrate that JAM achieves higher jailbreak success ($\sim$ $\times$ 19.88) and lower filtered-out rates ($\sim$ $\times$ 1/6) than baselines.

------------

`[2405.20646] Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item <https://arxiv.org/abs/2405.20646>`__ 大型语言模型增强了长尾用户和项目的顺序推荐

::

    Fri, 31 May 2024 07:24:42 GMT
    Qidong Liu, Xian Wu, Xiangyu Zhao, Yejing Wang, Zijian Zhang, Feng Tian and Yefeng Zheng

Sequential recommendation systems (SRS) serve the purpose of predicting users' subsequent preferences based on their past interactions and have been applied across various domains such as e-commerce and social networking platforms. However, practical SRS encounters challenges due to the fact that most users engage with only a limited number of items, while the majority of items are seldom consumed. These challenges, termed as the long-tail user and long-tail item dilemmas, often create obstacles for traditional SRS methods.
Mitigating these challenges is crucial as they can significantly impact user satisfaction and business profitability. While some research endeavors have alleviated these issues, they still grapple with issues such as seesaw or noise stemming from the scarcity of interactions. The emergence of large language models (LLMs) presents a promising avenue to address these challenges from a semantic standpoint. In this study, we introduce the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR), which leverages semantic embeddings from LLMs to enhance SRS performance without increasing computational overhead. To combat the long-tail item challenge, we propose a dual-view modeling approach that fuses semantic information from LLMs with collaborative signals from traditional SRS. To address the long-tail user challenge, we introduce a retrieval augmented self-distillation technique to refine user preference representations by incorporating richer interaction data from similar users. Through comprehensive experiments conducted on three authentic datasets using three widely used SRS models, our proposed enhancement framework demonstrates superior performance compared to existing methodologies.

------------

`[2405.20684] Joint Embeddings for Graph Instruction Tuning <https://arxiv.org/abs/2405.20684>`__ 基于联合嵌入的图指令调优

::

    Fri, 31 May 2024 08:26:47 GMT
    Vlad Argatu, Aaron Haag, Oliver Lohse

Large Language Models (LLMs) have achieved impressive performance in text understanding and have become an essential tool for building smart assistants.
Originally focusing on text, they have been enhanced with multimodal capabilities in recent works that successfully built visual instruction following assistants. As far as the graph modality goes, however, no such assistants have yet been developed. Graph structures are complex in that they represent relation between different features and are permutation invariant.
Moreover, representing them in purely textual form does not always lead to good LLM performance even for finetuned models. As a result, there is a need to develop a new method to integrate graphs in LLMs for general graph understanding. This work explores the integration of the graph modality in LLM for general graph instruction following tasks. It aims at producing a deep learning model that enhances an underlying LLM with graph embeddings and trains it to understand them and to produce, given an instruction, an answer grounded in the graph representation. The approach performs significantly better than a graph to text approach and remains consistent even for larger graphs.

------------

`[2405.20551] EM-Assist: Safe Automated ExtractMethod Refactoring with LLMs <https://arxiv.org/abs/2405.20551>`__ EM-Assist:基于LLMs的安全自动提取方法重构

::

    Fri, 31 May 2024 00:32:04 GMT
    Dorin Pomian, Abhiram Bellur, Malinda Dilhara, Zarina Kurbatova, Egor Bogomolov, Andrey Sokolov, Timofey Bryksin, Danny Dig

Excessively long methods, loaded with multiple responsibilities, are challenging to understand, debug, reuse, and maintain. The solution lies in the widely recognized Extract Method refactoring. While the application of this refactoring is supported in modern IDEs, recommending which code fragments to extract has been the topic of many research tools. However, they often struggle to replicate real-world developer practices, resulting in recommendations that do not align with what a human developer would do in real life. To address this issue, we introduce EM-Assist, an IntelliJ IDEA plugin that uses LLMs to generate refactoring suggestions and subsequently validates, enhances, and ranks them. Finally, EM-Assist uses the IntelliJ IDE to apply the user-selected recommendation. In our extensive evaluation of 1,752 real-world refactorings that actually took place in open-source projects, EM-Assist's recall rate was 53.4% among its top-5 recommendations, compared to 39.4% for the previous best-in-class tool that relies solely on static analysis. Moreover, we conducted a usability survey with 18 industrial developers and 94.4% gave a positive rating.

------------

`[2405.20777] Black-Box Detection of Language Model Watermarks <https://arxiv.org/abs/2405.20777>`__ 语言模型水印的黑盒检测

::

    Tue, 28 May 2024 08:41:30 GMT
    Gloaguen Thibaud, Jovanovi\'c Nikola, Staab Robin, Vechev Martin

Watermarking has emerged as a promising way to detect LLM-generated text. To apply a watermark an LLM provider, given a secret key, augments generations with a signal that is later detectable by any party with the same key. Recent work has proposed three main families of watermarking schemes, two of which focus on the property of preserving the LLM distribution. This is motivated by it being a tractable proxy for maintaining LLM capabilities, but also by the idea that concealing a watermark deployment makes it harder for malicious actors to hide misuse by avoiding a certain LLM or attacking its watermark.
Yet, despite much discourse around detectability, no prior work has investigated if any of these scheme families are detectable in a realistic black-box setting. We tackle this for the first time, developing rigorous statistical tests to detect the presence of all three most popular watermarking scheme families using only a limited number of black-box queries. We experimentally confirm the effectiveness of our methods on a range of schemes and a diverse set of open-source models. Our findings indicate that current watermarking schemes are more detectable than previously believed, and that obscuring the fact that a watermark was deployed may not be a viable way for providers to protect against adversaries. We further apply our methods to test for watermark presence behind the most popular public APIs: GPT4, Claude 3, Gemini 1.0 Pro, finding no strong evidence of a watermark at this point in time.

------------

`[2405.20778] Improved Generation of Adversarial Examples Against Safety-aligned LLMs <https://arxiv.org/abs/2405.20778>`__ 改进的针对安全对齐llm的对抗样本生成

::

    Tue, 28 May 2024 06:10:12 GMT
    Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen

Despite numerous efforts to ensure large language models (LLMs) adhere to safety standards and produce harmless content, some successes have been achieved in bypassing these restrictions, known as jailbreak attacks against LLMs. Adversarial prompts generated using gradient-based methods exhibit outstanding performance in performing jailbreak attacks automatically.
Nevertheless, due to the discrete nature of texts, the input gradient of LLMs struggles to precisely reflect the magnitude of loss change that results from token replacements in the prompt, leading to limited attack success rates against safety-aligned LLMs, even in the white-box setting. In this paper, we explore a new perspective on this problem, suggesting that it can be alleviated by leveraging innovations inspired in transfer-based attacks that were originally proposed for attacking black-box image classification models. For the first time, we appropriate the ideologies of effective methods among these transfer-based attacks, i.e., Skip Gradient Method and Intermediate Level Attack, for improving the effectiveness of automatically generated adversarial examples against white-box LLMs. With appropriate adaptations, we inject these ideologies into gradient-based adversarial prompt generation processes and achieve significant performance gains without introducing obvious computational cost. Meanwhile, by discussing mechanisms behind the gains, new insights are drawn, and proper combinations of these methods are also developed. Our empirical results show that the developed combination achieves >30% absolute increase in attack success rates compared with GCG for attacking the Llama-2-7B-Chat model on AdvBench.

------------

`[2402.03962] Position: Stop Making Unscientific AGI Performance Claims <https://arxiv.org/abs/2402.03962>`__ 立场:停止发表不科学的AGI性能声明

::

    replaced with revised version Fri, 31 May 2024 15:16:21 GMT
    Submission history From: Patrick Altmeyer [view email]
    [v1] Tue, 6 Feb 2024 12:42:21 UTC (8,410 KB)
    [v2] Wed, 7 Feb 2024 08:33:23 UTC (8,410 KB)
    [v3] Fri, 31 May 2024 15:16:21 UTC (8,186 KB)
    Patrick Altmeyer, Andrew M. Demetriou, Antony Bartlett, Cynthia C. S. Liem

Developments in the field of Artificial Intelligence (AI), and particularly large language models (LLMs), have created a 'perfect storm' for observing 'sparks' of Artificial General Intelligence (AGI) that are spurious. Like simpler models, LLMs distill meaningful representations in their latent embeddings that have been shown to correlate with external variables. Nonetheless, the correlation of such representations has often been linked to human-like intelligence in the latter but not the former. We probe models of varying complexity including random projections, matrix decompositions, deep autoencoders and transformers: all of them successfully distill information that can be used to predict latent or external variables and yet none of them have previously been linked to AGI. We argue and empirically demonstrate that the finding of meaningful patterns in latent spaces of models cannot be seen as evidence in favor of AGI. Additionally, we review literature from the social sciences that shows that humans are prone to seek such patterns and anthropomorphize. We conclude that both the methodological setup and common public image of AI are ideal for the misinterpretation that correlations between model representations and some variables of interest are 'caused' by the model's understanding of underlying 'ground truth' relationships. We, therefore, call for the academic community to exercise extra caution, and to be keenly aware of principles of academic integrity, in interpreting and communicating about AI research outcomes.

------------

`[2402.10705] AutoSAT: Automatically Optimize SAT Solvers via Large Language Models <https://arxiv.org/abs/2402.10705>`__ AutoSAT:基于大型语言模型自动优化SAT求解器

::

    replaced with revised version Fri, 31 May 2024 11:38:00 GMT
    Submission history From: Yiwen Sun [view email]
    [v1] Fri, 16 Feb 2024 14:04:56 UTC (2,799 KB)
    [v2] Fri, 31 May 2024 11:38:00 UTC (561 KB)
    Yiwen Sun, Xianyin Zhang, Shiyu Huang, Shaowei Cai, BingZhen Zhang, Ke Wei

Heuristics are crucial in SAT solvers, but no heuristic rules are suitable for all SAT problems. Therefore, it is helpful to refine specific heuristics for specific problems. In this context, we present AutoSAT, a novel framework for automatically optimizing heuristics in SAT solvers. AutoSAT is based on Large Language Models (LLMs) which is able to autonomously generate codes, conduct evaluation, and then utilize feedback to further optimize heuristics, thereby reducing human intervention and enhancing solver capabilities. AutoSAT operates on a plug-and-play basis, eliminating the need for extensive enterprise and model training, and fosters a Multi-Agent-based collaborative process with fault tolerance to ensure robust heuristic optimization. We implement AutoSAT on a lightweight Conflict-Driven Clause Learning (CDCL) solver EasySAT (the volume of EasySAT is about one-fiftieth of the State-of-the-Art hybrid solver Kissat) and extensive experiments on seven datasets demonstrate its superior performance. Out of the seven testing datasets, AutoSAT shows a superior performance to Kissat in two datasets and displays an overall similar performance in three datasets. Some heuristics generated by AutoSAT are even counter-intuitive but are very effective.

------------

`[2405.18870] LLMs achieve adult human performance on higher-order theory of mind tasks <https://arxiv.org/abs/2405.18870>`__ llm在高阶心智理论任务上实现成人的表现

::

    replaced with revised version Fri, 31 May 2024 12:45:50 GMT
    Submission history From: Winnie Street [view email]
    [v1] Wed, 29 May 2024 08:31:16 UTC (1,418 KB)
    [v2] Fri, 31 May 2024 12:45:50 UTC (1,418 KB)
    Winnie Street, John Oliver Siy, Geoff Keeling, Adrien Baranes, Benjamin Barnett, Michael McKibben, Tatenda Kanyere, Alison Lentz, Blaise Aguera y Arcas, Robin I. M. Dunbar

This paper examines the extent to which large language models (LLMs) have developed higher-order theory of mind (ToM); the human ability to reason about multiple mental and emotional states in a recursive manner (e.g. I think that you believe that she knows). This paper builds on prior work by introducing a handwritten test suite -- Multi-Order Theory of Mind Q&A -- and using it to compare the performance of five LLMs to a newly gathered adult human benchmark. We find that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences. Our results suggest that there is an interplay between model size and finetuning for the realisation of ToM abilities, and that the best-performing LLMs have developed a generalised capacity for ToM. Given the role that higher-order ToM plays in a wide range of cooperative and competitive human behaviours, these findings have significant implications for user-facing LLM applications.

------------

`[2305.15255] Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM <https://arxiv.org/abs/2305.15255>`__ 语谱图驱动的LLM口语问答和语音延续

::

    replaced with revised version Fri, 31 May 2024 01:29:27 GMT
    Submission history From: Julian Salazar [view email]
    [v1] Wed, 24 May 2023 15:39:43 UTC (184 KB)
    [v2] Thu, 1 Jun 2023 08:04:19 UTC (184 KB)
    [v3] Fri, 20 Oct 2023 05:55:39 UTC (225 KB)
    [v4] Fri, 31 May 2024 01:29:27 UTC (228 KB)
    Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, Michelle Tadmor Ramanovich

We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (this https URL) and spoken QA dataset (this https URL).

------------

`[2312.09085] The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation <https://arxiv.org/abs/2312.09085>`__ 地球是平的，因为……:通过说服性对话调查llm对错误信息的信念

::

    replaced with revised version Fri, 31 May 2024 15:13:33 GMT
    Submission history From: Rongwu Xu [view email]
    [v1] Thu, 14 Dec 2023 16:16:50 UTC (6,944 KB)
    [v2] Wed, 20 Dec 2023 08:03:12 UTC (6,951 KB)
    [v3] Fri, 29 Dec 2023 05:30:50 UTC (6,952 KB)
    [v4] Tue, 9 Jan 2024 05:40:00 UTC (6,949 KB)
    [v5] Fri, 31 May 2024 15:13:33 UTC (6,950 KB)
    Rongwu Xu, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, Han Qiu

Large language models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies.

------------

`[2401.00368] Improving Text Embeddings with Large Language Models <https://arxiv.org/abs/2401.00368>`__ 用大型语言模型改进文本嵌入

::

    replaced with revised version Fri, 31 May 2024 07:22:01 GMT
    Submission history From: Liang Wang [view email]
    [v1] Sun, 31 Dec 2023 02:13:18 UTC (129 KB)
    [v2] Fri, 19 Jan 2024 05:16:20 UTC (133 KB)
    [v3] Fri, 31 May 2024 07:22:01 UTC (152 KB)
    Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei

In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.

------------

`[2401.11052] Mining experimental data from Materials Science literature with Large Language Models: an evaluation study <https://arxiv.org/abs/2401.11052>`__ 基于大型语言模型的材料科学文献实验数据挖掘:评估研究

::

    replaced with revised version Thu, 30 May 2024 20:28:08 GMT
    Submission history From: Luca Foppiano [view email]
    [v1] Fri, 19 Jan 2024 23:00:31 UTC (479 KB)
    [v2] Tue, 9 Apr 2024 07:32:37 UTC (449 KB)
    [v3] Thu, 30 May 2024 20:28:08 UTC (449 KB)
    Luca Foppiano, Guillaume Lambard, Toshiyuki Amagasa, Masashi Ishii

This study is dedicated to assessing the capabilities of large language models (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in extracting structured information from scientific documents in materials science. To this end, we primarily focus on two critical tasks of information extraction: (i) a named entity recognition (NER) of studied materials and physical properties and (ii) a relation extraction (RE) between these entities. Due to the evident lack of datasets within Materials Informatics (MI), we evaluated using SuperMat, based on superconductor research, and MeasEval, a generic measurement evaluation corpus. The performance of LLMs in executing these tasks is benchmarked against traditional models based on the BERT architecture and rule-based approaches (baseline). We introduce a novel methodology for the comparative analysis of intricate material expressions, emphasising the standardisation of chemical formulas to tackle the complexities inherent in materials science information assessment. For NER, LLMs fail to outperform the baseline with zero-shot prompting and exhibit only limited improvement with few-shot prompting. However, a GPT-3.5-Turbo fine-tuned with the appropriate strategy for RE outperforms all models, including the baseline. Without any fine-tuning, GPT-4 and GPT-4-Turbo display remarkable reasoning and relationship extraction capabilities after being provided with merely a couple of examples, surpassing the baseline. Overall, the results suggest that although LLMs demonstrate relevant reasoning skills in connecting concepts, specialised models are currently a better choice for tasks requiring extracting complex domain-specific entities like materials. These insights provide initial guidance applicable to other materials science sub-domains in future work.

------------

`[2402.12146] Enabling Weak LLMs to Judge Response Reliability via Meta Ranking <https://arxiv.org/abs/2402.12146>`__ 通过元排序使弱llm能够判断响应可靠性

::

    replaced with revised version Fri, 31 May 2024 03:25:42 GMT
    Submission history From: Zijun Liu [view email]
    [v1] Mon, 19 Feb 2024 13:57:55 UTC (8,907 KB)
    [v2] Sun, 26 May 2024 17:46:42 UTC (9,415 KB)
    [v3] Fri, 31 May 2024 03:25:42 UTC (9,415 KB)
    Zijun Liu, Boqun Kou, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu

Despite the strong performance of large language models (LLMs) across a wide range of tasks, they still have reliability issues. Previous studies indicate that strong LLMs like GPT-4-turbo excel in evaluating the reliability of responses from LLMs, but face efficiency and local deployment issues. Thus, to enable weak LLMs to effectively assess the reliability of LLM responses, we propose a novel cross-query-comparison-based method called $\textit{Meta Ranking}$ (MR). Unlike previous few-shot methods that solely based on in-context learning capabilities in LLMs, MR assesses reliability by pairwisely ranking the target query-response pair with multiple reference query-response pairs. We found that MR is highly effective in error detection for LLM responses, where weak LLMs, such as Phi-2, could surpass strong baselines like GPT-3.5-turbo, requiring only five reference samples and significantly improving efficiency. We further demonstrate that MR can enhance strong LLMs' performance in two practical applications: model cascading and instruction tuning. In model cascading, we combine open- and closed-source LLMs to achieve performance comparable to GPT-4-turbo with lower costs. In instruction tuning, we use MR for iterative training data filtering, significantly reducing data processing time and enabling LLaMA-7B and Phi-2 to surpass Alpaca-13B with fewer training tokens. These results underscore the high potential of MR in both efficiency and effectiveness.

------------

`[2402.15938] Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models <https://arxiv.org/abs/2402.15938>`__ 泛化或记忆:大型语言模型的数据污染和可信评估

::

    replaced with revised version Fri, 31 May 2024 17:49:03 GMT
    Submission history From: Yihong Dong [view email]
    [v1] Sat, 24 Feb 2024 23:54:41 UTC (129 KB)
    [v2] Thu, 16 May 2024 12:34:24 UTC (144 KB)
    [v3] Fri, 31 May 2024 17:49:03 UTC (241 KB)
    Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li

Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's output distribution. To facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval, for data contamination detection and contamination mitigation evaluation tasks. Extensive experimental results show that CDD achieves the average relative improvements of 21.8\%-30.2\% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect implicit contamination. TED substantially mitigates performance improvements up to 66.9\% attributed to data contamination across various contamination setups. In real-world applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark.

------------

`[2404.07611] NoticIA: A Clickbait Article Summarization Dataset in Spanish <https://arxiv.org/abs/2404.07611>`__ NoticIA:西班牙语标题党文章摘要数据集

::

    replaced with revised version Fri, 31 May 2024 15:19:18 GMT
    Submission history From: Iker García-Ferrero [view email]
    [v1] Thu, 11 Apr 2024 09:59:01 UTC (1,374 KB)
    [v2] Fri, 31 May 2024 15:19:18 UTC (1,613 KB)
    Iker Garc\'ia-Ferrero, Bego\~na Altuna

We present NoticIA, a dataset consisting of 850 Spanish news articles featuring prominent clickbait headlines, each paired with high-quality, single-sentence generative summarizations written by humans. This task demands advanced text understanding and summarization abilities, challenging the models' capacity to infer and connect diverse pieces of information to meet the user's informational needs generated by the clickbait headline. We evaluate the Spanish text comprehension capabilities of a wide range of state-of-the-art large language models. Additionally, we use the dataset to train ClickbaitFighter, a task-specific model that achieves near-human performance in this task.

------------

`[2405.08295] SpeechVerse: A Large-scale Generalizable Audio Language Model <https://arxiv.org/abs/2405.08295>`__ SpeechVerse:大规模可泛化音频语言模型

::

    replaced with revised version Fri, 31 May 2024 17:47:40 GMT
    Submission history From: Saket Dingliwal [view email]
    [v1] Tue, 14 May 2024 03:33:31 UTC (1,398 KB)
    [v2] Fri, 31 May 2024 17:47:40 UTC (1,398 KB)
    Nilaksh Das, Saket Dingliwal, Srikanth Ronanki, Rohit Paturi, Zhaocheng Huang, Prashant Mathur, Jie Yuan, Dhanush Bekal, Xing Niu, Sai Muralidhar Jayanthi, Xilai Li, Karel Mundnich, Monica Sunkara, Sundararajan Srinivasan, Kyu J Han, Katrin Kirchhoff

Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instructions. Recently, many works have further expanded this capability to perceive multimodal audio and text inputs, but their capabilities are often limited to specific fine-tuned tasks such as automatic speech recognition and translation. We therefore develop SpeechVerse, a robust multi-task training and curriculum learning framework that combines pre-trained speech and text foundation models via a small set of learnable parameters, while keeping the pre-trained models frozen during training. The models are instruction finetuned using continuous latent representations extracted from the speech foundation model to achieve optimal zero-shot performance on a diverse range of speech processing tasks using natural language instructions. We perform extensive benchmarking that includes comparing our model performance against traditional baselines across several datasets and tasks. Furthermore, we evaluate the model's capability for generalized instruction following by testing on out-of-domain datasets, novel prompts, and unseen tasks. Our empirical experiments reveal that our multi-task SpeechVerse model is even superior to conventional task-specific baselines on 9 out of the 11 tasks.

------------

`[2405.13923] Why Not Transform Chat Large Language Models to Non-English? <https://arxiv.org/abs/2405.13923>`__ 为什么不将聊天大型语言模型转换为非英语语言模型?

::

    replaced with revised version Fri, 31 May 2024 07:22:45 GMT
    Submission history From: Xiang Geng [view email]
    [v1] Wed, 22 May 2024 18:53:25 UTC (978 KB)
    [v2] Fri, 31 May 2024 07:22:45 UTC (978 KB)
    Xiang Geng, Ming Zhu, Jiahuan Li, Zhejian Lai, Wei Zou, Shuaijie She, Jiaxin Guo, Xiaofeng Zhao, Yinglu Li, Yuang Li, Chang Su, Yanqing Zhao, Xinglin Lyu, Min Zhang, Jiajun Chen, Hao Yang, Shujian Huang

The scarcity of non-English data limits the development of non-English large language models (LLMs). Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety. However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data? (2) How can we prevent the original knowledge from catastrophic forgetting during transformation? We target these issues by introducing a simple framework called TransLLM. For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step. We further enhance the performance of sub-tasks with publicly available data. For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai language. Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.

------------

`[2405.15032] Aya 23: Open Weight Releases to Further Multilingual Progress <https://arxiv.org/abs/2405.15032>`__ Aya 23: Open Weight释放了进一步的多语言进展

::

    replaced with revised version Fri, 31 May 2024 14:47:55 GMT
    Submission history From: Ahmet Üstün [view email]
    [v1] Thu, 23 May 2024 20:10:38 UTC (8,040 KB)
    [v2] Fri, 31 May 2024 14:47:55 UTC (8,040 KB)
    Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet \"Ust\"un, Sara Hooker

This technical report introduces Aya 23, a family of multilingual language models. Aya 23 builds on the recent release of the Aya model (Üstün et al., 2024), focusing on pairing a highly performant pre-trained model with the recently released Aya collection (Singh et al., 2024). The result is a powerful multilingual large language model serving 23 languages, expanding state-of-art language modeling capabilities to approximately half of the world's population. The Aya model covered 101 languages whereas Aya 23 is an experiment in depth vs breadth, exploring the impact of allocating more capacity to fewer languages that are included during pre-training. Aya 23 outperforms both previous massively multilingual models like Aya 101 for the languages it covers, as well as widely used models like Gemma, Mistral and Mixtral on an extensive range of discriminative and generative tasks. We release the open weights for both the 8B and 35B models as part of our continued commitment for expanding access to multilingual progress.

------------

`[2405.19787] From Symbolic Tasks to Code Generation: Diversification Yields Better Task Performers <https://arxiv.org/abs/2405.19787>`__ 从符号任务到代码生成:多样化产生更好的任务执行者

::

    replaced with revised version Fri, 31 May 2024 01:23:41 GMT
    Submission history From: Dylan Zhang [view email]
    [v1] Thu, 30 May 2024 07:54:07 UTC (1,333 KB)
    [v2] Fri, 31 May 2024 01:23:41 UTC (1,333 KB)
    Dylan Zhang, Justin Wang, Francois Charton

Instruction tuning -- tuning large language models on instruction-output pairs -- is a promising technique for making models better adapted to the real world. Yet, the key factors driving the model's capability to understand and follow instructions not seen during training remain under-explored. Our investigation begins with a series of synthetic experiments within the theoretical framework of a Turing-complete algorithm called Markov algorithm, which allows fine-grained control over the instruction-tuning data. Generalization and robustness with respect to the training distribution emerge once a diverse enough set of tasks is provided, even though very few examples are provided for each task. We extend these initial results to a real-world application scenario of code generation and find that a more diverse instruction set, extending beyond code-related tasks, improves the performance of code generation. Our observations suggest that a more diverse semantic space for instruction-tuning sets greatly improves the model's ability to follow instructions and perform tasks.

------------

`[2310.02905] Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers <https://arxiv.org/abs/2310.02905>`__ 用你的直觉:使用神经匪结合transformer对llm进行指令优化

::

    replaced with revised version Fri, 31 May 2024 16:27:53 GMT
    Submission history From: Zhongxiang Dai [view email]
    [v1] Mon, 2 Oct 2023 02:01:16 UTC (1,300 KB)
    [v2] Fri, 31 May 2024 16:27:53 UTC (1,729 KB)
    Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low

Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algorithm which replaces the GP in BO by an NN surrogate to optimize instructions for black-box LLMs. More importantly, the neural bandit algorithm allows us to naturally couple the NN surrogate with the hidden representation learned by a pre-trained transformer (i.e., an open-source LLM), which significantly boosts its performance. These motivate us to propose our INSTruction optimization usIng Neural bandits Coupled with Transformers (INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use extensive experiments to show that INSTINCT consistently outperforms baselines in different tasks, e.g., various instruction induction tasks and the task of improving zero-shot chain-of-thought instructions. Our code is available at this https URL.

------------

`[2402.03299] GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models <https://arxiv.org/abs/2402.03299>`__ GUARD:角色扮演生成自然语言越狱以测试大型语言模型的准则遵循性

::

    replaced with revised version Thu, 30 May 2024 21:14:26 GMT
    Submission history From: Ruoxi Chen [view email]
    [v1] Mon, 5 Feb 2024 18:54:43 UTC (5,101 KB)
    [v2] Tue, 27 Feb 2024 00:09:00 UTC (5,101 KB)
    [v3] Wed, 6 Mar 2024 04:28:09 UTC (5,101 KB)
    [v4] Thu, 30 May 2024 21:14:26 UTC (5,057 KB)
    Haibo Jin, Ruoxi Chen, Andy Zhou, Yang Zhang, Haohan Wang

The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.

------------

`[2402.07043] A Tale of Tails: Model Collapse as a Change of Scaling Laws <https://arxiv.org/abs/2402.07043>`__ 尾巴的故事:模型崩溃作为缩放定律的变化

::

    replaced with revised version Fri, 31 May 2024 12:27:52 GMT
    Submission history From: Yunzhen Feng [view email]
    [v1] Sat, 10 Feb 2024 21:06:34 UTC (3,128 KB)
    [v2] Fri, 31 May 2024 12:27:52 UTC (3,209 KB)
    Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton and Julia Kempe

As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation using the large language model Llama2.

------------

`[2404.08846] Experimental Design for Active Transductive Inference in Large Language Models <https://arxiv.org/abs/2404.08846>`__ 大型语言模型中主动转导推理的实验设计

::

    replaced with revised version Fri, 31 May 2024 02:37:10 GMT
    Submission history From: Subhojyoti Mukherjee [view email]
    [v1] Fri, 12 Apr 2024 23:27:46 UTC (24,594 KB)
    [v2] Fri, 31 May 2024 02:37:10 UTC (28,206 KB)
    Subhojyoti Mukherjee, Anusha Lalitha, Aniket Deshmukh, Ge Liu, Yifei Ma, Branislav Kveton

One emergent ability of large language models (LLMs) is that query-specific examples can be included in the prompt at inference time. In this work, we use active learning for adaptive prompt design and call it Active In-context Prompt Design (AIPD). We design the LLM prompt by adaptively choosing few-shot examples from a training set to optimize performance on a test set. The training examples are initially unlabeled and we obtain the label of the most informative ones, which maximally reduces uncertainty in the LLM prediction. We propose two algorithms, GO and SAL, which differ in how the few-shot examples are chosen. We analyze these algorithms in linear models: first GO and then use its equivalence with SAL. We experiment with many different tasks in small, medium-sized, and large language models; and show that GO and SAL outperform other methods for choosing few-shot examples in the LLM prompt at inference time.

------------

`[2404.18239] SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning <https://arxiv.org/abs/2404.18239>`__ 灵魂:解锁LLM忘却的二阶优化力量

::

    replaced with revised version Fri, 31 May 2024 17:38:51 GMT
    Submission history From: Jinghan Jia [view email]
    [v1] Sun, 28 Apr 2024 16:31:32 UTC (332 KB)
    [v2] Fri, 31 May 2024 17:38:51 UTC (331 KB)
    Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, Sijia Liu

Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility out of the scope of unlearning. While interest in studying LLM unlearning is growing,the impact of the optimizer choice for LLM unlearning remains under-explored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between {second-order optimization} and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order unlearning framework, termed SOUL, built upon the second-order clipped stochastic optimization (Sophia)-based LLM training method. SOUL extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, suggesting the promise of second-order optimization in providing a scalable and easily implementable solution for LLM unlearning.

------------

`[2405.14622] Calibrated Self-Rewarding Vision Language Models <https://arxiv.org/abs/2405.14622>`__ 校准自奖励视觉语言模型

::

    replaced with revised version Fri, 31 May 2024 16:37:53 GMT
    Submission history From: Huaxiu Yao [view email]
    [v1] Thu, 23 May 2024 14:30:33 UTC (1,977 KB)
    [v2] Sat, 25 May 2024 19:36:07 UTC (1,977 KB)
    [v3] Fri, 31 May 2024 16:37:53 UTC (1,977 KB)
    Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, Huaxiu Yao

Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches may not effectively reflect the target LVLM's preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR enhances performance and reduces hallucinations across ten benchmarks and tasks, achieving substantial improvements over existing methods by 7.62%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning. Our data and code are available at this https URL.

------------

`[2405.18641] Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning <https://arxiv.org/abs/2405.18641>`__ 针对有害微调的大型语言模型的惰性安全对齐

::

    replaced with revised version Thu, 30 May 2024 20:03:37 GMT
    Submission history From: Tiansheng Huang [view email]
    [v1] Tue, 28 May 2024 22:53:43 UTC (1,592 KB)
    [v2] Thu, 30 May 2024 20:03:37 UTC (1,592 KB)
    Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu

Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data. First time in the literature, we show that the jail-broken effect can be mitigated by separating states in the finetuning stage to optimize the alignment and user datasets. Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance. By statistical analysis, we show that the \textit{excess drift} towards consensus could be a probable reason for the instability. To remedy this issue, we propose \textbf{L}azy(\textbf{i}) \textbf{s}afety \textbf{a}lignment (\textbf{Lisa}), which introduces a proximal term to constraint the drift of each state. Theoretically, the benefit of the proximal term is supported by the convergence analysis, wherein we show that a sufficient large proximal factor is necessary to guarantee Lisa's convergence. Empirically, our results on four downstream finetuning tasks show that Lisa with a proximal term can significantly increase alignment performance while maintaining the LLM's accuracy on the user tasks. Code is available at \url{this https URL}.

------------

`[2402.16714] Quantum linear algebra is all you need for Transformer architectures <https://arxiv.org/abs/2402.16714>`__ 量子线性代数是Transformer架构所需要的全部

::

    replaced with revised version Fri, 31 May 2024 03:34:57 GMT
    Submission history From: Naixu Guo [view email]
    [v1] Mon, 26 Feb 2024 16:31:28 UTC (738 KB)
    [v2] Fri, 31 May 2024 03:34:57 UTC (1,064 KB)
    Naixu Guo, Zhan Yu, Matthew Choi, Aman Agrawal, Kouhei Nakaji, Al\'an Aspuru-Guzik and Patrick Rebentrost

Generative machine learning methods such as large-language models are revolutionizing the creation of text and images. While these models are powerful they also harness a large amount of computational resources. The transformer is a key component in large language models that aims to generate a suitable completion of a given partial sequence. In this work, we investigate transformer architectures under the lens of fault-tolerant quantum computing. The input model is one where trained weight matrices are given as block encodings and we construct the query, key, and value matrices for the transformer. We show how to prepare a block encoding of the self-attention matrix, with a new subroutine for the row-wise application of the softmax function. In addition, we combine quantum subroutines to construct important building blocks in the transformer, the residual connection and layer normalization, and the feed-forward neural network. Our subroutines prepare an amplitude encoding of the transformer output, which can be measured to obtain a prediction. Based on common open-source large-language models, we provide insights into the behavior of important parameters determining the run time of the quantum algorithm. We discuss the potential and challenges for obtaining a quantum advantage.

------------

`[2405.19358] Robustifying Safety-Aligned Large Language Models through Clean Data Curation <https://arxiv.org/abs/2405.19358>`__ 通过干净数据管理对安全对齐的大型语言模型的鲁棒性

::

    replaced with revised version Fri, 31 May 2024 02:09:51 GMT
    Submission history From: Xiaoqun Liu [view email]
    [v1] Fri, 24 May 2024 04:50:38 UTC (5,041 KB)
    [v2] Fri, 31 May 2024 02:09:51 UTC (5,040 KB)
    Xiaoqun Liu, Jiacheng Liang, Muchao Ye and Zhaohan Xi

Large language models (LLMs) are vulnerable when trained on datasets containing harmful content, which leads to potential jailbreaking attacks in two scenarios: the integration of harmful texts within crowdsourced data used for pre-training and direct tampering with LLMs through fine-tuning. In both scenarios, adversaries can compromise the safety alignment of LLMs, exacerbating malfunctions. Motivated by the need to mitigate these adversarial influences, our research aims to enhance safety alignment by either neutralizing the impact of malicious texts in pre-training datasets or increasing the difficulty of jailbreaking during downstream fine-tuning. In this paper, we propose a data curation framework designed to counter adversarial impacts in both scenarios. Our method operates under the assumption that we have no prior knowledge of attack details, focusing solely on curating clean texts. We introduce an iterative process aimed at revising texts to reduce their perplexity as perceived by LLMs, while simultaneously preserving their text quality. By pre-training or fine-tuning LLMs with curated clean texts, we observe a notable improvement in LLM robustness regarding safety alignment against harmful queries. For instance, when pre-training LLMs using a crowdsourced dataset containing 5\% harmful instances, adding an equivalent amount of curated texts significantly mitigates the likelihood of providing harmful responses in LLMs and reduces the attack success rate by 71\%. Our study represents a significant step towards mitigating the risks associated with training-based jailbreaking and fortifying the secure utilization of LLMs.

------------

`[2405.20081] NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models <https://arxiv.org/abs/2405.20081>`__ NoiseBoost:用噪声扰动缓解多模态大型语言模型的幻觉

::

    replaced with revised version Fri, 31 May 2024 07:40:04 GMT
    Submission history From: Kai Wu [view email]
    [v1] Thu, 30 May 2024 14:11:27 UTC (16,093 KB)
    [v2] Fri, 31 May 2024 07:40:04 UTC (16,093 KB)
    Kai Wu, Boyuan Jiang, Zhengkai Jiang, Qingdong He, Donghao Luo, Shengzhi Wang, Qingwen Liu, Chengjie Wang

Multimodal large language models (MLLMs) contribute a powerful mechanism to understanding visual information building on large language models. However, MLLMs are notorious for suffering from hallucinations, especially when generating lengthy, detailed descriptions for images. Our analysis reveals that hallucinations stem from the inherent summarization mechanism of large language models, leading to excessive dependence on linguistic tokens while neglecting vision information. In this paper, we propose NoiseBoost, a broadly applicable and simple method for alleviating hallucinations for MLLMs through the integration of noise feature perturbations. Noise perturbation acts as a regularizer, facilitating a balanced distribution of attention weights among visual and linguistic tokens. Despite its simplicity, NoiseBoost consistently enhances the performance of MLLMs across common training strategies, including supervised fine-tuning and reinforcement learning. Further, NoiseBoost pioneerly enables semi-supervised learning for MLLMs, unleashing the power of unlabeled data. Comprehensive experiments demonstrate that NoiseBoost improves dense caption accuracy by 8.1% with human evaluation and achieves comparable results with 50% of the data by mining unlabeled data. Code and models are available at this https URL.

------------

`[2405.20319] ParSEL: Parameterized Shape Editing with Language <https://arxiv.org/abs/2405.20319>`__ ParSEL:基于语言的参数化形状编辑

::

    replaced with revised version Fri, 31 May 2024 04:09:41 GMT
    Submission history From: Aditya Ganeshan [view email]
    [v1] Thu, 30 May 2024 17:55:46 UTC (38,860 KB)
    [v2] Fri, 31 May 2024 04:09:41 UTC (38,849 KB)
    Aditya Ganeshan, Ryan Y. Huang, Xianghao Xu, R. Kenny Jones, Daniel Ritchie

The ability to edit 3D assets from natural language presents a compelling paradigm to aid in the democratization of 3D content creation. However, while natural language is often effective at communicating general intent, it is poorly suited for specifying precise manipulation. To address this gap, we introduce ParSEL, a system that enables controllable editing of high-quality 3D assets from natural language. Given a segmented 3D mesh and an editing request, ParSEL produces a parameterized editing program. Adjusting the program parameters allows users to explore shape variations with a precise control over the magnitudes of edits. To infer editing programs which align with an input edit request, we leverage the abilities of large-language models (LLMs). However, while we find that LLMs excel at identifying initial edit operations, they often fail to infer complete editing programs, and produce outputs that violate shape semantics. To overcome this issue, we introduce Analytical Edit Propagation (AEP), an algorithm which extends a seed edit with additional operations until a complete editing program has been formed. Unlike prior methods, AEP searches for analytical editing operations compatible with a range of possible user edits through the integration of computer algebra systems for geometric analysis. Experimentally we demonstrate ParSEL's effectiveness in enabling controllable editing of 3D objects through natural language requests over alternative system designs.

------------

`[2405.19732] Two Optimizers Are Better Than One: LLM Catalyst for Enhancing Gradient-Based Optimization <https://arxiv.org/abs/2405.19732>`__ 两个优化器优于一个:用于增强基于梯度的优化的LLM催化剂

::

    replaced with revised version Fri, 31 May 2024 08:13:34 GMT
    Submission history From: Zixian Guo [view email]
    [v1] Thu, 30 May 2024 06:24:14 UTC (442 KB)
    [v2] Fri, 31 May 2024 08:13:34 UTC (442 KB)
    Zixian Guo, Ming Liu, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo

Learning a skill generally relies on both practical experience by doer and insightful high-level guidance by instructor. Will this strategy also work well for solving complex non-convex optimization problems? Here, a common gradient-based optimizer acts like a disciplined doer, making locally optimal update at each step. Recent methods utilize large language models (LLMs) to optimize solutions for concrete problems by inferring from natural language instructions, akin to a high-level instructor. In this paper, we show that these two optimizers are complementary to each other, suggesting a collaborative optimization approach. The gradient-based optimizer and LLM-based optimizer are combined in an interleaved manner. We instruct LLMs using task descriptions and timely optimization trajectories recorded during gradient-based optimization. Inferred results from LLMs are used as restarting points for the next stage of gradient optimization. By leveraging both the locally rigorous gradient-based optimizer and the high-level deductive LLM-based optimizer, our combined optimization method consistently yields improvements over competitive baseline prompt tuning methods. Our results demonstrate the synergistic effect of conventional gradient-based optimization and the inference ability of LLMs. The code is released at this https URL.

------------

----------
Index (73)
----------

`[2405.20519] Diffusion On Syntax Trees For Program Synthesis <https://arxiv.org/abs/2405.20519>`__ 面向程序合成的语法树扩散

`[2405.20526] Automated Generation and Tagging of Knowledge Components from Multiple-Choice Questions <https://arxiv.org/abs/2405.20526>`__ 多项选择题中知识成分的自动生成与标注

`[2405.20625] Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning <https://arxiv.org/abs/2405.20625>`__ LLM-Modulo框架的鲁棒规划:旅行规划案例研究

`[2405.20628] ToxVidLLM: A Multimodal LLM-based Framework for Toxicity Detection in Code-Mixed Videos <https://arxiv.org/abs/2405.20628>`__ ToxVidLLM:基于多模态llm的混合视频毒性检测框架

`[2405.20653] Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens <https://arxiv.org/abs/2405.20653>`__ 利用静默标记增强针对大型语言模型的越狱攻击

`[2405.21030] Standards for Belief Representations in LLMs <https://arxiv.org/abs/2405.21030>`__ llm中的信念表示标准

`[2405.20404] XPrompt:Explaining Large Language Model's Generation via Joint Prompt Attribution <https://arxiv.org/abs/2405.20404>`__ XPrompt:基于联合提示归因的大型语言模型生成解释

`[2405.20477] Automated Focused Feedback Generation for Scientific Writing Assistance <https://arxiv.org/abs/2405.20477>`__ 科学写作辅助的自动聚焦反馈生成

`[2405.20505] SPOT: Text Source Prediction from Originality Score Thresholding <https://arxiv.org/abs/2405.20505>`__ SPOT:基于原创分数阈值的文本来源预测

`[2405.20512] How Multilingual Are Large Language Models Fine-Tuned for Translation? <https://arxiv.org/abs/2405.20512>`__ 如何对多语言大型语言模型进行翻译微调?

`[2405.20527] Towards Ontology-Enhanced Representation Learning for Large Language Models <https://arxiv.org/abs/2405.20527>`__ 本体增强的大型语言模型表示学习研究

`[2405.20582] The Point of View of a Sentiment: Towards Clinician Bias Detection in Psychiatric Notes <https://arxiv.org/abs/2405.20582>`__ 从情感的角度看:精神病学笔记中对临床医生偏见的检测

`[2405.20585] GAMedX: Generative AI-based Medical Entity Data Extractor Using Large Language Models <https://arxiv.org/abs/2405.20585>`__ GAMedX:基于生成式人工智能的大型语言模型医疗实体数据提取器

`[2405.20588] DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models <https://arxiv.org/abs/2405.20588>`__ DAFNet:大型语言模型中序列模型编辑的动态辅助融合

`[2405.20612] UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation <https://arxiv.org/abs/2405.20612>`__ UniBias:通过内部注意力和FFN操纵揭开和减轻LLM偏见

`[2405.20613] FineRadScore: A Radiology Report Line-by-Line Evaluation Technique Generating Corrections with Severity Scores <https://arxiv.org/abs/2405.20613>`__ FineRadScore:一种放射科报告逐行评估技术，根据严重程度评分生成修正

`[2405.20654] Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models <https://arxiv.org/abs/2405.20654>`__ 基于大型语言模型问答中篇章重排序的篇章特定提示调优

`[2405.20657] DORY: Deliberative Prompt Recovery for LLM <https://arxiv.org/abs/2405.20657>`__ 多莉:对LLM的审慎迅速恢复

`[2405.20701] Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement <https://arxiv.org/abs/2405.20701>`__ 揭示LLMs的词法敏感性:快速增强的组合优化

`[2405.20787] PGA-SciRE: Harnessing LLM on Data Augmentation for Enhancing Scientific Relation Extraction <https://arxiv.org/abs/2405.20787>`__ PGA-SciRE:利用LLM增强数据增强科学关系抽取

`[2405.20805] Multilingual Text Style Transfer: Datasets & Models for Indian Languages <https://arxiv.org/abs/2405.20805>`__ 多语言文本风格迁移:印度语言的数据集和模型

`[2405.20830] Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment <https://arxiv.org/abs/2405.20830>`__ 自增强偏好优化:语言模型对齐的非策略范式

`[2405.20833] That's Optional: A Contemporary Exploration of "that" Omission in English Subordinate Clauses <https://arxiv.org/abs/2405.20833>`__ 这是可选的:对英语从句中省略“That”的当代探索

`[2405.20850] Improving Reward Models with Synthetic Critiques <https://arxiv.org/abs/2405.20850>`__ 用综合评论改进奖励模型

`[2405.20900] Large Language Models: A New Approach for Privacy Policy Analysis at Scale <https://arxiv.org/abs/2405.20900>`__ 大型语言模型:一种大规模隐私策略分析的新方法

`[2405.20974] SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales <https://arxiv.org/abs/2405.20974>`__ 自我:教LLMs用自我反思的理由表达自信

`[2405.21028] LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models <https://arxiv.org/abs/2405.21028>`__ LACIE:基于听者感知的大型语言模型置信度校准微调

`[2405.21040] Direct Alignment of Language Models via Quality-Aware Self-Refinement <https://arxiv.org/abs/2405.21040>`__ 基于质量感知自我细化的语言模型直接对齐

`[2405.20835] Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs <https://arxiv.org/abs/2405.20835>`__ 离群点和校准集对现代llm的量化影响越来越小

`[2405.20971] Amortizing intractable inference in diffusion models for vision, language, and control <https://arxiv.org/abs/2405.20971>`__ 视觉、语言和控制扩散模型中的难解推理

`[2405.20973] LCQ: Low-Rank Codebook based Quantization for Large Language Models <https://arxiv.org/abs/2405.20973>`__ LCQ:基于低秩码本的大型语言模型量化

`[2405.21018] Improved Techniques for Optimization-Based Jailbreaking on Large Language Models <https://arxiv.org/abs/2405.21018>`__ 改进的大型语言模型优化越狱技术

`[2405.20389] Designing an Evaluation Framework for Large Language Models in Astronomy Research <https://arxiv.org/abs/2405.20389>`__ 天文学研究中大型语言模型评估框架的设计

`[2405.20434] Facilitating Human-LLM Collaboration through Factuality Scores and Source Attributions <https://arxiv.org/abs/2405.20434>`__ 通过事实性得分和来源归因促进人类- llm协作

`[2405.20450] Decentralized AI: Permissionless LLM Inference on POKT Network <https://arxiv.org/abs/2405.20450>`__ 去中心化人工智能:POKT网络上的无许可LLM推理

`[2405.20681] No Free Lunch Theorem for Privacy-Preserving LLM Inference <https://arxiv.org/abs/2405.20681>`__ 隐私保护LLM推理没有免费的午餐定理

`[2405.20773] Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Characte <https://arxiv.org/abs/2405.20773>`__ visual - role -play:基于角色扮演图像特征的多模态大规模语言模型通用越狱攻击

`[2405.20774] Exploring Backdoor Attacks against Large Language Model-based Decision Making <https://arxiv.org/abs/2405.20774>`__ 探索针对基于大型语言模型决策的后门攻击

`[2405.20775] Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models <https://arxiv.org/abs/2405.20775>`__ 医学多模态大型语言模型的跨模态越狱与错配攻击

`[2405.20797] Ovis: Structural Embedding Alignment for Multimodal Large Language Model <https://arxiv.org/abs/2405.20797>`__ Ovis:多模态大型语言模型的结构嵌入对齐

`[2405.20962] Large Language Models are Zero-Shot Next Location Predictors <https://arxiv.org/abs/2405.20962>`__ 大型语言模型是零样本下一位置预测器

`[2405.20413] Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters <https://arxiv.org/abs/2405.20413>`__ 通过密码字符破解大型语言模型的适度防护护栏

`[2405.20646] Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item <https://arxiv.org/abs/2405.20646>`__ 大型语言模型增强了长尾用户和项目的顺序推荐

`[2405.20684] Joint Embeddings for Graph Instruction Tuning <https://arxiv.org/abs/2405.20684>`__ 基于联合嵌入的图指令调优

`[2405.20551] EM-Assist: Safe Automated ExtractMethod Refactoring with LLMs <https://arxiv.org/abs/2405.20551>`__ EM-Assist:基于LLMs的安全自动提取方法重构

`[2405.20777] Black-Box Detection of Language Model Watermarks <https://arxiv.org/abs/2405.20777>`__ 语言模型水印的黑盒检测

`[2405.20778] Improved Generation of Adversarial Examples Against Safety-aligned LLMs <https://arxiv.org/abs/2405.20778>`__ 改进的针对安全对齐llm的对抗样本生成

`[2402.03962] Position: Stop Making Unscientific AGI Performance Claims <https://arxiv.org/abs/2402.03962>`__ 立场:停止发表不科学的AGI性能声明

`[2402.10705] AutoSAT: Automatically Optimize SAT Solvers via Large Language Models <https://arxiv.org/abs/2402.10705>`__ AutoSAT:基于大型语言模型自动优化SAT求解器

`[2405.18870] LLMs achieve adult human performance on higher-order theory of mind tasks <https://arxiv.org/abs/2405.18870>`__ llm在高阶心智理论任务上实现成人的表现

`[2305.15255] Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM <https://arxiv.org/abs/2305.15255>`__ 语谱图驱动的LLM口语问答和语音延续

`[2312.09085] The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation <https://arxiv.org/abs/2312.09085>`__ 地球是平的，因为……:通过说服性对话调查llm对错误信息的信念

`[2401.00368] Improving Text Embeddings with Large Language Models <https://arxiv.org/abs/2401.00368>`__ 用大型语言模型改进文本嵌入

`[2401.11052] Mining experimental data from Materials Science literature with Large Language Models: an evaluation study <https://arxiv.org/abs/2401.11052>`__ 基于大型语言模型的材料科学文献实验数据挖掘:评估研究

`[2402.12146] Enabling Weak LLMs to Judge Response Reliability via Meta Ranking <https://arxiv.org/abs/2402.12146>`__ 通过元排序使弱llm能够判断响应可靠性

`[2402.15938] Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models <https://arxiv.org/abs/2402.15938>`__ 泛化或记忆:大型语言模型的数据污染和可信评估

`[2404.07611] NoticIA: A Clickbait Article Summarization Dataset in Spanish <https://arxiv.org/abs/2404.07611>`__ NoticIA:西班牙语标题党文章摘要数据集

`[2405.08295] SpeechVerse: A Large-scale Generalizable Audio Language Model <https://arxiv.org/abs/2405.08295>`__ SpeechVerse:大规模可泛化音频语言模型

`[2405.13923] Why Not Transform Chat Large Language Models to Non-English? <https://arxiv.org/abs/2405.13923>`__ 为什么不将聊天大型语言模型转换为非英语语言模型?

`[2405.15032] Aya 23: Open Weight Releases to Further Multilingual Progress <https://arxiv.org/abs/2405.15032>`__ Aya 23: Open Weight释放了进一步的多语言进展

`[2405.19787] From Symbolic Tasks to Code Generation: Diversification Yields Better Task Performers <https://arxiv.org/abs/2405.19787>`__ 从符号任务到代码生成:多样化产生更好的任务执行者

`[2310.02905] Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers <https://arxiv.org/abs/2310.02905>`__ 用你的直觉:使用神经匪结合transformer对llm进行指令优化

`[2402.03299] GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models <https://arxiv.org/abs/2402.03299>`__ GUARD:角色扮演生成自然语言越狱以测试大型语言模型的准则遵循性

`[2402.07043] A Tale of Tails: Model Collapse as a Change of Scaling Laws <https://arxiv.org/abs/2402.07043>`__ 尾巴的故事:模型崩溃作为缩放定律的变化

`[2404.08846] Experimental Design for Active Transductive Inference in Large Language Models <https://arxiv.org/abs/2404.08846>`__ 大型语言模型中主动转导推理的实验设计

`[2404.18239] SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning <https://arxiv.org/abs/2404.18239>`__ 灵魂:解锁LLM忘却的二阶优化力量

`[2405.14622] Calibrated Self-Rewarding Vision Language Models <https://arxiv.org/abs/2405.14622>`__ 校准自奖励视觉语言模型

`[2405.18641] Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning <https://arxiv.org/abs/2405.18641>`__ 针对有害微调的大型语言模型的惰性安全对齐

`[2402.16714] Quantum linear algebra is all you need for Transformer architectures <https://arxiv.org/abs/2402.16714>`__ 量子线性代数是Transformer架构所需要的全部

`[2405.19358] Robustifying Safety-Aligned Large Language Models through Clean Data Curation <https://arxiv.org/abs/2405.19358>`__ 通过干净数据管理对安全对齐的大型语言模型的鲁棒性

`[2405.20081] NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models <https://arxiv.org/abs/2405.20081>`__ NoiseBoost:用噪声扰动缓解多模态大型语言模型的幻觉

`[2405.20319] ParSEL: Parameterized Shape Editing with Language <https://arxiv.org/abs/2405.20319>`__ ParSEL:基于语言的参数化形状编辑

`[2405.19732] Two Optimizers Are Better Than One: LLM Catalyst for Enhancing Gradient-Based Optimization <https://arxiv.org/abs/2405.19732>`__ 两个优化器优于一个:用于增强基于梯度的优化的LLM催化剂

