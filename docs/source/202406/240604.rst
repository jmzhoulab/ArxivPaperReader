240604
========

----------
Survey (8)
----------

`[2406.00252] Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey <https://arxiv.org/abs/2406.00252>`__ 多模态和多智能体系统满足理性:综述

::

    Sat, 1 Jun 2024 01:17:25 GMT
    Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Weijie J. Su, Camillo J. Taylor, Tanwi Mallick

Rationality is the quality of being guided by reason, characterized by logical thinking and decision-making that align with evidence and logical rules. This quality is essential for effective problem-solving, as it ensures that solutions are well-founded and systematically derived. Despite the advancements of large language models (LLMs) in generating human-like text with remarkable accuracy, they present biases inherited from the training data, inconsistency across different contexts, and difficulty understanding complex scenarios involving multiple layers of context. Therefore, recent research attempts to leverage the strength of multiple agents working collaboratively with various types of data and tools for enhanced consistency and reliability.
To that end, this paper aims to understand whether multi-modal and multi-agent systems are advancing toward rationality by surveying the state-of-the-art works, identifying advancements over single-agent and single-modal systems in terms of rationality, and discussing open problems and future directions. We maintain an open repository at https://github.com/bowen-upenn/MMMA_Rationality.

------------

`[2406.00515] A Survey on Large Language Models for Code Generation <https://arxiv.org/abs/2406.00515>`__ 面向代码生成的大型语言模型综述

::

    Sat, 1 Jun 2024 17:48:15 GMT
    Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim

Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, and real-world applications.
In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the widely recognized HumanEval and MBPP benchmarks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development.
Furthermore, we have established a dedicated resource website (https://codellm.github.io) to continuously document and disseminate the most recent advances in the field.

------------

`[2406.00936] A Survey of Useful LLM Evaluation <https://arxiv.org/abs/2406.00936>`__ 有用的LLM评估综述

::

    Mon, 3 Jun 2024 02:20:03 GMT
    Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen

LLMs have gotten attention across various research domains due to their exceptional performance on a wide range of complex tasks. Therefore, refined methods to evaluate the capabilities of LLMs are needed to determine the tasks and responsibility they should undertake. Our study mainly discussed how LLMs, as useful tools, should be effectively assessed. We proposed the two-stage framework: from ``core ability'' to ``agent'', clearly explaining how LLMs can be applied based on their specific capabilities, along with the evaluation methods in each stage. Core ability refers to the capabilities that LLMs need in order to generate high-quality natural language texts. After confirming LLMs possess core ability, they can solve real-world and complex tasks as agent. In the "core ability" stage, we discussed the reasoning ability, societal impact, and domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied action, planning, and tool learning of LLMs agent applications. Finally, we examined the challenges currently confronting the evaluation methods for LLMs, as well as the directions for future development.

------------

`[2406.01171] Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization <https://arxiv.org/abs/2406.01171>`__ llm中的两个人物故事:角色扮演和个性化的调查

::

    Mon, 3 Jun 2024 10:08:23 GMT
    Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Yu-Ching Hsu, Jia-Yin Foo, Chao-Wei Huang, Yun-Nung Chen

Recently, methods investigating how to adapt large language models (LLMs) for specific scenarios have gained great attention. Particularly, the concept of \textit{persona}, originally adopted in dialogue literature, has re-surged as a promising avenue. However, the growing research on persona is relatively disorganized, lacking a systematic overview. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. To the best of our knowledge, we present the first survey tailored for LLM role-playing and LLM personalization under the unified view of persona, including taxonomy, current challenges, and potential directions. To foster future endeavors, we actively maintain a paper collection available to the community: https://github.com/MiuLab/PersonaLLM-Survey

------------

`[2406.01252] Towards Scalable Automated Alignment of LLMs: A Survey <https://arxiv.org/abs/2406.01252>`__ llm可扩展自动对齐综述

::

    Mon, 3 Jun 2024 12:10:26 GMT
    Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, Le Sun, Hongyu Lin, Bowen Yu

Alignment is the most critical step in building large language models (LLMs) that meet human needs. With the rapid development of LLMs gradually surpassing human capabilities, traditional alignment methods based on human-annotation are increasingly unable to meet the scalability demands. Therefore, there is an urgent need to explore new sources of automated alignment signals and technical approaches. In this paper, we systematically review the recently emerging methods of automated alignment, attempting to explore how to achieve effective, scalable, automated alignment once the capabilities of LLMs exceed those of humans. Specifically, we categorize existing automated alignment methods into 4 major categories based on the sources of alignment signals and discuss the current status and potential development of each category. Additionally, we explore the underlying mechanisms that enable automated alignment and discuss the essential factors that make automated alignment technologies feasible and effective from the fundamental role of alignment.

------------

`[2406.01297] When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs <https://arxiv.org/abs/2406.01297>`__ 什么时候llm可以真正纠正自己的错误?llm自我纠正的批判性调查

::

    Mon, 3 Jun 2024 13:05:46 GMT
    Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, Rui Zhang

Self-correction is an approach to improving responses from large language models (LLMs) by refining the responses using LLMs during inference. Prior work has proposed various self-correction frameworks using different sources of feedback, including self-evaluation and external feedback. However, there is still no consensus on the question of when LLMs can correct their own mistakes, as recent studies also report negative results. In this work, we critically survey broad papers and discuss the conditions required for successful self-correction. We first find that prior studies often do not define their research questions in detail and involve impractical frameworks or unfair evaluations that over-evaluate self-correction. To tackle these issues, we categorize research questions in self-correction research and provide a checklist for designing appropriate experiments. Our critical survey based on the newly categorized research questions shows that (1) no prior work demonstrates successful self-correction with feedback from prompted LLMs in general tasks, (2) self-correction works well in tasks that can use reliable external feedback, and (3) large-scale fine-tuning enables self-correction.

------------

`[2406.00240] Exploring Vulnerabilities and Protections in Large Language Models: A Survey <https://arxiv.org/abs/2406.00240>`__ 探索大型语言模型中的漏洞和保护:综述

::

    Sat, 1 Jun 2024 00:11:09 GMT
    Frank Weizhen Liu, Chenhui Hu

As Large Language Models (LLMs) increasingly become key components in various AI applications, understanding their security vulnerabilities and the effectiveness of defense mechanisms is crucial. This survey examines the security challenges of LLMs, focusing on two main areas: Prompt Hacking and Adversarial Attacks, each with specific types of threats. Under Prompt Hacking, we explore Prompt Injection and Jailbreaking Attacks, discussing how they work, their potential impacts, and ways to mitigate them. Similarly, we analyze Adversarial Attacks, breaking them down into Data Poisoning Attacks and Backdoor Attacks. This structured examination helps us understand the relationships between these vulnerabilities and the defense strategies that can be implemented. The survey highlights these security challenges and discusses robust defensive frameworks to protect LLMs against these threats. By detailing these security issues, the survey contributes to the broader discussion on creating resilient AI systems that can resist sophisticated attacks.

------------

`[2404.14387] A Survey on Self-Evolution of Large Language Models <https://arxiv.org/abs/2404.14387>`__ 大型语言模型自演化研究综述

::

    replaced with revised version Mon, 3 Jun 2024 17:47:30 GMT
    Submission history From: Ting-En Lin [view email]
    [v1] Mon, 22 Apr 2024 17:43:23 UTC (3,858 KB)
    [v2] Mon, 3 Jun 2024 17:47:30 UTC (1,545 KB)
    Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, Jingren Zhou

Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs. Our corresponding GitHub repository is available at this https URL

------------

--------------
Benchmark (11)
--------------

`[2406.01126] TCMBench: A Comprehensive Benchmark for Evaluating Large Language Models in Traditional Chinese Medicine <https://arxiv.org/abs/2406.01126>`__ 

::

    Mon, 3 Jun 2024 09:11:13 GMT
    Wenjing Yue and Xiaoling Wang and Wei Zhu and Ming Guan and Huanran Zheng and Pengfei Wang and Changzhi Sun and Xin Ma

Large language models (LLMs) have performed remarkably well in various natural language processing tasks by benchmarking, including in the Western medical domain. However, the professional evaluation benchmarks for LLMs have yet to be covered in the traditional Chinese medicine(TCM) domain, which has a profound history and vast influence. To address this research gap, we introduce TCM-Bench, an comprehensive benchmark for evaluating LLM performance in TCM. It comprises the TCM-ED dataset, consisting of 5,473 questions sourced from the TCM Licensing Exam (TCMLE), including 1,300 questions with authoritative analysis. It covers the core components of TCMLE, including TCM basis and clinical practice. To evaluate LLMs beyond accuracy of question answering, we propose TCMScore, a metric tailored for evaluating the quality of answers generated by LLMs for TCM related questions. It comprehensively considers the consistency of TCM semantics and knowledge. After conducting comprehensive experimental analyses from diverse perspectives, we can obtain the following findings: (1) The unsatisfactory performance of LLMs on this benchmark underscores their significant room for improvement in TCM. (2) Introducing domain knowledge can enhance LLMs' performance. However, for in-domain models like ZhongJing-TCM, the quality of generated analysis text has decreased, and we hypothesize that their fine-tuning process affects the basic LLM capabilities. (3) Traditional metrics for text generation quality like Rouge and BertScore are susceptible to text length and surface semantic ambiguity, while domain-specific metrics such as TCMScore can further supplement and explain their evaluation results. These findings highlight the capabilities and limitations of LLMs in the TCM and aim to provide a more profound assistance to medical research.

------------

`[2406.01359] R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models <https://arxiv.org/abs/2406.01359>`__ R2C2-Coder:代码大型语言模型的真实库级代码补全能力增强和基准测试

::

    Mon, 3 Jun 2024 14:24:29 GMT
    Ken Deng, Jiaheng Liu, He Zhu, Congnan Liu, Jingxin Li, Jiakai Wang, Peng Zhao, Chenchen Zhang, Yanan Wu, Xueqiao Yin, Yuanxing Zhang, Wenbo Su, Bangyu Xiang, Tiezheng Ge, Bo Zheng

Code completion models have made significant progress in recent years.
Recently, repository-level code completion has drawn more attention in modern software development, and several baseline methods and benchmarks have been proposed. However, existing repository-level code completion methods often fall short of fully using the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies. Besides, the existing benchmarks usually focus on limited code completion scenarios, which cannot reflect the repository-level code completion abilities well of existing methods. To address these limitations, we propose the R2C2-Coder to enhance and benchmark the real-world repository-level code completion abilities of code Large Language Models, where the R2C2-Coder includes a code prompt construction method R2C2-Enhance and a well-designed benchmark R2C2-Bench. Specifically, first, in R2C2-Enhance, we first construct the candidate retrieval pool and then assemble the completion prompt by retrieving from the retrieval pool for each completion cursor position. Second, based on R2C2 -Enhance, we can construct a more challenging and diverse R2C2-Bench with training, validation and test splits, where a context perturbation strategy is proposed to simulate the real-world repository-level code completion well. Extensive results on multiple benchmarks demonstrate the effectiveness of our R2C2-Coder.

------------

`[2406.00583] CMDBench: A Benchmark for Coarse-to-fine Multimodal Data Discovery in Compound AI Systems <https://arxiv.org/abs/2406.00583>`__ CMDBench:复合人工智能系统中从粗到细的多模态数据发现基准

::

    Sun, 2 Jun 2024 01:10:41 GMT
    Yanlin Feng and Sajjadur Rahman and Aaron Feng and Vincent Chen and Eser Kandogan

Compound AI systems (CASs) that employ LLMs as agents to accomplish knowledge-intensive tasks via interactions with tools and data retrievers have garnered significant interest within database and AI communities. While these systems have the potential to supplement typical analysis workflows of data analysts in enterprise data platforms, unfortunately, CASs are subject to the same data discovery challenges that analysts have encountered over the years -- silos of multimodal data sources, created across teams and departments within an organization, make it difficult to identify appropriate data sources for accomplishing the task at hand. Existing data discovery benchmarks do not model such multimodality and multiplicity of data sources. Moreover, benchmarks of CASs prioritize only evaluating end-to-end task performance. To catalyze research on evaluating the data discovery performance of multimodal data retrievers in CASs within a real-world setting, we propose CMDBench, a benchmark modeling the complexity of enterprise data platforms. We adapt existing datasets and benchmarks in open-domain -- from question answering and complex reasoning tasks to natural language querying over structured data -- to evaluate coarse- and fine-grained data discovery and task execution performance. Our experiments reveal the impact of data retriever design on downstream task performance -- a 46% drop in task accuracy on average -- across various modalities, data sources, and task difficulty. The results indicate the need to develop optimization strategies to identify appropriate LLM agents and retrievers for efficient execution of CASs over enterprise data.

------------

`[2406.01364] BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards <https://arxiv.org/abs/2406.01364>`__ bell:评估LLM保障的未来证明基准框架

::

    Mon, 3 Jun 2024 14:32:30 GMT
    Diego Dorn, Alexandre Variengien, Charbel-Rapha\"el Segerie, Vincent Corruble

Input-output safeguards are used to detect anomalies in the traces produced by Large Language Models (LLMs) systems. These detectors are at the core of diverse safety-critical applications such as real-time monitoring, offline evaluation of traces, and content moderation. However, there is no widely recognized methodology to evaluate them. To fill this gap, we introduce the Benchmarks for the Evaluation of LLM Safeguards (BELLS), a structured collection of tests, organized into three categories: (1) established failure tests, based on already-existing benchmarks for well-defined failure modes, aiming to compare the performance of current input-output safeguards; (2) emerging failure tests, to measure generalization to never-seen-before failure modes and encourage the development of more general safeguards; (3) next-gen architecture tests, for more complex scaffolding (such as LLM-agents and multi-agent systems), aiming to foster the development of safeguards that could adapt to future applications for which no safeguard currently exists.
Furthermore, we implement and share the first next-gen architecture test, using the MACHIAVELLI environment, along with an interactive visualization of the dataset.

------------

`[2402.06044] OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models <https://arxiv.org/abs/2402.06044>`__ OpenToM:评估大型语言模型心智理论推理能力的综合基准

::

    replaced with revised version Mon, 3 Jun 2024 10:48:16 GMT
    Submission history From: Hainiu Xu [view email]
    [v1] Thu, 8 Feb 2024 20:35:06 UTC (4,577 KB)
    [v2] Wed, 14 Feb 2024 13:23:51 UTC (4,559 KB)
    [v3] Mon, 3 Jun 2024 10:48:16 UTC (4,561 KB)
    Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, Yulan He

Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.

------------

`[2402.07688] CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation for Evaluating LLMs in Cybersecurity Knowledge <https://arxiv.org/abs/2402.07688>`__ CyberMetric:基于检索增强生成的用于评估网络安全知识LLMs的基准数据集

::

    replaced with revised version Mon, 3 Jun 2024 08:14:45 GMT
    Submission history From: Norbert Tihanyi Dr. [view email]
    [v1] Mon, 12 Feb 2024 14:53:28 UTC (338 KB)
    [v2] Mon, 3 Jun 2024 08:14:45 UTC (229 KB)
    Norbert Tihanyi, Mohamed Amine Ferrag, Ridhi Jain, Tamas Bisztray, Merouane Debbah

Large Language Models (LLMs) are increasingly used across various domains, from software development to cyber threat intelligence. Understanding all the different fields of cybersecurity, which includes topics such as cryptography, reverse engineering, and risk assessment, poses a challenge even for human experts. To accurately test the general knowledge of LLMs in cybersecurity, the research community needs a diverse, accurate, and up-to-date dataset. To address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000, and CyberMetric-10000, which are multiple-choice Q&A benchmark datasets comprising 80, 500, 2000, and 10,000 questions respectively. By utilizing GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents, including NIST standards, research papers, publicly accessible books, RFCs, and other publications in the cybersecurity domain, to generate questions, each with four possible answers. The results underwent several rounds of error checking and refinement. Human experts invested over 200 hours validating the questions and solutions to ensure their accuracy and relevance, and to filter out any questions unrelated to cybersecurity. We have evaluated and compared 25 state-of-the-art LLM models on the CyberMetric datasets. In addition to our primary goal of evaluating LLMs, we involved 30 human participants to solve CyberMetric-80 in a closed-book scenario. The results can serve as a reference for comparing the general cybersecurity knowledge of humans and LLMs. The findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct, Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs. Additionally, the top LLMs were more accurate than humans on CyberMetric-80, although highly experienced human experts still outperformed small models such as Llama-3-8B, Phi-2 or Gemma-7b.

------------

`[2401.17167] Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios <https://arxiv.org/abs/2401.17167>`__ 规划、创建、使用:对现实世界复杂场景中全面工具使用的llm进行基准测试

::

    replaced with revised version Mon, 3 Jun 2024 11:28:29 GMT
    Submission history From: Shijue Huang [view email]
    [v1] Tue, 30 Jan 2024 16:52:56 UTC (2,996 KB)
    [v2] Fri, 16 Feb 2024 15:55:34 UTC (3,628 KB)
    [v3] Mon, 3 Jun 2024 11:28:29 UTC (3,659 KB)
    Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng Xu, Qun Liu

The recent trend of using Large Language Models (LLMs) as tool agents in real-world applications underscores the necessity for comprehensive evaluations of their capabilities, particularly in complex scenarios involving planning, creating, and using tools. However, existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization. To address this issue, we present UltraTool, a novel benchmark designed to improve and evaluate LLMs' ability in tool utilization within real-world scenarios. UltraTool focuses on the entire process of using tools - from planning and creating to applying them in complex tasks. It emphasizes real-world complexities, demanding accurate, multi-step planning for effective problem-solving. A key feature of UltraTool is its independent evaluation of planning with natural language, which happens before tool usage and simplifies the task solving by mapping out the intermediate steps. Thus, unlike previous work, it eliminates the restriction of pre-defined toolset. Through extensive experiments on various LLMs, we offer novel insights into the evaluation of capabilities of LLMs in tool utilization, thereby contributing a fresh perspective to this rapidly evolving field. The benchmark is publicly available at this https URL.

------------

`[2402.14809] CriticBench: Benchmarking LLMs for Critique-Correct Reasoning <https://arxiv.org/abs/2402.14809>`__ CriticBench:评价正确推理的llm基准测试

::

    replaced with revised version Sat, 1 Jun 2024 07:46:28 GMT
    Submission history From: Zicheng Lin [view email]
    [v1] Thu, 22 Feb 2024 18:59:02 UTC (1,194 KB)
    [v2] Fri, 8 Mar 2024 15:15:47 UTC (1,187 KB)
    [v3] Tue, 28 May 2024 14:33:27 UTC (1,281 KB)
    [v4] Sat, 1 Jun 2024 07:46:28 UTC (1,307 KB)
    Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang

The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement.

------------

`[2405.12063] CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models <https://arxiv.org/abs/2405.12063>`__ CLAMBER:识别和澄清大型语言模型中模糊信息需求的基准

::

    replaced with revised version Sat, 1 Jun 2024 07:35:26 GMT
    Submission history From: Chen Huang [view email]
    [v1] Mon, 20 May 2024 14:34:01 UTC (820 KB)
    [v2] Sat, 1 Jun 2024 07:35:26 UTC (823 KB)
    Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, Tat-Seng Chua

Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct ~12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs. Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge. In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs. Our dataset is available at this https URL

------------

`[2310.12815] Formalizing and Benchmarking Prompt Injection Attacks and Defenses <https://arxiv.org/abs/2310.12815>`__ 对提示注入攻击和防御进行形式化和基准测试

::

    replaced with revised version Sat, 1 Jun 2024 21:21:07 GMT
    Submission history From: Yupei Liu [view email]
    [v1] Thu, 19 Oct 2023 15:12:09 UTC (3,941 KB)
    [v2] Thu, 30 May 2024 17:09:56 UTC (8,303 KB)
    [v3] Sat, 1 Jun 2024 21:21:07 UTC (8,303 KB)
    Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong

A prompt injection attack aims to inject malicious instruction/data into the input of an LLM-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 LLMs and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses. To facilitate research on this topic, we make our platform public at this https URL.

------------

`[2401.07529] MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception <https://arxiv.org/abs/2401.07529>`__ MM-SAP:评估多模态大型语言模型感知自我意识的综合基准

::

    replaced with revised version Sat, 1 Jun 2024 06:14:37 GMT
    Submission history From: Yuhao Wang [view email]
    [v1] Mon, 15 Jan 2024 08:19:22 UTC (2,762 KB)
    [v2] Mon, 26 Feb 2024 09:28:34 UTC (3,074 KB)
    [v3] Sat, 1 Jun 2024 06:14:37 UTC (6,275 KB)
    Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yu Wang, Yanfeng Wang

Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models' struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been overlooked in prior studies. In this paper, we aim to define and evaluate the self-awareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of popular MLLMs, offering a comprehensive analysis of their self-awareness and providing detailed insights. The experiment results reveal that current MLLMs possess limited self-awareness capabilities, pointing to a crucial area for future advancement in the development of trustworthy MLLMs. Code and data are available at this https URL.

------------

---------------
Accelerate (17)
---------------

`[2406.00023] LocMoE+: Enhanced Router with Token Feature Awareness for Efficient LLM Pre-Training <https://arxiv.org/abs/2406.00023>`__ LocMoE+:具有Token特征感知的增强路由器，用于高效的LLM预训练

::

    Fri, 24 May 2024 02:50:44 GMT
    Jing Li, Zhijie Sun, Dachao Lin, Xuan He, Yi Lin, Binfan Zheng, Li Zeng, Rongqian Zhao, Xin Chen

Mixture-of-Experts (MoE) architectures have recently gained increasing popularity within the domain of large language models (LLMs) due to their ability to significantly reduce training and inference overhead. However, MoE architectures face challenges, such as significant disparities in the number of tokens assigned to each expert and a tendency toward homogenization among experts, which adversely affects the model's semantic generation capabilities.
In this paper, we introduce LocMoE+, a refined version of the low-overhead LocMoE, incorporating the following enhancements: (1) Quantification and definition of the affinity between experts and tokens. (2) Implementation of a global-level adaptive routing strategy to rearrange tokens based on their affinity scores. (3) Reestimation of the lower bound for expert capacity, which has been shown to progressively decrease as the token feature distribution evolves. Experimental results demonstrate that, without compromising model convergence or efficacy, the number of tokens each expert processes can be reduced by over 60%. Combined with communication optimizations, this leads to an average improvement in training efficiency ranging from 5.4% to 46.6%. After fine-tuning, LocMoE+ exhibits a performance improvement of 9.7% to 14.1% across the GDAD, C-Eval, and TeleQnA datasets.

------------

`[2406.00059] Conveyor: Efficient Tool-aware LLM Serving with Tool Partial Execution <https://arxiv.org/abs/2406.00059>`__ 输送机:工具部分执行的高效工具感知LLM

::

    Wed, 29 May 2024 21:24:15 GMT
    Yechen Xu, Xinhao Kong, Tingjun Chen, Danyang Zhuo

The complexity of large language model (LLM) serving workloads has substantially increased due to the integration with external tool invocations, such as ChatGPT plugins. In this paper, we identify a new opportunity for efficient LLM serving for requests that trigger tools: tool partial execution alongside LLM decoding. To this end, we design Conveyor, an efficient LLM serving system optimized for handling requests involving external tools. We introduce a novel interface for tool developers to expose partial execution opportunities to the LLM serving system and a request scheduler that facilitates partial tool execution. Our results demonstrate that tool partial execution can improve request completion latency by up to 38.8%.

------------

`[2406.00605] LongSkywork: A Training Recipe for Efficiently Extending Context Length in Large Language Models <https://arxiv.org/abs/2406.00605>`__ LongSkywork:在大型语言模型中有效扩展上下文长度的训练秘诀

::

    Sun, 2 Jun 2024 03:34:41 GMT
    Liang Zhao, Tianwen Wei, Liang Zeng, Cheng Cheng, Liu Yang, Peng Cheng, Lijie Wang, Chenxia Li, Xuejie Wu, Bo Zhu, Yimeng Gan, Rui Hu, Shuicheng Yan, Han Fang, Yahui Zhou

We introduce LongSkywork, a long-context Large Language Model (LLM) capable of processing up to 200,000 tokens. We provide a training recipe for efficiently extending context length of LLMs. We identify that the critical element in enhancing long-context processing capability is to incorporate a long-context SFT stage following the standard SFT stage. A mere 200 iterations can convert the standard SFT model into a long-context model. To reduce the effort in collecting and annotating data for long-context language modeling, we develop two novel methods for creating synthetic data. These methods are applied during the continual pretraining phase as well as the Supervised Fine-Tuning (SFT) phase, greatly enhancing the training efficiency of our long-context LLMs. Our findings suggest that synthetic long-context SFT data can surpass the performance of data curated by humans to some extent.
LongSkywork achieves outstanding performance on a variety of long-context benchmarks. In the Needle test, a benchmark for long-context information retrieval, our models achieved perfect accuracy across multiple context spans.
Moreover, in realistic application scenarios, LongSkywork-13B demonstrates performance on par with Claude2.1, the leading long-context model, underscoring the effectiveness of our proposed methods.

------------

`[2406.01238] EffiQA: Efficient Question-Answering with Strategic Multi-Model Collaboration on Knowledge Graphs <https://arxiv.org/abs/2406.01238>`__ EffiQA:知识图谱上多模型策略性协作的高效问答

::

    Mon, 3 Jun 2024 11:56:07 GMT
    Zixuan Dong, Baoyun Peng, Yufei Wang, Jia Fu, Xiaodong Wang, Yongxue Shan, Xin Zhou

While large language models (LLMs) have shown remarkable capabilities in natural language processing, they struggle with complex, multi-step reasoning tasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs and KGs either underutilize the reasoning abilities of LLMs or suffer from prohibitive computational costs due to tight coupling. To address these limitations, we propose a novel collaborative framework named EffiQA that can strike a balance between performance and efficiency via an iterative paradigm.
EffiQA consists of three stages: global planning, efficient KG exploration, and self-reflection. Specifically, EffiQA leverages the commonsense capability of LLMs to explore potential reasoning pathways through global planning. Then, it offloads semantic pruning to a small plug-in model for efficient KG exploration. Finally, the exploration results are fed to LLMs for self-reflection to further improve the global planning and efficient KG exploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's effectiveness, achieving an optimal balance between reasoning accuracy and computational costs. We hope the proposed new framework will pave the way for efficient, knowledge-intensive querying by redefining the integration of LLMs and KGs, fostering future research on knowledge-based question answering.

------------

`[2406.01306] Unsupervised Distractor Generation via Large Language Model Distilling and Counterfactual Contrastive Decoding <https://arxiv.org/abs/2406.01306>`__ 基于大型语言模型提取和反事实对比解码的无监督干扰项生成

::

    Mon, 3 Jun 2024 13:20:05 GMT
    Fanyi Qu, Hao Sun, Yunfang Wu

Within the context of reading comprehension, the task of Distractor Generation (DG) aims to generate several incorrect options to confuse readers.
Traditional supervised methods for DG rely heavily on expensive human-annotated distractor labels. In this paper, we propose an unsupervised DG framework, leveraging Large Language Models (LLMs) as cost-effective annotators to enhance the DG capability of smaller student models. Specially, to perform knowledge distilling, we propose a dual task training strategy that integrates pseudo distractors from LLMs and the original answer in-formation as the objective targets with a two-stage training process. Moreover, we devise a counterfactual contrastive decoding mechanism for increasing the distracting capability of the DG model. Experiments show that our unsupervised generation method with Bart-base greatly surpasses GPT-3.5-turbo performance with only 200 times fewer model parameters. Our proposed unsupervised DG method offers a cost-effective framework for practical reading comprehension applications, without the need of laborious distractor annotation and costly large-size models

------------

`[2406.01392] Sparsity-Accelerated Training for Large Language Models <https://arxiv.org/abs/2406.01392>`__ 大型语言模型的稀疏加速训练

::

    Mon, 3 Jun 2024 14:56:09 GMT
    Da Ma and Lu Chen and Pengyu Wang and Hongshen Xu and Hanqi Li and Liangtai Sun and Su Zhu and Shuai Fan and Kai Yu

Large language models (LLMs) have demonstrated proficiency across various natural language processing (NLP) tasks but often require additional training, such as continual pre-training and supervised fine-tuning. However, the costs associated with this, primarily due to their large parameter count, remain high. This paper proposes leveraging \emph{sparsity} in pre-trained LLMs to expedite this training process. By observing sparsity in activated neurons during forward iterations, we identify the potential for computational speed-ups by excluding inactive neurons. We address associated challenges by extending existing neuron importance evaluation metrics and introducing a ladder omission rate scheduler. Our experiments on Llama-2 demonstrate that Sparsity-Accelerated Training (SAT) achieves comparable or superior performance to standard training while significantly accelerating the process.
Specifically, SAT achieves a $45\%$ throughput improvement in continual pre-training and saves $38\%$ training time in supervised fine-tuning in practice. It offers a simple, hardware-agnostic, and easily deployable framework for additional LLM training. Our code is available at https://github.com/OpenDFM/SAT.

------------

`[2406.00132] QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation <https://arxiv.org/abs/2406.00132>`__ QuanTA:基于量子信息张量自适应的llm高效高秩微调

::

    Fri, 31 May 2024 18:47:30 GMT
    Zhuo Chen, Rumen Dangovski, Charlotte Loh, Owen Dugan, Di Luo, Marin Solja\v{c}i\'c

We propose Quantum-informed Tensor Adaptation (QuanTA), a novel, easy-to-implement, fine-tuning method with no inference overhead for large-scale pre-trained language models. By leveraging quantum-inspired methods derived from quantum circuit structures, QuanTA enables efficient high-rank fine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank approximation may fail for complicated downstream tasks. Our approach is theoretically supported by the universality theorem and the rank representation theorem to achieve efficient high-rank adaptations. Experiments demonstrate that QuanTA significantly enhances commonsense reasoning, arithmetic reasoning, and scalability compared to traditional methods. Furthermore, QuanTA shows superior performance with fewer trainable parameters compared to other approaches and can be designed to integrate with existing fine-tuning algorithms for further improvement, providing a scalable and efficient solution for fine-tuning large language models and advancing state-of-the-art in natural language processing.

------------

`[2406.00965] Efficient Behavior Tree Planning with Commonsense Pruning and Heuristic <https://arxiv.org/abs/2406.00965>`__ 基于常识剪枝和启发式的高效行为树规划

::

    Mon, 3 Jun 2024 03:38:56 GMT
    Xinglin Chen, Yishuai Cai, Yunxin Mao, Minglong Li, Zhou Yang, Wen Shanghua, Wenjing Yang, Weixia Xu, Ji Wang

Behavior Tree (BT) planning is crucial for autonomous robot behavior control, yet its application in complex scenarios is hampered by long planning times.
Pruning and heuristics are common techniques to accelerate planning, but it is difficult to design general pruning strategies and heuristic functions for BT planning problems. This paper proposes improving BT planning efficiency for everyday service robots leveraging commonsense reasoning provided by Large Language Models (LLMs), leading to model-free pre-planning action space pruning and heuristic generation. This approach takes advantage of the modularity and interpretability of BT nodes, represented by predicate logic, to enable LLMs to predict the task-relevant action predicates and objects, and even the optimal path, without an explicit action model. We propose the Heuristic Optimal Behavior Tree Expansion Algorithm (HOBTEA) with two heuristic variants and provide a formal comparison and discussion of their efficiency and optimality.
We introduce a learnable and transferable commonsense library to enhance the LLM's reasoning performance without fine-tuning. The action space expansion based on the commonsense library can further increase the success rate of planning. Experiments show the theoretical bounds of commonsense pruning and heuristic, and demonstrate the actual performance of LLM learning and reasoning with the commonsense library. Results in four datasets showcase the practical effectiveness of our approach in everyday service robot applications.

------------

`[2402.02416] Aligner: Efficient Alignment by Learning to Correct <https://arxiv.org/abs/2402.02416>`__ Aligner:通过学习纠正有效的对齐

::

    replaced with revised version Mon, 3 Jun 2024 14:33:45 GMT
    Submission history From: Jiaming Ji [view email]
    [v1] Sun, 4 Feb 2024 09:24:51 UTC (2,207 KB)
    [v2] Tue, 6 Feb 2024 18:02:01 UTC (2,206 KB)
    [v3] Mon, 3 Jun 2024 14:33:45 UTC (2,808 KB)
    Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, Tianyi Qiu, Yaodong Yang

With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-and-play module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model's performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of 68.9\% in helpfulness and 23.8\% in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0\% to 58.3\%, surpassing GPT-4 Omni's 57.5\% Win Rate (community report).

------------

`[2402.11896] SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning <https://arxiv.org/abs/2402.11896>`__ SIBO:参数高效微调的简单助推器

::

    replaced with revised version Sat, 1 Jun 2024 07:13:15 GMT
    Submission history From: Zhihao Wen [view email]
    [v1] Mon, 19 Feb 2024 07:22:29 UTC (7,404 KB)
    [v2] Sat, 1 Jun 2024 07:13:15 UTC (8,322 KB)
    Zhihao Wen, Jie Zhang, Yuan Fang

Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time. Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs. Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straightforward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT methods on the arithmetic and commonsense reasoning tasks, respectively.

------------

`[2403.15447] Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression <https://arxiv.org/abs/2403.15447>`__ 压缩信任解码:高效llm在压缩下的可信性审查

::

    replaced with revised version Mon, 3 Jun 2024 14:49:00 GMT
    Submission history From: Junyuan Hong [view email]
    [v1] Mon, 18 Mar 2024 01:38:19 UTC (1,676 KB)
    [v2] Mon, 3 Jun 2024 14:49:00 UTC (1,676 KB)
    Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian Bartoldson, Ajay Jaiswal, Kaidi Xu, Bhavya Kailkhura, Dan Hendrycks, Dawn Song, Zhangyang Wang, Bo Li

Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of three (3) leading LLMs using five (5) SoTA compression techniques across eight (8) trustworthiness dimensions. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% sparsity. Moreover, employing quantization within a moderate bit range could unexpectedly improve certain trustworthiness dimensions such as ethics and fairness. Conversely, extreme quantization to very low bit levels (3 bits) tends to reduce trustworthiness significantly. This increased risk cannot be uncovered by looking at benign performance alone, in turn, mandating comprehensive trustworthiness evaluation in practice. These findings culminate in practical recommendations for simultaneously achieving high utility, efficiency, and trustworthiness in LLMs. Code and models are available at this https URL.

------------

`[2405.14259] Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Multi-modal Text Recognition <https://arxiv.org/abs/2405.14259>`__ 让我们一步步融合:基于LLMs的生成式融合解码算法用于多模态文本识别

::

    replaced with revised version Sun, 2 Jun 2024 16:30:00 GMT
    Submission history From: Chan-Jan Hsu [view email]
    [v1] Thu, 23 May 2024 07:39:42 UTC (285 KB)
    [v2] Tue, 28 May 2024 14:45:30 UTC (282 KB)
    [v3] Sun, 2 Jun 2024 16:30:00 UTC (282 KB)
    Chan-Jan Hsu, Yi-Chang Chen, Feng-Ting Liao, Pei-Chen Ho, Yu-Hsiang Wang, Po-Chun Hsu, Da-shan Shiu

We introduce "Generative Fusion Decoding" (GFD), a novel shallow fusion framework, utilized to integrate Large Language Models (LLMs) into multi-modal text recognition systems such as automatic speech recognition (ASR) and optical character recognition (OCR). We derive the formulas necessary to enable GFD to operate across mismatched token spaces of different models by mapping text token space to byte token space, enabling seamless fusion during the decoding process. The framework is plug-and-play, compatible with various auto-regressive models, and does not require re-training for feature alignment, thus overcoming limitations of previous fusion techniques. We highlight three main advantages of GFD: First, by simplifying the complexity of aligning different model sample spaces, GFD allows LLMs to correct errors in tandem with the recognition model, reducing computation latencies. Second, the in-context learning ability of LLMs is fully capitalized by GFD, increasing robustness in long-form speech recognition and instruction aware speech recognition. Third, GFD enables fusing recognition models deficient in Chinese text recognition with LLMs extensively trained on Chinese. Our evaluation demonstrates that GFD significantly improves performance in ASR and OCR tasks, with ASR reaching state-of-the-art in the NTUML2021 benchmark. GFD provides a significant step forward in model integration, offering a unified solution that could be widely applicable to leveraging existing pre-trained models through step by step fusion.

------------

`[2405.20314] S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs <https://arxiv.org/abs/2405.20314>`__ S3D:一种简单且低成本的低内存gpu自推测解码方案

::

    replaced with revised version Sat, 1 Jun 2024 15:24:10 GMT
    Submission history From: Wei Zhong [view email]
    [v1] Thu, 30 May 2024 17:54:35 UTC (755 KB)
    [v2] Sat, 1 Jun 2024 15:24:10 UTC (755 KB)
    Wei Zhong and Manasa Bharadwaj

Speculative decoding (SD) has attracted a significant amount of research attention due to the substantial speedup it can achieve for LLM inference. However, despite the high speedups they offer, speculative decoding methods often achieve optimal performance on high-end devices or with a substantial GPU memory overhead. Given limited memory and the necessity of quantization, a high-performing model on a high-end GPU can slow down by up to 7 times. To this end, we propose Skippy Simultaneous Speculative Decoding (or S3D), a cost-effective self-speculative SD method based on simultaneous multi-token decoding and mid-layer skipping. When compared against recent effective open-source SD systems, our method has achieved one of the top performance-memory ratios while requiring minimal architecture changes and training data. Leveraging our memory efficiency, we created a smaller yet more effective SD model based on Phi-3. It is 1.4 to 2 times faster than the quantized EAGLE model and operates in half-precision while using less VRAM.

------------

`[2403.03507] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection <https://arxiv.org/abs/2403.03507>`__ GaLore:基于梯度低秩投影的高效内存LLM训练

::

    replaced with revised version Sun, 2 Jun 2024 21:24:12 GMT
    Submission history From: Jiawei Zhao [view email]
    [v1] Wed, 6 Mar 2024 07:29:57 UTC (325 KB)
    [v2] Sun, 2 Jun 2024 21:24:12 UTC (365 KB)
    Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, Yuandong Tian

Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.

------------

`[2405.18628] Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference <https://arxiv.org/abs/2405.18628>`__ 基于硬件感知并行提示解码的高效内存加速LLM推理

::

    replaced with revised version Sun, 2 Jun 2024 14:58:48 GMT
    Submission history From: Hao Mark Chen [view email]
    [v1] Tue, 28 May 2024 22:19:30 UTC (3,159 KB)
    [v2] Sun, 2 Jun 2024 14:58:48 UTC (3,155 KB)
    Hao Mark Chen, Wayne Luk, Ka Fai Cedric Yiu, Rui Li, Konstantin Mishchenko, Stylianos I. Venieris, Hongxiang Fan

The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\times$ further speed improvement. Our code is available at this https URL.

------------

`[2401.02051] Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model <https://arxiv.org/abs/2401.02051>`__ 启发式进化:基于大型语言模型的高效自动算法设计

::

    replaced with revised version Sat, 1 Jun 2024 16:48:37 GMT
    Submission history From: Fei Liu [view email]
    [v1] Thu, 4 Jan 2024 04:11:59 UTC (946 KB)
    [v2] Wed, 1 May 2024 12:33:37 UTC (1,041 KB)
    [v3] Sat, 1 Jun 2024 16:48:37 UTC (498 KB)
    Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, Qingfu Zhang

Heuristics are widely used for dealing with complex search and optimization problems. However, manual design of heuristics can be often very labour extensive and requires rich working experience and knowledge. This paper proposes Evolution of Heuristic (EoH), a novel evolutionary paradigm that leverages both Large Language Models (LLMs) and Evolutionary Computation (EC) methods for Automatic Heuristic Design (AHD). EoH represents the ideas of heuristics in natural language, termed thoughts. They are then translated into executable codes by LLMs. The evolution of both thoughts and codes in an evolutionary search framework makes it very effective and efficient for generating high-performance heuristics. Experiments on three widely studied combinatorial optimization benchmark problems demonstrate that EoH outperforms commonly used handcrafted heuristics and other recent AHD methods including FunSearch. Particularly, the heuristic produced by EoH with a low computational budget (in terms of the number of queries to LLMs) significantly outperforms widely-used human hand-crafted baseline algorithms for the online bin packing problem.

------------

`[2404.11343] Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System <https://arxiv.org/abs/2404.11343>`__ 大型语言模型满足协同过滤:一个高效的全方位基于llm的推荐系统

::

    replaced with revised version Sat, 1 Jun 2024 07:08:49 GMT
    Submission history From: Sein Kim [view email]
    [v1] Wed, 17 Apr 2024 13:03:07 UTC (1,054 KB)
    [v2] Sat, 1 Jun 2024 07:08:49 UTC (1,054 KB)
    Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Minchul Yang, Chanyoung Park

Collaborative filtering recommender systems (CF-RecSys) have shown successive results in enhancing the user experience on social media and e-commerce platforms. However, as CF-RecSys struggles under cold scenarios with sparse user-item interactions, recent strategies have focused on leveraging modality information of user/items (e.g., text or images) based on pre-trained modality encoders and Large Language Models (LLMs). Despite their effectiveness under cold scenarios, we observe that they underperform simple traditional collaborative filtering models under warm scenarios due to the lack of collaborative knowledge. In this work, we propose an efficient All-round LLM-based Recommender system, called A-LLMRec, that excels not only in the cold scenario but also in the warm scenario. Our main idea is to enable an LLM to directly leverage the collaborative knowledge contained in a pre-trained state-of-the-art CF-RecSys so that the emergent ability of the LLM as well as the high-quality user/item embeddings that are already trained by the state-of-the-art CF-RecSys can be jointly exploited. This approach yields two advantages: (1) model-agnostic, allowing for integration with various existing CF-RecSys, and (2) efficiency, eliminating the extensive fine-tuning typically required for LLM-based recommenders. Our extensive experiments on various real-world datasets demonstrate the superiority of A-LLMRec in various scenarios, including cold/warm, few-shot, cold user, and cross-domain scenarios. Beyond the recommendation task, we also show the potential of A-LLMRec in generating natural language outputs based on the understanding of the collaborative knowledge by performing a favorite genre prediction task. Our code is available at this https URL .

------------

-----------------------
In-Context Learning (6)
-----------------------

`[2406.01224] Demonstration Augmentation for Zero-shot In-context Learning <https://arxiv.org/abs/2406.01224>`__ 零样本上下文学习的演示增强

::

    Mon, 3 Jun 2024 11:46:42 GMT
    Yi Su, Yunpeng Tai, Yixin Ji, Juntao Li, Bowen Yan, Min Zhang

Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies have highlighted that the model's performance is sensitive to the choice of demonstrations, presenting a significant challenge for practical applications where we lack prior knowledge of user queries. Consequently, we need to construct an extensive demonstration pool and incorporate external databases to assist the model, leading to considerable time and financial costs. In light of this, some recent research has shifted focus towards zero-shot ICL, aiming to reduce the model's reliance on external information by leveraging their inherent generative capabilities. Despite the effectiveness of these approaches, the content generated by the model may be unreliable, and the generation process is time-consuming. To address these issues, we propose Demonstration Augmentation for In-context Learning (DAIL), which employs the model's previously predicted historical samples as demonstrations for subsequent ones. DAIL brings no additional inference cost and does not rely on the model's generative capabilities. Our experiments reveal that DAIL can significantly improve the model's performance over direct zero-shot inference and can even outperform few-shot ICL without any external information.

------------

`[2406.00131] How In-Context Learning Emerges from Training on Unstructured Data: On the Role of Co-Occurrence, Positional Information, and Noise Structures <https://arxiv.org/abs/2406.00131>`__ 非结构化数据训练如何产生上下文学习:共现、位置信息和噪声结构的作用

::

    Fri, 31 May 2024 18:46:06 GMT
    Kevin Christian Wibisono, Yixin Wang

Large language models (LLMs) like transformers have impressive in-context learning (ICL) capabilities; they can generate predictions for new queries based on input-output sequences in prompts without parameter updates. While many theories have attempted to explain ICL, they often focus on structured training data similar to ICL tasks, such as regression. In practice, however, these models are trained in an unsupervised manner on unstructured text data, which bears little resemblance to ICL tasks. To this end, we investigate how ICL emerges from unsupervised training on unstructured data. The key observation is that ICL can arise simply by modeling co-occurrence information using classical language models like continuous bag of words (CBOW), which we theoretically prove and empirically validate. Furthermore, we establish the necessity of positional information and noise structure to generalize ICL to unseen data. Finally, we present instances where ICL fails and provide theoretical explanations; they suggest that the ICL ability of LLMs to identify certain tasks can be sensitive to the structure of the training data.

------------

`[2406.00793] Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective <https://arxiv.org/abs/2406.00793>`__ 大型语言模型中的上下文学习是贝叶斯的吗?鞅视角

::

    Sun, 2 Jun 2024 16:20:30 GMT
    Fabian Falck, Ziyu Wang, Chris Holmes

In-context learning (ICL) has emerged as a particularly remarkable characteristic of Large Language Models (LLM): given a pretrained LLM and an observed dataset, LLMs can make predictions for new data points from the same distribution without fine-tuning. Numerous works have postulated ICL as approximately Bayesian inference, rendering this a natural hypothesis. In this work, we analyse this hypothesis from a new angle through the martingale property, a fundamental requirement of a Bayesian learning system for exchangeable data. We show that the martingale property is a necessary condition for unambiguous predictions in such scenarios, and enables a principled, decomposed notion of uncertainty vital in trustworthy, safety-critical systems. We derive actionable checks with corresponding theory and test statistics which must hold if the martingale property is satisfied. We also examine if uncertainty in LLMs decreases as expected in Bayesian learning when more data is observed. In three experiments, we provide evidence for violations of the martingale property, and deviations from a Bayesian scaling behaviour of uncertainty, falsifying the hypothesis that ICL is Bayesian.

------------

`[2405.10288] Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction <https://arxiv.org/abs/2405.10288>`__ 基于时间线的句子分解与上下文学习的时序事实抽取

::

    replaced with revised version Sun, 2 Jun 2024 08:04:26 GMT
    Submission history From: Jianhao Chen [view email]
    [v1] Thu, 16 May 2024 17:48:21 UTC (716 KB)
    [v2] Sun, 2 Jun 2024 08:04:26 UTC (716 KB)
    Jianhao Chen, Haoyuan Ouyang, Junyang Ren, Wentao Ding, Wei Hu, Yuzhong Qu

Facts extraction is pivotal for constructing knowledge graphs. Recently, the increasing demand for temporal facts in downstream tasks has led to the emergence of the task of temporal fact extraction. In this paper, we specifically address the extraction of temporal facts from natural language text. Previous studies fail to handle the challenge of establishing time-to-fact correspondences in complex sentences. To overcome this hurdle, we propose a timeline-based sentence decomposition strategy using large language models (LLMs) with in-context learning, ensuring a fine-grained understanding of the timeline associated with various facts. In addition, we evaluate the performance of LLMs for direct temporal fact extraction and get unsatisfactory results. To this end, we introduce TSDRE, a method that incorporates the decomposition capabilities of LLMs into the traditional fine-tuning of smaller pre-trained language models (PLMs). To support the evaluation, we construct ComplexTRED, a complex temporal fact extraction dataset. Our experiments show that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and ComplexTRED datasets.

------------

`[2405.10738] Feature-Adaptive and Data-Scalable In-Context Learning <https://arxiv.org/abs/2405.10738>`__ 特征自适应和数据可扩展的上下文学习

::

    replaced with revised version Mon, 3 Jun 2024 06:42:32 GMT
    Submission history From: Jiahao Li [view email]
    [v1] Fri, 17 May 2024 12:32:53 UTC (8,155 KB)
    [v2] Mon, 3 Jun 2024 06:42:32 UTC (8,155 KB)
    Jiahao Li, Quan Wang, Licheng Zhang, Guoqing Jin and Zhendong Mao

In-context learning (ICL), which promotes inference with several demonstrations, has become a widespread paradigm to stimulate LLM capabilities for downstream tasks. Due to context length constraints, it cannot be further improved in spite of more training data, and general features directly from LLMs in ICL are not adaptive to the specific downstream task. In this paper, we propose a feature-adaptive and data-scalable in-context learning framework (FADS-ICL), which can leverage task-adaptive features to promote inference on the downstream task, with the supervision of beyond-context samples. Specifically, it first extracts general features of beyond-context samples via the LLM with ICL input form one by one, and introduces a task-specific modulator to perform feature refinement and prediction after fitting a specific downstream task. We conduct extensive experiments on FADS-ICL under varying data settings (4$\sim$128 shots) and LLM scale (0.8$\sim$70B) settings. Experimental results show that FADS-ICL consistently outperforms previous state-of-the-art methods by a significant margin under all settings, verifying the effectiveness and superiority of FADS-ICL. For example, under the 1.5B and 32 shots setting, FADS-ICL can achieve \textbf{+14.3} average accuracy from feature adaptation over vanilla ICL on 10 datasets, with \textbf{+6.2} average accuracy over the previous state-of-the-art method, and the performance can further improve with increasing training data. Code and data are publicly available at \url{this https URL}.

------------

`[2405.06270] XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare <https://arxiv.org/abs/2405.06270>`__ XAI4LLM。让机器学习模型和LLMs合作，以增强医疗保健中的上下文学习

::

    replaced with revised version Mon, 3 Jun 2024 16:23:28 GMT
    Submission history From: Yashar Deldjoo [view email]
    [v1] Fri, 10 May 2024 06:52:44 UTC (2,502 KB)
    [v2] Wed, 15 May 2024 11:59:41 UTC (2,515 KB)
    [v3] Mon, 3 Jun 2024 16:23:28 UTC (2,516 KB)
    Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio

The integration of Large Language Models (LLMs) into healthcare diagnostics offers a promising avenue for clinical decision-making. This study outlines the development of a novel method for zero-shot/few-shot in-context learning (ICL) by integrating medical domain knowledge using a multi-layered structured prompt. We also explore the efficacy of two communication styles between the user and LLMs: the Numerical Conversational (NC) style, which processes data incrementally, and the Natural Language Single-Turn (NL-ST) style, which employs long narrative prompts.
Our study systematically evaluates the diagnostic accuracy and risk factors, including gender bias and false negative rates, using a dataset of 920 patient records in various few-shot scenarios. Results indicate that traditional clinical machine learning (ML) models generally outperform LLMs in zero-shot and few-shot settings. However, the performance gap narrows significantly when employing few-shot examples alongside effective explainable AI (XAI) methods as sources of domain knowledge. Moreover, with sufficient time and an increased number of examples, the conversational style (NC) nearly matches the performance of ML models. Most notably, LLMs demonstrate comparable or superior cost-sensitive accuracy relative to ML models.
This research confirms that, with appropriate domain knowledge and tailored communication strategies, LLMs can significantly enhance diagnostic processes. The findings highlight the importance of optimizing the number of training examples and communication styles to improve accuracy and reduce biases in LLM applications.

------------

--------------
Reasoning (13)
--------------

`[2406.00257] Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning? An Extensive Investigation into the Capabilities and Limitations of LVLMs <https://arxiv.org/abs/2406.00257>`__ 大型视觉语言模型能否应对图表理解和推理的挑战?对LVLMs的能力和局限性的广泛调查

::

    Sat, 1 Jun 2024 01:43:30 GMT
    Mohammed Saidul Islam, Raian Rahman, Ahmed Masry, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, Enamul Hoque

Natural language is a powerful complementary modality of communication for data visualizations, such as bar and line charts. To facilitate chart-based reasoning using natural language, various downstream tasks have been introduced recently such as chart question answering, chart summarization, and fact-checking with charts. These tasks pose a unique challenge, demanding both vision-language reasoning and a nuanced understanding of chart data tables, visual encodings, and natural language prompts. Despite the recent success of Large Language Models (LLMs) across diverse NLP tasks, their abilities and limitations in the realm of data visualization remain under-explored, possibly due to their lack of multi-modal capabilities. To bridge the gap, this paper presents the first comprehensive evaluation of the recently developed large vision language models (LVLMs) for chart understanding and reasoning tasks. Our evaluation includes a comprehensive assessment of LVLMs, including GPT-4V and Gemini, across four major chart reasoning tasks. Furthermore, we perform a qualitative evaluation of LVLMs' performance on a diverse range of charts, aiming to provide a thorough analysis of their strengths and weaknesses. Our findings reveal that LVLMs demonstrate impressive abilities in generating fluent texts covering high-level data insights while also encountering common problems like hallucinations, factual errors, and data bias. We highlight the key strengths and limitations of chart comprehension tasks, offering insights for future research.

------------

`[2406.00284] A Closer Look at Logical Reasoning with LLMs: The Choice of Tool Matters <https://arxiv.org/abs/2406.00284>`__ 用llm进行逻辑推理的仔细观察:工具的选择很重要

::

    Sat, 1 Jun 2024 03:29:56 GMT
    Long Hei Matthew Lam, Ehsan Shareghi

Logical reasoning serves as a cornerstone for human cognition. Recently, the emergence of Large Language Models (LLMs) has demonstrated promising progress in solving logical reasoning tasks effectively. To improve this capability, recent studies have delved into integrating LLMs with various symbolic solvers using diverse techniques and methodologies. While some combinations excel on specific datasets, others fall short. However, it remains unclear whether the variance in performance stems from the methodologies employed or the specific symbolic solvers utilized. Therefore, there is a lack of consistent comparison between symbolic solvers and how they influence LLM's logical reasoning ability. We perform experiments on LLMs integrated with 3 symbolic solvers: Z3, Pyke, and Prover9, and compare their performance on 3 logical reasoning datasets: ProofWriter, PrOntoQA, and FOLIO. Our findings indicate that when combined with LLMs Pyke's performance is significantly inferior to that of Prover9 and Z3. Z3's overall accuracy performance slightly surpasses Prover9, but Prover9 could execute more questions.

------------

`[2406.00755] Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction <https://arxiv.org/abs/2406.00755>`__ 评估大型语言模型的数学推理:错误识别和纠正的重点

::

    Sun, 2 Jun 2024 14:16:24 GMT
    Xiaoyuan Li, Wenjie Wang, Moxin Li, Junrong Guo, Yang Zhang, Fuli Feng

The rapid advancement of Large Language Models (LLMs) in the realm of mathematical reasoning necessitates comprehensive evaluations to gauge progress and inspire future directions. Existing assessments predominantly focus on problem-solving from the examinee perspective, overlooking a dual perspective of examiner regarding error identification and correction. From the examiner perspective, we define four evaluation tasks for error identification and correction along with a new dataset with annotated error types and steps. We also design diverse prompts to thoroughly evaluate eleven representative LLMs.
Our principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-2-7B demonstrates comparable abilities to closed-source models GPT-3.5 and Gemini Pro. Notably, calculation error proves the most challenging error type. Moreover, prompting LLMs with the error types can improve the average correction accuracy by 47.9\%. These results reveal potential directions for developing the mathematical reasoning abilities of LLMs. Our code and dataset is available on https://github.com/LittleCirc1e/EIC.

------------

`[2406.00922] MEDIQ: Question-Asking LLMs for Adaptive and Reliable Medical Reasoning <https://arxiv.org/abs/2406.00922>`__ MEDIQ:向LLMs提问，以获得适应性和可靠的医学推理

::

    Mon, 3 Jun 2024 01:32:52 GMT
    Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson, Pang Wei Koh, Yulia Tsvetkov

In high-stakes domains like medical reasoning, AI assistants powered by large language models (LLMs) are yet to be reliable and safe. We identify a key obstacle towards reliability: existing LLMs are trained to answer any question, even with incomplete context in the prompt or insufficient parametric knowledge. We propose to change this paradigm to develop more careful LLMs that ask follow-up questions to gather necessary and sufficient information and respond reliably. We introduce MEDIQ, a framework to simulate realistic clinical interactions, which incorporates a Patient System and an adaptive Expert System. The Patient may provide incomplete information in the beginning; the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details from the Patient via follow-up questions. To evaluate MEDIQ, we convert MEDQA and CRAFT-MD -- medical benchmarks for diagnostic question answering -- into an interactive setup. We develop a reliable Patient system and prototype several Expert systems, first showing that directly prompting state-of-the-art LLMs to ask questions degrades the quality of clinical reasoning, indicating that adapting LLMs to interactive information-seeking settings is nontrivial. We then augment the Expert with a novel abstention module to better estimate model confidence and decide whether to ask more questions, thereby improving diagnostic accuracy by 20.3%; however, performance still lags compared to an (unrealistic in practice) upper bound when full information is given upfront. Further analyses reveal that interactive performance can be improved by filtering irrelevant contexts and reformatting conversations. Overall, our paper introduces a novel problem towards LLM reliability, a novel MEDIQ framework, and highlights important future directions to extend the information-seeking abilities of LLM assistants in critical domains.

------------

`[2406.01145] Explore then Determine: A GNN-LLM Synergy Framework for Reasoning over Knowledge Graph <https://arxiv.org/abs/2406.01145>`__ 探索并确定:面向知识图谱推理的GNN-LLM协同框架

::

    Mon, 3 Jun 2024 09:38:28 GMT
    Guangyi Liu, Yongqi Zhang, Yong Li, Quanming Yao

The task of reasoning over Knowledge Graphs (KGs) poses a significant challenge for Large Language Models (LLMs) due to the complex structure and large amounts of irrelevant information. Existing LLM reasoning methods overlook the importance of compositional learning on KG to supply with precise knowledge. Besides, the fine-tuning and frequent interaction with LLMs incur substantial time and resource costs. This paper focuses on the Question Answering over Knowledge Graph (KGQA) task and proposes an Explore-then-Determine (EtD) framework that synergizes LLMs with graph neural networks (GNNs) for reasoning over KGs. The Explore stage employs a lightweight GNN to explore promising candidates and relevant fine-grained knowledge to the questions, while the Determine stage utilizes the explored information to construct a knowledge-enhanced multiple-choice prompt, guiding a frozen LLM to determine the final answer. Extensive experiments on three benchmark KGQA datasets demonstrate that EtD achieves state-of-the-art performance and generates faithful reasoning results.

------------

`[2402.02805] Graph-enhanced Large Language Models in Asynchronous Plan Reasoning <https://arxiv.org/abs/2402.02805>`__ 异步规划推理中的图增强大型语言模型

::

    replaced with revised version Mon, 3 Jun 2024 13:07:06 GMT
    Submission history From: Fangru Lin [view email]
    [v1] Mon, 5 Feb 2024 08:26:33 UTC (2,886 KB)
    [v2] Mon, 3 Jun 2024 13:07:06 UTC (2,757 KB)
    Fangru Lin, Emanuele La Malfa, Valentin Hofmann, Elle Michelle Yang, Anthony Cohn, Janet B. Pierrehumbert

Planning is a fundamental property of human intelligence. Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents. Our code and data are available at this https URL.

------------

`[2402.06044] OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models <https://arxiv.org/abs/2402.06044>`__ OpenToM:评估大型语言模型心智理论推理能力的综合基准

::

    replaced with revised version Mon, 3 Jun 2024 10:48:16 GMT
    Submission history From: Hainiu Xu [view email]
    [v1] Thu, 8 Feb 2024 20:35:06 UTC (4,577 KB)
    [v2] Wed, 14 Feb 2024 13:23:51 UTC (4,559 KB)
    [v3] Mon, 3 Jun 2024 10:48:16 UTC (4,561 KB)
    Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, Yulan He

Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.

------------

`[2306.06427] Boosting Language Models Reasoning with Chain-of-Knowledge Prompting <https://arxiv.org/abs/2306.06427>`__ 基于知识链提示的语言模型推理增强

::

    replaced with revised version Mon, 3 Jun 2024 14:59:11 GMT
    Submission history From: Jianing Wang [view email]
    [v1] Sat, 10 Jun 2023 12:42:36 UTC (246 KB)
    [v2] Thu, 20 Jul 2023 08:47:14 UTC (248 KB)
    [v3] Mon, 3 Jun 2024 14:59:11 UTC (361 KB)
    Jianing Wang, Qiushi Sun, Xiang Li, Ming Gao

Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.

------------

`[2306.17820] Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models <https://arxiv.org/abs/2306.17820>`__ 元推理:大型语言模型的语义符号解构

::

    replaced with revised version Sun, 2 Jun 2024 03:48:21 GMT
    Submission history From: Yiming Wang [view email]
    [v1] Fri, 30 Jun 2023 17:38:10 UTC (418 KB)
    [v2] Thu, 2 Nov 2023 16:38:29 UTC (8,373 KB)
    [v3] Fri, 16 Feb 2024 12:46:27 UTC (9,037 KB)
    [v4] Sun, 2 Jun 2024 03:48:21 UTC (8,900 KB)
    Yiming Wang, Zhuosheng Zhang, Pei Zhang, Baosong Yang, Rui Wang

Neural-symbolic methods have demonstrated efficiency in enhancing the reasoning abilities of large language models (LLMs). However, existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits. To broaden symbolic methods' applicability and adaptability in the real world, we propose the Meta-Reasoning from a linguistic perspective. This method empowers LLMs to deconstruct reasoning-independent semantic information into generic symbolic representations, thereby efficiently capturing more generalized reasoning knowledge. We conduct extensive experiments on more than ten datasets encompassing conventional reasoning tasks like arithmetic, symbolic, and logical reasoning, and the more complex interactive reasoning tasks like theory-of-mind reasoning. Experimental results demonstrate that Meta-Reasoning significantly enhances in-context reasoning accuracy, learning efficiency, out-of-domain generalization, and output stability compared to the Chain-of-Thought technique. Code and data are publicly available at \url{this https URL}.

------------

`[2401.06853] Large Language Models Can Learn Temporal Reasoning <https://arxiv.org/abs/2401.06853>`__ 大型语言模型可以学习时序推理

::

    replaced with revised version Sat, 1 Jun 2024 22:38:32 GMT
    Submission history From: Siheng Xiong [view email]
    [v1] Fri, 12 Jan 2024 19:00:26 UTC (7,033 KB)
    [v2] Tue, 20 Feb 2024 00:14:31 UTC (7,186 KB)
    [v3] Mon, 22 Apr 2024 04:00:00 UTC (7,192 KB)
    [v4] Sat, 1 Jun 2024 22:38:32 UTC (7,191 KB)
    Siheng Xiong, Ali Payani, Ramana Kompella, Faramarz Fekri

While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards language-based TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation.

------------

`[2402.14809] CriticBench: Benchmarking LLMs for Critique-Correct Reasoning <https://arxiv.org/abs/2402.14809>`__ CriticBench:评价正确推理的llm基准测试

::

    replaced with revised version Sat, 1 Jun 2024 07:46:28 GMT
    Submission history From: Zicheng Lin [view email]
    [v1] Thu, 22 Feb 2024 18:59:02 UTC (1,194 KB)
    [v2] Fri, 8 Mar 2024 15:15:47 UTC (1,187 KB)
    [v3] Tue, 28 May 2024 14:33:27 UTC (1,281 KB)
    [v4] Sat, 1 Jun 2024 07:46:28 UTC (1,307 KB)
    Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang

The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement.

------------

`[2402.14856] Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning <https://arxiv.org/abs/2402.14856>`__ 演绎推理中人类推理策略与大型语言模型的比较

::

    replaced with revised version Mon, 3 Jun 2024 13:53:01 GMT
    Submission history From: Philipp Mondorf [view email]
    [v1] Tue, 20 Feb 2024 12:58:14 UTC (9,401 KB)
    [v2] Mon, 3 Jun 2024 13:53:01 UTC (10,253 KB)
    Philipp Mondorf and Barbara Plank

Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the architecture and scale of the model significantly affect its preferred method of reasoning, with more advanced models tending to adopt strategies more frequently than less sophisticated ones. Importantly, we assert that a model's accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its reasoning process. This distinction underscores the necessity for more nuanced evaluation procedures in the field.

------------

`[2405.05189] MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning <https://arxiv.org/abs/2405.05189>`__ 

::

    replaced with revised version Sun, 2 Jun 2024 18:47:44 GMT
    Submission history From: Inderjeet Nair [view email]
    [v1] Wed, 8 May 2024 16:25:42 UTC (1,711 KB)
    [v2] Sun, 2 Jun 2024 18:47:44 UTC (1,711 KB)
    Inderjeet Nair, Lu Wang

We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability. Additionally, relying solely on a single sample may result in the omission of true nodes and edges. To counter this, we draw inspiration from self-consistency (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer. To tackle the substantial challenge of applying SC on generated graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of Reasoning in Directed acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM. This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision. Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts.

------------

-----------
ToolUse (4)
-----------

`[2406.00059] Conveyor: Efficient Tool-aware LLM Serving with Tool Partial Execution <https://arxiv.org/abs/2406.00059>`__ 输送机:工具部分执行的高效工具感知LLM

::

    Wed, 29 May 2024 21:24:15 GMT
    Yechen Xu, Xinhao Kong, Tingjun Chen, Danyang Zhuo

The complexity of large language model (LLM) serving workloads has substantially increased due to the integration with external tool invocations, such as ChatGPT plugins. In this paper, we identify a new opportunity for efficient LLM serving for requests that trigger tools: tool partial execution alongside LLM decoding. To this end, we design Conveyor, an efficient LLM serving system optimized for handling requests involving external tools. We introduce a novel interface for tool developers to expose partial execution opportunities to the LLM serving system and a request scheduler that facilitates partial tool execution. Our results demonstrate that tool partial execution can improve request completion latency by up to 38.8%.

------------

`[2406.00284] A Closer Look at Logical Reasoning with LLMs: The Choice of Tool Matters <https://arxiv.org/abs/2406.00284>`__ 用llm进行逻辑推理的仔细观察:工具的选择很重要

::

    Sat, 1 Jun 2024 03:29:56 GMT
    Long Hei Matthew Lam, Ehsan Shareghi

Logical reasoning serves as a cornerstone for human cognition. Recently, the emergence of Large Language Models (LLMs) has demonstrated promising progress in solving logical reasoning tasks effectively. To improve this capability, recent studies have delved into integrating LLMs with various symbolic solvers using diverse techniques and methodologies. While some combinations excel on specific datasets, others fall short. However, it remains unclear whether the variance in performance stems from the methodologies employed or the specific symbolic solvers utilized. Therefore, there is a lack of consistent comparison between symbolic solvers and how they influence LLM's logical reasoning ability. We perform experiments on LLMs integrated with 3 symbolic solvers: Z3, Pyke, and Prover9, and compare their performance on 3 logical reasoning datasets: ProofWriter, PrOntoQA, and FOLIO. Our findings indicate that when combined with LLMs Pyke's performance is significantly inferior to that of Prover9 and Z3. Z3's overall accuracy performance slightly surpasses Prover9, but Prover9 could execute more questions.

------------

`[2406.00008] KnowledgeHub: An end-to-end Tool for Assisted Scientific Discovery <https://arxiv.org/abs/2406.00008>`__ KnowledgeHub:一个端到端的科学发现辅助工具

::

    Thu, 16 May 2024 13:17:14 GMT
    Shinnosuke Tanaka, James Barry, Vishnudev Kuruvanthodi, Movina Moses, Maxwell J. Giammona, Nathan Herr, Mohab Elkaref, Geeth De Mel

This paper describes the KnowledgeHub tool, a scientific literature Information Extraction (IE) and Question Answering (QA) pipeline. This is achieved by supporting the ingestion of PDF documents that are converted to text and structured representations. An ontology can then be constructed where a user defines the types of entities and relationships they want to capture. A browser-based annotation tool enables annotating the contents of the PDF documents according to the ontology. Named Entity Recognition (NER) and Relation Classification (RC) models can be trained on the resulting annotations and can be used to annotate the unannotated portion of the documents. A knowledge graph is constructed from these entity and relation triples which can be queried to obtain insights from the data. Furthermore, we integrate a suite of Large Language Models (LLMs) that can be used for QA and summarisation that is grounded in the included documents via a retrieval component. KnowledgeHub is a unique tool that supports annotation, IE and QA, which gives the user full insight into the knowledge discovery pipeline.

------------

`[2401.17167] Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios <https://arxiv.org/abs/2401.17167>`__ 规划、创建、使用:对现实世界复杂场景中全面工具使用的llm进行基准测试

::

    replaced with revised version Mon, 3 Jun 2024 11:28:29 GMT
    Submission history From: Shijue Huang [view email]
    [v1] Tue, 30 Jan 2024 16:52:56 UTC (2,996 KB)
    [v2] Fri, 16 Feb 2024 15:55:34 UTC (3,628 KB)
    [v3] Mon, 3 Jun 2024 11:28:29 UTC (3,659 KB)
    Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng Xu, Qun Liu

The recent trend of using Large Language Models (LLMs) as tool agents in real-world applications underscores the necessity for comprehensive evaluations of their capabilities, particularly in complex scenarios involving planning, creating, and using tools. However, existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization. To address this issue, we present UltraTool, a novel benchmark designed to improve and evaluate LLMs' ability in tool utilization within real-world scenarios. UltraTool focuses on the entire process of using tools - from planning and creating to applying them in complex tasks. It emphasizes real-world complexities, demanding accurate, multi-step planning for effective problem-solving. A key feature of UltraTool is its independent evaluation of planning with natural language, which happens before tool usage and simplifies the task solving by mapping out the intermediate steps. Thus, unlike previous work, it eliminates the restriction of pre-defined toolset. Through extensive experiments on various LLMs, we offer novel insights into the evaluation of capabilities of LLMs in tool utilization, thereby contributing a fresh perspective to this rapidly evolving field. The benchmark is publicly available at this https URL.

------------

------------------------
Retrieval-Augmented (15)
------------------------

`[2406.00029] Clustered Retrieved Augmented Generation (CRAG) <https://arxiv.org/abs/2406.00029>`__ 集群检索增强生成(CRAG)

::

    Fri, 24 May 2024 16:36:47 GMT
    Simon Akesson and Frances A. Santos

Providing external knowledge to Large Language Models (LLMs) is a key point for using these models in real-world applications for several reasons, such as incorporating up-to-date content in a real-time manner, providing access to domain-specific knowledge, and contributing to hallucination prevention. The vector database-based Retrieval Augmented Generation (RAG) approach has been widely adopted to this end. Thus, any part of external knowledge can be retrieved and provided to some LLM as the input context. Despite RAG approach's success, it still might be unfeasible for some applications, because the context retrieved can demand a longer context window than the size supported by LLM. Even when the context retrieved fits into the context window size, the number of tokens might be expressive and, consequently, impact costs and processing time, becoming impractical for most applications. To address these, we propose CRAG, a novel approach able to effectively reduce the number of prompting tokens without degrading the quality of the response generated compared to a solution using RAG. Through our experiments, we show that CRAG can reduce the number of tokens by at least 46\%, achieving more than 90\% in some cases, compared to RAG. Moreover, the number of tokens with CRAG does not increase considerably when the number of reviews analyzed is higher, unlike RAG, where the number of tokens is almost 9x higher when there are 75 reviews compared to 4 reviews.

------------

`[2406.00033] Retrieval-Augmented Conversational Recommendation with Prompt-based Semi-Structured Natural Language State Tracking <https://arxiv.org/abs/2406.00033>`__ 基于提示的半结构化自然语言状态跟踪的检索增强对话推荐

::

    Sat, 25 May 2024 15:41:26 GMT
    Sara Kemper, Justin Cui, Kai Dicarlantonio, Kathy Lin, Danjie Tang, Anton Korikov, Scott Sanner

Conversational recommendation (ConvRec) systems must understand rich and diverse natural language (NL) expressions of user preferences and intents, often communicated in an indirect manner (e.g., "I'm watching my weight"). Such complex utterances make retrieving relevant items challenging, especially if only using often incomplete or out-of-date metadata. Fortunately, many domains feature rich item reviews that cover standard metadata categories and offer complex opinions that might match a user's interests (e.g., "classy joint for a date"). However, only recently have large language models (LLMs) let us unlock the commonsense connections between user preference utterances and complex language in user-generated reviews. Further, LLMs enable novel paradigms for semi-structured dialogue state tracking, complex intent and preference understanding, and generating recommendations, explanations, and question answers. We thus introduce a novel technology RA-Rec, a Retrieval-Augmented, LLM-driven dialogue state tracking system for ConvRec, showcased with a video, open source GitHub repository, and interactive Google Colab notebook.

------------

`[2406.00036] EMERGE: Integrating RAG for Improved Multimodal EHR Predictive Modeling <https://arxiv.org/abs/2406.00036>`__ EMERGE:集成RAG改进的多模态EHR预测建模

::

    Mon, 27 May 2024 10:53:15 GMT
    Yinghao Zhu, Changyu Ren, Zixiang Wang, Xiaochen Zheng, Shiyun Xie, Junlan Feng, Xi Zhu, Zhoujun Li, Liantao Ma, Chengwei Pan

The integration of multimodal Electronic Health Records (EHR) data has notably advanced clinical predictive capabilities. However, current models that utilize clinical notes and multivariate time-series EHR data often lack the necessary medical context for precise clinical tasks. Previous methods using knowledge graphs (KGs) primarily focus on structured knowledge extraction. To address this, we propose EMERGE, a Retrieval-Augmented Generation (RAG) driven framework aimed at enhancing multimodal EHR predictive modeling. Our approach extracts entities from both time-series data and clinical notes by prompting Large Language Models (LLMs) and aligns them with professional PrimeKG to ensure consistency. Beyond triplet relationships, we include entities' definitions and descriptions to provide richer semantics. The extracted knowledge is then used to generate task-relevant summaries of patients' health statuses. These summaries are fused with other modalities utilizing an adaptive multimodal fusion network with cross-attention. Extensive experiments on the MIMIC-III and MIMIC-IV datasets for in-hospital mortality and 30-day readmission tasks demonstrate the superior performance of the EMERGE framework compared to baseline models. Comprehensive ablation studies and analyses underscore the efficacy of each designed module and the framework's robustness to data sparsity. EMERGE significantly enhances the use of multimodal EHR data in healthcare, bridging the gap with nuanced medical contexts crucial for informed clinical predictions.

------------

`[2406.00562] SPAGHETTI: Open-Domain Question Answering from Heterogeneous Data Sources with Retrieval and Semantic Parsing <https://arxiv.org/abs/2406.00562>`__ 意大利面条:基于检索和语义解析的异构数据源开放域问答

::

    Sat, 1 Jun 2024 21:54:09 GMT
    Heidi C. Zhang, Sina J. Semnani, Farhad Ghassemi, Jialiang Xu, Shicheng Liu, Monica S. Lam

We introduce SPAGHETTI: Semantic Parsing Augmented Generation for Hybrid English information from Text Tables and Infoboxes, a hybrid question-answering (QA) pipeline that utilizes information from heterogeneous knowledge sources, including knowledge base, text, tables, and infoboxes. Our LLM-augmented approach achieves state-of-the-art performance on the Compmix dataset, the most comprehensive heterogeneous open-domain QA dataset, with 56.5% exact match (EM) rate. More importantly, manual analysis on a sample of the dataset suggests that SPAGHETTI is more than 90% accurate, indicating that EM is no longer suitable for assessing the capabilities of QA systems today.

------------

`[2406.00944] Unveil the Duality of Retrieval-Augmented Generation: Theoretical Analysis and Practical Solution <https://arxiv.org/abs/2406.00944>`__ 揭示检索增强生成的对偶性:理论分析和实际解决方案

::

    Mon, 3 Jun 2024 02:56:14 GMT
    Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng

Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large language models (LLMs). However, studies show that RAG is not consistently effective and can even mislead LLMs due to noisy or incorrect retrieved texts. This suggests that RAG possesses a duality including both benefit and detriment. Although many existing methods attempt to address this issue, they lack a theoretical explanation for the duality in RAG. The benefit and detriment within this duality remain a black box that cannot be quantified or compared in an explainable manner. This paper takes the first step in theoretically giving the essential explanation of benefit and detriment in RAG by: (1) decoupling and formalizing them from RAG prediction, (2) approximating the gap between their values by representation similarity and (3) establishing the trade-off mechanism between them, to make them explainable, quantifiable, and comparable. We demonstrate that the distribution difference between retrieved texts and LLMs' knowledge acts as double-edged sword, bringing both benefit and detriment. We also prove that the actual effect of RAG can be predicted at token level. Based on our theory, we propose a practical novel method, X-RAG, which achieves collaborative generation between pure LLM and RAG at token level to preserve benefit and avoid detriment. Experiments in real-world tasks based on LLMs including OPT, LLaMA-2, and Mistral show the effectiveness of our method and support our theoretical results.

------------

`[2406.01549] An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation <https://arxiv.org/abs/2406.01549>`__ 检索增强生成中有效噪声过滤的信息瓶颈视角

::

    Mon, 3 Jun 2024 17:31:06 GMT
    Kun Zhu, Xiaocheng Feng, Xiyuan Du, Yuxuan Gu, Weijiang Yu, Haotian Wang, Qianglong Chen, Zheng Chu, Jingchang Chen, Bing Qin

Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data. One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression. In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation. Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage. In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards. Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with $2.5\%$ compression rate.

------------

`[2406.01462] Understanding Preference Fine-Tuning Through the Lens of Coverage <https://arxiv.org/abs/2406.01462>`__ 通过覆盖率的视角理解偏好微调

::

    Mon, 3 Jun 2024 15:51:04 GMT
    Yuda Song, Gokul Swamy, Aarti Singh, J. Andrew Bagnell, Wen Sun

Learning from human preference data has emerged as the dominant paradigm for fine-tuning large language models (LLMs). The two most common families of techniques -- online reinforcement learning (RL) such as Proximal Policy Optimization (PPO) and offline contrastive methods such as Direct Preference Optimization (DPO) -- were positioned as equivalent in prior work due to the fact that both have to start from the same offline preference dataset. To further expand our theoretical understanding of the similarities and differences between online and offline techniques for preference fine-tuning, we conduct a rigorous analysis through the lens of dataset coverage, a concept that captures how the training data covers the test distribution and is widely used in RL. We prove that a global coverage condition is both necessary and sufficient for offline contrastive methods to converge to the optimal policy, but a weaker partial coverage condition suffices for online RL methods. This separation provides one explanation of why online RL methods can perform better than offline methods, especially when the offline preference data is not diverse enough. Finally, motivated by our preceding theoretical observations, we derive a hybrid preference optimization (HyPO) algorithm that uses offline data for contrastive-based preference optimization and online data for KL regularization. Theoretically and empirically, we demonstrate that HyPO is more performant than its pure offline counterpart DPO, while still preserving its computation and memory efficiency.

------------

`[2406.00083] BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models <https://arxiv.org/abs/2406.00083>`__ BadRAG:大型语言模型检索增强生成中的漏洞识别

::

    Mon, 3 Jun 2024 02:25:33 GMT
    Jiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun Chen, Qian Lou

Large Language Models (LLMs) are constrained by outdated information and a tendency to generate incorrect data, commonly referred to as "hallucinations." Retrieval-Augmented Generation (RAG) addresses these limitations by combining the strengths of retrieval-based methods and generative models. This approach involves retrieving relevant information from a large, up-to-date dataset and using it to enhance the generation process, leading to more accurate and contextually appropriate responses. Despite its benefits, RAG introduces a new attack surface for LLMs, particularly because RAG databases are often sourced from public data, such as the web. In this paper, we propose \TrojRAG{} to identify the vulnerabilities and attacks on retrieval parts (RAG database) and their indirect attacks on generative parts (LLMs). Specifically, we identify that poisoning several customized content passages could achieve a retrieval backdoor, where the retrieval works well for clean queries but always returns customized poisoned adversarial queries. Triggers and poisoned passages can be highly customized to implement various attacks. For example, a trigger could be a semantic group like "The Republican Party, Donald Trump, etc." Adversarial passages can be tailored to different contents, not only linked to the triggers but also used to indirectly attack generative LLMs without modifying them.
These attacks can include denial-of-service attacks on RAG and semantic steering attacks on LLM generations conditioned by the triggers. Our experiments demonstrate that by just poisoning 10 adversarial passages can induce 98.2\% success rate to retrieve the adversarial passages. Then, these passages can increase the reject ratio of RAG-based GPT-4 from 0.01\% to 74.6\% or increase the rate of negative responses from 0.22\% to 72\% for targeted queries.

------------

`[2402.07688] CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation for Evaluating LLMs in Cybersecurity Knowledge <https://arxiv.org/abs/2402.07688>`__ CyberMetric:基于检索增强生成的用于评估网络安全知识LLMs的基准数据集

::

    replaced with revised version Mon, 3 Jun 2024 08:14:45 GMT
    Submission history From: Norbert Tihanyi Dr. [view email]
    [v1] Mon, 12 Feb 2024 14:53:28 UTC (338 KB)
    [v2] Mon, 3 Jun 2024 08:14:45 UTC (229 KB)
    Norbert Tihanyi, Mohamed Amine Ferrag, Ridhi Jain, Tamas Bisztray, Merouane Debbah

Large Language Models (LLMs) are increasingly used across various domains, from software development to cyber threat intelligence. Understanding all the different fields of cybersecurity, which includes topics such as cryptography, reverse engineering, and risk assessment, poses a challenge even for human experts. To accurately test the general knowledge of LLMs in cybersecurity, the research community needs a diverse, accurate, and up-to-date dataset. To address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000, and CyberMetric-10000, which are multiple-choice Q&A benchmark datasets comprising 80, 500, 2000, and 10,000 questions respectively. By utilizing GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents, including NIST standards, research papers, publicly accessible books, RFCs, and other publications in the cybersecurity domain, to generate questions, each with four possible answers. The results underwent several rounds of error checking and refinement. Human experts invested over 200 hours validating the questions and solutions to ensure their accuracy and relevance, and to filter out any questions unrelated to cybersecurity. We have evaluated and compared 25 state-of-the-art LLM models on the CyberMetric datasets. In addition to our primary goal of evaluating LLMs, we involved 30 human participants to solve CyberMetric-80 in a closed-book scenario. The results can serve as a reference for comparing the general cybersecurity knowledge of humans and LLMs. The findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct, Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs. Additionally, the top LLMs were more accurate than humans on CyberMetric-80, although highly experienced human experts still outperformed small models such as Llama-3-8B, Phi-2 or Gemma-7b.

------------

`[2405.20680] Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models <https://arxiv.org/abs/2405.20680>`__ 检索增强大型语言模型中检索器不一致性的解开和缓解

::

    replaced with revised version Mon, 3 Jun 2024 06:20:18 GMT
    Submission history From: Mingda Li [view email]
    [v1] Fri, 31 May 2024 08:22:49 UTC (5,343 KB)
    [v2] Mon, 3 Jun 2024 06:20:18 UTC (5,343 KB)
    Mingda Li, Xinyu Li, Yifan Chen, Wenfeng Xuan, Weinan Zhang

Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their superiority in terms of factuality, they do not consistently outperform the original retrieval-free Language Models (LMs). Our experiments reveal that this example-level performance inconsistency exists not only between retrieval-augmented and retrieval-free LM but also among different retrievers. To understand this phenomenon, we investigate the degeneration behavior of RALMs and theoretically decompose it into four categories. Further analysis based on our decomposition reveals that the innate difference in knowledge sources and the unpredictable degeneration of the reader model contribute most to the inconsistency. Drawing from our analysis, we introduce Ensemble of Retrievers (EoR), a trainable framework that can adaptively retrieve from different knowledge sources and effectively decrease unpredictable reader errors. Our experiments on Open Domain Question Answering show that EoR substantially improves performance over the RALM with a single retriever by considerably reducing inconsistent behaviors.

------------

`[2310.12516] ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks <https://arxiv.org/abs/2310.12516>`__ ReEval:基于可转移对抗攻击的检索增强大型语言模型自动幻觉评估

::

    replaced with revised version Fri, 31 May 2024 23:46:24 GMT
    Submission history From: Xiaodong Yu [view email]
    [v1] Thu, 19 Oct 2023 06:37:32 UTC (8,139 KB)
    [v2] Fri, 31 May 2024 23:46:24 UTC (8,120 KB)
    Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, Jianfeng Gao

Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the potential of data contamination (e.g., leading to memorization), good static benchmark performance does not ensure that model can reliably use the provided evidence for responding, which is essential to avoid hallucination when the required knowledge is new or private. Inspired by adversarial machine learning, we investigate the feasibility of automatically perturbing existing static one for dynamic evaluation. Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs' reliability in using new evidence for answering.
We implement ReEval using ChatGPT and evaluate the resulting variants of two popular open-domain QA datasets on a collection of LLMs under various prompting settings. Our generated data is human-readable and useful to trigger hallucination in LLM. Accurate models on static data are observed to produce unsupported answers from the perturbed evidence, with pronounced accuracy drops across LLMs including GPT-4. We find that our adversarial examples are transferable across all considered LLMs. The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.

------------

`[2401.17244] LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation <https://arxiv.org/abs/2401.17244>`__ LLaMP:用于高保真材料知识检索和蒸馏的大型语言模型

::

    replaced with revised version Sun, 2 Jun 2024 07:50:21 GMT
    Submission history From: Yuan Chiang [view email]
    [v1] Tue, 30 Jan 2024 18:37:45 UTC (1,477 KB)
    [v2] Sun, 2 Jun 2024 07:50:21 UTC (6,934 KB)
    Yuan Chiang, Elvis Hsieh, Chia-Hong Chou, Janosh Riebesell

Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences, where reliability and reproducibility are crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can dynamically and recursively interact with computational and experimental data on Materials Project (MP) and run atomistic simulations via high-throughput workflow interface. Without fine-tuning, LLaMP demonstrates strong tool usage ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structure and elastic tensor), and streamline complex tasks in computational materials and chemistry. We propose a simple metric combining uncertainty and confidence estimates to evaluate the self-consistency of responses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli, electronic bandgaps, and formation energies that seem to derive from mixed data sources. We also demonstrate LLaMP's capability to edit crystal structures and run annealing molecular dynamics simulations using pre-trained machine-learning force fields. The framework offers an intuitive and nearly hallucination-free approach to exploring and scaling materials informatics, and establishes a pathway for knowledge distillation and fine-tuning other language models. Code and live demo are available at this https URL

------------

`[2402.04411] DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton <https://arxiv.org/abs/2402.04411>`__ 

::

    replaced with revised version Mon, 3 Jun 2024 01:40:46 GMT
    Submission history From: Yiyou Sun [view email]
    [v1] Tue, 6 Feb 2024 21:14:45 UTC (4,888 KB)
    [v2] Mon, 3 Jun 2024 01:40:46 UTC (4,898 KB)
    Yiyou Sun and Junjie Hu and Wei Cheng and Haifeng Chen

This paper introduces the retrieval-augmented large language model with Definite Finite Automaton (DFA-RAG), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach acts as a semantic router which enables the LLM to adhere to a deterministic response pathway. The routing is achieved by the retrieval-augmentation generation (RAG) strategy, which carefully selects dialogue examples aligned with the current conversational context. The advantages of DFA-RAG include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-RAG's effectiveness, indicating its potential as a valuable contribution to the conversational agent.

------------

`[2404.14367] Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data <https://arxiv.org/abs/2404.14367>`__ 

::

    replaced with revised version Sun, 2 Jun 2024 22:00:42 GMT
    Submission history From: Fahim Tajwar [view email]
    [v1] Mon, 22 Apr 2024 17:20:18 UTC (28,308 KB)
    [v2] Tue, 23 Apr 2024 04:49:49 UTC (28,309 KB)
    [v3] Sun, 2 Jun 2024 22:00:42 UTC (28,309 KB)
    Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, Aviral Kumar

Learning from preference labels plays a crucial role in fine-tuning large language models. There are several distinct approaches for preference fine-tuning, including supervised learning, on-policy reinforcement learning (RL), and contrastive learning. Different methods come with different implementation tradeoffs and performance differences, and existing empirical findings present different conclusions, for instance, some results show that online RL is quite important to attain good fine-tuning results, while others find (offline) contrastive or even purely supervised methods sufficient. This raises a natural question: what kind of approaches are important for fine-tuning with preference data and why? In this paper, we answer this question by performing a rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems. Our main finding is that, in general, approaches that use on-policy sampling or attempt to push down the likelihood on certain responses (i.e., employ a "negative gradient") outperform offline and maximum likelihood objectives. We conceptualize our insights and unify methods that use on-policy sampling or negative gradient under a notion of mode-seeking objectives for categorical distributions. Mode-seeking objectives are able to alter probability mass on specific bins of a categorical distribution at a fast rate compared to maximum likelihood, allowing them to relocate masses across bins more effectively. Our analysis prescribes actionable insights for preference fine-tuning of LLMs and informs how data should be collected for maximal improvement.

------------

`[2405.16444] CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion <https://arxiv.org/abs/2405.16444>`__ CacheBlend:基于缓存知识融合的RAG快速大型语言模型

::

    replaced with revised version Mon, 3 Jun 2024 10:57:57 GMT
    Submission history From: Jiayi Yao [view email]
    [v1] Sun, 26 May 2024 06:00:17 UTC (7,493 KB)
    [v2] Mon, 3 Jun 2024 10:57:57 UTC (7,493 KB)
    Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang

Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, and when they are not, their precomputed KV caches cannot be directly used since they ignore the text's cross-attention with the preceding text in the LLM input. Thus, the benefits of reusing KV caches remain largely unrealized.
This paper tackles just one question: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? We present CacheBlend, a scheme that reuses the pre-computed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime,the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job,allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3X and increases the inference throughput by 2.8-5X, compared with full KV recompute, without compromising generation quality or incurring more storage cost.

------------

---------
Agent (6)
---------

`[2406.00252] Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey <https://arxiv.org/abs/2406.00252>`__ 多模态和多智能体系统满足理性:综述

::

    Sat, 1 Jun 2024 01:17:25 GMT
    Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Weijie J. Su, Camillo J. Taylor, Tanwi Mallick

Rationality is the quality of being guided by reason, characterized by logical thinking and decision-making that align with evidence and logical rules. This quality is essential for effective problem-solving, as it ensures that solutions are well-founded and systematically derived. Despite the advancements of large language models (LLMs) in generating human-like text with remarkable accuracy, they present biases inherited from the training data, inconsistency across different contexts, and difficulty understanding complex scenarios involving multiple layers of context. Therefore, recent research attempts to leverage the strength of multiple agents working collaboratively with various types of data and tools for enhanced consistency and reliability.
To that end, this paper aims to understand whether multi-modal and multi-agent systems are advancing toward rationality by surveying the state-of-the-art works, identifying advancements over single-agent and single-modal systems in terms of rationality, and discussing open problems and future directions. We maintain an open repository at https://github.com/bowen-upenn/MMMA_Rationality.

------------

`[2406.00244] Controlling Large Language Model Agents with Entropic Activation Steering <https://arxiv.org/abs/2406.00244>`__ 熵激活转向控制大型语言模型智能体

::

    Sat, 1 Jun 2024 00:25:00 GMT
    Nate Rahn, Pierluca D'Oro, Marc G. Bellemare

The generality of pretrained large language models (LLMs) has prompted increasing interest in their use as in-context learning agents. To be successful, such agents must form beliefs about how to achieve their goals based on limited interaction with their environment, resulting in uncertainty about the best action to take at each step. In this paper, we study how LLM agents form and act on these beliefs by conducting experiments in controlled sequential decision-making tasks. To begin, we find that LLM agents are overconfident: They draw strong conclusions about what to do based on insufficient evidence, resulting in inadequately explorative behavior. We dig deeper into this phenomenon and show how it emerges from a collapse in the entropy of the action distribution implied by sampling from the LLM. We then demonstrate that existing token-level sampling techniques are by themselves insufficient to make the agent explore more. Motivated by this fact, we introduce Entropic Activation Steering (EAST), an activation steering method for in-context LLM agents. EAST computes a steering vector as an entropy-weighted combination of representations, and uses it to manipulate an LLM agent's uncertainty over actions by intervening on its activations during the forward pass. We show that EAST can reliably increase the entropy in an LLM agent's actions, causing more explorative behavior to emerge. Finally, EAST modifies the subjective uncertainty an LLM agent expresses, paving the way to interpreting and controlling how LLM agents represent uncertainty about their decisions.

------------

`[2406.01014] Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration <https://arxiv.org/abs/2406.01014>`__ Mobile- agent -v2:基于多agent协作的有效导航的移动设备操作助手

::

    Mon, 3 Jun 2024 05:50:00 GMT
    Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, Jitao Sang

Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario. Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants. Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario. However, the two major navigation challenges in mobile device operation tasks, task progress navigation and focus content navigation, are significantly complicated under the single-agent architecture of existing work. This is due to the overly long token sequences and the interleaved text-image data format, which limit performance. To address these navigation challenges effectively, we propose Mobile-Agent-v2, a multi-agent architecture for mobile device operation assistance. The architecture comprises three agents: planning agent, decision agent, and reflection agent. The planning agent generates task progress, making the navigation of history operations more efficient. To retain focus content, we design a memory unit that updates with task progress. Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistakes accordingly. Experimental results indicate that Mobile-Agent-v2 achieves over a 30% improvement in task completion compared to the single-agent architecture of Mobile-Agent. The code is open-sourced at https://github.com/X-PLUG/MobileAgent.

------------

`[2402.08567] Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast <https://arxiv.org/abs/2402.08567>`__ Agent Smith:一个图像可以以指数级的速度越狱一百万个多模态LLM代理

::

    replaced with revised version Mon, 3 Jun 2024 14:15:03 GMT
    Submission history From: Tianyu Pang [view email]
    [v1] Tue, 13 Feb 2024 16:06:17 UTC (9,500 KB)
    [v2] Mon, 3 Jun 2024 14:15:03 UTC (10,295 KB)
    Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, Min Lin

A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. Our project page is available at this https URL.

------------

`[2402.11941] CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation <https://arxiv.org/abs/2402.11941>`__ CoCo-Agent:一种面向智能手机GUI自动化的综合认知MLLM Agent

::

    replaced with revised version Sun, 2 Jun 2024 13:25:05 GMT
    Submission history From: Xinbei Ma [view email]
    [v1] Mon, 19 Feb 2024 08:29:03 UTC (15,465 KB)
    [v2] Sat, 9 Mar 2024 12:15:34 UTC (16,640 KB)
    [v3] Sun, 2 Jun 2024 13:25:05 UTC (16,643 KB)
    Xinbei Ma, Zhuosheng Zhang, Hai Zhao

Multimodal large language models (MLLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation. However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response. We propose a Comprehensive Cognitive LLM Agent, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel. Second, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type. With our technical design, our agent achieves new state-of-the-art performance on AITW and META-GUI benchmarks, showing promising abilities in realistic scenarios. Code is available at this https URL.

------------

`[2405.12059] STYLE: Improving Domain Transferability of Asking Clarification Questions in Large Language Model Powered Conversational Agents <https://arxiv.org/abs/2405.12059>`__ 风格:提高在大型语言模型支持的会话代理中提出澄清性问题的领域可移植性

::

    replaced with revised version Sat, 1 Jun 2024 07:38:37 GMT
    Submission history From: Chen Huang [view email]
    [v1] Mon, 20 May 2024 14:28:25 UTC (9,991 KB)
    [v2] Sat, 1 Jun 2024 07:38:37 UTC (9,926 KB)
    Yue Chen, Chen Huang, Yang Deng, Wenqiang Lei, Dingnan Jin, Jia Liu, Tat-Seng Chua

Equipping a conversational search engine with strategies regarding when to ask clarification questions is becoming increasingly important across various domains. Attributing to the context understanding capability of LLMs and their access to domain-specific sources of knowledge, LLM-based clarification strategies feature rapid transfer to various domains in a post-hoc manner. However, they still struggle to deliver promising performance on unseen domains, struggling to achieve effective domain transferability. We take the first step to investigate this issue and existing methods tend to produce one-size-fits-all strategies across diverse domains, limiting their search effectiveness. In response, we introduce a novel method, called Style, to achieve effective domain transferability. Our experimental results indicate that Style bears strong domain transferability, resulting in an average search performance improvement of ~10% on four unseen domains.

------------

-----------
Other (167)
-----------

`[2406.00092] How Random is Random? Evaluating the Randomness and Humaness of LLMs' Coin Flips <https://arxiv.org/abs/2406.00092>`__ 有多随机才是随机?评估LLMs抛硬币的随机性和人性化

::

    Fri, 31 May 2024 17:56:07 GMT
    Katherine Van Koevering, Jon Kleinberg

One uniquely human trait is our inability to be random. We see and produce patterns where there should not be any and we do so in a predictable way. LLMs are supplied with human data and prone to human biases. In this work, we explore how LLMs approach randomness and where and how they fail through the lens of the well studied phenomena of generating binary random sequences. We find that GPT 4 and Llama 3 exhibit and exacerbate nearly every human bias we test in this context, but GPT 3.5 exhibits more random behavior. This dichotomy of randomness or humaness is proposed as a fundamental question of LLMs and that either behavior may be useful in different circumstances.

------------

`[2406.00765] The Embodied World Model Based on LLM with Visual Information and Prediction-Oriented Prompts <https://arxiv.org/abs/2406.00765>`__ 基于LLM的具有视觉信息和预测提示的具身世界模型

::

    Sun, 2 Jun 2024 14:50:01 GMT
    Wakana Haijima, Kou Nakakubo, Masahiro Suzuki, and Yutaka Matsuo

In recent years, as machine learning, particularly for vision and language understanding, has been improved, research in embedded AI has also evolved.
VOYAGER is a well-known LLM-based embodied AI that enables autonomous exploration in the Minecraft world, but it has issues such as underutilization of visual data and insufficient functionality as a world model. In this research, the possibility of utilizing visual data and the function of LLM as a world model were investigated with the aim of improving the performance of embodied AI. The experimental results revealed that LLM can extract necessary information from visual data, and the utilization of the information improves its performance as a world model. It was also suggested that devised prompts could bring out the LLM's function as a world model.

------------

`[2406.00018] Large Language Models' Detection of Political Orientation in Newspapers <https://arxiv.org/abs/2406.00018>`__ 大型语言模型对报纸政治倾向的识别

::

    Thu, 23 May 2024 06:18:03 GMT
    Alessio Buscemi and Daniele Proverbio

Democratic opinion-forming may be manipulated if newspapers' alignment to political or economical orientation is ambiguous. Various methods have been developed to better understand newspapers' positioning. Recently, the advent of Large Language Models (LLM), and particularly the pre-trained LLM chatbots like ChatGPT or Gemini, hold disruptive potential to assist researchers and citizens alike. However, little is know on whether LLM assessment is trustworthy: do single LLM agrees with experts' assessment, and do different LLMs answer consistently with one another? In this paper, we address specifically the second challenge. We compare how four widely employed LLMs rate the positioning of newspapers, and compare if their answers align with one another. We observe that this is not the case. Over a woldwide dataset, articles in newspapers are positioned strikingly differently by single LLMs, hinting to inconsistent training or excessive randomness in the algorithms. We thus raise a warning when deciding which tools to use, and we call for better training and algorithm development, to cover such significant gap in a highly sensitive matter for democracy and societies worldwide. We also call for community engagement in benchmark evaluation, through our open initiative navai.pro.

------------

`[2406.00020] Harmful Speech Detection by Language Models Exhibits Gender-Queer Dialect Bias <https://arxiv.org/abs/2406.00020>`__ 语言模型的有害语音检测表现出性别歧视的方言偏见

::

    Thu, 23 May 2024 18:07:28 GMT
    Rebecca Dorn, Lee Kezar, Fred Morstatter, Kristina Lerman

Content moderation on social media platforms shapes the dynamics of online discourse, influencing whose voices are amplified and whose are suppressed.
Recent studies have raised concerns about the fairness of content moderation practices, particularly for aggressively flagging posts from transgender and non-binary individuals as toxic. In this study, we investigate the presence of bias in harmful speech classification of gender-queer dialect online, focusing specifically on the treatment of reclaimed slurs. We introduce a novel dataset, QueerReclaimLex, based on 109 curated templates exemplifying non-derogatory uses of LGBTQ+ slurs. Dataset instances are scored by gender-queer annotators for potential harm depending on additional context about speaker identity. We systematically evaluate the performance of five off-the-shelf language models in assessing the harm of these texts and explore the effectiveness of chain-of-thought prompting to teach large language models (LLMs) to leverage author identity context. We reveal a tendency for these models to inaccurately flag texts authored by gender-queer individuals as harmful. Strikingly, across all LLMs the performance is poorest for texts that show signs of being written by individuals targeted by the featured slur (F1 <= 0.24). We highlight an urgent need for fairness and inclusivity in content moderation systems. By uncovering these biases, this work aims to inform the development of more equitable content moderation practices and contribute to the creation of inclusive online spaces for all users.

------------

`[2406.00024] Embedding-Aligned Language Models <https://arxiv.org/abs/2406.00024>`__ 嵌入对齐语言模型

::

    Fri, 24 May 2024 06:11:17 GMT
    Guy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Lior Shani, Ethan Liang, Craig Boutilier

We propose a novel approach for training large language models (LLMs) to adhere to objectives defined within a latent embedding space. Our method leverages reinforcement learning (RL), treating a pre-trained LLM as an environment. Our embedding-aligned guided language (EAGLE) agent is trained to iteratively steer the LLM's generation towards optimal regions of the latent embedding space, w.r.t. some predefined criterion. We demonstrate the effectiveness of the EAGLE agent using the MovieLens 25M dataset to surface content gaps that satisfy latent user demand. We also demonstrate the benefit of using an optimal design of a state-dependent action set to improve EAGLE's efficiency. Our work paves the way for controlled and grounded text generation using LLMs, ensuring consistency with domain-specific knowledge and data representations.

------------

`[2406.00025] SCALM: Towards Semantic Caching for Automated Chat Services with Large Language Models <https://arxiv.org/abs/2406.00025>`__ SCALM:基于大型语言模型的自动聊天服务语义缓存

::

    Fri, 24 May 2024 08:16:22 GMT
    Jiaxing Li, Chi Xu, Feng Wang, Isaac M von Riedemann, Cong Zhang, Jiangchuan Liu

Large Language Models (LLMs) have become increasingly popular, transforming a wide range of applications across various domains. However, the real-world effectiveness of their query cache systems has not been thoroughly investigated. In this work, we for the first time conducted an analysis on real-world human-to-LLM interaction data, identifying key challenges in existing caching solutions for LLM-based chat services. Our findings reveal that current caching methods fail to leverage semantic connections, leading to inefficient cache performance and extra token costs. To address these issues, we propose SCALM, a new cache architecture that emphasizes semantic analysis and identifies significant cache entries and patterns. We also detail the implementations of the corresponding cache storage and eviction strategies. Our evaluations show that SCALM increases cache hit ratios and reduces operational costs for LLMChat services. Compared with other state-of-the-art solutions in GPTCache, SCALM shows, on average, a relative increase of 63% in cache hit ratio and a relative improvement of 77% in tokens savings.

------------

`[2406.00027] Adapting PromptORE for Modern History: Information Extraction from Hispanic Monarchy Documents of the XVIth Century <https://arxiv.org/abs/2406.00027>`__ 为近代史改编的提示:从16世纪的西班牙君主制文件中提取信息

::

    Fri, 24 May 2024 13:39:47 GMT
    H\`ector Loopez Hidalgo, Michel Boeglin, David Kahn, Josiane Mothe, Diego Ortiz, David Panzoli

Semantic relations among entities are a widely accepted method for relation extraction. PromptORE (Prompt-based Open Relation Extraction) was designed to improve relation extraction with Large Language Models on generalistic documents. However, it is less effective when applied to historical documents, in languages other than English. In this study, we introduce an adaptation of PromptORE to extract relations from specialized documents, namely digital transcripts of trials from the Spanish Inquisition. Our approach involves fine-tuning transformer models with their pretraining objective on the data they will perform inference. We refer to this process as "biasing". Our Biased PromptORE addresses complex entity placements and genderism that occur in Spanish texts. We solve these issues by prompt engineering. We evaluate our method using Encoder-like models, corroborating our findings with experts' assessments. Additionally, we evaluate the performance using a binomial classification benchmark. Our results show a substantial improvement in accuracy -up to a 50% improvement with our Biased PromptORE models in comparison to the baseline models using standard PromptORE.

------------

`[2406.00030] Large Language Model Pruning <https://arxiv.org/abs/2406.00030>`__ 大型语言模型剪枝

::

    Fri, 24 May 2024 18:22:15 GMT
    Hanjuan Huang (1)(2), Hao-Jia Song (1) and Hsing-Kuo Pao (1) ((1) Dept. of Computer Science and Information Engineering National Taiwan University of Science and Technology, Taipei, Taiwan, (2) College of Mechanical and Electrical Engineering, WUYI University, Wuyishan, China)

We surely enjoy the larger the better models for their superior performance in the last couple of years when both the hardware and software support the birth of such extremely huge models. The applied fields include text mining and others. In particular, the success of LLMs on text understanding and text generation draws attention from researchers who have worked on NLP and related areas for years or even decades. On the side, LLMs may suffer from problems like model overfitting, hallucination, and device limitation to name a few. In this work, we suggest a model pruning technique specifically focused on LLMs.
The proposed methodology emphasizes the explainability of deep learning models.
By having the theoretical foundation, we obtain a trustworthy deep model so that huge models with a massive number of model parameters become not quite necessary. A mutual information-based estimation is adopted to find neurons with redundancy to eliminate. Moreover, an estimator with well-tuned parameters helps to find precise estimation to guide the pruning procedure. At the same time, we also explore the difference between pruning on large-scale models vs.
pruning on small-scale models. The choice of pruning criteria is sensitive in small models but not for large-scale models. It is a novel finding through this work. Overall, we demonstrate the superiority of the proposed model to the state-of-the-art models.

------------

`[2406.00031] AMGPT: a Large Language Model for Contextual Querying in Additive Manufacturing <https://arxiv.org/abs/2406.00031>`__ AMGPT:面向增材制造上下文查询的大型语言模型

::

    Fri, 24 May 2024 20:03:32 GMT
    Achuth Chandrasekhar, Jonathan Chan, Francis Ogoke, Olabode Ajenifujah, Amir Barati Farimani

Generalized large language models (LLMs) such as GPT-4 may not provide specific answers to queries formulated by materials science researchers. These models may produce a high-level outline but lack the capacity to return detailed instructions on manufacturing and material properties of novel alloys.
Enhancing a smaller model with specialized domain knowledge may provide an advantage over large language models which cannot be retrained quickly enough to keep up with the rapid pace of research in metal additive manufacturing (AM). We introduce "AMGPT," a specialized LLM text generator designed for metal AM queries. The goal of AMGPT is to assist researchers and users in navigating the extensive corpus of literature in AM. Instead of training from scratch, we employ a pre-trained Llama2-7B model from Hugging Face in a Retrieval-Augmented Generation (RAG) setup, utilizing it to dynamically incorporate information from $\sim$50 AM papers and textbooks in PDF format. Mathpix is used to convert these PDF documents into TeX format, facilitating their integration into the RAG pipeline managed by LlamaIndex. Expert evaluations of this project highlight that specific embeddings from the RAG setup accelerate response times and maintain coherence in the generated text.

------------

`[2406.00034] Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories <https://arxiv.org/abs/2406.00034>`__ 自适应激活转向:面向不同幻觉类别的无调谐LLM真实性改进方法

::

    Sun, 26 May 2024 21:39:53 GMT
    Tianlong Wang, Xianfeng Jiao, Yifan He, Zhongzhi Chen, Yinghao Zhu, Xu Chu, Junyi Gao, Yasha Wang, Liantao Ma

Recent studies have indicated that Large Language Models (LLMs) harbor an inherent understanding of truthfulness, yet often fail to express fully and generate false statements. This gap between "knowing" and "telling" poses a challenge for ensuring the truthfulness of generated content. To address this, we introduce Adaptive Activation Steering (ACT), a tuning-free method that adaptively shift LLM's activations in "truthful" direction during inference.
ACT addresses diverse categories of hallucinations by utilizing diverse steering vectors and adjusting the steering intensity adaptively. Applied as an add-on across various models, ACT significantly improves truthfulness in LLaMA ($\uparrow$ 142\%), LLaMA2 ($\uparrow$ 24\%), Alpaca ($\uparrow$ 36\%), Vicuna ($\uparrow$ 28\%), and LLaMA2-Chat ($\uparrow$ 19\%). Furthermore, we verify ACT's scalability across larger models (13B, 33B, 65B), underscoring the adaptability of ACT to large-scale language models.

------------

`[2406.00037] Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering <https://arxiv.org/abs/2406.00037>`__ 基于多角度用户偏好排序反馈的llm编程问答对齐

::

    Mon, 27 May 2024 14:21:31 GMT
    Hongyu Yang, Liyang He, Min Hou, Shuanghong Shen, Rui Li, Jiahui Hou, Jianhui Ma, Junda Zhao

Code Community Question Answering (CCQA) seeks to tackle programming-related issues, thereby boosting productivity in both software engineering and academic research. Recent advancements in Reinforcement Learning from Human Feedback (RLHF) have transformed the fine-tuning process of Large Language Models (LLMs) to produce responses that closely mimic human behavior. Leveraging LLMs with RLHF for practical CCQA applications has thus emerged as a promising area of study. Unlike standard code question-answering tasks, CCQA involves multiple possible answers, with varying user preferences for each response.
Additionally, code communities often show a preference for new APIs. These challenges prevent LLMs from generating responses that cater to the diverse preferences of users in CCQA tasks. To address these issues, we propose a novel framework called Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering (ALMupQA) to create user-focused responses. Our approach starts with Multi-perspective Preference Ranking Alignment (MPRA), which synthesizes varied user preferences based on the characteristics of answers from code communities. We then introduce a Retrieval-augmented In-context Learning (RIL) module to mitigate the problem of outdated answers by retrieving responses to similar questions from a question bank. Due to the limited availability of high-quality, multi-answer CCQA datasets, we also developed a dataset named StaCCQA from real code communities.
Extensive experiments demonstrated the effectiveness of the ALMupQA framework in terms of accuracy and user preference. Compared to the base model, ALMupQA showed nearly an 11% improvement in BLEU, with increases of 20% and 17.5% in BERTScore and CodeBERTScore, respectively.

------------

`[2406.00039] How Ready Are Generative Pre-trained Large Language Models for Explaining Bengali Grammatical Errors? <https://arxiv.org/abs/2406.00039>`__ 生成式预训练大型语言模型用于解释孟加拉语语法错误的准备情况如何?

::

    Mon, 27 May 2024 15:56:45 GMT
    Subhankar Maity, Aniket Deroy, Sudeshna Sarkar

Grammatical error correction (GEC) tools, powered by advanced generative artificial intelligence (AI), competently correct linguistic inaccuracies in user input. However, they often fall short in providing essential natural language explanations, which are crucial for learning languages and gaining a deeper understanding of the grammatical rules. There is limited exploration of these tools in low-resource languages such as Bengali. In such languages, grammatical error explanation (GEE) systems should not only correct sentences but also provide explanations for errors. This comprehensive approach can help language learners in their quest for proficiency. Our work introduces a real-world, multi-domain dataset sourced from Bengali speakers of varying proficiency levels and linguistic complexities. This dataset serves as an evaluation benchmark for GEE systems, allowing them to use context information to generate meaningful explanations and high-quality corrections. Various generative pre-trained large language models (LLMs), including GPT-4 Turbo, GPT-3.5 Turbo, Text-davinci-003, Text-babbage-001, Text-curie-001, Text-ada-001, Llama-2-7b, Llama-2-13b, and Llama-2-70b, are assessed against human experts for performance comparison. Our research underscores the limitations in the automatic deployment of current state-of-the-art generative pre-trained LLMs for Bengali GEE. Advocating for human intervention, our findings propose incorporating manual checks to address grammatical errors and improve feedback quality. This approach presents a more suitable strategy to refine the GEC tools in Bengali, emphasizing the educational aspect of language learning.

------------

`[2406.00041] QUB-Cirdan at "Discharge Me!": Zero shot discharge letter generation by open-source LLM <https://arxiv.org/abs/2406.00041>`__ QUB-Cirdan在“让我出院!”: Zero shot discharge letter generation by开源LLM

::

    Mon, 27 May 2024 17:55:36 GMT
    Rui Guo, Greg Farnan, Niall McLaughlin, Barry Devereux

The BioNLP ACL'24 Shared Task on Streamlining Discharge Documentation aims to reduce the administrative burden on clinicians by automating the creation of critical sections of patient discharge letters. This paper presents our approach using the Llama3 8B quantized model to generate the "Brief Hospital Course" and "Discharge Instructions" sections. We employ a zero-shot method combined with Retrieval-Augmented Generation (RAG) to produce concise, contextually accurate summaries. Our contributions include the development of a curated template-based approach to ensure reliability and consistency, as well as the integration of RAG for word count prediction. We also describe several unsuccessful experiments to provide insights into our pathway for the competition. Our results demonstrate the effectiveness and efficiency of our approach, achieving high scores across multiple evaluation metrics.

------------

`[2406.00045] Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization <https://arxiv.org/abs/2406.00045>`__ 大型语言模型的个性化转向:基于双向偏好优化的通用转向向量

::

    Tue, 28 May 2024 05:10:40 GMT
    Yuanpu Cao, Tianrong Zhang, Bochuan Cao, Ziyi Yin, Lu Lin, Fenglong Ma, Jinghui Chen

Researchers have been studying approaches to steer the behavior of Large Language Models (LLMs) and build personalized LLMs tailored for various applications. While fine-tuning seems to be a direct solution, it requires substantial computational resources and may significantly affect the utility of the original LLM. Recent endeavors have introduced more lightweight strategies, focusing on extracting "steering vectors" to guide the model's output toward desired behaviors by adjusting activations within specific layers of the LLM's transformer architecture. However, such steering vectors are directly extracted from the activations of human preference data and thus often lead to suboptimal results and occasional failures, especially in alignment-related scenarios.
This work proposes an innovative approach that could produce more effective steering vectors through bi-directional preference optimization. Our method is designed to allow steering vectors to directly influence the generation probability of contrastive human preference data pairs, thereby offering a more precise representation of the target behavior. By carefully adjusting the direction and magnitude of the steering vector, we enabled personalized control over the desired behavior across a spectrum of intensities. Extensive experimentation across various open-ended generation tasks, particularly focusing on steering AI personas, has validated the efficacy of our approach.
Moreover, we comprehensively investigate critical alignment-concerning scenarios, such as managing truthfulness, mitigating hallucination, and addressing jailbreaking attacks. Remarkably, our method can still demonstrate outstanding steering effectiveness across these scenarios. Furthermore, we showcase the transferability of our steering vectors across different models/LoRAs and highlight the synergistic benefits of applying multiple vectors simultaneously.

------------

`[2406.00049] QUEST: Quality-Aware Metropolis-Hastings Sampling for Machine Translation <https://arxiv.org/abs/2406.00049>`__ QUEST:面向机器翻译的质量感知Metropolis-Hastings采样

::

    Tue, 28 May 2024 17:36:06 GMT
    Gon\c{c}alo R. A. Faria, Sweta Agrawal, Ant\'onio Farinhas, Ricardo Rei, Jos\'e G. C. de Souza, Andr\'e F. T. Martins

An important challenge in machine translation (MT) is to generate high-quality and diverse translations. Prior work has shown that the estimated likelihood from the MT model correlates poorly with translation quality. In contrast, quality evaluation metrics (such as COMET or BLEURT) exhibit high correlations with human judgments, which has motivated their use as rerankers (such as quality-aware and minimum Bayes risk decoding). However, relying on a single translation with high estimated quality increases the chances of "gaming the metric''. In this paper, we address the problem of sampling a set of high-quality and diverse translations. We provide a simple and effective way to avoid over-reliance on noisy quality estimates by using them as the energy function of a Gibbs distribution. Instead of looking for a mode in the distribution, we generate multiple samples from high-density areas through the Metropolis-Hastings algorithm, a simple Markov chain Monte Carlo approach. The results show that our proposed method leads to high-quality and diverse outputs across multiple language pairs (English$\leftrightarrow${German, Russian}) with two strong decoder-only LLMs (Alma-7b, Tower-7b).

------------

`[2406.00050] An Empirical Analysis on Large Language Models in Debate Evaluation <https://arxiv.org/abs/2406.00050>`__ 大型语言模型在辩论评估中的实证分析

::

    Tue, 28 May 2024 18:34:53 GMT
    Xinyi Liu, Pinxin Liu, Hangfeng He

In this study, we investigate the capabilities and inherent biases of advanced large language models (LLMs) such as GPT-3.5 and GPT-4 in the context of debate evaluation. We discover that LLM's performance exceeds humans and surpasses the performance of state-of-the-art methods fine-tuned on extensive datasets in debate evaluation. We additionally explore and analyze biases present in LLMs, including positional bias, lexical bias, order bias, which may affect their evaluative judgments. Our findings reveal a consistent bias in both GPT-3.5 and GPT-4 towards the second candidate response presented, attributed to prompt design. We also uncover lexical biases in both GPT-3.5 and GPT-4, especially when label sets carry connotations such as numerical or sequential, highlighting the critical need for careful label verbalizer selection in prompt design. Additionally, our analysis indicates a tendency of both models to favor the debate's concluding side as the winner, suggesting an end-of-discussion bias.

------------

`[2406.00062] Unlocking the Potential of Large Language Models for Clinical Text Anonymization: A Comparative Study <https://arxiv.org/abs/2406.00062>`__ 释放临床文本匿名的大型语言模型的潜力:比较研究

::

    Wed, 29 May 2024 23:07:58 GMT
    David Pissarra, Isabel Curioso, Jo\~ao Alveira, Duarte Pereira, Bruno Ribeiro, Tom\'as Souper, Vasco Gomes, Andr\'e V. Carreiro, Vitor Rolla

Automated clinical text anonymization has the potential to unlock the widespread sharing of textual health data for secondary usage while assuring patient privacy and safety. Despite the proposal of many complex and theoretically successful anonymization solutions in literature, these techniques remain flawed. As such, clinical institutions are still reluctant to apply them for open access to their data. Recent advances in developing Large Language Models (LLMs) pose a promising opportunity to further the field, given their capability to perform various tasks. This paper proposes six new evaluation metrics tailored to the challenges of generative anonymization with LLMs. Moreover, we present a comparative study of LLM-based methods, testing them against two baseline techniques. Our results establish LLM-based models as a reliable alternative to common approaches, paving the way toward trustworthy anonymization of clinical text.

------------

`[2406.00069] Confidence-Aware Sub-Structure Beam Search (CABS): Mitigating Hallucination in Structured Data Generation with Large Language Models <https://arxiv.org/abs/2406.00069>`__ 置信度感知的子结构波束搜索(CABS):用大型语言模型缓解结构化数据生成中的幻觉

::

    Thu, 30 May 2024 18:21:05 GMT
    Chengwei Wei, Kee Kiat Koo, Amir Tavanaei, Karim Bouyarmane

Large Language Models (LLMs) have facilitated structured data generation, with applications in domains like tabular data, document databases, product catalogs, etc. However, concerns persist about generation veracity due to incorrect references or hallucinations, necessitating the incorporation of some form of model confidence for mitigation. Existing confidence estimation methods on LLM generations primarily focus on the confidence at the individual token level or the entire output sequence level, limiting their applicability to structured data generation, which consists of an intricate mix of both independent and correlated entries at the sub-structure level. In this paper, we first investigate confidence estimation methods for generated sub-structure-level data. We introduce the concept of Confidence Network that applies on the hidden state of the LLM transformer, as a more targeted estimate than the traditional token conditional probability. We further propose Confidence-Aware sub-structure Beam Search (CABS), a novel decoding method operating at the sub-structure level in structured data generation. CABS enhances the faithfulness of structured data generation by considering confidence scores from the Confidence Network for each sub-structure-level data and iteratively refining the prompts. Results show that CABS outperforms traditional token-level beam search for structured data generation by 16.7% Recall at 90% precision averagely on the problem of product attribute generation.

------------

`[2406.00179] Long-Span Question-Answering: Automatic Question Generation and QA-System Ranking via Side-by-Side Evaluation <https://arxiv.org/abs/2406.00179>`__ 长跨度问答:基于并列评价的问题自动生成和问答系统排序

::

    Fri, 31 May 2024 20:15:10 GMT
    Bernd Bohnet, Kevin Swersky, Rosanne Liu, Pranjal Awasthi, Azade Nova, Javier Snaider, Hanie Sedghi, Aaron T Parisi, Michael Collins, Angeliki Lazaridou, Orhan Firat, Noah Fiedel

We explore the use of long-context capabilities in large language models to create synthetic reading comprehension data from entire books. Previous efforts to construct such datasets relied on crowd-sourcing, but the emergence of transformers with a context size of 1 million or more tokens now enables entirely automatic approaches. Our objective is to test the capabilities of LLMs to analyze, understand, and reason over problems that require a detailed comprehension of long spans of text, such as questions involving character arcs, broader themes, or the consequences of early actions later in the story.
We propose a holistic pipeline for automatic data generation including question generation, answering, and model scoring using an ``Evaluator''. We find that a relative approach, comparing answers between models in a pairwise fashion and ranking with a Bradley-Terry model, provides a more consistent and differentiating scoring mechanism than an absolute scorer that rates answers individually. We also show that LLMs from different model families produce moderate agreement in their ratings. We ground our approach using the manually curated NarrativeQA dataset, where our evaluator shows excellent agreement with human judgement and even finds errors in the dataset. Using our automatic evaluation approach, we show that using an entire book as context produces superior reading comprehension performance compared to baseline no-context (parametric knowledge only) and retrieval-based approaches.

------------

`[2406.00197] Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision <https://arxiv.org/abs/2406.00197>`__ 协同文档修订建模的整体框架和数据集

::

    Fri, 31 May 2024 21:19:09 GMT
    Qian Ruan, Ilia Kuznetsov, Iryna Gurevych

Collaborative review and revision of textual documents is the core of knowledge work and a promising target for empirical analysis and NLP assistance. Yet, a holistic framework that would allow modeling complex relationships between document revisions, reviews and author responses is lacking. To address this gap, we introduce Re3, a framework for joint analysis of collaborative document revision. We instantiate this framework in the scholarly domain, and present Re3-Sci, a large corpus of aligned scientific paper revisions manually labeled according to their action and intent, and supplemented with the respective peer reviews and human-written edit summaries.
We use the new data to provide first empirical insights into collaborative document revision in the academic domain, and to assess the capabilities of state-of-the-art LLMs at automating edit analysis and facilitating text-based collaboration. We make our annotation environment and protocols, the resulting data and experimental code publicly available.

------------

`[2406.00222] Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training <https://arxiv.org/abs/2406.00222>`__ 学习阐明:基于动作对比自我训练的多轮对话

::

    Fri, 31 May 2024 22:44:48 GMT
    Maximillian Chen and Ruoxi Sun and Sercan \"O. Ar{\i}k and Tomas Pfister

Large language models (LLMs) aligned through reinforcement learning from human feedback (RLHF) have quickly become one of the dominant paradigms for building intelligent conversational assistant agents. However, despite their strong performance across many benchmarks, LLM-based agents still lack conversational skills such as disambiguation: when generalized assistants are faced with ambiguity, they often overhedge or implicitly guess users' ground-truth intents rather than asking clarification questions, and under task-specific settings, high-quality conversation samples are often limited, affecting models' ability to learn optimal dialogue action policies. We propose Action-Based Contrastive Self-Training (henceforth ACT), a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO) which allows for sample-efficient dialogue policy learning in multi-turn conversation. We demonstrate ACT's efficacy under sample-efficient conditions in three difficult conversational tasks: tabular-grounded question-answering, machine reading comprehension, and AmbigSQL, a novel task for disambiguating information-seeking requests for text-to-SQL generation. Additionally, we propose evaluating LLMs' ability to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard approaches to supervised fine-tuning and DPO.

------------

`[2406.00343] Beyond Metrics: Evaluating LLMs' Effectiveness in Culturally Nuanced, Low-Resource Real-World Scenarios <https://arxiv.org/abs/2406.00343>`__ 超越指标:评估llm在文化细微差别、低资源现实世界场景中的有效性

::

    Sat, 1 Jun 2024 07:36:59 GMT
    Millicent Ochieng, Varun Gumma, Sunayana Sitaram, Jindong Wang, Keshet Ronen, Kalika Bali, Jacki O'Neill

The deployment of Large Language Models (LLMs) in real-world applications presents both opportunities and challenges, particularly in multilingual and code-mixed communication settings. This research evaluates the performance of seven leading LLMs in sentiment analysis on a dataset derived from multilingual and code-mixed WhatsApp chats, including Swahili, English and Sheng. Our evaluation includes both quantitative analysis using metrics like F1 score and qualitative assessment of LLMs' explanations for their predictions. We find that, while Mistral-7b and Mixtral-8x7b achieved high F1 scores, they and other LLMs such as GPT-3.5-Turbo, Llama-2-70b, and Gemma-7b struggled with understanding linguistic and contextual nuances, as well as lack of transparency in their decision-making process as observed from their explanations. In contrast, GPT-4 and GPT-4-Turbo excelled in grasping diverse linguistic inputs and managing various contextual information, demonstrating high consistency with human alignment and transparency in their decision-making process. The LLMs however, encountered difficulties in incorporating cultural nuance especially in non-English settings with GPT-4s doing so inconsistently.
The findings emphasize the necessity of continuous improvement of LLMs to effectively tackle the challenges of culturally nuanced, low-resource real-world settings.

------------

`[2406.00380] The Best of Both Worlds: Toward an Honest and Helpful Large Language Model <https://arxiv.org/abs/2406.00380>`__ 两全其美:建立一个诚实而有用的大型语言模型

::

    Sat, 1 Jun 2024 09:36:16 GMT
    Chujie Gao, Qihui Zhang, Dongping Chen, Yue Huang, Siyuan Wu, Zhengyan Fu, Yao Wan, Xiangliang Zhang, Lichao Sun

Large Language Models (LLMs) have achieved remarkable success across various industries due to their exceptional generative capabilities. However, for safe and effective real-world deployments, ensuring honesty and helpfulness is critical. This paper addresses the question: Can we prioritize the helpfulness of LLMs while preserving their honesty? To begin with, we establish exhaustive principles aimed at guaranteeing the honesty of LLM. Additionally, we introduce a novel dataset, referred to as HoneSet, comprising 930 queries spanning six categories meticulously crafted to assess an LLM's capacity for maintaining honesty. Subsequently, we present two approaches to augmenting honesty and helpfulness in LLMs: a training-free enhancement and a fine-tuning-based improvement. The training-free approach, which is based on curiosity-driven prompting, empowers LLMs to articulate internal confusion and uncertainty regarding queries, thereby optimizing their responses. Conversely, the fine-tuning-based method employs a two-stage process inspired by curriculum learning: initially instructing LLMs to discern between honest and dishonest responses, then refining their training to enhance helpfulness. Experiments conducted on nine prominent LLMs demonstrate a significant improvement in alignment with honesty across all models through the implementation of our proposed enhancements. Particularly noteworthy is the 65.3% enhancement observed in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as measured by the H$^{2}$ (honest and helpful) assessment. We believe that our work can pave the way for developing more trustworthy LLMs for real-world applications.

------------

`[2406.00507] Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization <https://arxiv.org/abs/2406.00507>`__ 提示链还是逐步提示?文本摘要中的精炼

::

    Sat, 1 Jun 2024 17:28:38 GMT
    Shichao Sun, Ruifeng Yuan, Ziqiang Cao, Wenjie Li, Pengfei Liu

Large language models (LLMs) have demonstrated the capacity to improve summary quality by mirroring a human-like iterative process of critique and refinement starting from the initial draft. Two strategies are designed to perform this iterative process: Prompt Chaining and Stepwise Prompt. Prompt chaining orchestrates the drafting, critiquing, and refining phases through a series of three discrete prompts, while Stepwise prompt integrates these phases within a single prompt. However, the relative effectiveness of the two methods has not been extensively studied. This paper is dedicated to examining and comparing these two methods in the context of text summarization to ascertain which method stands out as the most effective. Experimental results show that the prompt chaining method can produce a more favorable outcome. This might be because stepwise prompt might produce a simulated refinement process according to our various experiments. Since refinement is adaptable to diverse tasks, our conclusions have the potential to be extrapolated to other applications, thereby offering insights that may contribute to the broader development of LLMs.

------------

`[2406.00554] Guiding and Diversifying LLM-Based Story Generation via Answer Set Programming <https://arxiv.org/abs/2406.00554>`__ 通过答案集编程引导和多样化基于llm的故事生成

::

    Sat, 1 Jun 2024 21:14:25 GMT
    Phoebe J. Wang, Max Kreminski

Instruction-tuned large language models (LLMs) are capable of generating stories in response to open-ended user requests, but the resulting stories tend to be limited in their diversity. Older, symbolic approaches to story generation (such as planning) can generate substantially more diverse plot outlines, but are limited to producing stories that recombine a fixed set of hand-engineered character action templates. Can we combine the strengths of these approaches while mitigating their weaknesses? We propose to do so by using a higher-level and more abstract symbolic specification of high-level story structure -- implemented via answer set programming (ASP) -- to guide and diversify LLM-based story generation. Via semantic similarity analysis, we demonstrate that our approach produces more diverse stories than an unguided LLM, and via code excerpts, we demonstrate the improved compactness and flexibility of ASP-based outline generation over full-fledged narrative planning.

------------

`[2406.00606] LLMs Could Autonomously Learn Without External Supervision <https://arxiv.org/abs/2406.00606>`__ llm可以在没有外部监督的情况下自主学习

::

    Sun, 2 Jun 2024 03:36:37 GMT
    Ke Ji and Junying Chen and Anningzhe Gao and Wenya Xie and Xiang Wan and Benyou Wang

In the quest for super-human performance, Large Language Models (LLMs) have traditionally been tethered to human-annotated datasets and predefined training objectives-a process that is both labor-intensive and inherently limited. This paper presents a transformative approach: Autonomous Learning for LLMs, a self-sufficient learning paradigm that frees models from the constraints of human supervision. This method endows LLMs with the ability to self-educate through direct interaction with text, akin to a human reading and comprehending literature. Our approach eliminates the reliance on annotated data, fostering an Autonomous Learning environment where the model independently identifies and reinforces its knowledge gaps. Empirical results from our comprehensive experiments, which utilized a diverse array of learning materials and were evaluated against standard public quizzes, reveal that Autonomous Learning outstrips the performance of both Pre-training and Supervised Fine-Tuning (SFT), as well as retrieval-augmented methods. These findings underscore the potential of Autonomous Learning to not only enhance the efficiency and effectiveness of LLM training but also to pave the way for the development of more advanced, self-reliant AI systems.

------------

`[2406.00627] Prompt Framework for Role-playing: Generation and Evaluation <https://arxiv.org/abs/2406.00627>`__ 角色扮演提示框架:生成与评估

::

    Sun, 2 Jun 2024 06:09:56 GMT
    Xun Liu, Zhengwei Ni

Large language models (LLM) have demonstrated remarkable abilities in generating natural language, understanding user instruction, and mimicking human language use. These capabilities have garnered considerable interest in applications such as role-playing. However, the process of collecting individual role scripts (or profiles) data and manually evaluating the performance can be costly. We introduce a framework that uses prompts to leverage the state-of-the-art (SOTA) LLMs to construct role-playing dialogue datasets and evaluate the role-playing performance. Additionally, we employ recall-oriented evaluation Rouge-L metric to support the result of the LLM evaluator.

------------

`[2406.00628] Transforming Computer Security and Public Trust Through the Exploration of Fine-Tuning Large Language Models <https://arxiv.org/abs/2406.00628>`__ 通过微调大型语言模型的探索来转变计算机安全和公共信任

::

    Sun, 2 Jun 2024 06:10:31 GMT
    Garrett Crumrine, Izzat Alsmadi, Jesus Guerrero, Yuvaraj Munian

Large language models (LLMs) have revolutionized how we interact with machines. However, this technological advancement has been paralleled by the emergence of "Mallas," malicious services operating underground that exploit LLMs for nefarious purposes. Such services create malware, phishing attacks, and deceptive websites, escalating the cyber security threats landscape. This paper delves into the proliferation of Mallas by examining the use of various pre-trained language models and their efficiency and vulnerabilities when misused. Building on a dataset from the Common Vulnerabilities and Exposures (CVE) program, it explores fine-tuning methodologies to generate code and explanatory text related to identified vulnerabilities. This research aims to shed light on the operational strategies and exploitation techniques of Mallas, leading to the development of more secure and trustworthy AI applications. The paper concludes by emphasizing the need for further research, enhanced safeguards, and ethical guidelines to mitigate the risks associated with the malicious application of LLMs.

------------

`[2406.00656] Presence or Absence: Are Unknown Word Usages in Dictionaries? <https://arxiv.org/abs/2406.00656>`__ 存在还是缺席:词典中未登录词的用法?

::

    Sun, 2 Jun 2024 07:57:45 GMT
    Xianghe Ma, Dominik Schlechtweg, Wei Zhao

In this work, we outline the components and results of our system submitted to the AXOLOTL-24 shared task for Finnish, Russian and German languages. Our system is fully unsupervised. It leverages a graph-based clustering approach to predict mappings between unknown word usages and dictionary entries for Subtask 1, and generates dictionary-like definitions for those novel word usages through the state-of-the-art Large Language Models such as GPT-4 and LLaMA-3 for Subtask 2. In Subtask 1, our system outperforms the baseline system by a large margin, and it offers interpretability for the mapping results by distinguishing between matched and unmatched (novel) word usages through our graph-based clustering approach. Our system ranks first in Finnish and German, and ranks second in Russian on the Subtask 2 test-phase leaderboard. These results show the potential of our system in managing dictionary entries, particularly for updating dictionaries to include novel sense entries. Our code and data are made publicly available\footnote{\url{https://github.com/xiaohemaikoo/axolotl24-ABDN-NLP}}.

------------

`[2406.00697] Topic Modeling for Short Texts with Large Language Models <https://arxiv.org/abs/2406.00697>`__ 基于大型语言模型的短文本主题建模

::

    Sun, 2 Jun 2024 10:25:02 GMT
    Tomoki Doi, Masaru Isonuma, Hitomi Yanaka

As conventional topic models rely on word co-occurrence to infer latent topics, topic modeling for short texts has been a long-standing challenge.
Large Language Models (LLMs) can potentially overcome this challenge by contextually learning the semantics of words via pretraining. This paper studies two approaches, parallel prompting and sequential prompting, to use LLMs for topic modeling. Due to the input length limitations, LLMs cannot process many texts at once. By splitting the texts into smaller subsets and processing them parallelly or sequentially, an arbitrary number of texts can be handled by LLMs. Experimental results demonstrated that our methods can identify more coherent topics than existing ones while maintaining the diversity of the induced topics. Furthermore, we found that the inferred topics adequately covered the input texts, while hallucinated topics were hardly generated.

------------

`[2406.00770] Automatic Instruction Evolving for Large Language Models <https://arxiv.org/abs/2406.00770>`__ 大型语言模型的自动指令演化

::

    Sun, 2 Jun 2024 15:09:00 GMT
    Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, Weizhu Chen

Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval.

------------

`[2406.00832] BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling <https://arxiv.org/abs/2406.00832>`__ 大型语言模型的BoNBoN对齐和n最佳抽样的甜蜜

::

    Sun, 2 Jun 2024 18:42:57 GMT
    Lin Gui, Cristina G\^arbacea, Victor Veitch

This paper concerns the problem of aligning samples from large language models to human preferences using best-of-$n$ sampling, where we draw $n$ samples, rank them, and return the best one. We consider two fundamental problems. First: what is the relationship between best-of-$n$ and approaches to alignment that train LLMs to output samples with a high expected reward (e.g., RLHF or DPO)? To answer this, we embed both the best-of-$n$ distribution and the sampling distributions learned by alignment procedures in a common class of tiltings of the base LLM distribution. We then show that, within this class, best-of-$n$ is essentially optimal in terms of the trade-off between win-rate against the base model vs KL distance from the base model. That is, best-of-$n$ is the best choice of alignment distribution if the goal is to maximize win rate. However, best-of-$n$ requires drawing $n$ samples for each inference, a substantial cost. To avoid this, the second problem we consider is how to fine-tune a LLM to mimic the best-of-$n$ sampling distribution. We derive BoNBoN Alignment to achieve this by exploiting the special structure of the best-of-$n$ distribution. Experiments show that BoNBoN alignment yields substantial improvements in producing a model that is preferred to the base policy while minimally affecting off-target aspects.

------------

`[2406.00839] FOCUS: Forging Originality through Contrastive Use in Self-Plagiarism for Language Models <https://arxiv.org/abs/2406.00839>`__ 重点:通过对比使用语言模型的自抄袭来锻造独创性

::

    Sun, 2 Jun 2024 19:17:00 GMT
    Kaixin Lan, Tao Fang, Derek F. Wong, Yabo Xu, Lidia S. Chao, and Cecilia G. Zhao

Pre-trained Language Models (PLMs) have shown impressive results in various Natural Language Generation (NLG) tasks, such as powering chatbots and generating stories. However, an ethical concern arises due to their potential to produce verbatim copies of paragraphs from their training data. This is problematic as PLMs are trained on corpora constructed by human authors. As such, there is a pressing need for research to promote the generation of original content by these models. In this study, we introduce a unique "self-plagiarism" contrastive decoding strategy, aimed at boosting the originality of text produced by PLMs. Our method entails modifying prompts in LLMs to develop an amateur model and a professional model. Specifically, the amateur model is urged to plagiarize using three plagiarism templates we have designed, while the professional model maintains its standard language model status. This strategy employs prompts to stimulate the model's capacity to identify non-original candidate token combinations and subsequently impose penalties. The application of this strategy is integrated prior to the model's final layer, ensuring smooth integration with most existing PLMs (T5, GPT, LLaMA) without necessitating further adjustments. Implementing our strategy, we observe a significant decline in non-original sequences comprised of more than three words in the academic AASC dataset and the story-based ROCStories dataset.

------------

`[2406.00888] Show, Don't Tell: Aligning Language Models with Demonstrated Feedback <https://arxiv.org/abs/2406.00888>`__ 展示，而不是告诉:将语言模型与演示反馈相对齐

::

    Sun, 2 Jun 2024 23:13:56 GMT
    Omar Shaikh, Michelle Lam, Joey Hejna, Yijia Shao, Michael Bernstein, Diyi Yang

Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number ($<10$) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. We evaluate DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts.
Additionally, we conduct a user study soliciting a range of demonstrations from participants ($N=16$). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an average of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.

------------

`[2406.00954] Annotation Guidelines-Based Knowledge Augmentation: Towards Enhancing Large Language Models for Educational Text Classification <https://arxiv.org/abs/2406.00954>`__ 基于标注指南的知识增强:面向教育文本分类的大型语言模型增强

::

    Mon, 3 Jun 2024 03:09:01 GMT
    Shiqi Liu, Sannyuya Liu, Lele Sha, Zijie Zeng, Dragan Gasevic, Zhi Liu

Various machine learning approaches have gained significant popularity for the automated classification of educational text to identify indicators of learning engagement -- i.e. learning engagement classification (LEC). LEC can offer comprehensive insights into human learning processes, attracting significant interest from diverse research communities, including Natural Language Processing (NLP), Learning Analytics, and Educational Data Mining.
Recently, Large Language Models (LLMs), such as ChatGPT, have demonstrated remarkable performance in various NLP tasks. However, their comprehensive evaluation and improvement approaches in LEC tasks have not been thoroughly investigated. In this study, we propose the Annotation Guidelines-based Knowledge Augmentation (AGKA) approach to improve LLMs. AGKA employs GPT 4.0 to retrieve label definition knowledge from annotation guidelines, and then applies the random under-sampler to select a few typical examples.
Subsequently, we conduct a systematic evaluation benchmark of LEC, which includes six LEC datasets covering behavior classification (question and urgency level), emotion classification (binary and epistemic emotion), and cognition classification (opinion and cognitive presence). The study results demonstrate that AGKA can enhance non-fine-tuned LLMs, particularly GPT 4.0 and Llama 3 70B. GPT 4.0 with AGKA few-shot outperforms full-shot fine-tuned models such as BERT and RoBERTa on simple binary classification datasets. However, GPT 4.0 lags in multi-class tasks that require a deep understanding of complex semantic information. Notably, Llama 3 70B with AGKA is a promising combination based on open-source LLM, because its performance is on par with closed-source GPT 4.0 with AGKA. In addition, LLMs struggle to distinguish between labels with similar names in multi-class classification.

------------

`[2406.00969] Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media <https://arxiv.org/abs/2406.00969>`__ 使用RL来识别分裂的观点，提高了llm在社交媒体上识别社区的能力

::

    Mon, 3 Jun 2024 03:45:31 GMT
    Nikhil Mehta and Dan Goldwasser

The large scale usage of social media, combined with its significant impact, has made it increasingly important to understand it. In particular, identifying user communities, can be helpful for many downstream tasks. However, particularly when models are trained on past data and tested on future, doing this is difficult.
In this paper, we hypothesize to take advantage of Large Language Models (LLMs), to better identify user communities. Due to the fact that many LLMs, such as ChatGPT, are fixed and must be treated as black-boxes, we propose an approach to better prompt them, by training a smaller LLM to do this. We devise strategies to train this smaller model, showing how it can improve the larger LLMs ability to detect communities. Experimental results show improvements on Reddit and Twitter data, on the tasks of community detection, bot detection, and news media profiling.

------------

`[2406.00975] Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost <https://arxiv.org/abs/2406.00975>`__ Luna:一种高精度低成本捕捉语言模型幻觉的评估基础模型

::

    Mon, 3 Jun 2024 04:14:21 GMT
    Masha Belyi, Robert Friel, Shuai Shao, Atindriyo Sanyal

Retriever Augmented Generation (RAG) systems have become pivotal in enhancing the capabilities of language models by incorporating external knowledge retrieval mechanisms. However, a significant challenge in deploying these systems in industry applications is the detection and mitigation of hallucinations: instances where the model generates information that is not grounded in the retrieved context. Addressing this issue is crucial for ensuring the reliability and accuracy of responses generated by large language models (LLMs) in diverse industry settings. Current hallucination detection techniques fail to deliver accuracy, low latency, and low cost simultaneously.
We introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination detection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and commercial evaluation frameworks on the hallucination detection task, with 97% and 96% reduction in cost and latency, respectively. Luna is lightweight and generalizes across multiple industry verticals and out-of-domain data, making it an ideal candidate for industry LLM applications.

------------

`[2406.01006] SemCoder: Training Code Language Models with Comprehensive Semantics <https://arxiv.org/abs/2406.01006>`__ SemCoder:具有全面语义的代码语言模型训练

::

    Mon, 3 Jun 2024 05:36:57 GMT
    Yangruibo Ding, Jinjun Peng, Marcus J. Min, Gail Kaiser, Junfeng Yang, Baishakhi Ray

Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states. This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for thorough semantic understanding for complex tasks like debugging and program repair. We introduce a novel strategy to train Code LLMs with comprehensive semantics, encompassing high-level functional descriptions, local execution effects of individual statements, and overall input/output behavior, thereby linking static code text with dynamic execution states. We begin by collecting PyX, a clean code corpus of fully executable samples with functional descriptions and execution tracing. We propose training Code LLMs to write code and represent and reason about execution behaviors using natural language, mimicking human verbal debugging.
This approach led to the development of SemCoder, a Code LLM with only 6.7B parameters, which shows competitive performance with GPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder achieves 81.1% on HumanEval (GPT-3.5-turbo: 76.8%) and 54.5% on CRUXEval-I (GPT-3.5-turbo: 50.3%). We also study the effectiveness of SemCoder's monologue-style execution reasoning compared to concrete scratchpad reasoning, showing that our approach integrates semantics from multiple dimensions more smoothly. Finally, we demonstrate the potential of applying learned semantics to improve Code LLMs' debugging and self-refining capabilities.

------------

`[2406.01026] Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors <https://arxiv.org/abs/2406.01026>`__ 加强的符号绑定使大型语言模型成为可靠的多项选择器

::

    Mon, 3 Jun 2024 06:20:12 GMT
    Mengge Xue, Zhenyu Hu, Meng Zhao, Liqun Liu, Kuo Liao, Shuang Li, Honglin Han, Chengguo Yin

Multiple-Choice Questions (MCQs) constitute a critical area of research in the study of Large Language Models (LLMs). Previous works have investigated the selection bias problem in MCQs within few-shot scenarios, in which the LLM's performance may be influenced by the presentation of answer choices, leaving the selection bias during Supervised Fine-Tuning (SFT) unexplored. In this paper, we reveal that selection bias persists in the SFT phase , primarily due to the LLM's inadequate Multiple Choice Symbol Binding (MCSB) ability. This limitation implies that the model struggles to associate the answer options with their corresponding symbols (e.g., A/B/C/D) effectively. To enhance the model's MCSB capability, we first incorporate option contents into the loss function and subsequently adjust the weights of the option symbols and contents, guiding the model to understand the option content of the current symbol. Based on this, we introduce an efficient SFT algorithm for MCQs, termed Point-wise Intelligent Feedback (PIF). PIF constructs negative instances by randomly combining the incorrect option contents with all candidate symbols, and proposes a point-wise loss to provide feedback on these negative samples into LLMs. Our experimental results demonstrate that PIF significantly reduces the model's selection bias by improving its MCSB capability. Remarkably, PIF exhibits a substantial enhancement in the accuracy for MCQs.

------------

`[2406.01045] Decompose, Enrich, and Extract! Schema-aware Event Extraction using LLMs <https://arxiv.org/abs/2406.01045>`__ 分解、丰富和提取!模式感知的事件提取使用llm

::

    Mon, 3 Jun 2024 06:55:10 GMT
    Fatemeh Shiri, Van Nguyen, Farhad Moghimifar, John Yoo, Gholamreza Haffari, Yuan-Fang Li

Large Language Models (LLMs) demonstrate significant capabilities in processing natural language data, promising efficient knowledge extraction from diverse textual sources to enhance situational awareness and support decision-making. However, concerns arise due to their susceptibility to hallucination, resulting in contextually inaccurate content. This work focuses on harnessing LLMs for automated Event Extraction, introducing a new method to address hallucination by decomposing the task into Event Detection and Event Argument Extraction. Moreover, the proposed method integrates dynamic schema-aware augmented retrieval examples into prompts tailored for each specific inquiry, thereby extending and adapting advanced prompting techniques such as Retrieval-Augmented Generation. Evaluation findings on prominent event extraction benchmarks and results from a synthesized benchmark illustrate the method's superior performance compared to baseline approaches.

------------

`[2406.01179] Are AI-Generated Text Detectors Robust to Adversarial Perturbations? <https://arxiv.org/abs/2406.01179>`__ 人工智能生成的文本检测器对对抗性扰动鲁棒吗?

::

    Mon, 3 Jun 2024 10:21:48 GMT
    Guanhua Huang, Yuchen Zhang, Zhe Li, Yongjian You, Mingze Wang, Zhouwang Yang

The widespread use of large language models (LLMs) has sparked concerns about the potential misuse of AI-generated text, as these models can produce content that closely resembles human-generated text. Current detectors for AI-generated text (AIGT) lack robustness against adversarial perturbations, with even minor changes in characters or words causing a reversal in distinguishing between human-created and AI-generated text. This paper investigates the robustness of existing AIGT detection methods and introduces a novel detector, the Siamese Calibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction network to add and remove noise from text, extracting a semantic representation that is robust to local perturbations. We also propose a siamese calibration technique to train the model to make equally confidence predictions under different noise, which improves the model's robustness against adversarial perturbations. Experiments on four publicly available datasets show that the SCRN outperforms all baseline methods, achieving 6.5\%-18.25\% absolute accuracy improvement over the best baseline method under adversarial attacks.
Moreover, it exhibits superior generalizability in cross-domain, cross-genre, and mixed-source scenarios. The code is available at \url{https://github.com/CarlanLark/Robust-AIGC-Detector}.

------------

`[2406.01288] Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses <https://arxiv.org/abs/2406.01288>`__ 改进的少次越狱可以绕过对齐语言模型及其防御

::

    Mon, 3 Jun 2024 12:59:17 GMT
    Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin

Recently, Anil et al. (2024) show that many-shot (up to hundreds of) demonstrations can jailbreak state-of-the-art LLMs by exploiting their long-context capability. Nevertheless, is it possible to use few-shot demonstrations to efficiently jailbreak LLMs within limited context sizes? While the vanilla few-shot jailbreaking may be inefficient, we propose improved techniques such as injecting special system tokens like [/INST] and employing demo-level random search from a collected demo pool. These simple techniques result in surprisingly effective jailbreaking against aligned LLMs (even with advanced defenses). For examples, our method achieves >80% (mostly >95%) ASRs on Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are enhanced by strong defenses such as perplexity detection and/or SmoothLLM, which is challenging for suffix-based jailbreaking. In addition, we conduct comprehensive and elaborate (e.g., making sure to use correct system prompts) evaluations against other aligned LLMs and advanced defenses, where our method consistently achieves nearly 100% ASRs. Our code is available at https://github.com/sail-sg/I-FSJ.

------------

`[2406.01311] FactGenius: Combining Zero-Shot Prompting and Fuzzy Relation Mining to Improve Fact Verification with Knowledge Graphs <https://arxiv.org/abs/2406.01311>`__ FactGenius:结合零样本提示和模糊关系挖掘改进基于知识图谱的事实验证

::

    Mon, 3 Jun 2024 13:24:37 GMT
    Sushant Gautam

Fact-checking is a crucial natural language processing (NLP) task that verifies the truthfulness of claims by considering reliable evidence.
Traditional methods are often limited by labour-intensive data curation and rule-based approaches. In this paper, we present FactGenius, a novel method that enhances fact-checking by combining zero-shot prompting of large language models (LLMs) with fuzzy text matching on knowledge graphs (KGs). Leveraging DBpedia, a structured linked data dataset derived from Wikipedia, FactGenius refines LLM-generated connections using similarity measures to ensure accuracy.
The evaluation of FactGenius on the FactKG, a benchmark dataset for fact verification, demonstrates that it significantly outperforms existing baselines, particularly when fine-tuning RoBERTa as a classifier. The two-stage approach of filtering and validating connections proves crucial, achieving superior performance across various reasoning types and establishing FactGenius as a promising tool for robust fact-checking. The code and materials are available at https://github.com/SushantGautam/FactGenius.

------------

`[2406.01333] Probing Language Models for Pre-training Data Detection <https://arxiv.org/abs/2406.01333>`__ 用于预训练数据检测的语言模型探测

::

    Mon, 3 Jun 2024 13:58:04 GMT
    Zhenhua Liu, Tong Zhu, Chuanyuan Tan, Haonan Lu, Bing Liu, Wenliang Chen

Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection.
Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy (Our code and dataset are available at https://github.com/zhliu0106/probing-lm-data).

------------

`[2406.01363] Privacy in LLM-based Recommendation: Recent Advances and Future Directions <https://arxiv.org/abs/2406.01363>`__ 基于llm推荐的隐私保护:最新进展与未来方向

::

    Mon, 3 Jun 2024 14:31:47 GMT
    Sichun Luo, Wei Shao, Yuxuan Yao, Jian Xu, Mingyang Liu, Qintong Li, Bowei He, Maolin Wang, Guanzhi Deng, Hanxu Hou, Xinyi Zhang, Linqi Song

Nowadays, large language models (LLMs) have been integrated with conventional recommendation models to improve recommendation performance. However, while most of the existing works have focused on improving the model performance, the privacy issue has only received comparatively less attention. In this paper, we review recent advancements in privacy within LLM-based recommendation, categorizing them into privacy attacks and protection mechanisms. Additionally, we highlight several challenges and propose future directions for the community to address these critical problems.

------------

`[2406.01375] D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models <https://arxiv.org/abs/2406.01375>`__ D-CPT定律:大型语言模型特定领域连续预训练缩放定律

::

    Mon, 3 Jun 2024 14:40:31 GMT
    Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, Xu Tan, Jie Fu, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng

Continual Pre-Training (CPT) on Large Language Models (LLMs) has been widely used to expand the model's fundamental understanding of specific downstream domains (e.g., math and code). For the CPT on domain-specific LLMs, one important question is how to choose the optimal mixture ratio between the general-corpus (e.g., Dolma, Slim-pajama) and the downstream domain-corpus.
Existing methods usually adopt laborious human efforts by grid-searching on a set of mixture ratios, which require high GPU training consumption costs.
Besides, we cannot guarantee the selected ratio is optimal for the specific domain. To address the limitations of existing methods, inspired by the Scaling Law for performance prediction, we propose to investigate the Scaling Law of the Domain-specific Continual Pre-Training (D-CPT Law) to decide the optimal mixture ratio with acceptable training costs for LLMs of different sizes.
Specifically, by fitting the D-CPT Law, we can easily predict the general and downstream performance of arbitrary mixture ratios, model sizes, and dataset sizes using small-scale training costs on limited experiments. Moreover, we also extend our standard D-CPT Law on cross-domain settings and propose the Cross-Domain D-CPT Law to predict the D-CPT law of target domains, where very small training costs (about 1% of the normal training costs) are needed for the target domains. Comprehensive experimental results on six downstream domains demonstrate the effectiveness and generalizability of our proposed D-CPT Law and Cross-Domain D-CPT Law.

------------

`[2406.01382] Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function <https://arxiv.org/abs/2406.01382>`__ 大型语言模型的表现是否如人们预期的那样?测量人类的泛化功能

::

    Mon, 3 Jun 2024 14:45:21 GMT
    Keyon Vafa, Ashesh Rambachan, Sendhil Mullainathan

What makes large language models (LLMs) impressive is also what makes them hard to evaluate: their diversity of uses. To evaluate these models, we must understand the purposes they will be used for. We consider a setting where these deployment decisions are made by people, and in particular, people's beliefs about where an LLM will perform well. We model such beliefs as the consequence of a human generalization function: having seen what an LLM gets right or wrong, people generalize to where else it might succeed. We collect a dataset of 19K examples of how humans make generalizations across 79 tasks from the MMLU and BIG-Bench benchmarks. We show that the human generalization function can be predicted using NLP methods: people have consistent structured ways to generalize. We then evaluate LLM alignment with the human generalization function. Our results show that -- especially for cases where the cost of mistakes is high -- more capable models (e.g. GPT-4) can do worse on the instances people choose to use them for, exactly because they are not aligned with the human generalization function.

------------

`[2406.01428] Superhuman performance in urology board questions by an explainable large language model enabled for context integration of the European Association of Urology guidelines: the UroBot study <https://arxiv.org/abs/2406.01428>`__ 通过一个可解释的大型语言模型在泌尿外科委员会问题上的超人表现，使欧洲泌尿外科协会指南的上下文集成成为可能:UroBot研究

::

    Mon, 3 Jun 2024 15:26:06 GMT
    Martin J. Hetz, Nicolas Carl, Sarah Haggenm\"uller, Christoph Wies, Maurice Stephan Michel, Frederik Wessels, Titus J. Brinker

Large Language Models (LLMs) are revolutionizing medical Question-Answering (medQA) through extensive use of medical literature. However, their performance is often hampered by outdated training data and a lack of explainability, which limits clinical applicability. This study aimed to create and assess UroBot, a urology-specialized chatbot, by comparing it with state-of-the-art models and the performance of urologists on urological board questions, ensuring full clinician-verifiability. UroBot was developed using OpenAI's GPT-3.5, GPT-4, and GPT-4o models, employing retrieval-augmented generation (RAG) and the latest 2023 guidelines from the European Association of Urology (EAU). The evaluation included ten runs of 200 European Board of Urology (EBU) In-Service Assessment (ISA) questions, with performance assessed by the mean Rate of Correct Answers (RoCA). UroBot-4o achieved an average RoCA of 88.4%, surpassing GPT-4o by 10.8%, with a score of 77.6%. It was also clinician-verifiable and exhibited the highest run agreement as indicated by Fleiss' Kappa (k = 0.979).
By comparison, the average performance of urologists on board questions, as reported in the literature, is 68.7%. UroBot's clinician-verifiable nature and superior accuracy compared to both existing models and urologists on board questions highlight its potential for clinical integration. The study also provides the necessary code and instructions for further development of UroBot.

------------

`[2406.01436] Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models <https://arxiv.org/abs/2406.01436>`__ 编辑巨人的思维:大型语言模型知识编辑陷阱的深入探索

::

    Mon, 3 Jun 2024 15:28:21 GMT
    Cheng-Hsun Hsueh, Paul Kuo-Ming Huang, Tzu-Han Lin, Che-Wei Liao, Hung-Chieh Fang, Chao-Wei Huang, Yun-Nung Chen

Knowledge editing is a rising technique for efficiently updating factual knowledge in Large Language Models (LLMs) with minimal alteration of parameters. However, recent studies have identified concerning side effects, such as knowledge distortion and the deterioration of general abilities, that have emerged after editing. This survey presents a comprehensive study of these side effects, providing a unified view of the challenges associated with knowledge editing in LLMs. We discuss related works and summarize potential research directions to overcome these limitations. Our work highlights the limitations of current knowledge editing methods, emphasizing the need for deeper understanding of inner knowledge structures of LLMs and improved knowledge editing methods. To foster future research, we have released the complementary materials such as paper collection publicly at https://github.com/MiuLab/EditLLM-Survey

------------

`[2406.01441] LexMatcher: Dictionary-centric Data Collection for LLM-based Machine Translation <https://arxiv.org/abs/2406.01441>`__ LexMatcher:基于llm机器翻译的以字典为中心的数据收集

::

    Mon, 3 Jun 2024 15:30:36 GMT
    Yongjing Yin, Jiali Zeng, Yafu Li, Fandong Meng, Yue Zhang

The fine-tuning of open-source large language models (LLMs) for machine translation has recently received considerable attention, marking a shift towards data-centric research from traditional neural machine translation.
However, the area of data collection for instruction fine-tuning in machine translation remains relatively underexplored. In this paper, we present LexMatcher, a simple yet effective method for data collection that leverages bilingual dictionaries to generate a dataset, the design of which is driven by the coverage of senses found in these dictionaries. The dataset comprises a subset retrieved from an existing corpus and a smaller synthesized subset which supplements the infrequent senses of polysemous words. Utilizing LLaMA2 as our base model, our approach outperforms the established baselines on the WMT2022 test sets and also exhibits significant performance improvements in tasks related to word sense disambiguation and specialized terminology translation.
These results underscore the effectiveness of LexMatcher in enhancing LLM-based machine translation.

------------

`[2406.01506] The Geometry of Categorical and Hierarchical Concepts in Large Language Models <https://arxiv.org/abs/2406.01506>`__ 大型语言模型中范畴和层次概念的几何

::

    Mon, 3 Jun 2024 16:34:01 GMT
    Kiho Park, Yo Joong Choe, Yibo Jiang, Victor Veitch

Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability. In this paper, we study the two foundational questions in this area. First, how are categorical concepts, such as {'mammal', 'bird', 'reptile', 'fish'}, represented? Second, how are hierarchical relations between concepts encoded? For example, how is the fact that 'dog' is a kind of 'mammal' encoded? We show how to extend the linear representation hypothesis to answer these questions.
We find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure. We validate these theoretical results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from WordNet.

------------

`[2406.01514] Decoupled Alignment for Robust Plug-and-Play Adaptation <https://arxiv.org/abs/2406.01514>`__ 面向健壮即插即用适应的解耦对齐

::

    Mon, 3 Jun 2024 16:46:18 GMT
    Haozheng Luo, Jiahao Yu, Wenxin Zhang, Jialong Li, Jerry Yao-Chieh Hu, Xingyu Xin, Han Liu

We introduce a low-resource safety enhancement method for aligning large language models (LLMs) without the need for supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF). Our main idea is to exploit knowledge distillation to extract the alignment information from existing well-aligned LLMs and integrate it into unaligned LLMs in a plug-and-play fashion. Methodology, we employ delta debugging to identify the critical components of knowledge necessary for effective distillation. On the harmful question dataset, our method significantly enhances the average defense success rate by approximately 14.41%, reaching as high as 51.39%, in 17 unaligned pre-trained LLMs, without compromising performance.

------------

`[2406.01538] What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores <https://arxiv.org/abs/2406.01538>`__ 大型语言模型在大脑中映射到什么?反对过度依赖大脑分数的案例

::

    Mon, 3 Jun 2024 17:13:27 GMT
    Ebrahim Feghhi, Nima Hadidi, Bryan Song, Idan A. Blank, Jonathan C. Kao

Given the remarkable capabilities of large language models (LLMs), there has been a growing interest in evaluating their similarity to the human brain. One approach towards quantifying this similarity is by measuring how well a model predicts neural signals, also called "brain score". Internal representations from LLMs achieve state-of-the-art brain scores, leading to speculation that they share computational principles with human language processing. This inference is only valid if the subset of neural activity predicted by LLMs reflects core elements of language processing. Here, we question this assumption by analyzing three neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages. We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain. We therefore use contiguous splits moving forward. Second, we explain the surprisingly high brain scores of untrained LLMs by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position. This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like. Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence length, position, and pronoun-dereferenced static word embeddings; a small, additional amount is explained by sense-specific embeddings and contextual representations of sentence structure. We conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.

------------

`[2406.01563] LoFiT: Localized Fine-tuning on LLM Representations <https://arxiv.org/abs/2406.01563>`__ LoFiT: LLM表示的局部微调

::

    Mon, 3 Jun 2024 17:45:41 GMT
    Fangcong Yin, Xi Ye, Greg Durrett

Recent work in interpretability shows that large language models (LLMs) can be adapted for new tasks in a learning-free way: it is possible to intervene on LLM representations to elicit desired behaviors for alignment. For instance, adding certain bias vectors to the outputs of certain attention heads is reported to boost the truthfulness of models. In this work, we show that localized fine-tuning serves as an effective alternative to such representation intervention methods. We introduce a framework called Localized Fine-Tuning on LLM Representations (LoFiT), which identifies a subset of attention heads that are most important for learning a specific task, then trains offset vectors to add to the model's hidden representations at those selected heads. LoFiT localizes to a sparse set of heads (3%) and learns the offset vectors from limited training data, comparable to the settings used for representation intervention. For truthfulness and reasoning tasks, we find that LoFiT's intervention vectors are more effective for LLM adaptation than vectors from representation intervention methods such as Inference-time Intervention. We also find that the localization step is important: selecting a task-specific set of attention heads can lead to higher performance than intervening on heads selected for a different task. Finally, for the tasks we study, LoFiT achieves comparable performance to other parameter-efficient fine-tuning methods such as LoRA, despite modifying 20x-200x fewer parameters than these methods.

------------

`[2406.00104] Scalable Bayesian Learning with posteriors <https://arxiv.org/abs/2406.00104>`__ 基于后验的可扩展贝叶斯学习

::

    Fri, 31 May 2024 18:00:12 GMT
    Samuel Duffield, Kaelan Donatella, Johnathan Chiu, Phoebe Klett, Daniel Simpson

Although theoretically compelling, Bayesian learning with modern machine learning models is computationally challenging since it requires approximating a high dimensional posterior distribution. In this work, we (i) introduce posteriors, an easily extensible PyTorch library hosting general-purpose implementations making Bayesian learning accessible and scalable to large data and parameter regimes; (ii) present a tempered framing of stochastic gradient Markov chain Monte Carlo, as implemented in posteriors, that transitions seamlessly into optimization and unveils a minor modification to deep ensembles to ensure they are asymptotically unbiased for the Bayesian posterior, and (iii) demonstrate and compare the utility of Bayesian approximations through experiments including an investigation into the cold posterior effect and applications with large language models.

------------

`[2406.00144] Query2CAD: Generating CAD models using natural language queries <https://arxiv.org/abs/2406.00144>`__ Query2CAD:使用自然语言查询生成CAD模型

::

    Fri, 31 May 2024 19:17:00 GMT
    Akshay Badagabettu, Sai Sravan Yarlagadda, Amir Barati Farimani

Computer Aided Design (CAD) engineers typically do not achieve their best prototypes in a single attempt. Instead, they iterate and refine their designs to achieve an optimal solution through multiple revisions. This traditional approach, though effective, is time-consuming and relies heavily on the expertise of skilled engineers. To address these challenges, we introduce Query2CAD, a novel framework to generate CAD designs. The framework uses a large language model to generate executable CAD macros. Additionally, Query2CAD refines the generation of the CAD model with the help of its self-refinement loops. Query2CAD operates without supervised data or additional training, using the LLM as both a generator and a refiner. The refiner leverages feedback generated by the BLIP2 model, and to address false negatives, we have incorporated human-in-the-loop feedback into our system. Additionally, we have developed a dataset that encompasses most operations used in CAD model designing and have evaluated our framework using this dataset. Our findings reveal that when we used GPT-4 Turbo as our language model, the architecture achieved a success rate of 53.6\% on the first attempt. With subsequent refinements, the success rate increased by 23.1\%. In particular, the most significant improvement in the success rate was observed with the first iteration of the refinement. With subsequent refinements, the accuracy of the correct designs did not improve significantly. We have open-sourced our data, model, and code (github.com/akshay140601/Query2CAD).

------------

`[2406.00209] Mamba State-Space Models Can Be Strong Downstream Learners <https://arxiv.org/abs/2406.00209>`__ Mamba状态空间模型可以成为强大的下游学习者

::

    Fri, 31 May 2024 21:46:23 GMT
    John T. Halloran, Manbir Gulati, Paul F. Roysdon

Mamba state-space models (SSMs) have recently outperformed state-of-the-art (SOTA) Transformer large language models (LLMs) in various tasks and been widely adapted. However, Mamba's downstream learning capabilities remain either unexplored$\unicode{x2013}$e.g., mixed-precision (MPFT) and parameter-efficient fine-tuning (PEFT)--or under-evaluated$\unicode{x2013}$e.g., in-context learning (ICL). For the latter, recent works reported Mamba's ICL rivals SOTA Transformer LLMs using non-standard benchmarks. In contrast, we show that on standard benchmarks, pretrained Mamba models achieve only 38% of the ICL performance improvements (over zero-shot) of comparable Transformers.
Enabling MPFT and PEFT in Mamba architectures is challenging due to recurrent dynamics and highly customized CUDA kernels, respectively. However, we prove that Mamba's recurrent dynamics are robust to small input changes using dynamical systems theory. Empirically, we show that performance changes in Mamba's inference and fine-tuning due to mixed-precision align with Transformer LLMs. Furthermore, we show that targeting key memory buffers in Mamba's customized CUDA kernels for low-rank adaptation regularizes SSM parameters, thus achieving parameter efficiency while retaining speedups. We show that combining MPFT and PEFT enables up to 2.15 times more tokens-per-second and 65.5% reduced per-token-memory compared to full Mamba fine-tuning, while achieving up to 81.5% of the ICL performance improvements (over zero-shot) of comparably fine-tuned Transformers.

------------

`[2406.00426] InterpreTabNet: Distilling Predictive Signals from Tabular Data by Salient Feature Interpretation <https://arxiv.org/abs/2406.00426>`__ InterpreTabNet:通过显著特征解释从表格数据中提取预测信号

::

    Sat, 1 Jun 2024 12:48:11 GMT
    Jacob Si, Wendy Yusi Cheng, Michael Cooper, Rahul G. Krishnan

Tabular data are omnipresent in various sectors of industries. Neural networks for tabular data such as TabNet have been proposed to make predictions while leveraging the attention mechanism for interpretability. However, the inferred attention masks are often dense, making it challenging to come up with rationales about the predictive signal. To remedy this, we propose InterpreTabNet, a variant of the TabNet model that models the attention mechanism as a latent variable sampled from a Gumbel-Softmax distribution. This enables us to regularize the model to learn distinct concepts in the attention masks via a KL Divergence regularizer. It prevents overlapping feature selection by promoting sparsity which maximizes the model's efficacy and improves interpretability to determine the important features when predicting the outcome. To assist in the interpretation of feature interdependencies from our model, we employ a large language model (GPT-4) and use prompt engineering to map from the learned feature mask onto natural language text describing the learned signal. Through comprehensive experiments on real-world datasets, we demonstrate that InterpreTabNet outperforms previous methods for interpreting tabular data while attaining competitive accuracy.

------------

`[2406.00509] Empirical influence functions to understand the logic of fine-tuning <https://arxiv.org/abs/2406.00509>`__ 经验影响函数来理解微调的逻辑

::

    Sat, 1 Jun 2024 17:31:06 GMT
    Jordan K. Matelsky, Lyle Ungar, Konrad P. Kording

Understanding the process of learning in neural networks is crucial for improving their performance and interpreting their behavior. This can be approximately understood by asking how a model's output is influenced when we fine-tune on a new training sample. There are desiderata for such influences, such as decreasing influence with semantic distance, sparseness, noise invariance, transitive causality, and logical consistency. Here we use the empirical influence measured using fine-tuning to demonstrate how individual training samples affect outputs. We show that these desiderata are violated for both for simple convolutional networks and for a modern LLM. We also illustrate how prompting can partially rescue this failure. Our paper presents an efficient and practical way of quantifying how well neural networks learn from fine-tuning stimuli. Our results suggest that popular models cannot generalize or perform logic in the way they appear to.

------------

`[2406.00548] LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models <https://arxiv.org/abs/2406.00548>`__ LIDAO:关于去偏见(大型)语言模型的有限干预

::

    Sat, 1 Jun 2024 20:12:54 GMT
    Tianci Liu, Haoyu Wang, Shiyang Wang, Yu Cheng, Jing Gao

Large language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups (e.g., female), raising severe fairness concerns. As remedies, prior works intervened the generation by removing attitude or demographic information, inevitably degrading the generation quality and resulting in notable \textit{fairness-fluency} trade-offs. However, it is still under-explored to what extent the fluency \textit{has to} be affected in order to achieve a desired level of fairness. In this work, we conduct the first formal study from an information-theoretic perspective. We show that previous approaches are excessive for debiasing and propose LIDAO, a general framework to debias a (L)LM at a better fluency provably. We further robustify LIDAO in adversarial scenarios, where a carefully-crafted prompt may stimulate LLMs exhibiting instruction-following abilities to generate texts with fairness issue appears only when the prompt is also taken into account. Experiments on three LMs ranging from 0.7B to 7B parameters demonstrate the superiority of our method.

------------

`[2406.00806] Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection <https://arxiv.org/abs/2406.00806>`__ 

::

    Sun, 2 Jun 2024 17:09:48 GMT
    Chentao Cao, Zhun Zhong, Zhanke Zhou, Yang Liu, Tongliang Liu, Bo Han

Detecting out-of-distribution (OOD) samples is essential when deploying machine learning models in open-world scenarios. Zero-shot OOD detection, requiring no training on in-distribution (ID) data, has been possible with the advent of vision-language models like CLIP. Existing methods build a text-based classifier with only closed-set labels. However, this largely restricts the inherent capability of CLIP to recognize samples from large and open label space. In this paper, we propose to tackle this constraint by leveraging the expert knowledge and reasoning capability of large language models (LLM) to Envision potential Outlier Exposure, termed EOE, without access to any actual OOD data. Owing to better adaptation to open-world scenarios, EOE can be generalized to different tasks, including far, near, and fine-grained OOD detection. Technically, we design (1) LLM prompts based on visual similarity to generate potential outlier class labels specialized for OOD detection, as well as (2) a new score function based on potential outlier penalty to distinguish hard OOD samples effectively. Empirically, EOE achieves state-of-the-art performance across different OOD tasks and can be effectively scaled to the ImageNet-1K dataset. The code is publicly available at: https://github.com/tmlr-group/EOE.

------------

`[2406.00894] Pretrained Hybrids with MAD Skills <https://arxiv.org/abs/2406.00894>`__ 预训练的具有疯狂技能的混血儿

::

    Sun, 2 Jun 2024 23:24:30 GMT
    Nicholas Roberts, Samuel Guo, Zhiqi Gao, Satya Sai Srinath Namburi GNVV, Sonia Cromp, Chengjun Wu, Chengyu Duan, Frederic Sala

While Transformers underpin modern large language models (LMs), there is a growing list of alternative architectures with new capabilities, promises, and tradeoffs. This makes choosing the right LM architecture challenging.
Recently-proposed $\textit{hybrid architectures}$ seek a best-of-all-worlds approach that reaps the benefits of all architectures. Hybrid design is difficult for two reasons: it requires manual expert-driven search, and new hybrids must be trained from scratch. We propose $\textbf{Manticore}$, a framework that addresses these challenges. Manticore $\textit{automates the design of hybrid architectures}$ while reusing pretrained models to create $\textit{pretrained}$ hybrids. Our approach augments ideas from differentiable Neural Architecture Search (NAS) by incorporating simple projectors that translate features between pretrained blocks from different architectures. We then fine-tune hybrids that combine pretrained models from different architecture families -- such as the GPT series and Mamba -- end-to-end. With Manticore, we enable LM selection without training multiple models, the construction of pretrained hybrids from existing pretrained models, and the ability to $\textit{program}$ pretrained hybrids to have certain capabilities.
Manticore hybrids outperform existing manually-designed hybrids, achieve strong performance on Long Range Arena (LRA) tasks, and can improve on pretrained transformers and state space models.

------------

`[2406.01032] LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning <https://arxiv.org/abs/2406.01032>`__ LLM和GNN是互补的:提取LLM用于多模态图学习

::

    Mon, 3 Jun 2024 06:33:51 GMT
    Junjie Xu, Zongyu Wu, Minhua Lin, Xiang Zhang, Suhang Wang

Recent progress in Graph Neural Networks (GNNs) has greatly enhanced the ability to model complex molecular structures for predicting properties.
Nevertheless, molecular data encompasses more than just graph structures, including textual and visual information that GNNs do not handle well. To bridge this gap, we present an innovative framework that utilizes multimodal molecular data to extract insights from Large Language Models (LLMs). We introduce GALLON (Graph Learning from Large Language Model Distillation), a framework that synergizes the capabilities of LLMs and GNNs by distilling multimodal knowledge into a unified Multilayer Perceptron (MLP). This method integrates the rich textual and visual data of molecules with the structural analysis power of GNNs. Extensive experiments reveal that our distilled MLP model notably improves the accuracy and efficiency of molecular property predictions.

------------

`[2406.01124] Latent Logic Tree Extraction for Event Sequence Explanation from LLMs <https://arxiv.org/abs/2406.01124>`__ llm中事件序列解释的潜在逻辑树抽取

::

    Mon, 3 Jun 2024 09:10:42 GMT
    Zitao Song, Chao Yang, Chaojie Wang, Bo An, Shuang Li

Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences. Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence.
Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees. We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables. In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences. LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution. We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables. The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters. In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations. Empirical demonstrations showcase the promising performance and adaptability of our framework.

------------

`[2406.01457] Differentially Private Tabular Data Synthesis using Large Language Models <https://arxiv.org/abs/2406.01457>`__ 基于大型语言模型的差分隐私表格数据合成

::

    Mon, 3 Jun 2024 15:43:57 GMT
    Toan V. Tran and Li Xiong

Synthetic tabular data generation with differential privacy is a crucial problem to enable data sharing with formal privacy. Despite a rich history of methodological research and development, developing differentially private tabular data generators that can provide realistic synthetic datasets remains challenging. This paper introduces DP-LLMTGen -- a novel framework for differentially private tabular data synthesis that leverages pretrained large language models (LLMs). DP-LLMTGen models sensitive datasets using a two-stage fine-tuning procedure with a novel loss function specifically designed for tabular data. Subsequently, it generates synthetic data through sampling the fine-tuned LLMs. Our empirical evaluation demonstrates that DP-LLMTGen outperforms a variety of existing mechanisms across multiple datasets and privacy settings. Additionally, we conduct an ablation study and several experimental analyses to deepen our understanding of LLMs in addressing this important problem. Finally, we highlight the controllable generation ability of DP-LLMTGen through a fairness-constrained generation setting.

------------

`[2406.01539] Physics-informed deep learning and compressive collocation for high-dimensional diffusion-reaction equations: practical existence theory and numerics <https://arxiv.org/abs/2406.01539>`__ 基于物理学的深度学习与压缩配置的高维扩散-反应方程:实践存在理论与数值

::

    Mon, 3 Jun 2024 17:16:11 GMT
    Simone Brugiapaglia, Nick Dexter, Samir Karam and Weiqi Wang

On the forefront of scientific computing, Deep Learning (DL), i.e., machine learning with Deep Neural Networks (DNNs), has emerged a powerful new tool for solving Partial Differential Equations (PDEs). It has been observed that DNNs are particularly well suited to weakening the effect of the curse of dimensionality, a term coined by Richard E. Bellman in the late `50s to describe challenges such as the exponential dependence of the sample complexity, i.e., the number of samples required to solve an approximation problem, on the dimension of the ambient space. However, although DNNs have been used to solve PDEs since the `90s, the literature underpinning their mathematical efficiency in terms of numerical analysis (i.e., stability, accuracy, and sample complexity), is only recently beginning to emerge. In this paper, we leverage recent advancements in function approximation using sparsity-based techniques and random sampling to develop and analyze an efficient high-dimensional PDE solver based on DL. We show, both theoretically and numerically, that it can compete with a novel stable and accurate compressive spectral collocation method. In particular, we demonstrate a new practical existence theorem, which establishes the existence of a class of trainable DNNs with suitable bounds on the network architecture and a sufficient condition on the sample complexity, with logarithmic or, at worst, linear scaling in dimension, such that the resulting networks stably and accurately approximate a diffusion-reaction PDE with high probability.

------------

`[2406.00004] Navigating the Future of Federated Recommendation Systems with Foundation Models <https://arxiv.org/abs/2406.00004>`__ 

::

    Sun, 12 May 2024 04:15:05 GMT
    Zhiwei Li, Guodong Long

In recent years, the integration of federated learning (FL) and recommendation systems (RSs), known as Federated Recommendation Systems (FRSs), has attracted attention for preserving user privacy by keeping private data on client devices. However, FRS faces inherent limitations such as data heterogeneity and scarcity, due to the privacy requirements of FL and the typical data sparsity issues of RSs. Foundation models (FMs), such as diffusion models (DMs) and large language models (LLMs), which are widely recognized for producing high-quality content in the image and NLP domains, focus on understanding and mimicking the underlying distribution of the training data.
Unlike discriminative models, which learn the boundaries between categories, FMs aim to learn the entire probability distribution of the input data. Thus, the achievements of FMs inspire the design of FRS and suggest a promising research direction: integrating foundation models to address the above limitations. In this study, we conduct a comprehensive review of FRSs with FMs.
Specifically, we: 1) summarise the common approaches of current FRSs and FMs; 2) review the challenges posed by FRSs and FMs; 3) discuss potential future research directions; and 4) introduce some common benchmarks and evaluation metrics in the FRS field. We hope that this survey provides the necessary background and guidance to explore this interesting and emerging topic.

------------

`[2406.00006] A Prompt-driven Task Planning Method for Multi-drones based on Large Language Model <https://arxiv.org/abs/2406.00006>`__ 一种基于大规模语言模型的提示驱动多无人机任务规划方法

::

    Tue, 14 May 2024 12:24:39 GMT
    Yaohua Liu

With the rapid development of drone technology, the application of multi-drones is becoming increasingly widespread in various fields. However, the task planning technology for multi-drones still faces challenges such as the complexity of remote operation and the convenience of human-machine interaction. To address these issues, this paper proposes a prompt-driven task planning method for multi-drones based on large language models. By introducing the Prompt technique, appropriate prompt information is provided for the multi-drone system.

------------

`[2406.00014] KU-DMIS at EHRSQL 2024:Generating SQL query via question templatization in EHR <https://arxiv.org/abs/2406.00014>`__ KU-DMIS在EHRSQL 2024:在EHR中通过问题模板化生成SQL查询

::

    Wed, 22 May 2024 02:15:57 GMT
    Hajung Kim, Chanhwi Kim, Hoonick Lee, Kyochul Jang, Jiwoo Lee, Kyungjae Lee, Gangwoo Kim, Jaewoo Kang

Transforming natural language questions into SQL queries is crucial for precise data retrieval from electronic health record (EHR) databases. A significant challenge in this process is detecting and rejecting unanswerable questions that request information beyond the database's scope or exceed the system's capabilities. In this paper, we introduce a novel text-to-SQL framework that robustly handles out-of-domain questions and verifies the generated queries with query execution.Our framework begins by standardizing the structure of questions into a templated format. We use a powerful large language model (LLM), fine-tuned GPT-3.5 with detailed prompts involving the table schemas of the EHR database system. Our experimental results demonstrate the effectiveness of our framework on the EHRSQL-2024 benchmark benchmark, a shared task in the ClinicalNLP workshop. Although a straightforward fine-tuning of GPT shows promising results on the development set, it struggled with the out-of-domain questions in the test set. With our framework, we improve our system's adaptability and achieve competitive performances in the official leaderboard of the EHRSQL-2024 challenge.

------------

`[2406.00231] LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking <https://arxiv.org/abs/2406.00231>`__ LLM-RankFusion:缓解基于llm排名的内在不一致性

::

    Fri, 31 May 2024 23:29:42 GMT
    Yifan Zeng, Ojas Tendolkar, Raymond Baartmans, Qingyun Wu, Huazheng Wang, Lizhong Chen

Ranking passages by prompting a large language model (LLM) can achieve promising performance in modern information retrieval (IR) systems. A common approach is to sort the ranking list by prompting LLMs for pairwise comparison.
However, sorting-based methods require consistent comparisons to correctly sort the passages, which we show that LLMs often violate. We identify two kinds of intrinsic inconsistency in LLM-based pairwise comparisons: order inconsistency which leads to conflicting results when switching the passage order, and transitive inconsistency which leads to non-transitive triads among all preference pairs. In this paper, we propose LLM-RankFusion, an LLM-based ranking framework that mitigates these inconsistencies and produces a robust ranking list. LLM-RankFusion mitigates order inconsistency using in-context learning (ICL) to demonstrate order-agnostic comparisons and calibration to estimate the underlying preference probability between two passages. We then address transitive inconsistency by aggregating the ranking results from multiple rankers. In our experiments, we empirically show that LLM-RankFusion can significantly reduce inconsistent pairwise comparison results, and improve the ranking quality by making the final ranking list more robust.

------------

`[2406.00247] Large Language Models for Relevance Judgment in Product Search <https://arxiv.org/abs/2406.00247>`__ 

::

    Sat, 1 Jun 2024 00:52:41 GMT
    Navid Mehrdad, Hrushikesh Mohapatra, Mossaab Bagdouri, Prijith Chandran, Alessandro Magnani, Xunfan Cai, Ajit Puthenputhussery, Sachin Yadav, Tony Lee, ChengXiang Zhai, Ciya Liao

High relevance of retrieved and re-ranked items to the search query is the cornerstone of successful product search, yet measuring relevance of items to queries is one of the most challenging tasks in product information retrieval, and quality of product search is highly influenced by the precision and scale of available relevance-labelled data. In this paper, we present an array of techniques for leveraging Large Language Models (LLMs) for automating the relevance judgment of query-item pairs (QIPs) at scale. Using a unique dataset of multi-million QIPs, annotated by human evaluators, we test and optimize hyper parameters for finetuning billion-parameter LLMs with and without Low Rank Adaption (LoRA), as well as various modes of item attribute concatenation and prompting in LLM finetuning, and consider trade offs in item attribute inclusion for quality of relevance predictions. We demonstrate considerable improvement over baselines of prior generations of LLMs, as well as off-the-shelf models, towards relevance annotations on par with the human relevance evaluators. Our findings have immediate implications for the growing field of relevance judgment automation in product search.

------------

`[2406.00258] Artemis: Towards Referential Understanding in Complex Videos <https://arxiv.org/abs/2406.00258>`__ Artemis:复杂视频中的参照理解

::

    Sat, 1 Jun 2024 01:43:56 GMT
    Jihao Qiu and Yuan Zhang and Xi Tang and Lingxi Xie and Tianren Ma and Pengyu Yan and David Doermann and Qixiang Ye and Yunjie Tian

Videos carry rich visual information including object description, action, interaction, etc., but the existing multimodal large language models (MLLMs) fell short in referential understanding scenarios such as video-based referring. In this paper, we present Artemis, an MLLM that pushes video-based referential understanding to a finer level. Given a video, Artemis receives a natural-language question with a bounding box in any video frame and describes the referred target in the entire video. The key to achieving this goal lies in extracting compact, target-specific video features, where we set a solid baseline by tracking and selecting spatiotemporal features from the video. We train Artemis on the newly established VideoRef45K dataset with 45K video-QA pairs and design a computationally efficient, three-stage training procedure.
Results are promising both quantitatively and qualitatively. Additionally, we show that \model can be integrated with video grounding and text summarization tools to understand more complex scenarios. Code and data are available at https://github.com/qiujihao19/Artemis.

------------

`[2406.00430] Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners <https://arxiv.org/abs/2406.00430>`__ 基于不确定性的闭环LLM规划器失效检测评估

::

    Sat, 1 Jun 2024 12:52:06 GMT
    Zhi Zheng, Qian Feng, Hang Li, Alois Knoll, Jianxiang Feng

Recently, Large Language Models (LLMs) have witnessed remarkable performance as zero-shot task planners for robotic manipulation tasks. However, the open-loop nature of previous works makes LLM-based planning error-prone and fragile. On the other hand, failure detection approaches for closed-loop planning are often limited by task-specific heuristics or following an unrealistic assumption that the prediction is trustworthy all the time. As a general-purpose reasoning machine, LLMs or Multimodal Large Language Models (MLLMs) are promising for detecting failures. However, However, the appropriateness of the aforementioned assumption diminishes due to the notorious hullucination problem. In this work, we attempt to mitigate these issues by introducing a framework for closed-loop LLM-based planning called KnowLoop, backed by an uncertainty-based MLLMs failure detector, which is agnostic to any used MLLMs or LLMs. Specifically, we evaluate three different ways for quantifying the uncertainty of MLLMs, namely token probability, entropy, and self-explained confidence as primary metrics based on three carefully designed representative prompting strategies. With a self-collected dataset including various manipulation tasks and an LLM-based robot system, our experiments demonstrate that token probability and entropy are more reflective compared to self-explained confidence. By setting an appropriate threshold to filter out uncertain predictions and seek human help actively, the accuracy of failure detection can be significantly enhanced. This improvement boosts the effectiveness of closed-loop planning and the overall success rate of tasks.

------------

`[2406.00584] A Blueprint Architecture of Compound AI Systems for Enterprise <https://arxiv.org/abs/2406.00584>`__ 面向企业的复合人工智能系统蓝图体系结构

::

    Sun, 2 Jun 2024 01:16:32 GMT
    Eser Kandogan, Sajjadur Rahman, Nikita Bhutani, Dan Zhang, Rafael Li Chen, Kushan Mitra, Sairam Gurajada, Pouya Pezeshkpour, Hayate Iso, Yanlin Feng, Hannah Kim, Chen Shen, Jin Wang, Estevam Hruschka

Large Language Models (LLMs) have showcased remarkable capabilities surpassing conventional NLP challenges, creating opportunities for use in production use cases. Towards this goal, there is a notable shift to building compound AI systems, wherein LLMs are integrated into an expansive software infrastructure with many components like models, retrievers, databases and tools. In this paper, we introduce a blueprint architecture for compound AI systems to operate in enterprise settings cost-effectively and feasibly. Our proposed architecture aims for seamless integration with existing compute and data infrastructure, with ``stream'' serving as the key orchestration concept to coordinate data and instructions among agents and other components. Task and data planners, respectively, break down, map, and optimize tasks and data to available agents and data sources defined in respective registries, given production constraints such as accuracy and latency.

------------

`[2406.00667] An Early Investigation into the Utility of Multimodal Large Language Models in Medical Imaging <https://arxiv.org/abs/2406.00667>`__ 多模态大型语言模型在医学成像中的应用的早期研究

::

    Sun, 2 Jun 2024 08:29:23 GMT
    Sulaiman Khan, Md. Rafiul Biswas, Alina Murad, Hazrat Ali, Zubair Shah

Recent developments in multimodal large language models (MLLMs) have spurred significant interest in their potential applications across various medical imaging domains. On the one hand, there is a temptation to use these generative models to synthesize realistic-looking medical image data, while on the other hand, the ability to identify synthetic image data in a pool of data is also significantly important. In this study, we explore the potential of the Gemini (\textit{gemini-1.0-pro-vision-latest}) and GPT-4V (gpt-4-vision-preview) models for medical image analysis using two modalities of medical image data.
Utilizing synthetic and real imaging data, both Gemini AI and GPT-4V are first used to classify real versus synthetic images, followed by an interpretation and analysis of the input images. Experimental results demonstrate that both Gemini and GPT-4 could perform some interpretation of the input images. In this specific experiment, Gemini was able to perform slightly better than the GPT-4V on the classification task. In contrast, responses associated with GPT-4V were mostly generic in nature. Our early investigation presented in this work provides insights into the potential of MLLMs to assist with the classification and interpretation of retinal fundoscopy and lung X-ray images. We also identify key limitations associated with the early investigation study on MLLMs for specialized tasks in medical image analysis.

------------

`[2406.00872] OLIVE: Object Level In-Context Visual Embeddings <https://arxiv.org/abs/2406.00872>`__ 

::

    Sun, 2 Jun 2024 21:36:31 GMT
    Timothy Ossowski, Junjie Hu

Recent generalist vision-language models (VLMs) have demonstrated impressive reasoning capabilities across diverse multimodal tasks. However, these models still struggle with fine-grained object-level understanding and grounding. In terms of modeling, existing VLMs implicitly align text tokens with image patch tokens, which is ineffective for embedding alignment at the same granularity and inevitably introduces noisy spurious background features. Additionally, these models struggle when generalizing to unseen visual concepts and may not be reliable for domain-specific tasks without further fine-tuning. To address these limitations, we propose a novel method to prompt large language models with in-context visual object vectors, thereby enabling controllable object-level reasoning. This eliminates the necessity of fusing a lengthy array of image patch features and significantly speeds up training. Furthermore, we propose region-level retrieval using our object representations, facilitating rapid adaptation to new objects without additional training. Our experiments reveal that our method achieves competitive referring object classification and captioning performance, while also offering zero-shot generalization and robustness to visually challenging contexts.

------------

`[2406.01168] How Ethical Should AI Be? How AI Alignment Shapes the Risk Preferences of LLMs <https://arxiv.org/abs/2406.01168>`__ 人工智能应该有多道德?AI对齐如何塑造llm的风险偏好

::

    Mon, 3 Jun 2024 10:05:25 GMT
    Shumiao Ouyang, Hayong Yun, Xingjian Zheng

This study explores the risk preferences of Large Language Models (LLMs) and how the process of aligning them with human ethical standards influences their economic decision-making. By analyzing 30 LLMs, we uncover a broad range of inherent risk profiles ranging from risk-averse to risk-seeking. We then explore how different types of AI alignment, a process that ensures models act according to human values and that focuses on harmlessness, helpfulness, and honesty, alter these base risk preferences. Alignment significantly shifts LLMs towards risk aversion, with models that incorporate all three ethical dimensions exhibiting the most conservative investment behavior. Replicating a prior study that used LLMs to predict corporate investments from company earnings call transcripts, we demonstrate that although some alignment can improve the accuracy of investment forecasts, excessive alignment results in overly cautious predictions. These findings suggest that deploying excessively aligned LLMs in financial decision-making could lead to severe underinvestment.
We underline the need for a nuanced approach that carefully balances the degree of ethical alignment with the specific requirements of economic domains when leveraging LLMs within finance.

------------

`[2406.01285] Large Language Models as Recommender Systems: A Study of Popularity Bias <https://arxiv.org/abs/2406.01285>`__ 大型语言模型作为推荐系统:流行度偏差的研究

::

    Mon, 3 Jun 2024 12:53:37 GMT
    Jan Malte Lichtenberg, Alexander Buchholz, Pola Schw\"obel

The issue of popularity bias -- where popular items are disproportionately recommended, overshadowing less popular but potentially relevant items -- remains a significant challenge in recommender systems. Recent advancements have seen the integration of general-purpose Large Language Models (LLMs) into the architecture of such systems. This integration raises concerns that it might exacerbate popularity bias, given that the LLM's training data is likely dominated by popular items. However, it simultaneously presents a novel opportunity to address the bias via prompt tuning. Our study explores this dichotomy, examining whether LLMs contribute to or can alleviate popularity bias in recommender systems. We introduce a principled way to measure popularity bias by discussing existing metrics and proposing a novel metric that fulfills a series of desiderata. Based on our new metric, we compare a simple LLM-based recommender to traditional recommender systems on a movie recommendation task. We find that the LLM recommender exhibits less popularity bias, even without any explicit mitigation.

------------

`[2406.01309] REvolve: Reward Evolution with Large Language Models for Autonomous Driving <https://arxiv.org/abs/2406.01309>`__ 旋转:奖励自动驾驶大型语言模型的进化

::

    Mon, 3 Jun 2024 13:23:27 GMT
    Rishi Hazra, Alkis Sygkounas, Andreas Persson, Amy Loutfi, Pedro Zuidberg Dos Martires

Designing effective reward functions is crucial to training reinforcement learning (RL) algorithms. However, this design is non-trivial, even for domain experts, due to the subjective nature of certain tasks that are hard to quantify explicitly. In recent works, large language models (LLMs) have been used for reward generation from natural language task descriptions, leveraging their extensive instruction tuning and commonsense understanding of human behavior. In this work, we hypothesize that LLMs, guided by human feedback, can be used to formulate human-aligned reward functions. Specifically, we study this in the challenging setting of autonomous driving (AD), wherein notions of "good" driving are tacit and hard to quantify. To this end, we introduce REvolve, an evolutionary framework that uses LLMs for reward design in AD.
REvolve creates and refines reward functions by utilizing human feedback to guide the evolution process, effectively translating implicit human knowledge into explicit reward functions for training (deep) RL agents. We demonstrate that agents trained on REvolve-designed rewards align closely with human driving standards, thereby outperforming other state-of-the-art baselines.

------------

`[2406.01394] PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration <https://arxiv.org/abs/2406.01394>`__ 

::

    Mon, 3 Jun 2024 14:57:39 GMT
    Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Huiping Zhuang, Cen Chen

The widespread usage of online Large Language Models (LLMs) inference services has raised significant privacy concerns about the potential exposure of private information in user inputs to eavesdroppers or untrustworthy service providers. Existing privacy protection methods for LLMs suffer from insufficient privacy protection, performance degradation, or severe inference time overhead. In this paper, we propose PrivacyRestore to protect the privacy of user inputs during LLM inference. PrivacyRestore directly removes privacy spans in user inputs and restores privacy information via activation steering during inference. The privacy spans are encoded as restoration vectors. We propose Attention-aware Weighted Aggregation (AWA) which aggregates restoration vectors of all privacy spans in the input into a meta restoration vector. AWA not only ensures proper representation of all privacy spans but also prevents attackers from inferring the privacy spans from the meta restoration vector alone. This meta restoration vector, along with the query with privacy spans removed, is then sent to the server. The experimental results show that PrivacyRestore can protect private information while maintaining acceptable levels of performance and inference efficiency.

------------

`[2406.00799] Are you still on track!? Catching LLM Task Drift with Activations <https://arxiv.org/abs/2406.00799>`__ 你还在正轨上吗?用激活捕捉LLM任务漂移

::

    Sun, 2 Jun 2024 16:53:21 GMT
    Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, Andrew Paverd

Large Language Models (LLMs) are routinely used in retrieval-augmented applications to orchestrate tasks and process inputs from users and other sources. These inputs, even in a single LLM interaction, can come from a variety of sources, of varying trustworthiness and provenance. This opens the door to prompt injection attacks, where the LLM receives and acts upon instructions from supposedly data-only sources, thus deviating from the user's original instructions. We define this as task drift, and we propose to catch it by scanning and analyzing the LLM's activations. We compare the LLM's activations before and after processing the external input in order to detect whether this input caused instruction drift. We develop two probing methods and find that simply using a linear classifier can detect drift with near perfect ROC AUC on an out-of-distribution test set. We show that this approach generalizes surprisingly well to unseen task domains, such as prompt injections, jailbreaks, and malicious instructions, without being trained on any of these attacks. Our setup does not require any modification of the LLM (e.g., fine-tuning) or any text generation, thus maximizing deployability and cost efficiency and avoiding reliance on unreliable model output. To foster future research on activation-based task inspection, decoding, and interpretability, we will release our large-scale TaskTracker toolkit, comprising a dataset of over 500K instances, representations from 4 SoTA language models, and inspection tools.

------------

`[2406.01422] How to Understand Whole Software Repository? <https://arxiv.org/abs/2406.01422>`__ 如何理解整个软件仓库?

::

    Mon, 3 Jun 2024 15:20:06 GMT
    Yingwei Ma and Qingping Yang and Rongyu Cao and Binhua Li and Fei Huang and Yongbin Li

Recently, Large Language Model (LLM) based agents have advanced the significant development of Automatic Software Engineering (ASE). Although verified effectiveness, the designs of the existing methods mainly focus on the local information of codes, e.g., issues, classes, and functions, leading to limitations in capturing the global context and interdependencies within the software system. From the practical experiences of the human SE developers, we argue that an excellent understanding of the whole repository will be the critical path to ASE. However, understanding the whole repository raises various challenges, e.g., the extremely long code input, the noisy code information, the complex dependency relationships, etc. To this end, we develop a novel ASE method named RepoUnderstander by guiding agents to comprehensively understand the whole repositories. Specifically, we first condense the critical information of the whole repository into the repository knowledge graph in a top-to-down mode to decrease the complexity of repository. Subsequently, we empower the agents the ability of understanding whole repository by proposing a Monte Carlo tree search based repository exploration strategy. In addition, to better utilize the repository-level knowledge, we guide the agents to summarize, analyze, and plan. Then, they can manipulate the tools to dynamically acquire information and generate the patches to solve the real-world GitHub issues. Extensive experiments demonstrate the superiority and effectiveness of the proposed RepoUnderstander. It achieved 18.5\% relative improvement on the SWE-bench Lite benchmark compared to SWE-agent.

------------

`[2406.00054] $\epsilon$-Optimally Solving Zero-Sum POSGs <https://arxiv.org/abs/2406.00054>`__ $\epsilon$-零和posg的最优求解

::

    Wed, 29 May 2024 08:34:01 GMT
    Erwan Escudie, Matthia Sabatelli, Jilles Dibangoye

A recent method for solving zero-sum partially observable stochastic games (zs-POSGs) embeds the original game into a new one called the occupancy Markov game. This reformulation allows applying Bellman's principle of optimality to solve zs-POSGs. However, improving a current solution requires solving a linear program with exponentially many potential constraints, which significantly restricts the scalability of this approach. This paper exploits the optimal value function's novel uniform continuity properties to overcome this limitation. We first construct a new operator that is computationally more efficient than the state-of-the-art update rules without compromising optimality. In particular, improving a current solution now involves a linear program with an exponential drop in constraints. We then also show that point-based value iteration algorithms utilizing our findings improve the scalability of existing methods while maintaining guarantees in various domains.

------------

`[2406.00237] A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases <https://arxiv.org/abs/2406.00237>`__ CNN、ResNet和视觉transformer用于胸部疾病多分类的比较研究

::

    Fri, 31 May 2024 23:56:42 GMT
    Ananya Jain, Aviral Bhardwaj, Kaushik Murali, Isha Surani

Large language models, notably utilizing Transformer architectures, have emerged as powerful tools due to their scalability and ability to process large amounts of data. Dosovitskiy et al. expanded this architecture to introduce Vision Transformers (ViT), extending its applicability to image processing tasks. Motivated by this advancement, we fine-tuned two variants of ViT models, one pre-trained on ImageNet and another trained from scratch, using the NIH Chest X-ray dataset containing over 100,000 frontal-view X-ray images. Our study evaluates the performance of these models in the multi-label classification of 14 distinct diseases, while using Convolutional Neural Networks (CNNs) and ResNet architectures as baseline models for comparison.
Through rigorous assessment based on accuracy metrics, we identify that the pre-trained ViT model surpasses CNNs and ResNet in this multilabel classification task, highlighting its potential for accurate diagnosis of various lung conditions from chest X-ray images.

------------

`[2406.01566] Helix: Distributed Serving of Large Language Models via Max-Flow on Heterogeneous GPUs <https://arxiv.org/abs/2406.01566>`__ 

::

    Mon, 3 Jun 2024 17:47:53 GMT
    Yixuan Mei, Yonghao Zhuang, Xupeng Miao, Juncheng Yang, Zhihao Jia, Rashmi Vinayak

This paper introduces Helix, a distributed system for high-throughput, low-latency large language model (LLM) serving on heterogeneous GPU clusters. A key idea behind Helix is to formulate inference computation of LLMs over heterogeneous GPUs and network connections as a max-flow problem for a directed, weighted graph, whose nodes represent GPU instances and edges capture both GPU and network heterogeneity through their capacities. Helix then uses a mixed integer linear programming (MILP) algorithm to discover highly optimized strategies to serve LLMs. This approach allows Helix to jointly optimize model placement and request scheduling, two highly entangled tasks in heterogeneous LLM serving. Our evaluation on several heterogeneous cluster settings ranging from 24 to 42 GPU nodes shows that Helix improves serving throughput by up to 2.7$\times$ and reduces prompting and decoding latency by up to 2.8$\times$ and 1.3$\times$, respectively, compared to best existing approaches.

------------

`[2310.17807] Clover: Closed-Loop Verifiable Code Generation <https://arxiv.org/abs/2310.17807>`__ Clover:闭环可验证代码生成

::

    replaced with revised version Mon, 3 Jun 2024 16:59:37 GMT
    Submission history From: Chuyue Sun [view email]
    [v1] Thu, 26 Oct 2023 22:58:19 UTC (141 KB)
    [v2] Tue, 30 Jan 2024 06:52:10 UTC (174 KB)
    [v3] Mon, 3 Jun 2024 16:59:37 UTC (214 KB)
    Chuyue Sun, Ying Sheng, Oded Padon, Clark Barrett

The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results show that for this dataset, (i) LLMs are reasonably successful at automatically generating formal specifications; and (ii) our consistency checker achieves a promising acceptance rate (up to 87%) for correct instances while maintaining zero tolerance for incorrect ones (no false positives).

------------

`[2403.12451] INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations <https://arxiv.org/abs/2403.12451>`__ 洞察:基于语言解释的端到端神经符号视觉强化学习

::

    replaced with revised version Mon, 3 Jun 2024 06:50:51 GMT
    Submission history From: Lirui Luo [view email]
    [v1] Tue, 19 Mar 2024 05:21:20 UTC (4,298 KB)
    [v2] Mon, 27 May 2024 04:30:01 UTC (8,554 KB)
    [v3] Mon, 3 Jun 2024 06:50:51 UTC (8,587 KB)
    Lirui Luo, Guoxi Zhang, Hongming Xu, Yaodong Yang, Cong Fang, Qing Li

Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising paradigm for explainable decision-making, characterized by the interpretability of symbolic policies. NS-RL entails structured state representations for tasks with visual observations, but previous methods are unable to refine the structured states with rewards due to a lack of efficiency. Accessibility also remains to be an issue, as extensive domain knowledge is required to interpret symbolic policies. In this paper, we present a framework for learning structured states and symbolic policies jointly, whose key idea is to distill vision foundation models into a scalable perception module and refine it during policy learning. Moreover, we design a pipeline to generate language explanations for policies and decisions using large language models. In experiments on nine Atari tasks, we verify the efficacy of our approach, and we also present explanations for policies and decisions.

------------

`[2404.11515] Embedding Privacy in Computational Social Science and Artificial Intelligence Research <https://arxiv.org/abs/2404.11515>`__ 计算社会科学与人工智能研究中的隐私嵌入

::

    replaced with revised version Mon, 3 Jun 2024 14:32:04 GMT
    Submission history From: Jason R.C. Nurse Dr [view email]
    [v1] Wed, 17 Apr 2024 16:07:53 UTC (293 KB)
    [v2] Mon, 3 Jun 2024 14:32:04 UTC (23 KB)
    Keenan Jones and Fatima Zahrah and Jason R.C. Nurse

Privacy is a human right. It ensures that individuals are free to engage in discussions, participate in groups, and form relationships online or offline without fear of their data being inappropriately harvested, analyzed, or otherwise used to harm them. Preserving privacy has emerged as a critical factor in research, particularly in the computational social science (CSS), artificial intelligence (AI) and data science domains, given their reliance on individuals' data for novel insights. The increasing use of advanced computational models stands to exacerbate privacy concerns because, if inappropriately used, they can quickly infringe privacy rights and lead to adverse effects for individuals -- especially vulnerable groups -- and society. We have already witnessed a host of privacy issues emerge with the advent of large language models (LLMs), such as ChatGPT, which further demonstrate the importance of embedding privacy from the start. This article contributes to the field by discussing the role of privacy and the issues that researchers working in CSS, AI, data science and related domains are likely to face. It then presents several key considerations for researchers to ensure participant privacy is best preserved in their research design, data collection and use, analysis, and dissemination of research results.

------------

`[2405.11143] OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework <https://arxiv.org/abs/2405.11143>`__ OpenRLHF:一个易用、可扩展、高性能的RLHF框架

::

    replaced with revised version Mon, 3 Jun 2024 12:19:18 GMT
    Submission history From: Jian Hu [view email]
    [v1] Mon, 20 May 2024 01:04:40 UTC (1,063 KB)
    [v2] Mon, 3 Jun 2024 12:19:18 UTC (1,068 KB)
    Jian Hu, Xibin Wu, Weixun Wang, Xianyu, Dehao Zhang, Yu Cao

As large language models (LLMs) continue to grow by scaling laws, reinforcement learning from human feedback (RLHF) has gained significant attention due to its outstanding performance. However, unlike pretraining or fine-tuning a single model, scaling reinforcement learning from human feedback (RLHF) for training large language models poses coordination challenges across four models. We present OpenRLHF, an open-source framework enabling efficient RLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the same GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters using Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and diverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF provides an out-of-the-box solution with optimized algorithms and launch scripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO, rejection sampling, and other alignment techniques. Empowering state-of-the-art LLM development, OpenRLHF's code is available at this https URL.

------------

`[2405.19616] Easy Problems That LLMs Get Wrong <https://arxiv.org/abs/2405.19616>`__ llm会出错的简单问题

::

    replaced with revised version Sat, 1 Jun 2024 03:00:37 GMT
    Submission history From: James Huckle [view email]
    [v1] Thu, 30 May 2024 02:09:51 UTC (82 KB)
    [v2] Sat, 1 Jun 2024 03:00:37 UTC (82 KB)
    Sean Williams, James Huckle

We introduce a comprehensive Linguistic Benchmark designed to evaluate the limitations of Large Language Models (LLMs) in domains such as logical reasoning, spatial intelligence, and linguistic understanding, among others. Through a series of straightforward questions, it uncovers the significant limitations of well-regarded models to perform tasks that humans manage with ease. It also highlights the potential of prompt engineering to mitigate some errors and underscores the necessity for better training methodologies. Our findings stress the importance of grounding LLMs with human reasoning and common sense, emphasising the need for human-in-the-loop for enterprise applications. We hope this work paves the way for future research to enhance the usefulness and reliability of new models.

------------

`[2308.07120] Position: Key Claims in LLM Research Have a Long Tail of Footnotes <https://arxiv.org/abs/2308.07120>`__ 职位:LLM研究中的关键主张有很多脚注

::

    replaced with revised version Sat, 1 Jun 2024 15:20:25 GMT
    Submission history From: Anna Rogers [view email]
    [v1] Mon, 14 Aug 2023 13:00:53 UTC (102 KB)
    [v2] Sat, 1 Jun 2024 15:20:25 UTC (62 KB)
    Anna Rogers and Alexandra Sasha Luccioni

Much of the recent discourse within the ML community has been centered around Large Language Models (LLMs), their functionality and potential -- yet not only do we not have a working definition of LLMs, but much of this discourse relies on claims and assumptions that are worth re-examining. We contribute a definition of LLMs, critically examine five common claims regarding their properties (including 'emergent properties'), and conclude with suggestions for future research directions and their framing.

------------

`[2308.10379] Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models <https://arxiv.org/abs/2308.10379>`__ 

::

    replaced with revised version Sun, 2 Jun 2024 16:01:35 GMT
    Submission history From: Bilgehan Sel [view email]
    [v1] Sun, 20 Aug 2023 22:36:23 UTC (331 KB)
    [v2] Thu, 28 Sep 2023 11:28:24 UTC (331 KB)
    [v3] Sun, 2 Jun 2024 16:01:35 UTC (334 KB)
    Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, Ming Jin

Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to external modi operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. Due to their myopic perspective, they escalate the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways. By employing algorithmic examples fully in-context, this overarching view of the whole process exploits the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and even more recent multi-query strategies that employ an extensive tree search algorithms while using significantly fewer tokens. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application. The code and related content can be found in: this https URL.

------------

`[2309.08637] TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild <https://arxiv.org/abs/2309.08637>`__ TextBind:野外多回合交错多模态指令跟踪

::

    replaced with revised version Mon, 3 Jun 2024 13:39:40 GMT
    Submission history From: Huayang Li [view email]
    [v1] Thu, 14 Sep 2023 15:34:01 UTC (16,424 KB)
    [v2] Tue, 19 Sep 2023 02:04:11 UTC (16,815 KB)
    [v3] Wed, 8 Nov 2023 09:33:31 UTC (16,863 KB)
    [v4] Mon, 8 Jan 2024 13:02:04 UTC (16,867 KB)
    [v5] Mon, 3 Jun 2024 13:39:40 UTC (16,861 KB)
    Huayang Li and Siheng Li and Deng Cai and Longyue Wang and Lemao Liu and Taro Watanabe and Yujiu Yang and Shuming Shi

Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. To accommodate interleaved image-text inputs and outputs, we devise MIM, a language model-centric architecture that seamlessly integrates image encoder and decoder models. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.

------------

`[2310.08540] Do pretrained Transformers Learn In-Context by Gradient Descent? <https://arxiv.org/abs/2310.08540>`__ 预训练transformer是通过梯度下降在上下文中学习吗?

::

    replaced with revised version Mon, 3 Jun 2024 04:18:11 GMT
    Submission history From: Lingfeng Shen [view email]
    [v1] Thu, 12 Oct 2023 17:32:09 UTC (1,761 KB)
    [v2] Fri, 24 Nov 2023 20:24:52 UTC (3,222 KB)
    [v3] Thu, 30 Nov 2023 01:34:31 UTC (3,199 KB)
    [v4] Thu, 29 Feb 2024 18:47:18 UTC (3,213 KB)
    [v5] Mon, 3 Jun 2024 04:18:11 UTC (3,262 KB)
    Lingfeng Shen, Aayush Mishra, Daniel Khashabi

The emergence of In-Context Learning (ICL) in LLMs remains a remarkable phenomenon that is partially understood. To explain ICL, recent studies have created theoretical connections to Gradient Descent (GD). We ask, do such connections hold up in actual pre-trained language models? We highlight the limiting assumptions in prior works that make their setup considerably different from the practical setup in which language models are trained. For example, their experimental verification uses \emph{ICL objective} (training models explicitly for ICL), which differs from the emergent ICL in the wild. Furthermore, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons of three performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and the number of demonstrations. We observe that ICL and GD modify the output distribution of language models differently. These results indicate that \emph{the equivalence between ICL and GD remains an open hypothesis} and calls for further studies.

------------

`[2311.07466] On Measuring Faithfulness or Self-consistency of Natural Language Explanations <https://arxiv.org/abs/2311.07466>`__ 自然语言解释的忠实度或自洽性的测量

::

    replaced with revised version Sat, 1 Jun 2024 07:57:52 GMT
    Submission history From: Letitia Parcalabescu [view email]
    [v1] Mon, 13 Nov 2023 16:53:51 UTC (7,696 KB)
    [v2] Sat, 10 Feb 2024 18:31:13 UTC (8,499 KB)
    [v3] Sat, 1 Jun 2024 07:57:52 UTC (8,503 KB)
    Letitia Parcalabescu and Anette Frank

Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model's input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to compare LLM behaviour when making predictions and to analyse the effect of other consistency tests at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests. Our code is available at \url{this https URL}

------------

`[2311.08045] Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM Game <https://arxiv.org/abs/2311.08045>`__ 对抗性偏好优化:通过RM-LLM游戏增强您的对齐

::

    replaced with revised version Mon, 3 Jun 2024 11:34:05 GMT
    Submission history From: Pengyu Cheng [view email]
    [v1] Tue, 14 Nov 2023 10:10:31 UTC (428 KB)
    [v2] Mon, 19 Feb 2024 07:25:00 UTC (1,372 KB)
    [v3] Fri, 23 Feb 2024 08:58:34 UTC (1,393 KB)
    [v4] Mon, 3 Jun 2024 11:34:05 UTC (1,511 KB)
    Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, Tianhao Hu, Peixin Cao, Nan Du, Xiaolong Li

Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing alignment methods depend on manually annotated preference data to guide the LLM optimization directions. However, continuously updating LLMs for alignment raises a distribution gap between model-generated samples and human-annotated responses, hindering training effectiveness. To mitigate this issue, previous methods require additional preference annotation on newly generated samples to adapt to the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an Adversarial Preference Optimization (APO) framework, in which the LLM and the reward model update alternatively via a min-max game. Through adversarial training, the reward model can adapt to the shifted generation distribution of the LLM without any additional annotation. With comprehensive experiments, we find the proposed adversarial training framework further enhances existing alignment baselines in terms of LLM helpfulness and harmlessness. The code is at this https URL.

------------

`[2311.09071] How Vocabulary Sharing Facilitates Multilingualism in LLaMA? <https://arxiv.org/abs/2311.09071>`__ 

::

    replaced with revised version Mon, 3 Jun 2024 06:11:06 GMT
    Submission history From: Fei Yuan [view email]
    [v1] Wed, 15 Nov 2023 16:13:14 UTC (7,482 KB)
    [v2] Mon, 3 Jun 2024 06:11:06 UTC (808 KB)
    Fei Yuan, Shuai Yuan, Zhiyong Wu, Lei Li

Large Language Models (LLMs), often show strong performance on English tasks, while exhibiting limitations on other languages. What is an LLM's multilingual capability when it is trained only on certain languages? The underlying mechanism remains unclear. This study endeavors to examine the multilingual capability of LLMs from the vocabulary sharing perspective by conducting an exhaustive analysis across 101 languages. Through the investigation of the performance gap before and after embedding fine-tuning, we discovered four distinct quadrants. By delving into each quadrant we provide actionable and efficient guidelines for tuning these languages. Extensive experiments reveal that existing LLMs possess multilingual capabilities that surpass our expectations, and we can significantly improve the multilingual performance of LLMs based on these attributes of each quadrant~\footnote{\url{this https URL}.}.

------------

`[2311.09154] CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models <https://arxiv.org/abs/2311.09154>`__ Clean - eval:受污染大型语言模型的干净评估

::

    replaced with revised version Mon, 3 Jun 2024 03:06:55 GMT
    Submission history From: Wenhong Zhu [view email]
    [v1] Wed, 15 Nov 2023 17:50:30 UTC (1,793 KB)
    [v2] Wed, 28 Feb 2024 10:43:12 UTC (3,409 KB)
    [v3] Mon, 3 Jun 2024 03:06:55 UTC (3,643 KB)
    Wenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu Hu, Yiran Wei, Rui Wang, Hongyuan Lu

We are currently in an era of fierce competition among various large language models (LLMs) continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination, and it wastes dozens of time and effort for researchers and engineers to download and try those contaminated models. To save our precious time, we propose a novel and useful method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter the generated low-quality samples to narrow down this candidate set. The best candidate is finally selected from this set based on the BLEURT score. According to human assessment, this best candidate is semantically similar to the original contamination data but expressed differently. All candidates can form a new benchmark to evaluate the model. Our experiments illustrate that Clean-Eval substantially restores the actual evaluation results on contaminated LLMs under both few-shot learning and fine-tuning scenarios.

------------

`[2311.09189] PsyEval: A Suite of Mental Health Related Tasks for Evaluating Large Language Models <https://arxiv.org/abs/2311.09189>`__ pyeval:一套用于评估大型语言模型的心理健康相关任务

::

    replaced with revised version Mon, 3 Jun 2024 08:37:10 GMT
    Submission history From: Haoan Jin [view email]
    [v1] Wed, 15 Nov 2023 18:32:27 UTC (674 KB)
    [v2] Mon, 3 Jun 2024 08:37:10 UTC (3,927 KB)
    Haoan Jin, Siyuan Chen, Dilawaier Dilixiati, Yewei Jiang, Mengyue Wu, Kenny Q. Zhu

Evaluating Large Language Models (LLMs) in the mental health domain poses distinct challenged from other domains, given the subtle and highly subjective nature of symptoms that exhibit significant variability among individuals. This paper presents PsyEval, the first comprehensive suite of mental health-related tasks for evaluating LLMs. PsyEval encompasses five sub-tasks that evaluate three critical dimensions of mental health. This comprehensive framework is designed to thoroughly assess the unique challenges and intricacies of mental health-related tasks, making PsyEval a highly specialized and valuable tool for evaluating LLM performance in this domain. We evaluate twelve advanced LLMs using PsyEval. Experiment results not only demonstrate significant room for improvement in current LLMs concerning mental health but also unveil potential directions for future model optimization.

------------

`[2312.10302] One-Shot Learning as Instruction Data Prospector for Large Language Models <https://arxiv.org/abs/2312.10302>`__ 单次学习作为大型语言模型的指令数据勘探者

::

    replaced with revised version Mon, 3 Jun 2024 13:46:16 GMT
    Submission history From: Yunshui Li [view email]
    [v1] Sat, 16 Dec 2023 03:33:12 UTC (976 KB)
    [v2] Tue, 19 Dec 2023 03:48:21 UTC (3,292 KB)
    [v3] Thu, 4 Jan 2024 18:00:11 UTC (3,291 KB)
    [v4] Mon, 3 Jun 2024 13:46:16 UTC (8,575 KB)
    Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Ling-Hao Chen, Junhao Liu, Tongliang Liu, Fei Huang, Yongbin Li

Contemporary practices in instruction tuning often hinge on enlarging data scaling without a clear strategy for ensuring data quality, inadvertently introducing noise that may compromise model performance. To address this challenge, we introduce \textsc{Nuggets}, a novel and efficient methodology that leverages one-shot learning to discern and select high-quality instruction data from extensive datasets. \textsc{Nuggets} assesses the potential of individual instruction examples to act as effective one-shot learning instances, thereby identifying those that can significantly improve performance across diverse tasks. \textsc{Nuggets} utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most advantageous data for instruction tuning. Through comprehensive evaluations on two benchmarks, including MT-Bench and Alpaca-Eval, we show that instruction tuning with the top 1\% of examples curated by \textsc{Nuggets} substantially outperforms conventional methods employing the entire dataset.

------------

`[2312.11075] Split and Rephrase with Large Language Models <https://arxiv.org/abs/2312.11075>`__ 用大型语言模型进行拆分和重短语

::

    replaced with revised version Mon, 3 Jun 2024 10:00:13 GMT
    Submission history From: David Ponce Martínez [view email]
    [v1] Mon, 18 Dec 2023 10:16:37 UTC (7,876 KB)
    [v2] Tue, 19 Dec 2023 07:47:09 UTC (7,875 KB)
    [v3] Fri, 16 Feb 2024 10:47:16 UTC (8,236 KB)
    [v4] Mon, 3 Jun 2024 10:00:13 UTC (8,237 KB)
    David Ponce, Thierry Etchegoyhen, Jes\'us Calleja P\'erez, Harritxu Gete

The Split and Rephrase (SPRP) task, which consists in splitting complex sentences into a sequence of shorter grammatical sentences, while preserving the original meaning, can facilitate the processing of complex texts for humans and machines alike. It is also a valuable testbed to evaluate natural language processing models, as it requires modelling complex grammatical aspects. In this work, we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance. Results from two human evaluations further support the conclusions drawn from automated metric results. We provide a comprehensive study that includes prompting variants, domain shift, fine-tuned pretrained language models of varying parameter size and training data volumes, contrasted with both zero-shot and few-shot approaches on instruction-tuned language models. Although the latter were markedly outperformed by fine-tuned models, they may constitute a reasonable off-the-shelf alternative. Our results provide a fine-grained analysis of the potential and limitations of large language models for SPRP, with significant improvements achievable using relatively small amounts of training data and model parameters overall, and remaining limitations for all models on the task.

------------

`[2312.12999] Machine Mindset: An MBTI Exploration of Large Language Models <https://arxiv.org/abs/2312.12999>`__ 机器思维:大型语言模型的MBTI探索

::

    replaced with revised version Sun, 2 Jun 2024 03:57:06 GMT
    Submission history From: Jiaxi Cui [view email]
    [v1] Wed, 20 Dec 2023 12:59:31 UTC (4,886 KB)
    [v2] Thu, 21 Dec 2023 07:45:43 UTC (4,886 KB)
    [v3] Sat, 30 Dec 2023 17:39:56 UTC (4,886 KB)
    [v4] Sun, 2 Jun 2024 03:57:06 UTC (4,886 KB)
    Jiaxi Cui, Liuzhenghao Lv, Jing Wen, Rongsheng Wang, Jing Tang, YongHong Tian, Li Yuan

We present a novel approach for integrating Myers-Briggs Type Indicator (MBTI) personality traits into large language models (LLMs), addressing the challenges of personality consistency in personalized AI. Our method, "Machine Mindset," involves a two-phase fine-tuning and Direct Preference Optimization (DPO) to embed MBTI traits into LLMs. This approach ensures that models internalize these traits, offering a stable and consistent personality profile. We demonstrate the effectiveness of our models across various domains, showing alignment between model performance and their respective MBTI traits. The paper highlights significant contributions in the development of personality datasets and a new training methodology for personality integration in LLMs, enhancing the potential for personalized AI applications. We also open-sourced our model and part of the data at \url{this https URL}.

------------

`[2312.14591] Reasons to Reject? Aligning Language Models with Judgments <https://arxiv.org/abs/2312.14591>`__ 拒绝的理由?将语言模型与判断对齐

::

    replaced with revised version Mon, 3 Jun 2024 02:24:38 GMT
    Submission history From: Weiwen Xu [view email]
    [v1] Fri, 22 Dec 2023 10:29:43 UTC (449 KB)
    [v2] Mon, 27 May 2024 12:22:14 UTC (472 KB)
    [v3] Mon, 3 Jun 2024 02:24:38 UTC (484 KB)
    Weiwen Xu, Deng Cai, Zhisong Zhang, Wai Lam, Shuming Shi

As humans, we consistently interact with our peers and receive feedback in the form of natural language. This language feedback allows us to maintain appropriate behavior, and rectify potential errors. The question arises naturally: can we use language feedback to align large language models (LLMs)? In contrast to previous research that aligns LLMs with scalar rewards, we present the first systematic exploration of alignment through the lens of language feedback (i.e., judgment). We start with an in-depth investigation of potential methods that can be adapted for aligning LLMs with judgments, revealing that these methods cannot fully capitalize on judgments. To facilitate more effective utilization of judgments, we propose a novel framework, Contrastive Unlikelihood Training (CUT), that allows for fine-grained inappropriate content detection and correction based on judgments. Our results show that, with merely 1317 off-the-shelf judgment data, CUT (LLaMA2-13b) can beat the 175B DaVinci003 and surpass the best baseline by 50.84 points on AlpacaEval. CUT (LLaMA2-chat-13b) can also align LLMs in an iterative fashion using up-to-date model-specific judgments, improving performance from 81.09 to 91.68 points on AlpacaEval. Further analysis suggests that judgments hold greater potential than rewards in LLM alignment.

------------

`[2401.04518] The Critique of Critique <https://arxiv.org/abs/2401.04518>`__ 批判的批判

::

    replaced with revised version Sat, 1 Jun 2024 17:52:14 GMT
    Submission history From: Shichao Sun [view email]
    [v1] Tue, 9 Jan 2024 12:20:41 UTC (8,048 KB)
    [v2] Sat, 1 Jun 2024 17:52:14 UTC (8,238 KB)
    Shichao Sun, Junlong Li, Weizhe Yuan, Ruifeng Yuan, Wenjie Li, Pengfei Liu

Critique, as a natural language description for assessing the quality of model-generated content, has played a vital role in the training, evaluation, and refinement of LLMs. However, a systematic method to evaluate the quality of critique is lacking. In this paper, we pioneer the critique of critique, termed MetaCritique, which builds specific quantification criteria. To achieve a reliable evaluation outcome, we propose Atomic Information Units (AIUs), which describe the critique in a more fine-grained manner. MetaCritique aggregates each AIU's judgment for the overall score. Moreover, MetaCritique delivers a natural language rationale for the intricate reasoning within each judgment. Lastly, we construct a meta-evaluation dataset covering 4 tasks across 16 public datasets involving human-written and LLM-generated critiques. Experiments demonstrate that MetaCritique can achieve near-human performance. Our study can facilitate future research in LLM critiques based on our following observations and released resources: (1) superior critiques judged by MetaCritique can lead to better refinements, indicating that it can potentially enhance the alignment of existing LLMs; (2) the leaderboard of critique models reveals that open-source critique models commonly suffer from factuality issues; (3) relevant code and data are publicly available at this https URL to support deeper exploration; (4) an API at PyPI with the usage documentation in Appendix C allows users to assess the critique conveniently.

------------

`[2401.04854] Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs <https://arxiv.org/abs/2401.04854>`__ 语言模型更像图书馆还是更像图书管理员?Bibliotechnism, Novel Reference Problem，以及LLMs的态度

::

    replaced with revised version Mon, 3 Jun 2024 17:01:06 GMT
    Submission history From: Kyle Mahowald [view email]
    [v1] Wed, 10 Jan 2024 00:05:45 UTC (34 KB)
    [v2] Thu, 15 Feb 2024 22:02:32 UTC (39 KB)
    [v3] Mon, 3 Jun 2024 17:01:06 UTC (41 KB)
    Harvey Lederman, Kyle Mahowald

Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism, is that LLMs generate novel text. We begin with a defense of bibliotechnism, showing how even novel text may inherit its meaning from original human-generated text. We then argue that bibliotechnism faces an independent challenge from examples in which LLMs generate novel reference, using new names to refer to new entities. Such examples could be explained if LLMs were not cultural technologies but had beliefs, desires, and intentions. According to interpretationism in the philosophy of mind, a system has such attitudes if and only if its behavior is well explained by the hypothesis that it does. Interpretationists may hold that LLMs have attitudes, and thus have a simple solution to the novel reference problem. We emphasize, however, that interpretationism is compatible with very simple creatures having attitudes and differs sharply from views that presuppose these attitudes require consciousness, sentience, or intelligence (topics about which we make no claims).

------------

`[2401.08417] Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation <https://arxiv.org/abs/2401.08417>`__ 对比偏好优化:突破机器翻译中LLM性能的边界

::

    replaced with revised version Mon, 3 Jun 2024 01:28:06 GMT
    Submission history From: Haoran Xu [view email]
    [v1] Tue, 16 Jan 2024 15:04:51 UTC (180 KB)
    [v2] Thu, 18 Jan 2024 09:31:28 UTC (180 KB)
    [v3] Fri, 2 Feb 2024 09:10:11 UTC (1,308 KB)
    [v4] Mon, 3 Jun 2024 01:28:06 UTC (1,311 KB)
    Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, Young Jin Kim

Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.

------------

`[2402.02801] KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models <https://arxiv.org/abs/2402.02801>`__ KS-Lottery:为多语言语言模型查找认证彩票

::

    replaced with revised version Mon, 3 Jun 2024 07:35:25 GMT
    Submission history From: Fei Yuan [view email]
    [v1] Mon, 5 Feb 2024 08:19:56 UTC (1,844 KB)
    [v2] Mon, 3 Jun 2024 07:35:25 UTC (2,959 KB)
    Fei Yuan, Chang Ma, Shuai Yuan, Qiushi Sun, Lei Li

The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embedding of LLaMA suffices to reach the fine-tuning translation performance~\footnote{this https URL.}.

------------

`[2402.04601] Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector <https://arxiv.org/abs/2402.04601>`__ Alirector:对齐增强的中文语法纠错器

::

    replaced with revised version Sun, 2 Jun 2024 15:50:40 GMT
    Submission history From: Haihui Yang [view email]
    [v1] Wed, 7 Feb 2024 05:56:54 UTC (229 KB)
    [v2] Sun, 2 Jun 2024 15:50:40 UTC (231 KB)
    Haihui Yang and Xiaojun Quan

Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment knowledge from two alignment models to the correction model, instructing it on how to avoid overcorrection. Experimental results on three CGEC datasets demonstrate the effectiveness of our approach in alleviating overcorrection and improving overall performance. Our code has been made publicly available.

------------

`[2402.07616] Anchor-based Large Language Models <https://arxiv.org/abs/2402.07616>`__ 基于锚点的大型语言模型

::

    replaced with revised version Sat, 1 Jun 2024 04:52:17 GMT
    Submission history From: Jianhui Pang [view email]
    [v1] Mon, 12 Feb 2024 12:48:02 UTC (7,774 KB)
    [v2] Fri, 16 Feb 2024 16:58:04 UTC (7,782 KB)
    [v3] Sat, 1 Jun 2024 04:52:17 UTC (7,799 KB)
    Jianhui Pang, Fanghua Ye, Derek Fai Wong, Xin He, Wanshun Chen, Longyue Wang

Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces Anchor-based LLMs (AnLLMs), which utilize an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments on question-answering benchmarks reveal that AnLLMs maintain similar accuracy levels while achieving up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the substantial enhancements of AnLLMs employing the AnSAN technique in resource utilization and computational efficiency underscore their potential for practical LLM applications.

------------

`[2402.08277] Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering <https://arxiv.org/abs/2402.08277>`__ 忠实和强大的LLM专家以证据为基础的问题回答

::

    replaced with revised version Mon, 3 Jun 2024 16:48:59 GMT
    Submission history From: Tobias Schimanski [view email]
    [v1] Tue, 13 Feb 2024 08:12:48 UTC (8,720 KB)
    [v2] Fri, 16 Feb 2024 11:49:21 UTC (8,720 KB)
    [v3] Mon, 26 Feb 2024 11:59:28 UTC (8,731 KB)
    [v4] Tue, 21 May 2024 10:17:03 UTC (8,779 KB)
    [v5] Mon, 3 Jun 2024 16:48:59 UTC (8,779 KB)
    Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold

Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.

------------

`[2402.09025] SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks <https://arxiv.org/abs/2402.09025>`__ SLEB:通过冗余验证和消除变压器块精简llm

::

    replaced with revised version Sat, 1 Jun 2024 12:10:48 GMT
    Submission history From: Jiwon Song [view email]
    [v1] Wed, 14 Feb 2024 09:01:13 UTC (8,750 KB)
    [v2] Wed, 29 May 2024 02:53:16 UTC (8,759 KB)
    [v3] Sat, 1 Jun 2024 12:10:48 UTC (8,760 KB)
    Jiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon Kim

Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB outperforms previous LLM pruning methods in accelerating LLM inference while also maintaining superior perplexity and accuracy, making SLEB as a promising technique for enhancing the efficiency of LLMs. The code is available at: this https URL.

------------

`[2402.09259] SyntaxShap: Syntax-aware Explainability Method for Text Generation <https://arxiv.org/abs/2402.09259>`__ SyntaxShap:面向文本生成的句法感知可解释性方法

::

    replaced with revised version Mon, 3 Jun 2024 10:30:00 GMT
    Submission history From: Kenza Amara [view email]
    [v1] Wed, 14 Feb 2024 15:45:56 UTC (2,086 KB)
    [v2] Mon, 3 Jun 2024 10:30:00 UTC (5,038 KB)
    Kenza Amara, Rita Sevastjanova, Mennatallah El-Assady

To harness the power of large language models in safety-critical domains, we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data. The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, coherency, and semantic alignment of the explanations to the model. We show that our syntax-aware method produces explanations that help build more faithful and coherent explanations for predictions by autoregressive models. Confronted with the misalignment of human and AI model reasoning, this paper also highlights the need for cautious evaluation strategies in explainable AI.

------------

`[2402.11073] AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators <https://arxiv.org/abs/2402.11073>`__ AFaCTA:使用可靠的LLM标注者辅助标注事实声明检测

::

    replaced with revised version Sun, 2 Jun 2024 18:35:25 GMT
    Submission history From: Jingwei Ni [view email]
    [v1] Fri, 16 Feb 2024 20:59:57 UTC (7,932 KB)
    [v2] Wed, 22 May 2024 14:42:23 UTC (7,932 KB)
    [v3] Sun, 2 Jun 2024 18:35:25 UTC (7,932 KB)
    Jingwei Ni, Minjing Shi, Dominik Stammbach, Mrinmaya Sachan, Elliott Ash, Markus Leippold

With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.

------------

`[2402.11192] I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses <https://arxiv.org/abs/2402.11192>`__ 如果你说我的语言，我会学得更好:用llm生成的响应来理解微调大型语言模型的优越性能

::

    replaced with revised version Sat, 1 Jun 2024 03:36:23 GMT
    Submission history From: Xuan Ren [view email]
    [v1] Sat, 17 Feb 2024 05:05:31 UTC (2,624 KB)
    [v2] Sat, 1 Jun 2024 03:36:23 UTC (565 KB)
    Xuan Ren and Biao Wu and Lingqiao Liu

This paper explores an intriguing observation: fine-tuning a large language model (LLM) with responses generated by a LLM often yields better results than using responses generated by humans. We conduct an in-depth investigation to understand why this occurs. Contrary to the common belief that these instances is simply due to the more detailed nature of LLM-generated content, our study identifies another contributing factor: an LLM is inherently more "familiar" with LLM generated responses. This familiarity is evidenced by lower perplexity before fine-tuning. We design a series of experiments to understand the impact of the "familiarity" and our conclusion reveals that this "familiarity" significantly impacts learning performance. Training with LLM-generated responses not only enhances performance but also helps maintain the model's capabilities in other tasks after fine-tuning on a specific task.

------------

`[2402.11900] Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models <https://arxiv.org/abs/2402.11900>`__ 大型语言模型知识编辑中的多跳事实捷径研究

::

    replaced with revised version Sun, 2 Jun 2024 09:17:37 GMT
    Submission history From: Tianjie Ju [view email]
    [v1] Mon, 19 Feb 2024 07:34:10 UTC (840 KB)
    [v2] Sun, 2 Jun 2024 09:17:37 UTC (850 KB)
    Tianjie Ju, Yijin Chen, Xinwei Yuan, Zhuosheng Zhang, Wei Du, Yubin Zheng, Gongshen Liu

Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximately 20% of the failures are attributed to shortcuts, and the initial and terminal entities in these failure instances usually have higher co-occurrences in the pre-training corpus. Finally, we propose erasing shortcut neurons to mitigate the associated risks and find that this approach significantly reduces failures in multiple-hop knowledge editing caused by shortcuts.

------------

`[2402.13963] Towards Building Multilingual Language Model for Medicine <https://arxiv.org/abs/2402.13963>`__ 面向医学的多语言语言模型构建

::

    replaced with revised version Sun, 2 Jun 2024 10:02:00 GMT
    Submission history From: Pengcheng Qiu [view email]
    [v1] Wed, 21 Feb 2024 17:47:20 UTC (5,297 KB)
    [v2] Mon, 26 Feb 2024 11:01:25 UTC (5,424 KB)
    [v3] Wed, 29 May 2024 06:15:38 UTC (4,516 KB)
    [v4] Sun, 2 Jun 2024 10:02:00 UTC (4,521 KB)
    Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, Weidi Xie

The development of open-source, multilingual medical language models can benefit a wide, linguistically diverse audience from different regions. To promote this domain, we present contributions from the following: First, we construct a multilingual medical corpus, containing approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, enabling auto-regressive domain adaptation for general LLMs; Second, to monitor the development of multilingual medical LLMs, we propose a multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; Third, we have assessed a number of open-source large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC. Our final model, MMed-Llama 3, with only 8B parameters, achieves superior performance compared to all other open-source models on both MMedBench and English benchmarks, even rivaling GPT-4. In conclusion, in this work, we present a large-scale corpus, a benchmark and a series of models to support the development of multilingual medical LLMs.

------------

`[2402.14526] Balanced Data Sampling for Language Model Training with Clustering <https://arxiv.org/abs/2402.14526>`__ 基于聚类的语言模型训练的平衡数据采样

::

    replaced with revised version Mon, 3 Jun 2024 06:48:34 GMT
    Submission history From: Yunfan Shao [view email]
    [v1] Thu, 22 Feb 2024 13:20:53 UTC (7,013 KB)
    [v2] Mon, 3 Jun 2024 06:48:34 UTC (7,018 KB)
    Yunfan Shao, Linyang Li, Zhaoye Fei, Hang Yan, Dahua Lin, Xipeng Qiu

Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperforms random sampling and other cluster-based sampling variants under various training datasets and large language models.

------------

`[2402.15043] KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models <https://arxiv.org/abs/2402.15043>`__ KIEval:基于知识的大型语言模型交互式评估框架

::

    replaced with revised version Mon, 3 Jun 2024 06:02:39 GMT
    Submission history From: Zhuohao Yu [view email]
    [v1] Fri, 23 Feb 2024 01:30:39 UTC (300 KB)
    [v2] Mon, 3 Jun 2024 06:02:39 UTC (1,246 KB)
    Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang

Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval's effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.

------------

`[2402.17959] An Iterative Associative Memory Model for Empathetic Response Generation <https://arxiv.org/abs/2402.17959>`__ 面向共情反应生成的迭代联想记忆模型

::

    replaced with revised version Sun, 2 Jun 2024 10:46:13 GMT
    Submission history From: Zhou Yang [view email]
    [v1] Wed, 28 Feb 2024 00:49:06 UTC (104 KB)
    [v2] Sun, 2 Jun 2024 10:46:13 UTC (105 KB)
    Zhou Yang, Zhaochun Ren, Yufeng Wang, Chao Chen, Haizhou Sun, Xiaofei Zhu, Xiangwen Liao

Empathetic response generation aims to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Variant experiments on LLMs also demonstrate that attending to associated words improves empathetic comprehension and expression.

------------

`[2403.00862] NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism <https://arxiv.org/abs/2403.00862>`__ NewsBench:一个评估中文新闻大型语言模型编辑能力的系统评估框架

::

    replaced with revised version Sun, 2 Jun 2024 13:38:01 GMT
    Submission history From: Miao Li [view email]
    [v1] Thu, 29 Feb 2024 21:05:14 UTC (165 KB)
    [v2] Thu, 21 Mar 2024 10:14:09 UTC (165 KB)
    [v3] Sun, 2 Jun 2024 13:38:01 UTC (134 KB)
    Miao Li and Ming-Bin Chen and Bo Tang and Shengbin Hou and Pengyu Wang and Haiying Deng and Zhiyu Li and Feiyu Xiong and Keming Mao and Peng Cheng and Yi Luo

We present NewsBench, a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. Our constructed benchmark dataset is focused on four facets of writing proficiency and six facets of safety adherence, and it comprises manually and carefully designed 1,267 test samples in the types of multiple choice questions and short answer questions for five editorial tasks in 24 news domains. To measure performances, we propose different GPT-4 based automatic evaluation protocols to assess LLM generations for short answer questions in terms of writing proficiency and safety adherence, and both are validated by the high correlations with human evaluations. Based on the systematic evaluation framework, we conduct a comprehensive analysis of ten popular LLMs which can handle Chinese. The experimental results highlight GPT-4 and ERNIE Bot as top performers, yet reveal a relative deficiency in journalistic safety adherence in creative writing tasks. Our findings also underscore the need for enhanced ethical guidance in machine-generated journalistic content, marking a step forward in aligning LLMs with journalistic standards and safety considerations.

------------

`[2403.01748] NeuSpeech: Decode Neural signal as Speech <https://arxiv.org/abs/2403.01748>`__ 

::

    replaced with revised version Mon, 3 Jun 2024 16:58:04 GMT
    Submission history From: Yiqian Yang [view email]
    [v1] Mon, 4 Mar 2024 05:55:01 UTC (330 KB)
    [v2] Tue, 26 Mar 2024 15:26:21 UTC (329 KB)
    [v3] Mon, 3 Jun 2024 16:58:04 UTC (251 KB)
    Yiqian Yang, Yiqun Duan, Qiang Zhang, Hyejeong Jo, Jinni Zhou, Won Hee Lee, Renjing Xu, Hui Xiong

Decoding language from brain dynamics is an important open direction in the realm of brain-computer interface (BCI), especially considering the rapid growth of large language models. Compared to invasive-based signals which require electrode implantation surgery, non-invasive neural signals (e.g. EEG, MEG) have attracted increasing attention considering their safety and generality. However, the exploration is not adequate in three aspects: 1) previous methods mainly focus on EEG but none of the previous works address this problem on MEG with better signal quality; 2) prior works have predominantly used $``teacher-forcing"$ during generative decoding, which is impractical; 3) prior works are mostly $``BART-based"$ not fully auto-regressive, which performs better in other sequence tasks. In this paper, we explore the brain-to-text translation of MEG signals in a speech-decoding formation. Here we are the first to investigate a cross-attention-based ``whisper" model for generating text directly from MEG signals without teacher forcing. Our model achieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining $\&$ teacher-forcing on two major datasets ($\textit{GWilliams}$ and $\textit{Schoffelen}$). This paper conducts a comprehensive review to understand how speech decoding formation performs on the neural decoding tasks, including pretraining initialization, training $\&$ evaluation set splitting, augmentation, and scaling law. Code is available at this https URL.

------------

`[2403.09732] PET-SQL: A Prompt-Enhanced Two-Round Refinement of Text-to-SQL with Cross-consistency <https://arxiv.org/abs/2403.09732>`__ PET-SQL:基于提示增强的两轮跨一致性Text-to-SQL精化

::

    replaced with revised version Sun, 2 Jun 2024 02:58:53 GMT
    Submission history From: Zhishuai Li [view email]
    [v1] Wed, 13 Mar 2024 02:32:41 UTC (153 KB)
    [v2] Mon, 18 Mar 2024 12:45:41 UTC (159 KB)
    [v3] Fri, 29 Mar 2024 03:21:01 UTC (159 KB)
    [v4] Sun, 2 Jun 2024 02:58:53 UTC (1,492 KB)
    Zhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye, Ziyue Li, Rui Zhao, Hangyu Mao

Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the prompt's schema information and instruct the LLM to produce the final SQL. Finally, as the post-refinement module, we propose using cross-consistency across different LLMs rather than self-consistency within a particular LLM. Our methods achieve new SOTA results on the Spider benchmark, with an execution accuracy of 87.6%.

------------

`[2404.06395] MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies <https://arxiv.org/abs/2404.06395>`__ MiniCPM:用可扩展的训练策略揭示小型语言模型的潜力

::

    replaced with revised version Mon, 3 Jun 2024 08:54:38 GMT
    Submission history From: Shengding Hu [view email]
    [v1] Tue, 9 Apr 2024 15:36:50 UTC (17,017 KB)
    [v2] Mon, 22 Apr 2024 08:26:33 UTC (20,009 KB)
    [v3] Mon, 3 Jun 2024 08:54:38 UTC (20,009 KB)
    Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun

The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at this https URL .

------------

`[2404.10306] Balancing Speciality and Versatility: a Coarse to Fine Framework for Supervised Fine-tuning Large Language Model <https://arxiv.org/abs/2404.10306>`__ 平衡特性和通用性:监督式微调大型语言模型的粗到精框架

::

    replaced with revised version Mon, 3 Jun 2024 10:42:36 GMT
    Submission history From: Hengyuan Zhang [view email]
    [v1] Tue, 16 Apr 2024 06:27:39 UTC (598 KB)
    [v2] Sun, 28 Apr 2024 12:22:41 UTC (579 KB)
    [v3] Thu, 16 May 2024 10:53:50 UTC (579 KB)
    [v4] Mon, 3 Jun 2024 10:42:36 UTC (599 KB)
    Hengyuan Zhang, Yanru Wu, Dawei Li, Sak Yang, Rui Zhao, Yong Jiang, Fei Tan

Aligned Large Language Models (LLMs) showcase remarkable versatility, capable of handling diverse real-world tasks. Meanwhile, aligned LLMs are also expected to exhibit speciality, excelling in specific applications. However, fine-tuning with extra data, a common practice to gain speciality, often leads to catastrophic forgetting (CF) of previously acquired versatility, hindering the model's performance across diverse tasks. In response to this challenge, we propose CoFiTune, a coarse to fine framework in an attempt to strike the balance between speciality and versatility. At the coarse-grained level, an empirical tree-search algorithm is utilized to pinpoint and update specific modules that are crucial for speciality, while keeping other parameters frozen; at the fine-grained level, a soft-masking mechanism regulates the update to the LLMs, mitigating the CF issue without harming speciality. In an overall evaluation of both speciality and versatility, CoFiTune consistently outperforms baseline methods across diverse tasks and model scales. Compared to the full-parameter SFT, CoFiTune leads to about 14% versatility improvement and marginal speciality loss on a 13B model. Lastly, based on further analysis, we provide a speculative insight into the information forwarding process in LLMs, which helps explain the effectiveness of the proposed method. The code is available at this https URL.

------------

`[2404.11999] Token-level Direct Preference Optimization <https://arxiv.org/abs/2404.11999>`__ 令牌级直接偏好优化

::

    replaced with revised version Sun, 2 Jun 2024 16:21:59 GMT
    Submission history From: Yongcheng Zeng [view email]
    [v1] Thu, 18 Apr 2024 08:49:38 UTC (305 KB)
    [v2] Tue, 28 May 2024 14:37:40 UTC (354 KB)
    [v3] Sun, 2 Jun 2024 16:21:59 UTC (355 KB)
    Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, Jun Wang

Fine-tuning pre-trained Large Language Models (LLMs) is essential to align them with human values and intentions. This process often utilizes methods like pairwise comparisons and KL divergence against a reference LLM, focusing on the evaluation of full answers generated by the models. However, the generation of these responses occurs in a token level, following a sequential, auto-regressive fashion. In this paper, we introduce Token-level Direct Preference Optimization (TDPO), a novel approach to align LLMs with human preferences by optimizing policy at the token level. Unlike previous methods, which face challenges in divergence efficiency, TDPO incorporates forward KL divergence constraints for each token, improving alignment and diversity. Utilizing the Bradley-Terry model for a token-based reward system, TDPO enhances the regulation of KL divergence, while preserving simplicity without the need for explicit reward modeling. Experimental results across various text tasks demonstrate TDPO's superior performance in balancing alignment with generation diversity. Notably, fine-tuning with TDPO strikes a better balance than DPO in the controlled sentiment generation and single-turn dialogue datasets, and significantly improves the quality of generated responses compared to both DPO and PPO-based RLHF methods. Our code is open-sourced at this https URL.

------------

`[2405.12933] Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs <https://arxiv.org/abs/2405.12933>`__ 蒙皮:llm中基于多利益相关者联盟的决策

::

    replaced with revised version Sun, 2 Jun 2024 18:48:56 GMT
    Submission history From: Bilgehan Sel [view email]
    [v1] Tue, 21 May 2024 17:04:44 UTC (628 KB)
    [v2] Sun, 2 Jun 2024 18:48:56 UTC (630 KB)
    Bilgehan Sel, Priya Shanmugasundaram, Mohammad Kachuee, Kun Zhou, Ruoxi Jia, Ming Jin

Large Language Models (LLMs) have shown remarkable capabilities in tasks such as summarization, arithmetic reasoning, and question answering. However, they encounter significant challenges in the domain of moral reasoning and ethical decision-making, especially in complex scenarios with multiple stakeholders. This paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing moral reasoning in LLMs by exploring decisions' consequences from multiple stakeholder perspectives. Central to SKIG's mechanism is simulating accountability for actions, which, alongside empathy exercises and risk assessment, is pivotal to its effectiveness. We validate SKIG's performance across various moral reasoning benchmarks with proprietary and opensource LLMs, and investigate its crucial components through extensive ablation analyses.

------------

`[2405.14555] Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models <https://arxiv.org/abs/2405.14555>`__ 细微偏差需要更细微的度量:评估大型语言模型中的代表性和亲和性偏差的双重指标

::

    replaced with revised version Mon, 3 Jun 2024 16:43:16 GMT
    Submission history From: Ali Emami Dr. [view email]
    [v1] Thu, 23 May 2024 13:35:34 UTC (8,747 KB)
    [v2] Sat, 25 May 2024 15:38:38 UTC (8,747 KB)
    [v3] Tue, 28 May 2024 03:05:45 UTC (8,746 KB)
    [v4] Mon, 3 Jun 2024 16:43:16 UTC (8,747 KB)
    Abhishek Kumar, Sarfaroz Yunusov, and Ali Emami

Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models' outputs toward particular social narratives. This study addresses two such biases within LLMs: representative bias, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and affinity bias, reflecting the models' evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to `bias fingerprints'. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions.

------------

`[2405.16277] Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge <https://arxiv.org/abs/2405.16277>`__ 描绘模糊性:Winograd模式挑战的视觉扭曲

::

    replaced with revised version Mon, 3 Jun 2024 16:42:55 GMT
    Submission history From: Ali Emami Dr. [view email]
    [v1] Sat, 25 May 2024 15:28:22 UTC (8,320 KB)
    [v2] Tue, 28 May 2024 03:13:46 UTC (8,320 KB)
    [v3] Mon, 3 Jun 2024 16:42:55 UTC (8,320 KB)
    Brendan Park, Madeline Janecek, Naser Ezzati-Jivan, Yifeng Li, and Ali Emami

Large Language Models (LLMs) have demonstrated remarkable success in tasks like the Winograd Schema Challenge (WSC), showcasing advanced textual common-sense reasoning. However, applying this reasoning to multimodal domains, where understanding text and images together is essential, remains a substantial challenge. To address this, we introduce WinoVis, a novel dataset specifically designed to probe text-to-image models on pronoun disambiguation within multimodal contexts. Utilizing GPT-4 for prompt generation and Diffusion Attentive Attribution Maps (DAAM) for heatmap analysis, we propose a novel evaluation framework that isolates the models' ability in pronoun disambiguation from other visual processing challenges. Evaluation of successive model versions reveals that, despite incremental advancements, Stable Diffusion 2.0 achieves a precision of 56.7% on WinoVis, only marginally surpassing random guessing. Further error analysis identifies important areas for future research aimed at advancing text-to-image models in their ability to interpret and interact with the complex visual world.

------------

`[2405.16282] Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models <https://arxiv.org/abs/2405.16282>`__ 幕后的信心:大型语言模型的信心概率对齐研究

::

    replaced with revised version Mon, 3 Jun 2024 16:41:53 GMT
    Submission history From: Ali Emami Dr. [view email]
    [v1] Sat, 25 May 2024 15:42:04 UTC (9,154 KB)
    [v2] Wed, 29 May 2024 13:05:16 UTC (9,152 KB)
    [v3] Mon, 3 Jun 2024 16:41:53 UTC (9,152 KB)
    Abhishek Kumar, Robert Morabito, Sanzhar Umbet, Jad Kabbara, and Ali Emami

As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM's internal confidence, quantified by token probabilities, to the confidence conveyed in the model's response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model's confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI's GPT-4 showed the strongest confidence-probability alignment, with an average Spearman's $\hat{\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness.

------------

`[2405.18741] Genshin: General Shield for Natural Language Processing with Large Language Models <https://arxiv.org/abs/2405.18741>`__ Genshin:基于大型语言模型的自然语言处理通用盾

::

    replaced with revised version Mon, 3 Jun 2024 08:35:07 GMT
    Submission history From: Xiao Peng [view email]
    [v1] Wed, 29 May 2024 04:04:05 UTC (1,787 KB)
    [v2] Mon, 3 Jun 2024 08:35:07 UTC (1,787 KB)
    Xiao Peng, Tao Liu, Ying Wang

Large language models (LLMs) like ChatGPT, Gemini, or LLaMA have been trending recently, demonstrating considerable advancement and generalizability power in countless domains. However, LLMs create an even bigger black box exacerbating opacity, with interpretability limited to few approaches. The uncertainty and opacity embedded in LLMs' nature restrict their application in high-stakes domains like financial fraud, phishing, etc. Current approaches mainly rely on traditional textual classification with posterior interpretable algorithms, suffering from attackers who may create versatile adversarial samples to break the system's defense, forcing users to make trade-offs between efficiency and robustness. To address this issue, we propose a novel cascading framework called Genshin (General Shield for Natural Language Processing with Large Language Models), utilizing LLMs as defensive one-time plug-ins. Unlike most applications of LLMs that try to transform text into something new or structural, Genshin uses LLMs to recover text to its original state. Genshin aims to combine the generalizability of the LLM, the discrimination of the median model, and the interpretability of the simple model. Our experiments on the task of sentimental analysis and spam detection have shown fatal flaws of the current median models and exhilarating results on LLMs' recovery ability, demonstrating that Genshin is both effective and efficient. In our ablation study, we unearth several intriguing observations. Utilizing the LLM defender, a tool derived from the 4th paradigm, we have reproduced BERT's 15% optimal mask rate results in the 3rd paradigm of NLP. Additionally, when employing the LLM as a potential adversarial tool, attackers are capable of executing effective attacks that are nearly semantically lossless.

------------

`[2405.18952] Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets <https://arxiv.org/abs/2405.18952>`__ 你确定吗?再次排序:为更好的偏好数据集进行重复排序

::

    replaced with revised version Sat, 1 Jun 2024 02:18:06 GMT
    Submission history From: Peter Devine [view email]
    [v1] Wed, 29 May 2024 10:08:31 UTC (7,070 KB)
    [v2] Sat, 1 Jun 2024 02:18:06 UTC (7,070 KB)
    Peter Devine

Training Large Language Models (LLMs) with Reinforcement Learning from AI Feedback (RLAIF) aligns model outputs more closely with human preferences. This involves an evaluator model ranking multiple candidate responses to user prompts. However, the rankings from popular evaluator models such as GPT-4 can be inconsistent. We propose the Repeat Ranking method - where we evaluate the same responses multiple times and train only on those responses which are consistently ranked. Using 2,714 prompts in 62 languages, we generated responses from 7 top multilingual LLMs and had GPT-4 rank them five times each. Evaluating on MT-Bench chat benchmarks in six languages, our method outperformed the standard practice of training on all available prompts. Our work highlights the quality versus quantity trade-off in RLAIF dataset generation and offers a stackable strategy for enhancing dataset and thus model quality.

------------

`[2405.19086] MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors <https://arxiv.org/abs/2405.19086>`__ MEMoE:基于混合专家适配器的模型编辑增强

::

    replaced with revised version Sun, 2 Jun 2024 02:32:31 GMT
    Submission history From: RenZhi Wang [view email]
    [v1] Wed, 29 May 2024 13:49:44 UTC (486 KB)
    [v2] Sun, 2 Jun 2024 02:32:31 UTC (481 KB)
    Renzhi Wang, Piji Li

Model editing aims to efficiently alter the behavior of Large Language Models (LLMs) within a desired scope, while ensuring no adverse impact on other inputs. Recent years have witnessed various model editing methods been proposed. However, these methods either exhibit poor overall performance or struggle to strike a balance between generalization and locality. We propose MEMoE, a model editing adapter utilizing a Mixture of Experts (MoE) architecture with a knowledge anchor routing strategy. MEMoE updates knowledge using a bypass MoE structure, keeping the original parameters unchanged to preserve the general ability of LLMs. And, the knowledge anchor routing ensures that inputs requiring similar knowledge are routed to the same expert, thereby enhancing the generalization of the updated knowledge. Experimental results show the superiority of our approach over both batch editing and sequential batch editing tasks, exhibiting exceptional overall performance alongside outstanding balance between generalization and locality. Our code will be available.

------------

`[2405.19266] PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications <https://arxiv.org/abs/2405.19266>`__ 儿科sgpt:用于儿科应用的大型语言模型作为中文医疗助理

::

    replaced with revised version Mon, 3 Jun 2024 15:27:10 GMT
    Submission history From: Dingkang Yang [view email]
    [v1] Wed, 29 May 2024 16:59:38 UTC (2,301 KB)
    [v2] Mon, 3 Jun 2024 15:27:10 UTC (2,299 KB)
    Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang

Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. Our model and dataset will be open-source for community development.

------------

`[2405.19327] MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series <https://arxiv.org/abs/2405.19327>`__ MAP-Neo:高性能透明双语大型语言模型系列

::

    replaced with revised version Sun, 2 Jun 2024 06:09:49 GMT
    Submission history From: Tianyu Zheng [view email]
    [v1] Wed, 29 May 2024 17:57:16 UTC (3,776 KB)
    [v2] Thu, 30 May 2024 17:17:21 UTC (3,777 KB)
    [v3] Sun, 2 Jun 2024 06:09:49 UTC (3,777 KB)
    Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen

Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.

------------

`[2405.19799] Unsupervised Mutual Learning of Dialogue Discourse Parsing and Topic Segmentation <https://arxiv.org/abs/2405.19799>`__ 对话篇章分析与话题分割的无监督互学习

::

    replaced with revised version Mon, 3 Jun 2024 08:13:10 GMT
    Submission history From: Jiahui Xu [view email]
    [v1] Thu, 30 May 2024 08:10:50 UTC (1,612 KB)
    [v2] Mon, 3 Jun 2024 08:13:10 UTC (1,612 KB)
    Jiahui Xu, Feng Jiang, Anningzhe Gao, Haizhou Li

The advancement of large language models (LLMs) has propelled the development of dialogue systems. Unlike the popular ChatGPT-like assistant model, which only satisfies the user's preferences, task-oriented dialogue systems have also faced new requirements and challenges in the broader business field. They are expected to provide correct responses at each dialogue turn, at the same time, achieve the overall goal defined by the task. By understanding rhetorical structures and topic structures via topic segmentation and discourse parsing, a dialogue system may do a better planning to achieve both objectives. However, while both structures belong to discourse structure in linguistics, rhetorical structure and topic structure are mostly modeled separately or with one assisting the other in the prior work. The interaction between these two structures has not been considered for joint modeling and mutual learning. Furthermore, unsupervised learning techniques to achieve the above are not well explored. To fill this gap, we propose an unsupervised mutual learning framework of two structures leveraging the global and local connections between them. We extend the topic modeling between non-adjacent discourse units to ensure global structural relevance with rhetorical structures. We also incorporate rhetorical structures into the topic structure through a graph neural network model to ensure local coherence consistency. Finally, we utilize the similarity between the two fused structures for mutual learning. The experimental results demonstrate that our methods outperform all strong baselines on two dialogue rhetorical datasets (STAC and Molweni), as well as dialogue topic datasets (Doc2Dial and TIAGE). We provide our code at this https URL.

------------

`[2308.13049] Bayesian Exploration Networks <https://arxiv.org/abs/2308.13049>`__ 

::

    replaced with revised version Mon, 3 Jun 2024 08:23:19 GMT
    Submission history From: Mattie Fellows [view email]
    [v1] Thu, 24 Aug 2023 19:35:58 UTC (6,418 KB)
    [v2] Tue, 19 Sep 2023 18:36:08 UTC (6,757 KB)
    [v3] Mon, 3 Jun 2024 08:23:19 UTC (12,544 KB)
    Mattie Fellows, Brandon Kaplowitz, Christian Schroeder de Witt and Shimon Whiteson

Bayesian reinforcement learning (RL) offers a principled and elegant approach for sequential decision making under uncertainty. Most notably, Bayesian agents do not face an exploration/exploitation dilemma, a major pathology of frequentist methods. However theoretical understanding of model-free approaches is lacking. In this paper, we introduce a novel Bayesian model-free formulation and the first analysis showing that model-free approaches can yield Bayes-optimal policies. We show all existing model-free approaches make approximations that yield policies that can be arbitrarily Bayes-suboptimal. As a first step towards model-free Bayes optimality, we introduce the Bayesian exploration network (BEN) which uses normalising flows to model both the aleatoric uncertainty (via density estimation) and epistemic uncertainty (via variational inference) in the Bellman operator. In the limit of complete optimisation, BEN learns true Bayes-optimal policies, but like in variational expectation-maximisation, partial optimisation renders our approach tractable. Empirical results demonstrate that BEN can learn true Bayes-optimal policies in tasks where existing model-free approaches fail.

------------

`[2309.00255] SortedNet: A Scalable and Generalized Framework for Training Modular Deep Neural Networks <https://arxiv.org/abs/2309.00255>`__ SortedNet:用于训练模块化深度神经网络的可扩展通用框架

::

    replaced with revised version Sat, 1 Jun 2024 08:04:02 GMT
    Submission history From: Mojtaba Valipour [view email]
    [v1] Fri, 1 Sep 2023 05:12:25 UTC (1,870 KB)
    [v2] Sun, 3 Mar 2024 05:26:03 UTC (3,078 KB)
    [v3] Sat, 1 Jun 2024 08:04:02 UTC (5,363 KB)
    Mojtaba Valipour, Mehdi Rezagholizadeh, Hossein Rajabzadeh, Parsa Kavehzadeh, Marzieh Tahaei, Boxing Chen, and Ali Ghodsi

Deep neural networks (DNNs) must cater to a variety of users with different performance needs and budgets, leading to the costly practice of training, storing, and maintaining numerous user/task-specific models. There are solutions in the literature to deal with single dynamic or many-in-one models instead of many individual networks; however, they suffer from significant drops in performance, lack of generalization across different model architectures or different dimensions (e.g. depth, width, attention blocks), heavy model search requirements during training, and training a limited number of sub-models. To address these limitations, we propose SortedNet, a generalized and scalable training solution to harness the inherent modularity of DNNs. Thanks to a generalized nested architecture (which we refer as \textit{sorted} architecture in this paper) with shared parameters and its novel update scheme combining random sub-model sampling and a new gradient accumulation mechanism, SortedNet enables the training of sub-models simultaneously along with the training of the main model (without any significant extra training or inference overhead), simplifies dynamic model selection, customizes deployment during inference, and reduces the model storage requirement significantly. The versatility and scalability of SortedNet are validated through various architectures and tasks, including LLaMA, BERT, RoBERTa (NLP tasks), ResNet and MobileNet (image classification) demonstrating its superiority over existing dynamic training methods. For example, we introduce a novel adaptive self-speculative approach based on sorted-training to accelerate large language models decoding. Moreover, SortedNet is able to train 160 sub-models at once, achieving at least 96\% of the original model's performance.

------------

`[2312.17493] Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning <https://arxiv.org/abs/2312.17493>`__ 基于联邦学习的大型语言模型差分隐私低秩自适应

::

    replaced with revised version Sun, 2 Jun 2024 06:31:21 GMT
    Submission history From: Daochen Zha [view email]
    [v1] Fri, 29 Dec 2023 06:50:38 UTC (1,806 KB)
    [v2] Sun, 2 Jun 2024 06:31:21 UTC (1,813 KB)
    Xiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Gao, Shan Zhong, Matt White, Meikang Qiu

The surge in interest and application of large language models (LLMs) has sparked a drive to fine-tune these models to suit specific applications, such as finance and medical science. However, concerns regarding data privacy have emerged, especially when multiple stakeholders aim to collaboratively enhance LLMs using sensitive data. In this scenario, federated learning becomes a natural choice, allowing decentralized fine-tuning without exposing raw data to central servers. Motivated by this, we investigate how data privacy can be ensured in LLM fine-tuning through practical federated learning approaches, enabling secure contributions from multiple parties to enhance LLMs. Yet, challenges arise: 1) despite avoiding raw data exposure, there is a risk of inferring sensitive information from model outputs, and 2) federated learning for LLMs incurs notable communication overhead. To address these challenges, this article introduces DP-LoRA, a novel federated learning algorithm tailored for LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism that adds noise in weight updates, maintaining individual data privacy while facilitating collaborative model training. Moreover, DP-LoRA optimizes communication efficiency via low-rank adaptation, minimizing the transmission of updated weights during distributed training. The experimental results across medical, financial, and general datasets using various LLMs demonstrate that DP-LoRA effectively ensures strict privacy constraints while minimizing communication overhead.

------------

`[2401.17505] Arrows of Time for Large Language Models <https://arxiv.org/abs/2401.17505>`__ 大型语言模型的时间箭头

::

    replaced with revised version Mon, 3 Jun 2024 17:35:04 GMT
    Submission history From: Vassilis Papadopoulos [view email]
    [v1] Tue, 30 Jan 2024 23:46:35 UTC (1,422 KB)
    [v2] Sun, 10 Mar 2024 14:33:49 UTC (1,423 KB)
    [v3] Mon, 3 Jun 2024 17:35:04 UTC (2,243 KB)
    Vassilis Papadopoulos, J\'er\'emie Wenger, Cl\'ement Hongler

We study the probabilistic modeling performed by Autoregressive Large Language Models (LLMs) through the angle of time directionality, addressing a question first raised in (Shannon, 1951). For large enough models, we empirically find a time asymmetry in their ability to learn natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.

------------

`[2401.18018] On Prompt-Driven Safeguarding for Large Language Models <https://arxiv.org/abs/2401.18018>`__ 大型语言模型的提示驱动保护

::

    replaced with revised version Mon, 3 Jun 2024 06:52:58 GMT
    Submission history From: Chujie Zheng [view email]
    [v1] Wed, 31 Jan 2024 17:28:24 UTC (3,000 KB)
    [v2] Mon, 4 Mar 2024 06:31:21 UTC (3,015 KB)
    [v3] Tue, 21 May 2024 05:51:46 UTC (3,015 KB)
    [v4] Mon, 3 Jun 2024 06:52:58 UTC (3,016 KB)
    Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, Nanyun Peng

Prepending model inputs with safety prompts is a common practice for safeguarding large language models (LLMs) against queries with harmful intents. However, the underlying working mechanisms of safety prompts have not been unraveled yet, restricting the possibility of automatically optimizing them to improve LLM safety. In this work, we investigate how LLMs' behavior (i.e., complying with or refusing user queries) is affected by safety prompts from the perspective of model representation. We find that in the representation space, the input queries are typically moved by safety prompts in a "higher-refusal" direction, in which models become more prone to refusing to provide assistance, even when the queries are harmless. On the other hand, LLMs are naturally capable of distinguishing harmful and harmless queries without safety prompts. Inspired by these findings, we propose a method for safety prompt optimization, namely DRO (Directed Representation Optimization). Treating a safety prompt as continuous, trainable embeddings, DRO learns to move the queries' representations along or opposite the refusal direction, depending on their harmfulness. Experiments with eight LLMs on out-of-domain and jailbreak benchmarks demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts, without compromising the models' general performance.

------------

`[2402.01306] KTO: Model Alignment as Prospect Theoretic Optimization <https://arxiv.org/abs/2402.01306>`__ KTO:基于前景理论优化的模型对齐

::

    replaced with revised version Mon, 3 Jun 2024 02:36:09 GMT
    Submission history From: Kawin Ethayarajh [view email]
    [v1] Fri, 2 Feb 2024 10:53:36 UTC (857 KB)
    [v2] Mon, 3 Jun 2024 02:36:09 UTC (975 KB)
    Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, Douwe Kiela

Kahneman & Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.

------------

`[2402.02456] tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs) <https://arxiv.org/abs/2402.02456>`__ tnGPS:基于大型语言模型的未知张量网络结构搜索算法

::

    replaced with revised version Sat, 1 Jun 2024 15:54:54 GMT
    Submission history From: Junhua Zeng [view email]
    [v1] Sun, 4 Feb 2024 12:06:13 UTC (1,663 KB)
    [v2] Sat, 1 Jun 2024 15:54:54 UTC (11,425 KB)
    Junhua Zeng, Chao Li, Zhun Sun, Qibin Zhao, Guoxu Zhou

Tensor networks are efficient for extremely high-dimensional representation, but their model selection, known as tensor network structure search (TN-SS), is a challenging problem. Although several works have targeted TN-SS, most existing algorithms are manually crafted heuristics with poor performance, suffering from the curse of dimensionality and local convergence. In this work, we jump out of the box, studying how to harness large language models (LLMs) to automatically discover new TN-SS algorithms, replacing the involvement of human experts. By observing how human experts innovate in research, we model their common workflow and propose an automatic algorithm discovery framework called tnGPS. The proposed framework is an elaborate prompting pipeline that instruct LLMs to generate new TN-SS algorithms through iterative refinement and enhancement. The experimental results demonstrate that the algorithms discovered by tnGPS exhibit superior performance in benchmarks compared to the current state-of-the-art methods.

------------

`[2402.02713] Position: What Can Large Language Models Tell Us about Time Series Analysis <https://arxiv.org/abs/2402.02713>`__ 职位:大型语言模型能告诉我们关于时间序列分析的什么

::

    replaced with revised version Sat, 1 Jun 2024 06:42:09 GMT
    Submission history From: Ming Jin [view email]
    [v1] Mon, 5 Feb 2024 04:17:49 UTC (878 KB)
    [v2] Sat, 1 Jun 2024 06:42:09 UTC (892 KB)
    Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang, Shirui Pan, Qingsong Wen

Time series analysis is essential for comprehending the complexities inherent in various realworld systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including time series modality switching and question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research.

------------

`[2403.06833] Can LLMs Separate Instructions From Data? And What Do We Even Mean By That? <https://arxiv.org/abs/2403.06833>`__ llm可以将指令与数据分离吗?这到底是什么意思呢?

::

    replaced with revised version Mon, 3 Jun 2024 12:04:50 GMT
    Submission history From: Egor Zverev [view email]
    [v1] Mon, 11 Mar 2024 15:48:56 UTC (181 KB)
    [v2] Mon, 3 Jun 2024 12:04:50 UTC (100 KB)
    Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert

Instruction-tuned Large Language Models (LLMs) show impressive results in numerous practical applications, but they lack essential safety features that are common in other areas of computer science, particularly an explicit separation of instructions and data. This makes them vulnerable to manipulations such as indirect prompt injections and generally unsuitable for safety-critical tasks. Surprisingly, there is currently no established definition or benchmark to quantify this phenomenon. In this work, we close this gap by introducing a formal measure for instruction-data separation and an empirical variant that is calculable from a model's outputs. We also present a new dataset, SEP, that allows estimating the measure for real-world models. Our results on various LLMs show that the problem of instruction-data separation is real: all models fail to achieve high separation, and canonical mitigation techniques, such as prompt engineering and fine-tuning, either fail to substantially improve separation or reduce model utility. The source code and SEP dataset are openly accessible at this https URL.

------------

`[2404.18239] SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning <https://arxiv.org/abs/2404.18239>`__ 灵魂:解锁LLM忘却的二阶优化力量

::

    replaced with revised version Mon, 3 Jun 2024 01:10:53 GMT
    Submission history From: Jinghan Jia [view email]
    [v1] Sun, 28 Apr 2024 16:31:32 UTC (332 KB)
    [v2] Fri, 31 May 2024 17:38:51 UTC (331 KB)
    [v3] Mon, 3 Jun 2024 01:10:53 UTC (332 KB)
    Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, Sijia Liu

Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility out of the scope of unlearning. While interest in studying LLM unlearning is growing,the impact of the optimizer choice for LLM unlearning remains under-explored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between {second-order optimization} and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order unlearning framework, termed SOUL, built upon the second-order clipped stochastic optimization (Sophia)-based LLM training method. SOUL extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, suggesting the promise of second-order optimization in providing a scalable and easily implementable solution for LLM unlearning.

------------

`[2404.18400] LLM-SR: Scientific Equation Discovery via Programming with Large Language Models <https://arxiv.org/abs/2404.18400>`__ LLM-SR:基于大型语言模型编程的科学方程发现

::

    replaced with revised version Sun, 2 Jun 2024 20:17:59 GMT
    Submission history From: Parshin Shojaee [view email]
    [v1] Mon, 29 Apr 2024 03:30:06 UTC (11,329 KB)
    [v2] Sun, 2 Jun 2024 20:17:59 UTC (11,316 KB)
    Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K Reddy

Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely high-dimensional combinatorial and nonlinear hypothesis spaces. Traditional methods of equation discovery, commonly known as symbolic regression, largely focus on extracting equations from data alone, often neglecting the rich domain-specific prior knowledge that scientists typically depend on. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data in an efficient manner. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its physical understanding, which are then optimized against data to estimate skeleton parameters. We demonstrate LLM-SR's effectiveness across three diverse scientific domains, where it discovers physically accurate equations that provide significantly better fits to in-domain and out-of-domain data compared to the well-established symbolic regression baselines. Incorporating scientific prior knowledge also enables LLM-SR to search the equation space more efficiently than baselines. Code is available at: this https URL

------------

`[2405.11930] Data Contamination Calibration for Black-box LLMs <https://arxiv.org/abs/2405.11930>`__ 黑箱llm的数据污染校准

::

    replaced with revised version Mon, 3 Jun 2024 05:21:54 GMT
    Submission history From: Wentao Ye [view email]
    [v1] Mon, 20 May 2024 10:12:23 UTC (396 KB)
    [v2] Mon, 3 Jun 2024 05:21:54 UTC (396 KB)
    Wentao Ye, Jiaqi Hu, Liyao Li, Haobo Wang, Gang Chen, Junbo Zhao

The rapid advancements of Large Language Models (LLMs) tightly associate with the expansion of the training data size. However, the unchecked ultra-large-scale training sets introduce a series of potential risks like data contamination, i.e. the benchmark data is used for training. In this work, we propose a holistic method named Polarized Augment Calibration (PAC) along with a new to-be-released dataset to detect the contaminated data and diminish the contamination effect. PAC extends the popular MIA (Membership Inference Attack) -- from machine learning community -- by forming a more global target at detecting training data to Clarify invisible training data. As a pioneering work, PAC is very much plug-and-play that can be integrated with most (if not all) current white- and black-box LLMs. By extensive experiments, PAC outperforms existing methods by at least 4.5%, towards data contamination detection on more 4 dataset formats, with more than 10 base LLMs. Besides, our application in real-world scenarios highlights the prominent presence of contamination and related issues.

------------

`[2405.12807] FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information <https://arxiv.org/abs/2405.12807>`__ FAdam: Adam是使用对角经验费雪信息的自然梯度优化器

::

    replaced with revised version Mon, 3 Jun 2024 11:55:11 GMT
    Submission history From: Dongseong Hwang [view email]
    [v1] Tue, 21 May 2024 13:58:17 UTC (166 KB)
    [v2] Thu, 23 May 2024 14:46:39 UTC (167 KB)
    [v3] Sun, 26 May 2024 10:59:04 UTC (166 KB)
    [v4] Tue, 28 May 2024 15:07:28 UTC (167 KB)
    [v5] Mon, 3 Jun 2024 11:55:11 UTC (167 KB)
    Dongseong Hwang

This paper establishes a mathematical foundation for the Adam optimizer, elucidating its connection to natural gradient descent through Riemannian and information geometry. We rigorously analyze the diagonal empirical Fisher information matrix (FIM) in Adam, clarifying all detailed approximations and advocating for the use of log probability functions as loss, which should be based on discrete distributions, due to the limitations of empirical FIM. Our analysis uncovers flaws in the original Adam algorithm, leading to proposed corrections such as enhanced momentum calculations, adjusted bias corrections, adaptive epsilon, and gradient clipping. We refine the weight decay term based on our theoretical framework. Our modified algorithm, Fisher Adam (FAdam), demonstrates superior performance across diverse domains including LLM, ASR, and VQ-VAE, achieving state-of-the-art results in ASR.

------------

`[2405.17233] CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs <https://arxiv.org/abs/2405.17233>`__ CLAQ:挑战llm低比特训练后量化的极限

::

    replaced with revised version Mon, 3 Jun 2024 02:46:53 GMT
    Submission history From: Haoyu Wang [view email]
    [v1] Mon, 27 May 2024 14:49:39 UTC (1,659 KB)
    [v2] Mon, 3 Jun 2024 02:46:53 UTC (1,659 KB)
    Haoyu Wang, Bei Liu, Hang Shao, Bo Xiao, Ke Zeng, Guanglu Wan, Yanmin Qian

Parameter quantization for Large Language Models (LLMs) has attracted increasing attentions recently in reducing memory costs and improving computational efficiency. Early approaches have been widely adopted. However, the existing methods suffer from poor performance in low-bit (such as 2 to 3 bits) scenarios. In this paper, we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ) framework by introducing three different types of adaptive strategies for LLM quantization. Firstly, a K-Means clustering based algorithm is proposed that allows dynamic generation of quantization centroids for each column of a parameter matrix. Secondly, we design an outlier-guided adaptive precision search strategy which can dynamically assign varying bit-widths to different columns. Finally, a dynamic outlier reservation scheme is developed to retain some parameters in their original float point precision, in trade off of boosted model performance. Experiments on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate that our methods achieve the state-of-the-art results across different bit settings, especially in extremely low-bit scenarios. Code is available at this https URL.

------------

`[2311.00889] Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code <https://arxiv.org/abs/2311.00889>`__ Generate and Pray:使用SALLMS评估LLM生成代码的安全性

::

    replaced with revised version Mon, 3 Jun 2024 15:50:23 GMT
    Submission history From: Mohammed Latif Siddiq [view email]
    [v1] Wed, 1 Nov 2023 22:46:31 UTC (304 KB)
    [v2] Mon, 3 Jun 2024 15:50:23 UTC (433 KB)
    Mohammed Latif Siddiq, Joanna C. S. Santos, Sajith Devareddy and Anna Muller

With the growing popularity of Large Language Models (LLMs) in software engineers' daily practices, it is important to ensure that the code generated by these tools is not only functionally correct but also free of vulnerabilities. Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code. There are two contributing factors to the insecure code generation. First, existing datasets used to evaluate LLMs do not adequately represent genuine software engineering tasks sensitive to security. Instead, they are often based on competitive programming challenges or classroom-type coding tasks. In real-world applications, the code produced is integrated into larger codebases, introducing potential security risks. Second, existing evaluation metrics primarily focus on the functional correctness of the generated code while ignoring security considerations. Therefore, in this paper, we described SALLM, a framework to benchmark LLMs' abilities to generate secure code systematically. This framework has three major components: a novel dataset of security-centric Python prompts, configurable assessment techniques to evaluate the generated code, and novel metrics to evaluate the models' performance from the perspective of secure code generation.

------------

`[2311.03356] GLaMM: Pixel Grounding Large Multimodal Model <https://arxiv.org/abs/2311.03356>`__ 

::

    replaced with revised version Sun, 2 Jun 2024 00:33:53 GMT
    Submission history From: Hanoona Bangalath Rasheed Ms [view email]
    [v1] Mon, 6 Nov 2023 18:59:57 UTC (3,416 KB)
    [v2] Fri, 29 Dec 2023 01:53:21 UTC (7,955 KB)
    [v3] Sun, 2 Jun 2024 00:33:53 UTC (7,951 KB)
    Hanoona Rasheed, Muhammad Maaz, Sahal Shaji Mullappilly, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Erix Xing, Ming-Hsuan Yang, Fahad S. Khan

Large Multimodal Models (LMMs) extend Large Language Models to the vision domain. Initial LMMs used holistic images and text prompts to generate ungrounded textual responses. Recently, region-level LMMs have been used to generate visually grounded responses. However, they are limited to only referring to a single object category at a time, require users to specify the regions, or cannot offer dense pixel-wise object grounding. In this work, we present Grounding LMM (GLaMM), the first model that can generate natural language responses seamlessly intertwined with corresponding object segmentation masks. GLaMM not only grounds objects appearing in the conversations but is flexible enough to accept both textual and optional visual prompts (region of interest) as input. This empowers users to interact with the model at various levels of granularity, both in textual and visual domains. Due to the lack of standard benchmarks for the novel setting of visually Grounded Conversation Generation (GCG), we introduce a comprehensive evaluation protocol with our curated grounded conversations. Our proposed GCG task requires densely grounded concepts in natural scenes at a large-scale. To this end, we propose a densely annotated Grounding-anything Dataset (GranD) using our proposed automated annotation pipeline that encompasses 7.5M unique concepts grounded in a total of 810M regions available with segmentation masks. Besides GCG, GLaMM also performs effectively on several downstream tasks, e.g., referring expression segmentation, image and region-level captioning and vision-language conversations.

------------

`[2312.14867] VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation <https://arxiv.org/abs/2312.14867>`__ VIEScore:条件图像合成评估的可解释指标

::

    replaced with revised version Mon, 3 Jun 2024 16:59:20 GMT
    Submission history From: Wing-Fung Ku [view email]
    [v1] Fri, 22 Dec 2023 17:45:19 UTC (2,561 KB)
    [v2] Mon, 3 Jun 2024 16:59:20 UTC (2,857 KB)
    Max Ku and Dongfu Jiang and Cong Wei and Xiang Yue and Wenhu Chen

In the rapidly advancing field of conditional image generation research, challenges such as limited explainability lie in effectively evaluating the performance and capabilities of various models. This paper introduces VIEScore, a Visual Instruction-guided Explainable metric for evaluating any conditional image generation tasks. VIEScore leverages general knowledge from Multimodal Large Language Models (MLLMs) as the backbone and does not require training or fine-tuning. We evaluate VIEScore on seven prominent tasks in conditional image tasks and found: (1) VIEScore (GPT4-o) achieves a high Spearman correlation of 0.4 with human evaluations, while the human-to-human correlation is 0.45. (2) VIEScore (with open-source MLLM) is significantly weaker than GPT-4o and GPT-4v in evaluating synthetic images. (3) VIEScore achieves a correlation on par with human ratings in the generation tasks but struggles in editing tasks. With these results, we believe VIEScore shows its great potential to replace human judges in evaluating image synthesis tasks.

------------

`[2402.01789] The Political Preferences of LLMs <https://arxiv.org/abs/2402.01789>`__ llm的政治偏好

::

    replaced with revised version Sun, 2 Jun 2024 04:48:36 GMT
    Submission history From: David Rozado [view email]
    [v1] Fri, 2 Feb 2024 02:43:10 UTC (2,098 KB)
    [v2] Sun, 2 Jun 2024 04:48:36 UTC (2,140 KB)
    David Rozado

I report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, I administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both closed and open source. When probed with questions/statements with political connotations, most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. This does not appear to be the case for five additional base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, the weak performance of the base models at coherently answering the tests' questions makes this subset of results inconclusive. Finally, I demonstrate that LLMs can be steered towards specific locations in the political spectrum through Supervised Fine-Tuning (SFT) with only modest amounts of politically aligned data, suggesting SFT's potential to embed political orientation in LLMs. With LLMs beginning to partially displace traditional information sources like search engines and Wikipedia, the societal implications of political biases embedded in LLMs are substantial.

------------

`[2402.10659] Network Formation and Dynamics Among Multi-LLMs <https://arxiv.org/abs/2402.10659>`__ 多llm之间的网络形成和动态

::

    replaced with revised version Sun, 2 Jun 2024 13:50:14 GMT
    Submission history From: Marios Papachristou [view email]
    [v1] Fri, 16 Feb 2024 13:10:14 UTC (13,793 KB)
    [v2] Tue, 12 Mar 2024 19:12:55 UTC (23,971 KB)
    [v3] Sun, 2 Jun 2024 13:50:14 UTC (24,986 KB)
    Marios Papachristou, Yuan Yuan

Social networks shape opinions, behaviors, and information dissemination in human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social interactions and networks becomes essential. Our study analyzes LLMs' network formation behavior to examine whether the dynamics of multiple LLMs are similar to or different from human social dynamics. We observe that LLMs exhibit key social network principles, including preferential attachment, triadic closure, homophily, community structure, and the small-world phenomenon, when asked about their preferences in network formation. We also investigate LLMs' decision-making based on real-world networks, revealing that triadic closure and homophily have a stronger influence than preferential attachment and that LLMs perform well in network formation predictions. Overall, our study opens up new possibilities for using LLMs in network science research and helps develop socially aware LLMs by shedding light on their social interaction behaviors and exploring their impacts on social dynamics.

------------

`[2402.14859] The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative <https://arxiv.org/abs/2402.14859>`__ 内部的狼:通过MLLM的操作人员向MLLM社会隐蔽地注入恶意

::

    replaced with revised version Mon, 3 Jun 2024 03:29:07 GMT
    Submission history From: Zhen Tan [view email]
    [v1] Tue, 20 Feb 2024 23:08:21 UTC (4,892 KB)
    [v2] Mon, 3 Jun 2024 03:29:07 UTC (19,451 KB)
    Zhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan Li, Yu Kong, Tianlong Chen, Huan Liu

Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. Our findings reveal that, an MLLM agent, when manipulated to produce specific prompts or instructions, can effectively ``infect'' other agents within a society of MLLMs. This infection leads to the generation and circulation of harmful outputs, such as dangerous instructions or misinformation, across the society. We also show the transferability of these indirectly generated prompts, highlighting their possibility in propagating malice through inter-agent communication. This research provides a critical insight into a new dimension of threat posed by MLLMs, where a single agent can act as a catalyst for widespread malevolent influence. Our work underscores the urgent need for developing robust mechanisms to detect and mitigate such covert manipulations within MLLM societies, ensuring their safe and ethical utilization in societal applications.

------------

`[2403.03407] Human vs. Machine: Behavioral Differences Between Expert Humans and Language Models in Wargame Simulations <https://arxiv.org/abs/2403.03407>`__ 人类vs.机器:战争模拟中人类专家和语言模型的行为差异

::

    replaced with revised version Mon, 3 Jun 2024 15:00:47 GMT
    Submission history From: Max Lamparth [view email]
    [v1] Wed, 6 Mar 2024 02:23:32 UTC (152 KB)
    [v2] Mon, 3 Jun 2024 15:00:47 UTC (196 KB)
    Max Lamparth, Anthony Corso, Jacob Ganz, Oriana Skylar Mastro, Jacquelyn Schneider, Harold Trinkunas

To some, the advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness while reducing the influence of human error and emotions. However, there is still debate about how AI systems, especially large language models (LLMs), behave compared to humans in high-stakes military decision-making scenarios with the potential for increased risks towards escalation and unnecessary conflicts. To test this potential and scrutinize the use of LLMs for such purposes, we use a new wargame experiment with 107 national security experts designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses in separate simulations. Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. Here, we show a considerable high-level agreement in the LLM and human responses and significant quantitative and qualitative differences in individual actions and strategic tendencies. These differences depend on intrinsic biases in LLMs regarding the appropriate level of violence following strategic instructions, the choice of LLM, and whether the LLMs are tasked to decide for a team of players directly or first to simulate dialog between players. When simulating the dialog, the discussions lack quality and maintain a farcical harmony. The LLM simulations cannot account for human player characteristics, showing no significant difference even for extreme traits, such as "pacifist" or "aggressive sociopath". Our results motivate policymakers to be cautious before granting autonomy or following AI-based strategy recommendations.

------------

`[2403.16687] Using EEG to investigate the effectiveness of applying ChatGPT <https://arxiv.org/abs/2403.16687>`__ 用脑电图研究ChatGPT应用的有效性

::

    replaced with revised version Mon, 3 Jun 2024 08:37:42 GMT
    Submission history From: Jiayue Zhang [view email]
    [v1] Mon, 25 Mar 2024 12:23:12 UTC (454 KB)
    [v2] Mon, 8 Apr 2024 09:23:43 UTC (455 KB)
    [v3] Mon, 22 Apr 2024 14:48:00 UTC (495 KB)
    [v4] Mon, 3 Jun 2024 08:37:42 UTC (495 KB)
    Jiayue Zhang, Yiheng Liu, Wenqi Cai, Lanlan Wu, Yali Peng, Jingjing Yu, Senqing Qi, Taotao Long, Bao Ge

In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course "Digital Image Processing". The research findings show comparable scores between the two groups on the retention test. However, students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test. Electroencephalography data revealed that students who interacted with ChatGPT exhibited higher levels of cognitive activity, suggesting that ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students. knowledge application and creativity were insignificant. Based upon the research findings, it is evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the dialogue teaching in information related courses. Combining ChatGPT with traditional human teachers might be a more ideal approach. The synergistic use of both can provide students with more comprehensive learning support, thus contributing to enhancing the quality of teaching.

------------

`[2405.18732] Gemini & Physical World: Large Language Models Can Estimate the Intensity of Earthquake Shaking from Multi-Modal Social Media Posts <https://arxiv.org/abs/2405.18732>`__ Gemini & Physical World:大型语言模型可以从多模态社交媒体帖子中估计地震震动的强度

::

    replaced with revised version Sun, 2 Jun 2024 21:21:34 GMT
    Submission history From: Seyed Mostafa Mousavi [view email]
    [v1] Wed, 29 May 2024 03:23:34 UTC (2,022 KB)
    [v2] Sun, 2 Jun 2024 21:21:34 UTC (11,519 KB)
    S. Mostafa Mousavi, Marc Stogaitis, Tajinder Gadh, Richard M Allen, Alexei Barski, Robert Bosch, Patrick Robertson, Nivetha Thiruverahan, Youngmin Cho

This paper presents a novel approach for estimating the ground shaking intensity using social media data and CCTV footage. Employing the Gemini Pro (Reid et al. 2024) model, a multi-modal language model, we demonstrate the ability to extract relevant information from unstructured data utilizing generative AI and natural language processing. The model output, in the form of Modified Mercalli Intensity (MMI) values, align well with independent observational data. Furthermore, our results suggest that beyond its advanced visual and auditory understanding abilities, Gemini appears to utilize additional sources of knowledge, including a simplified understanding of the general relationship between earthquake magnitude, distance, and MMI intensity, which it presumably acquired during its training, in its reasoning and decision-making processes. These findings raise intriguing questions about the extent of Gemini's general understanding of the physical world and its phenomena. The ability of Gemini to generate results consistent with established scientific knowledge highlights the potential of LLMs like Gemini in augmenting our understanding of complex physical phenomena such as earthquakes. More specifically, the results of this study highlight the potential of LLMs like Gemini to revolutionize citizen seismology by enabling rapid, effective, and flexible analysis of crowdsourced data from eyewitness accounts for assessing earthquake impact and providing crisis situational awareness. This approach holds great promise for improving early warning systems, disaster response, and overall resilience in earthquake-prone regions. This study provides a significant step toward harnessing the power of social media and AI for earthquake disaster mitigation.

------------

`[2405.20132] LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics <https://arxiv.org/abs/2405.20132>`__ LLaMEA:自动生成元启发式的大型语言模型进化算法

::

    replaced with revised version Sat, 1 Jun 2024 19:59:37 GMT
    Submission history From: Niki Van Stein [view email]
    [v1] Thu, 30 May 2024 15:10:59 UTC (8,079 KB)
    [v2] Sat, 1 Jun 2024 19:59:37 UTC (6,576 KB)
    Niki van Stein and Thomas B\"ack

Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to understand natural language and generate complex code snippets. This paper introduces a novel Large Language Model Evolutionary Algorithm (LLaMEA) framework, leveraging GPT models for the automated generation and refinement of algorithms. Given a set of criteria and a task definition (the search space), LLaMEA iteratively generates, mutates and selects algorithms based on performance metrics and feedback from runtime evaluations. This framework offers a unique approach to generating optimized algorithms without requiring extensive prior expertise. We show how this framework can be used to generate novel black-box metaheuristic optimization algorithms automatically. LLaMEA generates multiple algorithms that outperform state-of-the-art optimization algorithms (Covariance Matrix Adaptation Evolution Strategy and Differential Evolution) on the five dimensional black box optimization benchmark (BBOB). The algorithms also show competitive performance on the 10- and 20-dimensional instances of the test functions, although they have not seen such instances during the automated generation process. The results demonstrate the feasibility of the framework and identify future directions for automated generation and optimization of algorithms via LLMs.

------------

`[2405.20962] Large Language Models are Zero-Shot Next Location Predictors <https://arxiv.org/abs/2405.20962>`__ 大型语言模型是零样本下一位置预测器

::

    replaced with revised version Mon, 3 Jun 2024 15:10:53 GMT
    Submission history From: Massimiliano Luca [view email]
    [v1] Fri, 31 May 2024 16:07:33 UTC (252 KB)
    [v2] Mon, 3 Jun 2024 15:10:53 UTC (203 KB)
    Ciro Beneduce, Bruno Lepri, Massimiliano Luca

Predicting the locations an individual will visit in the future is crucial for solving many societal issues like disease diffusion and reduction of pollution among many others. The models designed to tackle next-location prediction, however, require a significant amount of individual-level information to be trained effectively. Such data may be scarce or even unavailable in some geographic regions or peculiar scenarios (e.g., cold-start in recommendation systems). Moreover, the design of a next-location predictor able to generalize or geographically transfer knowledge is still an open research challenge. Recent advances in natural language processing have led to a rapid diffusion of Large Language Models (LLMs) which have shown good generalization and reasoning capabilities. These insights, coupled with the recent findings that LLMs are rich in geographical knowledge, allowed us to believe that these models can act as zero-shot next-location predictors. This paper evaluates the capabilities of many popular LLMs in this role, specifically Llama, GPT-3.5 and Mistral 7B. After designing a proper prompt, we tested the models on three real-world mobility datasets. The results show that LLMs can obtain accuracies up to 32.4%, a significant relative improvement of over 600% when compared to sophisticated DL models specifically designed for human mobility. Moreover, we show that other LLMs are unable to perform the task properly. To prevent positively biased results, we also propose a framework inspired by other studies to test data contamination. Finally, we explored the possibility of using LLMs as text-based explainers for next-location prediction showing that can effectively provide an explanation for their decision. Notably, 7B models provide more generic, but still reliable, explanations compared to larger counterparts. Code: this http URL

------------

`[2401.04514] Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search <https://arxiv.org/abs/2401.04514>`__ 

::

    replaced with revised version Mon, 3 Jun 2024 06:50:26 GMT
    Submission history From: Haochen Li [view email]
    [v1] Tue, 9 Jan 2024 12:12:50 UTC (341 KB)
    [v2] Mon, 3 Jun 2024 06:50:26 UTC (342 KB)
    Haochen Li, Xin Zhou, Zhiqi Shen

In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%), and fine-tuned dense (up to 23.6%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances. The source code and data are available at \url{this https URL}.

------------

`[2401.15641] PRE: A Peer Review Based Large Language Model Evaluator <https://arxiv.org/abs/2401.15641>`__ PRE:基于同行评议的大型语言模型评估器

::

    replaced with revised version Mon, 3 Jun 2024 11:11:13 GMT
    Submission history From: Zhumin Chu [view email]
    [v1] Sun, 28 Jan 2024 12:33:14 UTC (978 KB)
    [v2] Mon, 3 Jun 2024 11:11:13 UTC (1,000 KB)
    Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, Yiqun Liu

The impressive performance of large language models (LLMs) has attracted considerable attention from the academic and industrial communities. Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem. Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in long term. In order to address these issues, inspired by the peer review systems widely used in academic publication process, we propose a novel framework that can automatically evaluate LLMs through a peer-review process. Specifically, for the evaluation of a specific task, we first construct a small qualification exam to select "reviewers" from a couple of powerful LLMs. Then, to actually evaluate the "submissions" written by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to rate or compare the submissions. The final ranking of evaluatee LLMs is generated based on the results provided by all reviewers. We conducted extensive experiments on text summarization tasks with eleven LLMs including GPT-4. The results demonstrate the existence of biasness when evaluating using a single LLM. Also, our PRE model outperforms all the baselines, illustrating the effectiveness of the peer review mechanism.

------------

`[2402.03161] Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization <https://arxiv.org/abs/2402.03161>`__ Video-LaVIT:基于解耦视觉-运动标记化的统一视频语言预训练

::

    replaced with revised version Mon, 3 Jun 2024 08:09:09 GMT
    Submission history From: Yang Jin [view email]
    [v1] Mon, 5 Feb 2024 16:30:49 UTC (31,035 KB)
    [v2] Tue, 6 Feb 2024 06:35:36 UTC (31,035 KB)
    [v3] Mon, 3 Jun 2024 08:09:09 UTC (8,825 KB)
    Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, Yadong Mu

In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models are available at this https URL.

------------

`[2402.08017] Lumos : Empowering Multimodal LLMs with Scene Text Recognition <https://arxiv.org/abs/2402.08017>`__ Lumos:用场景文本识别赋予多模态llm

::

    replaced with revised version Sat, 1 Jun 2024 21:46:50 GMT
    Submission history From: Ashish Shenoy [view email]
    [v1] Mon, 12 Feb 2024 19:27:26 UTC (4,466 KB)
    [v2] Sat, 1 Jun 2024 21:46:50 UTC (4,468 KB)
    Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee, Mohsen Moslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu, Shicong Zhao, Longfang Zhao, Ankit Ramchandani, Xin Luna Dong, Anuj Kumar

We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.

------------

`[2405.18672] LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained Image Classification <https://arxiv.org/abs/2405.18672>`__ 基于llm的可解释细粒度图像分类的层次概念分解

::

    replaced with revised version Sun, 2 Jun 2024 23:30:46 GMT
    Submission history From: Renyi Qu [view email]
    [v1] Wed, 29 May 2024 00:36:56 UTC (789 KB)
    [v2] Sun, 2 Jun 2024 23:30:46 UTC (789 KB)
    Renyi Qu, Mark Yatskar

(Renyi Qu's Master's Thesis) Recent advancements in interpretable models for vision-language tasks have achieved competitive performance; however, their interpretability often suffers due to the reliance on unstructured text outputs from large language models (LLMs). This introduces randomness and compromises both transparency and reliability, which are essential for addressing safety issues in AI systems. We introduce \texttt{Hi-CoDe} (Hierarchical Concept Decomposition), a novel framework designed to enhance model interpretability through structured concept analysis. Our approach consists of two main components: (1) We use GPT-4 to decompose an input image into a structured hierarchy of visual concepts, thereby forming a visual concept tree. (2) We then employ an ensemble of simple linear classifiers that operate on concept-specific features derived from CLIP to perform classification. Our approach not only aligns with the performance of state-of-the-art models but also advances transparency by providing clear insights into the decision-making process and highlighting the importance of various concepts. This allows for a detailed analysis of potential failure modes and improves model compactness, therefore setting a new benchmark in interpretability without compromising the accuracy.

------------

`[2405.19076] Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials Analysis and Design <https://arxiv.org/abs/2405.19076>`__ cepalo:面向仿生材料分析与设计的多模态视觉-语言模型

::

    replaced with revised version Sun, 2 Jun 2024 15:03:24 GMT
    Submission history From: Markus Buehler [view email]
    [v1] Wed, 29 May 2024 13:34:32 UTC (41,481 KB)
    [v2] Sun, 2 Jun 2024 15:03:24 UTC (41,993 KB)
    Markus J. Buehler

We present Cephalo, a series of multimodal vision large language models (V-LLMs) designed for materials science applications, integrating visual and linguistic data for enhanced understanding and interaction within human-AI and multi-agent AI frameworks. A key innovation of Cephalo is its advanced dataset generation method, which employs a sophisticated algorithm to accurately detect and separate images and their corresponding textual descriptions from PDF documents, such as scientific papers. The method includes a careful refinement of image-text pairs through integrated vision and language processing, ensuring high-quality, contextually relevant, and well reasoned training data. Cephalo is trained on integrated image and text data extracted from thousands of scientific papers and science-focused Wikipedia pages demonstrates can interpret complex visual scenes, generate precise language descriptions, and answer queries about images effectively. The combination of a vision encoder with an autoregressive transformer supports complex natural language understanding in an integrated model, which can be coupled with other generative methods to create an image-to-text-to-image or image-to-text-to-3D pipeline. To explore the development of larger models from smaller ones, we report both mixture-of-expert methods and model merging. These hybrid approaches allow us to leverage the domain-specific expertise and general conversational capabilities to harness the strengths of multiple models. We examine the models in diverse use cases that incorporate biological materials, fracture and engineering analysis, protein biophysics, and bio-inspired design based on insect behavior. Generative applications include bio-inspired designs, including pollen-inspired architected materials, as well as the synthesis of bio-inspired material microstructures from a photograph of a solar eclipse.

------------

`[2402.01258] Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape <https://arxiv.org/abs/2402.01258>`__ transformer在上下文中学习非线性特征:注意力领域的非凸平均场动态

::

    replaced with revised version Sun, 2 Jun 2024 06:31:43 GMT
    Submission history From: Juno Kim [view email]
    [v1] Fri, 2 Feb 2024 09:29:40 UTC (137 KB)
    [v2] Sun, 2 Jun 2024 06:31:43 UTC (144 KB)
    Juno Kim and Taiji Suzuki

Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest.

------------

-----------
Index (167)
-----------

`[2406.00092] How Random is Random? Evaluating the Randomness and Humaness of LLMs' Coin Flips <https://arxiv.org/abs/2406.00092>`__ 有多随机才是随机?评估LLMs抛硬币的随机性和人性化

`[2406.00765] The Embodied World Model Based on LLM with Visual Information and Prediction-Oriented Prompts <https://arxiv.org/abs/2406.00765>`__ 基于LLM的具有视觉信息和预测提示的具身世界模型

`[2406.00018] Large Language Models' Detection of Political Orientation in Newspapers <https://arxiv.org/abs/2406.00018>`__ 大型语言模型对报纸政治倾向的识别

`[2406.00020] Harmful Speech Detection by Language Models Exhibits Gender-Queer Dialect Bias <https://arxiv.org/abs/2406.00020>`__ 语言模型的有害语音检测表现出性别歧视的方言偏见

`[2406.00024] Embedding-Aligned Language Models <https://arxiv.org/abs/2406.00024>`__ 嵌入对齐语言模型

`[2406.00025] SCALM: Towards Semantic Caching for Automated Chat Services with Large Language Models <https://arxiv.org/abs/2406.00025>`__ SCALM:基于大型语言模型的自动聊天服务语义缓存

`[2406.00027] Adapting PromptORE for Modern History: Information Extraction from Hispanic Monarchy Documents of the XVIth Century <https://arxiv.org/abs/2406.00027>`__ 为近代史改编的提示:从16世纪的西班牙君主制文件中提取信息

`[2406.00030] Large Language Model Pruning <https://arxiv.org/abs/2406.00030>`__ 大型语言模型剪枝

`[2406.00031] AMGPT: a Large Language Model for Contextual Querying in Additive Manufacturing <https://arxiv.org/abs/2406.00031>`__ AMGPT:面向增材制造上下文查询的大型语言模型

`[2406.00034] Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories <https://arxiv.org/abs/2406.00034>`__ 自适应激活转向:面向不同幻觉类别的无调谐LLM真实性改进方法

`[2406.00037] Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering <https://arxiv.org/abs/2406.00037>`__ 基于多角度用户偏好排序反馈的llm编程问答对齐

`[2406.00039] How Ready Are Generative Pre-trained Large Language Models for Explaining Bengali Grammatical Errors? <https://arxiv.org/abs/2406.00039>`__ 生成式预训练大型语言模型用于解释孟加拉语语法错误的准备情况如何?

`[2406.00041] QUB-Cirdan at "Discharge Me!": Zero shot discharge letter generation by open-source LLM <https://arxiv.org/abs/2406.00041>`__ QUB-Cirdan在“让我出院!”: Zero shot discharge letter generation by开源LLM

`[2406.00045] Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization <https://arxiv.org/abs/2406.00045>`__ 大型语言模型的个性化转向:基于双向偏好优化的通用转向向量

`[2406.00049] QUEST: Quality-Aware Metropolis-Hastings Sampling for Machine Translation <https://arxiv.org/abs/2406.00049>`__ QUEST:面向机器翻译的质量感知Metropolis-Hastings采样

`[2406.00050] An Empirical Analysis on Large Language Models in Debate Evaluation <https://arxiv.org/abs/2406.00050>`__ 大型语言模型在辩论评估中的实证分析

`[2406.00062] Unlocking the Potential of Large Language Models for Clinical Text Anonymization: A Comparative Study <https://arxiv.org/abs/2406.00062>`__ 释放临床文本匿名的大型语言模型的潜力:比较研究

`[2406.00069] Confidence-Aware Sub-Structure Beam Search (CABS): Mitigating Hallucination in Structured Data Generation with Large Language Models <https://arxiv.org/abs/2406.00069>`__ 置信度感知的子结构波束搜索(CABS):用大型语言模型缓解结构化数据生成中的幻觉

`[2406.00179] Long-Span Question-Answering: Automatic Question Generation and QA-System Ranking via Side-by-Side Evaluation <https://arxiv.org/abs/2406.00179>`__ 长跨度问答:基于并列评价的问题自动生成和问答系统排序

`[2406.00197] Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision <https://arxiv.org/abs/2406.00197>`__ 协同文档修订建模的整体框架和数据集

`[2406.00222] Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training <https://arxiv.org/abs/2406.00222>`__ 学习阐明:基于动作对比自我训练的多轮对话

`[2406.00343] Beyond Metrics: Evaluating LLMs' Effectiveness in Culturally Nuanced, Low-Resource Real-World Scenarios <https://arxiv.org/abs/2406.00343>`__ 超越指标:评估llm在文化细微差别、低资源现实世界场景中的有效性

`[2406.00380] The Best of Both Worlds: Toward an Honest and Helpful Large Language Model <https://arxiv.org/abs/2406.00380>`__ 两全其美:建立一个诚实而有用的大型语言模型

`[2406.00507] Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization <https://arxiv.org/abs/2406.00507>`__ 提示链还是逐步提示?文本摘要中的精炼

`[2406.00554] Guiding and Diversifying LLM-Based Story Generation via Answer Set Programming <https://arxiv.org/abs/2406.00554>`__ 通过答案集编程引导和多样化基于llm的故事生成

`[2406.00606] LLMs Could Autonomously Learn Without External Supervision <https://arxiv.org/abs/2406.00606>`__ llm可以在没有外部监督的情况下自主学习

`[2406.00627] Prompt Framework for Role-playing: Generation and Evaluation <https://arxiv.org/abs/2406.00627>`__ 角色扮演提示框架:生成与评估

`[2406.00628] Transforming Computer Security and Public Trust Through the Exploration of Fine-Tuning Large Language Models <https://arxiv.org/abs/2406.00628>`__ 通过微调大型语言模型的探索来转变计算机安全和公共信任

`[2406.00656] Presence or Absence: Are Unknown Word Usages in Dictionaries? <https://arxiv.org/abs/2406.00656>`__ 存在还是缺席:词典中未登录词的用法?

`[2406.00697] Topic Modeling for Short Texts with Large Language Models <https://arxiv.org/abs/2406.00697>`__ 基于大型语言模型的短文本主题建模

`[2406.00770] Automatic Instruction Evolving for Large Language Models <https://arxiv.org/abs/2406.00770>`__ 大型语言模型的自动指令演化

`[2406.00832] BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling <https://arxiv.org/abs/2406.00832>`__ 大型语言模型的BoNBoN对齐和n最佳抽样的甜蜜

`[2406.00839] FOCUS: Forging Originality through Contrastive Use in Self-Plagiarism for Language Models <https://arxiv.org/abs/2406.00839>`__ 重点:通过对比使用语言模型的自抄袭来锻造独创性

`[2406.00888] Show, Don't Tell: Aligning Language Models with Demonstrated Feedback <https://arxiv.org/abs/2406.00888>`__ 展示，而不是告诉:将语言模型与演示反馈相对齐

`[2406.00954] Annotation Guidelines-Based Knowledge Augmentation: Towards Enhancing Large Language Models for Educational Text Classification <https://arxiv.org/abs/2406.00954>`__ 基于标注指南的知识增强:面向教育文本分类的大型语言模型增强

`[2406.00969] Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media <https://arxiv.org/abs/2406.00969>`__ 使用RL来识别分裂的观点，提高了llm在社交媒体上识别社区的能力

`[2406.00975] Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost <https://arxiv.org/abs/2406.00975>`__ Luna:一种高精度低成本捕捉语言模型幻觉的评估基础模型

`[2406.01006] SemCoder: Training Code Language Models with Comprehensive Semantics <https://arxiv.org/abs/2406.01006>`__ SemCoder:具有全面语义的代码语言模型训练

`[2406.01026] Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors <https://arxiv.org/abs/2406.01026>`__ 加强的符号绑定使大型语言模型成为可靠的多项选择器

`[2406.01045] Decompose, Enrich, and Extract! Schema-aware Event Extraction using LLMs <https://arxiv.org/abs/2406.01045>`__ 分解、丰富和提取!模式感知的事件提取使用llm

`[2406.01179] Are AI-Generated Text Detectors Robust to Adversarial Perturbations? <https://arxiv.org/abs/2406.01179>`__ 人工智能生成的文本检测器对对抗性扰动鲁棒吗?

`[2406.01288] Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses <https://arxiv.org/abs/2406.01288>`__ 改进的少次越狱可以绕过对齐语言模型及其防御

`[2406.01311] FactGenius: Combining Zero-Shot Prompting and Fuzzy Relation Mining to Improve Fact Verification with Knowledge Graphs <https://arxiv.org/abs/2406.01311>`__ FactGenius:结合零样本提示和模糊关系挖掘改进基于知识图谱的事实验证

`[2406.01333] Probing Language Models for Pre-training Data Detection <https://arxiv.org/abs/2406.01333>`__ 用于预训练数据检测的语言模型探测

`[2406.01363] Privacy in LLM-based Recommendation: Recent Advances and Future Directions <https://arxiv.org/abs/2406.01363>`__ 基于llm推荐的隐私保护:最新进展与未来方向

`[2406.01375] D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models <https://arxiv.org/abs/2406.01375>`__ D-CPT定律:大型语言模型特定领域连续预训练缩放定律

`[2406.01382] Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function <https://arxiv.org/abs/2406.01382>`__ 大型语言模型的表现是否如人们预期的那样?测量人类的泛化功能

`[2406.01428] Superhuman performance in urology board questions by an explainable large language model enabled for context integration of the European Association of Urology guidelines: the UroBot study <https://arxiv.org/abs/2406.01428>`__ 通过一个可解释的大型语言模型在泌尿外科委员会问题上的超人表现，使欧洲泌尿外科协会指南的上下文集成成为可能:UroBot研究

`[2406.01436] Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models <https://arxiv.org/abs/2406.01436>`__ 编辑巨人的思维:大型语言模型知识编辑陷阱的深入探索

`[2406.01441] LexMatcher: Dictionary-centric Data Collection for LLM-based Machine Translation <https://arxiv.org/abs/2406.01441>`__ LexMatcher:基于llm机器翻译的以字典为中心的数据收集

`[2406.01506] The Geometry of Categorical and Hierarchical Concepts in Large Language Models <https://arxiv.org/abs/2406.01506>`__ 大型语言模型中范畴和层次概念的几何

`[2406.01514] Decoupled Alignment for Robust Plug-and-Play Adaptation <https://arxiv.org/abs/2406.01514>`__ 面向健壮即插即用适应的解耦对齐

`[2406.01538] What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores <https://arxiv.org/abs/2406.01538>`__ 大型语言模型在大脑中映射到什么?反对过度依赖大脑分数的案例

`[2406.01563] LoFiT: Localized Fine-tuning on LLM Representations <https://arxiv.org/abs/2406.01563>`__ LoFiT: LLM表示的局部微调

`[2406.00104] Scalable Bayesian Learning with posteriors <https://arxiv.org/abs/2406.00104>`__ 基于后验的可扩展贝叶斯学习

`[2406.00144] Query2CAD: Generating CAD models using natural language queries <https://arxiv.org/abs/2406.00144>`__ Query2CAD:使用自然语言查询生成CAD模型

`[2406.00209] Mamba State-Space Models Can Be Strong Downstream Learners <https://arxiv.org/abs/2406.00209>`__ Mamba状态空间模型可以成为强大的下游学习者

`[2406.00426] InterpreTabNet: Distilling Predictive Signals from Tabular Data by Salient Feature Interpretation <https://arxiv.org/abs/2406.00426>`__ InterpreTabNet:通过显著特征解释从表格数据中提取预测信号

`[2406.00509] Empirical influence functions to understand the logic of fine-tuning <https://arxiv.org/abs/2406.00509>`__ 经验影响函数来理解微调的逻辑

`[2406.00548] LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models <https://arxiv.org/abs/2406.00548>`__ LIDAO:关于去偏见(大型)语言模型的有限干预

`[2406.00806] Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection <https://arxiv.org/abs/2406.00806>`__

`[2406.00894] Pretrained Hybrids with MAD Skills <https://arxiv.org/abs/2406.00894>`__ 预训练的具有疯狂技能的混血儿

`[2406.01032] LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning <https://arxiv.org/abs/2406.01032>`__ LLM和GNN是互补的:提取LLM用于多模态图学习

`[2406.01124] Latent Logic Tree Extraction for Event Sequence Explanation from LLMs <https://arxiv.org/abs/2406.01124>`__ llm中事件序列解释的潜在逻辑树抽取

`[2406.01457] Differentially Private Tabular Data Synthesis using Large Language Models <https://arxiv.org/abs/2406.01457>`__ 基于大型语言模型的差分隐私表格数据合成

`[2406.01539] Physics-informed deep learning and compressive collocation for high-dimensional diffusion-reaction equations: practical existence theory and numerics <https://arxiv.org/abs/2406.01539>`__ 基于物理学的深度学习与压缩配置的高维扩散-反应方程:实践存在理论与数值

`[2406.00004] Navigating the Future of Federated Recommendation Systems with Foundation Models <https://arxiv.org/abs/2406.00004>`__

`[2406.00006] A Prompt-driven Task Planning Method for Multi-drones based on Large Language Model <https://arxiv.org/abs/2406.00006>`__ 一种基于大规模语言模型的提示驱动多无人机任务规划方法

`[2406.00014] KU-DMIS at EHRSQL 2024:Generating SQL query via question templatization in EHR <https://arxiv.org/abs/2406.00014>`__ KU-DMIS在EHRSQL 2024:在EHR中通过问题模板化生成SQL查询

`[2406.00231] LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking <https://arxiv.org/abs/2406.00231>`__ LLM-RankFusion:缓解基于llm排名的内在不一致性

`[2406.00247] Large Language Models for Relevance Judgment in Product Search <https://arxiv.org/abs/2406.00247>`__

`[2406.00258] Artemis: Towards Referential Understanding in Complex Videos <https://arxiv.org/abs/2406.00258>`__ Artemis:复杂视频中的参照理解

`[2406.00430] Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners <https://arxiv.org/abs/2406.00430>`__ 基于不确定性的闭环LLM规划器失效检测评估

`[2406.00584] A Blueprint Architecture of Compound AI Systems for Enterprise <https://arxiv.org/abs/2406.00584>`__ 面向企业的复合人工智能系统蓝图体系结构

`[2406.00667] An Early Investigation into the Utility of Multimodal Large Language Models in Medical Imaging <https://arxiv.org/abs/2406.00667>`__ 多模态大型语言模型在医学成像中的应用的早期研究

`[2406.00872] OLIVE: Object Level In-Context Visual Embeddings <https://arxiv.org/abs/2406.00872>`__

`[2406.01168] How Ethical Should AI Be? How AI Alignment Shapes the Risk Preferences of LLMs <https://arxiv.org/abs/2406.01168>`__ 人工智能应该有多道德?AI对齐如何塑造llm的风险偏好

`[2406.01285] Large Language Models as Recommender Systems: A Study of Popularity Bias <https://arxiv.org/abs/2406.01285>`__ 大型语言模型作为推荐系统:流行度偏差的研究

`[2406.01309] REvolve: Reward Evolution with Large Language Models for Autonomous Driving <https://arxiv.org/abs/2406.01309>`__ 旋转:奖励自动驾驶大型语言模型的进化

`[2406.01394] PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration <https://arxiv.org/abs/2406.01394>`__

`[2406.00799] Are you still on track!? Catching LLM Task Drift with Activations <https://arxiv.org/abs/2406.00799>`__ 你还在正轨上吗?用激活捕捉LLM任务漂移

`[2406.01422] How to Understand Whole Software Repository? <https://arxiv.org/abs/2406.01422>`__ 如何理解整个软件仓库?

`[2406.00054] $\epsilon$-Optimally Solving Zero-Sum POSGs <https://arxiv.org/abs/2406.00054>`__ $\epsilon$-零和posg的最优求解

`[2406.00237] A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases <https://arxiv.org/abs/2406.00237>`__ CNN、ResNet和视觉transformer用于胸部疾病多分类的比较研究

`[2406.01566] Helix: Distributed Serving of Large Language Models via Max-Flow on Heterogeneous GPUs <https://arxiv.org/abs/2406.01566>`__

`[2310.17807] Clover: Closed-Loop Verifiable Code Generation <https://arxiv.org/abs/2310.17807>`__ Clover:闭环可验证代码生成

`[2403.12451] INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations <https://arxiv.org/abs/2403.12451>`__ 洞察:基于语言解释的端到端神经符号视觉强化学习

`[2404.11515] Embedding Privacy in Computational Social Science and Artificial Intelligence Research <https://arxiv.org/abs/2404.11515>`__ 计算社会科学与人工智能研究中的隐私嵌入

`[2405.11143] OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework <https://arxiv.org/abs/2405.11143>`__ OpenRLHF:一个易用、可扩展、高性能的RLHF框架

`[2405.19616] Easy Problems That LLMs Get Wrong <https://arxiv.org/abs/2405.19616>`__ llm会出错的简单问题

`[2308.07120] Position: Key Claims in LLM Research Have a Long Tail of Footnotes <https://arxiv.org/abs/2308.07120>`__ 职位:LLM研究中的关键主张有很多脚注

`[2308.10379] Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models <https://arxiv.org/abs/2308.10379>`__

`[2309.08637] TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild <https://arxiv.org/abs/2309.08637>`__ TextBind:野外多回合交错多模态指令跟踪

`[2310.08540] Do pretrained Transformers Learn In-Context by Gradient Descent? <https://arxiv.org/abs/2310.08540>`__ 预训练transformer是通过梯度下降在上下文中学习吗?

`[2311.07466] On Measuring Faithfulness or Self-consistency of Natural Language Explanations <https://arxiv.org/abs/2311.07466>`__ 自然语言解释的忠实度或自洽性的测量

`[2311.08045] Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM Game <https://arxiv.org/abs/2311.08045>`__ 对抗性偏好优化:通过RM-LLM游戏增强您的对齐

`[2311.09071] How Vocabulary Sharing Facilitates Multilingualism in LLaMA? <https://arxiv.org/abs/2311.09071>`__

`[2311.09154] CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models <https://arxiv.org/abs/2311.09154>`__ Clean - eval:受污染大型语言模型的干净评估

`[2311.09189] PsyEval: A Suite of Mental Health Related Tasks for Evaluating Large Language Models <https://arxiv.org/abs/2311.09189>`__ pyeval:一套用于评估大型语言模型的心理健康相关任务

`[2312.10302] One-Shot Learning as Instruction Data Prospector for Large Language Models <https://arxiv.org/abs/2312.10302>`__ 单次学习作为大型语言模型的指令数据勘探者

`[2312.11075] Split and Rephrase with Large Language Models <https://arxiv.org/abs/2312.11075>`__ 用大型语言模型进行拆分和重短语

`[2312.12999] Machine Mindset: An MBTI Exploration of Large Language Models <https://arxiv.org/abs/2312.12999>`__ 机器思维:大型语言模型的MBTI探索

`[2312.14591] Reasons to Reject? Aligning Language Models with Judgments <https://arxiv.org/abs/2312.14591>`__ 拒绝的理由?将语言模型与判断对齐

`[2401.04518] The Critique of Critique <https://arxiv.org/abs/2401.04518>`__ 批判的批判

`[2401.04854] Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs <https://arxiv.org/abs/2401.04854>`__ 语言模型更像图书馆还是更像图书管理员?Bibliotechnism, Novel Reference Problem，以及LLMs的态度

`[2401.08417] Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation <https://arxiv.org/abs/2401.08417>`__ 对比偏好优化:突破机器翻译中LLM性能的边界

`[2402.02801] KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models <https://arxiv.org/abs/2402.02801>`__ KS-Lottery:为多语言语言模型查找认证彩票

`[2402.04601] Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector <https://arxiv.org/abs/2402.04601>`__ Alirector:对齐增强的中文语法纠错器

`[2402.07616] Anchor-based Large Language Models <https://arxiv.org/abs/2402.07616>`__ 基于锚点的大型语言模型

`[2402.08277] Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering <https://arxiv.org/abs/2402.08277>`__ 忠实和强大的LLM专家以证据为基础的问题回答

`[2402.09025] SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks <https://arxiv.org/abs/2402.09025>`__ SLEB:通过冗余验证和消除变压器块精简llm

`[2402.09259] SyntaxShap: Syntax-aware Explainability Method for Text Generation <https://arxiv.org/abs/2402.09259>`__ SyntaxShap:面向文本生成的句法感知可解释性方法

`[2402.11073] AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators <https://arxiv.org/abs/2402.11073>`__ AFaCTA:使用可靠的LLM标注者辅助标注事实声明检测

`[2402.11192] I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses <https://arxiv.org/abs/2402.11192>`__ 如果你说我的语言，我会学得更好:用llm生成的响应来理解微调大型语言模型的优越性能

`[2402.11900] Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models <https://arxiv.org/abs/2402.11900>`__ 大型语言模型知识编辑中的多跳事实捷径研究

`[2402.13963] Towards Building Multilingual Language Model for Medicine <https://arxiv.org/abs/2402.13963>`__ 面向医学的多语言语言模型构建

`[2402.14526] Balanced Data Sampling for Language Model Training with Clustering <https://arxiv.org/abs/2402.14526>`__ 基于聚类的语言模型训练的平衡数据采样

`[2402.15043] KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models <https://arxiv.org/abs/2402.15043>`__ KIEval:基于知识的大型语言模型交互式评估框架

`[2402.17959] An Iterative Associative Memory Model for Empathetic Response Generation <https://arxiv.org/abs/2402.17959>`__ 面向共情反应生成的迭代联想记忆模型

`[2403.00862] NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism <https://arxiv.org/abs/2403.00862>`__ NewsBench:一个评估中文新闻大型语言模型编辑能力的系统评估框架

`[2403.01748] NeuSpeech: Decode Neural signal as Speech <https://arxiv.org/abs/2403.01748>`__

`[2403.09732] PET-SQL: A Prompt-Enhanced Two-Round Refinement of Text-to-SQL with Cross-consistency <https://arxiv.org/abs/2403.09732>`__ PET-SQL:基于提示增强的两轮跨一致性Text-to-SQL精化

`[2404.06395] MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies <https://arxiv.org/abs/2404.06395>`__ MiniCPM:用可扩展的训练策略揭示小型语言模型的潜力

`[2404.10306] Balancing Speciality and Versatility: a Coarse to Fine Framework for Supervised Fine-tuning Large Language Model <https://arxiv.org/abs/2404.10306>`__ 平衡特性和通用性:监督式微调大型语言模型的粗到精框架

`[2404.11999] Token-level Direct Preference Optimization <https://arxiv.org/abs/2404.11999>`__ 令牌级直接偏好优化

`[2405.12933] Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs <https://arxiv.org/abs/2405.12933>`__ 蒙皮:llm中基于多利益相关者联盟的决策

`[2405.14555] Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models <https://arxiv.org/abs/2405.14555>`__ 细微偏差需要更细微的度量:评估大型语言模型中的代表性和亲和性偏差的双重指标

`[2405.16277] Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge <https://arxiv.org/abs/2405.16277>`__ 描绘模糊性:Winograd模式挑战的视觉扭曲

`[2405.16282] Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models <https://arxiv.org/abs/2405.16282>`__ 幕后的信心:大型语言模型的信心概率对齐研究

`[2405.18741] Genshin: General Shield for Natural Language Processing with Large Language Models <https://arxiv.org/abs/2405.18741>`__ Genshin:基于大型语言模型的自然语言处理通用盾

`[2405.18952] Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets <https://arxiv.org/abs/2405.18952>`__ 你确定吗?再次排序:为更好的偏好数据集进行重复排序

`[2405.19086] MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors <https://arxiv.org/abs/2405.19086>`__ MEMoE:基于混合专家适配器的模型编辑增强

`[2405.19266] PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications <https://arxiv.org/abs/2405.19266>`__ 儿科sgpt:用于儿科应用的大型语言模型作为中文医疗助理

`[2405.19327] MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series <https://arxiv.org/abs/2405.19327>`__ MAP-Neo:高性能透明双语大型语言模型系列

`[2405.19799] Unsupervised Mutual Learning of Dialogue Discourse Parsing and Topic Segmentation <https://arxiv.org/abs/2405.19799>`__ 对话篇章分析与话题分割的无监督互学习

`[2308.13049] Bayesian Exploration Networks <https://arxiv.org/abs/2308.13049>`__

`[2309.00255] SortedNet: A Scalable and Generalized Framework for Training Modular Deep Neural Networks <https://arxiv.org/abs/2309.00255>`__ SortedNet:用于训练模块化深度神经网络的可扩展通用框架

`[2312.17493] Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning <https://arxiv.org/abs/2312.17493>`__ 基于联邦学习的大型语言模型差分隐私低秩自适应

`[2401.17505] Arrows of Time for Large Language Models <https://arxiv.org/abs/2401.17505>`__ 大型语言模型的时间箭头

`[2401.18018] On Prompt-Driven Safeguarding for Large Language Models <https://arxiv.org/abs/2401.18018>`__ 大型语言模型的提示驱动保护

`[2402.01306] KTO: Model Alignment as Prospect Theoretic Optimization <https://arxiv.org/abs/2402.01306>`__ KTO:基于前景理论优化的模型对齐

`[2402.02456] tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs) <https://arxiv.org/abs/2402.02456>`__ tnGPS:基于大型语言模型的未知张量网络结构搜索算法

`[2402.02713] Position: What Can Large Language Models Tell Us about Time Series Analysis <https://arxiv.org/abs/2402.02713>`__ 职位:大型语言模型能告诉我们关于时间序列分析的什么

`[2403.06833] Can LLMs Separate Instructions From Data? And What Do We Even Mean By That? <https://arxiv.org/abs/2403.06833>`__ llm可以将指令与数据分离吗?这到底是什么意思呢?

`[2404.18239] SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning <https://arxiv.org/abs/2404.18239>`__ 灵魂:解锁LLM忘却的二阶优化力量

`[2404.18400] LLM-SR: Scientific Equation Discovery via Programming with Large Language Models <https://arxiv.org/abs/2404.18400>`__ LLM-SR:基于大型语言模型编程的科学方程发现

`[2405.11930] Data Contamination Calibration for Black-box LLMs <https://arxiv.org/abs/2405.11930>`__ 黑箱llm的数据污染校准

`[2405.12807] FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information <https://arxiv.org/abs/2405.12807>`__ FAdam: Adam是使用对角经验费雪信息的自然梯度优化器

`[2405.17233] CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs <https://arxiv.org/abs/2405.17233>`__ CLAQ:挑战llm低比特训练后量化的极限

`[2311.00889] Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code <https://arxiv.org/abs/2311.00889>`__ Generate and Pray:使用SALLMS评估LLM生成代码的安全性

`[2311.03356] GLaMM: Pixel Grounding Large Multimodal Model <https://arxiv.org/abs/2311.03356>`__

`[2312.14867] VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation <https://arxiv.org/abs/2312.14867>`__ VIEScore:条件图像合成评估的可解释指标

`[2402.01789] The Political Preferences of LLMs <https://arxiv.org/abs/2402.01789>`__ llm的政治偏好

`[2402.10659] Network Formation and Dynamics Among Multi-LLMs <https://arxiv.org/abs/2402.10659>`__ 多llm之间的网络形成和动态

`[2402.14859] The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative <https://arxiv.org/abs/2402.14859>`__ 内部的狼:通过MLLM的操作人员向MLLM社会隐蔽地注入恶意

`[2403.03407] Human vs. Machine: Behavioral Differences Between Expert Humans and Language Models in Wargame Simulations <https://arxiv.org/abs/2403.03407>`__ 人类vs.机器:战争模拟中人类专家和语言模型的行为差异

`[2403.16687] Using EEG to investigate the effectiveness of applying ChatGPT <https://arxiv.org/abs/2403.16687>`__ 用脑电图研究ChatGPT应用的有效性

`[2405.18732] Gemini & Physical World: Large Language Models Can Estimate the Intensity of Earthquake Shaking from Multi-Modal Social Media Posts <https://arxiv.org/abs/2405.18732>`__ Gemini & Physical World:大型语言模型可以从多模态社交媒体帖子中估计地震震动的强度

`[2405.20132] LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics <https://arxiv.org/abs/2405.20132>`__ LLaMEA:自动生成元启发式的大型语言模型进化算法

`[2405.20962] Large Language Models are Zero-Shot Next Location Predictors <https://arxiv.org/abs/2405.20962>`__ 大型语言模型是零样本下一位置预测器

`[2401.04514] Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search <https://arxiv.org/abs/2401.04514>`__

`[2401.15641] PRE: A Peer Review Based Large Language Model Evaluator <https://arxiv.org/abs/2401.15641>`__ PRE:基于同行评议的大型语言模型评估器

`[2402.03161] Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization <https://arxiv.org/abs/2402.03161>`__ Video-LaVIT:基于解耦视觉-运动标记化的统一视频语言预训练

`[2402.08017] Lumos : Empowering Multimodal LLMs with Scene Text Recognition <https://arxiv.org/abs/2402.08017>`__ Lumos:用场景文本识别赋予多模态llm

`[2405.18672] LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained Image Classification <https://arxiv.org/abs/2405.18672>`__ 基于llm的可解释细粒度图像分类的层次概念分解

`[2405.19076] Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials Analysis and Design <https://arxiv.org/abs/2405.19076>`__ cepalo:面向仿生材料分析与设计的多模态视觉-语言模型

`[2402.01258] Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape <https://arxiv.org/abs/2402.01258>`__ transformer在上下文中学习非线性特征:注意力领域的非凸平均场动态

